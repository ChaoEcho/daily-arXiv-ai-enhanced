{"id": "2509.18113", "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u9886\u57df\u8bbe\u7f6e\u4e0b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u9650\u5236\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\uff08\u5982SPoT\uff09\u4f9d\u8d56\u56fa\u5b9a\u7684\u63d0\u793a\u6a21\u677f\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u3002\u8be5\u673a\u5236\u901a\u8fc7\u63d0\u793a\u6c60\u548c\u4efb\u52a1\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u52a8\u6001\u7ec4\u5408\u5e76\u5bf9\u9f50\u4e0d\u540c\u4efb\u52a1\u7684\u63d0\u793a\u3002\u5728\u63d0\u793a\u878d\u5408\u9636\u6bb5\uff0c\u5229\u7528\u4efb\u52a1\u5d4c\u5165\u548c\u95e8\u63a7\u673a\u5236\u7cbe\u7ec6\u63a7\u5236\u63d0\u793a\u4fe1\u53f7\uff0c\u786e\u4fdd\u4e0e\u4efb\u52a1\u9700\u6c42\u5bf9\u9f50\u5e76\u6784\u5efa\u7075\u6d3b\u7684\u4efb\u52a1\u95f4\u5171\u4eab\u8def\u5f84\u3002\u4f18\u5316\u76ee\u6807\u4fa7\u91cd\u4e8e\u8054\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5e76\u7eb3\u5165\u81ea\u52a8\u5b66\u4e60\u8c03\u5ea6\u6743\u91cd\u7684\u7b56\u7565\uff0c\u4ee5\u51cf\u8f7b\u4efb\u52a1\u5e72\u6270\u548c\u8d1f\u8fc1\u79fb\u3002", "result": "\u654f\u611f\u6027\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6240\u63d0\u673a\u5236\u5728\u4fdd\u6301\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u589e\u5f3a\u53ef\u8fc1\u79fb\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u63d0\u793a\u8c03\u5ea6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u8bed\u8a00\u7406\u89e3\u548c\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u5728\u7edf\u4e00\u591a\u4efb\u52a1\u5efa\u6a21\u548c\u8de8\u9886\u57df\u9002\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u96be\u9898\u3002"}}
{"id": "2509.18411", "pdf": "https://arxiv.org/pdf/2509.18411", "abs": "https://arxiv.org/abs/2509.18411", "authors": ["Sara Gonzalez", "Martin Vasquez", "Wilder Castellanos"], "title": "LIFY: IoT System for Monitoring Vital Signs of Elderly People", "categories": ["cs.NI", "68N01 (Primary), 68N99 (Secondary)", "J.3; B.4.1"], "comment": "4 pages, Presenten in Conference: VII Colombian Congress of\n  Bioengineering and Biomedical Engineering", "summary": "This article describes the implementation of a technological solution aimed\nat improving the recording of physiological signals in the elderly population\nresiding in geriatric facilities. The developed system consists of a smart\ndevice equipped with sensors for body temperature, heart rate, and blood oxygen\nlevels. This device establishes an Internet connection to transmit data to a\ncloud-based platform for storage. Within this platform, a dashboard has been\ncreated to visualize real-time values captured by the sensors, along with\nadditional functionalities such as user management and the configuration of\npersonalized alerts, which are transmitted to the solution's users through the\ninstant messaging system called Telegram.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u667a\u80fd\u8bbe\u5907\u4e0e\u4e91\u5e73\u53f0\u7ed3\u5408\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b\u517b\u8001\u673a\u6784\u8001\u5e74\u4eba\u7684\u751f\u7406\u4fe1\u53f7\uff08\u4f53\u6e29\u3001\u5fc3\u7387\u3001\u8840\u6c27\uff09\uff0c\u5e76\u901a\u8fc7Telegram\u53d1\u9001\u4e2a\u6027\u5316\u8b66\u62a5\u3002", "motivation": "\u65e8\u5728\u6539\u8fdb\u517b\u8001\u673a\u6784\u4e2d\u8001\u5e74\u4eba\u751f\u7406\u4fe1\u53f7\u7684\u8bb0\u5f55\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86\u914d\u5907\u4f53\u6e29\u3001\u5fc3\u7387\u3001\u8840\u6c27\u4f20\u611f\u5668\u7684\u667a\u80fd\u8bbe\u5907\uff0c\u901a\u8fc7\u4e92\u8054\u7f51\u5c06\u6570\u636e\u4f20\u8f93\u81f3\u4e91\u5e73\u53f0\u5b58\u50a8\u3002\u4e91\u5e73\u53f0\u63d0\u4f9b\u5b9e\u65f6\u53ef\u89c6\u5316\u4eea\u8868\u677f\u3001\u7528\u6237\u7ba1\u7406\u548c\u4e2a\u6027\u5316\u8b66\u62a5\u914d\u7f6e\u529f\u80fd\uff0c\u8b66\u62a5\u901a\u8fc7Telegram\u53d1\u9001\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542b\u667a\u80fd\u4f20\u611f\u8bbe\u5907\u3001\u4e91\u5b58\u50a8\u3001\u5b9e\u65f6\u53ef\u89c6\u5316\u4eea\u8868\u677f\u53ca\u4e2a\u6027\u5316\u8b66\u62a5\u529f\u80fd\u7684\u751f\u7406\u4fe1\u53f7\u76d1\u6d4b\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u63d0\u9ad8\u517b\u8001\u673a\u6784\u8001\u5e74\u4eba\u751f\u7406\u4fe1\u53f7\u76d1\u6d4b\u7684\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2509.18122", "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "categories": ["cs.CL"], "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GAUSS\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6570\u5b66\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u5341\u4e8c\u4e2a\u6838\u5fc3\u6280\u80fd\u7ef4\u5ea6\uff0c\u65e8\u5728\u63d0\u4f9b\u6a21\u578b\u6570\u5b66\u667a\u80fd\u7684\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u53ef\u80fd\u672a\u80fd\u5168\u9762\u53cd\u6620LLMs\u7684\u6f5c\u5728\u6570\u5b66\u667a\u80fd\u3002\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u6a21\u578b\u6570\u5b66\u80fd\u529b\u7684\u7efc\u5408\u753b\u50cf\uff0c\u8bc6\u522b\u5176\u771f\u5b9e\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "method": "\u5f15\u5165\u4e86GAUSS\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5c06\u6570\u5b66\u80fd\u529b\u5206\u4e3a\u5341\u4e8c\u4e2a\u6838\u5fc3\u6280\u80fd\u7ef4\u5ea6\uff0c\u5f52\u5c5e\u4e8e\u77e5\u8bc6\u4e0e\u7406\u89e3\u3001\u95ee\u9898\u89e3\u51b3\u4e0e\u6c9f\u901a\u3001\u5143\u6280\u80fd\u4e0e\u521b\u9020\u529b\u4e09\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u6839\u636e\u8ba4\u77e5\u6280\u80fd\u5bf9\u95ee\u9898\u8fdb\u884c\u5206\u7c7b\u5e76\u8bbe\u8ba1\u4efb\u52a1\u4ee5\u9694\u79bb\u7279\u5b9a\u80fd\u529b\uff0cGAUSS\u65e8\u5728\u6784\u5efa\u6a21\u578b\u6570\u5b66\u80fd\u529b\u7684\u5168\u9762\u3001\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u753b\u50cf\u3002", "result": "\u901a\u8fc7\u4f7f\u7528GAUSS\u57fa\u51c6\uff0c\u5206\u6790\u4e86GPT-5-thinking\u7684\u6280\u80fd\u6982\u51b5\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4e0eo4-mini-high\u6a21\u578b\u7684\u5dee\u5f02\u3002\u8fd9\u7a81\u51fa\u4e86\u591a\u7ef4\u5ea6\u3001\u57fa\u4e8e\u6280\u80fd\u8bc4\u4f30\u7684\u4ef7\u503c\u3002", "conclusion": "GAUSS\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u591a\u7ef4\u5ea6\u3001\u57fa\u4e8e\u6280\u80fd\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u5fe0\u5b9e\u5730\u53cd\u6620LLMs\u6f5c\u5728\u7684\u6570\u5b66\u667a\u80fd\uff0c\u5e76\u4e3a\u7406\u89e3\u6a21\u578b\u80fd\u529b\u5dee\u5f02\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2509.18443", "pdf": "https://arxiv.org/pdf/2509.18443", "abs": "https://arxiv.org/abs/2509.18443", "authors": ["Ioannis Panitsas", "Tolga O. Atalay", "Dragoslav Stojadinovic", "Angelos Stavrou", "Leandros Tassiulas"], "title": "5GC-Bench: A Framework for Stress-Testing and Benchmarking 5G Core VNFs", "categories": ["cs.NI"], "comment": null, "summary": "The disaggregated, cloud-native design of the 5G Core (5GC) enables\nflexibility and scalability but introduces significant challenges.\nControl-plane procedures involve complex interactions across multiple Virtual\nNetwork Functions (VNFs), while the user plane must sustain diverse and\nresource-intensive traffic. Existing tools often benchmark these dimensions in\nisolation, rely on synthetic workloads, or lack visibility into fine-grained\nresource usage. This paper presents 5GC-Bench, a modular framework for\nstress-testing the 5GC under realistic workloads. 5GC-Bench jointly emulates\nsignaling and service traffic, supporting both VNF profiling and end-to-end\nservice-chain analysis. By characterizing bottlenecks and resource demands, it\nprovides actionable insights for capacity planning and performance\noptimization. We integrated 5GC-Bench with the OpenAirInterface (OAI) 5GC and\ndeployed it on a real 5G testbed, demonstrating its ability to uncover resource\nconstraints and expose cross-VNF dependencies under scenarios that mirror\noperational 5G deployments. To foster reproducibility and further research, we\nrelease publicly all the artifacts.", "AI": {"tldr": "\u63d0\u51fa5GC-Bench\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u8d1f\u8f7d\u5bf95G\u6838\u5fc3\u7f51\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u4ee5\u8bc6\u522b\u74f6\u9888\u5e76\u4f18\u5316\u6027\u80fd\u3002", "motivation": "5G\u6838\u5fc3\u7f51\u7684\u89e3\u8026\u548c\u4e91\u539f\u751f\u8bbe\u8ba1\u867d\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u590d\u6742\u6311\u6218\uff08\u63a7\u5236\u9762VNF\u4ea4\u4e92\u590d\u6742\uff0c\u7528\u6237\u9762\u6d41\u91cf\u5bc6\u96c6\uff09\u3002\u73b0\u6709\u5de5\u5177\u591a\u5b64\u7acb\u57fa\u51c6\u6d4b\u8bd5\u3001\u4f9d\u8d56\u5408\u6210\u8d1f\u8f7d\u6216\u7f3a\u4e4f\u7cbe\u7ec6\u8d44\u6e90\u53ef\u89c1\u6027\u3002", "method": "\u63d0\u51fa\u4e865GC-Bench\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5bf95G\u6838\u5fc3\u7f51\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002\u5b83\u8054\u5408\u6a21\u62df\u4fe1\u4ee4\u548c\u670d\u52a1\u6d41\u91cf\uff0c\u652f\u6301VNF\u6027\u80fd\u5206\u6790\u548c\u7aef\u5230\u7aef\u670d\u52a1\u94fe\u5206\u6790\u3002", "result": "5GC-Bench\u6210\u529f\u4e0eOpenAirInterface (OAI) 5GC\u96c6\u6210\uff0c\u5e76\u5728\u771f\u5b9e5G\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u63ed\u793a\u4e86\u8d44\u6e90\u9650\u5236\u548c\u8de8VNF\u4f9d\u8d56\u6027\uff0c\u4e3a\u5bb9\u91cf\u89c4\u5212\u548c\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "conclusion": "5GC-Bench\u80fd\u6709\u6548\u8868\u5f815G\u6838\u5fc3\u7f51\u7684\u74f6\u9888\u548c\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u5b9e\u7528\u6d1e\u5bdf\u3002\u9879\u76ee\u6210\u679c\u5df2\u516c\u5f00\uff0c\u4fc3\u8fdb\u590d\u73b0\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.18156", "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRubin\u56e0\u679c\u6a21\u578b\u548c\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u8bc6\u522b\u6587\u672c\u4e2d\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u201c\u5408\u6210\u53cc\u80de\u80ce\u201d\u6765\u6a21\u62df\u56e0\u679c\u5e72\u9884\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u4ef6\u56e0\u679c\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u6a21\u5f0f\u548c\u591a\u8df3\u5173\u7cfb\u63a8\u7406\uff0c\u5bb9\u6613\u56e0\u975e\u6b63\u5f0f\u7684\u56e0\u679c\u8868\u8fbe\u548c\u865a\u5047\u56fe\u5f62\u63a8\u7406\u5bfc\u81f4\u9519\u8bef\u8bc6\u522b\u3002\u7814\u7a76\u65e8\u5728\u66f4\u9c81\u68d2\u5730\u533a\u5206\u56e0\u679c\u4e0e\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528Rubin\u56e0\u679c\u6a21\u578b\uff0c\u5c06\u524d\u4e00\u4e2a\u4e8b\u4ef6\u89c6\u4e3a\u201c\u6cbb\u7597\u201d\uff0c\u540e\u4e00\u4e2a\u4e8b\u4ef6\u89c6\u4e3a\u201c\u7ed3\u679c\u201d\u3002\u9274\u4e8e\u6587\u672c\u9886\u57df\u65e0\u6cd5\u5b9e\u9645\u5e72\u9884\uff0c\u6982\u5ff5\u4e0a\u5bfb\u627e\u4e00\u4e2a\u4e0e\u4e3b\u89d2\u5177\u6709\u76f8\u540c\u5386\u53f2\u4f46\u5728\u201c\u6cbb\u7597\u201d\u4e0a\u5b58\u5728\u5e72\u9884\u7684\u201c\u53cc\u80de\u80ce\u201d\u3002\u4e3a\u89e3\u51b3\u5339\u914d\u56f0\u96be\uff0c\u4f7f\u7528\u5408\u6210\u63a7\u5236\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u5408\u6210\u548c\u53cd\u6f14\u6280\u672f\uff0c\u4ece\u76f8\u5173\u5386\u53f2\u6570\u636e\u4e2d\u751f\u6210\u8fd9\u6837\u7684\u201c\u5408\u6210\u53cc\u80de\u80ce\u201d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56e0\u679c\u5173\u7cfb\u57fa\u51c6\u6570\u636e\u96c6COPES-hard\u4e0a\u8868\u73b0\u51fa\u6bd4\u5305\u62ecGPT-4\u5728\u5185\u7684\u5148\u524d\u65b9\u6cd5\u66f4\u5f3a\u7684\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408Rubin\u56e0\u679c\u6a21\u578b\u548c\u5408\u6210\u63a7\u5236\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u80fd\u591f\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u5730\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6280\u672f\u3002"}}
{"id": "2509.18465", "pdf": "https://arxiv.org/pdf/2509.18465", "abs": "https://arxiv.org/abs/2509.18465", "authors": ["Hongjae Nam", "Vishrant Tripathi", "David J. Love"], "title": "Using Age of Information for Throughput Optimal Spectrum Sharing", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": "16 pages, 10 figures", "summary": "We consider a spectrum sharing problem where two users attempt to communicate\nover N channels. The Primary User (PU) has prioritized transmissions and its\noccupancy on each channel over time can be modeled as a Markov chain. The\nSecondary User (SU) needs to determine which channels are free at each\ntime-slot and attempt opportunistic transmissions. The goal of the SU is to\nmaximize its own throughput, while simultaneously minimizing collisions with\nthe PU, and satisfying spectrum access constraints. To solve this problem, we\nfirst decouple the multiple-channel problem into N single-channel problems. For\neach decoupled problem, we prove that there exists an optimal threshold policy\nthat depends on the last observed PU occupancy and the freshness of this\noccupancy information. Second, we establish the indexability of the decoupled\nproblems by analyzing the structure of the optimal threshold policy. Using this\nstructure, we derive a Whittle index-based scheduling policy that allocates SU\ntransmissions using the Age of Information (AoI) of accessed channels. We also\nextend our insights to PU occupancy models that are correlated across channels\nand incorporate learning of unknown Markov transition matrices into our\npolicies. Finally, we provide detailed numerical simulations that demonstrate\nthe performance gains of our approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWhittle\u6307\u6570\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u5229\u7528\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u5e2e\u52a9\u6b21\u7528\u6237\uff08SU\uff09\u5728\u591a\u901a\u9053\u9891\u8c31\u5171\u4eab\u4e2d\u6700\u5927\u5316\u541e\u5410\u91cf\u5e76\u6700\u5c0f\u5316\u4e0e\u4e3b\u7528\u6237\uff08PU\uff09\u7684\u51b2\u7a81\u3002", "motivation": "\u6b21\u7528\u6237\uff08SU\uff09\u9700\u8981\u5728\u591a\u901a\u9053\u9891\u8c31\u5171\u4eab\u73af\u5883\u4e2d\uff0c\u5728\u6700\u5927\u5316\u81ea\u8eab\u541e\u5410\u91cf\u7684\u540c\u65f6\uff0c\u6700\u5c0f\u5316\u4e0e\u4e3b\u7528\u6237\uff08PU\uff09\u7684\u51b2\u7a81\u5e76\u6ee1\u8db3\u9891\u8c31\u8bbf\u95ee\u7ea6\u675f\u3002", "method": ["\u5c06\u591a\u901a\u9053\u95ee\u9898\u89e3\u8026\u4e3aN\u4e2a\u5355\u901a\u9053\u95ee\u9898\u3002", "\u8bc1\u660e\u6bcf\u4e2a\u5355\u901a\u9053\u95ee\u9898\u5b58\u5728\u4f9d\u8d56\u4e8e\u4e0a\u6b21\u89c2\u5bdf\u5230\u7684PU\u5360\u7528\u60c5\u51b5\u53ca\u5176\u65b0\u9c9c\u5ea6\u7684\u6700\u4f18\u9608\u503c\u7b56\u7565\u3002", "\u901a\u8fc7\u5206\u6790\u6700\u4f18\u9608\u503c\u7b56\u7565\u7684\u7ed3\u6784\uff0c\u5efa\u7acb\u4e86\u89e3\u8026\u95ee\u9898\u7684\u53ef\u7d22\u5f15\u6027\uff08indexability\uff09\u3002", "\u57fa\u4e8e\u6b64\u7ed3\u6784\uff0c\u63a8\u5bfc\u51fa\u5e76\u4f7f\u7528\u4e86\u4e00\u79cd\u57fa\u4e8eWhittle\u6307\u6570\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u8bbf\u95ee\u901a\u9053\u7684\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u6765\u5206\u914dSU\u4f20\u8f93\u3002", "\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u901a\u9053\u95f4\u76f8\u5173\u7684PU\u5360\u7528\u6a21\u578b\uff0c\u5e76\u878d\u5165\u672a\u77e5\u9a6c\u5c14\u53ef\u592b\u8f6c\u79fb\u77e9\u9635\u7684\u5b66\u4e60\u673a\u5236\u3002"], "result": ["\u8bc1\u660e\u4e86\u5355\u901a\u9053\u95ee\u9898\u4e2d\u5b58\u5728\u6700\u4f18\u9608\u503c\u7b56\u7565\u3002", "\u5efa\u7acb\u4e86\u95ee\u9898\u7684\u53ef\u7d22\u5f15\u6027\u3002", "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Whittle\u6307\u6570\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8be6\u7ec6\u7684\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u589e\u76ca\u3002"], "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Whittle\u6307\u6570\u8c03\u5ea6\u7b56\u7565\uff0c\u5229\u7528\u4fe1\u606f\u5e74\u9f84\u89e3\u51b3\u4e86\u6b21\u7528\u6237\u5728\u591a\u901a\u9053\u9891\u8c31\u5171\u4eab\u4e2d\u7684\u541e\u5410\u91cf\u6700\u5927\u5316\u4e0e\u51b2\u7a81\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.18158", "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "ZERA\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\uff0c\u4ee5\u4f4e\u5f00\u9500\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u5728\u591aLLM\u548c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709APO\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u7528\u6237\u63d0\u793a\uff0c\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6837\u672c\u548c\u957f\u8fed\u4ee3\u5468\u671f\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u6602\u4e14\u8106\u5f31\u3002", "method": "\u672c\u6587\u63d0\u51faZERA\uff08Zero-init Instruction Evolving Refinement Agent\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u539f\u5219\u3001\u4f4e\u5f00\u9500\u7684\u7cbe\u70bc\uff0c\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\u3002ZERA\u4f7f\u7528\u516b\u4e2a\u53ef\u6cdb\u5316\u6807\u51c6\u548c\u81ea\u52a8\u63a8\u65ad\u7684\u6743\u91cd\u5bf9\u63d0\u793a\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u7ed3\u6784\u5316\u8bc4\u4f30\u6765\u4fee\u6539\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZERA\u5728\u4e94\u79cdLLM\u548c\u4e5d\u4e2a\u6db5\u76d6\u63a8\u7406\u3001\u6458\u8981\u548c\u4ee3\u7801\u751f\u6210\u7684\u591a\u5143\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u7a81\u51fa\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u6709\u6548\u63d0\u793a\u6784\u5efa\u7684\u8d21\u732e\u3002", "conclusion": "ZERA\u901a\u8fc7\u5176\u72ec\u7279\u7684\u8054\u5408\u63d0\u793a\u4f18\u5316\u548c\u7ed3\u6784\u5316\u8bc4\u4f30\u673a\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edfAPO\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u8d44\u6e90\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18519", "pdf": "https://arxiv.org/pdf/2509.18519", "abs": "https://arxiv.org/abs/2509.18519", "authors": ["Michael Luby", "John Byers"], "title": "Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network Paths", "categories": ["cs.NI", "cs.DC"], "comment": "Technical report, 18 pages. Includes formal proofs of packet spray\n  discrepancy bounds and example path profile updates", "summary": "We present Whack-a-Mole, a deterministic packet spraying algorithm for\ndistributing packets across multiple network paths with provably tight\ndiscrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML\ntraining and inference workloads, where collective completion time (CCT) and\neffective training time ratio (ETTR) are highly sensitive to tail latency and\ntransport imbalance. Whack-a-Mole represents the path profile as a discrete\nallocation of $m$ selection units across $n$ paths and uses a bit-reversal\ncounter to choose a path for each packet. We prove that the discrepancy between\nexpected and actual packet counts per path is bounded by $O(\\log m)$ over any\ncontiguous packet sequence. The algorithm responds quickly to congestion\nfeedback by reducing allocations to degraded paths and redistributing load to\nhealthier ones. This combination of deterministic distribution, low per-packet\noverhead, and compatibility with erasure-coded transport makes Whack-a-Mole an\neffective building block for multipath transport protocols that aim to minimize\nCCT and maximize GPU utilization.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18101", "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5546\u4e1aLLM\u670d\u52a1\u4e0e\u672c\u5730\u90e8\u7f72\u5f00\u6e90LLM\u7684\u6210\u672c\u6548\u76ca\uff0c\u4ee5\u5e2e\u52a9\u7ec4\u7ec7\u786e\u5b9a\u672c\u5730\u90e8\u7f72\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u4f7f\u7528\u91cf\u548c\u6027\u80fd\u7684\u76c8\u4e8f\u5e73\u8861\u70b9\u3002", "motivation": "\u968f\u7740LLM\u7684\u666e\u53ca\uff0c\u7ec4\u7ec7\u9762\u4e34\u9009\u62e9\u5546\u4e1a\u4e91\u670d\u52a1\u6216\u672c\u5730\u90e8\u7f72\u5f00\u6e90\u6a21\u578b\u7684\u51b3\u7b56\u3002\u5546\u4e1a\u670d\u52a1\u867d\u6613\u7528\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u6570\u636e\u9690\u79c1\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u548c\u957f\u671f\u6210\u672c\u95ee\u9898\uff1b\u672c\u5730\u90e8\u7f72\u5219\u53d7\u9690\u79c1\u548c\u6210\u672c\u9a71\u52a8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30\u4f55\u65f6\u672c\u5730\u90e8\u7f72\u5728\u7ecf\u6d4e\u4e0a\u662f\u53ef\u884c\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u5206\u6790\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u6700\u65b0\u5f00\u6e90\u6a21\u578b\uff08\u5982Qwen, Llama, Mistral\u7b49\uff09\u7684\u786c\u4ef6\u9700\u6c42\u3001\u8fd0\u8425\u5f00\u652f\u548c\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u5c06\u5176\u672c\u5730\u90e8\u7f72\u7684\u603b\u6210\u672c\u4e0e\u4e3b\u8981\u4e91\u670d\u52a1\u5546\u7684\u8ba2\u9605\u8d39\u7528\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4f7f\u7528\u6c34\u5e73\u548c\u6027\u80fd\u9700\u6c42\u7684\u672c\u5730\u90e8\u7f72\u76c8\u4e8f\u5e73\u8861\u70b9\u9884\u4f30\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7ec4\u7ec7\u89c4\u5212\u5176LLM\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2509.18159", "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "PolypSeg-GradCAM\u662f\u4e00\u4e2a\u7ed3\u5408U-Net\u548cGrad-CAM\u7684\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u900f\u660e\u7684\u606f\u8089\u5206\u5272\uff0c\u6709\u671b\u6539\u8fdbAI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u548c\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u9884\u9632\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c (CRC) \u662f\u4e3b\u8981\u764c\u75c7\u6b7b\u56e0\uff0c\u80c3\u80a0\u606f\u8089\u662f\u5176\u524d\u5146\u3002\u65e9\u671f\u51c6\u786e\u5206\u5272\u606f\u8089\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u64cd\u4f5c\u8d39\u65f6\u4e14\u6613\u53d8\u3002\u6df1\u5ea6\u5b66\u4e60\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u963b\u788d\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86PolypSeg-GradCAM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06U-Net\u67b6\u6784\u4e0e\u68af\u5ea6\u52a0\u6743\u7c7b\u6fc0\u6d3b\u6620\u5c04 (Grad-CAM) \u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u5b9e\u73b0\u900f\u660e\u7684\u606f\u8089\u5206\u5272\u3002\u6a21\u578b\u5728\u5305\u542b1000\u5f20\u5e26\u6ce8\u91ca\u5185\u7aa5\u955c\u56fe\u50cf\u7684Kvasir-SEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "PolypSeg-GradCAM\u5c55\u73b0\u4e86\u7a33\u5065\u7684\u5206\u5272\u6027\u80fd\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e860.9257\u7684\u5e73\u5747IoU\uff0c\u5e76\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u9ad8\u4e8e0.96\u7684Dice\u7cfb\u6570 (F-score)\u3002Grad-CAM\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u6a21\u578b\u9884\u6d4b\u7531\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u9a71\u52a8\uff0c\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "conclusion": "PolypSeg-GradCAM\u901a\u8fc7\u7ed3\u5408\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5b9e\u73b0\u53ef\u9760\u3001\u503c\u5f97\u4fe1\u8d56\u7684AI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u9884\u9632\u3002"}}
{"id": "2509.18103", "pdf": "https://arxiv.org/pdf/2509.18103", "abs": "https://arxiv.org/abs/2509.18103", "authors": ["Jennifer Dodgson", "Michael Joedhitya", "Adith Ramdas", "Surender Suresh Kumar", "Adarsh Singh Chauhan", "Akira Rafhael", "Wang Mingshu", "Nordine Lotfi"], "title": "Machine Learnability as a Measure of Order in Aperiodic Sequences", "categories": ["cs.LG", "math.NT"], "comment": null, "summary": "Research on the distribution of prime numbers has revealed a dual character:\ndeterministic in definition yet exhibiting statistical behavior reminiscent of\nrandom processes. In this paper we show that it is possible to use an\nimage-focused machine learning model to measure the comparative regularity of\nprime number fields at specific regions of an Ulam spiral. Specifically, we\ndemonstrate that in pure accuracy terms, models trained on blocks extracted\nfrom regions of the spiral in the vicinity of 500m outperform models trained on\nblocks extracted from the region representing integers lower than 25m. This\nimplies existence of more easily learnable order in the former region than in\nthe latter. Moreover, a detailed breakdown of precision and recall scores seem\nto imply that the model is favouring a different approach to classification in\ndifferent regions of the spiral, focusing more on identifying prime patterns at\nlower numbers and more on eliminating composites at higher numbers. This aligns\nwith number theory conjectures suggesting that at higher orders of magnitude we\nshould see diminishing noise in prime number distributions, with averages\n(density, AP equidistribution) coming to dominate, while local randomness\nregularises after scaling by log x. Taken together, these findings point toward\nan interesting possibility: that machine learning can serve as a new\nexperimental instrument for number theory. Notably, the method shows potential\n1 for investigating the patterns in strong and weak primes for cryptographic\npurposes.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u4e4c\u62c9\u59c6\u87ba\u65cb\u4e2d\u7d20\u6570\u7684\u5206\u5e03\u89c4\u5f8b\uff0c\u53d1\u73b0\u9ad8\u9636\u6570\u533a\u57df\uff08\u7ea65\u4ebf\uff09\u6bd4\u4f4e\u9636\u6570\u533a\u57df\uff082500\u4e07\u4ee5\u4e0b\uff09\u5b58\u5728\u66f4\u5bb9\u6613\u5b66\u4e60\u7684\u89c4\u5f8b\u6027\uff0c\u8868\u660e\u673a\u5668\u5b66\u4e60\u53ef\u4f5c\u4e3a\u6570\u8bba\u7684\u65b0\u5b9e\u9a8c\u5de5\u5177\u3002", "motivation": "\u7d20\u6570\u5b9a\u4e49\u4e0a\u786e\u5b9a\uff0c\u884c\u4e3a\u4e0a\u5374\u5448\u73b0\u7edf\u8ba1\u548c\u968f\u673a\u6027\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6d4b\u91cf\u4e4c\u62c9\u59c6\u87ba\u65cb\u7279\u5b9a\u533a\u57df\u7d20\u6570\u573a\u7684\u6bd4\u8f83\u89c4\u5f8b\u6027\uff0c\u5e76\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u80fd\u5426\u4f5c\u4e3a\u6570\u8bba\uff0c\u7279\u522b\u662f\u52a0\u5bc6\u5e94\u7528\u4e2d\u7d20\u6570\u6a21\u5f0f\u7814\u7a76\u7684\u65b0\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u56fe\u50cf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4e4c\u62c9\u59c6\u87ba\u65cb\u4e0a\u5bf9\u7d20\u6570\u5206\u5e03\u8fdb\u884c\u5206\u6790\u3002\u5177\u4f53\u5730\uff0c\u6a21\u578b\u5728\u4ece\u87ba\u65cb\u7684\u7279\u5b9a\u533a\u57df\uff08\u4f8b\u5982\uff0c\u63a5\u8fd15\u4ebf\u7684\u533a\u57df\u548c\u4f4e\u4e8e2500\u4e07\u7684\u533a\u57df\uff09\u63d0\u53d6\u7684\u56fe\u50cf\u5757\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u6bd4\u8f83\u5176\u51c6\u786e\u5ea6\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u4ee5\u8861\u91cf\u89c4\u5f8b\u6027\u3002", "result": "\u5728\u7eaf\u51c6\u786e\u5ea6\u65b9\u9762\uff0c\u6a21\u578b\u57285\u4ebf\u5de6\u53f3\u533a\u57df\u8bad\u7ec3\u7684\u8868\u73b0\u4f18\u4e8e2500\u4e07\u4ee5\u4e0b\u533a\u57df\uff0c\u8868\u660e\u9ad8\u9636\u6570\u533a\u57df\u5b58\u5728\u66f4\u5bb9\u6613\u5b66\u4e60\u7684\u89c4\u5f8b\u3002\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u91c7\u7528\u4e0d\u540c\u5206\u7c7b\u7b56\u7565\uff1a\u4f4e\u9636\u6570\u65f6\u4fa7\u91cd\u8bc6\u522b\u7d20\u6570\u6a21\u5f0f\uff0c\u9ad8\u9636\u6570\u65f6\u4fa7\u91cd\u6392\u9664\u5408\u6570\uff0c\u8fd9\u4e0e\u6570\u8bba\u5173\u4e8e\u9ad8\u9636\u7d20\u6570\u5206\u5e03\u566a\u58f0\u9012\u51cf\u7684\u731c\u60f3\u4e00\u81f4\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u4f5c\u4e3a\u6570\u8bba\u7684\u4e00\u79cd\u65b0\u5b9e\u9a8c\u5de5\u5177\uff0c\u4e3a\u63a2\u7d22\u7d20\u6570\u5206\u5e03\u89c4\u5f8b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u8be5\u65b9\u6cd5\u5728\u9ad8\u9636\u6570\u7d20\u6570\u4e2d\u53d1\u73b0\u7684\u66f4\u6613\u5b66\u4e60\u7684\u79e9\u5e8f\u4e0e\u73b0\u6709\u6570\u8bba\u731c\u60f3\u76f8\u7b26\uff0c\u5e76\u663e\u793a\u51fa\u5728\u5bc6\u7801\u5b66\u4e2d\u7814\u7a76\u5f3a\u5f31\u7d20\u6570\u6a21\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18163", "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u5206\u6b65\u601d\u8003\u6a21\u5f0f\u201d\u662f\u628a\u53cc\u5203\u5251\uff1a\u6709\u76ca\u4fe1\u606f\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u8bef\u5bfc\u4fe1\u606f\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0c\u4e14\u601d\u8003\u8fc7\u7a0b\u4f1a\u653e\u5927\u9519\u8bef\u3002\u6311\u6218\u5728\u4e8e\u8d4b\u4e88\u6a21\u578b\u8bc4\u4f30\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4ec5\u4f7f\u5176\u601d\u8003\u3002", "motivation": "LLMs\u5728\u590d\u6742\u9886\u57df\u5e94\u7528\u4e2d\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u5e38\u8f85\u4ee5\u53ef\u80fd\u6709\u7528\u3001\u65e0\u5173\u6216\u8bef\u5bfc\u7684\u5916\u90e8\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u6b64\u7c7b\u8f85\u52a9\u4fe1\u606f\u5bf9\u5177\u5907\u663e\u5f0f\u5206\u6b65\u601d\u8003\u80fd\u529b\u7684LLMs\u63a8\u7406\u8fc7\u7a0b\u7684\u56e0\u679c\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aSciAux\u7684\u65b0\u6570\u636e\u96c6\uff08\u6e90\u81eaScienceQA\uff09\uff0c\u7cfb\u7edf\u5730\u6d4b\u8bd5\u6a21\u578b\u5bf9\u4e0d\u540c\u7c7b\u578b\u8f85\u52a9\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u7684\u201c\u5ba1\u614e\u601d\u8003\u6a21\u5f0f\u201d\u662f\u4e00\u628a\u53cc\u5203\u5251\u3002\u6709\u76ca\u4e0a\u4e0b\u6587\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u8bef\u5bfc\u4fe1\u606f\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0c\u4e14\u8fd9\u79cd\u4e0b\u964d\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u88ab\u653e\u5927\u3002\u601d\u8003\u975e\u4f46\u672a\u80fd\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u53cd\u800c\u4f1a\u52a0\u5267\u8bef\u5bfc\u4fe1\u606f\u5e26\u6765\u7684\u9519\u8bef\u7a0b\u5ea6\u3002", "conclusion": "\u5173\u952e\u6311\u6218\u4e0d\u5728\u4e8e\u4ec5\u4ec5\u8ba9\u6a21\u578b\u201c\u601d\u8003\u201d\uff0c\u800c\u5728\u4e8e\u8d4b\u4e88\u5b83\u4eec\u8bc4\u4f30\u5176\u63a8\u7406\u6240\u4f9d\u636e\u4fe1\u606f\u7684\u6279\u5224\u6027\u80fd\u529b\uff0c\u4ee5\u6709\u6548\u5e94\u5bf9\u8bef\u5bfc\u4fe1\u606f\u3002"}}
{"id": "2509.18545", "pdf": "https://arxiv.org/pdf/2509.18545", "abs": "https://arxiv.org/abs/2509.18545", "authors": ["Ioannis Panitsas", "Tolga O. Atalay", "Dragoslav Stojadinovic", "Angelos Stavrou", "Leandros Tassiulas"], "title": "Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning", "categories": ["cs.NI"], "comment": null, "summary": "Cellular networks are increasingly realized through software-based entities,\nwith core functions deployed as Virtual Network Functions (VNFs) on\nCommercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key\nenabler of 5G by providing logically isolated Quality of Service (QoS)\nguarantees for diverse applications. With the adoption of cloud-native\ninfrastructures, the placement of network slices across heterogeneous\nmulti-cloud environments poses new challenges due to variable resource\ncapabilities and slice-specific requirements. This paper introduces a modular\nframework for autonomous and near-optimal VNF placement based on a\ndisaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework\nincorporates real traffic profiles to estimate slice resource demands and\nemploys a MARL-based scheduler to minimize deployment cost while meeting QoS\nconstraints. Experimental evaluation on a multi-cloud testbed shows a 19x\nspeed-up compared to combinatorial optimization, with deployment costs within\n7.8% of the optimal. While the method incurs up to 2.42x more QoS violations\nunder high load, the trade-off provides significantly faster decision-making\nand reduced computational complexity. These results suggest that MARL-based\napproaches offer a scalable and cost-efficient solution for real-time network\nslice placement in heterogeneous infrastructures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u89e3\u8026\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5f02\u6784\u591a\u4e91\u73af\u5883\u4e2d\u81ea\u4e3b\u4e14\u8fd1\u4f18\u5730\u653e\u7f6eVNF\uff0c\u4ee5\u4f18\u5316\u7f51\u7edc\u5207\u7247\u90e8\u7f72\u6210\u672c\u548cQoS\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51b3\u7b56\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u8702\u7a9d\u7f51\u7edc\u5411\u8f6f\u4ef6\u5316\u548c\u4e91\u539f\u751f\u57fa\u7840\u8bbe\u65bd\u6f14\u8fdb\uff0c\u5728\u5f02\u6784\u591a\u4e91\u73af\u5883\u4e0b\u90e8\u7f72\u7f51\u7edc\u5207\u7247\u4e2d\u7684\u865a\u62df\u7f51\u7edc\u529f\u80fd\uff08VNF\uff09\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8003\u8651\u591a\u53d8\u7684\u8d44\u6e90\u80fd\u529b\u548c\u5207\u7247\u7279\u5b9a\u9700\u6c42\uff0c\u5bfb\u627e\u81ea\u4e3b\u4e14\u8fd1\u4f18\u7684VNF\u653e\u7f6e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u91c7\u7528\u89e3\u8026\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u8fdb\u884cVNF\u653e\u7f6e\u3002\u8be5\u6846\u67b6\u5229\u7528\u771f\u5b9e\u6d41\u91cf\u914d\u7f6e\u6587\u4ef6\u4f30\u7b97\u5207\u7247\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eMARL\u7684\u8c03\u5ea6\u5668\uff0c\u5728\u6ee1\u8db3QoS\u7ea6\u675f\u7684\u540c\u65f6\u6700\u5c0f\u5316\u90e8\u7f72\u6210\u672c\u3002", "result": "\u5728\u591a\u4e91\u6d4b\u8bd5\u5e73\u53f0\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u7ec4\u5408\u4f18\u5316\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u534719\u500d\uff0c\u90e8\u7f72\u6210\u672c\u5728\u6700\u4f18\u89e3\u76847.8%\u4ee5\u5185\u3002\u5c3d\u7ba1\u5728\u9ad8\u8d1f\u8f7d\u4e0bQoS\u8fdd\u89c4\u589e\u52a0\u9ad8\u8fbe2.42\u500d\uff0c\u4f46\u5176\u63d0\u4f9b\u4e86\u663e\u8457\u66f4\u5feb\u7684\u51b3\u7b56\u901f\u5ea6\u548c\u964d\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u65b9\u6cd5\u4e3a\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5b9e\u65f6\u7f51\u7edc\u5207\u7247\u653e\u7f6e\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18123", "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff0c\u7279\u522b\u662fChatGPT-4.1\uff09\u5bf9\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u96f6\u6837\u672c\u704c\u6e89\u6a21\u5f0f\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5206\u6790\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\uff08\u57fa\u4e8e\u9608\u503c\u6216\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u5b66\u4e60/\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u51c6\u786e\u89e3\u91ca\u571f\u58e4\u6e7f\u5ea6\u6a21\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86SPADE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528ChatGPT-4.1\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\u5e76\u8bbe\u8ba1\u9886\u57df\u77e5\u60c5\u7684\u63d0\u793a\u6a21\u677f\uff0c\u8fdb\u884c\u96f6\u6837\u672c\u5206\u6790\u3002SPADE\u80fd\u591f\u8bc6\u522b\u704c\u6e89\u4e8b\u4ef6\u3001\u4f30\u7b97\u51c0\u704c\u6e89\u589e\u76ca\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u5f02\u5e38\uff0c\u5e76\u751f\u6210\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u62a5\u544a\u3002", "result": "SPADE\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0c\u5e76\u80fd\u51c6\u786e\u5206\u7c7b\u5f02\u5e38\u7c7b\u578b\u3002\u5728\u704c\u6e89\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\uff0cSPADE\u4e5f\u53d6\u5f97\u4e86\u9ad8\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u3002\u6b64\u5916\uff0cSPADE\u751f\u6210\u7684\u62a5\u544a\u63d0\u4f9b\u4e86\u571f\u58e4\u6e7f\u5ea6\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86LLMs\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u7684\u5de5\u5177\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u6f5c\u529b\uff0c\u5b83\u4eec\u80fd\u591f\u6574\u5408\u5b9a\u6027\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u63a8\u7406\uff0c\u4e3a\u51c6\u786e\u7684\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u548c\u6539\u8fdb\u704c\u6e89\u8c03\u5ea6\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18160", "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "PerceptronCARE\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdc\u7a0b\u773c\u79d1\u5e94\u7528\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff0c\u65e8\u5728\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u53ef\u53ca\u6027\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u6210\u4eba\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u8d44\u6e90\u532e\u4e4f\u5730\u533a\uff0c\u56e0\u6b64\u9700\u8981\u65e9\u671f\u8bca\u65ad\u548c\u6269\u5927\u7b5b\u67e5\u8986\u76d6\u3002", "method": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86PerceptronCARE\u7cfb\u7edf\uff0c\u4f7f\u7528ResNet-18\u3001EfficientNet-B0\u548cSqueezeNet\u7b49\u591a\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u7cfb\u7edf\u8fd8\u96c6\u6210\u4e86\u4e91\u7aef\u53ef\u4f38\u7f29\u6027\u3001\u5b89\u5168\u6570\u636e\u7ba1\u7406\u548c\u591a\u7528\u6237\u6846\u67b6\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5728\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e8685.4%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u5b9e\u73b0\u4e34\u5e8a\u548c\u8fdc\u7a0b\u533b\u7597\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u7b5b\u67e5\u3002", "conclusion": "PerceptronCARE\u5c55\u793a\u4e86AI\u9a71\u52a8\u7684\u8fdc\u7a0b\u533b\u7597\u89e3\u51b3\u65b9\u6848\u5728\u6269\u5927\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7b5b\u67e5\u53ef\u53ca\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u504f\u8fdc\u548c\u8d44\u6e90\u53d7\u9650\u5730\u533a\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u8bca\u65ad\u3001\u6539\u5584\u533b\u60a3\u4e92\u52a8\u5e76\u964d\u4f4e\u533b\u7597\u6210\u672c\u3002"}}
{"id": "2509.18104", "pdf": "https://arxiv.org/pdf/2509.18104", "abs": "https://arxiv.org/abs/2509.18104", "authors": ["Wenqian Li", "Youjia Yang", "Ruoxi Jia", "Yan Pang"], "title": "Data Valuation and Selection in a Federated Model Marketplace", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the era of Artificial Intelligence (AI), marketplaces have become\nessential platforms for facilitating the exchange of data products to foster\ndata sharing. Model transactions provide economic solutions in data\nmarketplaces that enhance data reusability and ensure the traceability of data\nownership. To establish trustworthy data marketplaces, Federated Learning (FL)\nhas emerged as a promising paradigm to enable collaborative learning across\nsiloed datasets while safeguarding data privacy. However, effective data\nvaluation and selection from heterogeneous sources in the FL setup remain key\nchallenges. This paper introduces a comprehensive framework centered on a\nWasserstein-based estimator tailored for FL. The estimator not only predicts\nmodel performance across unseen data combinations but also reveals the\ncompatibility between data heterogeneity and FL aggregation algorithms. To\nensure privacy, we propose a distributed method to approximate Wasserstein\ndistance without requiring access to raw data. Furthermore, we demonstrate that\nmodel performance can be reliably extrapolated under the neural scaling law,\nenabling effective data selection without full-scale training. Extensive\nexperiments across diverse scenarios, such as label skew, mislabeled, and\nunlabeled sources, show that our approach consistently identifies\nhigh-performing data combinations, paving the way for more reliable FL-based\nmodel marketplaces.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5f02\u6784\u6570\u636e\u4f30\u503c\u4e0e\u9009\u62e9\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u4f30\u7b97\u6846\u67b6\uff0c\u80fd\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u9ad8\u6548\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u5e76\u7b5b\u9009\u51fa\u9ad8\u8d28\u91cf\u6570\u636e\u7ec4\u5408\u3002", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u534f\u4f5c\u5b66\u4e60\uff0c\u4f46\u5728FL\u8bbe\u7f6e\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u548c\u9009\u62e9\u6765\u81ea\u5f02\u6784\u6e90\u7684\u6570\u636e\uff0c\u4ecd\u662f\u5efa\u7acb\u53ef\u4fe1\u6570\u636e\u5e02\u573a\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u4ee5Wasserstein\u8ddd\u79bb\u4f30\u7b97\u5668\u4e3a\u6838\u5fc3\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u4f30\u7b97\u5668\u80fd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u5e76\u63ed\u793a\u6570\u636e\u5f02\u6784\u6027\u4e0eFL\u805a\u5408\u7b97\u6cd5\u7684\u517c\u5bb9\u6027\u3002\u4e3a\u4fdd\u62a4\u9690\u79c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u7684\u5206\u5e03\u5f0fWasserstein\u8ddd\u79bb\u8fd1\u4f3c\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u5b9a\u5f8b\u63a8\u65ad\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u4e0d\u8fdb\u884c\u5168\u9762\u8bad\u7ec3\u7684\u9ad8\u6548\u6570\u636e\u9009\u62e9\u3002", "result": "\u901a\u8fc7\u5728\u6807\u7b7e\u504f\u659c\u3001\u9519\u8bef\u6807\u8bb0\u548c\u65e0\u6807\u7b7e\u6e90\u7b49\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u8bc6\u522b\u51fa\u9ad8\u6027\u80fd\u7684\u6570\u636e\u7ec4\u5408\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u4f30\u503c\u4e0e\u9009\u62e9\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5e02\u573a\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18167", "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "categories": ["cs.CL"], "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u7a0b\u76d1\u7763\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u51b3\u7b56\u8005\u548c\u77e5\u8bc6\u9009\u62e9\u5668\uff0c\u5e76\u7ed3\u5408LLM\u4f5c\u4e3a\u6cd5\u5b98\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u5956\u52b1\u76d1\u7763\u548cPPO\u8bad\u7ec3\uff0c\u4ee5\u4f18\u5316RAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u95ee\u7b54\u51c6\u786e\u6027\u3001\u6536\u655b\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u7684\u6548\u679c\u53d7\u9650\u4e8e\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u4e4b\u95f4\u7684\u534f\u540c\uff0c\u4f46\u7531\u4e8e\u4e24\u8005\u72ec\u7acb\u5f00\u53d1\uff0c\u4ea4\u4e92\u5f80\u5f80\u4e0d\u7406\u60f3\uff1a\u68c0\u7d22\u5668\u53ef\u80fd\u8fd4\u56de\u4e0d\u76f8\u5173\u6216\u5197\u4f59\u6587\u6863\uff0c\u800c\u751f\u6210\u5668\u53ef\u80fd\u672a\u80fd\u5145\u5206\u5229\u7528\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f25\u5408\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fc7\u7a0b\u76d1\u7763\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\uff1a\u4e00\u4e2a\u201c\u51b3\u7b56\u8005\u201d\u51b3\u5b9a\u4f55\u65f6\u7ee7\u7eed\u68c0\u7d22\u6216\u505c\u6b62\u751f\u6210\u7b54\u6848\uff1b\u4e00\u4e2a\u201c\u77e5\u8bc6\u9009\u62e9\u5668\u201d\u8fc7\u6ee4\u68c0\u7d22\u5230\u7684\u6587\u6863\u4ee5\u4fdd\u7559\u6700\u6709\u7528\u8bc1\u636e\u3002\u4e3a\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u91c7\u7528\u201cLLM\u4f5c\u4e3a\u6cd5\u5b98\u201d\u6765\u8bc4\u4f30\u6bcf\u4e2a\u4e2d\u95f4\u52a8\u4f5c\u5e76\u63d0\u4f9b\u8fc7\u7a0b\u7ea7\u5956\u52b1\u3002\u6b64\u5916\uff0c\u91c7\u7528\u6811\u5f62\u5c55\u5f00\u7b56\u7565\u63a2\u7d22\u591a\u6837\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7aef\u5230\u7aef\u8bad\u7ec3\u4e24\u4e2a\u667a\u80fd\u4f53\u3002\u8be5\u6846\u67b6\u5177\u6709\u6a21\u5757\u5316\u548c\u5373\u63d2\u5373\u7528\u7684\u7279\u70b9\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u68c0\u7d22\u5668\u6216\u751f\u6210\u5668\u3002", "result": "\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6RAG\u57fa\u7ebf\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u5730\u5f25\u5408\u4e86RAG\u4e2d\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u56e0\u5176\u6a21\u5757\u5316\u548c\u5373\u63d2\u5373\u7528\u7279\u6027\uff0c\u5728\u5b9e\u9645RAG\u5e94\u7528\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18654", "pdf": "https://arxiv.org/pdf/2509.18654", "abs": "https://arxiv.org/abs/2509.18654", "authors": ["Mohamed A. Abd-Elmagid", "Ming Shi", "Eylem Ekici", "Ness B. Shroff"], "title": "Online Learning for Optimizing AoI-Energy Tradeoff under Unknown Channel Statistics", "categories": ["cs.NI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We consider a real-time monitoring system where a source node (with energy\nlimitations) aims to keep the information status at a destination node as fresh\nas possible by scheduling status update transmissions over a set of channels.\nThe freshness of information at the destination node is measured in terms of\nthe Age of Information (AoI) metric. In this setting, a natural tradeoff exists\nbetween the transmission cost (or equivalently, energy consumption) of the\nsource and the achievable AoI performance at the destination. This tradeoff has\nbeen optimized in the existing literature under the assumption of having a\ncomplete knowledge of the channel statistics. In this work, we develop online\nlearning-based algorithms with finite-time guarantees that optimize this\ntradeoff in the practical scenario where the channel statistics are unknown to\nthe scheduler. In particular, when the channel statistics are known, the\noptimal scheduling policy is first proven to have a threshold-based structure\nwith respect to the value of AoI (i.e., it is optimal to drop updates when the\nAoI value is below some threshold). This key insight was then utilized to\ndevelop the proposed learning algorithms that surprisingly achieve an\norder-optimal regret (i.e., $O(1)$) with respect to the time horizon length.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\u4e0e\u4f20\u8f93\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u53d6\u5f97\u4e86O(1)\u7684\u9636\u6700\u4f18\u540e\u6094\u503c\u3002", "motivation": "\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u80fd\u91cf\u53d7\u9650\u7684\u6e90\u8282\u70b9\u9700\u5728\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\u4e0e\u4f20\u8f93\u80fd\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u73b0\u6709\u6587\u732e\u5728\u6b64\u4f18\u5316\u95ee\u9898\u4e0a\u4f9d\u8d56\u4e8e\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u7684\u5b8c\u6574\u77e5\u8bc6\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u901a\u5e38\u662f\u672a\u77e5\u7684\uff0c\u8fd9\u6784\u6210\u4e86\u4e00\u4e2a\u5b9e\u9645\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u8bc1\u660e\u4e86\u5728\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u5df2\u77e5\u65f6\uff0c\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u5177\u6709\u57fa\u4e8eAoI\u9608\u503c\u7684\u7ed3\u6784\u3002\u57fa\u4e8e\u8fd9\u4e00\u5173\u952e\u6d1e\u5bdf\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u5177\u6709\u6709\u9650\u65f6\u95f4\u4fdd\u8bc1\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u5728\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u4f18\u5316AoI\u4e0e\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5f53\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u5df2\u77e5\u65f6\uff0c\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u88ab\u8bc1\u660e\u662f\u57fa\u4e8eAoI\u9608\u503c\u7684\u3002\u6240\u63d0\u51fa\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u65f6\u95f4\u8303\u56f4\u957f\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u9636\u6700\u4f18\u7684\u540e\u6094\u503c\uff08O(1)\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u672a\u77e5\u573a\u666f\u4e0b\uff0c\u5b9e\u65f6\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u4fe1\u606f\u65b0\u9c9c\u5ea6\u4e0e\u4f20\u8f93\u80fd\u8017\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\uff08\u9636\u6700\u4f18\uff09\u6027\u80fd\u3002"}}
{"id": "2509.18132", "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "categories": ["cs.AI"], "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u533b\u7597AI\u65e0\u6cd5\u6709\u6548\u91cf\u5316\u548c\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08XUE\uff09\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u63d0\u5347\u533b\u7597AI\u7684\u4fe1\u4efb\u5ea6\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u533b\u7597\u5b9e\u8df5\u4e2d\u4e0d\u786e\u5b9a\u6027\u662f\u6838\u5fc3\u6311\u6218\uff0c\u4f46\u73b0\u6709\u533b\u7597AI\u672a\u80fd\u660e\u786e\u91cf\u5316\u6216\u4ee5\u7b26\u5408\u4e34\u5e8a\u63a8\u7406\u7684\u65b9\u5f0f\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u7684XAI\u5de5\u4f5c\u4fa7\u91cd\u4e8e\u89e3\u91ca\u9884\u6d4b\u800c\u975e\u7f6e\u4fe1\u5ea6\uff0c\u800cUE\u6280\u672f\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u4f46\u7f3a\u4e4f\u76f4\u89c2\u89e3\u91ca\uff0c\u4e8c\u8005\u8131\u8282\u9650\u5236\u4e86AI\u5728\u533b\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faXUE\uff0c\u6574\u5408\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u7cfb\u7edf\u5730\u5c06\u533b\u5b66\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u5230AI\u4e0d\u786e\u5b9a\u6027\u6982\u5ff5\uff0c\u8bc6\u522bXUE\u5b9e\u65bd\u7684\u5173\u952e\u6311\u6218\u3002\u6982\u8ff0\u4e86\u63a8\u8fdbXUE\u7684\u6280\u672f\u65b9\u5411\uff0c\u5305\u62ec\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u6a21\u578b\u65e0\u5173\u53ef\u89c6\u5316\u6280\u672f\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0XUE\u7684\u6307\u5bfc\u539f\u5219\u3002", "result": "\u5206\u6790\u5f3a\u8c03\u4e86AI\u7cfb\u7edf\u9700\u751f\u6210\u53ef\u9760\u9884\u6d4b\u5e76\u4ee5\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u65b9\u5f0f\u8868\u8fbe\u7f6e\u4fe1\u6c34\u5e73\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u8fde\u63a5\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5f00\u53d1\u503c\u5f97\u4fe1\u8d56\u7684\u533b\u7597AI\u505a\u51fa\u4e86\u8d21\u732e\u3002", "conclusion": "\u901a\u8fc7\u5f25\u5408\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u672c\u7814\u7a76\u4e3a\u5f00\u53d1\u7b26\u5408\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u590d\u6742\u6027\u3001\u503c\u5f97\u4fe1\u8d56\u7684\u533b\u7597AI\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18165", "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "categories": ["cs.CV", "cs.LG"], "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf Identity Mapping (SIM)\u7684\u65b0\u578b\u6570\u636e\u5185\u5728\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u6620\u5c04\u673a\u5236\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4f20\u7edf\u6b63\u5219\u5316\u6280\u672f\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u591a\u6837\u5316\u8bbe\u7f6e\u4e2d\u53ef\u9760\u6027\u6216\u6709\u6548\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u3001\u6570\u636e\u5185\u5728\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u5e76\u7f13\u89e3\u8fc7\u62df\u5408\u3002", "method": "\u5f15\u5165Self Identity Mapping (SIM)\uff0c\u5229\u7528\u9006\u6620\u5c04\u673a\u5236\u4ece\u53d8\u6362\u540e\u7684\u8f93\u51fa\u91cd\u5efa\u8f93\u5165\uff0c\u4ee5\u51cf\u5c11\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u5e76\u4fc3\u8fdb\u66f4\u5e73\u6ed1\u7684\u68af\u5ea6\u6d41\u3002\u4e3a\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0cSIM\u88ab\u5b9e\u4f8b\u5316\u4e3a\u03c1SIM\uff0c\u901a\u8fc7\u8865\u4e01\u7ea7\u7279\u5f81\u91c7\u6837\u548c\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u91cd\u5efa\u6f5c\u5728\u7279\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002\u5b83\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u3001\u4efb\u52a1\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002", "result": "\u03c1SIM\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5c11\u6837\u672c\u63d0\u793a\u5b66\u4e60\u548c\u9886\u57df\u6cdb\u5316\u7b49\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u03c1SIM\u80fd\u6709\u6548\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u4e14\u4e0e\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u6b63\u4ea4\uff0c\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6548\u679c\u3002\u6b64\u5916\uff0c\u03c1SIM\u5728\u8bed\u4e49\u5206\u5272\u3001\u56fe\u50cf\u7ffb\u8bd1\u7b49\u5bc6\u96c6\u5230\u5bc6\u96c6\u4efb\u52a1\u4ee5\u53ca\u97f3\u9891\u5206\u7c7b\u548c\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7b49\u975e\u89c6\u89c9\u9886\u57df\u4e2d\u4e5f\u80fd\u6709\u6548\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\u5e76\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SIM\uff08\u7279\u522b\u662f\u03c1SIM\uff09\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u3001\u6570\u636e\u5185\u5728\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u3001\u4efb\u52a1\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u5b83\u5728\u5404\u79cd\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u80fd\u591f\u8865\u5145\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.18105", "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u5e93\u5b58\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\uff0c\u5bf9\u4e8e\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u7684\u566a\u58f0\uff0c\u5305\u542b\u7269\u7406\u7ed3\u6784\u7684\u901a\u7528\u5fae\u5206\u65b9\u7a0b\uff08UDE\uff09\u8868\u73b0\u4f18\u4e8e\u7eaf\u795e\u7ecfODE\uff1b\u800c\u5bf9\u4e8e\u91cd\u5c3e\u6216\u6781\u7aef\u4e8b\u4ef6\uff0c\u7eaf\u795e\u7ecfODE\u7684\u7075\u6d3b\u6027\u66f4\u4f73\u3002", "motivation": "\u7ecf\u5178\u4f9b\u5e94\u94fe\u6a21\u578b\u89e3\u91ca\u4e86\u725b\u97ad\u6548\u5e94\uff0c\u4f46\u65b0\u5174\u7684\u7269\u7406\u4fe1\u606f\u548c\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u6df7\u5408\u65b9\u6cd5\u5c1a\u672a\u660e\u786e\u7ed3\u6784\u6027\u504f\u5dee\u5728\u4e0d\u540c\u9700\u6c42\u60c5\u51b5\u4e0b\u662f\u5e2e\u52a9\u8fd8\u662f\u963b\u788d\u9884\u6d4b\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u6df7\u5408\u5efa\u6a21\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5b8c\u5168\u5b66\u4e60\u7684\u795e\u7ecfODE (NODE) \u548c\u4fdd\u7559\u7ed3\u6784\u5e76\u5b66\u4e60\u6b8b\u5dee\u7b56\u7565\u9879\u7684\u7269\u7406\u4fe1\u606f\u901a\u7528\u5fae\u5206\u65b9\u7a0b (UDE)\uff0c\u5728\u5355\u4e00\u68af\u961f\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u4f7f\u7528AR(1)\u3001i.i.d.\u9ad8\u65af\u548c\u91cd\u5c3e\u5bf9\u6570\u6b63\u6001\u4e09\u79cd\u9700\u6c42\u6a21\u5f0f\u8fdb\u884c\u6d4b\u8bd5\u3002\u6a21\u578b\u5728\u4e0d\u540c\u6bd4\u4f8b\u7684\u8f68\u8ff9\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bc4\u4f30\u5e93\u5b58\u3001\u8ba2\u5355\u7387\u548c\u9700\u6c42\u7684\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u7ed3\u6784\u5316\u9700\u6c42\uff08AR(1)\u3001\u9ad8\u65af\uff09\u4e0b\uff0cUDE\u7684\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8eNODE\uff08\u4f8b\u5982\uff0cAR(1)\u4e0b\u5e93\u5b58RMSE\u4ece4.92\u964d\u81f30.26\uff09\u3002\u4f46\u5728\u91cd\u5c3e\u5bf9\u6570\u6b63\u6001\u51b2\u51fb\u4e0b\uff0cNODE\u7684\u7075\u6d3b\u6027\u8868\u73b0\u66f4\u597d\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\uff0c\u8fd9\u4e9b\u8d8b\u52bf\u4f9d\u7136\u5b58\u5728\uff0cNODE\u5728\u63a8\u65ad\u65f6\u8868\u73b0\u51fa\u76f8\u79fb\u6f02\u79fb\uff0c\u800cUDE\u867d\u7136\u7a33\u5b9a\u4f46\u5bf9\u7a00\u6709\u5c16\u5cf0\u53cd\u5e94\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u5efa\u8bae\u5728\u566a\u58f0\u4e3a\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u65f6\u5e94\u5f3a\u5236\u6267\u884c\u7ed3\u6784\uff1b\u5f53\u6781\u7aef\u4e8b\u4ef6\u4e3b\u5bfc\u65f6\u5219\u5e94\u653e\u5bbd\u7ed3\u6784\u3002\u6b64\u6307\u5bfc\u539f\u5219\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u79d1\u5b66\u548c\u5de5\u7a0b\u7cfb\u7edf\u7684\u6df7\u5408\u5efa\u6a21\uff0c\u5373\u5728\u5b88\u6052\u5f8b\u548c\u9002\u5ea6\u566a\u58f0\u4e3b\u5bfc\u65f6\u5e94\u7528\u5df2\u77e5\u7ed3\u6784\uff0c\u800c\u5728\u7a00\u6709\u4e8b\u4ef6\u9a71\u52a8\u52a8\u529b\u5b66\u65f6\u653e\u677e\u7ed3\u6784\u4ee5\u6355\u6349\u6781\u7aef\u60c5\u51b5\u3002"}}
{"id": "2509.18175", "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "categories": ["cs.CL"], "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u4e0e\u9884\u6d4b\u67b6\u6784\uff08ERFC\uff09\uff0c\u8003\u8651\u4e0a\u4e0b\u6587\u548c\u8bf4\u8bdd\u8005\u4ea4\u4e92\uff0c\u65e8\u5728\u9884\u6d4b\u672a\u6765\u5bf9\u8bdd\u60c5\u611f\uff0c\u5df2\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u5bf9\u547c\u53eb\u4e2d\u5fc3\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u5546\u4e1a\u4ef7\u503c\u3002", "motivation": "\u5728\u547c\u53eb\u4e2d\u5fc3\u7b49\u573a\u666f\u4e2d\uff0c\u4ee3\u7406\u4eba\u82e5\u80fd\u9884\u6d4b\u5ba2\u6237\u672a\u6765\u8bdd\u8bed\u4e2d\u7684\u60c5\u611f\uff0c\u5c06\u80fd\u66f4\u597d\u5730\u63d0\u4f9b\u670d\u52a1\u3001\u5b89\u629a\u5ba2\u6237\u3001\u63d0\u9ad8\u5ba2\u6237\u6ee1\u610f\u5ea6\uff0c\u4ece\u800c\u5c06\u4e0d\u6ee1\u610f\u7684\u5ba2\u6237\u8f6c\u53d8\u4e3a\u6ee1\u610f\u7684\u5ba2\u6237\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u4e0e\u9884\u6d4b (Emotion Recognition and Forecasting in Conversation, ERFC)\u201d\u67b6\u6784\u3002\u8be5\u67b6\u6784\u7efc\u5408\u8003\u8651\u591a\u6a21\u6001\u4fe1\u606f\u3001\u60c5\u611f\u7684\u4e0d\u540c\u5c5e\u6027\u3001\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4ee5\u53ca\u5bf9\u8bdd\u4e2d\u8bf4\u8bdd\u8005\u8bdd\u8bed\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u7684\u60c5\u611f\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684ERFC\u67b6\u6784\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8be5ERFC\u65b9\u6cd5\u80fd\u591f\u4e3a\u547c\u53eb\u4e2d\u5fc3\u7b49\u5ba2\u6237\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u63d0\u4f9b\u5de8\u5927\u7684\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2509.18933", "pdf": "https://arxiv.org/pdf/2509.18933", "abs": "https://arxiv.org/abs/2509.18933", "authors": ["Gabriele Formis", "Gianluca Cena", "Lukasz Wisniewski", "Stefano Scanzio"], "title": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "accepted version in IEEE Transactions on Industrial Informatics, 12\n  pages, 2025", "summary": "Wireless communications are characterized by their unpredictability, posing\nchallenges for maintaining consistent communication quality. This paper\npresents a comprehensive analysis of various prediction models, with a focus on\nachieving accurate and efficient Wi-Fi link quality forecasts using machine\nlearning techniques. Specifically, the paper evaluates the performance of\ndata-driven models based on the linear combination of exponential moving\naverages, which are designed for low-complexity implementations and are then\nsuitable for hardware platforms with limited processing resources. Accuracy of\nthe proposed approaches was assessed using experimental data from a real-world\nWi-Fi testbed, considering both channel-dependent and channel-independent\ntraining data. Remarkably, channel-independent models, which allow for\ngeneralized training by equipment manufacturers, demonstrated competitive\nperformance. Overall, this study provides insights into the practical\ndeployment of machine learning-based prediction models for enhancing Wi-Fi\ndependability in industrial environments.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u57fa\u4e8e\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\u6027\u7ec4\u5408\u7684\u4f4e\u590d\u6742\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4bWi-Fi\u94fe\u8def\u8d28\u91cf\uff0c\u53d1\u73b0\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4fe1\u9053\u65e0\u5173\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u5bf9\u5de5\u4e1a\u73af\u5883\u4e2d\u7684Wi-Fi\u53ef\u9760\u6027\u63d0\u5347\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u7ed9\u7ef4\u6301\u4e00\u81f4\u7684\u901a\u4fe1\u8d28\u91cf\u5e26\u6765\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u51c6\u786e\u9ad8\u6548\u7684Wi-Fi\u94fe\u8def\u8d28\u91cf\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u8bc4\u4f30\u57fa\u4e8e\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\u6027\u7ec4\u5408\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u4e13\u4e3a\u4f4e\u590d\u6742\u5ea6\u5b9e\u73b0\u800c\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u5e73\u53f0\u3002\u4f7f\u7528\u771f\u5b9eWi-Fi\u6d4b\u8bd5\u53f0\u7684\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u540c\u65f6\u8003\u8651\u4e86\u4fe1\u9053\u76f8\u5173\u548c\u4fe1\u9053\u65e0\u5173\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u4fe1\u9053\u65e0\u5173\u6a21\u578b\uff08\u5141\u8bb8\u8bbe\u5907\u5236\u9020\u5546\u8fdb\u884c\u901a\u7528\u8bad\u7ec3\uff09\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5b9e\u9645\u90e8\u7f72\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\u4ee5\u589e\u5f3aWi-Fi\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6df1\u523b\u89c1\u89e3\u3002"}}
{"id": "2509.18168", "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "categories": ["cs.AI"], "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18170", "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "categories": ["cs.CV"], "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "\u63d0\u51faMAGIA\uff0c\u4e00\u79cd\u57fa\u4e8e\u52a8\u91cf\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u81ea\u9002\u5e94\u4fee\u6b63\u6846\u67b6\uff0c\u5728\u5927\u6279\u91cf\u5355\u8f6e\u5e73\u5747\u68af\u5ea6\uff08SAG\uff09\u673a\u5236\u4e0b\uff0c\u65e0\u9700\u8f85\u52a9\u4fe1\u606f\uff0c\u9ad8\u6548\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6\u53cd\u6f14\u65b9\u6cd5\u5728\u6311\u6218\u6027\u7684\u5355\u8f6e\u5e73\u5747\u68af\u5ea6\uff08SAG\uff09\u673a\u5236\u4e0b\uff0c\u7531\u4e8e\u6bcf\u4e2a\u6837\u672c\u4fe1\u606f\u5728\u5355\u4e2a\u6279\u6b21\u5e73\u5747\u68af\u5ea6\u4e2d\u9ad8\u5ea6\u7ea0\u7f20\uff0c\u96be\u4ee5\u5728\u5927\u6279\u91cf\u573a\u666f\u4e0b\u8fdb\u884c\u9ad8\u4fdd\u771f\u5ea6\u7684\u6570\u636e\u91cd\u5efa\u3002", "method": "\u5f15\u5165MAGIA\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u63a8\u7406\u7684\u3001\u57fa\u4e8e\u52a8\u91cf\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u81ea\u9002\u5e94\u4fee\u6b63\u65b9\u6cd5\u3002\u5176\u76ee\u6807\u51fd\u6570\u6574\u5408\u4e86\u4e24\u9879\u6838\u5fc3\u521b\u65b0\uff1a1) \u4e00\u4e2a\u95ed\u5f0f\u7ec4\u5408\u91cd\u7f29\u653e\uff0c\u53ef\u5b9e\u73b0\u66f4\u7d27\u5bc6\u7684\u4f18\u5316\u8fb9\u754c\uff1b2) \u4e00\u4e2a\u57fa\u4e8e\u52a8\u91cf\u7684\u6574\u4f53\u6279\u6b21\u635f\u5931\u4e0e\u5b50\u96c6\u635f\u5931\u6df7\u5408\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a2\u6d4b\u968f\u673a\u6570\u636e\u5b50\u96c6\u6765\u611f\u77e5\u6f5c\u5728\u7684\u5355\u56fe\u50cf\u4fe1\u53f7\u3002", "result": "MAGIA\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5927\u6279\u91cf\u573a\u666f\uff08\u73b0\u6709\u65b9\u6cd5\u5931\u8d25\u7684\u573a\u666f\uff09\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u56fe\u50cf\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u4e0e\u6807\u51c6\u6c42\u89e3\u5668\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u8f85\u52a9\u4fe1\u606f\u3002", "conclusion": "MAGIA\u4e3a\u5927\u6279\u91cfSAG\u673a\u5236\u4e0b\u7684\u68af\u5ea6\u53cd\u6f14\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u65e0\u9700\u8f85\u52a9\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u7c7b\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18106", "pdf": "https://arxiv.org/pdf/2509.18106", "abs": "https://arxiv.org/abs/2509.18106", "authors": ["Elisa Tomassini", "Enrique Garc\u00eda-Mac\u00edas", "Filippo Ubertini"], "title": "Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks", "categories": ["cs.LG"], "comment": null, "summary": "The growing use of permanent monitoring systems has increased data\navailability, offering new opportunities for structural assessment but also\nposing scalability challenges, especially across large bridge networks.\nManaging multiple structures requires tracking and comparing long-term\nbehaviour efficiently. To address this, knowledge transfer between similar\nstructures becomes essential. This study proposes a model-based transfer\nlearning approach using neural network surrogate models, enabling a model\ntrained on one bridge to be adapted to another with similar characteristics.\nThese models capture shared damage mechanisms, supporting a scalable and\ngeneralizable monitoring framework. The method was validated using real data\nfrom two bridges. The transferred model was integrated into a Bayesian\ninference framework for continuous damage assessment based on modal features\nfrom monitoring data. Results showed high sensitivity to damage location,\nseverity, and extent. This approach enhances real-time monitoring and enables\ncross-structure knowledge transfer, promoting smart monitoring strategies and\nimproved resilience at the network level.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4f20\u8f93\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u6865\u6881\u7ed3\u6784\u76d1\u6d4b\u6570\u636e\u7684\u8de8\u7ed3\u6784\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u6865\u6881\u7f51\u7edc\u7684\u635f\u4f24\u8bc4\u4f30\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u76d1\u6d4b\u7cfb\u7edf\u6570\u636e\u91cf\u7684\u589e\u957f\uff0c\u5927\u578b\u6865\u6881\u7f51\u7edc\u9762\u4e34\u7ed3\u6784\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u9700\u8981\u9ad8\u6548\u8ffd\u8e2a\u548c\u6bd4\u8f83\u957f\u671f\u884c\u4e3a\uff0c\u5e76\u5b9e\u73b0\u76f8\u4f3c\u7ed3\u6784\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ee5\u5e94\u5bf9\u591a\u7ed3\u6784\u7ba1\u7406\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u4f20\u8f93\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u3002\u5728\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u7684\u6865\u6881\u4e4b\u95f4\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9002\u5e94\uff0c\u6355\u6349\u5171\u4eab\u635f\u4f24\u673a\u5236\u3002\u5c06\u4f20\u8f93\u540e\u7684\u6a21\u578b\u6574\u5408\u5230\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u76d1\u6d4b\u6570\u636e\u7684\u6a21\u6001\u7279\u5f81\u8fdb\u884c\u8fde\u7eed\u635f\u4f24\u8bc4\u4f30\u3002\u65b9\u6cd5\u5df2\u901a\u8fc7\u4e24\u5ea7\u6865\u6881\u7684\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u635f\u4f24\u7684\u4f4d\u7f6e\u3001\u4e25\u91cd\u7a0b\u5ea6\u548c\u8303\u56f4\u5177\u6709\u9ad8\u5ea6\u654f\u611f\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5b9e\u65f6\u76d1\u6d4b\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u8de8\u7ed3\u6784\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4fc3\u8fdb\u4e86\u667a\u80fd\u76d1\u6d4b\u7b56\u7565\u548c\u7f51\u7edc\u5c42\u9762\u97e7\u6027\u7684\u63d0\u5347\u3002"}}
{"id": "2509.18293", "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u516b\u4e2a\u5f00\u6e90LLM\u68c0\u6d4b\u53cd\u72b9\u592a\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u5f15\u5165\u4e86\u65b0\u7684Guided-CoT\u63d0\u793a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86LLM\u7684\u8bef\u5dee\u3001\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u5dee\u5f02\u3002", "motivation": "\u68c0\u6d4b\u4ec7\u6068\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u53cd\u72b9\u592a\u5185\u5bb9\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u9700\u8981\u6301\u7eed\u8bad\u7ec3\u4ee5\u9002\u5e94\u793e\u4ea4\u5a92\u4f53\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u8bc4\u4f30\u4e86\u516b\u4e2a\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u68c0\u6d4b\u53cd\u72b9\u592a\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u5c06\u60c5\u5883\u5b9a\u4e49\u4f5c\u4e3a\u7b56\u7565\u6307\u5bfc\uff1b\u63a2\u7d22\u4e86\u591a\u79cd\u63d0\u793a\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684CoT\u7c7b\u63d0\u793a\u2014\u2014Guided-CoT\uff1b\u540c\u65f6\uff0c\u68c0\u67e5\u4e86LLM\u7684\u9519\u8bef\uff0c\u5e76\u5f15\u5165\u4e86\u91cf\u5316\u6a21\u578b\u751f\u6210\u7406\u7531\u8bed\u4e49\u5dee\u5f02\u7684\u6307\u6807\u3002", "result": "Guided-CoT\u80fd\u5f88\u597d\u5730\u5904\u7406\u60c5\u5883\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u89e3\u7801\u914d\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u6216\u63a8\u7406\u80fd\u529b\u5982\u4f55\uff1bLlama 3.1 70B\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5fae\u8c03\u7684GPT-3.5\uff1bLLM\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u751f\u6210\u7406\u7531\u4e2d\u663e\u8457\u7684\u8bed\u4e49\u5dee\u5f02\u548c\u77db\u76fe\u884c\u4e3a\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u4e0d\u540cLLM\u5728\u5b9e\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2509.19034", "pdf": "https://arxiv.org/pdf/2509.19034", "abs": "https://arxiv.org/abs/2509.19034", "authors": ["Lai Yi Ohlsen", "Pavlos Sermpezis", "Melissa Newcomb"], "title": "Poster: The Internet Quality Barometer Framework", "categories": ["cs.NI", "cs.CY"], "comment": "ACM Internet Measurement Conference (IMC) 2025", "summary": "In this paper, we introduce the Internet Quality Barometer (IQB), a framework\naiming to redefine Internet quality beyond ``speed''. IQB (i) defines Internet\nquality in a user-centric way by considering popular use cases, (ii) maps\nnetwork requirements to use cases through a set of weights and quality\nthresholds, and (iii) leverages publicly available Internet performance\ndatasets, to calculate the IQB score, a composite metric that reflects the\nquality of Internet experience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e92\u8054\u7f51\u8d28\u91cf\u6674\u96e8\u8868\uff08IQB\uff09\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8003\u8651\u7528\u6237\u7528\u4f8b\u548c\u7f51\u7edc\u8981\u6c42\uff0c\u91cd\u65b0\u5b9a\u4e49\u8d85\u8d8a\u901f\u5ea6\u7684\u4e92\u8054\u7f51\u8d28\u91cf\uff0c\u5e76\u8ba1\u7b97\u4e00\u4e2a\u7efc\u5408\u5f97\u5206\u3002", "motivation": "\u73b0\u6709\u4e92\u8054\u7f51\u8d28\u91cf\u5b9a\u4e49\u4e3b\u8981\u5173\u6ce8\u201c\u901f\u5ea6\u201d\uff0c\u672a\u80fd\u5168\u9762\u53cd\u6620\u7528\u6237\u5b9e\u9645\u4f53\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u4e14\u7efc\u5408\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "IQB\u6846\u67b6(i)\u4ee5\u6d41\u884c\u7528\u4f8b\u4e3a\u57fa\u7840\uff0c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u5b9a\u4e49\u4e92\u8054\u7f51\u8d28\u91cf\uff1b(ii)\u901a\u8fc7\u6743\u91cd\u548c\u8d28\u91cf\u9608\u503c\u5c06\u7f51\u7edc\u8981\u6c42\u6620\u5c04\u5230\u7528\u4f8b\uff1b(iii)\u5229\u7528\u516c\u5f00\u7684\u4e92\u8054\u7f51\u6027\u80fd\u6570\u636e\u96c6\u8ba1\u7b97IQB\u5206\u6570\uff0c\u4f5c\u4e3a\u53cd\u6620\u4e92\u8054\u7f51\u4f53\u9a8c\u8d28\u91cf\u7684\u7efc\u5408\u6307\u6807\u3002", "result": "\u6210\u529f\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4e92\u8054\u7f51\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u2014\u2014IQB\uff0c\u80fd\u591f\u8ba1\u7b97\u51fa\u53cd\u6620\u4e92\u8054\u7f51\u4f53\u9a8c\u8d28\u91cf\u7684\u7efc\u5408IQB\u5206\u6570\u3002", "conclusion": "IQB\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4e92\u8054\u7f51\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u5355\u4e00\u7684\u901f\u5ea6\u6307\u6807\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u7528\u6237\u7684\u771f\u5b9e\u4f53\u9a8c\u3002"}}
{"id": "2509.18178", "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0OpenFOAM\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08CFD\uff09\u4eff\u771f\u6d41\u7a0b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86CFD\u7684\u5b66\u4e60\u548c\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08CFD\uff09\u662f\u5de5\u7a0b\u9886\u57df\u91cd\u8981\u7684\u4eff\u771f\u5de5\u5177\uff0c\u4f46\u5176\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u548c\u624b\u52a8\u8bbe\u7f6e\u590d\u6742\u662f\u4e3b\u8981\u969c\u788d\u3002", "method": "Foam-Agent\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4eff\u771f\u81ea\u52a8\u5316\uff0c\u5305\u62ec\u9ad8\u7ea7\u7f51\u683c\u5212\u5206\u3001HPC\u63d0\u4ea4\u811a\u672c\u81ea\u52a8\u751f\u6210\u548c\u540e\u5904\u7406\u53ef\u89c6\u5316\u3002\u5b83\u4f7f\u7528\u53ef\u7ec4\u5408\u670d\u52a1\u67b6\u6784\uff08Model Context Protocol\uff09\u66b4\u9732\u6838\u5fc3\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u591a\u7d22\u5f15RAG\u548c\u4f9d\u8d56\u611f\u77e5\u751f\u6210\u8fc7\u7a0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u914d\u7f6e\u3002", "result": "\u5728110\u4e2a\u4eff\u771f\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFoam-Agent\u4f7f\u7528Claude 3.5 Sonnet\u53d6\u5f97\u4e8688.2%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff08\u5982MetaOpenFOAM\u768455.5%\uff09\u3002", "conclusion": "Foam-Agent\u6781\u5927\u5730\u964d\u4f4e\u4e86CFD\u7684\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u4e13\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u666e\u53ca\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18174", "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86Baseer\uff0c\u4e00\u4e2a\u4e13\u4e3a\u963f\u62c9\u4f2f\u8bed\u6587\u6863OCR\u5fae\u8c03\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u5b9e\u73b0\u4e860.25\u7684\u8bcd\u9519\u8bef\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u57fa\u51c6Misraj-DocOCR\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bedOCR\u56e0\u5176\u8fde\u7b14\u4e66\u5199\u3001\u591a\u6837\u5b57\u4f53\u3001\u53d8\u97f3\u7b26\u53f7\u53ca\u4ece\u53f3\u5230\u5de6\u7684\u4e66\u5199\u65b9\u5411\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5176\u4ed6\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\u7684\u6027\u80fd\u4ecd\u7136\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86Baseer\uff0c\u4e00\u4e2a\u901a\u8fc7\u4ec5\u89e3\u7801\u5668\u5fae\u8c03\u7b56\u7565\u9002\u5e94\u9884\u8bad\u7ec3MLLM\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u6587\u6863OCR\u3002\u8be5\u6a21\u578b\u5229\u7528\u5927\u89c4\u6a21\u7684\u5408\u6210\u4e0e\u771f\u5b9e\u6587\u6863\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86Misraj-DocOCR\uff0c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u4e13\u5bb6\u9a8c\u8bc1\u7684\u963f\u62c9\u4f2f\u8bedOCR\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cBaseer\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e860.25\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u5e76\u5728\u963f\u62c9\u4f2f\u8bed\u6587\u6863OCR\u9886\u57df\u786e\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5bf9\u901a\u7528MLLMs\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u9002\u5e94\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u7684\u9ad8\u7cbe\u5ea6OCR\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2509.18107", "pdf": "https://arxiv.org/pdf/2509.18107", "abs": "https://arxiv.org/abs/2509.18107", "authors": ["Huanyao Zhang", "Jiaye Lin", "Wentao Zhang", "Haitao Yuan", "Guoliang Li"], "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Multivariate time series forecasting involves predicting future values based\non historical observations. However, existing approaches primarily rely on\npredefined single-scale patches or lack effective mechanisms for multi-scale\nfeature fusion. These limitations hinder them from fully capturing the complex\npatterns inherent in time series, leading to constrained performance and\ninsufficient generalizability. To address these challenges, we propose a novel\narchitecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers\n(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both\nGeneral Pre-trained Models (GPM) and Domain-specific Models (DSM) for\nmulti-scale feature extraction. To accommodate the heterogeneity of temporal\nfeatures, AdaMixT incorporates a gating network that dynamically allocates\nweights among different experts, enabling more accurate predictions through\nadaptive multi-scale fusion. Comprehensive experiments on eight widely used\nbenchmarks, including Weather, Traffic, Electricity, ILI, and four ETT\ndatasets, consistently demonstrate the effectiveness of AdaMixT in real-world\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaMixT\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u591a\u5c3a\u5ea6\u4e13\u5bb6Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u52a8\u6001\u6743\u91cd\u5206\u914d\u5b9e\u73b0\u81ea\u9002\u5e94\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u5355\u4e00\u5c3a\u5ea6\u8865\u4e01\u6216\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u590d\u6742\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAdaMixT\u67b6\u6784\uff0c\u5f15\u5165\u591a\u79cd\u8865\u4e01\u5e76\u7ed3\u5408\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b(GPM)\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b(DSM)\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002\u901a\u8fc7\u4e00\u4e2a\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u5206\u914d\u4e0d\u540c\u4e13\u5bb6\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u4ee5\u9002\u5e94\u65f6\u95f4\u7279\u5f81\u7684\u5f02\u8d28\u6027\u3002", "result": "\u5728\u5305\u62ecWeather\u3001Traffic\u3001Electricity\u3001ILI\u4ee5\u53ca\u56db\u79cdETT\u6570\u636e\u96c6\u5728\u5185\u7684\u516b\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cAdaMixT\u6301\u7eed\u4e14\u663e\u8457\u5730\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "AdaMixT\u901a\u8fc7\u5176\u65b0\u9896\u7684\u81ea\u9002\u5e94\u52a0\u6743\u591a\u5c3a\u5ea6\u4e13\u5bb6\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u6355\u83b7\u548c\u878d\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18314", "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18356", "pdf": "https://arxiv.org/pdf/2509.18356", "abs": "https://arxiv.org/abs/2509.18356", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Optimal Service Mode Assignment in a Simple Computation Offloading System: Extended Version", "categories": ["eess.SY", "cs.NI", "cs.SY"], "comment": null, "summary": "We consider a simple computation offloading model where jobs can either be\nfully processed in the cloud or be partially processed at a local server before\nbeing sent to the cloud to complete processing. Our goal is to design a policy\nfor assigning jobs to service modes, i.e., full offloading or partial\noffloading, based on the state of the system, in order to minimize delay in the\nsystem. We show that when the cloud server is idle, the optimal policy is to\nassign the next job in the system queue to the cloud for processing. However,\nwhen the cloud server is busy, we show that, under mild assumptions, the\noptimal policy is of a threshold type, that sends the next job in the system\nqueue to the local server if the queue exceeds a certain threshold. Finally, we\ndemonstrate this policy structure through simulations.", "AI": {"tldr": "\u7814\u7a76\u8ba1\u7b97\u5378\u8f7d\u6a21\u578b\u4e2d\u7684\u4f5c\u4e1a\u5206\u914d\u7b56\u7565\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u7cfb\u7edf\u5ef6\u8fdf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\u76f4\u63a5\u5378\u8f7d\u81f3\u4e91\u7aef\u6700\u4f18\uff1b\u5f53\u4e91\u670d\u52a1\u5668\u7e41\u5fd9\u65f6\uff0c\u6700\u4f18\u7b56\u7565\u662f\u57fa\u4e8e\u961f\u5217\u957f\u5ea6\u7684\u9608\u503c\u7b56\u7565\uff0c\u51b3\u5b9a\u662f\u5426\u5148\u672c\u5730\u5904\u7406\u3002", "motivation": "\u5728\u8ba1\u7b97\u5378\u8f7d\u6a21\u578b\u4e2d\uff0c\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u72b6\u6001\u7684\u4f5c\u4e1a\u5206\u914d\u7b56\u7565\uff08\u5b8c\u5168\u5378\u8f7d\u5230\u4e91\u7aef\u6216\u90e8\u5206\u672c\u5730\u5904\u7406\u540e\u518d\u4f20\u81f3\u4e91\u7aef\uff09\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u5ef6\u8fdf\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u4e86\u5728\u4e0d\u540c\u4e91\u670d\u52a1\u5668\u72b6\u6001\uff08\u7a7a\u95f2\u6216\u7e41\u5fd9\uff09\u4e0b\u7684\u6700\u4f18\u4f5c\u4e1a\u5206\u914d\u7b56\u7565\uff0c\u5e76\u5229\u7528\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b56\u7565\u7684\u7ed3\u6784\u3002", "result": "\u5f53\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\uff0c\u6700\u4f18\u7b56\u7565\u662f\u5c06\u4e0b\u4e00\u4e2a\u4f5c\u4e1a\u5206\u914d\u7ed9\u4e91\u7aef\u5904\u7406\u3002\u5f53\u4e91\u670d\u52a1\u5668\u7e41\u5fd9\u65f6\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u662f\u9608\u503c\u7c7b\u578b\uff1a\u5982\u679c\u7cfb\u7edf\u961f\u5217\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\uff0c\u5219\u5c06\u4e0b\u4e00\u4e2a\u4f5c\u4e1a\u53d1\u9001\u5230\u672c\u5730\u670d\u52a1\u5668\u8fdb\u884c\u9884\u5904\u7406\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u80fd\u591f\u6700\u5c0f\u5316\u8ba1\u7b97\u5378\u8f7d\u7cfb\u7edf\u5ef6\u8fdf\u7684\u4f5c\u4e1a\u5206\u914d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u6839\u636e\u4e91\u670d\u52a1\u5668\u7684\u5fd9\u95f2\u72b6\u6001\u548c\u7cfb\u7edf\u961f\u5217\u957f\u5ea6\u52a8\u6001\u8c03\u6574\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.18180", "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "categories": ["cs.AI"], "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fd0\u7b79\u5b66\uff08OR\uff09\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5c06\u5176\u96c6\u6210\u65b9\u6cd5\u5f52\u7eb3\u4e3a\u81ea\u52a8\u5efa\u6a21\u3001\u8f85\u52a9\u4f18\u5316\u548c\u76f4\u63a5\u6c42\u89e3\u4e09\u5927\u65b9\u5411\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u8fd0\u7b79\u5b66\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u52a8\u6001\u3001\u591a\u7ea6\u675f\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\u3002LLMs\u51ed\u501f\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u6784\u751f\u6210\u548c\u63a8\u7406\u63a7\u5236\u80fd\u529b\uff0c\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u6570\u5b66\u6a21\u578b/\u4ee3\u7801\u7684\u8f6c\u6362\u3001\u542f\u53d1\u5f0f\u751f\u6210\u548c\u76f4\u63a5\u4f18\u5316\u3002", "method": "\u672c\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u5730\u68b3\u7406\u4e86LLMs\u4e0e\u8fd0\u7b79\u5b66\u7ed3\u5408\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5c06\u76f8\u5173\u65b9\u6cd5\u5206\u4e3a\u81ea\u52a8\u5efa\u6a21\u3001\u8f85\u52a9\u4f18\u5316\u548c\u76f4\u63a5\u6c42\u89e3\u4e09\u4e2a\u4e3b\u8981\u65b9\u5411\u3002\u540c\u65f6\uff0c\u8fd8\u56de\u987e\u4e86\u8bc4\u4f30\u57fa\u51c6\u548c\u7279\u5b9a\u9886\u57df\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u96c6\u6210\u5b58\u5728\u7684\u5173\u952e\u5f00\u653e\u95ee\u9898\uff0c\u5305\u62ec\u8bed\u4e49\u5230\u7ed3\u6784\u6620\u5c04\u7684\u4e0d\u7a33\u5b9a\u6027\u3001\u7814\u7a76\u8fdb\u5c55\u7684\u788e\u7247\u5316\u3001\u6cdb\u5316\u80fd\u529b\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u8bc4\u4f30\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "conclusion": "\u4e3a\u63a8\u52a8LLMs\u5728\u8fd0\u7b79\u5b66\u9886\u57df\u7684\u53d1\u5c55\uff0c\u672c\u7efc\u8ff0\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.18176", "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08CNN-LSTM\uff09\uff0c\u901a\u8fc7\u5c06\u7a00\u758fInSAR\u6570\u636e\u8f6c\u6362\u4e3a\u5bc6\u96c6\u65f6\u7a7a\u5f20\u91cf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5730\u9762\u5f62\u53d8\u7684\u9ad8\u7cbe\u5ea6\u3001\u7a7a\u95f4\u8fde\u8d2f\u6027\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5730\u9762\u4f4d\u79fb\u76d1\u6d4b\u5bf9\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7a33\u5b9a\u548c\u5730\u8d28\u707e\u5bb3\u7f13\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ece\u7a00\u758f\u7684InSAR\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u9884\u6d4b\u672a\u6765\u5f62\u53d8\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7a00\u758fInSAR\u70b9\u6d4b\u91cf\u8f6c\u6362\u4e3a\u5bc6\u96c6\u65f6\u7a7a\u5f20\u91cf\u3002\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08CNN-LSTM\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u540c\u6b65\u5b66\u4e60\u7a7a\u95f4\u6a21\u5f0f\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u4f7f\u7528\u7231\u5c14\u5170\u4e1c\u90e8\u7684Sentinel-1\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0eLight Gradient Boosting Machine\u548cLASSO\u56de\u5f52\u7b49\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u6240\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u663e\u8457\u66f4\u51c6\u786e\u4e14\u7a7a\u95f4\u8fde\u8d2f\u7684\u9884\u6d4b\uff0c\u4e3a\u8be5\u4efb\u52a1\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\uff0c\u57fa\u7ebf\u6a21\u578b\u5e38\u9ed8\u8ba4\u7b80\u5355\u7684\u6301\u4e45\u6027\u6a21\u5f0f\uff0c\u51f8\u663e\u4e86\u672c\u6587\u96c6\u6210\u65f6\u7a7a\u65b9\u6cd5\u6355\u6349\u590d\u6742\u5730\u9762\u5f62\u53d8\u52a8\u6001\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u65f6\u7a7a\u6df1\u5ea6\u5b66\u4e60\u5728\u9ad8\u5206\u8fa8\u7387\u5f62\u53d8\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2509.18108", "pdf": "https://arxiv.org/pdf/2509.18108", "abs": "https://arxiv.org/abs/2509.18108", "authors": ["Adam Viktorin", "Tomas Kadavy", "Jozef Kovac", "Michal Pluhacek", "Roman Senkerik"], "title": "Solve it with EASE", "categories": ["cs.LG", "cs.AI", "I.2.2; I.2.7"], "comment": "EASE framework landing paper", "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an\nopen-source and fully modular framework for iterative algorithmic solution\ngeneration leveraging large language models (LLMs). EASE integrates generation,\ntesting, analysis, and evaluation into a reproducible feedback loop, giving\nusers full control over error handling, analysis, and quality assessment. Its\narchitecture supports the orchestration of multiple LLMs in complementary\nroles-such as generator, analyst, and evaluator. By abstracting the complexity\nof prompt design and model management, EASE provides a transparent and\nextensible platform for researchers and practitioners to co-design algorithms\nand other generative solutions across diverse domains.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fed\u4ee3\u751f\u6210\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u53cd\u9988\u5faa\u73af\u548c\u5168\u9762\u63a7\u5236\u3002", "motivation": "\u73b0\u6709LLMs\u751f\u6210\u89e3\u51b3\u65b9\u6848\u8fc7\u7a0b\u590d\u6742\uff0c\u7f3a\u4e4f\u53ef\u590d\u73b0\u6027\u548c\u7cbe\u7ec6\u63a7\u5236\uff1b\u9700\u8981\u4e00\u4e2a\u5e73\u53f0\u6765\u7b80\u5316\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u7ba1\u7406\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u80fd\u591f\u9ad8\u6548\u5171\u540c\u8bbe\u8ba1\u7b97\u6cd5\u53ca\u5176\u4ed6\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "EASE\u6846\u67b6\u5c06\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u5206\u6790\u548c\u8bc4\u4f30\u96c6\u6210\u5230\u53ef\u590d\u73b0\u7684\u53cd\u9988\u5faa\u73af\u4e2d\uff0c\u8d4b\u4e88\u7528\u6237\u5bf9\u9519\u8bef\u5904\u7406\u3001\u5206\u6790\u548c\u8d28\u91cf\u8bc4\u4f30\u7684\u5b8c\u5168\u63a7\u5236\u3002\u5176\u67b6\u6784\u652f\u6301\u7f16\u6392\u591a\u4e2aLLM\u62c5\u4efb\u751f\u6210\u5668\u3001\u5206\u6790\u5668\u3001\u8bc4\u4f30\u5668\u7b49\u4e92\u8865\u89d2\u8272\u3002", "result": "EASE\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u62bd\u8c61\u4e86\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u7ba1\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7b97\u6cd5\u53ca\u5176\u4ed6\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u7684\u534f\u540c\u8bbe\u8ba1\u3002", "conclusion": "EASE\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u5ea6\u6a21\u5757\u5316\u3001\u53ef\u63a7\u4e14\u591aLLM\u534f\u8c03\u7684\u5e73\u53f0\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7b97\u6cd5\u548c\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u7684\u8bbe\u8ba1\u95e8\u69db\uff0c\u8d4b\u80fd\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u8de8\u9886\u57df\u8fdb\u884c\u9ad8\u6548\u521b\u65b0\u3002"}}
{"id": "2509.18316", "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u5224\u65ad\u8bca\u65ad\u8def\u5f84\u7684\u6b63\u786e\u6027\u3002\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u5956\u52b1\u4f18\u5316\u80fd\u5e26\u6765\u5f3a\u5927\u7684\u8def\u5f84\u5224\u65ad\u6027\u80fd\uff0c\u4f46\u5411\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bca\u65ad\u63a8\u7406\u4e2d\u7f3a\u4e4f\u53ef\u9760\u548c\u77e5\u8bc6\u9a71\u52a8\u7684\u63a8\u7406\u80fd\u529b\u3002\u4f20\u7edf\u7684\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6216\u5fae\u8c03\uff09\u672a\u80fd\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5229\u7528KG\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u6765\u63d0\u5347LLM\u7684\u8bca\u65ad\u63a8\u7406\u7684\u53ef\u4fe1\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u66ff\u4ee3\u8303\u5f0f\uff1a\u5c06LLM\u89c6\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\u3002LLM\u5b66\u4e60\u5224\u65ad\u7ed9\u5b9a\u60a3\u8005\u8f93\u5165\u4e0b\uff0c\u5019\u9009\u63a8\u7406\u8def\u5f84\u662f\u5426\u80fd\u5bfc\u5411\u6b63\u786e\u7684\u8bca\u65ad\u3002\u8be5\u65b9\u6cd5\u53d7\u5956\u52b1\u8bad\u7ec3\u548c\u201c\u9a8c\u8bc1\u65b9\u6848\u6bd4\u4ece\u5934\u751f\u6210\u66f4\u5bb9\u6613\u201d\u7684\u8ba1\u7b97\u7406\u8bba\u542f\u53d1\uff0c\u5e76\u7c7b\u6bd4\u533b\u751f\u8bc4\u4f30\u8bca\u65ad\u8fc7\u7a0b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u7cfb\u7edf\u8bc4\u4f30\u4e94\u79cd\u77e5\u8bc6\u8def\u5f84\u5224\u65ad\u4efb\u52a1\u8868\u8ff0\u548c\u516b\u79cd\u8bad\u7ec3\u8303\u5f0f\u30022. \u6d4b\u8bd5\u8def\u5f84\u5224\u65ad\u80fd\u529b\u662f\u5426\u80fd\u6cdb\u5316\u5230\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\uff08\u5982\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\uff09\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e09\u4e2a\u5f00\u6e90\u7684\u6307\u4ee4\u5fae\u8c03LLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u5956\u52b1\u4f18\u5316\u548c\u84b8\u998f\uff0c\u6a21\u578b\u5728\u8def\u5f84\u5224\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u80fd\u529b\u5411\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\uff08\u53ef\u8fc1\u79fb\u6027\uff09\u80fd\u529b\u4ecd\u7136\u8f83\u5f31\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u201c\u5956\u52b1\u6a21\u578b\u98ce\u683c\u201d\u5728\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u76d1\u7763\u5982\u4f55\u5f71\u54cd\u533b\u7597\u9886\u57df\u751f\u6210\u5f0fAI\u7cfb\u7edf\u8bca\u65ad\u63a8\u7406\u7684\u89c1\u89e3\u3002\u5c3d\u7ba1\u8def\u5f84\u5224\u65ad\u80fd\u529b\u663e\u8457\uff0c\u4f46\u5176\u5728\u590d\u6742\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2509.18181", "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAPA\u6846\u67b6\uff0c\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5408\u6210\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u6001\u5ea6\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u7f51\u7ea6\u8f66\u51fa\u884c\u6a21\u5f0f\u9009\u62e9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u5206\u5c42\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6355\u6349\u5fc3\u7406\u56e0\u7d20\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u7f51\u7ea6\u8f66\u51fa\u884c\u6a21\u5f0f\u9009\u62e9\u5efa\u6a21\u5bf9\u4ea4\u901a\u7ba1\u7406\u653f\u7b56\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u65e0\u6cd5\u6355\u6349\u5173\u952e\u5fc3\u7406\u56e0\u7d20\uff0c\u4e14\u9762\u4e34\u7f51\u7ea6\u8f66\u51fa\u884c\u5360\u65e5\u5e38\u51fa\u884c\u6bd4\u4f8b\u5c0f\u7684\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165SAPA\uff08Synthesizing Attitudes, Predicting Actions\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5206\u5c42\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u4f7f\u7528LLM\u4ece\u539f\u59cb\u51fa\u884c\u8c03\u67e5\u6570\u636e\u751f\u6210\u5b9a\u6027\u65c5\u884c\u8005\u753b\u50cf\u3002\u5176\u6b21\uff0c\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u884c\u4e3a\u7279\u5f81\uff08\u5e76\u7ed3\u5408\u753b\u50cf\uff09\u8bad\u7ec3\u503e\u5411\u5f97\u5206\u6a21\u578b\uff0c\u751f\u6210\u4e2a\u4f53\u5f97\u5206\u3002\u518d\u6b21\uff0cLLM\u4e3a\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u53d8\u91cf\uff08\u5982\u65f6\u95f4/\u6210\u672c\u654f\u611f\u5ea6\uff09\u5206\u914d\u5b9a\u91cf\u5f97\u5206\u3002\u6700\u540e\uff0c\u4e00\u4e2a\u5206\u7c7b\u5668\u6574\u5408\u503e\u5411\u5f97\u5206\u3001\u6f5c\u5728\u53d8\u91cf\u5f97\u5206\uff08\u53ca\u5176\u4ea4\u4e92\u9879\uff09\u548c\u53ef\u89c2\u6d4b\u7684\u51fa\u884c\u5c5e\u6027\u6765\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u3002", "result": "\u5728\u5927\u89c4\u6a21\u3001\u591a\u5e74\u671f\u51fa\u884c\u8c03\u67e5\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAPA\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\uff0cPR-AUC\u6307\u6807\u4e0b\u7684\u7f51\u7ea6\u8f66\u9009\u62e9\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe75.9%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9884\u6d4b\u7f51\u7ea6\u8f66\u51fa\u884c\u6a21\u5f0f\u9009\u62e9\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6613\u4e8e\u63a8\u5e7f\u5230\u5176\u4ed6\u591a\u79cd\u5e94\u7528\u9886\u57df\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2509.18177", "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corr\u00eaa de Ara\u00fajo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "Scrapbook\u6846\u67b6\u751f\u6210\u6570\u636e\u96c6\u4ee5\u63a2\u7a76AI\u6a21\u578b\u5bf9\u57fa\u672c\u6982\u5ff5\u7684\u7406\u89e3\u3002\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u64c5\u957f\u7269\u4f53\u8bc6\u522b\u4f46\u96be\u4ee5\u7406\u89e3\u4f4d\u7f6e\u548c\u590d\u6742\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8bc4\u4f30AI\u6a21\u578b\u5bf9\u7269\u4f53\u8bc6\u522b\u3001\u4f4d\u7f6e\u7b49\u57fa\u672c\u6982\u5ff5\u7684\u7406\u89e3\u6df1\u5ea6\u3002\u9700\u8981\u4e00\u4e2a\u6846\u67b6\u6765\u751f\u6210\u5e7f\u6cdb\u6570\u636e\u96c6\uff0c\u4ee5\u9a8c\u8bc1\u6a21\u578b\u5728\u5904\u7406\u66f4\u590d\u6742\u4efb\u52a1\u524d\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u63d0\u51faScrapbook\u6846\u67b6\uff0c\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u751f\u6210\u63a2\u6d4bAI\u6a21\u578b\u5b66\u4e60\u6982\u5ff5\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8be5\u6846\u67b6\u805a\u7126\u7269\u4f53\u8bc6\u522b\u3001\u7edd\u5bf9\u548c\u76f8\u5bf9\u4f4d\u7f6e\u3001\u5c5e\u6027\u8bc6\u522b\u7b49\u57fa\u672c\u6982\u5ff5\uff0c\u901a\u8fc7\u751f\u6210\u5927\u91cf\u5173\u4e8e\u5355\u4e2a\u6982\u5ff5\u4e14\u8bed\u8a00\u53d8\u4f53\u5e7f\u6cdb\u7684\u95ee\u9898\u3002", "result": "\u73b0\u4ee3\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u679a\u4e3e\u65b9\u9762\u8868\u73b0\u719f\u7ec3\uff0c\u4f46\u96be\u4ee5\u7406\u89e3\u4f4d\u7f6e\u4fe1\u606f\u548c\u5904\u7406\u5e26\u989d\u5916\u7ea6\u675f\u7684\u67e5\u8be2\u3002MobileVLM-V2\u6a21\u578b\u663e\u793a\u7b54\u6848\u5206\u6b67\u5927\u4e14\u6613\u51fa\u9519\uff1b\u5176\u4ed6\u6a21\u578b\u5219\u503e\u5411\u80af\u5b9a\u7b54\u6848\uff0c\u5e76\u5728\u51e0\u4f55\u5f62\u72b6\u548c\u4f4d\u7f6e\u4fe1\u606f\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "Scrapbook\u6846\u67b6\u662f\u8bc4\u4f30\u548c\u589e\u5f3aAI\u6a21\u578b\u6027\u80fd\u7684\u5b9d\u8d35\u5de5\u5177\uff0c\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u57fa\u7840\u6982\u5ff5\uff08\u7279\u522b\u662f\u4f4d\u7f6e\u548c\u590d\u6742\u7ea6\u675f\uff09\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.18109", "pdf": "https://arxiv.org/pdf/2509.18109", "abs": "https://arxiv.org/abs/2509.18109", "authors": ["Jonatan Katz Nielsen"], "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks", "categories": ["cs.LG"], "comment": null, "summary": "Accurate recognition of vessel types from Automatic Identification System\n(AIS) tracks is essential for safety oversight and combating illegal,\nunreported, and unregulated (IUU) activity. This paper presents a strait-scale,\nmachine-learning pipeline that classifies moving vessels using only AIS data.\nWe analyze eight days of historical AIS from the Danish Maritime Authority\ncovering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After\nforward/backward filling voyage records, removing kinematic and geospatial\noutliers, and segmenting per-MMSI tracks while excluding stationary periods\n($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,\nSOG statistics), temporal, geospatial (Haversine distances, spans), and\nship-shape attributes computed from AIS A/B/C/D reference points (length,\nwidth, aspect ratio, bridge-position ratio). To avoid leakage, we perform\ngrouped train/test splits by MMSI and use stratified 5-fold cross-validation.\nAcross five classes (cargo, tanker, passenger, high-speed craft, fishing;\nN=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest\nwith SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall\n92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches\none-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the\nbridge-position ratio and maximum SOG as the most discriminative signals;\nprincipal errors occur between cargo and tanker, reflecting similar transit\nbehavior. We demonstrate operational value by backfilling missing ship types on\nunseen data and discuss improvements such as DBSCAN based trip segmentation and\ngradient-boosted ensembles to handle frequent-stop ferries and further lift\nperformance. The results show that lightweight features over AIS trajectories\nenable real-time vessel type classification in straits.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAIS\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u6d77\u5ce1\u5c3a\u5ea6\u8239\u8236\u7c7b\u578b\u5206\u7c7b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f68\u8ff9\u7279\u5f81\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5b9e\u73b0\u4e8692.15%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u8239\u8236\u7c7b\u578b\u5bf9\u4e8e\u4fdd\u969c\u822a\u8fd0\u5b89\u5168\u76d1\u7763\u548c\u6253\u51fb\u975e\u6cd5\u3001\u672a\u62a5\u544a\u548c\u65e0\u7ba1\u5236\uff08IUU\uff09\u6d3b\u52a8\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u4e39\u9ea6\u6d77\u4e8b\u5c402025\u5e741\u670822\u65e5\u81f330\u65e5\u8986\u76d6\u535a\u6069\u970d\u5c14\u59c6\u6d77\u5ce1\u76848\u5929AIS\u5386\u53f2\u6570\u636e\u3002\u6570\u636e\u7ecf\u8fc7\u822a\u884c\u8bb0\u5f55\u586b\u5145\u3001\u5f02\u5e38\u503c\u79fb\u9664\u3001\u6309MMSI\u822a\u8ff9\u5206\u5272\uff08\u6392\u9664\u505c\u6cca\u65f6\u6bb5\uff09\u7b49\u9884\u5904\u7406\u3002\u63d0\u53d6\u4e8631\u4e2a\u8f68\u8ff9\u7ea7\u7279\u5f81\uff0c\u6db5\u76d6\u8fd0\u52a8\u5b66\u3001\u65f6\u95f4\u3001\u5730\u7406\u7a7a\u95f4\u548c\u8239\u4f53\u5f62\u72b6\u5c5e\u6027\u3002\u91c7\u7528\u6309MMSI\u5206\u7ec4\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u548c\u5206\u5c425\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4f7f\u7528SMOTE\u7684\u968f\u673a\u68ee\u6797\u7b49\u6811\u5f62\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u8d27\u8239\u3001\u6cb9\u8f6e\u3001\u5ba2\u8239\u3001\u9ad8\u901f\u8239\u3001\u6e14\u8239\u4e94\u7c7b\u8239\u8236\u4e2d\uff0c\u7ed3\u5408SMOTE\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8692.15%\u7684\u51c6\u786e\u7387\uff08\u5b8f\u89c2F1\u503c\u4e3a93.27%\uff09\uff0c\u8c03\u4f18\u540e\u7684\u968f\u673a\u68ee\u6797\u6a21\u578bROC-AUC\u6700\u9ad8\u8fbe0.9897\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u8868\u660e\uff0c\u6865\u4f4d\u6bd4\u548c\u6700\u5927\u5bf9\u5730\u822a\u901f\u662f\u6700\u5177\u533a\u5206\u5ea6\u7684\u4fe1\u53f7\u3002\u4e3b\u8981\u7684\u5206\u7c7b\u9519\u8bef\u53d1\u751f\u5728\u8d27\u8239\u548c\u6cb9\u8f6e\u4e4b\u95f4\u3002\u8be5\u65b9\u6cd5\u8fd8\u5c55\u793a\u4e86\u56de\u586b\u7f3a\u5931\u8239\u8236\u7c7b\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eAIS\u8f68\u8ff9\u7684\u8f7b\u91cf\u7ea7\u7279\u5f81\u80fd\u591f\u5b9e\u73b0\u6d77\u5ce1\u5c3a\u5ea6\u7684\u5b9e\u65f6\u8239\u8236\u7c7b\u578b\u5206\u7c7b\uff0c\u4e3a\u672a\u6765\u7684\u6027\u80fd\u63d0\u5347\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18344", "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "SubSpec\u662f\u4e00\u79cd\u65e0\u635f\u3001\u514d\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u76ee\u6807LLM\u7684\u5378\u8f7d\u90e8\u5206\u751f\u6210\u91cf\u5316\u66ff\u4ee3\u5c42\u5e76\u5171\u4eabGPU\u5e38\u9a7b\u5c42\u548cKV\u7f13\u5b58\uff0c\u6784\u5efa\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u4ece\u800c\u663e\u8457\u52a0\u901f\u53c2\u6570\u5378\u8f7d\uff0c\u5b9e\u73b0LLM\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u6700\u9ad8\u53ef\u8fbe12.5\u500d\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5de8\u5927\u6a21\u578b\u5c3a\u5bf8\u9650\u5236\u4e86\u5176\u5728\u5185\u5b58\u6709\u9650\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u6709\u6a21\u578b\u538b\u7f29\u4f1a\u964d\u4f4e\u8d28\u91cf\uff0c\u53c2\u6570\u5378\u8f7d\u867d\u4fdd\u6301\u8d28\u91cf\u4f46\u63a8\u7406\u7f13\u6162\u3002\u63a8\u6d4b\u89e3\u7801\u53ef\u52a0\u901f\u5378\u8f7d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u9700\u989d\u5916\u8bad\u7ec3\u4ee5\u5bf9\u9f50\u5b9a\u5236\u6a21\u578b\uff0c\u6216\u56e0\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u5bf9\u9f50\u4e0d\u8db3\u800c\u52a0\u901f\u6709\u9650\u3002", "method": "\u63d0\u51faSubSpec\u65b9\u6cd5\uff0c\u5b83\u662f\u4e00\u79cd\u65e0\u635f\u3001\u514d\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u53c2\u6570\u5378\u8f7d\u52a0\u901f\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u76ee\u6807LLM\u7684\u5378\u8f7d\u90e8\u5206\u751f\u6210\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u66ff\u4ee3\u5c42\u6765\u6784\u5efa\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\u3002\u6b64\u5916\uff0cSubSpec\u8fd8\u5171\u4eabGPU\u4e0a\u5269\u4f59\u7684\u5e38\u9a7b\u5c42\u548cKV\u7f13\u5b58\uff0c\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u5e76\u589e\u5f3a\u5bf9\u9f50\u3002", "result": "SubSpec\u5b9e\u73b0\u4e86\u9ad8\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\uff0c\u57288GB\u663e\u5b58\u9650\u5236\u4e0b\uff0c\u4e3aQwen2.5 7B\u6a21\u578b\u5728MT-Bench\u4e0a\u5e26\u6765\u4e869.1\u500d\u7684\u52a0\u901f\u3002\u572824GB\u663e\u5b58\u9650\u5236\u4e0b\uff0c\u4e3aQwen2.5 32B\u6a21\u578b\u5728\u6d41\u884c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5e26\u6765\u4e8612.5\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SubSpec\u901a\u8fc7\u5176\u65e0\u635f\u3001\u514d\u8bad\u7ec3\u548c\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\u6784\u5efa\u65b9\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53c2\u6570\u5378\u8f7d\u5e26\u6765\u7684\u63a8\u7406\u901f\u5ea6\u74f6\u9888\uff0c\u4e3aLLMs\u5728\u5185\u5b58\u53d7\u9650\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.18186", "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogi\u0107", "Ivan Lukovi\u0107"], "title": "An Outcome-Based Educational Recommender System", "categories": ["cs.AI"], "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aOBER\u7684\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\uff0c\u5b83\u76f4\u63a5\u5d4c\u5165\u5b66\u4e60\u6210\u679c\u548c\u8bc4\u4f30\u9879\uff0c\u4ee5\u8bc4\u4f30\u5b66\u751f\u638c\u63e1\u5ea6\u3002\u901a\u8fc7\u4e00\u9879\u5305\u542b5700\u591a\u540d\u5b66\u4e60\u8005\u7684\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u53d1\u73b0\u534f\u540c\u8fc7\u6ee4\u6700\u5927\u5316\u4e86\u4fdd\u7559\u7387\uff0c\u800c\u56fa\u5b9a\u8def\u5f84\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5b66\u4e60\u638c\u63e1\u5ea6\u3002", "motivation": "\u5927\u591a\u6570\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4ec5\u57fa\u4e8e\u70b9\u51fb\u6216\u8bc4\u5206\u7b49\u76f8\u5173\u6027\u6307\u6807\u8fdb\u884c\u8c03\u6574\u548c\u5224\u65ad\uff0c\u5bfc\u81f4\u5176\u771f\u5b9e\u7684\u6559\u5b66\u5f71\u54cd\u4e0d\u660e\u786e\u3002", "method": "\u672c\u6587\u5f15\u5165OBER\uff08\u57fa\u4e8e\u6210\u679c\u7684\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\uff09\uff0c\u5b83\u5c06\u5b66\u4e60\u6210\u679c\u548c\u8bc4\u4f30\u9879\u76f4\u63a5\u5d4c\u5165\u6570\u636e\u6a21\u5f0f\u4e2d\uff0c\u4ee5\u4fbf\u4efb\u4f55\u7b97\u6cd5\u90fd\u80fd\u6839\u636e\u5176\u57f9\u517b\u7684\u638c\u63e1\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002OBER\u91c7\u7528\u6781\u7b80\u5b9e\u4f53\u5173\u7cfb\u6a21\u578b\u3001\u65e5\u5fd7\u9a71\u52a8\u7684\u638c\u63e1\u5ea6\u516c\u5f0f\u548c\u63d2\u4ef6\u5f0f\u67b6\u6784\u3002\u8be5\u7cfb\u7edf\u5728\u4e00\u4e2a\u975e\u6b63\u5f0f\u9886\u57df\u7684\u5728\u7ebf\u5b66\u4e60\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u4e3a\u671f\u4e24\u5468\u7684\u968f\u673a\u5206\u62c6\u6d4b\u8bd5\uff0c\u6d89\u53ca5700\u591a\u540d\u5b66\u4e60\u8005\uff0c\u6bd4\u8f83\u4e86\u56fa\u5b9a\u4e13\u5bb6\u8f68\u8ff9\u3001\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\u548c\u57fa\u4e8e\u77e5\u8bc6\uff08KB\uff09\u7684\u8fc7\u6ee4\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\u6700\u5927\u5316\u4e86\u5b66\u4e60\u8005\u7684\u4fdd\u7559\u7387\uff0c\u4f46\u56fa\u5b9a\u4e13\u5bb6\u8def\u5f84\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5b66\u4e60\u638c\u63e1\u5ea6\u3002", "conclusion": "\u7531\u4e8eOBER\u80fd\u4ece\u76f8\u540c\u7684\u65e5\u5fd7\u4e2d\u5f97\u51fa\u4e1a\u52a1\u3001\u76f8\u5173\u6027\u548c\u5b66\u4e60\u6307\u6807\uff0c\u5b83\u4f7f\u5b9e\u8df5\u8005\u65e0\u9700\u989d\u5916\u6d4b\u8bd5\u5f00\u9500\u5373\u53ef\u6743\u8861\u76f8\u5173\u6027\u3001\u53c2\u4e0e\u5ea6\u548c\u6210\u679c\u638c\u63e1\u5ea6\u3002\u8be5\u6846\u67b6\u4e0e\u5177\u4f53\u65b9\u6cd5\u65e0\u5173\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u672a\u6765\u7684\u81ea\u9002\u5e94\u6216\u60c5\u5883\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u3002"}}
{"id": "2509.18179", "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\uff0c\u6587\u672c\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u4f1a\u5bfc\u81f4\u56fe\u50cf\u4fe1\u606f\u4e25\u91cd\u4e22\u5931\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u8bc4\u4f30\u5176\u5c40\u9650\u6027\u9700\u8981\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u7ba1\u9053\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u3002\u7136\u800c\uff0c\u89c6\u89c9\u5185\u5bb9\u901a\u8fc7\u6587\u672c\u4e2d\u4ecb\u65f6\u53d1\u751f\u7684\u964d\u7ea7\u91cf\u5316\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u201c\u63cf\u8ff0-\u751f\u6210\u201d\u7ba1\u9053\u751f\u6210150\u5bf9\u56fe\u50cf\uff0c\u5e76\u5e94\u7528\u73b0\u6709\u6307\u6807\uff08LPIPS\u3001SSIM\u548c\u989c\u8272\u8ddd\u79bb\uff09\u6765\u8861\u91cf\u611f\u77e5\u3001\u7ed3\u6784\u548c\u8272\u5ea6\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u606f\u4fdd\u7559\u60c5\u51b5\uff0c\u8fdb\u884c\u7ecf\u9a8c\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c99.3%\u7684\u6837\u672c\u8868\u73b0\u51fa\u663e\u8457\u7684\u611f\u77e5\u964d\u7ea7\uff0c91.5%\u7684\u6837\u672c\u8868\u73b0\u51fa\u663e\u8457\u7684\u7ed3\u6784\u4fe1\u606f\u635f\u5931\u3002", "conclusion": "\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u201c\u63cf\u8ff0-\u751f\u6210\u201d\u74f6\u9888\u662f\u5f53\u4ee3\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u6d4b\u91cf\u4e14\u4e00\u81f4\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u4fe1\u606f\u4e22\u5931\u3002"}}
{"id": "2509.18110", "pdf": "https://arxiv.org/pdf/2509.18110", "abs": "https://arxiv.org/abs/2509.18110", "authors": ["Mrigank Dhingra", "Romit Maulik", "Adil Rasheed", "Omer San"], "title": "Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Neural operator learning has emerged as a powerful approach for solving\npartial differential equations (PDEs) in a data-driven manner. However,\napplying principal component analysis (PCA) to high-dimensional solution fields\nincurs significant computational overhead. To address this, we propose a\npatch-based PCA-Net framework that decomposes the solution fields into smaller\npatches, applies PCA within each patch, and trains a neural operator in the\nreduced PCA space. We investigate two different patch-based approaches that\nbalance computational efficiency and reconstruction accuracy: (1)\nlocal-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off\nbetween computational cost and accuracy is analyzed, highlighting the\nadvantages and limitations of each approach. Furthermore, within each approach,\nwe explore two refinements for the most computationally efficient method: (i)\nintroducing overlapping patches with a smoothing filter and (ii) employing a\ntwo-step process with a convolutional neural network (CNN) for refinement. Our\nresults demonstrate that patch-based PCA significantly reduces computational\ncomplexity while maintaining high accuracy, reducing end-to-end pipeline\nprocessing time by a factor of 3.7 to 4 times compared to global PCA, thefore\nmaking it a promising technique for efficient operator learning in PDE-based\nsystems.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u4e2d\u5168\u5c40PCA\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\u7684PCA-Net\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5728\u6570\u636e\u9a71\u52a8\u7684\u504f\u5fae\u5206\u65b9\u7a0b(PDEs)\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u4e2d\uff0c\u5c06\u4e3b\u6210\u5206\u5206\u6790(PCA)\u5e94\u7528\u4e8e\u9ad8\u7ef4\u89e3\u573a\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5206\u5757\u7684PCA-Net\u6846\u67b6\uff0c\u5c06\u89e3\u573a\u5206\u89e3\u4e3a\u5c0f\u5757\uff0c\u5728\u6bcf\u4e2a\u5757\u5185\u5e94\u7528PCA\uff0c\u5e76\u5728\u964d\u7ef4\u540e\u7684PCA\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\u3002\u7814\u7a76\u4e86\u4e24\u79cd\u5206\u5757\u65b9\u6cd5\uff08\u5c40\u90e8\u5230\u5168\u5c40\u3001\u5c40\u90e8\u5230\u5c40\u90e8\uff09\uff0c\u5e76\u4e3a\u6700\u9ad8\u6548\u7684\u65b9\u6cd5\u63a2\u7d22\u4e86\u4e24\u79cd\u7ec6\u5316\u65b9\u6848\uff08\u5f15\u5165\u5e26\u5e73\u6ed1\u6ee4\u6ce2\u5668\u7684\u91cd\u53e0\u5206\u5757\uff0c\u4ee5\u53ca\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u8fdb\u884c\u4e24\u6b65\u7ec6\u5316\uff09\u3002", "result": "\u57fa\u4e8e\u5206\u5757\u7684PCA\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e0e\u5168\u5c40PCA\u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u5904\u7406\u65f6\u95f4\u7f29\u77ed\u4e863.7\u52304\u500d\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5757\u7684PCA\u662f\u5b9e\u73b0\u57fa\u4e8ePDE\u7cfb\u7edf\u7684\u795e\u7ecf\u7b97\u5b50\u9ad8\u6548\u5b66\u4e60\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u6280\u672f\u3002"}}
{"id": "2509.18360", "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "Speech Vecalign\u662f\u4e00\u79cd\u65e0\u9700\u6587\u672c\u8f6c\u5f55\u7684\u5e76\u884c\u8bed\u97f3\u6587\u6863\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5bf9\u9f50\u8bed\u97f3\u6bb5\u5d4c\u5165\u6765\u751f\u6210\u66f4\u957f\u3001\u66f4\u9c81\u68d2\u7684\u8bed\u97f3\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u5728\u82f1\u5fb7\u8bed\u97f3\u5bf9\u9f50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4f7f\u7528\u66f4\u591a\u6570\u636e\u7684SpeechMatrix\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u5bf9\u9f50\u957f\u5ea6\u4e0d\u8db3\u3001\u9c81\u68d2\u6027\u5dee\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u6587\u672c\u8f6c\u5f55\u6570\u636e\u7b49\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6587\u672c\u8f6c\u5f55\u3001\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u66f4\u957f\u4e14\u66f4\u9c81\u68d2\u7684\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u5e76\u884c\u8bed\u97f3\u6570\u636e\uff0c\u4fc3\u8fdb\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u7b49\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86Speech Vecalign\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u8c03\u5bf9\u9f50\u8bed\u97f3\u6bb5\u5d4c\u5165\u6765\u5b9e\u73b0\u5e76\u884c\u8bed\u97f3\u6587\u6863\u5bf9\u9f50\uff0c\u4e14\u4e0d\u4f9d\u8d56\u6587\u672c\u8f6c\u5f55\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eVoxPopuli\u6570\u636e\u96c6\u4e2d3,000\u5c0f\u65f6\u7684\u65e0\u6807\u7b7e\u82f1\u5fb7\u5e76\u884c\u8bed\u97f3\u6587\u6863\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u82f1\u5fb7\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebfGlobal Mining\uff0cSpeech Vecalign\u751f\u6210\u4e86\u66f4\u957f\u7684\u8bed\u97f3\u5bf9\u9f50\uff1b\u76f8\u6bd4Local Mining\uff0c\u5b83\u5c55\u73b0\u4e86\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u5e76\u4ea7\u751f\u66f4\u5c11\u566a\u58f0\u3002\u57283,000\u5c0f\u65f6\u7684\u82f1\u5fb7\u8bed\u97f3\u6587\u6863\u4e0a\u751f\u6210\u4e86\u7ea61,000\u5c0f\u65f6\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u3002\u57fa\u4e8e\u6b64\u8bad\u7ec3\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\uff0c\u82f1\u8bd1\u5fb7\u6027\u80fd\u6bd4Global Mining\u63d0\u53470.37 ASR-BLEU\uff0c\u5fb7\u8bd1\u82f1\u63d0\u53470.18 ASR-BLEU\u3002\u5c3d\u7ba1\u4f7f\u7528\u4e86\u5c118\u500d\u7684\u539f\u59cb\u8bed\u97f3\u6587\u6863\uff0c\u5176\u6a21\u578b\u6027\u80fd\u4e0eSpeechMatrix\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "Speech Vecalign\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u65e0\u9700\u6587\u672c\u7684\u5e76\u884c\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u8bed\u97f3\u6570\u636e\u8fdb\u884c\u8de8\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.18198", "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMMCD\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u534f\u4f5c\u89c2\u6d4b\u548c\u5229\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u5728\u6570\u636e\u6a21\u6001\u7f3a\u5931\u65f6\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e8b\u6545\u591a\u53d1\u73af\u5883\u4e2d\uff0c\u9c81\u68d2\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u5355\u8f66\u611f\u77e5\u53d7\u9650\uff0c\u6613\u5bfc\u81f4\u4e8b\u6545\u3002\u73b0\u6709\u4e92\u8054\u591a\u6a21\u6001\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u6240\u6709\u6570\u636e\u6a21\u6001\u548c\u534f\u4f5c\u8f66\u8f86\u5747\u53ef\u7528\uff0c\u8fd9\u5728\u5b9e\u9645\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u53ef\u80fd\u56e0\u4f20\u611f\u5668\u6545\u969c\u6216\u8f66\u8f86\u7f3a\u5931\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4e86MMCD\uff08\u591a\u6a21\u6001\u534f\u4f5c\u51b3\u7b56\uff09\u6846\u67b6\uff0c\u878d\u5408\u81ea\u6211\u8f66\u8f86\u548c\u534f\u4f5c\u8f66\u8f86\u7684\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e\u3002\u4e3a\u786e\u4fdd\u5728\u6d4b\u8bd5\u65f6\u90e8\u5206\u6570\u636e\u6a21\u6001\u4e0d\u53ef\u7528\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5176\u4e2d\u6559\u5e08\u6a21\u578b\u4f7f\u7528\u591a\u79cd\u6570\u636e\u6a21\u6001\u8bad\u7ec3\uff0c\u5b66\u751f\u6a21\u578b\u8bbe\u8ba1\u4e3a\u5728\u51cf\u5c11\u6a21\u6001\u4e0b\u6709\u6548\u8fd0\u884c\u3002", "result": "\u5728\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u548c\u7a7a\u5730\u8f66\u8f86\u534f\u4f5c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u9a7e\u9a76\u5b89\u5168\u6027\u63d0\u9ad8\u4e8620.7%\uff0c\u5728\u68c0\u6d4b\u6f5c\u5728\u4e8b\u6545\u548c\u505a\u51fa\u5b89\u5168\u9a7e\u9a76\u51b3\u7b56\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u3002", "conclusion": "MMCD\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u5728\u6570\u636e\u6a21\u6001\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684\u51b3\u7b56\u9c81\u68d2\u6027\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b89\u5168\u9a7e\u9a76\u80fd\u529b\u3002"}}
{"id": "2509.18182", "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528AI\u548c\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\uff0c\u81ea\u52a8\u63a8\u65ad\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff08SIDS\uff09\u7684\u5c4b\u9876\u5c5e\u6027\uff0c\u4ee5\u5f25\u8865\u5176\u5728\u57ce\u5e02\u97e7\u6027\u89c4\u5212\u548c\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u65b9\u9762\u7684\u6570\u636e\u7a7a\u767d\u3002", "motivation": "\u6c14\u5019\u8106\u5f31\u7684\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff08SIDS\uff09\u901a\u5e38\u7f3a\u4e4f\u8be6\u7ec6\u7684\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u4f30\u8ba1\u707e\u5bb3\u6f5c\u5728\u635f\u5931\u3001\u57ce\u5e02\u97e7\u6027\u89c4\u5212\u548c\u707e\u5bb3\u98ce\u9669\u51cf\u7f13\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u81ea\u52a8\u63a8\u65ad\u5c4b\u9876\u5c5e\u6027\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u6d45\u5c42\u5206\u7c7b\u5668\u4e0e\u5fae\u8c03\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c4b\u9876\u5206\u7c7b\uff08\u5761\u5ea6\u548c\u6750\u6599\uff09\u4e0a\u7684\u6548\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u7eb3\u5165\u90bb\u8fd1SIDS\u989d\u5916\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u5c4b\u9876\u5761\u5ea6\u5206\u7c7b\u4e0a\u53d6\u5f97\u4e860.88\u7684F1\u5206\u6570\uff0c\u5728\u5c4b\u9876\u6750\u6599\u5206\u7c7b\u4e0a\u53d6\u5f97\u4e860.83\u7684F1\u5206\u6570\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e8\u5728\u7ed3\u5408\u5f53\u5730\u80fd\u529b\u5efa\u8bbe\uff0c\u4e3aSIDS\u63d0\u4f9b\u5229\u7528AI\u548c\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u65b0\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u57ce\u5e02\u6cbb\u7406\uff0c\u4ece\u800c\u63d0\u5347\u5e94\u5bf9\u707e\u5bb3\u4e8b\u4ef6\u7684\u97e7\u6027\u3002"}}
{"id": "2509.18111", "pdf": "https://arxiv.org/pdf/2509.18111", "abs": "https://arxiv.org/abs/2509.18111", "authors": ["Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The reliability of artificial intelligence (AI) systems in open-world\nsettings depends heavily on their ability to flag out-of-distribution (OOD)\ninputs unseen during training. Recent advances in large-scale vision-language\nmodels (VLMs) have enabled promising few-shot OOD detection frameworks using\nonly a handful of in-distribution (ID) samples. However, existing prompt\nlearning-based OOD methods rely solely on softmax probabilities, overlooking\nthe rich discriminative potential of the feature embeddings learned by VLMs\ntrained on millions of samples. To address this limitation, we propose a novel\ncontext optimization (CoOp)-based framework that integrates subspace\nrepresentation learning with prompt tuning. Our approach improves ID-OOD\nseparability by projecting the ID features into a subspace spanned by prompt\nvectors, while projecting ID-irrelevant features into an orthogonal null space.\nTo train such OOD detection framework, we design an easy-to-handle end-to-end\nlearning criterion that ensures strong OOD detection performance as well as\nhigh ID classification accuracy. Experiments on real-world datasets showcase\nthe effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u548c\u63d0\u793a\u8c03\u4f18\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528VLM\u7684\u7279\u5f81\u5d4c\u5165\u6765\u63d0\u9ad8ID-OOD\u53ef\u5206\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u4f9d\u8d56softmax\u6982\u7387\uff0c\u5ffd\u89c6\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b66\u4e60\u5230\u7684\u4e30\u5bcc\u7279\u5f81\u5d4c\u5165\u7684\u5224\u522b\u6f5c\u529b\uff0c\u5bfc\u81f4ID-OOD\u53ef\u5206\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4f18\u5316\uff08CoOp\uff09\u7684\u6846\u67b6\uff0c\u5c06\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u76f8\u7ed3\u5408\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06ID\u7279\u5f81\u6295\u5f71\u5230\u63d0\u793a\u5411\u91cf\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\u5c06ID\u4e0d\u76f8\u5173\u7279\u5f81\u6295\u5f71\u5230\u6b63\u4ea4\u7a7a\u7a7a\u95f4\uff0c\u4ee5\u63d0\u9ad8ID-OOD\u53ef\u5206\u6027\u3002\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6613\u4e8e\u5904\u7406\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u51c6\u5219\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8ID-OOD\u53ef\u5206\u6027\u548cID\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CoOp\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528VLM\u7684\u7279\u5f81\u5d4c\u5165\u548c\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u4e2dAI\u7cfb\u7edf\u7684OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.18377", "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u4e00\u4e2aLLM\u8f85\u52a9\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6821\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u53cd\u9988\u548c\u521b\u65b0\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u591a\u6570\u81ea\u52a8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u7f3a\u4e4f\u7528\u6237\u53cd\u9988\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u53d7\u9650\uff1b\u5f15\u5165\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\u6709\u671b\u5927\u5e45\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u8f85\u52a9\u7684\u5b9e\u65f6\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6821\u6b63\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6267\u884c\u6d41\u5f0fASR\u548c\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u5411\u7528\u6237\u63d0\u4f9b\u7b80\u6d01\u6458\u8981\uff0c\u5e76\u63a5\u53d7\u5373\u65f6\u53e3\u5934\u53cd\u9988\u3002\u6838\u5fc3\u6280\u672f\u5305\u62ec\uff1a1) \u5206\u88c2-\u5408\u5e76\uff08SWM\uff09\u6280\u672f\uff0c\u7528\u4e8e\u68c0\u6d4b\u5e76\u5206\u79bbASR\u9519\u8bef\u5f52\u56e0\u7684\u5355\u8bf4\u8bdd\u4eba\u591a\u8bf4\u8bdd\u4eba\u7247\u6bb5\uff1b2) \u57fa\u4e8e\u7528\u6237\u6821\u6b63\u8fdb\u884c\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\uff0c\u4ee5\u9884\u9632\u672a\u6765\u9519\u8bef\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0a\u7684LLM\u9a71\u52a8\u6a21\u62df\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5c06DER\uff08\u8bf4\u8bdd\u4eba\u9519\u8bef\u7387\uff09\u964d\u4f4e\u4e869.92%\uff0c\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u964d\u4f4e\u4e8644.23%\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\uff08\u5982\u6458\u8981/\u5b8c\u6574\u8f6c\u5f55\u663e\u793a\u3001\u5728\u7ebf\u6ce8\u518c\u6570\u91cf\u9650\u5236\u3001\u6821\u6b63\u9891\u7387\uff09\u7684\u6821\u6b63\u6548\u679c\u3002", "conclusion": "\u8be5LLM\u8f85\u52a9\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u7528\u6237\u53cd\u9988\u548c\u4e13\u7528\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u4e14\u5927\u5e45\u5ea6\u5730\u63d0\u5347\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u4eba\u673a\u534f\u4f5c\u7684\u573a\u666f\u3002"}}
{"id": "2509.18215", "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas \u010cyras", "Jos\u00e9 Ruiz Alarc\u00f3n"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "categories": ["cs.AI", "cs.LO", "cs.MA"], "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u6eaf\u8bba\u8bc1\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u7684\u539f\u56e0\u5230\u7279\u5b9a\u8bba\u8bc1\uff0c\u6765\u89e3\u91ca\u91cf\u5316\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08QBAF\uff09\u4e2d\u63a8\u7406\u53d8\u5316\u3002", "motivation": "\u5f53QBAF\u66f4\u65b0\u540e\uff0c\u9700\u8981\u7406\u89e3\u548c\u89e3\u91ca\u8bba\u8bc1\u5f3a\u5ea6\u504f\u5e8f\u5173\u7cfb\u7684\u53d8\u5316\uff08\u5373\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\uff09\u4e3a\u4f55\u4f1a\u53d1\u751f\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u7ed3\u8bba\u3002", "method": "1. \u63d0\u51fa\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u89e3\u91caQBAF\u4e2d\u7684\u63a8\u7406\u53d8\u5316\u30022. \u8ffd\u8e2aQBAF\u66f4\u65b0\u540e\uff0c\u8bed\u4e49\u5728\u7279\u5b9a\u201c\u4e3b\u9898\u8bba\u8bc1\u201d\u4e0a\u5efa\u7acb\u7684\u8bba\u8bc1\u5f3a\u5ea6\u504f\u5e8f\u5173\u7cfb\u7684\u53d8\u5316\uff08\u79f0\u4e3a\u201c\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u201d\uff09\u30023. \u5c06\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u7684\u539f\u56e0\u8ffd\u6eaf\u5230\u5177\u4f53\u7684\u8bba\u8bc1\u4f5c\u4e3a\u89e3\u91ca\u30024. \u8bc6\u522b\u5145\u5206\u3001\u5fc5\u8981\u548c\u53cd\u4e8b\u5b9e\u7684\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u89e3\u91ca\u30025. \u5b9a\u4e49\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u6765\u5bfb\u627e\u89e3\u91ca\uff0c\u5e76\u63d0\u4f9b\u76f8\u5e94\u5b9e\u73b0\u3002", "result": "1. \u8bc1\u660e\u4e86\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u89e3\u91ca\u5b58\u5728\u5f53\u4e14\u4ec5\u5f53QBAF\u66f4\u65b0\u5bfc\u81f4\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u30022. \u63d0\u4f9b\u4e86\u7528\u4e8e\u641c\u7d22\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u89e3\u91ca\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u5957\u5f62\u5f0f\u5316\u3001\u53ef\u5b9e\u73b0\u4e14\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u91caQBAF\u4e2d\u56e0\u66f4\u65b0\u5f15\u8d77\u7684\u63a8\u7406\u53d8\u5316\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u6790\u8bba\u8bc1\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u7684\u539f\u56e0\u3002"}}
{"id": "2509.18183", "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "\u63d0\u51faVLA-LPAF\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u591a\u89c6\u89d2\u89c2\u6d4b\uff0c\u63d0\u5347VLA\u6a21\u578b\u5bf9\u89c6\u89c9\u89c6\u89d2\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "VLA\u6a21\u578b\u4f9d\u8d56\u6807\u51c6\u6f14\u793a\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u5176\u89c6\u89c9\u89c2\u6d4b\uff08\u6765\u81ea\u4e0d\u540c\u76f8\u673a\u89c6\u89d2\u548c\u6570\u91cf\uff09\u5b58\u5728\u89c6\u89d2\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u89c6\u89c9\u7279\u5f81\u5dee\u5f02\u5927\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faVLA-LPAF\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u4ec5\u4f7f\u75282D\u6570\u636e\uff0c\u901a\u8fc7\u5355\u89c6\u89d2\u56fe\u50cf\u5fae\u8c03\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u5176\u4ed6\u591a\u89c6\u89d2\u89c2\u6d4b\uff0c\u6709\u6548\u5f25\u5408\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u901a\u8fc7RoboFlamingo-LPAF\u5b9e\u4f8b\u5316\u3002", "result": "RoboFlamingo-LPAF\u5728CALVIN\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u5347\u7ea68%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0cLIBERO\u4e0a\u63d0\u534715%\uff0c\u5b9a\u5236\u6a21\u62df\u57fa\u51c6\u4e0a\u63d0\u534730%\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u89c6\u89d2\u9002\u5e94\u6027\u3002", "conclusion": "VLA-LPAF\u6a21\u5757\u6709\u6548\u589e\u5f3a\u4e86VLA\u6a21\u578b\uff08\u5982RoboFlamingo\uff09\u7684\u89c6\u89d2\u9002\u5e94\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.18112", "pdf": "https://arxiv.org/pdf/2509.18112", "abs": "https://arxiv.org/abs/2509.18112", "authors": ["Sheng Wong", "Ravi Shankar", "Beth Albert", "Gabriel Davis Jones"], "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis", "categories": ["cs.LG"], "comment": "Preparing for journal", "summary": "Foundation models (FMs) and large language models (LLMs) demonstrate\nremarkable capabilities across diverse domains through training on massive\ndatasets. These models have demonstrated exceptional performance in healthcare\napplications, yet their potential for electronic fetal monitoring\n(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating\nfetal well-being, remains largely underexplored. Antepartum CTG interpretation\npresents unique challenges due to the complex nature of fetal heart rate (FHR)\npatterns and uterine activity, requiring sophisticated analysis of long\ntime-series data. The assessment of CTG is heavily based on subjective clinical\ninterpretation, often leading to variability in diagnostic accuracy and\ndeviation from timely pregnancy care. This study presents the first\ncomprehensive comparison of state-of-the-art AI approaches for automated\nantepartum CTG analysis. We systematically compare time-series FMs and LLMs\nagainst established CTG-specific architectures. Our evaluation encompasses over\n500 CTG recordings of varying durations reflecting real-world clinical\nrecordings, providing robust performance benchmarks across different modelling\nparadigms. Our results demonstrate that fine-tuned LLMs achieve superior\nperformance compared to both foundation models and domain-specific approaches,\noffering a promising alternative pathway for clinical CTG interpretation. These\nfindings provide critical insights into the relative strengths of different AI\nmethodologies for fetal monitoring applications and establish a foundation for\nfuture clinical AI development in prenatal care.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4ea7\u524d\u80ce\u5fc3\u76d1\u62a4\uff08CTG\uff09\u81ea\u52a8\u5316\u5206\u6790\u4e2d\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u548cLLMs\u5728\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u80ce\u513f\u7535\u5b50\u76d1\u62a4\uff08EFM\uff09/\u80ce\u5fc3\u76d1\u62a4\uff08CTG\uff09\u5206\u6790\u8fd9\u4e00\u5173\u952e\u9886\u57df\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4ea7\u524dCTG\u89e3\u8bfb\u56e0\u5176\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u4e3b\u89c2\u6027\u4e34\u5e8a\u5224\u65ad\u800c\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5e38\u5bfc\u81f4\u8bca\u65ad\u51c6\u786e\u6027\u53d8\u5f02\u548c\u5ef6\u8bef\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u81ea\u52a8\u5316\u4ea7\u524dCTG\u5206\u6790\u7684\u5148\u8fdbAI\u65b9\u6cd5\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u3002\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u73b0\u6709\u7684CTG\u7279\u5b9a\u67b6\u6784\u3002\u8bc4\u4f30\u4f7f\u7528\u4e86\u8d85\u8fc7500\u4efd\u53cd\u6620\u771f\u5b9e\u4e34\u5e8a\u60c5\u51b5\u7684CTG\u8bb0\u5f55\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684LLMs\u5728CTG\u89e3\u8bfb\u65b9\u9762\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u80ce\u513f\u76d1\u62a4\u5e94\u7528\u4e2d\u4e0d\u540cAI\u65b9\u6cd5\u7684\u76f8\u5bf9\u4f18\u52bf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u4ea7\u524d\u62a4\u7406\u4e2d\u7684\u4e34\u5e8aAI\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18395", "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "categories": ["cs.CL"], "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NormGenesis\uff0c\u4e00\u4e2a\u591a\u6587\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u6807\u6ce8\u8de8\u82f1\u8bed\u3001\u4e2d\u6587\u548c\u97e9\u8bed\u7684\u793e\u4f1a\u5316\u5bf9\u8bdd\uff0c\u5f15\u5165\u4e86\u8fdd\u89c4-\u89e3\u51b3\uff08V2R\uff09\u5bf9\u8bdd\u7c7b\u578b\u548c\u57fa\u4e8e\u793a\u4f8b\u7684\u8fed\u4ee3\u4f18\u5316\u3002\u5b83\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,800\u4e2a\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5bf9\u8bdd\u81ea\u7136\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u7528\u80fd\u529b\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u751f\u6210\u4e0d\u4ec5\u8fde\u8d2f\u800c\u4e14\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u54cd\u5e94\u3002\u73b0\u6709\u7684\u7cfb\u7edf\u53ef\u80fd\u96be\u4ee5\u5728\u591a\u6587\u5316\u80cc\u666f\u4e0b\u5b9e\u73b0\u8bed\u7528\u4e00\u81f4\u6027\u548c\u793e\u4f1a\u53ef\u63a5\u53d7\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u89c4\u8303\u8fdd\u53cd\u7684\u52a8\u6001\u4ea4\u4e92\u65f6\u3002", "method": "1. \u63d0\u51fa\u4e86NormGenesis\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u548c\u6807\u6ce8\u8de8\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u97e9\u8bed\u793e\u4f1a\u5316\u5bf9\u8bdd\u7684\u591a\u6587\u5316\u6846\u67b6\u3002 2. \u5f15\u5165\u4e86\u201c\u8fdd\u89c4-\u89e3\u51b3\uff08V2R\uff09\u201d\u5bf9\u8bdd\u7c7b\u578b\uff0c\u4ee5\u5efa\u6a21\u793e\u4f1a\u89c4\u8303\u8fdd\u53cd\u540e\u7684\u8bc6\u522b\u548c\u4fee\u590d\u8fc7\u7a0b\u3002 3. \u5728\u5bf9\u8bdd\u5408\u6210\u65e9\u671f\u5b9e\u65bd\u4e86\u57fa\u4e8e\u793a\u4f8b\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u6b20\u4ee3\u8868\u8bed\u8a00\u7684\u8bed\u7528\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u7b26\u5408\u8bed\u8a00\u3001\u60c5\u611f\u548c\u793e\u4f1a\u6587\u5316\u671f\u671b\u3002", "result": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,800\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8f6e\u6b21\u7ea7\u522b\u6807\u6ce8\u4e86\u89c4\u8303\u9075\u5b88\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u60c5\u611f\u54cd\u5e94\u3002 2. \u4eba\u5de5\u548cLLM\u8bc4\u4f30\u8868\u660e\uff0cNormGenesis\u5728\u7cbe\u70bc\u8d28\u91cf\u3001\u5bf9\u8bdd\u81ea\u7136\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002 3. \u7ecf\u8fc7V2R\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4f26\u7406\u654f\u611f\u60c5\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bed\u7528\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6587\u5316\u9002\u5e94\u6027\u5bf9\u8bdd\u5efa\u6a21\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u89c4\u8303\u611f\u77e5\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2509.18216", "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u795e\u7ecfDNA\u201d\uff08nDNA\uff09\uff0c\u901a\u8fc7\u6f5c\u5c42\u51e0\u4f55\u5b66\u7684\u4e09\u4e2a\u7ef4\u5ea6\uff08\u8c31\u66f2\u7387\u3001\u70ed\u529b\u5b66\u957f\u5ea6\u3001\u4fe1\u5ff5\u77e2\u91cf\u573a\uff09\u6355\u6349AI\u6a21\u578b\u7684\u5185\u90e8\u8ba4\u77e5\u8eab\u4efd\uff08\u8bed\u4e49\u57fa\u56e0\u578b\uff09\uff0c\u65e8\u5728\u5f00\u542f\u201c\u795e\u7ecf\u57fa\u56e0\u7ec4\u5b66\u201d\u9886\u57df\uff0c\u7528\u4e8e\u8ffd\u8e2a\u6a21\u578b\u6f14\u5316\u3001\u8bca\u65ad\u98ce\u9669\u548c\u7ba1\u7406\u53d8\u66f4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u6d4b\u91cfAI\u6a21\u578b\u7684\u884c\u4e3a\u548c\u8f93\u51fa\uff0c\u672a\u80fd\u6df1\u5165\u7406\u89e3\u5176\u5185\u90e8\u8ba4\u77e5\u8eab\u4efd\u548c\u201c\u7075\u9b42\u201d\uff0c\u5373\u6a21\u578b\u6f5c\u5c42\u51e0\u4f55\u5b66\u4e2d\u7684\u5185\u5728\u7279\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6355\u6349\u6a21\u578b\u7684\u8bed\u4e49\u57fa\u56e0\u578b\uff0c\u8d85\u8d8a\u8868\u9762\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u795e\u7ecfDNA (nDNA) \u4f5c\u4e3a\u4e00\u79cd\u8bed\u4e49-\u57fa\u56e0\u578b\u8868\u793a\uff0c\u901a\u8fc7\u4fe1\u5ff5\u7684\u5185\u5728\u51e0\u4f55\u5b66\u6355\u6349\u6a21\u578b\u7684\u6f5c\u5728\u8eab\u4efd\u3002nDNA\u7531\u4e09\u4e2a\u5173\u952e\u7684\u6f5c\u5c42\u51e0\u4f55\u7ef4\u5ea6\u7efc\u5408\u800c\u6210\uff1a\u8c31\u66f2\u7387\uff08\u63ed\u793a\u6982\u5ff5\u6d41\u66f2\u7387\uff09\u3001\u70ed\u529b\u5b66\u957f\u5ea6\uff08\u91cf\u5316\u8bed\u4e49\u8f6c\u6362\u52aa\u529b\uff09\u548c\u4fe1\u5ff5\u77e2\u91cf\u573a\uff08\u63cf\u7ed8\u4fe1\u5ff5\u65b9\u5411\u7684\u8bed\u4e49\u626d\u8f6c\u573a\uff09\u3002", "result": "nDNA\u80fd\u50cf\u751f\u7269DNA\u4e00\u6837\u7f16\u7801\u6a21\u578b\u7684\u201c\u8840\u7edf\u3001\u7a81\u53d8\u548c\u8bed\u4e49\u9057\u4f20\u201d\uff0c\u5f62\u6210\u7a33\u5b9a\u3001\u65e0\u5750\u6807\u7684\u795e\u7ecfDNA\u6307\u7eb9\u3002\u5229\u7528\u6b64\u6307\u7eb9\uff0c\u53ef\u4ee5\u8ffd\u8e2a\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u7b49\u8fc7\u7a0b\u4e2d\u7684\u8c31\u7cfb\u3001\u6d4b\u91cf\u7ee7\u627f\u6027\u3001\u68c0\u6d4b\u7279\u6027\u6f02\u79fb\uff0c\u4ece\u800c\u6bd4\u8f83\u6a21\u578b\u3001\u8bca\u65ad\u98ce\u9669\u5e76\u6cbb\u7406\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u795e\u7ecfDNA\uff0c\u5c06AI\u6a21\u578b\u89c6\u4e3a\u53ef\u8ffd\u6eaf\u5185\u90e8\u8ba4\u77e5\u7684\u6570\u5b57\u8bed\u4e49\u6709\u673a\u4f53\uff0c\u5f00\u521b\u201c\u795e\u7ecf\u57fa\u56e0\u7ec4\u5b66\u201d\u65b0\u9886\u57df\uff0c\u4ee5\u6df1\u5165\u7814\u7a76\u4eba\u5de5\u8ba4\u77e5\u8fdb\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u6a21\u578b\u7684\u6bd4\u8f83\u3001\u98ce\u9669\u8bca\u65ad\u548c\u957f\u671f\u6cbb\u7406\u3002"}}
{"id": "2509.18184", "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "categories": ["cs.CV"], "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faURNet\uff0c\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7f51\u7edc\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u6a21\u5757\u548cKL\u6563\u5ea6\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5728DSEC\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7b49\u663e\u8457\u4f18\u52bf\uff0c\u4f18\u4e8e\u4f20\u7edf\u5e27\u57fa\u76f8\u673a\uff0c\u56e0\u6b64\u7814\u7a76\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5f15\u5165\u4e86URNet\uff0c\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7f51\u7edc\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u6a21\u5757\uff0c\u6709\u6548\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ec6\u8282\u548c\u957f\u8ddd\u79bb\u5168\u5c40\u4e0a\u4e0b\u6587\uff1b\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8eKullback-Leibler (KL) \u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u53ef\u9760\u6027\u3002", "result": "\u5728DSEC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eURNet\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u3002", "conclusion": "URNet\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18114", "pdf": "https://arxiv.org/pdf/2509.18114", "abs": "https://arxiv.org/abs/2509.18114", "authors": ["Javed I. Khan an Henry Uwabor Moye"], "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU", "categories": ["cs.LG"], "comment": "12 pages, Technical Report 2025-07-01, Internetworking and Media\n  Communications Research Laboratories, Department of Computer Science, Kent\n  State University", "summary": "Autoregressive inference in large transformer-based language models (LLMs)\npresents significant challenges for runtime efficiency, particularly during the\ndecode phase where load imbalance across GPU shards can cause throughput\ndegradation and latency spikes. A DPU-assisted framework leveraged by\nBlueField-3 Data Processing Units can enable real-time detection and mitigation\nof load imbalance in multi-node tensor-parallel inference. By offloading\nmonitoring tasks to the DPU and analyzing GPU telemetry and inter-node\ncommunication patterns, the resulting system can provide actionable feedback to\ninference controllers and schedulers. The goal of this study is three-fold i)\nidentify the reported skews/imbalances/pathological conditions that arise in\nmuti-GPU execution of a) LLM tensor computing (both during training and\ninference), b) identify their impact on computational performance, and c) make\na critical assessment if those can be tracked for potential mitigation from a\nDPU network.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u56de\u5f52\u63a8\u7406\u5728\u89e3\u7801\u9636\u6bb5\u7684GPU\u8d1f\u8f7d\u4e0d\u5747\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u4e2aDPU\u8f85\u52a9\u6846\u67b6\uff0c\u5229\u7528BlueField-3\u6570\u636e\u5904\u7406\u5355\u5143\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u591a\u8282\u70b9\u5f20\u91cf\u5e76\u884c\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\uff0c\u4ee5\u63d0\u5347\u8fd0\u884c\u65f6\u6548\u7387\u3002", "motivation": "LLMs\u7684\u81ea\u56de\u5f52\u63a8\u7406\u5728\u89e3\u7801\u9636\u6bb5\u5b58\u5728\u663e\u8457\u7684\u8fd0\u884c\u65f6\u6548\u7387\u6311\u6218\uff0c\u4e3b\u8981\u8868\u73b0\u4e3aGPU\u5206\u7247\u95f4\u7684\u8d1f\u8f7d\u4e0d\u5747\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u541e\u5410\u91cf\u4e0b\u964d\u548c\u5ef6\u8fdf\u6fc0\u589e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc6\u522bLLM\u5f20\u91cf\u8ba1\u7b97\u4e2d\uff08\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\uff09\u591aGPU\u6267\u884c\u51fa\u73b0\u7684\u8d1f\u8f7d\u4e0d\u5747\u548c\u75c5\u6001\u6761\u4ef6\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u8ba1\u7b97\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53caDPU\u7f51\u7edc\u662f\u5426\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u5e76\u6f5c\u5728\u5730\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7531BlueField-3\u6570\u636e\u5904\u7406\u5355\u5143\uff08DPU\uff09\u8f85\u52a9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76d1\u63a7\u4efb\u52a1\u5378\u8f7d\u5230DPU\uff0c\u5e76\u5206\u6790GPU\u9065\u6d4b\u6570\u636e\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5bf9\u591a\u8282\u70b9\u5f20\u91cf\u5e76\u884c\u63a8\u7406\u4e2d\u8d1f\u8f7d\u4e0d\u5747\u7684\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u5411\u63a8\u7406\u63a7\u5236\u5668\u548c\u8c03\u5ea6\u5668\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "result": "\u8be5\u62bd\u8c61\u4e3b\u8981\u63cf\u8ff0\u4e86\u7814\u7a76\u76ee\u6807\u548c\u6240\u63d0\u6846\u67b6\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u800c\u975e\u5df2\u5b8c\u6210\u7814\u7a76\u7684\u5177\u4f53\u7ed3\u679c\u3002\u6240\u63d0\u7684DPU\u8f85\u52a9\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u5378\u8f7d\u76d1\u63a7\u4efb\u52a1\u5e76\u5206\u6790GPU\u9065\u6d4b\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u6a21\u5f0f\uff0c\u5b9e\u65f6\u68c0\u6d4b\u5e76\u7f13\u89e3\u8d1f\u8f7d\u4e0d\u5747\uff0c\u5e76\u5411\u63a8\u7406\u63a7\u5236\u5668\u548c\u8c03\u5ea6\u5668\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7DPU\u7f51\u7edc\u5bf9\u591aGPU LLM\u6267\u884c\u4e2d\u51fa\u73b0\u7684\u8d1f\u8f7d\u4e0d\u5747\u8fdb\u884c\u8ddf\u8e2a\u548c\u6f5c\u5728\u7f13\u89e3\uff0c\u4ece\u800c\u89e3\u51b3LLM\u81ea\u56de\u5f52\u63a8\u7406\u7684\u6548\u7387\u6311\u6218\u3002\u6240\u63d0\u51fa\u7684DPU\u8f85\u52a9\u6846\u67b6\u6709\u671b\u63d0\u5347LLM\u7684\u8fd0\u884c\u65f6\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2509.18401", "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5bcc\u542b\u6587\u5316\u5143\u7d20\u7684\u6ce2\u65af\u8bed\u6587\u5b66\u6587\u672c\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u6784\u5efa\u6ce2\u65af\u8bed\u6570\u636e\u96c6\u3001\u6539\u7f16\u6258\u5170\u65af\u521b\u9020\u6027\u601d\u7ef4\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5224\uff0c\u63ed\u793a\u4e86\u5176\u5728\u6ce2\u65af\u8bed\u6587\u5b66\u521b\u4f5c\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u8bedLLM\u6587\u5b66\u521b\u4f5c\uff0c\u5bf9\u975e\u82f1\u8bed\u6587\u5b66\u4f20\u7edf\uff08\u7279\u522b\u662f\u6ce2\u65af\u8bed\uff09\u7684\u63a2\u7d22\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": ["\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b20\u4e2a\u4e3b\u9898\u7684\u7528\u6237\u751f\u6210\u6ce2\u65af\u8bed\u6587\u5b66\u6587\u672c\u6570\u636e\u96c6\u3002", "\u6539\u7f16\u6258\u5170\u65af\u521b\u9020\u6027\u601d\u7ef4\u6d4b\u8bd5\uff08TTCT\uff09\uff0c\u4ece\u72ec\u521b\u6027\u3001\u6d41\u7545\u6027\u3001\u7075\u6d3b\u6027\u3001\u7cbe\u7ec6\u5316\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u3002", "\u91c7\u7528LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5206\u8bc4\u5224\uff0c\u5e76\u901a\u8fc7\u7ec4\u5185\u76f8\u5173\u7cfb\u6570\uff08ICC\uff09\u9a8c\u8bc1\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u53ef\u9760\u6027\uff0c\u7ed3\u679c\u663e\u793a\u9ad8\u5ea6\u4e00\u81f4\u3002", "\u5206\u6790\u6a21\u578b\u7406\u89e3\u548c\u8fd0\u7528\u56db\u79cd\u6838\u5fc3\u6587\u5b66\u4fee\u8f9e\uff08\u660e\u55bb\u3001\u6697\u55bb\u3001\u5938\u5f20\u3001\u5bf9\u5076\uff09\u7684\u80fd\u529b\u3002"], "result": ["\u6210\u529f\u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5206\u8bc4\u5224\u7684\u53ef\u9760\u6027\uff0c\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "\u63ed\u793a\u4e86LLMs\u5728\u751f\u6210\u6ce2\u65af\u8bed\u6587\u5b66\u6587\u672c\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"], "conclusion": "LLMs\u5728\u6ce2\u65af\u8bed\u6587\u5b66\u521b\u4f5c\u80fd\u529b\u4e0a\u4ecd\u9700\u8fdb\u4e00\u6b65\u5b8c\u5584\u548c\u6539\u8fdb\u3002"}}
{"id": "2509.18218", "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u76f8\u4f3c\u6027\u573a\u7406\u8bba\uff0c\u4e00\u4e2a\u5f62\u5f0f\u5316\u5b9e\u4f53\u95f4\u76f8\u4f3c\u6027\u5173\u7cfb\u53ca\u5176\u6f14\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9a\u4e49\u4e86\u751f\u6210\u5f0f\u667a\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6f14\u5316\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u65e8\u5728\u4e3a\u7406\u89e3\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u8bed\u8a00\u3002", "motivation": "\u4f5c\u8005\u8ba4\u4e3a\uff0c\u6301\u7eed\u5b58\u5728\u548c\u53d8\u5316\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\u662f\u4efb\u4f55\u53ef\u7406\u89e3\u52a8\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u57fa\u7840\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u5b9e\u4f53\u4e4b\u95f4\u76f8\u4f3c\u6027\u503c\u53ca\u5176\u6f14\u5316\u6240\u9075\u5faa\u7684\u539f\u7406\u3002", "method": "\u5f15\u5165\u76f8\u4f3c\u6027\u573a\u7406\u8bba\uff0c\u8be5\u7406\u8bba\u5b9a\u4e49\u4e86\uff1a(1) \u5b9e\u4f53\u5b87\u5b99U\u4e0a\u7684\u76f8\u4f3c\u6027\u573aS\uff0c\u5141\u8bb8\u975e\u5bf9\u79f0\u6027\u548c\u975e\u4f20\u9012\u6027\uff1b(2) \u901a\u8fc7\u5e8f\u5217Z_p\u8868\u793a\u7684\u7cfb\u7edf\u6f14\u5316\uff1b(3) \u8bf1\u5bfc\u7ea4\u7ef4\u7684\u6982\u5ff5K\uff1b(4) \u751f\u6210\u65b0\u5b9e\u4f53\u7684\u751f\u6210\u7b97\u5b50G\u3002\u5728\u6b64\u6846\u67b6\u5185\uff0c\u5f62\u5f0f\u5316\u4e86\u751f\u6210\u5f0f\u667a\u80fd\u7684\u5b9a\u4e49\uff1a\u5982\u679c\u751f\u6210\u7b97\u5b50G\u80fd\u751f\u6210\u5c5e\u4e8e\u7ed9\u5b9a\u6982\u5ff5K\u7ea4\u7ef4\u7684\u65b0\u5b9e\u4f53\uff0c\u5219\u5176\u5bf9K\u662f\u667a\u80fd\u7684\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i) \u4e0d\u5bf9\u79f0\u6027\u4f1a\u963b\u6b62\u76f8\u4e92\u5305\u542b\uff1b(ii) \u7a33\u5b9a\u6027\u9700\u8981\u951a\u5b9a\u5750\u6807\u6216\u6700\u7ec8\u9650\u5236\u5728f\u7684\u6c34\u5e73\u96c6\u5185\u3002\u8fd9\u4e9b\u7ed3\u679c\u786e\u4fdd\u4e86\u76f8\u4f3c\u6027\u573a\u6f14\u5316\u7684\u53d7\u9650\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u76f8\u4f3c\u6027\u573a\u7406\u8bba\u4e3a\u8868\u5f81\u3001\u6bd4\u8f83\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u8bed\u8a00\u3002\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u63a2\u7d22\u793e\u4f1a\u8ba4\u77e5\u7684\u5b9e\u9a8c\u5de5\u5177\u3002"}}
{"id": "2509.18185", "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "C\u00e9cile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "categories": ["cs.CV", "cs.AI"], "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "Visionerves\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408AI\u6846\u67b6\uff0c\u5229\u7528\u591a\u68af\u5ea6DWI\u548c\u5f62\u6001\u5b66MRI\u5bf9\u5468\u56f4\u795e\u7ecf\u7cfb\u7edf\u8fdb\u884c\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u60a3\u8005\u7684\u795e\u7ecf\u6210\u50cf\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u624b\u52a8ROI\u7684\u81ea\u52a8\u5316\u5206\u6790\u3002", "motivation": "\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u5e38\u5bfc\u81f4\u6162\u6027\u76c6\u8154\u75bc\u75db\u5e76\u53ef\u80fd\u7d2f\u53ca\u795e\u7ecf\uff0c\u4f46\u76ee\u524d\u5bf9\u5916\u5468\u795e\u7ecf\u7684\u6210\u50cf\u4ecd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165Visionerves\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u68af\u5ea6DWI\u548c\u5f62\u6001\u5b66MRI\u6570\u636e\u3002\u8be5\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a(A)\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u5206\u5272\u89e3\u5256\u7ed3\u6784\uff1b(B)\u901a\u8fc7\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u8fdb\u884c\u795e\u7ecf\u675f\u8ffd\u8e2a\u548c\u8bc6\u522b\uff0c\u5176\u4e2d\u901a\u8fc7\u6a21\u7cca\u7a7a\u95f4\u5173\u7cfb\u7f16\u7801\u89e3\u5256\u77e5\u8bc6\uff0c\u65e0\u9700\u624b\u52a8\u9009\u62e9ROI\u3002", "result": "\u5c06Visionerves\u5e94\u7528\u4e8e10\u540d\u786e\u8bca\u6216\u7591\u4f3c\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u5973\u6027\u7684\u8170\u9ab6\u4e1b\uff0c\u4e0e\u6807\u51c6\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u6280\u672f\u76f8\u6bd4\uff0cDice\u5206\u6570\u63d0\u9ad8\u4e86\u9ad8\u8fbe25%\uff0c\u7a7a\u95f4\u8bef\u5dee\u51cf\u5c0f\u5230\u5c0f\u4e8e5\u6beb\u7c73\u3002", "conclusion": "\u8fd9\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u4f7f\u5f97\u8be6\u7ec6\u7684\u795e\u7ecf\u5206\u6790\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u975e\u4fb5\u5165\u6027\u8bca\u65ad\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u76f8\u5173\u795e\u7ecf\u75c5\u53d8\u4ee5\u53ca\u5176\u4ed6\u6d89\u53ca\u795e\u7ecf\u7684\u75be\u75c5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18115", "pdf": "https://arxiv.org/pdf/2509.18115", "abs": "https://arxiv.org/abs/2509.18115", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Towards Scalable and Structured Spatiotemporal Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we propose a novel Spatial Balance Attention block for\nspatiotemporal forecasting. To strike a balance between obeying spatial\nproximity and capturing global correlation, we partition the spatial graph into\na set of subgraphs and instantiate Intra-subgraph Attention to learn local\nspatial correlation within each subgraph; to capture the global spatial\ncorrelation, we further aggregate the nodes to produce subgraph representations\nand achieve message passing among the subgraphs via Inter-subgraph Attention.\nBuilding on the proposed Spatial Balance Attention block, we develop a\nmultiscale spatiotemporal forecasting model by progressively increasing the\nsubgraph scales. The resulting model is both scalable and able to produce\nstructured spatial correlation, and meanwhile, it is easy to implement. We\nevaluate its efficacy and efficiency against the existing models on real-world\nspatiotemporal datasets from medium to large sizes. The experimental results\nshow that it can achieve performance improvements up to 7.7% over the baseline\nmethods at low running costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u6a21\u5757\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u5b50\u56fe\u5185\u548c\u5b50\u56fe\u95f4\u6ce8\u610f\u529b\uff0c\u5e73\u8861\u5c40\u90e8\u4e0e\u5168\u5c40\u7a7a\u95f4\u5173\u8054\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u7a7a\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e73\u8861\u7a7a\u95f4\u90bb\u8fd1\u6027\uff08\u5c40\u90e8\u5173\u8054\uff09\u4e0e\u6355\u83b7\u5168\u5c40\u7a7a\u95f4\u5173\u8054\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u6a21\u5757\u3002\u8be5\u6a21\u5757\u5c06\u7a7a\u95f4\u56fe\u5212\u5206\u4e3a\u5b50\u56fe\uff0c\u901a\u8fc7\u5b50\u56fe\u5185\u6ce8\u610f\u529b\u5b66\u4e60\u5c40\u90e8\u7a7a\u95f4\u5173\u8054\uff1b\u901a\u8fc7\u805a\u5408\u8282\u70b9\u751f\u6210\u5b50\u56fe\u8868\u793a\uff0c\u5e76\u5229\u7528\u5b50\u56fe\u95f4\u6ce8\u610f\u529b\u5b9e\u73b0\u5b50\u56fe\u95f4\u7684\u6d88\u606f\u4f20\u9012\uff0c\u4ece\u800c\u6355\u83b7\u5168\u5c40\u7a7a\u95f4\u5173\u8054\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u5b50\u56fe\u89c4\u6a21\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u5c3a\u5ea6\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.7%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u8fd0\u884c\u6210\u672c\u8f83\u4f4e\u3002\u6a21\u578b\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u751f\u6210\u7ed3\u6784\u5316\u7684\u7a7a\u95f4\u5173\u8054\uff0c\u5e76\u4e14\u6613\u4e8e\u5b9e\u73b0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u6a21\u5757\u53ca\u5176\u6784\u5efa\u7684\u591a\u5c3a\u5ea6\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u6709\u6548\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u7a7a\u95f4\u5173\u8054\uff0c\u5728\u5404\u79cd\u89c4\u6a21\u7684\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.18439", "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "categories": ["cs.CL", "cs.AI"], "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u548c\u4f1a\u8bdd\u5bf9\u9f50\uff08CA\uff09\u8bc4\u5206\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6d4b\u91cf\u533b\u60a3\u5171\u4eab\u51b3\u7b56\uff08SDM\uff09\uff0c\u5e76\u53d1\u73b0CA\u8bc4\u5206\u4e0e\u65e2\u5b9aSDM\u7ed3\u679c\u91cf\u8868\u76f8\u5173\u3002", "motivation": "\u5171\u4eab\u51b3\u7b56\uff08SDM\uff09\u662f\u5b9e\u73b0\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u62a4\u7406\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u76ee\u524d\u7f3a\u4e4f\u53ef\u81ea\u52a8\u3001\u5927\u89c4\u6a21\u6d4b\u91cfSDM\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8f6c\u5f55\u4e86157\u4f8b\u533b\u60a3\u5bf9\u8bdd\uff08\u517142,559\u53e5\u8bdd\uff09\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587-\u54cd\u5e94\u5bf9\u548c\u8d1f\u91c7\u6837\u8bad\u7ec3\u4e86\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u548c\u5fae\u8c03\u7684BERT\u6a21\u578b\uff0c\u4ee5\u6267\u884c\u4e0b\u4e00\u53e5\u9884\u6d4b\uff08NSP\uff09\u4efb\u52a1\u3002\u5229\u7528\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u8ba1\u7b97\u56db\u79cd\u4f1a\u8bdd\u5bf9\u9f50\uff08CA\uff09\u8bc4\u5206\u3002\u901a\u8fc7\u968f\u673a\u6548\u5e94\u5206\u6790\uff0c\u8c03\u6574\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u548c\u8bd5\u9a8c\u7ec4\u7b49\u56e0\u7d20\u540e\uff0c\u8bc4\u4f30CA\u8bc4\u5206\u4e0eSDM\u7ed3\u679c\uff08\u51b3\u7b56\u51b2\u7a81\u91cf\u8868DCS\u548c\u51b3\u7b56\u4e2d\u60a3\u8005\u53c2\u4e0e\u89c2\u5bdf\u91cf\u8868OPTION12\uff09\u7684\u5173\u8054\u3002\u4f7f\u7528Benjamini-Hochberg\u65b9\u6cd5\u6821\u6b63\u591a\u91cd\u6bd4\u8f83\u7684p\u503c\u3002", "result": "\u5fae\u8c03\u540e\u7684BERTbase\u6a21\u578b\uff08110M\uff09\u8fbe\u5230\u4e86\u6700\u9ad8\u7684recall@1 (0.640)\u3002DL\u6a21\u578b\u751f\u6210\u7684AbsMax CA\u8bc4\u5206 (p=0.025) \u548c Max CA\u8bc4\u5206 (p=0.012) \u4e0eOPTION12\u76f8\u5173\u3002\u5fae\u8c03BERTbase\uff08110M\uff09\u6a21\u578b\u751f\u6210\u7684Max CA\u8bc4\u5206\u4e0eDCS\u8bc4\u5206\u76f8\u5173 (p=0.037)\u3002BERT\u6a21\u578b\u5927\u5c0f\u5bf9CA\u8bc4\u5206\u4e0eSDM\u7684\u5173\u8054\u6ca1\u6709\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u52a8\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4f1a\u8bdd\u5bf9\u9f50\uff08CA\uff09\u8bc4\u5206\u6765\u6d4b\u91cf\u533b\u60a3\u5bf9\u8bdd\u4e2d\u7684SDM\uff0c\u8fd9\u5bf9\u4e8e\u5927\u89c4\u6a21\u8bc4\u4f30SDM\u7b56\u7565\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.18221", "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.", "AI": {"tldr": "\u63d0\u51faVL-RiskFormer\uff0c\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u548cLLM\u7684\u591a\u6a21\u6001Transformer\uff0c\u7528\u4e8e\u4e3b\u52a8\u9884\u6d4b\u4e2a\u4eba\u5065\u5eb7\u98ce\u9669\u3002", "motivation": "\u9762\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6162\u6027\u75c5\u8d1f\u62c5\u548c\u591a\u6a21\u6001\u5f02\u6784\u4e34\u5e8a\u6570\u636e\uff08\u5982\u533b\u5b66\u5f71\u50cf\u3001\u81ea\u7531\u6587\u672c\u8bb0\u5f55\u3001\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u6d41\uff09\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001AI\u6846\u67b6\u6765\u4e3b\u52a8\u9884\u6d4b\u4e2a\u4eba\u5065\u5eb7\u98ce\u9669\u3002", "method": "\u63d0\u51faVL-RiskFormer\uff0c\u4e00\u4e2a\u5206\u5c42\u5806\u53e0\u7684\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001Transformer\uff0c\u5176\u9876\u5c42\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u5934\u3002\u8be5\u7cfb\u7edf\u57fa\u4e8e\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u6d41\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u4e09\u9879\u4e3b\u8981\u521b\u65b0\uff1a(i) \u4f7f\u7528\u52a8\u91cf\u66f4\u65b0\u7f16\u7801\u5668\u548c\u53bb\u504fInfoNCE\u635f\u5931\u5bf9\u653e\u5c04\u56fe\u50cf\u3001\u773c\u5e95\u56fe\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u7167\u7247\u4e0e\u4e34\u5e8a\u53d9\u8ff0\u8fdb\u884c\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff1b(ii) \u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u95f4\u9694\u4f4d\u7f6e\u7f16\u7801\u5c06\u4e0d\u89c4\u5219\u5c31\u8bca\u5e8f\u5217\u6574\u5408\u5230\u56e0\u679cTransformer\u89e3\u7801\u5668\u4e2d\u7684\u65f6\u95f4\u878d\u5408\u5757\uff1b(iii) \u75be\u75c5\u672c\u4f53\u56fe\u9002\u914d\u5668\uff0c\u5c06ICD-10\u7f16\u7801\u6ce8\u5165\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\uff0c\u5e76\u501f\u52a9\u56fe\u6ce8\u610f\u529b\u673a\u5236\u63a8\u65ad\u5171\u75c5\u6a21\u5f0f\u3002", "result": "\u5728MIMIC-IV\u7eb5\u5411\u961f\u5217\u4e0a\uff0cVL-RiskFormer\u5b9e\u73b0\u4e86\u5e73\u5747AUROC 0.90\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee\u4e3a2.7%\u3002", "conclusion": "VL-RiskFormer\u6210\u529f\u5730\u5229\u7528\u521b\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00Transformer\u67b6\u6784\u4e0eLLM\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u6570\u636e\u7684\u9ad8\u6548\u3001\u51c6\u786e\u548c\u6821\u51c6\u826f\u597d\u7684\u4e3b\u52a8\u5065\u5eb7\u98ce\u9669\u9884\u6d4b\u3002"}}
{"id": "2509.18187", "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86V-SenseDrive\uff0c\u9996\u4e2a\u5728\u5df4\u57fa\u65af\u5766\u9a7e\u9a76\u73af\u5883\u4e2d\u6536\u96c6\u7684\u9690\u79c1\u4fdd\u62a4\u591a\u6a21\u6001\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u3002", "motivation": "\u9053\u8def\u4ea4\u901a\u4e8b\u6545\u662f\u4e25\u91cd\u7684\u516c\u5171\u536b\u751f\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u65b0\u5174\u7ecf\u6d4e\u4f53\u4e2d\u3002\u73b0\u6709\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u533a\u57df\uff08\u5982\u5df4\u57fa\u65af\u5766\uff09\u884c\u4e3a\u591a\u6837\u6027\u7684\u4ee3\u8868\u6027\uff0c\u4e14\u5b58\u5728\u9690\u79c1\u95ee\u9898\u3002\u53ef\u9760\u68c0\u6d4b\u4e0d\u5b89\u5168\u9a7e\u9a76\u884c\u4e3a\u662f\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3001\u652f\u6301ADAS\u548c\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u5173\u952e\u524d\u63d0\u3002", "method": "\u5f00\u53d1\u4e86V-SenseDrive\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b9a\u5236\u7684Android\u5e94\u7528\u7a0b\u5e8f\u5728\u5df4\u57fa\u65af\u5766\u9a7e\u9a76\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u667a\u80fd\u624b\u673a\u7684\u60ef\u6027\u4f20\u611f\u5668\u548cGPS\u6570\u636e\u4e0e\u540c\u6b65\u7684\u8def\u9762\u89c6\u9891\uff0c\u8bb0\u5f55\u4e86\u4e09\u79cd\u76ee\u6807\u9a7e\u9a76\u884c\u4e3a\uff08\u6b63\u5e38\u3001\u6fc0\u8fdb\u3001\u5371\u9669\uff09\uff0c\u6db5\u76d6\u4e86\u57ce\u5e02\u4e3b\u5e72\u9053\u3001\u6b21\u5e72\u9053\u548c\u9ad8\u901f\u516c\u8def\u7b49\u591a\u79cd\u9053\u8def\u7c7b\u578b\u3002\u5de5\u4f5c\u91cd\u70b9\u5728\u4e8e\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u5305\u62ec\u53c2\u4e0e\u8005\u9009\u62e9\u3001\u9a7e\u9a76\u573a\u666f\u3001\u73af\u5883\u8003\u91cf\u548c\u4f20\u611f\u5668\u89c6\u9891\u540c\u6b65\u6280\u672f\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86V-SenseDrive\u6570\u636e\u96c6\uff0c\u5b83\u662f\u9996\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u5df4\u57fa\u65af\u5766\u591a\u6a21\u6001\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5c06\u9ad8\u9891\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u548cGPS\u6570\u636e\u6d41\u4e0e\u8fde\u7eed\u89c6\u9891\u7ed3\u5408\uff0c\u6240\u6709\u6570\u636e\u6e90\u90fd\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\uff0c\u5e76\u88ab\u7ec4\u7ec7\u6210\u539f\u59cb\u3001\u5904\u7406\u548c\u8bed\u4e49\u5c42\uff0c\u786e\u4fdd\u4e86\u5176\u5728\u672a\u6765\u7814\u7a76\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "V-SenseDrive\u6570\u636e\u96c6\u586b\u8865\u4e86\u5168\u7403\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u5728\u4ee3\u8868\u6027\u4e0a\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u9a7e\u9a76\u884c\u4e3a\u5206\u7c7b\u3001\u4ea4\u901a\u5b89\u5168\u5206\u6790\u548c\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u60c5\u5883\u611f\u77e5\u7684\u667a\u80fd\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18116", "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u644a\u9500\u6f5c\u5728\u7a7a\u95f4\u5f15\u5bfc\u201d\uff08ALS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u4e00\u4e2a\u65b9\u5411\u5411\u91cf\u6765\u53d6\u4ee3\u6602\u8d35\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u5faa\u73af\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u8fed\u4ee3\u7ec6\u5316\u3001\u591a\u6b65\u9a8c\u8bc1\u3001\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\uff09\u56e0\u9700\u8981\u5927\u91cf\u8ba1\u7b97\uff0810-100\u500d\u4e8e\u6807\u51c6\u89e3\u7801\uff09\u800c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u5373\u4f7f\u662f\u6f5c\u5728\u7a7a\u95f4\u65b9\u6cd5\u4e5f\u9700\u8981\u591a\u6b65\u53cd\u5411\u4f20\u64ad\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "Amortized Latent Steering (ALS) \u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u79bb\u7ebf\u8ba1\u7b97\u6210\u529f\u4e0e\u4e0d\u6210\u529f\u751f\u6210\u4e4b\u95f4\u9690\u85cf\u72b6\u6001\u7684\u5e73\u5747\u5dee\u5f02\u5411\u91cf\u3002\u5728\u63a8\u7406\u65f6\uff0cALS\u4ee5\u6052\u5b9a\u6210\u672c\u5e94\u7528\u8fd9\u4e2a\u9884\u8ba1\u7b97\u7684\u5411\u91cf\uff0c\u5f53\u89e3\u7801\u504f\u79bb\u6210\u529f\u6d41\u5f62\u65f6\uff0c\u5b83\u4f1a\u5c06\u6fc0\u6d3b\u503c\u63a8\u56de\u8be5\u6d41\u5f62\uff0c\u4ece\u800c\u5c06\u8fed\u4ee3\u4f18\u5316\u6298\u53e0\u4e3a\u5355\u6b21\u64cd\u4f5c\u3002", "result": "\u5728GSM8K\u548cMATH-500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALS\u6bd4\u8fed\u4ee3\u65b9\u6cd5\u5feb2-5\u500d\uff0c\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u8d2a\u5a6a\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u81ea\u6d3d\u6027\u57fa\u7ebf\uff0c\u5728\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u65b9\u9762\u6700\u9ad8\u5b9e\u73b0\u4e86101%\u7684\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6f5c\u5728\u4f18\u5316\u7684\u8bb8\u591a\u76ca\u5904\u53ef\u4ee5\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u6355\u83b7\uff0c\u8fd9\u4f7f\u5f97\u590d\u6742\u7684\u63a8\u7406\u6280\u672f\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u53d8\u5f97\u53ef\u884c\u548c\u5b9e\u7528\u3002"}}
{"id": "2509.18458", "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "CogniLoad\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u7684\u65b0\u578b\u57fa\u51c6\uff0c\u7528\u4e8e\u8bca\u65adLLM\u5728\u957f\u6587\u672c\u63a8\u7406\u4e2d\u56e0\u4efb\u52a1\u957f\u5ea6\u3001\u5185\u5728\u590d\u6742\u6027\u548c\u5e72\u6270\u7269\u9020\u6210\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u957f\u6587\u672c\u63a8\u7406\u57fa\u51c6\u6a21\u7cca\u4e86\u5185\u5728\u4efb\u52a1\u590d\u6742\u6027\u3001\u5e72\u6270\u7269\u5e72\u6270\u548c\u4efb\u52a1\u957f\u5ea6\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5bfc\u81f4\u96be\u4ee5\u8fdb\u884c\u7cbe\u786e\u7684\u6545\u969c\u5206\u6790\u3002", "method": "\u5f15\u5165CogniLoad\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff08CLT\uff09\u7684\u5408\u6210\u57fa\u51c6\u3002\u5b83\u751f\u6210\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u8c1c\u9898\uff0c\u53c2\u6570\u53ef\u72ec\u7acb\u8c03\u8282\uff1a\u5185\u5728\u96be\u5ea6($d$)\u63a7\u5236\u5185\u5728\u8d1f\u8377\uff0c\u5e72\u6270\u7269\u4e0e\u4fe1\u53f7\u6bd4($\rho$)\u8c03\u8282\u65e0\u5173\u8d1f\u8377\uff0c\u4efb\u52a1\u957f\u5ea6($N$)\u4f5c\u4e3a\u6838\u5fc3\u8d1f\u8377\u7684\u4ee3\u7406\u3002\u8be5\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f3022\u4e2a\u6700\u5148\u8fdb\u7684\u63a8\u7406LLM\u3002", "result": "CogniLoad\u63ed\u793a\u4e86LLM\u72ec\u7279\u7684\u6027\u80fd\u654f\u611f\u6027\uff0c\u53d1\u73b0\u4efb\u52a1\u957f\u5ea6\u662f\u4e3b\u5bfc\u9650\u5236\u56e0\u7d20\uff0c\u5e76\u63ed\u793a\u4e86\u5bf9\u5185\u5728\u590d\u6742\u6027\u7684\u4e0d\u540c\u5bb9\u5fcd\u5ea6\u4ee5\u53ca\u5bf9\u5e72\u6270\u7269\u6bd4\u7387\u7684U\u5f62\u54cd\u5e94\u3002", "conclusion": "CogniLoad\u901a\u8fc7\u5bf9\u8ba4\u77e5\u8d1f\u8377\u7ef4\u5ea6\u8fdb\u884c\u7cfb\u7edf\u3001\u56e0\u5b50\u63a7\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u4e14\u8bca\u65ad\u4e30\u5bcc\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5256\u6790LLM\u7684\u63a8\u7406\u5c40\u9650\u6027\u5e76\u6307\u5bfc\u672a\u6765\u7684\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2509.18226", "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "categories": ["cs.AI"], "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands.", "AI": {"tldr": "ChefMind\u662f\u4e00\u4e2a\u7ed3\u5408CoE\u3001KG\u3001RAG\u548cLLM\u7684\u6df7\u5408\u67b6\u6784\uff0c\u80fd\u6709\u6548\u5904\u7406\u6a21\u7cca\u7684\u7528\u6237\u610f\u56fe\uff0c\u63d0\u9ad8\u4e2a\u6027\u5316\u98df\u8c31\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u4e2a\u6027\u5316\u98df\u8c31\u63a8\u8350\u9762\u4e34\u5904\u7406\u6a21\u7cca\u7528\u6237\u610f\u56fe\u3001\u4fdd\u8bc1\u8bed\u4e49\u51c6\u786e\u6027\u548c\u63d0\u4f9b\u8db3\u591f\u7ec6\u8282\u8986\u76d6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faChefMind\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408Chain of Exploration (CoE) \u5c06\u6a21\u7cca\u67e5\u8be2\u7cbe\u70bc\u4e3a\u7ed3\u6784\u5316\u6761\u4ef6\uff0c\u77e5\u8bc6\u56fe\u8c31 (KG) \u63d0\u4f9b\u8bed\u4e49\u63a8\u7406\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u8865\u5145\u70f9\u996a\u7ec6\u8282\uff0c\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u6574\u5408\u8f93\u51fa\u3002\u5728Xiachufang\u6570\u636e\u96c6\u548c\u624b\u52a8\u6807\u6ce8\u67e5\u8be2\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5355\u4e00LLM\u3001KG\u3001RAG\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "ChefMind\u5728\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e73\u5747\u5f97\u52068.7\uff0c\u8fdc\u9ad8\u4e8e\u6d88\u878d\u6a21\u578b\u76846.4-6.7\u3002\u6b64\u5916\uff0c\u5b83\u5c06\u672a\u5904\u7406\u67e5\u8be2\u964d\u4f4e\u81f31.6%\uff0c\u663e\u793a\u51fa\u5904\u7406\u6a21\u7cca\u9700\u6c42\u65f6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ChefMind\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u5148\u8fdb\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u98df\u8c31\u63a8\u8350\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u5b8c\u6574\u548c\u6e05\u6670\u7684\u63a8\u8350\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5904\u7406\u6a21\u7cca\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18189", "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Qianfan-VL\u7cfb\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff083B-70B\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9886\u57df\u589e\u5f3a\u6280\u672f\uff08\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u6570\u636e\u5408\u6210\uff09\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u901a\u7528\u53ca\u7279\u5b9a\u9886\u57df\uff08\u5982OCR\u3001\u6587\u6863\u7406\u89e3\u3001\u6570\u5b66\u63a8\u7406\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5168\u90e8\u5728\u4e2d\u56fd\u6606\u4ed1P800\u82af\u7247\u4e0a\u8bad\u7ec3\u5b8c\u6210\uff0c\u5c55\u793a\u4e86\u56fd\u5185\u5927\u89c4\u6a21AI\u57fa\u7840\u8bbe\u65bd\u7684\u5f3a\u5927\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u901a\u7528\u6027\u80fd\u540c\u65f6\u663e\u8457\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u591a\u6837\u5316\u4f01\u4e1a\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u53ca\u56fd\u5185AI\u57fa\u7840\u8bbe\u65bd\u8bad\u7ec3SOTA\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528Qianfan-VL\u7cfb\u5217\uff083B-70B\uff09\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1. \u521b\u65b0\u7684\u9886\u57df\u589e\u5f3a\u6280\u672f\uff1b2. \u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff1b3. \u9ad8\u7cbe\u5ea6\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff1b4. 8B\u548c70B\u7248\u672c\u878d\u5165\u957f\u94fe\u5f0f\u601d\u7ef4\u80fd\u529b\uff1b5. \u6a21\u578b\u5b8c\u5168\u5728\u767e\u5ea6\u6606\u4ed1P800\u82af\u7247\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Qianfan-VL\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u4e0e\u9886\u5148\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u5728CCBench\u3001SEEDBench IMG\u3001ScienceQA\u548cMMStar\u7b49\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002\u5728OCR\u548c\u6587\u6863\u7406\u89e3\u65b9\u9762\u4f18\u52bf\u663e\u8457\uff08OCRBench 873\uff0cDocVQA 94.75%\uff09\u30028B\u548c70B\u7248\u672c\u5728\u6570\u5b66\u63a8\u7406\uff08MathVista 78.6%\uff09\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u9a8c\u8bc1\u4e86\u767e\u5ea6\u6606\u4ed1P800\u82af\u7247\u57285000\u5757\u82af\u7247\u4e0a\u5355\u4efb\u52a1\u6269\u5c55\u6548\u7387\u8d85\u8fc790%\uff0c\u80fd\u591f\u8bad\u7ec3SOTA\u7ea7\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u79cd\u5f00\u53d1\u9886\u57df\u589e\u5f3a\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u4f01\u4e1a\u90e8\u7f72\u573a\u666f\u3002\u5b83\u4e5f\u8bc1\u660e\u4e86\u56fd\u5185\u5927\u89c4\u6a21AI\u57fa\u7840\u8bbe\u65bd\u5177\u5907\u8bad\u7ec3SOTA\u7ea7\u591a\u6a21\u6001\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u548c\u9ad8\u6269\u5c55\u6548\u7387\u3002"}}
{"id": "2509.18117", "pdf": "https://arxiv.org/pdf/2509.18117", "abs": "https://arxiv.org/abs/2509.18117", "authors": ["Eric Petit", "Denis Ch\u00eane"], "title": "Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs", "categories": ["cs.LG"], "comment": "soumis {\\`a} la conf{\\'e}rence IHM 2025", "summary": "The paper presents a machine learning approach to design digital interfaces\nthat can dynamically adapt to different users and usage strategies. The\nalgorithm uses Bayesian statistics to model users' browsing behavior, focusing\non their habits rather than group preferences. It is distinguished by its\nonline incremental learning, allowing reliable predictions even with little\ndata and in the case of a changing environment. This inference method generates\na task model, providing a graphical representation of navigation with the usage\nstatistics of the current user. The algorithm learns new tasks while preserving\nprior knowledge. The theoretical framework is described, and simulations show\nthe effectiveness of the approach in stationary and non-stationary\nenvironments. In conclusion, this research paves the way for adaptive systems\nthat improve the user experience by helping them to better navigate and act on\ntheir interface.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u589e\u91cf\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u80fd\u6839\u636e\u7528\u6237\u6d4f\u89c8\u4e60\u60ef\u52a8\u6001\u8c03\u6574\u7684\u6570\u5b57\u754c\u9762\uff0c\u65e8\u5728\u63d0\u5347\u7528\u6237\u5bfc\u822a\u548c\u64cd\u4f5c\u4f53\u9a8c\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7528\u6237\u548c\u4f7f\u7528\u7b56\u7565\u7684\u6570\u5b57\u754c\u9762\uff0c\u7279\u522b\u662f\u5173\u6ce8\u7528\u6237\u4e2a\u4f53\u4e60\u60ef\u800c\u975e\u7fa4\u4f53\u504f\u597d\uff0c\u4ee5\u671f\u901a\u8fc7\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\u754c\u9762\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u5efa\u6a21\u7528\u6237\u6d4f\u89c8\u884c\u4e3a\u548c\u4e60\u60ef\u3002\u5176\u7279\u70b9\u662f\uff1a\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\uff08\u5373\u4f7f\u6570\u636e\u91cf\u5c11\u4e5f\u80fd\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\uff0c\u5e76\u9002\u5e94\u73af\u5883\u53d8\u5316\uff09\uff0c\u80fd\u751f\u6210\u4efb\u52a1\u6a21\u578b\uff08\u63d0\u4f9b\u5f53\u524d\u7528\u6237\u5bfc\u822a\u7684\u56fe\u5f62\u5316\u8868\u793a\u548c\u4f7f\u7528\u7edf\u8ba1\uff09\uff0c\u5e76\u80fd\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u540c\u65f6\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u7406\u8bba\u6846\u67b6\u5f97\u5230\u4e86\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5e73\u7a33\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u81ea\u9002\u5e94\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u901a\u8fc7\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\u754c\u9762\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.18467", "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "LAWCAT\u662f\u4e00\u4e2a\u65b0\u7684\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u80fd\u5c06\u9884\u8bad\u7ec3Transformer\u7684\u80fd\u529b\u9ad8\u6548\u8fc1\u79fb\u5230\u9ad8\u6027\u80fd\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u957f\u5e8f\u5217\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u74f6\u9888\uff0c\u800c\u4ece\u5934\u8bad\u7ec3\u7ebf\u6027\u590d\u6742\u5ea6\u66ff\u4ee3\u65b9\u6848\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faLAWCAT (Linear Attention with Convolution Across Time)\uff0c\u901a\u8fc7\u96c6\u6210\u56e0\u679cConv1D\u5c42\u589e\u5f3a\u5c40\u90e8\u4f9d\u8d56\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u5f52\u4e00\u5316\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u5c06\u9884\u8bad\u7ec3Transformer\u7684\u80fd\u529b\u8fc1\u79fb\u5230\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u3002", "result": "LAWCAT\u5728Mistral-7B\u4e0a\u4ec5\u75281K\u957f\u5ea6\u5e8f\u5217\u84b8\u998f\uff0c\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe22K tokens\u768490%\u4ee5\u4e0a\u5bc6\u94a5\u68c0\u7d22\u51c6\u786e\u7387\u3002Llama3.2-1B LAWCAT\u53d8\u4f53\u5728S-NIAH\u548cBABILong\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6240\u9700\u9884\u8bad\u7ec3tokens\u4e0d\u52300.1%\u3002\u6b64\u5916\uff0c\u5728\u8d85\u8fc78K tokens\u7684\u5e8f\u5217\u4e0a\uff0cLAWCAT\u7684\u9884\u586b\u5145\u901f\u5ea6\u6bd4FlashAttention-2\u66f4\u5feb\u3002", "conclusion": "LAWCAT\u4e3a\u9ad8\u6027\u80fd\u3001\u957f\u4e0a\u4e0b\u6587\u7ebf\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u957f\u5e8f\u5217\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.18229", "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "categories": ["cs.AI", "70, 74, 76, 80"], "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08\u5982GPT\uff09\u5728\u89e3\u51b3\u673a\u68b0\u5de5\u7a0b\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u6f5c\u5728\u80fd\u529b\u4f46\u4e0d\u53ef\u9760\u3002\u672c\u6587\u63d0\u51fa\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u673a\u6784\uff0c\u901a\u8fc7\u96c6\u6210N\u4e2a\u72ec\u7acb\u6c42\u89e3\u5668\u548c\u4e00\u4e2a\u6bd4\u8f83\u5668\uff0c\u7ed3\u5408\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u6559\u80b2\u548c\u5de5\u7a0b\u5b9e\u8df5\u3002", "motivation": "\u5c3d\u7ba1GPT\u80fd\u4e3a\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u53ef\u9760\u6027\u4e0d\u9ad8\uff08\u4f8b\u5982\uff0c\u5728\u540c\u4e00\u95ee\u9898\u4e0a\u6210\u529f\u7387\u4ec585%\uff09\uff0c\u8fd9\u4f7f\u5f97\u201c\u5f00\u7bb1\u5373\u7528\u201d\u7684GPT\u4e0d\u9002\u5408\u5728\u6559\u80b2\u6216\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u673a\u6784\uff1a\u9996\u5148\u542f\u52a8N\u4e2a\u201cAgent Solve\u201d\u5b9e\u4f8b\u751f\u6210N\u4e2a\u72ec\u7acb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u8c03\u7528\u201cAgent Compare\u201d\u6765\u603b\u7ed3\u3001\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6848\u5e76\u63d0\u4f9b\u63a8\u8350\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\uff0c\u4ee5\u786e\u4fdd\u591a\u6570\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\u3002Agent Compare\u8fd8\u80fd\u6574\u5408\u6b21\u8981\u89e3\u51b3\u65b9\u6848\uff0c\u8003\u8651\u4e0d\u540c\u7684\u95ee\u9898\u89e3\u91ca\u6216\u6570\u5b66\u6a21\u578b\u3002", "result": "\u5bf9\u4e8e\u6bcf\u4e2a\u6c42\u89e3\u5668\u6210\u529f\u6982\u7387\u5927\u4e8e1/2\u7684\u95ee\u9898\uff0c\u8be5\u4ee3\u7406\u673a\u6784\u63a8\u8350\u7684\uff08Predominant\uff09\u89e3\u51b3\u65b9\u6848\u5c06\u4ee5\u9ad8\u6982\u7387\u5bf9\u5e94\u4e8e\u6b63\u786e\u65b9\u6848\u3002\u4e0e\u5546\u4e1a\u591a\u4ee3\u7406\u6a21\u578b\uff08\u5982Grok Heavy\uff09\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u4ee3\u7406\u673a\u6784\u5728\u8bbe\u8ba1\u548c\u6027\u80fd\u4e0a\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46\u5728\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u65b9\u9762\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u673a\u6784\u80fd\u6709\u6548\u514b\u670d\u751f\u6210\u5f0fAI\u5728\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u4e2d\u5b58\u5728\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u5408\u591a\u4e2aAI\u7684\u5224\u65ad\u5e76\u8fdb\u884c\u6bd4\u8f83\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u3001\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f5c\u4e3a\u673a\u68b0\u5de5\u7a0b\u95ee\u9898\u521d\u6b65\u5206\u6790\u7684\u4f4e\u6210\u672c\u5de5\u5177\u3002"}}
{"id": "2509.18190", "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHazeFlow\uff0c\u4e00\u4e2a\u57fa\u4e8eODE\u7684\u6846\u67b6\uff0c\u5c06ASM\u91cd\u6784\u4e3aODE\uff0c\u5e76\u901a\u8fc7\u7c7bRectified Flow\u5b66\u4e60\u6700\u4f73\u8f68\u8ff9\u8fdb\u884c\u5355\u6b65\u53bb\u96fe\uff1b\u540c\u65f6\u5f15\u5165MCBM\u751f\u6210\u975e\u5747\u5300\u96fe\u973e\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u53bb\u96fe\u65b9\u6cd5\u53d7\u9650\u4e8e\u771f\u5b9e\u4e16\u754c\u6210\u5bf9\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\u548c\u57df\u95f4\u9699\u95ee\u9898\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff1b\u4f20\u7edf\u57fa\u4e8e\u5927\u6c14\u6563\u5c04\u6a21\u578b(ASM)\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u590d\u6742\u591a\u6837\u7684\u96fe\u973e\u6a21\u5f0f\u3002", "method": "1. **HazeFlow\u6846\u67b6**\uff1a\u5c06\u5927\u6c14\u6563\u5c04\u6a21\u578b(ASM)\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e38\u5fae\u5206\u65b9\u7a0b(ODE)\uff0c\u5e76\u53d7Rectified Flow\u542f\u53d1\uff0c\u5b66\u4e60\u4ece\u96fe\u973e\u56fe\u50cf\u5230\u6e05\u6670\u56fe\u50cf\u7684\u6700\u4f73ODE\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5355\u6b65\u63a8\u7406\u53bb\u96fe\u30022. **\u975e\u5747\u5300\u96fe\u973e\u751f\u6210**\uff1a\u5f15\u5165\u9a6c\u5c14\u53ef\u592b\u94fe\u5e03\u6717\u8fd0\u52a8(MCBM)\u6765\u6a21\u62df\u751f\u6210\u903c\u771f\u7684\u975e\u5747\u5300\u96fe\u973e\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6210\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0cHazeFlow\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb(state-of-the-art)\u7684\u6027\u80fd\u3002", "conclusion": "HazeFlow\u901a\u8fc7\u521b\u65b0\u7684ODE-based\u6846\u67b6\u548cMCBM\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96fe\u6027\u80fd\u3002"}}
{"id": "2509.18118", "pdf": "https://arxiv.org/pdf/2509.18118", "abs": "https://arxiv.org/abs/2509.18118", "authors": ["Marcelo Ribeiro", "Diogo Costa", "Gon\u00e7alo Moreira", "Sandro Pinto", "Tiago Gomes"], "title": "Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Modern IoT devices increasingly rely on machine learning solutions to process\ndata locally. However, the lack of graphics processing units (GPUs) or\ndedicated accelerators on most platforms makes on-device training largely\ninfeasible, often requiring cloud-based services to perform this task. This\nprocedure often raises privacy-related concerns, and creates dependency on\nreliable and always-on connectivity. Federated Learning (FL) is a new trend\nthat addresses these issues by enabling decentralized and collaborative\ntraining directly on devices, but it requires highly efficient optimization\nalgorithms. L-SGD, a lightweight variant of stochastic gradient descent, has\nenabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).\nThis work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture\nthat still lacks robust support for on-device training. L-SGD was evaluated on\nboth Arm and RISC-V platforms using 32-bit floating-point arithmetic,\nhighlighting the performance impact of the absence of Floating-Point Units\n(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit\nquantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in\nmemory usage and a 2.2x speedup in training time, with negligible accuracy\ndegradation.", "AI": {"tldr": "\u672c\u6587\u5c06\u8f7b\u91cf\u7ea7\u968f\u673a\u68af\u5ea6\u4e0b\u964d (L-SGD) \u6269\u5c55\u5230RISC-V MCU\uff0c\u5e76\u63d0\u51fa\u51768\u4f4d\u91cf\u5316\u7248\u672c\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u548c\u52a0\u901f\u8bad\u7ec3\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u7269\u8054\u7f51\u8bbe\u5907\u5bf9\u672c\u5730\u673a\u5668\u5b66\u4e60\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u591a\u6570\u8bbe\u5907\u7f3a\u4e4fGPU\u6216\u4e13\u7528\u52a0\u901f\u5668\uff0c\u4f7f\u5f97\u8bbe\u5907\u4e0a\u8bad\u7ec3\u56f0\u96be\uff0c\u901a\u5e38\u4f9d\u8d56\u4e91\u670d\u52a1\uff0c\u4ece\u800c\u5f15\u53d1\u9690\u79c1\u548c\u8fde\u63a5\u6027\u95ee\u9898\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002RISC-V\u4f5c\u4e3a\u65b0\u5174\u5f00\u653e\u67b6\u6784\uff0c\u76ee\u524d\u5c1a\u7f3a\u4e4f\u5bf9\u8bbe\u5907\u4e0a\u8bad\u7ec3\u7684\u5f3a\u5927\u652f\u6301\u3002", "method": "1. \u5c06\u9002\u7528\u4e8eArm Cortex-M MCU\u7684L-SGD\u7b97\u6cd5\u6269\u5c55\u5230RISC-V MCU\u30022. \u4f7f\u752832\u4f4d\u6d6e\u70b9\u8fd0\u7b97\u5728Arm\u548cRISC-V\u5e73\u53f0\u4e0a\u5bf9L-SGD\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u91cf\u5316RISC-V MCU\u4e2dFPU\u7f3a\u5931\u5e26\u6765\u7684\u6027\u80fd\u5f71\u54cd\u30023. \u4e3a\u51cf\u8f7bRISC-V\u7684\u786c\u4ef6\u5c40\u9650\u6027\uff0c\u63d0\u51faL-SGD\u76848\u4f4d\u91cf\u5316\u7248\u672c\u3002", "result": "1. 32\u4f4d\u6d6e\u70b9L-SGD\u5728RISC-V\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u53d7\u5230FPU\u7f3a\u5931\u7684\u663e\u8457\u5f71\u54cd\u30022. \u63d0\u51fa\u76848\u4f4d\u91cf\u5316L-SGD\u7248\u672c\u5728RISC-V\u4e0a\u5b9e\u73b0\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\u8fd14\u500d\u7684\u51cf\u5c11\u30023. 8\u4f4d\u91cf\u5316L-SGD\u5728\u8bad\u7ec3\u65f6\u95f4\u4e0a\u8fbe\u5230\u4e862.2\u500d\u7684\u52a0\u901f\u30024. 8\u4f4d\u91cf\u5316L-SGD\u7684\u51c6\u786e\u6027\u9000\u5316\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165L-SGD\u76848\u4f4d\u91cf\u5316\u7248\u672c\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3RISC-V MCU\u5728\u8bbe\u5907\u4e0a\u8bad\u7ec3\u9762\u4e34\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218\uff0c\u4e3aRISC-V\u67b6\u6784\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u3001\u534f\u4f5c\u5f0f\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18487", "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u4e30\u5bcc\u56fe\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u8bc4\u4f30\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4ee3\u7801\u751f\u6210\uff09\u53ca\u5176\u5bf9\u56fe\u7ed3\u6784\u3001\u6587\u672c\u7279\u5f81\u7684\u4f9d\u8d56\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5728LLMs\u4e0e\u56fe\u6570\u636e\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3001\u539f\u5219\u6027\u7684\u7406\u89e3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8003\u5bdf\u4e86\u591a\u4e2a\u5173\u952e\u53d8\u91cf\u8f74\uff1aLLM-\u56fe\u4ea4\u4e92\u6a21\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4ee3\u7801\u751f\u6210\uff09\uff1b\u6570\u636e\u96c6\u9886\u57df\uff08\u5f15\u7528\u3001\u7f51\u9875\u94fe\u63a5\u3001\u7535\u5546\u3001\u793e\u4ea4\u7f51\u7edc\uff09\uff1b\u56fe\u7ed3\u6784\u7c7b\u578b\uff08\u540c\u8d28\u56fe\u3001\u5f02\u8d28\u56fe\uff09\uff1b\u7279\u5f81\u7c7b\u578b\uff08\u77ed\u6587\u672c\u3001\u957f\u6587\u672c\uff09\uff1b\u4ee5\u53caLLM\u6a21\u578b\u914d\u7f6e\uff08\u5927\u5c0f\u548c\u63a8\u7406\u80fd\u529b\uff09\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u622a\u65ad\u7279\u5f81\u3001\u5220\u9664\u8fb9\u548c\u79fb\u9664\u6807\u7b7e\u6765\u91cf\u5316LLMs\u5bf9\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\u7684\u4f9d\u8d56\u6027\u3002", "result": "(1) LLMs\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u5668\u5728\u56fe\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u6216\u9ad8\u5bc6\u5ea6\u56fe\u4e0a\u4f18\u52bf\u663e\u8457\u3002(2) \u6240\u6709\u4ea4\u4e92\u7b56\u7565\u5728\u5f02\u8d28\u56fe\u4e0a\u5747\u4fdd\u6301\u6709\u6548\uff0c\u6311\u6218\u4e86LLM\u65b9\u6cd5\u5728\u4f4e\u540c\u8d28\u6027\u4e0b\u5d29\u6e83\u7684\u5047\u8bbe\u3002(3) \u4ee3\u7801\u751f\u6210\u5668\u80fd\u7075\u6d3b\u8c03\u6574\u5bf9\u7ed3\u6784\u3001\u7279\u5f81\u6216\u6807\u7b7e\u7684\u4f9d\u8d56\uff0c\u4ee5\u5229\u7528\u6700\u5177\u4fe1\u606f\u91cf\u7684\u8f93\u5165\u7c7b\u578b\u3002", "conclusion": "\u7814\u7a76\u5168\u9762\u63ed\u793a\u4e86\u5f53\u524dLLM-\u56fe\u4ea4\u4e92\u6a21\u5f0f\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u539f\u5219\u548c\u5b9e\u7528\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.18230", "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faComputerAgent\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u684c\u9762\u5e94\u7528\u63a7\u5236\u3002\u5b83\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709MLLM\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u684c\u9762\u5e94\u7528\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u957f\u5468\u671f\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u6837\u672c\u6548\u7387\u4f4e\u4ee5\u53ca\u96be\u4ee5\u8bbe\u5907\u7aef\u90e8\u7f72\u7b49\u95ee\u9898\u3002", "method": "\u5f15\u5165ComputerAgent\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u64cd\u4f5c\u7cfb\u7edf\u63a7\u5236\u8868\u8ff0\u4e3a\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff08\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\uff09\uff0c\u91c7\u7528\u4e09\u6a21\u6001\u72b6\u6001\u7f16\u7801\u5668\uff08\u622a\u56fe\u3001\u4efb\u52a1ID\u3001\u6570\u5b57\u72b6\u6001\uff09\uff0c\u96c6\u6210\u5e26\u6709\u65e9\u671f\u505c\u6b62\u673a\u5236\u7684\u5143\u52a8\u4f5c\uff0c\u5e76\u4f7f\u7528\u7d27\u51d1\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u5c0f\u578b\u7b56\u7565\u7f51\u7edc\uff0815M\u53c2\u6570\uff09\u5b9e\u73b0\u8bbe\u5907\u7aef\u63a8\u7406\u3002", "result": "\u5728135\u4e2a\u771f\u5b9e\u4e16\u754c\u684c\u9762\u4efb\u52a1\u4e2d\uff0cComputerAgent\u5728\u7b80\u5355\u4efb\u52a1\uff08<8\u6b65\uff09\u4e0a\u8fbe\u523092.1%\u7684\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u4efb\u52a1\uff08>=8\u6b65\uff09\u4e0a\u8fbe\u523058.8%\u7684\u6210\u529f\u7387\u3002\u5b83\u5728\u7b80\u5355\u573a\u666f\u4e0b\u4e0e200B\u53c2\u6570\u7684MLLM\u57fa\u7ebf\u76f8\u5f53\u6216\u8d85\u8d8a\uff0c\u540c\u65f6\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e86\u56db\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u5e76\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e3a\u8ba1\u7b97\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u76f8\u8f83\u4e8e\u5355\u4e00\u7684\u57fa\u4e8eMLLM\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2509.18193", "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u3001QAT\u548cTensorRT\u4f18\u5316\u7684EcoWeedNet\u538b\u7f29\u7248\u6a21\u578b\uff0c\u6210\u529f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7cbe\u51c6\u7684\u519c\u4e1a\u6742\u8349\u68c0\u6d4b\uff0c\u5176\u6027\u80fd\u4f18\u4e8eYOLOv11n\u548cYOLOv12n\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u519c\u4e1a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u9762\u4e34\u8d44\u6e90\u53d7\u9650\u7684\u6311\u6218\u3002", "method": "\u5bf9EcoWeedNet\u6a21\u578b\u91c7\u7528\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u3001\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u548cNVIDIA TensorRT\u52a0\u901f\u6280\u672f\uff0c\u5e76\u5728Jetson Orin Nano\u4e0a\u8fdb\u884c\u90e8\u7f72\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u9ad8\u8fbe68.5%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c113.2 GFLOPs\u3002\u63a8\u7406\u901f\u5ea6\u8fbe\u5230184 FPS (FP16)\uff0c\u6bd4\u57fa\u7ebf\u5feb28.7%\u3002\u5728CottonWeedDet12\u6570\u636e\u96c6\u4e0a\uff0c\u526a\u679d39.5%\u7684EcoWeedNet\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cmAP50\u65b9\u9762\u5747\u4f18\u4e8eYOLOv11n\u548cYOLOv12n\uff0cmAP50\u8fbe\u523085.9%\u3002", "conclusion": "\u8be5\u538b\u7f29\u7248EcoWeedNet\u6a21\u578b\u88ab\u8bc1\u660e\u5bf9\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\u800c\u8a00\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684\u3002"}}
{"id": "2509.18119", "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "AI": {"tldr": "MOBILERL\u662f\u4e00\u4e2a\u5728\u7ebf\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u81ea\u9002\u5e94\u7b97\u6cd5\u548c\u5956\u52b1\u8c03\u6574\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79fb\u52a8GUI\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u5f00\u53d1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u79fb\u52a8GUI\u667a\u80fd\u4f53\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4efb\u52a1\u96be\u5ea6\u5206\u5e03\u5448\u91cd\u5c3e\u6548\u5e94\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u4ee5\u53ca\u5927\u89c4\u6a21\u73af\u5883\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMOBILERL\u5728\u7ebf\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u96be\u5ea6\u81ea\u9002\u5e94GRPO (ADAGRPO) \u7b97\u6cd5\u3002ADAGRPO\u901a\u8fc7\u8bbe\u8ba1\u96be\u5ea6\u81ea\u9002\u5e94\u6b63\u5411\u56de\u653e\u548c\u5931\u8d25\u8bfe\u7a0b\u8fc7\u6ee4\u6765\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u5f15\u5165\u6700\u77ed\u8def\u5f84\u5956\u52b1\u8c03\u6574\u7b56\u7565\u6765\u5904\u7406\u591a\u8f6e\u4efb\u52a1\u957f\u5ea6\uff0c\u5171\u540c\u63d0\u5347RL\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u91c7\u6837\u6548\u7387\u3002", "result": "\u5c06MOBILERL\u5e94\u7528\u4e8eQwen2.5-VL-7B\u548cGLM-4.1V-9B\u6a21\u578b\uff0c\u5176\u4e2dMOBILERL-9B\u6a21\u578b\u5728AndroidWorld (75.8%) \u548cAndroidLab (46.8%) \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\u3002", "conclusion": "MOBILERL\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86\u79fb\u52a8GUI\u667a\u80fd\u4f53RL\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u590d\u6742\u79fb\u52a8\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5df2\u88ab\u4ea7\u54c1\u91c7\u7528\u4e14\u5f00\u6e90\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2509.18514", "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eByT5\u7684\u5b57\u8282\u7ea7\u591a\u8bed\u8a00Transformer\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u963f\u62c9\u4f2f\u8bd7\u6b4c\u4e2d\u63d2\u5165\u7b26\u5408\u7279\u5b9a\u97f5\u5f8b\u7684\u77ed\u8bed\uff0c\u5b9e\u73b0\u4e86\u9ad8\u97f5\u5f8b\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u963f\u62c9\u4f2f\u8bd7\u6b4c\u521b\u4f5c\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u63d2\u5165\u77ed\u8bed\u4f7f\u5176\u7b26\u5408\u7279\u5b9a\u97f5\u5f8b\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528ByT5\u6a21\u578b\uff0c\u7ed3\u5408\u89c4\u5219\u578b\u5b57\u7d20\u5230\u8282\u62cd\u8f6c\u6362\u6765\u63d0\u53d6\u97f5\u5f8b\u3002\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u76ee\u6807\u5fae\u8c03ByT5\uff0c\u4f7f\u6a21\u578b\u91cd\u5efa\u906e\u853d\u8bcd\u6c47\u4ee5\u5339\u914d\u76ee\u6807\u97f5\u5f8b\u3002\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u901a\u7528\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u540e\u5728\u8bd7\u6b4c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff09\uff0c\u5e76\u63a2\u7d22\u4e86\u82f1-\u963f\u8de8\u8bed\u8a00\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u97f5\u5f8b\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u5728\u53e4\u5178\u963f\u62c9\u4f2f\u8bd7\u6b4c\u521b\u4f5c\u7684\u534f\u540c\u521b\u610f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18234", "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "\u5927\u578b\u6a21\u578b\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e94\u529b\u6d4b\u8bd5\u63ed\u793a\u5b83\u4eec\u5b58\u5728\u8106\u6027\u3001\u6295\u673a\u53d6\u5de7\uff0c\u4e14\u7f3a\u4e4f\u771f\u6b63\u7684\u533b\u5b66\u7406\u89e3\uff0c\u5f53\u524d\u7684\u57fa\u51c6\u5206\u6570\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u524d\u6cbf\u6a21\u578b\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9ad8\u5206\uff0c\u4f46\u4f5c\u8005\u6000\u7591\u8fd9\u4e9b\u5206\u6570\u662f\u5426\u771f\u6b63\u53cd\u6620\u4e86\u6a21\u578b\u7684\u533b\u5b66\u7406\u89e3\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u62c5\u5fe7\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u5956\u52b1\u201c\u5e94\u8bd5\u6280\u5de7\u201d\u800c\u975e\u771f\u5b9e\u533b\u5b66\u80fd\u529b\u3002", "method": "\u5bf9\u516d\u4e2a\u4e3b\u6d41\u6a21\u578b\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u533b\u7597\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e94\u529b\u6d4b\u8bd5\uff0c\u5305\u62ec\u79fb\u9664\u5173\u952e\u8f93\u5165\u3001\u6539\u53d8\u63d0\u793a\u8bed\uff0c\u5e76\u8f85\u4ee5\u4e34\u5e8a\u533b\u751f\u6307\u5bfc\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u5206\u6790\u57fa\u51c6\u6d4b\u91cf\u5185\u5bb9\u3002", "result": "\u9ad8\u57fa\u51c6\u5206\u6570\u63a9\u76d6\u4e86\u6a21\u578b\u7684\u8106\u6027\u548c\u201c\u6295\u673a\u53d6\u5de7\u201d\u7684\u5b66\u4e60\u65b9\u5f0f\uff1b\u6a21\u578b\u5728\u5173\u952e\u8f93\u5165\u7f3a\u5931\u65f6\u4ecd\u80fd\u731c\u5bf9\uff0c\u5728\u5fae\u5c0f\u63d0\u793a\u8bed\u53d8\u5316\u4e0b\u7b54\u6848\u7ffb\u8f6c\uff0c\u5e76\u634f\u9020\u6709\u7f3a\u9677\u7684\u63a8\u7406\u3002\u4e0d\u540c\u57fa\u51c6\u8861\u91cf\u5185\u5bb9\u5dee\u5f02\u5927\uff0c\u4e14\u88ab\u4e92\u6362\u4f7f\u7528\uff0c\u63a9\u76d6\u4e86\u6a21\u578b\u5931\u6548\u6a21\u5f0f\u3002", "conclusion": "\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u51c6\u5907\u7a0b\u5ea6\u3002\u4e3a\u4e86\u8ba9AI\u5728\u533b\u7597\u9886\u57df\u8d62\u5f97\u4fe1\u4efb\uff0c\u9700\u8981\u8d85\u8d8a\u6392\u884c\u699c\u6210\u7ee9\uff0c\u5e76\u8981\u6c42\u7cfb\u7edf\u5177\u5907\u9c81\u68d2\u6027\u3001\u53ef\u9760\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e0e\u5b9e\u9645\u533b\u7597\u9700\u6c42\u5bf9\u9f50\u3002"}}
{"id": "2509.18284", "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u589e\u5f3a\u578b\u6a21\u6001dropout\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u5904\u7406\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u6311\u6218\u6027\u573a\u666f\uff08\u5982\u5355\u6a21\u6001\u53ef\u7528\uff09\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u8bca\u65ad\u65e5\u76ca\u4f9d\u8d56\u591a\u6a21\u6001\u6570\u636e\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u6709\u6548\u878d\u5408\u5f02\u6784\u4fe1\u606f\uff0c\u5e76\u80fd\u5728\u6a21\u6001\u7f3a\u5931\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u589e\u5f3a\u578b\u6a21\u6001dropout\u548c\u5bf9\u6bd4\u5b66\u4e60\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6a21\u6001token\u4ee5\u6539\u8fdb\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u611f\u77e5\u878d\u5408\uff0c\u5e76\u4f7f\u7528\u878d\u5408\u7684\u591a\u6a21\u6001\u8868\u793a\u6765\u589e\u5f3a\u4f20\u7edf\u7684\u5355\u6a21\u6001\u5bf9\u6bd4\u76ee\u6807\u3002", "result": "\u5728\u5305\u542b\u89c6\u89c9\u548c\u8868\u683c\u6a21\u6001\u7684\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\uff08\u7528\u4e8e\u75be\u75c5\u68c0\u6d4b\u548c\u9884\u6d4b\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4ec5\u6709\u5355\u4e00\u6a21\u6001\u7684\u6311\u6218\u6027\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u6b64\u5916\uff0c\u8fd8\u6210\u529f\u4e0eCT\u57fa\u7840\u6a21\u578b\u96c6\u6210\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2509.18120", "pdf": "https://arxiv.org/pdf/2509.18120", "abs": "https://arxiv.org/abs/2509.18120", "authors": ["Thanh Linh Nguyen", "Quoc-Viet Pham"], "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DC", "cs.GT"], "comment": "Accepted in IEEE GLOBECOM 2025", "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or\nbanks) to collaboratively train artificial intelligence (AI) models while\npreserving data privacy by keeping data local. While prior work has primarily\naddressed statistical heterogeneity across organizations, a critical challenge\narises from economic competition, where organizations may act as market rivals,\nmaking them hesitant to participate in joint training due to potential utility\nloss (i.e., reduced net benefit). Furthermore, the combined effects of\nstatistical heterogeneity and inter-organizational competition on\norganizational behavior and system-wide social welfare remain underexplored. In\nthis paper, we propose CoCoGen, a coopetitive-compatible data generation\nframework, leveraging generative AI (GenAI) and potential game theory to model,\nanalyze, and optimize collaborative learning under heterogeneous and\ncompetitive settings. Specifically, CoCoGen characterizes competition and\nstatistical heterogeneity through learning performance and utility-based\nformulations and models each training round as a weighted potential game. We\nthen derive GenAI-based data generation strategies that maximize social\nwelfare. Experimental results on the Fashion-MNIST dataset reveal how varying\nheterogeneity and competition levels affect organizational behavior and\ndemonstrate that CoCoGen consistently outperforms baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoCoGen\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u6f5c\u5728\u535a\u5f08\u8bba\uff0c\u89e3\u51b3\u8de8\u7b52\u4ed3\u8054\u90a6\u5b66\u4e60\u4e2d\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ecf\u6d4e\u7ade\u4e89\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u7b56\u7565\u4f18\u5316\u793e\u4f1a\u798f\u5229\u3002", "motivation": "\u73b0\u6709\u8de8\u7b52\u4ed3\u8054\u90a6\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u4f46\u672a\u5145\u5206\u89e3\u51b3\u673a\u6784\u95f4\u7684\u7ecf\u6d4e\u7ade\u4e89\uff08\u5e02\u573a\u7ade\u4e89\u8005\uff09\u5bfc\u81f4\u7684\u53c2\u4e0e\u610f\u613f\u4f4e\u548c\u6f5c\u5728\u6536\u76ca\u635f\u5931\u95ee\u9898\u3002\u6b64\u5916\uff0c\u7edf\u8ba1\u5f02\u8d28\u6027\u4e0e\u673a\u6784\u95f4\u7ade\u4e89\u7684\u7efc\u5408\u5f71\u54cd\u53ca\u5176\u5bf9\u673a\u6784\u884c\u4e3a\u548c\u793e\u4f1a\u798f\u5229\u7684\u5f71\u54cd\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51faCoCoGen\u6846\u67b6\uff0c\u4e00\u4e2a\u5408\u4f5c\u7ade\u4e89\u517c\u5bb9\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\u3002\u5b83\u5229\u7528\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u548c\u6f5c\u5728\u535a\u5f08\u8bba\u6765\u5efa\u6a21\u3001\u5206\u6790\u548c\u4f18\u5316\u5f02\u8d28\u548c\u7ade\u4e89\u73af\u5883\u4e0b\u7684\u534f\u4f5c\u5b66\u4e60\u3002CoCoGen\u901a\u8fc7\u5b66\u4e60\u6027\u80fd\u548c\u57fa\u4e8e\u6548\u7528\u7684\u516c\u5f0f\u8868\u5f81\u7ade\u4e89\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u5e76\u5c06\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u6b21\u5efa\u6a21\u4e3a\u52a0\u6743\u6f5c\u5728\u535a\u5f08\u3002\u7136\u540e\u63a8\u5bfc\u51fa\u57fa\u4e8eGenAI\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3002", "result": "\u5728Fashion-MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u5f02\u8d28\u6027\u548c\u7ade\u4e89\u6c34\u5e73\u4f1a\u5f71\u54cd\u673a\u6784\u884c\u4e3a\u3002CoCoGen\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CoCoGen\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ecf\u6d4e\u7ade\u4e89\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u751f\u6210\u7b56\u7565\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u793e\u4f1a\u798f\u5229\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18535", "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "categories": ["cs.CL", "eess.SP"], "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u5668\u9762\u4e34\u7684\u8f6c\u8ff0\u3001\u504f\u5dee\u548c\u4fee\u6539\u6587\u672c\u7b49\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u7ed3\u5408\u53e5\u5b50\u5d4c\u5165\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u56e0\u679c\u56fe\uff0c\u6709\u6548\u68c0\u6d4b\u539f\u59cb\u53ca\u7ecf\u4fee\u6539\u7684AI\u751f\u6210\u6587\u672c\u3002", "motivation": "ChatGPT\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u6ee5\u7528\u62c5\u5fe7\uff0c\u4e9f\u9700\u9c81\u68d2\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bcd\u7ea7\u68c0\u6d4b\u5668\u6613\u53d7\u8f6c\u8ff0\u6216\u7b80\u5355\u63d0\u793a\u653b\u51fb\uff0c\u5b58\u5728ChatGPT\u8bcd\u7ea7\u6a21\u5f0f\u548c\u8bad\u7ec3\u6570\u636e\u5185\u5bb9\u5f15\u8d77\u7684\u504f\u5dee\uff0c\u5728\u4fee\u6539\u6587\u672c\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u4e14\u5e38\u9700\u5927\u578b\u6a21\u578b\u6216\u5728\u7ebfLLM\u4ea4\u4e92\u3002", "method": "\u5f15\u5165\u65b0\u4efb\u52a1\u4ee5\u68c0\u6d4b\u539f\u59cb\u53caPSP\u4fee\u6539\u7684AI\u751f\u6210\u6587\u672c\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u7684\u5185\u90e8\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b\uff0c\u8be5\u7ed3\u6784\u5728\u8bcd\u7ea7\u53d8\u5316\u4e0b\u4fdd\u6301\u4e0d\u53d8\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u5b83\u4eec\u7684\u5173\u7cfb\uff1b\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u51cf\u8f7b\u81ea\u56de\u5f52\u751f\u6210\u5e26\u6765\u7684\u5d4c\u5165\u504f\u5dee\uff1b\u7ed3\u5408\u56e0\u679c\u56fe\u4e0e\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u4ece\u4e3b\u9898\u76f8\u5173\u504f\u5dee\u4e2d\u5206\u79bb\u7ed3\u6784\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u6458\u8981\u6bd4\u8f83\u548c\u4fee\u8ba2\u540e\u7684\u751f\u6d3b\u5e38\u89c1\u95ee\u9898\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u90e8\u7ed3\u6784\u7684\u8f7b\u91cf\u7ea7AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u9762\u5bf9\u8f6c\u8ff0\u3001\u504f\u5dee\u548c\u4fee\u6539\u6587\u672c\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u539f\u59cb\u53ca\u7ecf\u4fee\u6539\u7684AI\u6587\u672c\u3002"}}
{"id": "2509.18382", "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "categories": ["cs.AI"], "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u63a8\u7406\u957f\u5ea6\u9650\u5236\u548c\u6a21\u578b\u91cf\u5316\u4e24\u79cd\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u5e8f\u5217\u867d\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63a2\u7d22\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\uff0c\u964d\u4f4e\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u7b56\u7565\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u7814\u7a76\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e24\u79cd\u5e94\u7528\u8ba1\u7b97\u7ea6\u675f\u7684\u65b9\u6cd5\uff1a1. \u4f7f\u7528\u57fa\u4e8e\u957f\u5ea6\u63a7\u5236\u7b56\u7565\u4f18\u5316\uff08LCPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5fae\u8c03\u63a8\u7406\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684CoT\u63a8\u7406\u957f\u5ea6\uff1b2. \u5e94\u7528\u6a21\u578b\u91cf\u5316\uff0c\u5728\u7528\u6237\u5b9a\u4e49\u7684\u8ba1\u7b97\u7ea6\u675f\u5185\u6700\u5927\u5316CoT\u5e8f\u5217\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u964d\u4f4e\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u5e76\u8bc4\u4f30\u5176\u5bf9\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bc4\u4f30\u63a8\u7406\u957f\u5ea6\u9650\u5236\u548c\u6a21\u578b\u91cf\u5316\u4e24\u79cd\u7b56\u7565\uff0c\u5206\u6790\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u5b89\u5168\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u4e4b\u95f4\u7684\u5177\u4f53\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5728\u4fdd\u8bc1\u6a21\u578b\u5b89\u5168\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4e\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u6709\u6548\u7b56\u7565\uff0c\u5e76\u91cf\u5316\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u95f4\u7684\u6743\u8861\uff0c\u4ece\u800c\u6307\u5bfc\u672a\u6765\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2509.18308", "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "categories": ["cs.CV"], "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e5d\u79cd\u4e3b\u6d41CNN\u548cViT\u5206\u5272\u67b6\u6784\u5728\u80ba\u6813\u585e\uff08PE\uff09\u5206\u5272\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a3D U-Net\uff08ResNet\u7f16\u7801\u5668\uff09\u6548\u679c\u6700\u4f73\uff0cCNN\u4f18\u4e8eViT\uff0c\u4e14\u9884\u8bad\u7ec3\u6709\u65f6\u4f1a\u964d\u4f4e\u5206\u5272\u6027\u80fd\uff1b\u8fdc\u7aef\u6813\u585e\u5206\u5272\u4ecd\u662f\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u5ba1\u8ba1\u80ba\u6813\u585e\uff08PE\uff09\u5206\u5272\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5404\u79cd\u6a21\u578b\u67b6\u6784\uff08CNN\u548cVision Transformer\uff09\u53ca\u5176\u521d\u59cb\u5316\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u6216\u968f\u673a\u6743\u91cd\uff09\u7684\u6027\u80fd\uff0c\u4ee5\u8bc6\u522b\u6709\u6548\u6a21\u578b\u5e76\u63ed\u793a\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b490\u4e2aCTPA\u626b\u63cf\u7684\u5bc6\u96c6\u6807\u6ce8\u81ea\u5efa\u6570\u636e\u96c6\u3002\u5728\u8be5\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u7edf\u4e00\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e5d\u79cd\u4e3b\u6d41CNN\u548cViT\u5206\u5272\u67b6\u6784\uff0c\u521d\u59cb\u5316\u65b9\u5f0f\u5305\u62ec\u9884\u8bad\u7ec3\u548c\u968f\u673a\u6743\u91cd\u3002", "result": "1. \u5e26\u6709ResNet\u7f16\u7801\u5668\u76843D U-Net\u4ecd\u662fPE\u5206\u5272\u7684\u9ad8\u6548\u67b6\u6784\u30022. 3D\u6a21\u578b\u7279\u522b\u9002\u7528\u4e8e\u6813\u585e\u7684\u5f62\u6001\u7279\u5f81\u30023. CNN\u6a21\u578b\u5728PE\u5206\u5272\u4e2d\u901a\u5e38\u4f18\u4e8eViT\u6a21\u578b\u30024. \u57fa\u4e8e\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\uff0c\u5373\u4f7f\u5728\u5927\u578bPE\u6570\u636e\u96c6\u4e0a\uff0c\u4e5f\u53ef\u80fd\u4e0d\u5229\u4e8e\u5206\u5272\u6027\u80fd\u30025. \u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u76f8\u540c\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5206\u5272\u6027\u80fd\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u7684\u6a21\u5f0f\u30026. \u4e2d\u592e\u548c\u5927\u578b\u6813\u585e\u53ef\u8fbe\u5230\u6ee1\u610f\u7cbe\u5ea6\uff0c\u4f46\u8fdc\u7aef\u6813\u585e\u4ecd\u5177\u6311\u6218\u30027. \u6700\u4f73\u6a21\u578b\u5e73\u5747Dice\u5206\u6570\u4e3a0.7131\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e2d\u68c0\u6d4b\u51fa181\u4e2a\u6813\u585e\uff0c\u670949\u4e2a\u5047\u9633\u6027\u548c28\u4e2a\u5047\u9634\u6027\uff0c\u5e76\u5df2\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "3D CNN\u6a21\u578b\uff08\u7279\u522b\u662f\u5e26ResNet\u7f16\u7801\u5668\u76843D U-Net\uff09\u5728\u80ba\u6813\u585e\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7387\uff0c\u4f18\u4e8eViT\u6a21\u578b\u4e14\u67d0\u4e9b\u9884\u8bad\u7ec3\u65b9\u5f0f\u53ef\u80fd\u4e0d\u5229\u4e8e\u5206\u5272\u3002\u867d\u7136\u5927\u578b\u6813\u585e\u53ef\u88ab\u6709\u6548\u5206\u5272\uff0c\u4f46\u8fdc\u7aef\u6813\u585e\u7684\u7cbe\u786e\u8bc6\u522b\u4ecd\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2509.18124", "pdf": "https://arxiv.org/pdf/2509.18124", "abs": "https://arxiv.org/abs/2509.18124", "authors": ["Edmund Agyemang", "Lawrence Agbota", "Vincent Agbenyeavu", "Peggy Akabuah", "Bismark Bimpong", "Christopher Attafuah"], "title": "Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters", "categories": ["cs.LG", "stat.AP"], "comment": "13 pages, 6 figures and 4 tables", "summary": "This study explores the application of supervised machine learning algorithms\nto predict coffee ratings based on a combination of influential textual and\nnumerical attributes extracted from user reviews. Through careful data\npreprocessing including text cleaning, feature extraction using TF-IDF, and\nselection with SelectKBest, the study identifies key factors contributing to\ncoffee quality assessments. Six models (Decision Tree, KNearest Neighbors,\nMulti-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained\nand evaluated using optimized hyperparameters. Model performance was assessed\nprimarily using F1-score, Gmean, and AUC metrics. Results demonstrate that\nensemble methods (Extra Trees, Random Forest, and XGBoost), as well as\nMulti-layer Perceptron, consistently outperform simpler classifiers (Decision\nTrees and K-Nearest Neighbors) in terms of evaluation metrics such as F1\nscores, G-mean and AUC. The findings highlight the essence of rigorous feature\nselection and hyperparameter tuning in building robust predictive systems for\nsensory product evaluation, offering a data driven approach to complement\ntraditional coffee cupping by expertise of trained professionals.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u7528\u6237\u8bc4\u8bba\u7684\u6587\u672c\u548c\u6570\u503c\u5c5e\u6027\uff0c\u9884\u6d4b\u5496\u5561\u8bc4\u5206\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7528\u6237\u8bc4\u8bba\u9884\u6d4b\u5496\u5561\u8bc4\u5206\uff0c\u4ee5\u8865\u5145\u4f20\u7edf\u7684\u4e13\u4e1a\u5496\u5561\u676f\u6d4b\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u6570\u636e\u9884\u5904\u7406\uff08\u6587\u672c\u6e05\u6d17\u3001TF-IDF\u7279\u5f81\u63d0\u53d6\u3001SelectKBest\u7279\u5f81\u9009\u62e9\uff09\uff0c\u8bad\u7ec3\u5e76\u4f18\u5316\u4e86\u516d\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u51b3\u7b56\u6811\u3001K\u8fd1\u90bb\u3001\u591a\u5c42\u611f\u77e5\u673a\u3001\u968f\u673a\u68ee\u6797\u3001\u6781\u7aef\u968f\u673a\u6811\u548cXGBoost\uff09\uff0c\u5e76\u4f7f\u7528F1\u5206\u6570\u3001Gmean\u548cAUC\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\uff08\u6781\u7aef\u968f\u673a\u6811\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\u548c\u591a\u5c42\u611f\u77e5\u673a\u5728F1\u5206\u6570\u3001Gmean\u548cAUC\u7b49\u8bc4\u4f30\u6307\u6807\u4e0a\uff0c\u6301\u7eed\u4f18\u4e8e\u7b80\u5355\u7684\u5206\u7c7b\u5668\uff08\u51b3\u7b56\u6811\u548cK\u8fd1\u90bb\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e25\u683c\u7684\u7279\u5f81\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u4e8e\u6784\u5efa\u7a33\u5065\u7684\u611f\u5b98\u4ea7\u54c1\u9884\u6d4b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4f20\u7edf\u5496\u5561\u676f\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u8865\u5145\u65b9\u6cd5\u3002"}}
{"id": "2509.18536", "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "CCQA\u662f\u4e00\u79cd\u57fa\u4e8e\u5faa\u73af\u4e00\u81f4\u6027\u7684\u65b0\u578b\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u95ee\u9898\u5e76\u8bc4\u4f30\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u76f8\u4f3c\u5ea6\u6765\u6539\u8fdb\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u7b56\u7565\u867d\u7136\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u65e0\u6cd5\u63d0\u5347SLMs\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86CCQA\u65b9\u6cd5\u3002\u5b83\u4ece\u6bcf\u4e2a\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\u751f\u6210\u4e00\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u8bc4\u4f30\u751f\u6210\u95ee\u9898\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u76f8\u4f3c\u5ea6\u6765\u9009\u62e9\u6700\u4f18\u89e3\u3002\u4e3a\u89e3\u51b3SLM\u751f\u6210\u95ee\u9898\u56f0\u96be\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7Flan-T5\u6a21\u578b\u8f85\u52a9\u95ee\u9898\u751f\u6210\u3002", "result": "CCQA\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u516b\u4e2a\u6a21\u578b\u5747\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5e76\u4e3aSLMs\u4e2d\u7684\u9ad8\u6548\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u7684\u5b9e\u7528\u57fa\u7ebf\u3002", "conclusion": "CCQA\u662f\u4e00\u79cd\u80fd\u6709\u6548\u5e94\u7528\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e3aSLMs\u7684\u9ad8\u6548\u63a8\u7406\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2509.18383", "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "G\u00f6del Test: Can Large Language Models Solve Easy Conjectures?", "categories": ["cs.AI", "cs.DM", "cs.LG"], "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u201c\u54e5\u5fb7\u5c14\u6d4b\u8bd5\u201d\u8bc4\u4f30GPT-5\u89e3\u51b3\u65b0\u7684\u3001\u7b80\u5355\u6570\u5b66\u731c\u60f3\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u5728\u8f83\u7b80\u5355\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u751a\u81f3\u80fd\u53cd\u9a73\u539f\u6709\u731c\u60f3\u5e76\u63d0\u51fa\u6709\u6548\u65b9\u6848\uff0c\u4f46\u5728\u8de8\u8bba\u6587\u7efc\u5408\u548c\u590d\u6742\u5206\u6790\u4e0a\u4ecd\u6709\u9650\u5236\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u5728\u9ad8\u4e2d\u548c\u672c\u79d1\u6570\u5b66\u7ade\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u80fd\u5426\u89e3\u51b3\u66f4\u9ad8\u7ea7\u6570\u5b66\u9886\u57df\u4e2d\u65b0\u7684\u3001\u7b80\u5355\u7684\u731c\u60f3\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u54e5\u5fb7\u5c14\u6d4b\u8bd5\u201d\uff0c\u5c06GPT-5\u5e94\u7528\u4e8e\u7ec4\u5408\u4f18\u5316\u9886\u57df\u7684\u4e94\u4e2a\u672a\u89e3\u51b3\u7684\u7b80\u5355\u731c\u60f3\u3002\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u4f9b\u4e86\u76f8\u5173\u6e90\u8bba\u6587\uff0c\u4f46\u672a\u544a\u77e5\u5177\u4f53\u731c\u60f3\u5185\u5bb9\uff0c\u5e76\u8be6\u7ec6\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "GPT-5\u5728\u4e09\u4e2a\u8f83\u7b80\u5355\u7684\u731c\u60f3\u4e0a\u7ed9\u51fa\u4e86\u63a5\u8fd1\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5bf9\u4e8e\u95ee\u98982\uff0c\u5b83\u5bfc\u51fa\u4e86\u4e00\u4e2a\u4e0d\u540c\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u53cd\u9a73\u4e86\u539f\u6709\u731c\u60f3\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u3002\u6a21\u578b\u5728\u9700\u8981\u7ed3\u5408\u4e24\u7bc7\u8bba\u6587\u7ed3\u679c\u7684\u95ee\u98984\u4e0a\u5931\u8d25\u3002\u5728\u95ee\u98985\u4e0a\uff0cGPT-5\u63d0\u51fa\u4e86\u4e0e\u7814\u7a76\u8005\u76f8\u540c\u7684\u7b97\u6cd5\uff0c\u4f46\u5728\u5206\u6790\u4e0a\u672a\u80fd\u6210\u529f\u3002", "conclusion": "\u5c3d\u7ba1\u6837\u672c\u91cf\u5c0f\uff0c\u4f46\u7ed3\u679c\u8868\u660e\uff0cGPT-5\u5728\u5e38\u89c4\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5076\u5c14\u5c55\u73b0\u51fa\u539f\u521b\u6027\uff0c\u4f46\u5728\u9700\u8981\u8de8\u8bba\u6587\u7efc\u5408\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002\u8fd9\u53ef\u80fd\u662f\u524d\u6cbf\u6a21\u578b\u6700\u7ec8\u901a\u8fc7\u54e5\u5fb7\u5c14\u6d4b\u8bd5\u7684\u65e9\u671f\u4e00\u6b65\u3002"}}
{"id": "2509.18309", "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "categories": ["cs.CV", "cs.LG", "I.2.10"], "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u79bb\u65f6\u5e8f\u52a8\u6001\u548c\u9759\u6001\u624b\u5f62\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u8bed\u624b\u5f62\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u51c6\u3002", "motivation": "\u624b\u5f62\u5728\u624b\u8bed\u4e2d\u626e\u6f14\u57fa\u672c\u97f3\u4f4d\u89d2\u8272\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u5f88\u5c11\u660e\u786e\u5efa\u6a21\u624b\u5f62\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u8bed\u8a00\u5206\u6790\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5c06\u65f6\u5e8f\u52a8\u6001\u4e0e\u9759\u6001\u624b\u5f62\u914d\u7f6e\u5206\u79bb\u3002\u5b83\u7ed3\u5408\u4e86\u89e3\u5256\u5b66\u542f\u53d1\u7684\u56fe\u7ed3\u6784\u4e0e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u624b\u5f62\u8bc6\u522b\u4e2d\u7684\u7ec6\u5fae\u7c7b\u522b\u5dee\u5f02\u548c\u65f6\u5e8f\u53d8\u5316\u95ee\u9898\u3002", "result": "\u5efa\u7acb\u4e86\u624b\u8bed\u5e8f\u5217\u4e2d\u7ed3\u6784\u5316\u624b\u5f62\u8bc6\u522b\u7684\u9996\u4e2a\u57fa\u51c6\uff0c\u572837\u4e2a\u624b\u5f62\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e8646%\u7684\u51c6\u786e\u7387\uff08\u57fa\u7ebf\u65b9\u6cd5\u4e3a25%\uff09\u3002", "conclusion": "\u8be5\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u663e\u8457\u5730\u63d0\u5347\u624b\u8bed\u624b\u5f62\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.18125", "pdf": "https://arxiv.org/pdf/2509.18125", "abs": "https://arxiv.org/abs/2509.18125", "authors": ["Harsha Koduri"], "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Healthcare systems face increasing pressure to allocate limited nursing\nresources efficiently while accounting for skill heterogeneity, patient acuity,\nstaff fatigue, and continuity of care. Traditional optimization and heuristic\nscheduling methods struggle to capture these dynamic, multi-constraint\nenvironments. I propose NurseSchedRL, a reinforcement learning framework for\nnurse-patient assignment that integrates structured state encoding, constrained\naction masking, and attention-based representations of skills, fatigue, and\ngeographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with\nfeasibility masks to ensure assignments respect real-world constraints, while\ndynamically adapting to patient arrivals and varying nurse availability. In\nsimulation with realistic nurse and patient data, NurseSchedRL achieves\nimproved scheduling efficiency, better alignment of skills to patient needs,\nand reduced fatigue compared to baseline heuristic and unconstrained RL\napproaches. These results highlight the potential of reinforcement learning for\ndecision support in complex, high-stakes healthcare workforce management.", "AI": {"tldr": "\u63d0\u51faNurseSchedRL\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u62a4\u58eb-\u75c5\u4eba\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u7ba1\u7406\u533b\u7597\u8d44\u6e90\uff0c\u89e3\u51b3\u591a\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u6392\u73ed\u96be\u9898\u3002", "motivation": "\u533b\u7597\u7cfb\u7edf\u9762\u4e34\u6709\u9650\u62a4\u7406\u8d44\u6e90\u3001\u6280\u80fd\u5dee\u5f02\u3001\u75c5\u4eba\u7d27\u6025\u7a0b\u5ea6\u3001\u5458\u5de5\u75b2\u52b3\u548c\u62a4\u7406\u8fde\u7eed\u6027\u7b49\u590d\u6742\u56e0\u7d20\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u52a8\u6001\u591a\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u62a4\u58eb\u6392\u73ed\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86NurseSchedRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7ed3\u6784\u5316\u72b6\u6001\u7f16\u7801\u3001\u53d7\u7ea6\u675f\u52a8\u4f5c\u63a9\u7801\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6280\u80fd\u3001\u75b2\u52b3\u53ca\u5730\u7406\u8bed\u5883\u8868\u793a\u3002\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u5e76\u7ed3\u5408\u53ef\u884c\u6027\u63a9\u7801\uff0c\u4ee5\u786e\u4fdd\u5206\u914d\u65b9\u6848\u7b26\u5408\u5b9e\u9645\u7ea6\u675f\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u75c5\u4eba\u5230\u8fbe\u548c\u62a4\u58eb\u53ef\u7528\u6027\u53d8\u5316\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u62a4\u58eb\u548c\u75c5\u4eba\u6570\u636e\u7684\u73af\u5883\u4e2d\uff0cNurseSchedRL\u76f8\u8f83\u4e8e\u57fa\u7ebf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u65e0\u7ea6\u675fRL\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6392\u73ed\u6548\u7387\u3001\u66f4\u597d\u7684\u6280\u80fd\u4e0e\u75c5\u4eba\u9700\u6c42\u5339\u914d\u5ea6\u4ee5\u53ca\u66f4\u4f4e\u7684\u62a4\u58eb\u75b2\u52b3\u5ea6\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u3001\u9ad8\u98ce\u9669\u7684\u533b\u7597\u52b3\u52a8\u529b\u7ba1\u7406\u51b3\u7b56\u652f\u6301\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18577", "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bcd\u5143\u5148\u9a8c\u7684\u5feb\u901f\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5728\u5927\u5e45\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86LLM\u9884\u8bad\u7ec3\u6570\u636e\u7684\u9009\u62e9\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u56f0\u60d1\u5ea6\uff08PPL\uff09\u7684\u8fc7\u6ee4\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u5728\u5904\u7406\u566a\u58f0\u6570\u636e\u65f6\u4e0d\u53ef\u9760\u3002", "method": "\u53d7\u8bed\u8a00\u5b66\u6d1e\u5bdf\u542f\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bcd\u5143\u5148\u9a8c\u7684\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u6599\u5e93\u7ea7\u522b\u7684\u8bcd\u9891\u7edf\u8ba1\u6765\u4f30\u8ba1\u8bcd\u5143\u5148\u9a8c\uff0c\u5e76\u6839\u636e\u8bcd\u5143\u5148\u9a8c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u8fc7\u6ee4\u6587\u6863\uff0c\u65e0\u9700\u6a21\u578b\u63a8\u7406\u3002", "result": "\u572820\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6027\u80fd\uff0c\u76f8\u6bd4PPL\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e861000\u500d\u4ee5\u4e0a\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4ee3\u7801\u3001\u6570\u5b66\u7b49\u7b26\u53f7\u8bed\u8a00\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u65e0\u9700\u76d1\u7763\u3002", "conclusion": "\u57fa\u4e8e\u8bcd\u5143\u5148\u9a8c\u7684\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6570\u636e\u9009\u62e9\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4e14\u9002\u7528\u8303\u56f4\u5e7f\u3002"}}
{"id": "2509.18400", "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "categories": ["cs.AI"], "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u6d77\u5173\u534f\u8c03\u5173\u7a0e\u5236\u5ea6\uff08HTS\uff09\u4ee3\u7801\u5206\u7c7b\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u540d\u4e3aAtlas\u7684\u5fae\u8c03LLM\u6a21\u578b\uff0c\u5176\u572810\u4f4d\u548c6\u4f4d\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41LLM\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3001\u53ef\u81ea\u6258\u7ba1\uff0c\u4f46\u8be5\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u6d77\u5173\u534f\u8c03\u5173\u7a0e\u5236\u5ea6\uff08HTS\uff09\u4e0b\u4ea7\u54c1\u5206\u7c7b\u662f\u5168\u7403\u8d38\u6613\u7684\u5173\u952e\u74f6\u9888\uff0c\u9519\u8bef\u5206\u7c7b\u53ef\u80fd\u5bfc\u81f4\u8d27\u7269\u505c\u8fd0\u3002\u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5bf9\u6b64\u5173\u6ce8\u751a\u5c11\u3002", "method": "\u7814\u7a76\u4ece\u7f8e\u56fd\u6d77\u5173\u88c1\u5b9a\u5728\u7ebf\u641c\u7d22\u7cfb\u7edf\uff08CROSS\uff09\u4e2d\u6784\u5efa\u4e86\u9996\u4e2aHTS\u4ee3\u7801\u5206\u7c7b\u57fa\u51c6\u3002\u8bc4\u4f30\u4e86\u9886\u5148\u7684LLM\u6a21\u578b\uff0c\u5e76\u5fae\u8c03\u4e86LLaMA-3.3-70B\u6a21\u578b\uff08\u547d\u540d\u4e3aAtlas\uff09\u3002", "result": "Atlas\u6a21\u578b\u572810\u4f4d\u5b8c\u5168\u6b63\u786e\u5206\u7c7b\u4e2d\u8fbe\u523040%\u7684\u51c6\u786e\u7387\uff0c\u57286\u4f4d\u6b63\u786e\u5206\u7c7b\u4e2d\u8fbe\u523057.5%\u7684\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4GPT-5-Thinking\u9ad815\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4Gemini-2.5-Pro-Thinking\u9ad827.5\u4e2a\u767e\u5206\u70b9\u3002\u6b64\u5916\uff0cAtlas\u7684\u6210\u672c\u7ea6\u4e3aGPT-5-Thinking\u7684\u4e94\u5206\u4e4b\u4e00\uff0cGemini-2.5-Pro-Thinking\u7684\u516b\u5206\u4e4b\u4e00\uff0c\u5e76\u53ef\u81ea\u6258\u7ba1\u4ee5\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u3002", "conclusion": "Atlas\u4e3aHTS\u5206\u7c7b\u4efb\u52a1\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u4f4610\u4f4d\u5206\u7c7b\u51c6\u786e\u7387\u4ec5\u4e3a40%\uff0c\u8868\u660e\u8be5\u57fa\u51c6\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u7814\u7a76\u65e8\u5728\u5c06HTS\u5206\u7c7b\u5b9a\u4f4d\u4e3a\u65b0\u7684\u793e\u533a\u57fa\u51c6\u4efb\u52a1\uff0c\u5e76\u9080\u8bf7\u672a\u6765\u5728\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u7814\u7a76\u3002"}}
{"id": "2509.18326", "pdf": "https://arxiv.org/pdf/2509.18326", "abs": "https://arxiv.org/abs/2509.18326", "authors": ["Chun Kit Wong", "Anders N. Christensen", "Cosmin I. Bercea", "Julia A. Schnabel", "Martin G. Tolsgaard", "Aasa Feragen"], "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684OOD\u68c0\u6d4b\u6027\u80fd\u53d7\u5206\u7c7b\u4efb\u52a1\u5f71\u54cd\uff0c\u6700\u4f73\u4efb\u52a1\u53d6\u51b3\u4e8eOOD\u7c7b\u578b\uff08\u56fe\u50cf\u6216\u89e3\u5256\u7279\u5f81\u6f02\u79fb\uff09\u3002\u540c\u65f6\uff0c\u4f18\u5f02\u7684OOD\u68c0\u6d4b\u4e0d\u4fdd\u8bc1\u6700\u4f73\u62d2\u7edd\u9884\u6d4b\uff0c\u5f3a\u8c03\u4efb\u52a1\u9009\u62e9\u9700\u4e0e\u4e0b\u6e38\u5e94\u7528\u5bf9\u9f50\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u5b89\u5168\u90e8\u7f72\u9700\u8981\u53ef\u9760\u7684\u57df\u5916(OOD)\u68c0\u6d4b\u80fd\u529b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5206\u7c7b\u4efb\u52a1\u672c\u8eab\u5bf9OOD\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u516b\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u548c\u56db\u79cd\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30OOD\u68c0\u6d4b\u6027\u80fd\u3002", "result": "OOD\u68c0\u6d4b\u6027\u80fd\u968f\u5206\u7c7b\u4efb\u52a1\u663e\u8457\u53d8\u5316\uff0c\u4e14\u6700\u4f73\u4efb\u52a1\u53d6\u51b3\u4e8eOOD\u6837\u672c\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u6f02\u79fb\u8fd8\u662f\u89e3\u5256\u7279\u5f81\u6f02\u79fb\u3002\u6b64\u5916\uff0c\u4f18\u8d8a\u7684OOD\u68c0\u6d4b\u6027\u80fd\u5e76\u4e0d\u4fdd\u8bc1\u6700\u4f73\u7684\u62d2\u7edd\u9884\u6d4b\u3002", "conclusion": "\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u5fc5\u987b\u6839\u636e\u5177\u4f53\u7684\u4e0b\u6e38\u5e94\u7528\uff0c\u8c03\u6574\u5206\u7c7b\u4efb\u52a1\u7684\u9009\u62e9\u548c\u4e0d\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u7684OOD\u68c0\u6d4b\u548c\u6a21\u578b\u90e8\u7f72\u3002"}}
{"id": "2509.18126", "pdf": "https://arxiv.org/pdf/2509.18126", "abs": "https://arxiv.org/abs/2509.18126", "authors": ["Bishal K C", "Amr Hilal", "Pawan Thapa"], "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u5f02\u6784\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\uff08EVCS\uff09\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u53d1\u73b0\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\uff0cFedAvgM\u8868\u73b0\u4f18\u4e8eFedAvg\uff0c\u80fd\u4e3aEVCS\u63d0\u4f9b\u7a33\u5065\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u786e\u4fdd\u7269\u8054\u7f51\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\uff08EVCS\uff09\u514d\u53d7\u7f51\u7edc\u5a01\u80c1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u5b58\u5728\u9690\u79c1\u62c5\u5fe7\uff0c\u800c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709FL-based IDS\u8bc4\u4f30\u5ffd\u7565\u4e86\u7cfb\u7edf\u5f02\u6784\u6027\u548c\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u7cfb\u7edf\u548c\u6570\u636e\u5f02\u6784\u6027\u4e0b\u5bf9\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002\u7814\u7a76\u4f7f\u7528\u4e86FedAvg\u548cFedAvgM\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\u6765\u5206\u6790\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u72ec\u7acb\u540c\u5206\u5e03\uff08IID\uff09\u8bbe\u7f6e\u4e0b\uff0cFedAvg\u6027\u80fd\u4f18\u4e8e\u4f7f\u7528\u76f8\u540c\u795e\u7ecf\u7f51\u7edc\u7684\u96c6\u4e2d\u5f0f\u6a21\u578b\u3002\u4f46\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u7cfb\u7edf\u5f02\u6784\u6027\u4e0b\uff0c\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002FedAvgM\u5728\u5f02\u6784\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8eFedAvg\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6536\u655b\u6027\u548c\u66f4\u9ad8\u7684\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u7269\u8054\u7f51\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u4e2d\u5904\u7406\u5f02\u6784\u6027\uff0c\u4e14\u6027\u80fd\u635f\u5931\u4e0d\u663e\u8457\u3002FedAvgM\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4e3a\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u63d0\u4f9b\u9c81\u68d2\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18585", "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.", "AI": {"tldr": "TsqLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u8d28\u91cf\u91c7\u6837\u548c\u5c42\u654f\u611f\u5ea6\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5927\u6a21\u578b\u5fae\u8c03\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5168\u53c2\u6570\u5fae\u8c03\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u6a21\u578b\u5c42\u654f\u611f\u6027\u548c\u8bad\u7ec3\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TsqLoRA\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u662f\u8d28\u91cf\u611f\u77e5\u91c7\u6837\u673a\u5236\uff0c\u7528\u4e8e\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8bad\u7ec3\u6570\u636e\uff1b\u4e8c\u662f\u52a8\u6001\u79e9\u5206\u914d\u6a21\u5757\uff0c\u6839\u636e\u5404\u5c42\u5bf9\u53c2\u6570\u66f4\u65b0\u7684\u654f\u611f\u5ea6\u8c03\u6574\u5176\u79e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTsqLoRA\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5fae\u8c03\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "TsqLoRA\u901a\u8fc7\u4f18\u5316\u6570\u636e\u9009\u62e9\u548c\u79e9\u5206\u914d\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2509.18420", "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165IFEval-FC\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u9075\u5faa\u5d4c\u5165\u5f0f\u683c\u5f0f\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u9876\u5c16\u6a21\u578b\u4e5f\u5e38\u5728\u6b64\u65b9\u9762\u51fa\u9519\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\uff08\u5982BFCL\u3001tau^2-Bench\u3001ACEBench\uff09\u4e3b\u8981\u8bc4\u4f30\u53c2\u6570\u7684\u6b63\u786e\u6027\uff0c\u4f46\u672a\u80fd\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u9075\u5faa\u53c2\u6570\u63cf\u8ff0\u4e2d\u5d4c\u5165\u7684\u683c\u5f0f\u6307\u4ee4\uff08\u4f8b\u5982\u4f7f\u7528\u53cc\u5f15\u53f7\u6216ISO\u65e5\u671f\u683c\u5f0f\uff09\uff0c\u8fd9\u5728\u5927\u6a21\u578b\u9a71\u52a8\u7684AI\u4ee3\u7406\u4e2d\u662f\u4e00\u4e2a\u5b9e\u9645\u9650\u5236\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86IFEval-FC\u57fa\u51c6\uff0c\u7075\u611f\u6765\u6e90\u4e8eIFEval\u3002\u8be5\u57fa\u51c6\u5c06\u53ef\u9a8c\u8bc1\u7684\u683c\u5f0f\u8981\u6c42\u76f4\u63a5\u7f16\u7801\u5230JSON schema\u63cf\u8ff0\u4e2d\uff08\u4f8b\u5982\u6307\u5b9a\u503c\u4e0d\u80fd\u5305\u542b\u6807\u70b9\uff09\u3002\u5b83\u5305\u542b750\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6bcf\u4e2a\u7528\u4f8b\u90fd\u7531\u4e00\u4e2a\u51fd\u6570\u3001\u5176\u67d0\u4e2a\u8f93\u5165\u53c2\u6570\u7684\u5d4c\u5165\u683c\u5f0f\u4ee5\u53ca\u76f8\u5e94\u7684\u7528\u6237\u67e5\u8be2\u7ec4\u6210\u3002\u8bc4\u4f30\u8fc7\u7a0b\u5b8c\u5168\u7b97\u6cd5\u5316\uff0c\u4ee5\u786e\u4fdd\u5ba2\u89c2\u6027\u3001\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662fGPT-5\u548cClaude 4.1 Opus\u7b49\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\uff0c\u4e5f\u9891\u7e41\u672a\u80fd\u9075\u5faa\u57fa\u672c\u7684\u683c\u5f0f\u89c4\u5219\u3002", "conclusion": "\u8fd9\u63ed\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u4ee3\u7406\u7cfb\u7edf\u7684\u4e00\u4e2a\u5b9e\u9645\u5c40\u9650\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u5728\u7cbe\u786e\u9075\u5faa\u683c\u5f0f\u6307\u4ee4\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.18350", "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOrthoLoC\u5927\u578b\u6570\u636e\u96c6\u548cAdHoP\u7cbe\u70bc\u6280\u672f\uff0c\u65e8\u5728\u5229\u7528\u8f7b\u91cf\u7ea7\u6b63\u5c04\u5730\u7406\u6570\u636e\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u504f\u5dee\u95ee\u9898\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5339\u914d\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u5728\u591a\u79cd\u573a\u666f\u4e0b\u9700\u9ad8\u7cbe\u5ea6\u4e14\u8d44\u6e90\u53d7\u9650\uff08\u65e0\u7f51\u7edc/GPS\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u6570\u636e\u5e93\u6216\u91cd\u578b3D\u6a21\u578b\u4e0d\u5207\u5b9e\u9645\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8f7b\u91cf\u7ea7\u3001\u6613\u83b7\u53d6\u7684\u6b63\u5c04\u5730\u7406\u6570\u636e\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u5374\u9c9c\u6709\u5173\u6ce8\u3002", "method": "1. \u63d0\u51fa\u4e86OrthoLoC\uff0c\u9996\u4e2a\u5305\u542b16,425\u5f20\u591a\u6a21\u6001\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u56fe\u50cf\u4e0e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u95f4\u7684\u9886\u57df\u504f\u5dee\uff0c\u5e76\u652f\u6301\u72ec\u7acb\u8bc4\u4f30\u56fe\u50cf\u68c0\u7d22\u548c\u7279\u5f81\u5339\u914d\u30022. \u5f15\u5165\u4e86AdHoP\u7cbe\u70bc\u6280\u672f\uff0c\u53ef\u4e0e\u4efb\u4f55\u7279\u5f81\u5339\u914d\u5668\u96c6\u6210\u3002", "result": "1. \u901a\u8fc7\u5168\u9762\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u9886\u57df\u504f\u5dee\u3001\u6570\u636e\u5206\u8fa8\u7387\u548c\u5171\u89c6\u6027\u5bf9\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u30022. AdHoP\u6280\u672f\u53ef\u5c06\u5339\u914d\u6027\u80fd\u63d0\u9ad8\u9ad8\u8fbe95%\uff0c\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe63%\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efaOrthoLoC\u6570\u636e\u96c6\u548c\u5f00\u53d1AdHoP\u7cbe\u70bc\u6280\u672f\uff0c\u6709\u6548\u5229\u7528\u8f7b\u91cf\u7ea7\u6b63\u5c04\u5730\u7406\u6570\u636e\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u96be\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5339\u914d\u4e0e\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2509.18127", "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "\u9488\u5bf9LLM\u5b89\u5168\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u91caSAE\u7279\u5f81\uff0c\u7cfb\u7edf\u8bc6\u522b\u5e76\u9ad8\u6548\u89e3\u91caLLM\u4e2d\u7684\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u4ee5\u589e\u5f3a\u5bf9\u5b89\u5168\u673a\u5236\u7684\u7406\u89e3\u3002", "motivation": "LLM\u5927\u89c4\u6a21\u90e8\u7f72\u5e26\u6765\u4e25\u91cd\u5b89\u5168\u95ee\u9898\uff0c\u73b0\u6709\u5b89\u5168\u7814\u7a76\u548cSAE\u5e94\u7528\u672a\u80fd\u7ec6\u7c92\u5ea6\u89e3\u91ca\u5b89\u5168\u76f8\u5173\u884c\u4e3a\uff08\u5982\u751f\u6210\u6bd2\u6027\u54cd\u5e94\uff09\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5e7f\u6cdb\u3001\u672a\u5b9a\u4e49\u7684\u98ce\u9669\u3002\u4e3a\u8fdb\u884c\u4e25\u683c\u5b89\u5168\u5206\u6790\uff0c\u9700\u8981\u63d0\u53d6\u4e30\u5bcc\u591a\u6837\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u8bc6\u522b\u6700\u80fd\u751f\u6210\u5b89\u5168\u6982\u5ff5\u7279\u5b9a\u795e\u7ecf\u5143\u7684SAE\uff0c\u4ee5\u53ca\u9ad8\u6602\u7684\u7279\u5f81\u89e3\u91ca\u6210\u672c\u3002", "method": "\u672c\u6587\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91caLLM\u4e2d\u7684SAE\u7279\u5f81\uff0c\u4ee5\u63a8\u8fdb\u5b89\u5168\u9886\u57df\u7684\u673a\u68b0\u7406\u89e3\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u8bc6\u522b\u5177\u6709\u6700\u4f73\u6982\u5ff5\u7279\u5b9a\u53ef\u89e3\u91ca\u6027\u7684SAE\uff0c\u89e3\u91ca\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7b56\u7565\u4ee5\u6269\u5927\u89e3\u91ca\u8fc7\u7a0b\u7684\u89c4\u6a21\u3002", "result": "Safe-SAIL\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u8bc6\u522b\u51fa\u6700\u80fd\u751f\u6210\u5b89\u5168\u6982\u5ff5\u7279\u5b9a\u795e\u7ecf\u5143\u7684SAE\uff0c\u5e76\u6709\u6548\u5730\u89e3\u91ca\u8fd9\u4e9b\u5b89\u5168\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u540c\u65f6\u5f15\u5165\u9ad8\u6548\u7b56\u7565\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89e3\u91ca\u7684\u6210\u672c\u95ee\u9898\u3002\u7814\u7a76\u6210\u679c\u5c06\u53d1\u5e03\u5305\u542bSAE\u68c0\u67e5\u70b9\u548c\u4eba\u7c7b\u53ef\u8bfb\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u7efc\u5408\u5de5\u5177\u5305\u3002", "conclusion": "Safe-SAIL\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u89e3\u91caLLM\u4e2d\u7684SAE\u5b89\u5168\u7279\u5f81\uff0c\u589e\u5f3a\u4e86LLM\u5b89\u5168\u9886\u57df\u7684\u673a\u68b0\u7406\u89e3\u3002\u901a\u8fc7\u53d1\u5e03\u5de5\u5177\u5305\uff0cSafe-SAIL\u5c06\u652f\u6301\u5bf9\u5b89\u5168\u98ce\u9669\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5e76\u4fc3\u8fdbLLM\u5b89\u5168\u7814\u7a76\u3002"}}
{"id": "2509.18588", "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "categories": ["cs.CL"], "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance.", "AI": {"tldr": "UniECG\u662f\u9996\u4e2a\u7edf\u4e00\u7684ECG\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u57fa\u4e8e\u8bc1\u636e\u7684ECG\u89e3\u8bfb\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684ECG\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u901a\u7528\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u80fd\u529b\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\uff08\u5982GPT-5\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u548c\u751f\u6210ECG\u4fe1\u53f7\uff0c\u4e5f\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7684\u533b\u5b66\u8bca\u65ad\u3002", "method": "\u63d0\u51faUniECG\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002\u9996\u5148\u5b66\u4e60\u57fa\u4e8e\u8bc1\u636e\u7684ECG\u89e3\u8bfb\u6280\u80fd\uff08ECG-to-Text\uff09\uff0c\u7136\u540e\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u6ce8\u5165ECG\u751f\u6210\u80fd\u529b\uff08Text-to-ECG\uff09\u3002", "result": "UniECG\u80fd\u591f\u6839\u636e\u7528\u6237\u8f93\u5165\u81ea\u4e3b\u9009\u62e9\u8fdb\u884cECG\u89e3\u8bfb\u6216\u751f\u6210\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5f53\u524dECG\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "UniECG\u6210\u529f\u5730\u5c06ECG\u89e3\u8bfb\u548c\u751f\u6210\u80fd\u529b\u6574\u5408\u5230\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u4e2d\uff0c\u586b\u8865\u4e86\u73b0\u6709\u901a\u7528\u6a21\u578b\u5728ECG\u9886\u57df\u7684\u80fd\u529b\u7a7a\u767d\uff0c\u4e3aECG\u5206\u6790\u5e26\u6765\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.18436", "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1Memory-QA\uff0c\u5373\u4ece\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u56de\u7b54\u5173\u4e8e\u89c6\u89c9\u5185\u5bb9\u7684\u53ec\u56de\u95ee\u9898\u3002\u4e3a\u5e94\u5bf9\u5176\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Pensieve\u7ba1\u9053\uff0c\u5e76\u5728\u65b0\u521b\u5efa\u7684\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6848\u7684\u6027\u80fd\uff08\u95ee\u7b54\u51c6\u786e\u7387\u63d0\u9ad8\u8fbe14%\uff09\u3002", "motivation": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1Memory-QA\uff0c\u5373\u4ece\u5148\u524d\u5b58\u50a8\u7684\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u56de\u7b54\u5173\u4e8e\u89c6\u89c9\u5185\u5bb9\u7684\u53ec\u56de\u95ee\u9898\u3002\u8be5\u4efb\u52a1\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u521b\u5efa\u9762\u5411\u4efb\u52a1\u7684\u8bb0\u5fc6\u3001\u6709\u6548\u5229\u7528\u8bb0\u5fc6\u4e2d\u7684\u65f6\u95f4\u4e0e\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u53ca\u5229\u7528\u591a\u4e2a\u8bb0\u5fc6\u6765\u56de\u7b54\u53ec\u56de\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPensieve\u7684\u7efc\u5408\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u6574\u5408\u4e86\u8bb0\u5fc6\u7279\u5b9a\u589e\u5f3a\u3001\u65f6\u95f4\u611f\u77e5\u548c\u4f4d\u7f6e\u611f\u77e5\u7684\u591a\u4fe1\u53f7\u68c0\u7d22\uff0c\u4ee5\u53ca\u591a\u8bb0\u5fc6\u95ee\u7b54\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6765\u5c55\u793a\u4efb\u52a1\u4e2d\u7684\u5404\u79cd\u5b9e\u9645\u6311\u6218\u3002", "result": "\u901a\u8fc7\u521b\u5efa\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5c55\u793a\u4e86Pensieve\u7ba1\u9053\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5176\u95ee\u7b54\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u9ad8\u51fa\u9ad8\u8fbe14%\u3002", "conclusion": "Memory-QA\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u65b0\u4efb\u52a1\uff0c\u800cPensieve\u7ba1\u9053\u4f5c\u4e3a\u4e00\u79cd\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4ece\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u56de\u7b54\u53ec\u56de\u95ee\u9898\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.18354", "pdf": "https://arxiv.org/pdf/2509.18354", "abs": "https://arxiv.org/abs/2509.18354", "authors": ["Mehrdad Moradi", "Shengzhe Chen", "Hao Yan", "Kamran Paynabar"], "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "comment": "12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference", "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSSDnet\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u7684\u5355\u5e45\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u6216\u53c2\u8003\u6837\u672c\uff0c\u901a\u8fc7\u81ea\u91cd\u5efa\u7f51\u7edc\u5b9e\u73b0\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u6216\u53c2\u8003\u6837\u672c\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u5355\u5e45\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u95ee\u9898\u3002", "method": "\u53d7DIP\u542f\u53d1\uff0c\u63d0\u51faSSDnet\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\u3002\u6838\u5fc3\u601d\u60f3\u662f\u81ea\u7136\u56fe\u50cf\u901a\u5e38\u5177\u6709\u7edf\u4e00\u7684\u7eb9\u7406\u548c\u6a21\u5f0f\uff0c\u5f02\u5e38\u8868\u73b0\u4e3a\u5c40\u90e8\u504f\u5dee\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8865\u4e01\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u50cf\u9001\u5165\u7f51\u7edc\u8fdb\u884c\u81ea\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u3001\u8865\u4e01\u6df7\u6d17\u3001\u9ad8\u65af\u566a\u58f0\u4ee5\u53ca\u57fa\u4e8e\u5185\u79ef\u76f8\u4f3c\u5ea6\u7684\u611f\u77e5\u635f\u5931\u6765\u907f\u514d\u5b66\u4e60\u6052\u7b49\u6620\u5c04\u3002", "result": "SSDnet\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.99 AUROC\u548c0.60 AUPRC\uff0c\u5728\u7ec7\u7269\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.98 AUROC\u548c0.67 AUPRC\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SSDnet\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3001\u6807\u7b7e\u6216\u53c2\u8003\u7684\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u5355\u5e45\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18128", "pdf": "https://arxiv.org/pdf/2509.18128", "abs": "https://arxiv.org/abs/2509.18128", "authors": ["Amirreza Tootchi", "Xiaoping Du"], "title": "Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning surrogates are increasingly employed to replace expensive\ncomputational models for physics-based reliability analysis. However, their use\nintroduces epistemic uncertainty from model approximation errors, which couples\nwith aleatory uncertainty in model inputs, potentially compromising the\naccuracy of reliability predictions. This study proposes a Gauss-Hermite\nquadrature approach to decouple these nested uncertainties and enable more\naccurate reliability analysis. The method evaluates conditional failure\nprobabilities under aleatory uncertainty using First and Second Order\nReliability Methods and then integrates these probabilities across realizations\nof epistemic uncertainty. Three examples demonstrate that the proposed approach\nmaintains computational efficiency while yielding more trustworthy predictions\nthan traditional methods that ignore model uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u65af-\u57c3\u5c14\u7c73\u7279\u6c42\u79ef\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u8026\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u4e2d\u5d4c\u5957\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u53ef\u9760\u6027\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u5728\u7269\u7406\u53ef\u9760\u6027\u5206\u6790\u4e2d\u5f15\u5165\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0e\u8f93\u5165\u4e2d\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u8026\u5408\uff0c\u53ef\u80fd\u635f\u5bb3\u53ef\u9760\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af-\u57c3\u5c14\u7c73\u7279\u6c42\u79ef\u65b9\u6cd5\u3002\u9996\u5148\u4f7f\u7528\u4e00\u9636/\u4e8c\u9636\u53ef\u9760\u6027\u65b9\u6cd5\u8bc4\u4f30\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6761\u4ef6\u5931\u6548\u6982\u7387\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6982\u7387\u5bf9\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u79ef\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u5ffd\u7565\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u7684\u9884\u6d4b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u8026\u5d4c\u5957\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u7684\u53ef\u9760\u6027\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.18632", "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0cLLM\u8ba1\u5212\u7684\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u504f\u597d\uff0c\u4f46\u7528\u6237/\u6a21\u578b\u504f\u597d\u548c\u4ee3\u7406\u6210\u529f\u7387\u5e76\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u8ba1\u5212\u5bf9\u7528\u6237\u7684\u5b9e\u9645\u5e2e\u52a9\u6027\uff08\u4efb\u52a1\u6210\u529f\uff09\u3002\u8868\u9762\u7ebf\u7d22\u6613\u5f71\u54cd\u504f\u597d\u4f46\u4e0d\u9884\u6d4b\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u7684\u53cd\u9988\u6765\u5bf9\u9f50LLM\u3002", "motivation": "LLM\u751f\u6210\u7684\u8ba1\u5212\u65e8\u5728\u8f85\u52a9\u7528\u6237\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u76ee\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u504f\u597d\u8fdb\u884c\u8bad\u7ec3\u6216\u8bc4\u4f30\uff0c\u4f46\u672c\u6587\u8d28\u7591\u8fd9\u79cd\u504f\u597d\u662f\u5426\u771f\u6b63\u53cd\u6620\u4e86\u8ba1\u5212\u5bf9\u7528\u6237\u7684\u5b9e\u9645\u5e2e\u52a9\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3a\u201cPlanorama\u201d\u7684\u754c\u9762\uff0c\u62db\u52df126\u540d\u7528\u6237\u4f7f\u7528LLM\u8ba1\u5212\u56de\u7b54300\u4e2a\u591a\u6b65\u9aa4\u95ee\u9898\u3002\u6536\u96c6\u4e864388\u6b21\u8ba1\u5212\u6267\u884c\u548c5584\u6b21\u6bd4\u8f83\u6570\u636e\uff0c\u4ee5\u8861\u91cf\u8ba1\u5212\u7684\u5e2e\u52a9\u6027\uff08QA\u6210\u529f\u7387\uff09\u548c\u7528\u6237\u504f\u597d\u3002\u540c\u65f6\uff0c\u5728\u4ee3\u7406\u548c\u5956\u52b1\u6a21\u578b\u4e2d\u590d\u73b0\u6b64\u8bbe\u7f6e\uff0c\u89c2\u5bdf\u5b83\u4eec\u662f\u5426\u80fd\u6a21\u62df\u6216\u504f\u597d\u5bf9\u7528\u6237\u6709\u76ca\u7684\u8ba1\u5212\u3002", "result": "1) \u7528\u6237/\u6a21\u578b\u504f\u597d\u548c\u4ee3\u7406\u6210\u529f\u7387\u672a\u80fd\u51c6\u786e\u9884\u6d4b\u54ea\u4e9b\u8ba1\u5212\u771f\u6b63\u5e2e\u52a9\u4e86\u7528\u6237\uff0c\u8868\u660e\u5e38\u89c1\u7684\u5bf9\u9f50\u53cd\u9988\u53ef\u80fd\u4e0e\u5b9e\u9645\u5e2e\u52a9\u6027\u4e0d\u7b26\u30022) \u8fd9\u79cd\u5dee\u8ddd\u5e76\u975e\u7531\u7528\u6237\u7279\u5b9a\u504f\u597d\u5f15\u8d77\uff0c\u7528\u6237\u5728\u4f7f\u7528\u5176\u504f\u597d\u6216\u4e0d\u504f\u597d\u7684\u8ba1\u5212\u65f6\uff0c\u6210\u529f\u7387\u76f8\u4f3c\u30023) \u7b80\u6d01\u6027\u3001\u95ee\u9898\u76f8\u4f3c\u6027\u7b49\u8868\u9762\u7ebf\u7d22\u4e0e\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u8fd9\u4e9b\u504f\u89c1\u65e0\u6cd5\u9884\u6d4b\u8ba1\u5212\u7684\u5b9e\u9645\u5e2e\u52a9\u6027\u3002", "conclusion": "\u4e3a\u4e86\u5bf9\u9f50\u771f\u6b63\u6709\u5e2e\u52a9\u7684LLM\uff0c\u9700\u8981\u4ece\u771f\u5b9e\u7684\u7528\u6237\u4ea4\u4e92\uff08\u5373\u4efb\u52a1\u6210\u529f\uff09\u4e2d\u83b7\u53d6\u53cd\u9988\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u770b\u8d77\u6765\u6709\u5e2e\u52a9\u7684\u504f\u597d\u3002\u7814\u7a76\u547c\u5401NLP\u7814\u7a76\u8005\u5173\u6ce8\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2509.18527", "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FERA\uff0c\u4e00\u4e2aAI\u51fb\u5251\u88c1\u5224\u52a9\u624b\u539f\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u59ff\u6001\u8bc6\u522b\u548c\u89c4\u5219\u63a8\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u82b1\u5251\u5224\u7f5a\uff0c\u5e76\u5728\u6709\u9650\u6570\u636e\u4e0b\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u51fb\u5251\u8fd0\u52a8\u7684\u88c1\u5224\u5de5\u4f5c\u9762\u4e34\u4e3b\u89c2\u5224\u65ad\u3001\u4eba\u4e3a\u9519\u8bef\u3001\u504f\u89c1\u4ee5\u53ca\u5b9e\u8df5\u73af\u5883\u53ef\u7528\u6027\u6709\u9650\u7b49\u6311\u6218\uff0c\u4fc3\u4f7f\u5f00\u53d1\u81ea\u52a8\u5316\u88c1\u5224\u8f85\u52a9\u7cfb\u7edf\u3002", "method": "FERA\u6574\u5408\u4e86\u57fa\u4e8e\u59ff\u6001\u7684\u591a\u6807\u7b7e\u52a8\u4f5c\u8bc6\u522b\u548c\u89c4\u5219\u63a8\u7406\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u5e76\u6807\u51c6\u53162D\u5173\u8282\u4f4d\u7f6e\uff1b\u8ba1\u7b97101\u7ef4\u8fd0\u52a8\u5b66\u7279\u5f81\uff1b\u4f7f\u7528Transformer\u8fdb\u884c\u591a\u6807\u7b7e\u52a8\u4f5c\u548c\u5251\u7684\u5206\u7c7b\uff1b\u5e76\u5e94\u7528\u4e00\u4e2a\u5e26\u6709\u7f16\u7801\u4f18\u5148\u6743\u89c4\u5219\u7684\u84b8\u998f\u8bed\u8a00\u6a21\u578b\u6765\u51b3\u5b9a\u5f97\u5206\u5e76\u63d0\u4f9b\u89e3\u91ca\u3002", "result": "\u5728\u6709\u9650\u7684\u624b\u5de5\u6807\u6ce8\u6570\u636e\u4e0b\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0cFERA\u53d6\u5f97\u4e860.549\u7684\u5e73\u5747\u5b8f\u89c2F1\u5206\u6570\uff0c\u5e76\u4f18\u4e8e\u5305\u62ecTCN\u3001BiLSTM\u548c\u666e\u901aTransformer\u5728\u5185\u7684\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5c3d\u7ba1FERA\u5c1a\u672a\u8fbe\u5230\u90e8\u7f72\u9636\u6bb5\uff0c\u4f46\u5176\u7ed3\u679c\u4e3a\u82b1\u5251\u8fd0\u52a8\u4e2d\u7684\u81ea\u52a8\u5316\u88c1\u5224\u8f85\u52a9\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u9053\u8def\uff0c\u5e76\u4e3aAI\u5728\u51fb\u5251\u9886\u57df\u7684\u5e94\u7528\uff08\u5982\u6559\u7ec3\uff09\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u3002"}}
{"id": "2509.18369", "pdf": "https://arxiv.org/pdf/2509.18369", "abs": "https://arxiv.org/abs/2509.18369", "authors": ["Riad Ahmed Anonto", "Sardar Md. Saffat Zabin", "M. Saifur Rahman"], "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5b5f\u52a0\u62c9\u8bed\uff09\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a5\u5730\u56f0\u96be\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u611f\u77e5\u578b\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u7ed3\u5408\u5229\u7528\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u3001\u521b\u65b0\u6027\u4e09\u635f\u5931\u51fd\u6570\uff08PAL+InfoNCE+OT\uff09\u6765\u63d0\u5347\u6a21\u578b\u63a5\u5730\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u5e76\u7f29\u5c0f\u4e86\u771f\u5b9e-\u5408\u6210\u6570\u636e\u95f4\u7684\u5bf9\u9f50\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8fdb\u884c\u63a5\u5730\u4ecd\u7136\u5145\u6ee1\u6311\u6218\uff0c\u5e38\u51fa\u73b0\u751f\u6210\u6d41\u7545\u6587\u672c\u5374\u4e0e\u9519\u8bef\u5bf9\u8c61\u5339\u914d\u7684\u73b0\u8c61\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u3001\u7ffb\u8bd1\u67a2\u7ebd\u7834\u574f\u5bf9\u9f50\u4ee5\u53ca\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u9884\u8bad\u7ec3\u5ffd\u89c6\u76ee\u6807\u8bed\u8a00\u8bed\u4e49\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u611f\u77e5\u7684\u5b5f\u52a0\u62c9\u8bed\u5b57\u5e55\u751f\u6210\u6d41\u6c34\u7ebf\u3002\u8be5\u6d41\u6c34\u7ebf\u57fa\u4e8eLaBSE\u9a8c\u8bc1\u7684\u82f1-\u5b5f\u52a0\u62c9\u8bed\u5bf9\u548c110k\u53cc\u8bed\u63d0\u793a\u7684\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e00\u4e2a\u51bb\u7ed3\u7684MaxViT\uff08\u63d0\u4f9b\u7a33\u5b9a\u7684\u89c6\u89c9\u8865\u4e01\uff09\u3001\u4e00\u4e2a\u5b5f\u52a0\u62c9\u8bed\u539f\u751f\u7684mBART-50\uff08\u89e3\u7801\u5668\uff09\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u6001\u6865\u63a5\u6a21\u5757\u3002\u6838\u5fc3\u521b\u65b0\u662f\u91c7\u7528\u4e09\u635f\u5931\u76ee\u6807\uff1a\u8865\u4e01\u5bf9\u9f50\u635f\u5931\uff08PAL\uff09\u5229\u7528\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u9f50\u771f\u5b9e\u548c\u5408\u6210\u8865\u4e01\u63cf\u8ff0\u7b26\uff1bInfoNCE\u635f\u5931\u5f3a\u5236\u5b9e\u73b0\u5168\u5c40\u771f\u5b9e-\u5408\u6210\u5206\u79bb\uff1b\u57fa\u4e8eSinkhorn\u7684Optimal Transport (OT) \u786e\u4fdd\u7ec6\u7c92\u5ea6\u7684\u8865\u4e01\u5e73\u8861\u5bf9\u5e94\u3002", "result": "PAL+InfoNCE+OT\u7684\u534f\u540c\u4f5c\u7528\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u7684\u63a5\u5730\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u865a\u5047\u5339\u914d\u3002\u6a21\u578b\u5728Flickr30k-1k\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u63d0\u5347\uff08BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20\uff09\uff0c\u5728MSCOCO-1k\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff08BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40\uff09\uff0c\u5747\u4f18\u4e8e\u5f3a\u5927\u7684\u4ea4\u53c9\u71b5\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c06\u771f\u5b9e-\u5408\u6210\u6570\u636e\u8d28\u5fc3\u5dee\u8ddd\u7f29\u5c0f\u4e8641%\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u548c\u4e09\u635f\u5931\u51fd\u6570\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u63a5\u5730\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b5f\u52a0\u62c9\u8bed\u56fe\u50cf\u5b57\u5e55\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u95f4\u7684\u5bf9\u9f50\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18130", "pdf": "https://arxiv.org/pdf/2509.18130", "abs": "https://arxiv.org/abs/2509.18130", "authors": ["Zijie Zhou", "Huichen Ma"], "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408STL\u5206\u89e3\u548cGRU\u7684\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u6a21\u578b\uff08STL-GRU\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5730\u94c1\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u51c6\u786e\u7684\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u662f\u4f18\u5316\u8fd0\u8425\u8ba1\u5212\u3001\u63d0\u9ad8\u8fd0\u8f93\u6548\u7387\u7684\u5173\u952e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8fdb\u4e00\u6b65\u5b8c\u5584\u5730\u94c1\u5185\u90e8\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u7406\u8bba\uff0c\u4e3a\u667a\u80fd\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u652f\u6301\u3002", "method": "\u8be5\u6a21\u578b\u9996\u5148\u5229\u7528Keras\u6784\u5efa\u548c\u8bad\u7ec3GRU\u6a21\u578b\uff1b\u63a5\u7740\u5bf9\u5730\u94c1\u5237\u5361\u539f\u59cb\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\u8bc6\u522b\u4e58\u5ba2\u8def\u5f84\u5e76\u6784\u5efa\u6362\u4e58\u5ba2\u6d41\u65f6\u95f4\u5e8f\u5217\uff1b\u7136\u540e\u91c7\u7528STL\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u7b97\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5468\u671f\u548c\u6b8b\u5dee\u5206\u91cf\uff0c\u5e76\u4f7f\u75283\u03c3\u539f\u5219\u6d88\u9664\u548c\u586b\u5145\u6b8b\u5dee\u5206\u91cf\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u6700\u7ec8\u5b8c\u6210\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u3002\u6a21\u578b\u4ee5\u67d0\u5730\u94c1\u7ad9\u7684\u6362\u4e58\u5ba2\u6d41\u6570\u636e\u4e3a\u6837\u672c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e0eLSTM\u3001GRU\u53caSTL-LSTM\u6a21\u578b\u76f8\u6bd4\uff0cSTL-GRU\u7ec4\u5408\u9884\u6d4b\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4f5c\u65e5\uff08\u975e\u5468\u4e94\uff09\u3001\u5468\u4e94\u548c\u4f11\u606f\u65e5\u7684\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u7cbe\u5ea6\u3002\u9884\u6d4b\u7ed3\u679c\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u5206\u522b\u81f3\u5c11\u964d\u4f4e\u4e862.3\u30011.36\u548c6.42\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "STL-GRU\u7ec4\u5408\u9884\u6d4b\u6a21\u578b\u5728\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u6709\u6548\u6027\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u80fd\u4e3a\u5730\u94c1\u667a\u80fd\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2509.18655", "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE.", "AI": {"tldr": "\u9488\u5bf9\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u4e2d\u7684\u53c2\u6570\u4fdd\u6301\u77e5\u8bc6\u7f16\u8f91\uff08PPKE\uff09\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86CAPE-KG\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u4fddKG\u7684\u6784\u5efa\u3001\u66f4\u65b0\u548c\u68c0\u7d22\u4e0eMHQA\u4efb\u52a1\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86PPKE\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u53c2\u6570\u4fdd\u6301\u77e5\u8bc6\u7f16\u8f91\uff08PPKE\uff09\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u77e5\u8bc6\u6c61\u67d3\u3001\u66f4\u65b0\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u68c0\u7d22\u884c\u4e3a\u4e0e\u9884\u671f\u7f16\u8f91\u4e0d\u7b26\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86PPKE\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86CAPE-KG\uff08Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs\uff09\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u4e00\u81f4\u6027\u611f\u77e5\u7684PPKE\u6846\u67b6\u3002CAPE-KG\u65e8\u5728\u786e\u4fdd\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u3001\u66f4\u65b0\u548c\u68c0\u7d22\u8fc7\u7a0b\u59cb\u7ec8\u4e0eMHQA\u4efb\u52a1\u7684\u8981\u6c42\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u672a\u7f16\u8f91\u548c\u5df2\u7f16\u8f91\u7684\u77e5\u8bc6\u4e0a\u90fd\u80fd\u4fdd\u6301\u8fde\u8d2f\u7684\u63a8\u7406\u3002", "result": "\u5728MQuAKE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCAPE-KG\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2dPPKE\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u89e3\u51b3PPKE\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u5bf9\u4e8e\u63d0\u9ad8MHQA\u4efb\u52a1\u7684PPKE\u6027\u80fd\u662f\u6709\u6548\u7684\uff0c\u672c\u6587\u63d0\u51fa\u7684CAPE-KG\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18557", "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "categories": ["cs.AI"], "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "\u63d0\u51faLLMZ+\uff0c\u901a\u8fc7\u63d0\u793a\u8bcd\u767d\u540d\u5355\u673a\u5236\u5f3a\u5316\u4ee3\u7406\u578bAI\u5b89\u5168\u6027\uff0c\u6709\u6548\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\u5e76\u4fdd\u6301\u5408\u6cd5\u901a\u4fe1\u7545\u901a\u3002", "motivation": "\u4ee3\u7406\u578bAI\u56e0\u5176\u5bf9\u6570\u636e\u6e90\u548cAPI\u5de5\u5177\u7684\u7279\u6743\u8bbf\u95ee\u4ee5\u53ca\u975e\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u6210\u4e3a\u9ad8\u4ef7\u503c\u653b\u51fb\u76ee\u6807\uff0c\u5e26\u6765\u663e\u8457\u64cd\u4f5c\u548c\u4fe1\u606f\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u57fa\u4e8e\u68c0\u6d4b\u7684\u9632\u5fa1\u673a\u5236\uff08\u5982\u9632\u6b62\u63d0\u793a\u6ce8\u5165\uff09\u9762\u4e34\u5c40\u9650\u3002", "method": "\u5f15\u5165LLMZ+\u65b9\u6cd5\uff0c\u91c7\u7528\u63d0\u793a\u8bcd\u767d\u540d\u5355\u673a\u5236\u800c\u975e\u4f20\u7edf\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u53ea\u5141\u8bb8\u7b26\u5408\u4e0a\u4e0b\u6587\u4e14\u5b89\u5168\u7684\u8bf7\u6c42\u4e0e\u4ee3\u7406\u578bLLM\u4ea4\u4e92\uff0c\u786e\u4fdd\u6240\u6709\u5916\u90e8\u7528\u6237\u4e0eLLM\u7684\u4ea4\u6d41\u90fd\u7b26\u5408\u9884\u5b9a\u4e49\u7684\u7528\u4f8b\u548c\u64cd\u4f5c\u8fb9\u754c\u3002", "result": "LLMZ+\u5bf9\u6700\u5e38\u89c1\u7684\u8d8a\u72f1\u63d0\u793a\u8bcd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u4e2d\u65ad\u5408\u6cd5\u7684\u4e1a\u52a1\u901a\u4fe1\uff0c\u6388\u6743\u6d41\u91cf\u80fd\u65e0\u7f1d\u6d41\u52a8\u3002\u5728\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0c\u8bef\u62a5\u7387\u548c\u6f0f\u62a5\u7387\u5747\u53ef\u964d\u81f30\u3002", "conclusion": "LLMZ+\u901a\u8fc7\u63d0\u793a\u8bcd\u767d\u540d\u5355\u673a\u5236\uff0c\u4e3a\u4ee3\u7406\u578bLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6d41\u7ebf\u578b\u3001\u957f\u671f\u5f39\u6027\u5f3a\u4e14\u8d44\u6e90\u9700\u6c42\u4f4e\u7684\u5168\u65b0\u5b89\u5168\u6846\u67b6\uff0c\u6709\u6548\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u969c\u6b63\u5e38\u4e1a\u52a1\u901a\u4fe1\u4e0d\u53d7\u5f71\u54cd\u3002"}}
{"id": "2509.18372", "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "categories": ["cs.CV"], "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "AI": {"tldr": "TinyBEV\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7eaf\u89c6\u89c9\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u5927\u578b\u611f\u77e5-\u89c4\u5212\u6a21\u578b\uff08UniAD\uff09\u7684\u5168\u6808\u80fd\u529b\u84b8\u998f\u5230\u4e00\u4e2a\u7d27\u51d1\u3001\u5b9e\u65f6\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u81ea\u52a8\u9a7e\u9a76\u6240\u9700\u7684\u5404\u9879\u529f\u80fd\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u6808\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff08\u5982UniAD\uff09\u53c2\u6570\u91cf\u5927\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u5927\u578b\u591a\u6a21\u6001\u611f\u77e5-\u89c4\u5212\u6a21\u578b\u4e0e\u90e8\u7f72\u5c31\u7eea\u7684\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u5168\u6808\u9a7e\u9a76\u667a\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TinyBEV\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u7eaf\u89c6\u89c9BEV\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u591a\u9636\u6bb5\u84b8\u998f\u7b56\u7565\uff0c\u5c06\u5927\u578b\u89c4\u5212\u5bfc\u5411\u6559\u5e08\u6a21\u578b\uff08UniAD\uff09\u7684\u5168\u6808\u80fd\u529b\uff08\u5305\u62ec3D\u68c0\u6d4b\u3001\u9ad8\u6e05\u5730\u56fe\u5206\u5272\u3001\u8fd0\u52a8\u9884\u6d4b\u3001\u5360\u7528\u7387\u9884\u6d4b\u548c\u76ee\u6807\u5bfc\u5411\u89c4\u5212\uff09\u8f6c\u79fb\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7BEV\u8868\u793a\u4e2d\u3002\u8be5\u84b8\u998f\u7b56\u7565\u7ed3\u5408\u4e86\u7279\u5f81\u7ea7\u3001\u8f93\u51fa\u7ea7\u548c\u81ea\u9002\u5e94\u533a\u57df\u611f\u77e5\u76d1\u7763\u3002", "result": "TinyBEV\u5c06\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e8678%\uff08\u4ec528M\u53c2\u6570\uff09\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb5\u500d\uff0811 FPS\uff09\uff0c\u4e14\u4ec5\u9700\u6444\u50cf\u5934\u8f93\u5165\u3002\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u8fbe\u523039.0 mAP\uff0c\u8fd0\u52a8\u9884\u6d4b\u8fbe\u52301.08 minADE\uff0c\u78b0\u649e\u73870.32\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5168\u6808\u9a7e\u9a76\u667a\u80fd\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5f97\u5230\u4fdd\u7559\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4e5f\u80fd\u4fdd\u7559\u5168\u6808\u9a7e\u9a76\u667a\u80fd\uff0c\u6210\u529f\u5f25\u5408\u4e86\u5927\u578b\u591a\u6a21\u6001\u611f\u77e5-\u89c4\u5212\u6a21\u578b\u4e0e\u53ef\u90e8\u7f72\u7684\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.18131", "pdf": "https://arxiv.org/pdf/2509.18131", "abs": "https://arxiv.org/abs/2509.18131", "authors": ["Jean-Michel Tucny", "Abhisek Ganguly", "Santosh Ansumali", "Sauro Succi"], "title": "Two ways to knowledge?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight.", "AI": {"tldr": "Transformer\u6a21\u578b\u5728\u7269\u7406\u5e94\u7528\u4e2d\uff0c\u5176\u6743\u91cd\u77e9\u9635\u5448\u73b0\u968f\u673a\u6027\uff0c\u4e0e\u7269\u7406\u7ed3\u6784\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u8fd9\u8868\u660e\u673a\u5668\u5b66\u4e60\u4e0e\u79d1\u5b66\u65b9\u6cd5\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u77e5\u8bc6\u83b7\u53d6\u8def\u5f84\uff0c\u4e14\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u6311\u6218\u3002", "motivation": "\u63a2\u7a76Transformer\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u65f6\uff0c\u5176\u5185\u90e8\u6743\u91cd\u77e9\u9635\u7684\u7279\u6027\u4ee5\u53ca\u4e0e\u7269\u7406\u7ed3\u6784\u7684\u5173\u8054\uff0c\u4ece\u800c\u7406\u89e3\u673a\u5668\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u77e5\u8bc6\u83b7\u53d6\u65b9\u5f0f\u4e0e\u4f20\u7edf\u79d1\u5b66\u65b9\u6cd5\u7684\u5f02\u540c\u3002", "method": "\u5206\u6790Transformer\u6a21\u578b\u5e94\u7528\u4e8e\u4e24\u4e2a\u4ee3\u8868\u6027\u7269\u7406\u95ee\u9898\u65f6\u7684\u6743\u91cd\u77e9\u9635\u7279\u5f81\uff1b\u5c06Transformer\u64cd\u4f5c\u4e0e\uff08\u5e7f\u4e49\uff09\u8def\u5f84\u79ef\u5206\u6280\u672f\u8fdb\u884c\u7c7b\u6bd4\u3002", "result": "Transformer\u7684\u6743\u91cd\u77e9\u9635\u8868\u73b0\u51fa\u968f\u673a\u6027\uff0c\u4e0e\u6240\u7814\u7a76\u7269\u7406\u95ee\u9898\u7684\u7269\u7406\u548c\u6570\u5b66\u7ed3\u6784\u65e0\u76f4\u63a5\u53ef\u8bc6\u522b\u7684\u8054\u7cfb\uff1b\u5c06Transformer\u64cd\u4f5c\u4e0e\u8def\u5f84\u79ef\u5206\u8fdb\u884c\u7c7b\u6bd4\u53ef\u80fd\u89e3\u91ca\u4e86\u6743\u91cd\u7684\u968f\u673a\u6027\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e0e\u79d1\u5b66\u65b9\u6cd5\u53ef\u80fd\u662f\u83b7\u53d6\u77e5\u8bc6\u7684\u4e24\u79cd\u4e0d\u540c\u4f46\u4e92\u8865\u7684\u9014\u5f84\uff0c\u7136\u800c\uff0c\u7f51\u7edc\u53c2\u6570\u4e0e\u7269\u7406\u7ed3\u6784\u76f4\u63a5\u5bf9\u5e94\u7684\u4e25\u683c\u53ef\u89e3\u91ca\u6027\u96be\u4ee5\u5b9e\u73b0\uff1b\u5728\u7f3a\u4e4f\u6df1\u523b\u6d1e\u5bdf\u529b\u7684\u60c5\u51b5\u4e0b\u83b7\u53d6\u77e5\u8bc6\u5b58\u5728\u56fa\u6709\u98ce\u9669\u3002"}}
{"id": "2509.18658", "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "categories": ["cs.CL"], "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment.", "AI": {"tldr": "\u9488\u5bf9LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff08LLM-as-a-judge\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u63d0\u4f9bLLM\u8bc4\u5206\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "LLM\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u7684\u8bc4\u4f30\u5668\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u5f0f\uff0c\u4f46\u5176\u8bc4\u4f30\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8fd9\u79cd\u53ef\u9760\u6027\u4e0d\u8db3\u9650\u5236\u4e86LLM\u8bc4\u4f30\u5668\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9996\u6b21\u5229\u7528\u5171\u5f62\u9884\u6d4b\uff08conformal prediction\uff09\u4e3aLLM\u8bc4\u5206\u63d0\u4f9b\u9884\u6d4b\u533a\u95f4\u7684\u6846\u67b6\uff0c\u4ee5\u5206\u6790\u5176\u4e0d\u786e\u5b9a\u6027\u3002\u4e3a\u79bb\u6563\u8bc4\u5206\u4efb\u52a1\u8bbe\u8ba1\u4e86\u5e8f\u6570\u8fb9\u754c\u8c03\u6574\uff0c\u5e76\u5efa\u8bae\u5c06\u533a\u95f4\u4e2d\u70b9\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u504f\u5dee\u7684\u66ff\u4ee3\u5206\u6570\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u5171\u5f62\u9884\u6d4b\u80fd\u591f\u63d0\u4f9b\u5177\u6709\u8986\u76d6\u4fdd\u8bc1\u7684\u6709\u6548\u9884\u6d4b\u533a\u95f4\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u533a\u95f4\u4e2d\u70b9\u548c\u88c1\u5224\u91cd\u65b0\u63d0\u793a\uff08judge reprompting\uff09\u5bf9\u4e8e\u6539\u5584\u5224\u65ad\u7684\u6709\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5f15\u5165\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u8bc4\u4f30\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86LLM\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18565", "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "categories": ["cs.AI"], "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LLM\u65b9\u7a0b\u751f\u6210\u3001\u5916\u90e8\u6c42\u89e3\u5668\u8ba1\u7b97\u548cLLM\u4f30\u7b97\u9a8c\u8bc1\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\uff08MWPs\uff09\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6570\u503c\u3001\u4ee3\u6570\u548c\u4e09\u89d2MWPs\u4e0a\u53d6\u5f97\u4e86SOTA\u6216\u6ee1\u610f\u7ed3\u679c\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\uff08MWPs\uff09\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u548c\u6570\u5b66\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u9996\u5148\uff0cLLM\u6839\u636e\u95ee\u9898\u5206\u89e3\u751f\u6210\u6570\u5b66\u65b9\u7a0b\uff1b\u5176\u6b21\uff0c\u4f7f\u7528\u5916\u90e8\u7b26\u53f7\u65b9\u7a0b\u6c42\u89e3\u5668\u5f97\u51fa\u7cbe\u786e\u7b54\u6848\u3002\u4e3a\u786e\u4fdd\u7b54\u6848\u51c6\u786e\u6027\uff0cLLM\u4f1a\u7b2c\u4e8c\u6b21\u4f30\u7b97\u7b54\u6848\u8fdb\u884c\u9a8c\u8bc1\uff1b\u82e5\u9a8c\u8bc1\u5931\u8d25\uff0c\u5219\u91c7\u7528\u8fed\u4ee3\u7ea0\u6b63\u8fc7\u7a0b\uff0c\u76f4\u81f3\u627e\u5230\u6b63\u786e\u7b54\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u548c\u4ee3\u6570MWP\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\uff0c\u5e73\u5747\u63d0\u5347\u8fd12%\uff1b\u9996\u6b21\u5728\u4e09\u89d2MWP\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e86\u6ee1\u610f\u7ed3\u679c\uff1b\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u6570\u636e\u96c6SVAMPClean\u548cTrig300\u3002", "conclusion": "\u7ed3\u5408LLM\u7684\u65b9\u7a0b\u751f\u6210\u3001\u5916\u90e8\u6c42\u89e3\u5668\u7684\u7cbe\u786e\u8ba1\u7b97\u4ee5\u53caLLM\u7684\u4f30\u7b97\u9a8c\u8bc1\u4e0e\u7ea0\u6b63\u673a\u5236\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u89e3\u51b3\u590d\u6742\u6570\u5b66\u5e94\u7528\u9898\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879MWP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u8868\u73b0\uff0c\u540c\u65f6\u62d3\u5c55\u4e86\u5176\u89e3\u51b3\u4e09\u89d2MWP\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18387", "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "AI": {"tldr": "\u9488\u5bf9\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f4\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u68c0\u6d4b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u7684\u4ee5\u6a21\u7cca\u4e2d\u5fc3\u6807\u8bb0\u7403\u4f53\u5e76\u6ce8\u91ca\u6a21\u7cca\u5c5e\u6027\u7684\u6807\u6ce8\u7b56\u7565\uff0c\u5e76\u53d1\u5e03\u65b0\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faBlurBall\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5e27\u8f93\u5165\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0SOTA\u7403\u4f53\u68c0\u6d4b\uff0c\u5e76\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u8fd0\u52a8\u6a21\u7cca\u964d\u4f4e\u5feb\u901f\u79fb\u52a8\u7269\u4f53\uff08\u5c24\u5176\u7403\u7c7b\u8fd0\u52a8\u4e2d\u7684\u7403\uff09\u7684\u6e05\u6670\u5ea6\uff0c\u5bfc\u81f4\u68c0\u6d4b\u56f0\u96be\u3002\u73b0\u6709\u6807\u6ce8\u65b9\u6cd5\u5c06\u7403\u4f53\u6807\u8bb0\u5728\u6a21\u7cca\u524d\u7aef\uff0c\u5f15\u5165\u4e0d\u5bf9\u79f0\u6027\uff0c\u5e76\u5ffd\u7565\u4e0e\u901f\u5ea6\u76f8\u5173\u7684\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u3002", "method": "1. \u63d0\u51fa\u65b0\u7684\u6807\u6ce8\u7b56\u7565\uff1a\u5c06\u7403\u4f53\u6807\u8bb0\u5728\u6a21\u7cca\u6761\u7eb9\u7684\u4e2d\u5fc3\uff0c\u5e76\u660e\u786e\u6807\u6ce8\u6a21\u7cca\u5c5e\u6027\u3002 2. \u57fa\u4e8e\u6b64\u7b56\u7565\uff0c\u53d1\u5e03\u4e00\u4e2a\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\u3002 3. \u5f15\u5165BlurBall\u6a21\u578b\uff1a\u8054\u5408\u4f30\u8ba1\u7403\u4f53\u4f4d\u7f6e\u548c\u8fd0\u52a8\u6a21\u7cca\u5c5e\u6027\u3002 4. \u6a21\u578b\u7ed3\u5408\u591a\u5e27\u8f93\u5165\u548cSqueeze-and-Excitation\u7b49\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "1. \u65b0\u7684\u6807\u6ce8\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u4e0d\u540c\u6a21\u578b\u7684\u68c0\u6d4b\u6027\u80fd\u3002 2. BlurBall\u6a21\u578b\u5728\u7403\u4f53\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\uff08State-of-the-Art\uff09\u7ed3\u679c\u3002 3. \u5229\u7528\u6a21\u7cca\u4fe1\u606f\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u4f7f\u8f68\u8ff9\u9884\u6d4b\u66f4\u53ef\u9760\u3002", "conclusion": "\u5de7\u5999\u5229\u7528\u8fd0\u52a8\u6a21\u7cca\u4fe1\u606f\uff0c\u4e0d\u4ec5\u80fd\u663e\u8457\u63d0\u9ad8\u7403\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u5bf9\u5b9e\u65f6\u4f53\u80b2\u5206\u6790\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18133", "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical.", "AI": {"tldr": "MoE-CL\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027MoE\u6846\u67b6\uff0c\u7528\u4e8eLLM\u7684\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u53cc\u4e13\u5bb6\u8bbe\u8ba1\u548c\u5bf9\u6297\u5b66\u4e60\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u7559\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u5e73\u8861\uff0c\u5e76\u5728\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u6301\u7eed\u5b66\u4e60\u4ee5\u9002\u5e94\u591a\u6837\u5316\u548c\u4e0d\u65ad\u6f14\u8fdb\u7684\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u56de\u653e\u548c\u53c2\u6570\u9694\u79bb\uff09\u5e38\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u65b0\u4efb\u52a1\u8bad\u7ec3\u4f1a\u56e0\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u51cf\u5f31\u800c\u635f\u5bb3\u65e7\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faMoE-CL\uff0c\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u7ea7LLM\u7684\u81ea\u8fdb\u5316\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u3002\u5b83\u91c7\u7528\u53cc\u4e13\u5bb6\u8bbe\u8ba1\uff1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u4e00\u4e2a\u4e13\u7528LoRA\u4e13\u5bb6\u4ee5\u4fdd\u7559\u4efb\u52a1\u77e5\u8bc6\u5e76\u7f13\u89e3\u9057\u5fd8\uff1b\u540c\u65f6\u4f7f\u7528\u4e00\u4e2a\u5171\u4eabLoRA\u4e13\u5bb6\u4ee5\u5b9e\u73b0\u8de8\u4efb\u52a1\u8fc1\u79fb\u3002\u4e3a\u9632\u6b62\u5171\u4eab\u4e13\u5bb6\u4f20\u9012\u65e0\u5173\u566a\u58f0\uff0c\u6846\u67b6\u5185\u96c6\u6210\u4e86\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u5224\u522b\u5668\uff08GAN\u7684\u4e00\u90e8\u5206\uff09\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u786e\u4fdd\u5171\u4eab\u4e13\u5bb6\u4ec5\u4f20\u9012\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4ece\u800c\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u4e0e\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "result": "\u5728\u516c\u5171MTL5\u548c\u5de5\u4e1a\u7ea7Tencent3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86MoE-CL\u7684\u6709\u6548\u6027\u3002\u5728\u817e\u8baf\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u5408\u89c4\u5ba1\u67e5\u7684\u771f\u5b9eA/B\u6d4b\u8bd5\u4e2d\uff0cMoE-CL\u6210\u529f\u5c06\u4eba\u5de5\u5ba1\u67e5\u6210\u672c\u964d\u4f4e\u4e8615.3%\u3002", "conclusion": "MoE-CL\u5728\u9700\u8981\u6301\u7eed\u9002\u5e94\u548c\u7a33\u5b9a\u8fc1\u79fb\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u90e8\u7f72\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18713", "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMemOrb\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u53e3\u5934\u5f3a\u5316\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u5c06\u591a\u8f6e\u4ea4\u4e92\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u7b56\u7565\u53cd\u601d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8eLLM\u7684\u5ba2\u670d\u4ee3\u7406\u7684\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u5ba2\u670d\u4e2d\u5e38\u51fa\u73b0\u8de8\u4f1a\u8bdd\u9057\u5fd8\u3001\u91cd\u590d\u9519\u8bef\u53ca\u7f3a\u4e4f\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86MemOrb\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u53e3\u5934\u5f3a\u5316\u8bb0\u5fc6\u5c42\u3002\u5b83\u5c06\u591a\u8f6e\u4ea4\u4e92\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u7b56\u7565\u53cd\u601d\uff0c\u5b58\u50a8\u5728\u5171\u4eab\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u5e76\u5728\u51b3\u7b56\u65f6\u68c0\u7d22\u4ee5\u63d0\u4f9b\u6307\u5bfc\uff0c\u65e0\u9700\u5fae\u8c03\u3002\u901a\u8fc7\u4efb\u52a1\u6210\u529f\u7387\u548c\u4e00\u81f4\u6027\u6307\u6807\uff08\u5982Pass^k\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MemOrb\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff08\u591a\u8f6e\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe63\u4e2a\u767e\u5206\u70b9\uff09\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u91cd\u590d\u8bd5\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u662f\u589e\u5f3a\u5ba2\u670d\u573a\u666f\u4e0b\u201c\u51bb\u7ed3\u201dLLM\u4ee3\u7406\u957f\u671f\u53ef\u9760\u6027\u7684\u5f3a\u5927\u673a\u5236\u3002"}}
{"id": "2509.18633", "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "categories": ["cs.AI", "q-fin.RM"], "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u6c14\u5019\u707e\u5bb3\u6570\u636e\u4e0e\u7ecf\u6d4e\u4ee3\u7406\u7684\u8fdb\u5316\u5b66\u4e60\uff0c\u4ee5\u8bc4\u4f30\u6c14\u5019\u98ce\u9669\u5bf9\u7ecf\u6d4e\u7cfb\u7edf\u7684\u5f71\u54cd\u53ca\u9002\u5e94\u7b56\u7565\u3002", "motivation": "\u6c14\u5019\u98ce\u9669\u8bc4\u4f30\u9700\u8981\u5bf9\u7a7a\u95f4\u5f02\u8d28\u6027\u707e\u5bb3\u548c\u9002\u5e94\u6027\u7ecf\u6d4e\u7cfb\u7edf\u4e4b\u95f4\u7684\u590d\u6742\u4e92\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u7406\u89e3\u548c\u91cf\u5316\u5176\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u57fa\u4e8eMesa\u7684\u7a7a\u95f4\u5efa\u6a21\u4e0eCLIMADA\u6c14\u5019\u5f71\u54cd\u8bc4\u4f30\u76f8\u7ed3\u5408\u3002\u5b83\u5f15\u5165\u4e86\u9002\u5e94\u6027\u5b66\u4e60\u884c\u4e3a\uff0c\u5141\u8bb8\u4f01\u4e1a\u901a\u8fc7\u57fa\u4e8e\u9002\u5e94\u5ea6\u548c\u7a81\u53d8\u7684\u9009\u62e9\uff0c\u8fdb\u5316\u9884\u7b97\u5206\u914d\u3001\u5b9a\u4ef7\u3001\u5de5\u8d44\u548c\u98ce\u9669\u9002\u5e94\u7b56\u7565\u3002", "result": "\u6846\u67b6\u5728RCP8.5\u60c5\u666f\u4e0b\u6a21\u62df\u4e86\u6cb3\u6d41\u6d2a\u6c34\u52302100\u5e74\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8fdb\u5316\u9002\u5e94\u4f7f\u4f01\u4e1a\u5728\u7ecf\u5386\u6570\u5341\u5e74\u7684\u6c14\u5019\u538b\u529b\u5e72\u6270\u540e\uff0c\u80fd\u591f\u6062\u590d\u5230\u57fa\u7ebf\uff08\u65e0\u707e\u5bb3\uff09\u751f\u4ea7\u6c34\u5e73\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u5373\u4f7f\u672a\u76f4\u63a5\u66b4\u9732\u4e8e\u6d2a\u6c34\u7684\u4ee3\u7406\u4e5f\u4f1a\u56e0\u4f9b\u5e94\u94fe\u4e2d\u65ad\u800c\u53d7\u5f71\u54cd\uff0c\u5230\u672c\u4e16\u7eaa\u672b\uff0cRCP8.5\u60c5\u666f\u4e0b\u7684\u5546\u54c1\u5e73\u5747\u4ef7\u683c\u6bd4\u57fa\u7ebf\u9ad85.6%\u3002", "conclusion": "\u8be5\u5f00\u6e90\u6846\u67b6\u4e3a\u91d1\u878d\u673a\u6784\u548c\u516c\u53f8\u63d0\u4f9b\u4e86\u91cf\u5316\u76f4\u63a5\u548c\u7ea7\u8054\u6c14\u5019\u98ce\u9669\uff0c\u5e76\u8bc4\u4f30\u7ecf\u6d4e\u6709\u6548\u9002\u5e94\u7b56\u7565\u7684\u5de5\u5177\u3002"}}
{"id": "2509.18388", "pdf": "https://arxiv.org/pdf/2509.18388", "abs": "https://arxiv.org/abs/2509.18388", "authors": ["Binhua Huang", "Ni Wang", "Wendong Yao", "Soumyabrata Dev"], "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection", "categories": ["cs.CV"], "comment": "5 pages, 1 figure", "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMVP\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u5728\u5173\u952e\u5e27\u4e0a\u8fd0\u884c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff08OWLv2\uff09\u5e76\u4f7f\u7528\u538b\u7f29\u57df\u8fd0\u52a8\u5411\u91cf\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u64ad\u5230\u4e2d\u95f4\u5e27\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u4e2d\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u7684\u6548\u7387\u3002", "motivation": "\u5728\u89c6\u9891\u7684\u6bcf\u4e00\u5e27\u4e0a\u8fd0\u884c\u5927\u578b\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u867d\u7136\u51c6\u786e\uff0c\u4f46\u6210\u672c\u9ad8\u6602\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u7ba1\u9053\uff0c\u4ec5\u5728\u56fa\u5b9a\u95f4\u9694\u7684\u5173\u952e\u5e27\u4e0a\u8c03\u7528OWLv2\u3002\u901a\u8fc7\u4f7f\u7528\u538b\u7f29\u57df\u8fd0\u52a8\u5411\u91cf\uff08MV\uff09\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u64ad\u5230\u4e2d\u95f4\u5e27\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528\u7b80\u5355\u76843x3\u7f51\u683c\u805a\u5408\u8fd0\u52a8\u5411\u91cf\u8fdb\u884c\u5e73\u79fb\u548c\u5747\u5300\u7f29\u653e\u66f4\u65b0\uff0c\u5e76\u8f85\u4ee5\u533a\u57df\u589e\u957f\u68c0\u67e5\u548c\u53ef\u9009\u7684\u5355\u7c7b\u522b\u5207\u6362\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u7b7e\u3001\u65e0\u9700\u5fae\u8c03\uff0c\u5e76\u5bf9\u6240\u6709\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u4f7f\u7528\u76f8\u540c\u7684\u63d0\u793a\u5217\u8868\u3002", "result": "\u5728ILSVRC2015-VID\uff08\u9a8c\u8bc1\u6570\u636e\u96c6\uff09\u4e0a\uff0cMVP\u8fbe\u5230\u4e86mAP@0.5=0.609\u548cmAP@[0.5:0.95]=0.316\u3002\u5728\u5bbd\u677e\u7684IoU\u9608\u503c\u4e0b\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u9010\u5e27OWLv2-Large\u3002\u5728\u76f8\u540c\u7684\u5173\u952e\u5e27\u8c03\u5ea6\u4e0b\uff0cMVP\u5728mAP@0.5\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u8ddf\u8e2a\u5668\u7684\u65b9\u6cd5\uff08MOSSE, KCF, CSRT\uff09\u3002\u867d\u7136\u6709\u76d1\u7763\u7684YOLOv12x\u5728mAP@0.5\u4e0a\u8fbe\u52300.631\uff0c\u4f46MVP\u662f\u514d\u6807\u7b7e\u548c\u5f00\u653e\u8bcd\u6c47\u7684\u3002", "conclusion": "\u538b\u7f29\u57df\u4f20\u64ad\u662f\u51cf\u5c11\u68c0\u6d4b\u5668\u8c03\u7528\u6b21\u6570\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u4e2d\u5f3a\u5927\u96f6\u6837\u672c\u8986\u76d6\u7684\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2509.18134", "pdf": "https://arxiv.org/pdf/2509.18134", "abs": "https://arxiv.org/abs/2509.18134", "authors": ["Furan Xie", "Bing Liu", "Li Chai"], "title": "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u4ee5\u6d88\u9664\u68af\u5ea6\u8ddf\u8e2a\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u5176\u7cbe\u786e\u6536\u655b\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u4fdd\u62a4\u4ee3\u7406\u7684\u79c1\u6709\u4fe1\u606f\u514d\u53d7\u6f5c\u5728\u653b\u51fb\u8005\u7a83\u53d6\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5148\u8fdb\u7684\u68af\u5ea6\u8ddf\u8e2a\u6280\u672f\u5b58\u5728\u56fa\u6709\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u9996\u5148\u63ed\u793a\u4e86\u68af\u5ea6\u8ddf\u8e2a\u56fa\u6709\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8870\u51cf\u6743\u91cd\u56e0\u5b50\u6d88\u9664\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u968f\u540e\uff0c\u8868\u5f81\u4e86\u7b97\u6cd5\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u4e0b\u7684\u6536\u655b\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u80fd\u7cbe\u786e\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002", "result": "\u63ed\u793a\u4e86\u68af\u5ea6\u8ddf\u8e2a\u7684\u56fa\u6709\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u6240\u63d0\u51fa\u7684\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u6210\u529f\u6d88\u9664\u4e86\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5176\u80fd\u7cbe\u786e\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002\u6570\u503c\u6a21\u62df\uff08\u5206\u5e03\u5f0f\u4f30\u8ba1\u95ee\u9898\u548c\u5206\u5e03\u5f0f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff09\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u68af\u5ea6\u8ddf\u8e2a\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u7b97\u6cd5\u7684\u7cbe\u786e\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.18722", "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field.", "AI": {"tldr": "LOTUSDIS\u662f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6cf0\u8bed\u8fdc\u573a\u4f1a\u8bae\u8bed\u6599\u5e93\uff0c\u5305\u542b114\u5c0f\u65f6\u591a\u8ddd\u79bb\u3001\u591a\u9ea6\u514b\u98ce\u5f55\u5236\u7684\u81ea\u53d1\u5bf9\u8bdd\uff0c\u65e8\u5728\u6539\u8fdb\u8fdc\u573aASR\u3002\u5728\u8be5\u8bed\u6599\u5e93\u4e0a\u5fae\u8c03Whisper\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u8fdc\u573a\u8bed\u97f3\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709ASR\u6a21\u578b\u5728\u8fdc\u573a\u6cf0\u8bed\u8bed\u97f3\u8bc6\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u8fdc\u573a\u6cf0\u8bed\u8bed\u97f3\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u968f\u8ddd\u79bb\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u8ddd\u79bb\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86LOTUSDIS\u6cf0\u8bed\u4f1a\u8bae\u8bed\u6599\u5e93\uff0c\u5305\u542b114\u5c0f\u65f6\u81ea\u53d1\u5bf9\u8bdd\uff0c\u7531\u4e09\u4e2a\u53c2\u4e0e\u8005\u572815-20\u5206\u949f\u4f1a\u8bdd\u4e2d\u5b8c\u6210\uff0c\u5177\u6709\u9891\u7e41\u7684\u91cd\u53e0\u8bed\u97f3\u3002\u8bed\u97f3\u901a\u8fc79\u4e2a\u72ec\u7acb\u5355\u901a\u9053\u8bbe\u5907\u57280.12\u7c73\u81f310\u7c73\u4e0d\u540c\u8ddd\u79bb\u30016\u79cd\u9ea6\u514b\u98ce\u7c7b\u578b\u4e0b\u540c\u6b65\u5f55\u5236\uff0c\u4fdd\u7559\u4e86\u6df7\u54cd\u3001\u566a\u97f3\u7b49\u771f\u5b9e\u6548\u679c\u3002\u63d0\u4f9b\u4e86\u6807\u51c6\u7684\u6570\u636e\u5212\u5206\u548c\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u4f7f\u7528\u591a\u79cdWhisper\u53d8\u4f53\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u73b0\u6210\u6a21\u578b\u5728\u8fdc\u8ddd\u79bb\u8bed\u97f3\u8bc6\u522b\u4e0a\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u8bc1\u5b9e\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u6cf0\u8bed\u8fdc\u573a\u8bed\u97f3\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002\u5728LOTUSDIS\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\uff0cThai Whisper\u57fa\u7ebf\u6a21\u578b\u7684\u6574\u4f53\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4ece64.3%\u964d\u81f338.3%\uff0c\u8fdc\u573aWER\u4ece81.6%\u964d\u81f349.5%\uff0c\u5c24\u5176\u5728\u6700\u8fdc\u8ddd\u79bb\u9ea6\u514b\u98ce\u4e0a\u7684\u6539\u8fdb\u6700\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8ddd\u79bb\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002LOTUSDIS\u8bed\u6599\u5e93\u53ca\u5176\u63d0\u4f9b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u811a\u672c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u7814\u7a76\u3002"}}
{"id": "2509.18667", "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "categories": ["cs.AI"], "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.", "AI": {"tldr": "\u73b0\u6709\u56fe\u57faRAG\u7cfb\u7edf\u56e0\u56fe\u6784\u5efa\u6210\u672c\u9ad8\u6602\u800c\u53d7\u9650\u3002TERAG\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u6784\u5efa\u4fe1\u606f\u56fe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e2a\u6027\u5316PageRank (PPR)\uff0c\u5728\u4fdd\u630180%\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11LLM token\u6d88\u8017\uff083%-11%\uff09\u3002", "motivation": "\u73b0\u6709\u56fe\u57faRAG\u7cfb\u7edf\u5728\u56fe\u6784\u5efa\u8fc7\u7a0b\u4e2dLLM token\u4f7f\u7528\u6210\u672c\u9ad8\u6602\uff0c\u963b\u788d\u4e86\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u63d0\u51faTERAG\u6846\u67b6\uff0c\u65e8\u5728\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u6784\u5efa\u4fe1\u606f\u56fe\u3002\u53d7HippoRAG\u542f\u53d1\uff0c\u5728\u68c0\u7d22\u9636\u6bb5\u878d\u5165\u4e2a\u6027\u5316PageRank (PPR)\u3002", "result": "TERAG\u5728\u4fdd\u6301\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u57faRAG\u65b9\u6cd5\u81f3\u5c1180%\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u4ec5\u6d88\u80173%-11%\u7684\u8f93\u51fatoken\u3002", "conclusion": "TERAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u56fe\u57faRAG\u7684\u6784\u5efa\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u6709\u5229\u4e8e\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2509.18390", "pdf": "https://arxiv.org/pdf/2509.18390", "abs": "https://arxiv.org/abs/2509.18390", "authors": ["Zitian Zhang", "Joshua Urban Davis", "Jeanne Phuong Anh Vu", "Jiangtao Kuang", "Jean-Fran\u00e7ois Lalonde"], "title": "Improving the color accuracy of lighting estimation models", "categories": ["cs.CV"], "comment": "Project page: https://lvsn.github.io/coloraccuracy", "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u767d\u5e73\u8861\u7f51\u7edc\u9884\u5904\u7406\u8f93\u5165\u56fe\u50cf\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u73b0\u6709\u5355\u56feHDR\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\u7684\u8272\u5f69\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u589e\u5f3aAR\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u6709\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u5149\u7167\u7684\u65b9\u6cd5\u5728\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u5e94\u7528\u4e2d\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u4f46\u5176\u8272\u5f69\u9c81\u68d2\u6027\uff08\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u5173\u952e\u56e0\u7d20\uff09\u5e38\u88ab\u5ffd\u89c6\u4e14\u4e0e\u5176\u5b83\u5149\u7167\u5c5e\u6027\uff08\u5982\u5f3a\u5ea6\u3001\u65b9\u5411\uff09\u6df7\u6dc6\u8bc4\u4f30\u3002\u7814\u7a76\u65e8\u5728\u72ec\u7acb\u63a2\u7a76\u8272\u5f69\u9c81\u68d2\u6027\uff0c\u5e76\u5bfb\u6c42\u65e0\u9700\u4fee\u6539\u6838\u5fc3\u7b97\u6cd5\u7684\u7b80\u5355\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u672c\u6587\u4e0d\u63d0\u51fa\u65b0\u7684\u5149\u7167\u4f30\u8ba1\u7b97\u6cd5\uff0c\u800c\u662f\u5229\u7528\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5149\u7167\u8272\u5f69\u7684\u65b0\u578bHDR\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u51e0\u79cd\u9002\u5e94\u7b56\u7565\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u6d4b\u8bd5\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u767d\u5e73\u8861\u7f51\u7edc\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4ee5\u671f\u63d0\u9ad8\u73b0\u6709\u6a21\u578b\u7684\u8272\u5f69\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u767d\u5e73\u8861\u7f51\u7edc\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8272\u5f69\u9c81\u68d2\u6027\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u539f\u6709\u7684\u5149\u7167\u4f30\u8ba1\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6280\u672f\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u4e09\u79cd\u6700\u65b0\u7684SOTA\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "\u7b80\u5355\u7684\u9002\u5e94\u6280\u672f\uff0c\u7279\u522b\u662f\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u767d\u5e73\u8861\u7f51\u7edc\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u5355\u56feHDR\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\u7684\u8272\u5f69\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0AR\u5e94\u7528\u7684\u89c6\u89c9\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.18135", "pdf": "https://arxiv.org/pdf/2509.18135", "abs": "https://arxiv.org/abs/2509.18135", "authors": ["Shaoxun Wang", "Xingjun Zhang", "Qianyang Li", "Jiawei Cao", "Zhendong Tan"], "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inter-series correlations are crucial for accurate multivariate time series\nforecasting, yet these relationships often exhibit complex dynamics across\ndifferent temporal scales. Existing methods are limited in modeling these\nmulti-scale dependencies and struggle to capture their intricate and evolving\nnature. To address this challenge, this paper proposes a novel Static-Dynamic\nGraph Fusion network (SDGF), whose core lies in capturing multi-scale\ninter-series correlations through a dual-path graph structure learning\napproach. Specifically, the model utilizes a static graph based on prior\nknowledge to anchor long-term, stable dependencies, while concurrently\nemploying Multi-level Wavelet Decomposition to extract multi-scale features for\nconstructing an adaptively learned dynamic graph to capture associations at\ndifferent scales. We design an attention-gated module to fuse these two\ncomplementary sources of information intelligently, and a multi-kernel dilated\nconvolutional network is then used to deepen the understanding of temporal\npatterns. Comprehensive experiments on multiple widely used real-world\nbenchmark datasets demonstrate the effectiveness of our proposed model.", "AI": {"tldr": "\u63d0\u51faSDGF\u7f51\u7edc\uff0c\u901a\u8fc7\u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u878d\u5408\u6765\u6355\u6349\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u5c3a\u5ea6\u5e8f\u5217\u95f4\u590d\u6742\u76f8\u5173\u6027\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u5c3a\u5ea6\u5e8f\u5217\u95f4\u76f8\u5173\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u6355\u6349\u5176\u590d\u6742\u4e14\u52a8\u6001\u53d8\u5316\u7684\u7279\u6027\u3002", "method": "\u63d0\u51faStatic-Dynamic Graph Fusion (SDGF) \u7f51\u7edc\u3002\u6838\u5fc3\u662f\u53cc\u8def\u5f84\u56fe\u7ed3\u6784\u5b66\u4e60\uff1a\u5229\u7528\u9759\u6001\u56fe\uff08\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\uff09\u951a\u5b9a\u957f\u671f\u7a33\u5b9a\u4f9d\u8d56\uff0c\u5e76\u7ed3\u5408\u591a\u7ea7\u5c0f\u6ce2\u5206\u89e3\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u6784\u5efa\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u52a8\u6001\u56fe\uff0c\u4ee5\u6355\u83b7\u4e0d\u540c\u5c3a\u5ea6\u7684\u5173\u8054\u3002\u8bbe\u8ba1\u6ce8\u610f\u529b\u95e8\u63a7\u6a21\u5757\u667a\u80fd\u878d\u5408\u8fd9\u4e24\u79cd\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u591a\u6838\u81a8\u80c0\u5377\u79ef\u7f51\u7edc\u6df1\u5316\u5bf9\u65f6\u95f4\u6a21\u5f0f\u7684\u7406\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "SDGF\u6a21\u578b\u901a\u8fc7\u6709\u6548\u878d\u5408\u9759\u6001\u548c\u52a8\u6001\u56fe\u7ed3\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u591a\u5c3a\u5ea6\u5e8f\u5217\u95f4\u590d\u6742\u76f8\u5173\u6027\u7684\u5efa\u6a21\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.18742", "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs.", "AI": {"tldr": "DyGRASP\u662f\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548c\u65f6\u95f4GNN\uff0c\u9ad8\u6548\u5904\u7406\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\uff08DyTAGs\uff09\u4e2d\u7684\u8fd1\u671f\u548c\u5168\u5c40\u65f6\u95f4\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9759\u6001\u56fe\u4e0a\u7684\u5c40\u9650\u6027\u548cLLM\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982GNNs\u548cLLMs\uff09\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\uff08DyTAGs\uff09\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u8fd1\u671f-\u5168\u5c40\u65f6\u95f4\u8bed\u4e49\uff08\u4ea4\u4e92\u6587\u672c\u7684\u8fd1\u671f\u8bed\u4e49\u4f9d\u8d56\u548c\u8282\u70b9\u968f\u65f6\u95f4\u7684\u5168\u5c40\u8bed\u4e49\u6f14\u53d8\uff09\u3002\u6b64\u5916\uff0c\u5c06LLMs\u5e94\u7528\u4e8eDyTAGs\u4e2d\u4e30\u5bcc\u4e14\u4e0d\u65ad\u53d8\u5316\u7684\u6587\u672c\u9762\u4e34\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faDynamic Global-Recent Adaptive Semantic Processing (DyGRASP) \u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408LLMs\u548c\u65f6\u95f4GNN\u6765\u5904\u7406DyTAGs\u3002\u5177\u4f53\u5305\u62ec\uff1a\u8bbe\u8ba1\u4ee5\u8282\u70b9\u4e3a\u4e2d\u5fc3\u7684\u9690\u5f0f\u63a8\u7406\u65b9\u6cd5\u548c\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u4ee5\u9ad8\u6548\u6355\u83b7\u8fd1\u671f\u65f6\u95f4\u8bed\u4e49\uff1b\u5229\u7528\u5e26\u6709\u5b9a\u5236\u63d0\u793a\u7684\u663e\u5f0f\u63a8\u7406\u548c\u7c7bRNN\u94fe\u5f0f\u7ed3\u6784\u6355\u83b7\u8282\u70b9\u7684\u5168\u5c40\u8bed\u4e49\u52a8\u6001\uff1b\u901a\u8fc7\u66f4\u65b0\u548c\u5408\u5e76\u5c42\uff0c\u6574\u5408\u8fd1\u671f\u548c\u5168\u5c40\u65f6\u95f4\u8bed\u4e49\u4ee5\u53ca\u52a8\u6001\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "result": "DyGRASP\u5728DyTAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u76ee\u6807\u8282\u70b9\u68c0\u7d22\u4efb\u52a1\u7684Hit@10\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe34%\u7684\u63d0\u5347\u3002\u6b64\u5916\uff0cDyGRASP\u5728\u4e0d\u540c\u7684\u65f6\u95f4GNN\u548cLLM\u4e4b\u95f4\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DyGRASP\u901a\u8fc7\u6709\u6548\u6355\u83b7\u8fd1\u671f\u548c\u5168\u5c40\u65f6\u95f4\u8bed\u4e49\uff0c\u5e76\u89e3\u51b3LLM\u5e94\u7528\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u6210\u529f\u5730\u5728\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u4e0a\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2509.18681", "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "categories": ["cs.AI"], "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.", "AI": {"tldr": "\u7b80\u8ff0\u822a\u7a7aML\u7cfb\u7edf\u5b89\u5168\u5408\u89c4\u6311\u6218\uff0c\u8bba\u6587\u63d0\u51fa\u533a\u5206ML\u6a21\u578b\u4e0eML\u6a21\u578b\u63cf\u8ff0(MLMD)\uff0c\u5e76\u7ec6\u5316\u8bed\u4e49\u4fdd\u5b58\u6982\u5ff5\u4ee5\u786e\u4fdd\u6a21\u578b\u7cbe\u786e\u590d\u5236\uff0c\u5e76\u5e94\u7528\u4e8e\u5de5\u4e1a\u7528\u4f8b\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e3a\u822a\u7a7a\u7cfb\u7edf\u5e26\u6765\u65b0\u80fd\u529b\uff0c\u4f46\u5176\u5fc5\u987b\u4fdd\u8bc1\u5b89\u5168\u8fd0\u884c\u5e76\u7b26\u5408\u76f8\u5173\u76d1\u7ba1\u6307\u5bfc\u3002\u73b0\u6709\u6307\u5357\uff08\u5982EASA\u548cED-324\uff09\u4ec5\u63d0\u4f9b\u4e86\u9ad8\u7ea7\u76ee\u6807\uff0c\u7f3a\u4e4f\u786e\u4fddML\u6a21\u578b\u529f\u80fd\u5b9e\u73b0\u548c\u8bad\u7ec3\u6027\u80fd\u7ef4\u6301\u7684\u5177\u4f53\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u65e8\u5728\u6f84\u6e05ML\u6a21\u578b\u4e0e\u5176\u660e\u786e\u63cf\u8ff0\uff08MLMD\uff09\u4e4b\u95f4\u7684\u533a\u522b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5b83\u7ec6\u5316\u4e86\u201c\u8bed\u4e49\u4fdd\u5b58\u201d\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u51c6\u786e\u590d\u5236\u3002", "result": "\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u591a\u4e2a\u5de5\u4e1a\u7528\u4f8b\uff0c\u6210\u529f\u6784\u5efa\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u76ee\u6807\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u660e\u786eMLMD\u548c\u8bed\u4e49\u4fdd\u5b58\u7684\u6982\u5ff5\uff0c\u8be5\u7814\u7a76\u4e3aML\u6a21\u578b\u5728\u822a\u7a7a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5408\u89c4\u590d\u5236\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2509.18405", "pdf": "https://arxiv.org/pdf/2509.18405", "abs": "https://arxiv.org/abs/2509.18405", "authors": ["Sourav Halder", "Jinjun Tong", "Xinyu Wu"], "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures, 2 tables", "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408VLM\u548cMLLM\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b0\u652f\u7968\u5b57\u6bb5\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53ef\u7528\u4e8e\u751f\u6210\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u652f\u7968\u6b3a\u8bc8\u68c0\u6d4b\u4e9f\u9700\u51c6\u786e\u8bc6\u522b\u5173\u952e\u5b57\u6bb5\uff0c\u4f46\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5b58\u5728\u8d44\u6e90\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u652f\u7968\u5b57\u6bb5\u81ea\u52a8\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u6d4b\u3002", "result": "\u5728\u5305\u542b110\u5f20\u4e0d\u540c\u683c\u5f0f\u548c\u5e03\u5c40\u652f\u7968\u7684\u624b\u5de5\u6574\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u652f\u7968\u7ec4\u4ef6\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u95e8\u69db\uff0c\u5e76\u53ef\u4f5c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u652f\u6301\u5f00\u53d1\u5b9a\u5236\u5316\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002"}}
{"id": "2509.18136", "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "AI": {"tldr": "\u5c3d\u7ba1LLMs\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7684\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u5173\u7cfb\u7814\u7a76\u3002\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u6316\u6398\u5206\u6790\uff0c\u63ed\u793a\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cdLLM\u6027\u80fd\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u6a21\u578b\u5f00\u53d1\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5173\u4e8e\u5176\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u5f00\u6e90LLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u7684\u5927\u578b\u6570\u636e\u96c6\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u6570\u636e\u6316\u6398\u5206\u6790\uff0c\u4ee5\u9a8c\u8bc1\u5e76\u91cf\u5316\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7814\u7a76\u8fd8\u56de\u987e\u4e86LLMs\u7684\u5386\u53f2\u53d1\u5c55\u3001\u63a2\u7d22\u672a\u6765\u8d8b\u52bf\uff0c\u5e76\u4f7f\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u7814\u7a76\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u4f18\u5316\u7684\u6570\u636e\u9a71\u52a8\u89c1\u89e3\uff0c\u5206\u6790\u4e86\u5404\u79cd\u7ed3\u6784\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8ba1\u5212\u53d1\u5e03\u6240\u6784\u5efa\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6d1e\u5bdf\uff0c\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u5b9a\u5411\u5f00\u53d1\u548c\u5e94\u7528\u3002"}}
{"id": "2509.18750", "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bcd\u5143\u91cd\u53e0\u6709\u52a9\u4e8e\u521b\u5efa\u6355\u83b7\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u7cfb\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u5347\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u63a2\u7a76\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u8bad\u7ec3\u7684\u5b50\u8bcd\u5206\u8bcd\u5668\u4e2d\uff0c\u8bcd\u5143\u91cd\u53e0\u662f\u4fc3\u8fdb\u8de8\u8bed\u8a00\u8fc1\u79fb\u8fd8\u662f\u5f15\u5165\u5e72\u6270\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u8bc1\u636e\u4e0d\u4e00\u4e14\u53d7\u8bbe\u7f6e\u548c\u6df7\u6742\u56e0\u7d20\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5728\u7cfb\u7edf\u5316\u53d8\u5316\u7684\u8bcd\u6c47\u91cd\u53e0\u8bbe\u7f6e\u4e0b\uff0c\u8bad\u7ec3\u591a\u4e2a\u8bed\u8a00\u5bf9\u7684\u53cc\u8bed\u81ea\u56de\u5f52\u6a21\u578b\u3002\u5f15\u5165\u5171\u4eab\u8bcd\u5143\u8bed\u4e49\u76f8\u4f3c\u6027\u7ef4\u5ea6\u6765\u5206\u6790\u91cd\u53e0\u7684\u5f71\u54cd\u3002\u9996\u5148\u5206\u6790\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\uff0c\u7136\u540e\u8bc4\u4f30\u5728XNLI\u548cXQuAD\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u4efb\u4f55\u5f62\u5f0f\u7684\u8bcd\u5143\u91cd\u53e0\u90fd\u80fd\u521b\u5efa\u6355\u83b7\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u7cfb\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u800c\u72ec\u7acb\u8bcd\u6c47\u6a21\u578b\u7684\u6548\u679c\u5219\u5f31\u5f97\u591a\u3002\u5728XNLI\u548cXQuAD\u4e0a\uff0c\u91cd\u53e0\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u72ec\u7acb\u8bcd\u6c47\u6a21\u578b\uff0c\u4e14\u8fc1\u79fb\u6027\u80fd\u666e\u904d\u968f\u91cd\u53e0\u7a0b\u5ea6\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u8bcd\u5143\u91cd\u53e0\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u8868\u660e\u5b9e\u8d28\u6027\u7684\u5171\u4eab\u8bcd\u6c47\u4ecd\u7136\u662f\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u7684\u4e00\u79cd\u6709\u76ca\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2509.18690", "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "categories": ["cs.AI"], "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u5176\u8bad\u7ec3\u3001\u5e94\u7528\u3001\u4f18\u7f3a\u70b9\uff0c\u5e76\u521b\u65b0\u6027\u5730\u5206\u7c7b\u4e86\u533b\u7597LLMs\u53ca\u5176\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6280\u672f\uff0c\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u5e76\u5728\u5404\u884c\u5404\u4e1a\u4ea7\u751f\u5f71\u54cd\uff0c\u5176\u4e2d\u533b\u7597\u9886\u57df\u662f\u7a81\u51fa\u7684\u5e94\u7528\u65b9\u5411\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5bf9\u533b\u7597\u9886\u57dfLLMs\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u8fdb\u884c\u6df1\u5165\u5206\u6790\u548c\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7efc\u8ff0\u533b\u7597\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u3002", "result": "\u6df1\u5165\u5206\u6790\u4e86\u533b\u7597\u5927\u578b\u6a21\u578b\u7684\u8bad\u7ec3\u6280\u672f\u3001\u5728\u533b\u7597\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3001\u76f8\u5173\u5e94\u7528\u3001\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u3002\u521b\u65b0\u6027\u5730\u5c06\u533b\u7597LLMs\u4f9d\u636e\u8bad\u7ec3\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff0c\u5e76\u5c06\u5176\u8bc4\u4f30\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b\u3002", "conclusion": "\u65e8\u5728\u5f3a\u8c03\u5f00\u53d1\u533b\u7597LLMs\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u4f9b\u5bf9\u5176\u5f53\u524d\u53d1\u5c55\u72b6\u6001\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u660e\u786e\u6307\u5bfc\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u73b0\u6709\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u89c4\u5212\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.18425", "pdf": "https://arxiv.org/pdf/2509.18425", "abs": "https://arxiv.org/abs/2509.18425", "authors": ["Philip Wootaek Shin", "Jack Sampson", "Vijaykrishnan Narayanan", "Andres Marquez", "Mahantesh Halappanavar"], "title": "Losing the Plot: How VLM responses degrade on imperfect charts", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4e2d\u53d7\u635f\u6216\u906e\u6321\u7684\u56fe\u8868\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86CHART NOISe\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u56fe\u8868\u7406\u89e3\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u5047\u8bbe\u56fe\u8868\u6e05\u6670\u4e14\u67e5\u8be2\u57fa\u4e8e\u4e8b\u5b9e\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u56fe\u8868\u5e38\u6709\u5931\u771f\uff0c\u5e76\u9700\u8981\u6df1\u5c42\u63a8\u7406\u3002\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\u4e14\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4e9f\u9700\u63d0\u5347\u5176\u5728\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "method": "\u8bc4\u4f30\u4e86ChatGPT 4o\u3001Claude Sonnet 4\u548cGemini 2.5 Pro\u6a21\u578b\u3002\u5f15\u5165\u4e86CHART NOISe\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u56fe\u8868\u635f\u574f\u3001\u906e\u6321\u548c\u8003\u8bd5\u98ce\u683c\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u521b\u65b0\u6027\u5730\u52a0\u5165\u4e86\u201c\u63d0\u793a\u53cd\u5411\u4e0d\u4e00\u81f4\u6027\u201d\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u8d28\u91cf\u8fc7\u6ee4\u548c\u906e\u6321\u68c0\u6d4b\u7b49\u57fa\u7ebf\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u5728\u56fe\u8868\u53d7\u635f\u6216\u906e\u6321\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u5e76\u9891\u7e41\u51fa\u73b0\u5e7b\u89c9\uff08\u5982\u6570\u503c\u7f16\u9020\u3001\u8d8b\u52bf\u8bef\u89e3\uff09\u3002\u6a21\u578b\u5728\u9000\u5316\u8bbe\u7f6e\u4e0b\u4ecd\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u65e0\u652f\u6301\u7684\u89e3\u91ca\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6700\u5148\u8fdbVLM\u5728\u56fe\u8868\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u8106\u5f31\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5bf9\u6700\u5148\u8fdbVLM\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u5728\u56fe\u8868\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u8106\u5f31\u6027\u3002\u53d1\u5e03\u4e86\u9996\u4e2a\u7ed3\u5408\u635f\u574f\u3001\u906e\u6321\u548c\u53cd\u5411\u4e0d\u4e00\u81f4\u6027\u7684CHART NOISe\u6570\u636e\u96c6\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u4e3a\u672a\u6765\u63d0\u5347\u56fe\u8868\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2509.18137", "pdf": "https://arxiv.org/pdf/2509.18137", "abs": "https://arxiv.org/abs/2509.18137", "authors": ["Shaoheng Wang", "Yao Lu", "Yuqi Li", "Yaxin Gao", "Jiaqi Nie", "Shanqing Yu", "Yingli Tian", "Qi Xuan"], "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation\n(LoRA) can save significant costs in storage and computing, but its strong\nadaptability to a single task is often accompanied by insufficient cross-task\ngeneralization capabilities. To improve this, existing work combines LoRA with\nmixture-of-experts (MoE) to enhance the model's adaptability through expert\nmodules and routing mechanisms. However, existing LoRA-MoE methods lack unified\nstandards in models, datasets, hyperparameters, and evaluation methods, making\nit difficult to conduct fair comparisons between different methods. To this\nend, we proposed a unified benchmark named LoRALib. Specifically, we\nstandardized datasets from $40$ downstream tasks into a unified format,\nfine-tuned them using the same hyperparameters and obtained $680$ LoRA modules\nacross $17$ model architectures. Based on this LoRA library, we conduct\nlarge-scale experiments on $3$ representative LoRA-MoE methods and different\nLoRA selection mechanisms using the open-sourced testing tool OpenCompass.\nExtensive experiments show that LoRAMoE performs best, and that prioritizing\nLoRAs relevant to the target task can further improve the performance of MoE.\nWe hope these findings will inspire future work. Our datasets and LoRA library\nare available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset\nand https://huggingface.co/YaoLuzjut/models.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709LoRA-MoE\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u5bfc\u81f4\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86LoRALib\u7edf\u4e00\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b40\u4e2a\u6807\u51c6\u5316\u4efb\u52a1\u7684680\u4e2aLoRA\u6a21\u5757\u3002\u57fa\u4e8eLoRALib\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u8868\u660e\uff0cLoRAMoE\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u4f18\u5148\u9009\u62e9\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684LoRA\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347MoE\u6027\u80fd\u3002", "motivation": "\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u5728\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709LoRA-MoE\u65b9\u6cd5\u65e8\u5728\u589e\u5f3aLoRA\u7684\u9002\u5e94\u6027\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u8d85\u53c2\u6570\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\uff0c\u4f7f\u5f97\u516c\u5e73\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u540d\u4e3aLoRALib\u7684\u7edf\u4e00\u57fa\u51c6\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5c06\u6765\u81ea40\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6570\u636e\u96c6\u6807\u51c6\u5316\u4e3a\u7edf\u4e00\u683c\u5f0f\uff1b\u4f7f\u7528\u76f8\u540c\u7684\u8d85\u53c2\u6570\u5bf917\u79cd\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5fae\u8c03\uff0c\u5f97\u5230680\u4e2aLoRA\u6a21\u5757\u3002\u57fa\u4e8e\u6b64LoRA\u5e93\uff0c\u4f7f\u7528\u5f00\u6e90\u6d4b\u8bd5\u5de5\u5177OpenCompass\u5bf93\u79cd\u4ee3\u8868\u6027LoRA-MoE\u65b9\u6cd5\u548c\u4e0d\u540c\u7684LoRA\u9009\u62e9\u673a\u5236\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLoRAMoE\u65b9\u6cd5\u6027\u80fd\u6700\u4f73\u3002\u6b64\u5916\uff0c\u4f18\u5148\u9009\u62e9\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684LoRA\u6a21\u5757\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8MoE\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u7edf\u4e00\u57fa\u51c6LoRALib\u89e3\u51b3\u4e86LoRA-MoE\u65b9\u6cd5\u6bd4\u8f83\u6807\u51c6\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86LoRAMoE\u7684\u4f18\u8d8a\u6027\u80fd\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173LoRA\u9009\u62e9\u673a\u5236\u7684\u6709\u6548\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u671b\u4e3a\u672a\u6765\u7684\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u542f\u53d1\u3002"}}
{"id": "2509.18762", "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u957f\u4e0a\u4e0b\u6587SFT\u80fd\u53cd\u5e38\u5730\u63d0\u5347LLM\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u4e86SFT\u6570\u636e\u957f\u5ea6\u5bf9\u6a21\u578b\u77e5\u8bc6\u504f\u597d\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u6df7\u5408\u8bad\u7ec3\u53ef\u7f13\u89e3\u6b64\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u5145\u5206\u63a2\u8ba8\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u957f\u5ea6\u5bf9LLMs\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f46\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6570\u636e\u957f\u5ea6\u5982\u4f55\u5f71\u54cdLLM\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u7684\u884c\u4e3a\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u7814\u7a76SFT\u6570\u636e\u957f\u5ea6\u5bf9LLM\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u8868\u73b0\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u89e3\u8026\u5e76\u5206\u6790\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u548c\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8fd9\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76\u5b83\u4eec\u7684\u4ea4\u4e92\u4f5c\u7528\u6765\u63ed\u793a\u6f5c\u5728\u673a\u5236\u3002", "result": "1. \u957f\u4e0a\u4e0b\u6587SFT\u51fa\u4e4e\u610f\u6599\u5730\u63d0\u5347\u4e86LLM\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8fd9\u4e0e\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u901a\u5e38\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u76f8\u53cd\u3002 2. MHA\u548cFFN\u4e24\u4e2a\u7ec4\u4ef6\u5747\u72ec\u7acb\u53d7\u76ca\u4e8e\u957f\u4e0a\u4e0b\u6587SFT\u3002 3. \u63ed\u793a\u4e86\u4e00\u79cd\u77e5\u8bc6\u504f\u597d\u504f\u5dee\uff1a\u957f\u4e0a\u4e0b\u6587SFT\u4fc3\u8fdb\u8bed\u5883\u77e5\u8bc6\uff0c\u800c\u77ed\u4e0a\u4e0b\u6587SFT\u504f\u5411\u53c2\u6570\u77e5\u8bc6\uff0c\u8fd9\u8868\u660e\u4ec5\u4f9d\u8d56\u957f\u4e0a\u4e0b\u6587SFT\u5e76\u975e\u6700\u4f18\u3002", "conclusion": "\u6df7\u5408\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u957f\u4e0a\u4e0b\u6587SFT\u548c\u77ed\u4e0a\u4e0b\u6587SFT\u4e4b\u95f4\u7684\u77e5\u8bc6\u504f\u597d\u504f\u5dee\uff0c\u4e3aLLM\u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.18710", "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "categories": ["cs.AI"], "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u6570\u636e\u667a\u80fd\u4f53\uff08DataAgents\uff09\uff0c\u4e00\u79cd\u96c6\u6210LLM\u63a8\u7406\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\uff0c\u5c06\u590d\u6742\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff0c\u6807\u5fd7\u7740\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u8303\u5f0f\u7684\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\u52b3\u52a8\u5bc6\u96c6\u3001\u91cd\u590d\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6570\u636e\u4e0eAI\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u901a\u5e38\u4e0d\u9002\u5408AI\u5229\u7528\u3002\u4f20\u7edf\u6570\u636e\u7ba1\u7406\u5de5\u5177\u7f3a\u4e4f\u52a8\u6001\u89c4\u5212\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165\u6570\u636e\u667a\u80fd\u4f53\uff08DataAgents\uff09\uff0c\u5176\u901a\u8fc7\u6574\u5408LLM\u63a8\u7406\u3001\u4efb\u52a1\u5206\u89e3\u3001\u884c\u52a8\u63a8\u7406\u4e0e\u843d\u5730\u4ee5\u53ca\u5de5\u5177\u8c03\u7528\uff0c\u81ea\u4e3b\u89e3\u91ca\u6570\u636e\u4efb\u52a1\u3001\u5206\u89e3\u5b50\u4efb\u52a1\u3001\u89c4\u5212\u5de5\u4f5c\u6d41\u3001\u6267\u884cPython\u4ee3\u7801\u6216\u5de5\u5177\u64cd\u4f5c\uff0c\u5904\u7406\u6570\u636e\u7684\u6536\u96c6\u3001\u6574\u5408\u3001\u9884\u5904\u7406\u3001\u9009\u62e9\u3001\u8f6c\u6362\u3001\u589e\u5f3a\u548c\u4fee\u590d\u7b49\u3002", "result": "\u6570\u636e\u667a\u80fd\u4f53\u4ee3\u8868\u4e86\u81ea\u4e3b\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u5c06\u590d\u6742\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u8fde\u8d2f\u4e14\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u3002\u672c\u6587\u5b9a\u4e49\u4e86DataAgents\u5e76\u8ba8\u8bba\u4e86\u5176\u67b6\u6784\u8bbe\u8ba1\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6240\u652f\u6301\u7684\u65b0\u80fd\u529b\u3002", "conclusion": "\u6570\u636e\u667a\u80fd\u4f53\u662f\u81ea\u4e3b\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u91cd\u8981\u8d8b\u52bf\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u5173\u6ce8\u884c\u52a8\u5de5\u4f5c\u6d41\u4f18\u5316\u3001\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5efa\u7acb\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u6548\u7387\u4e0e\u53ef\u4f38\u7f29\u6027\u5e73\u8861\uff0c\u4ee5\u53ca\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684DataAgent\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2509.18427", "pdf": "https://arxiv.org/pdf/2509.18427", "abs": "https://arxiv.org/abs/2509.18427", "authors": ["Xinyang Wu", "Muheng Li", "Xia Li", "Orso Pusterla", "Sairos Safai", "Philippe C. Cattin", "Antony J. Lomax", "Ye Zhang"], "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8868\u5f81\u76844D-MRI\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7SAN\u548cTMN\u7f51\u7edc\u5c06\u547c\u5438\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8fde\u7eed\u53d8\u5f62\uff0c\u53d6\u4ee3\u4f20\u7edf\u79bb\u6563\u6392\u5e8f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5bf94D\u653e\u7597\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u76844D-MRI\u91cd\u5efa\u65b9\u6cd5\uff08\u5982\u76f8\u4f4d\u5206\u7bb1\u6216\u72ec\u7acb\u6a21\u677f\u626b\u63cf\uff09\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u5de5\u4f5c\u6d41\u7a0b\u590d\u6742\u4e14\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u795e\u7ecf\u8868\u5f81\u6846\u67b6\uff0c\u5c06\u547c\u5438\u8fd0\u52a8\u89c6\u4e3a\u7531\u4e00\u7ef4\u66ff\u4ee3\u4fe1\u53f7\u9a71\u52a8\u7684\u5e73\u6ed1\u3001\u8fde\u7eed\u53d8\u5f62\u3002\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u8fd0\u52a8\u5efa\u6a21\u4e0e\u56fe\u50cf\u91cd\u5efa\uff0c\u901a\u8fc7\u4e24\u4e2a\u7f51\u7edc\u534f\u540c\u5de5\u4f5c\uff1a\u7a7a\u95f4\u89e3\u5256\u7f51\u7edc\uff08SAN\uff09\u7f16\u7801\u8fde\u7eed\u76843D\u89e3\u5256\u8868\u5f81\uff0c\u65f6\u95f4\u8fd0\u52a8\u7f51\u7edc\uff08TMN\uff09\u5728Transformer\u547c\u5438\u4fe1\u53f7\u5f15\u5bfc\u4e0b\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u5f62\u53d8\u573a\u3002", "result": "\u8be5\u65e0\u6a21\u677f\u3001\u65e0\u76f8\u4f4d\u7684\u65b9\u6cd5\u80fd\u51c6\u786e\u6355\u6349\u89c4\u5f8b\u548c\u4e0d\u89c4\u5f8b\u7684\u547c\u5438\u6a21\u5f0f\uff0c\u5e76\u9ad8\u7cbe\u5ea6\u5730\u4fdd\u6301\u8840\u7ba1\u548c\u652f\u6c14\u7ba1\u7684\u8fde\u7eed\u6027\u3002\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u603b\u5904\u7406\u65f6\u95f4\u4ece\u4f20\u7edf\u65b9\u6cd5\u7684\u7ea65\u5c0f\u65f6\u7f29\u77ed\u81f315\u5206\u949f\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u6bcf\u4e2a3D\u4f53\u7d20\u7684\u63a8\u7406\u65f6\u95f4\u4e0d\u52301\u79d2\u3002\u5728\u4efb\u4f55\u547c\u5438\u72b6\u6001\u4e0b\u90fd\u80fd\u51c6\u786e\u91cd\u5efa3D\u56fe\u50cf\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u51c6\u786e\u91cd\u5efa\u4efb\u610f\u547c\u5438\u72b6\u6001\u4e0b\u76843D\u56fe\u50cf\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u57284D\u653e\u5c04\u6cbb\u7597\u89c4\u5212\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u6cbb\u7597\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18138", "pdf": "https://arxiv.org/pdf/2509.18138", "abs": "https://arxiv.org/abs/2509.18138", "authors": ["Tiantian Zhang"], "title": "Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a new algorithm, \\emph{Rank-Induced Plackett--Luce Mirror\nDescent (RIPLM)}, which leverages the structural equivalence between the\n\\emph{rank benchmark} and the \\emph{distributional benchmark} established in\n\\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert\nidentities, RIPLM updates directly in the \\emph{rank-induced Plackett--Luce\n(PL)} parameterization. This ensures that the algorithm's played distributions\nremain within the class of rank-induced distributions at every round,\npreserving the equivalence with the rank benchmark. To our knowledge, RIPLM is\nthe first algorithm that is both (i) \\emph{rank-faithful} and (ii)\n\\emph{variance-adaptive} in the sleeping experts setting.", "AI": {"tldr": "\u5f15\u5165RIPLM\u7b97\u6cd5\uff0c\u5176\u5728\u201c\u6c89\u7761\u4e13\u5bb6\u201d\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u79e9\u5fe0\u8bda\u548c\u65b9\u5dee\u81ea\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u5728\u201c\u6c89\u7761\u4e13\u5bb6\u201d\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u79e9\u5fe0\u8bda\u548c\u65b9\u5dee\u81ea\u9002\u5e94\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u5229\u7528\u5df2\u5efa\u7acb\u7684\u79e9\u57fa\u51c6\u4e0e\u5206\u5e03\u57fa\u51c6\u4e4b\u95f4\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u79e9\u8bf1\u5bfcPlackett-Luce\u955c\u50cf\u4e0b\u964d\uff08RIPLM\uff09\u7684\u65b0\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u4e86\u79e9\u57fa\u51c6\u548c\u5206\u5e03\u57fa\u51c6\u4e4b\u95f4\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\uff0c\u5e76\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce (PL) \u53c2\u6570\u5316\u4e2d\u8fdb\u884c\u66f4\u65b0\uff0c\u4e0e\u5148\u524d\u64cd\u4f5c\u4e13\u5bb6\u8eab\u4efd\u7684\u65b9\u6cd5\u4e0d\u540c\u3002\u8fd9\u79cd\u65b9\u6cd5\u786e\u4fdd\u4e86\u7b97\u6cd5\u7684\u5206\u5e03\u5728\u6bcf\u4e00\u8f6e\u90fd\u4fdd\u6301\u5728\u79e9\u8bf1\u5bfc\u5206\u5e03\u7c7b\u522b\u5185\uff0c\u4ece\u800c\u4fdd\u7559\u4e86\u4e0e\u79e9\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\u3002", "result": "RIPLM\u662f\u9996\u4e2a\u5728\u201c\u6c89\u7761\u4e13\u5bb6\u201d\u8bbe\u7f6e\u4e2d\u540c\u65f6\u5177\u5907\u79e9\u5fe0\u8bda\u6027\u548c\u65b9\u5dee\u81ea\u9002\u5e94\u6027\u7684\u7b97\u6cd5\u3002\u5176\u72ec\u7279\u66f4\u65b0\u673a\u5236\u786e\u4fdd\u4e86\u6240\u73a9\u7684\u5206\u5e03\u59cb\u7ec8\u4fdd\u6301\u5728\u79e9\u8bf1\u5bfc\u5206\u5e03\u7c7b\u522b\u5185\uff0c\u4ece\u800c\u4fdd\u6301\u4e86\u4e0e\u79e9\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\u3002", "conclusion": "RIPLM\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u66f4\u65b0\uff0c\u6210\u529f\u5730\u5728\u201c\u6c89\u7761\u4e13\u5bb6\u201d\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u79e9\u5fe0\u8bda\u6027\u548c\u65b9\u5dee\u81ea\u9002\u5e94\u6027\u3002"}}
{"id": "2509.18775", "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c10-K\u6587\u4ef6\uff0c\u7cfb\u7edf\u5730\u63d0\u53d6\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\u5e76\u91cf\u5316\u5176\u5173\u8054\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bc6\u522b\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u8054\u5bf9\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u548c\u6295\u8d44\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edf\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\u548c\u624b\u52a8\u5206\u6790\u7684\u65b9\u6cd5\u4e3b\u89c2\u3001\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4ee5Form 10-K\u6587\u4ef6\u4e3a\u6570\u636e\u6e90\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08\u57fa\u4e8e\u65f6\u95f4\u5e8f\u548c\u8bcd\u6c47\u6a21\u5f0f\u7684\u65e0\u76d1\u7763\u5fae\u8c03\uff09\u6355\u6349\u9690\u542b\u548c\u62bd\u8c61\u7684\u98ce\u9669\u8fde\u63a5\uff0c\u6784\u5efa\u9886\u57df\u4e13\u7528\u91d1\u878d\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u91cf\u5316\u98ce\u9669\u5173\u8054\u5206\u6570\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u3001\u53ef\u89e3\u91ca\u4e14\u8868\u73b0\u5353\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18771", "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u7ecf\u9a8c\u7f29\u653e\u201d\u6846\u67b6\uff0c\u901a\u8fc7LLM\u90e8\u7f72\u540e\u7684\u81ea\u4e3b\u4ea4\u4e92\u548c\u7ecf\u9a8c\u5171\u4eab\uff0c\u5b9e\u73b0\u6301\u7eed\u8fdb\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u3001\u7ef4\u6301\u6027\u80fd\uff0c\u5e76\u8d85\u8d8a\u9759\u6001\u4eba\u7c7b\u6570\u636e\u5e26\u6765\u7684\u9650\u5236\u3002", "motivation": "\u5f53\u524dLLM\u7684\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6b63\u56e0\u4eba\u7c7b\u751f\u6210\u6587\u672c\u7684\u8017\u5c3d\u800c\u63a5\u8fd1\u9971\u548c\uff0c\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u201c\u7ecf\u9a8c\u7f29\u653e\u201d\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4e0e\u73af\u5883\u81ea\u4e3b\u4ea4\u4e92\u548c\u5171\u4eab\u7d2f\u79ef\u7ecf\u9a8c\uff0c\u5b9e\u73b0\u90e8\u7f72\u540e\u7684\u6301\u7eed\u8fdb\u5316\u3002\u8be5\u6846\u67b6\u6355\u83b7\u539f\u59cb\u4ea4\u4e92\uff0c\u5c06\u5176\u63d0\u70bc\u4e3a\u7d27\u51d1\u3001\u53ef\u590d\u7528\u7684\u77e5\u8bc6\uff0c\u5e76\u5b9a\u671f\u4f18\u5316\u5b58\u50a8\u5185\u5bb9\u4ee5\u4fdd\u6301\u76f8\u5173\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u573a\u666f\uff08\u65b0\u4efb\u52a1\u6cdb\u5316\u3001\u91cd\u590d\u67e5\u8be2\u3001\u77e5\u8bc6\u5e93\u9971\u548c\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u7ecf\u9a8c\u7f29\u653e\u6846\u67b6\u5747\u63d0\u9ad8\u4e86LLM\u7684\u51c6\u786e\u6027\uff0c\u968f\u65f6\u95f4\u63a8\u79fb\u7ef4\u6301\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u5e94\u7528\u4e8e\u65b0\u60c5\u5883\u65f6\u4fdd\u6301\u4e86\u589e\u76ca\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u90e8\u7f72\u540e\u5b66\u4e60\u80fd\u591f\u5c06LLM\u7684\u80fd\u529b\u6269\u5c55\u5230\u9759\u6001\u4eba\u7c7b\u751f\u6210\u6570\u636e\u7684\u9650\u5236\u4e4b\u5916\uff0c\u4e3a\u6301\u7eed\u7684\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18451", "pdf": "https://arxiv.org/pdf/2509.18451", "abs": "https://arxiv.org/abs/2509.18451", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony Maida"], "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects", "categories": ["cs.CV"], "comment": null, "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.", "AI": {"tldr": "\u8ffd\u8e2a\u5feb\u901f\u79fb\u52a8\u7684\u5c0f\u7269\u4f53\uff08\u5982\u58c1\u7403\uff09\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5b9a\u5236\u58c1\u7403\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u65b9\u6cd5\u90fd\u5b58\u5728\u663e\u8457\u8ddf\u8e2a\u6f02\u79fb\uff0c\u8bef\u5dee\u8fdc\u9ad8\u4e8e\u6807\u51c6\u57fa\u51c6\uff0c\u51f8\u663e\u4e86\u5bf9\u4e13\u7528\u8ddf\u8e2a\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "motivation": "\u8ffd\u8e2a\u5feb\u901f\u79fb\u52a8\u4e14\u89c6\u89c9\u6807\u8bb0\u5c0f\u3001\u8fd0\u52a8\u6a21\u5f0f\u4e0d\u53ef\u9884\u6d4b\u7684\u7269\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u96be\u9898\uff0c\u5c24\u5176\u5728\u4f53\u80b2\u673a\u5668\u4eba\u9886\u57df\uff0c\u9700\u8981\u8f7b\u91cf\u548c\u51c6\u786e\u7684\u8ddf\u8e2a\u7cfb\u7edf\u3002\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u901a\u7528\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u5feb\u901f\u3001\u4e0d\u89c4\u5219\u5f39\u8df3\u7684\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86OCSORT\u3001DeepOCSORT\u3001ByteTrack\u3001BoTSORT\u548cStrongSORT\u8fd9\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8ddf\u8e2a\u65b9\u6cd5\u3002\u4f7f\u7528\u4e00\u4e2a\u5305\u542b10,000\u5e27\u5e26\u6807\u6ce8\u58c1\u7403\u56fe\u50cf\uff08720p-1280p\u5206\u8fa8\u7387\uff09\u7684\u5b9a\u5236\u6570\u636e\u96c6\u8fdb\u884c\u3002\u5206\u6790\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u901f\u5ea6\u548c\u6bcf\u56fe\u50cf\u66f4\u65b0\u9891\u7387\u5982\u4f55\u5f71\u54cd\u5feb\u901f\u79fb\u52a8\u5fae\u5c0f\u7269\u4f53\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cDeepOCSORT\u5b9e\u73b0\u4e86\u6700\u4f4e\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5e73\u5747ADE\u4e3a31.15\u50cf\u7d20\uff08ByteTrack\u4e3a114.3\u50cf\u7d20\uff09\u3002ByteTrack\u5904\u7406\u901f\u5ea6\u6700\u5feb\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u4e3a26.6\u6beb\u79d2\uff08DeepOCSORT\u4e3a26.8\u6beb\u79d2\uff09\u3002\u7136\u800c\uff0c\u6240\u6709\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8ddf\u8e2a\u5668\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u8ddf\u8e2a\u6f02\u79fb\uff0c\u7a7a\u95f4\u8bef\u5dee\u57283-11\u5398\u7c73\u4e4b\u95f4\uff08ADE\u503c\uff1a31-114\u50cf\u7d20\uff09\uff0c\u8868\u660e\u5728\u5904\u7406\u5feb\u901f\u79fb\u52a8\u5fae\u5c0f\u7269\u4f53\u7684\u4e0d\u53ef\u9884\u6d4b\u8fd0\u52a8\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u6839\u672c\u5c40\u9650\u3002\u8bef\u5dee\u7387\u6bd4\u6807\u51c6\u7269\u4f53\u8ddf\u8e2a\u57fa\u51c6\u9ad83-4\u500d\u3002", "conclusion": "\u76ee\u524d\u7684\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8ddf\u8e2a\u65b9\u6cd5\u5728\u8ffd\u8e2a\u5feb\u901f\u79fb\u52a8\u7684\u5fae\u5c0f\u7269\u4f53\uff08\u5982\u58c1\u7403\uff09\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u9ad8\u8bef\u5dee\u7387\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u7c7b\u5177\u6709\u6311\u6218\u6027\u7684\u8ddf\u8e2a\u5e94\u7528\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18139", "pdf": "https://arxiv.org/pdf/2509.18139", "abs": "https://arxiv.org/abs/2509.18139", "authors": ["Akshay Murthy", "Shawn Sebastian", "Manil Shangle", "Huaduo Wang", "Sopam Dasgupta", "Gopal Gupta"], "title": "Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification", "categories": ["cs.LG"], "comment": "7 pages", "summary": "Recently, the demand for Machine Learning (ML) models that can balance\naccuracy, efficiency, and interpreability has grown significantly.\nTraditionally, there has been a tradeoff between accuracy and explainability in\npredictive models, with models such as Neural Networks achieving high accuracy\non complex datasets while sacrificing internal transparency. As such, new\nrule-based algorithms such as FOLD-SE have been developed that provide tangible\njustification for predictions in the form of interpretable rule sets. The\nprimary objective of this study was to compare FOLD-SE and FOLD-R++, both\nrule-based classifiers, in binary classification and evaluate how FOLD-SE\nperforms against XGBoost, a widely used ensemble classifier, when applied to\nmulti-category classification. We hypothesized that because FOLD-SE can\ngenerate a condensed rule set in a more explainable manner, it would lose\nupwards of an average of 3 percent in accuracy and F1 score when compared with\nXGBoost and FOLD-R++ in multiclass and binary classification, respectively. The\nresearch used data collections for classification, with accuracy, F1 scores,\nand processing time as the primary performance measures. Outcomes show that\nFOLD-SE is superior to FOLD-R++ in terms of binary classification by offering\nfewer rules but losing a minor percentage of accuracy and efficiency in\nprocessing time; in tasks that involve multi-category classifications, FOLD-SE\nis more precise and far more efficient compared to XGBoost, in addition to\ngenerating a comprehensible rule set. The results point out that FOLD-SE is a\nbetter choice for both binary tasks and classifications with multiple\ncategories. Therefore, these results demonstrate that rule-based approaches\nlike FOLD-SE can bridge the gap between explainability and performance,\nhighlighting their potential as viable alternatives to black-box models in\ndiverse classification tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u89c4\u5219\u57fa\u5206\u7c7b\u5668 FOLD-SE \u4e0e FOLD-R++\uff08\u4e8c\u5143\u5206\u7c7b\uff09\u548c XGBoost\uff08\u591a\u7c7b\u522b\u5206\u7c7b\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0cFOLD-SE \u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u89c4\u5219\u66f4\u5c11\uff0c\u5728\u591a\u7c7b\u522b\u5206\u7c7b\u4e2d\u6bd4 XGBoost \u66f4\u7cbe\u786e\u9ad8\u6548\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u96c6\uff0c\u6709\u6548\u5f25\u5408\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff08\u5982\u795e\u7ecf\u7f51\u7edc\u9ad8\u51c6\u786e\u6027\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff09\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u578b\u89c4\u5219\u57fa\u7b97\u6cd5\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u4f9d\u636e\u3002", "method": "\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u89c4\u5219\u57fa\u5206\u7c7b\u5668 FOLD-SE \u548c FOLD-R++ \u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30 FOLD-SE \u5728\u591a\u7c7b\u522b\u5206\u7c7b\u4e2d\u4e0e\u96c6\u6210\u5206\u7c7b\u5668 XGBoost \u7684\u6027\u80fd\u3002\u7814\u7a76\u5047\u8bbe FOLD-SE \u56e0\u751f\u6210\u66f4\u7cbe\u7b80\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u96c6\uff0c\u5176\u51c6\u786e\u6027\u548c F1 \u5206\u6570\u53ef\u80fd\u6bd4\u5bf9\u7167\u6a21\u578b\u4f4e\u7ea6 3%\u3002\u7814\u7a76\u4f7f\u7528\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u51c6\u786e\u6027\u3001F1 \u5206\u6570\u548c\u5904\u7406\u65f6\u95f4\u4f5c\u4e3a\u4e3b\u8981\u6027\u80fd\u8861\u91cf\u6307\u6807\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFOLD-SE \u5728\u89c4\u5219\u6570\u91cf\u4e0a\u4f18\u4e8e FOLD-R++\uff0c\u540c\u65f6\u4ec5\u727a\u7272\u4e86\u6781\u5c0f\u90e8\u5206\u7684\u51c6\u786e\u6027\u548c\u5904\u7406\u6548\u7387\u3002\u5728\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFOLD-SE \u76f8\u6bd4 XGBoost \u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u786e\u5ea6\u548c\u663e\u8457\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u6613\u4e8e\u7406\u89e3\u7684\u89c4\u5219\u96c6\u3002", "conclusion": "FOLD-SE \u5728\u4e8c\u5143\u548c\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u662f\u66f4\u4f18\u7684\u9009\u62e9\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u50cf FOLD-SE \u8fd9\u6837\u7684\u89c4\u5219\u57fa\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u5f25\u5408\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8868\u660e\u5b83\u4eec\u5728\u5404\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\u662f\u9ed1\u76d2\u6a21\u578b\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.18776", "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86AECBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u3001\u5de5\u7a0b\u548c\u65bd\u5de5\uff08AEC\uff09\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u9ad8\u7ea7\u8ba4\u77e5\u4efb\u52a1\u4e2d\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728AEC\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u5728\u8be5\u4e13\u4e1a\u4e14\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86AECBench\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9a\u4e49\u4e8623\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u6db5\u76d6\u77e5\u8bc6\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a8\u7406\u3001\u8ba1\u7b97\u548c\u5e94\u7528\u4e94\u4e2a\u8ba4\u77e5\u5c42\u9762\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b4,800\u4e2a\u95ee\u9898\u7684\u591a\u683c\u5f0f\u6570\u636e\u96c6\uff0c\u7531\u5de5\u7a0b\u5e08\u521b\u5efa\u5e76\u7ecf\u4e13\u5bb6\u4e24\u8f6e\u8bc4\u5ba1\u9a8c\u8bc1\u3002\u5f15\u5165\u4e86\u201cLLM\u5145\u5f53\u8bc4\u5224\u8005\u201d\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u590d\u6742\u7684\u957f\u7bc7\u56de\u590d\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u4e5d\u4e2aLLMs\uff0c\u7814\u7a76\u53d1\u73b0\u5176\u6027\u80fd\u5728\u4e94\u4e2a\u8ba4\u77e5\u5c42\u9762\u5448\u660e\u663e\u4e0b\u964d\u8d8b\u52bf\u3002\u6a21\u578b\u5728\u77e5\u8bc6\u8bb0\u5fc6\u548c\u7406\u89e3\u7b49\u57fa\u7840\u4efb\u52a1\u4e0a\u8868\u73b0\u719f\u7ec3\uff0c\u4f46\u5728\u89e3\u91ca\u5efa\u7b51\u89c4\u8303\u4e2d\u7684\u8868\u683c\u77e5\u8bc6\u3001\u6267\u884c\u590d\u6742\u63a8\u7406\u548c\u8ba1\u7b97\uff0c\u4ee5\u53ca\u751f\u6210\u7279\u5b9a\u9886\u57df\u6587\u6863\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u7f3a\u9677\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u65e8\u5728\u5c06LLMs\u7a33\u5065\u53ef\u9760\u5730\u96c6\u6210\u5230\u5b89\u5168\u5173\u952e\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u7814\u7a76\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18787", "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "categories": ["cs.AI", "C.2.4"], "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "ADS\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u76ee\u5f55\u670d\u52a1\uff0c\u7528\u4e8e\u53d1\u73b0AI\u4ee3\u7406\u7684\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6765\u6e90\uff0c\u652f\u6301\u8de8\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u548c\u591a\u7ef4\u5ea6\u53d1\u73b0\u3002", "motivation": "\u9700\u8981\u5728\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u5b9e\u73b0AI\u4ee3\u7406\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6765\u6e90\u7684\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u548c\u591a\u7ef4\u5ea6\u53d1\u73b0\u3002", "method": "ADS\u57fa\u4e8eOpen Agentic Schema Framework (OASF)\uff0c\u5229\u7528\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3001\u5206\u5c42\u5206\u7c7b\u548c\u52a0\u5bc6\u7b7e\u540d\u3002\u5b83\u901a\u8fc7\u57fa\u4e8eKademlia\u7684\u5206\u5e03\u5f0f\u54c8\u5e0c\u8868\uff08DHT\uff09\u5b9e\u73b0\u4e24\u7ea7\u6620\u5c04\uff0c\u5c06\u80fd\u529b\u7d22\u5f15\u4e0e\u5185\u5bb9\u4f4d\u7f6e\u89e3\u8026\u3002\u540c\u65f6\uff0c\u5b83\u91cd\u7528OCI/ORAS\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5de5\u4ef6\u5206\u53d1\uff0c\u96c6\u6210Sigstore\u7528\u4e8e\u6eaf\u6e90\uff0c\u5e76\u652f\u6301\u901a\u8fc7Schema\u9a71\u52a8\u7684\u6269\u5c55\u6027\u4ee5\u9002\u5e94\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\u3002", "result": "ADS\u5b9e\u73b0\u4e86AI\u4ee3\u7406\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6765\u6e90\u7684\u5206\u5e03\u5f0f\u3001\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u548c\u591a\u7ef4\u5ea6\u53d1\u73b0\u3002\u5b83\u901a\u8fc7\u67b6\u6784\u6a21\u578b\u3001\u5b58\u50a8\u548c\u53d1\u73b0\u5c42\u3001\u5b89\u5168\u548c\u6027\u80fd\u7279\u6027\u4ee5\u53ca\u5728\u4ee3\u7406\u6ce8\u518c\u548c\u4e92\u64cd\u4f5c\u6027\u9886\u57df\u4e2d\u7684\u5b9a\u4f4d\u5f97\u5230\u4e86\u5f62\u5f0f\u5316\u63cf\u8ff0\u3002", "conclusion": "ADS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5065\u58ee\u4e14\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u76ee\u5f55\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u89e3\u51b3\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7406\u53d1\u73b0\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u6eaf\u6e90\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.18473", "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "categories": ["cs.CV"], "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop\u662f\u4e00\u4e2a\u8fd0\u52a8\u611f\u77e5\u7684\u81ea\u9002\u5e94\u526a\u88c1\u6a21\u5757\uff0c\u5229\u7528H.264\u8fd0\u52a8\u5411\u91cf\u5728\u538b\u7f29\u57df\u4e2d\u9ad8\u6548\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u53ef\u63d2\u62d4\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6027\u80fd\u6216\u964d\u4f4e\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5728\u538b\u7f29\u57df\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u6216\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "method": "MoCrop\u6a21\u5757\u5229\u7528H.264\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u5411\u91cf\u6765\u5b9a\u4f4d\u8fd0\u52a8\u5bc6\u96c6\u533a\u57df\uff0c\u5e76\u751f\u6210\u4e00\u4e2a\u5355\u4e00\u7684\u526a\u8f91\u7ea7\u526a\u88c1\u5e94\u7528\u4e8e\u6240\u6709I\u5e27\u3002\u5b83\u65e0\u9700\u8bad\u7ec3\uff0c\u4e0d\u589e\u52a0\u53c2\u6570\uff0c\u53ef\u4e0e\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u7ed3\u5408\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u7ba1\u9053\uff0c\u5305\u62ec\u53bb\u566a\u4e0e\u5408\u5e76(DM)\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837(MCS)\u548c\u901a\u8fc7\u8fd0\u52a8\u5bc6\u5ea6\u5b50\u77e9\u9635\u641c\u7d22\u7684\u81ea\u9002\u5e94\u526a\u88c1(AC)\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\uff0cMoCrop\u7ed3\u5408ResNet-50\uff0c\u5728\u76f8\u540cFLOPs\u4e0bTop-1\u51c6\u786e\u7387\u63d0\u53473.5%\uff0c\u6216\u5728FLOPs\u51cf\u5c1126.5%\u7684\u60c5\u51b5\u4e0bTop-1\u51c6\u786e\u7387\u63d0\u53472.4%\u3002\u5e94\u7528\u4e8eCoViAR\uff0c\u5728\u539f\u59cb\u6210\u672c\u4e0b\u8fbe\u523089.2%\u7684Top-1\u51c6\u786e\u7387\uff0c\u5e76\u5728\u8ba1\u7b97\u91cf\u4ece11.6 GFLOPs\u964d\u81f38.5 GFLOPs\u65f6\u8fbe\u523088.5%\u7684Top-1\u51c6\u786e\u7387\u3002\u5728MobileNet-V3\u3001EfficientNet-B1\u548cSwin-B\u7b49\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MoCrop\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u6027\uff0c\u4f7f\u5176\u5728\u538b\u7f29\u57df\u4e2d\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e0a\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.18140", "pdf": "https://arxiv.org/pdf/2509.18140", "abs": "https://arxiv.org/abs/2509.18140", "authors": ["Iram Wajahat", "Amritpal Singh", "Fazel Keshtkar", "Syed Ahmad Chan Bukhari"], "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent\na significant global health burden, disproportionately impacting genetically\npredisposed populations such as the Pima Indians (a Native American tribe from\nsouth central Arizona). This study introduces a novel machine learning (ML)\nframework that integrates predictive modeling with gene-agnostic pathway\nmapping to identify high-risk individuals and uncover potential therapeutic\ntargets. Using the Pima Indian dataset, logistic regression and t-tests were\napplied to identify key predictors of T2DM, yielding an overall model accuracy\nof 78.43%. To bridge predictive analytics with biological relevance, we\ndeveloped a pathway mapping strategy that links identified predictors to\ncritical signaling networks, including insulin signaling, AMPK, and PPAR\npathways. This approach provides mechanistic insights without requiring direct\nmolecular data. Building upon these connections, we propose therapeutic\nstrategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1\nmodulators, and phytochemical, further validated through pathway enrichment\nanalyses. Overall, this framework advances precision medicine by offering\ninterpretable and scalable solutions for early detection and targeted\nintervention in metabolic disorders. The key contributions of this work are:\n(1) development of an ML framework combining logistic regression and principal\ncomponent analysis (PCA) for T2DM risk prediction; (2) introduction of a\ngene-agnostic pathway mapping approach to generate mechanistic insights; and\n(3) identification of novel therapeutic strategies tailored for high-risk\npopulations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u548c\u57fa\u56e0\u975e\u7279\u5f02\u6027\u901a\u8def\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u9ad8\u98ce\u9669\u4e2a\u4f53\u5e76\u53d1\u73b0\u6f5c\u5728\u6cbb\u7597\u9776\u70b9\u3002\u901a\u8fc7\u5206\u6790\u76ae\u9a6c\u5370\u7b2c\u5b89\u4eba\u6570\u636e\u96c6\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u6cbb\u7597\u7b56\u7565\uff0c\u4ee5\u63a8\u8fdb\u7cbe\u51c6\u533b\u7597\u3002", "motivation": "\u4ee3\u8c22\u6027\u75be\u75c5\uff0c\u7279\u522b\u662f2\u578b\u7cd6\u5c3f\u75c5\uff08T2DM\uff09\uff0c\u662f\u5168\u7403\u6027\u7684\u91cd\u5927\u5065\u5eb7\u8d1f\u62c5\uff0c\u5bf9\u76ae\u9a6c\u5370\u7b2c\u5b89\u4eba\u7b49\u5177\u6709\u9057\u4f20\u6613\u611f\u6027\u7684\u7fa4\u4f53\u5f71\u54cd\u5c24\u4e3a\u663e\u8457\u3002\u5f53\u524d\u9700\u8981\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u548c\u9776\u5411\u5e72\u9884\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6574\u5408\u9884\u6d4b\u5efa\u6a21\uff08\u91c7\u7528\u903b\u8f91\u56de\u5f52\u548c\u4e3b\u6210\u5206\u5206\u6790\uff0c\u7ed3\u5408t\u68c0\u9a8c\uff09\u4e0e\u57fa\u56e0\u975e\u7279\u5f02\u6027\u901a\u8def\u56fe\u8c31\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002\u5229\u7528\u76ae\u9a6c\u5370\u7b2c\u5b89\u4eba\u6570\u636e\u96c6\u8bc6\u522b2\u578b\u7cd6\u5c3f\u75c5\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002\u6784\u5efa\u901a\u8def\u56fe\u8c31\u7b56\u7565\uff0c\u5c06\u8bc6\u522b\u51fa\u7684\u9884\u6d4b\u56e0\u5b50\u4e0e\u80f0\u5c9b\u7d20\u4fe1\u53f7\u3001AMPK\u548cPPAR\u7b49\u5173\u952e\u4fe1\u53f7\u7f51\u7edc\u5173\u8054\u3002\u63d0\u51fa\u7684\u6cbb\u7597\u7b56\u7565\u901a\u8fc7\u901a\u8def\u5bcc\u96c6\u5206\u6790\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6a21\u578b\u57282\u578b\u7cd6\u5c3f\u75c5\u98ce\u9669\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e8678.43%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u6210\u529f\u8bc6\u522b\u4e86\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff0c\u5e76\u901a\u8fc7\u901a\u8def\u56fe\u8c31\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89c1\u89e3\uff0c\u5c06\u5176\u4e0e\u80f0\u5c9b\u7d20\u4fe1\u53f7\u3001AMPK\u548cPPAR\u7b49\u901a\u8def\u8054\u7cfb\u8d77\u6765\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u5982\u53ccGLP-1/GIP\u53d7\u4f53\u6fc0\u52a8\u5242\u3001AMPK\u6fc0\u6d3b\u5242\u3001SIRT1\u8c03\u8282\u5242\u548c\u690d\u7269\u5316\u5b66\u7269\u7b49\u6f5c\u5728\u6cbb\u7597\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u9776\u5411\u5e72\u9884\u65b9\u6848\uff0c\u63a8\u8fdb\u4e86\u4ee3\u8c22\u6027\u75be\u75c5\u7684\u7cbe\u51c6\u533b\u7597\u3002\u5176\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u5f00\u53d1\u7528\u4e8eT2DM\u98ce\u9669\u9884\u6d4b\u7684ML\u6846\u67b6\u3001\u5f15\u5165\u57fa\u56e0\u975e\u7279\u5f02\u6027\u901a\u8def\u56fe\u8c31\u4ee5\u751f\u6210\u673a\u5236\u6027\u89c1\u89e3\uff0c\u4ee5\u53ca\u4e3a\u9ad8\u98ce\u9669\u4eba\u7fa4\u8bc6\u522b\u65b0\u9896\u6cbb\u7597\u7b56\u7565\u3002"}}
{"id": "2509.18792", "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "categories": ["cs.CL"], "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u6a21\u578b\u5dee\u5f02\u5206\u6790\uff08model diffing\uff09\u65b9\u6cd5\uff0c\u6df1\u5165\u5256\u6790\u4e86Gemma-2-9b-it\u53ca\u5176SimPO\u589e\u5f3a\u7248\u672c\u4e4b\u95f4\u7684\u5177\u4f53\u80fd\u529b\u5dee\u5f02\uff0c\u53d1\u73b0SimPO\u4e3b\u8981\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3001\u591a\u8bed\u8a00\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\uff0c\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u81ea\u6307\u548c\u5e7b\u89c9\u7ba1\u7406\u3002", "motivation": "\u968f\u7740\u5fae\u8c03\u6210\u4e3a\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u7406\u89e3\u6b64\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u65e0\u6cd5\u89e3\u91ca\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u7684\u539f\u56e0\u3002", "method": "\u91c7\u7528\u6a21\u578b\u5dee\u5f02\u5206\u6790\uff08\u4e00\u79cd\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff09\uff0c\u5e76\u5229\u7528crosscoders\u8bc6\u522b\u548c\u5206\u7c7b\u533a\u5206\u4e24\u4e2a\u6a21\u578b\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u4ee5\u5206\u6790Gemma-2-9b-it\u53ca\u5176SimPO\u589e\u5f3a\u53d8\u4f53\u4e4b\u95f4\u7684\u7279\u5b9a\u80fd\u529b\u5dee\u5f02\u3002", "result": "SimPO\u83b7\u5f97\u7684\u6f5c\u5728\u6982\u5ff5\u4e3b\u8981\u589e\u5f3a\u4e86\u5b89\u5168\u673a\u5236\uff08+32.8%\uff09\u3001\u591a\u8bed\u8a00\u80fd\u529b\uff08+43.8%\uff09\u548c\u6307\u4ee4\u9075\u5faa\uff08+151.7%\uff09\uff0c\u540c\u65f6\u5176\u989d\u5916\u7684\u8bad\u7ec3\u4e5f\u964d\u4f4e\u4e86\u5bf9\u6a21\u578b\u81ea\u6307\uff08-44.1%\uff09\u548c\u5e7b\u89c9\u7ba1\u7406\uff08-68.5%\uff09\u7684\u91cd\u89c6\u3002", "conclusion": "\u6a21\u578b\u5dee\u5f02\u5206\u6790\u80fd\u591f\u63d0\u4f9b\u8d85\u8d8a\u6392\u884c\u699c\u6307\u6807\u7684\u7ec6\u7c92\u5ea6\u6d1e\u5bdf\uff0c\u5c06\u6027\u80fd\u5dee\u8ddd\u5f52\u56e0\u4e8e\u5177\u4f53\u7684\u673a\u68b0\u80fd\u529b\uff0c\u4e3aLLM\u7684\u6bd4\u8f83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u6709\u9488\u5bf9\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2509.18836", "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "categories": ["cs.AI"], "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "LLMCHECKER\u662f\u4e00\u79cd\u57fa\u4e8ePCTL\u6a21\u578b\u68c0\u67e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u5c5e\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5f15\u5165\u03b1-k\u8fb9\u754c\u6587\u672c\u751f\u6210\u6765\u9650\u5236\u5173\u6ce8\u7684token\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684PCTL\uff08\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91\uff09\u5c5e\u6027\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u80fd\u529b\uff1b\u540c\u65f6\uff0c\u7ecf\u9a8c\u89c2\u5bdf\u53d1\u73b0LLM\u5728\u751f\u6210\u65f6\u901a\u5e38\u53ea\u9009\u62e9\u6709\u9650\u4e14\u975e\u56fa\u5b9a\u7684token\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LLMCHECKER\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u7ecf\u9a8c\u6d1e\u5bdf\uff0c\u5f15\u5165\u4e86\u201c\u03b1-k\u8fb9\u754c\u6587\u672c\u751f\u6210\u201d\uff0c\u5c06\u9a8c\u8bc1\u7126\u70b9\u9650\u5b9a\u5728\u6bcf\u4e00\u6b65\u6587\u672c\u751f\u6210\u4e2d\u524dk\u4e2atoken\u4e2d\u7d2f\u79ef\u6982\u7387\u8fbe\u5230\u03b1\u6700\u5927\u503c\u7684token\u3002LLMCHECKER\u901a\u8fc7\u8003\u8651\u521d\u59cb\u5b57\u7b26\u4e32\u548c\u540e\u7eed\u7684\u524dk\u4e2atoken\uff0c\u5e76\u7ed3\u5408\u591a\u6837\u5316\u7684\u6587\u672c\u91cf\u5316\u65b9\u6cd5\uff08\u5982\u6587\u672c\u8d28\u91cf\u548c\u504f\u89c1\u8bc4\u4f30\uff09\uff0c\u6765\u5f62\u5f0f\u5316\u9a8c\u8bc1\u03b1-k\u8fb9\u754cLLM\u7684PCTL\u5c5e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5df2\u6210\u529f\u5e94\u7528\u4e8eLlama\u3001Gemma\u3001Mistral\u3001Genstruct\u548cBERT\u7b49\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u9002\u7528\u6027\u3002", "conclusion": "LLMCHECKER\u9996\u6b21\u5c06\u57fa\u4e8ePCTL\u7684\u6a21\u578b\u68c0\u67e5\u5e94\u7528\u4e8e\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u9014\u5f84\u3002"}}
{"id": "2509.18481", "pdf": "https://arxiv.org/pdf/2509.18481", "abs": "https://arxiv.org/abs/2509.18481", "authors": ["Xinyu Wang", "Zikun Zhou", "Yingjian Li", "Xin An", "Hongpeng Wang"], "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems", "categories": ["cs.CV"], "comment": null, "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAFC-SE\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u4fa7\u4f7f\u7528\u77e2\u91cf\u91cf\u5316\uff08VQ\uff09\u548c\u7801\u672c\u5c06\u8fde\u7eed\u89c6\u89c9\u7279\u5f81\u538b\u7f29\u4e3a\u79bb\u6563\u7d22\u5f15\uff0c\u4ee5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u63d0\u5347\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u673a\u5668\u5206\u6790\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u548c\u7279\u5f81\u538b\u7f29\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u4fdd\u7559\u5197\u4f59\u7ec6\u8282\u6216\u5b66\u4e60\u5230\u8fc7\u96c6\u4e2d\u7684\u7b26\u53f7\u5206\u5e03\uff0c\u9650\u5236\u4e86\u673a\u5668\u5206\u6790\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51faCodebook-based Adaptive Feature Compression framework with Semantic Enhancement (CAFC-SE)\u3002\u5b83\u5728\u8fb9\u7f18\u4fa7\u901a\u8fc7\u77e2\u91cf\u91cf\u5316\uff08VQ\uff09\u4f7f\u7528\u7801\u672c\u5c06\u8fde\u7eed\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u4e3a\u79bb\u6563\u7d22\u5f15\uff0c\u5e76\u9009\u62e9\u6027\u5730\u4f20\u8f93\u5230\u4e91\u7aef\u3002VQ\u64cd\u4f5c\u80fd\u591f\u5c06\u7279\u5f81\u5411\u91cf\u6295\u5c04\u5230\u6700\u8fd1\u7684\u89c6\u89c9\u57fa\u5143\u4e0a\uff0c\u4ece\u800c\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u4fdd\u7559\u66f4\u591a\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89c9\u6a21\u5f0f\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCAFC-SE\u5728\u7801\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAFC-SE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u673a\u5668\u5206\u6790\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u7279\u5f81\u538b\u7f29\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18141", "pdf": "https://arxiv.org/pdf/2509.18141", "abs": "https://arxiv.org/abs/2509.18141", "authors": ["Yao Zhao", "Haoyue Sun", "Yantian Ding", "Yanxun Xu"], "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "stat.ML"], "comment": null, "summary": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots\nprovides valuable insights for evidence synthesis in clinical research.\nHowever, existing approaches often rely on manual digitization, which is\nerror-prone and lacks scalability. To address these limitations, we develop\nKM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD\ndirectly from KM plots with high accuracy, robustness, and reproducibility.\nKM-GPT integrates advanced image preprocessing, multi-modal reasoning powered\nby GPT-5, and iterative reconstruction algorithms to generate high-quality IPD\nwithout manual input or intervention. Its hybrid reasoning architecture\nautomates the conversion of unstructured information into structured data flows\nand validates data extraction from complex KM plots. To improve accessibility,\nKM-GPT is equipped with a user-friendly web interface and an integrated AI\nassistant, enabling researchers to reconstruct IPD without requiring\nprogramming expertise. KM-GPT was rigorously evaluated on synthetic and\nreal-world datasets, consistently demonstrating superior accuracy. To\nillustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer\nimmunotherapy trials, reconstructing IPD to facilitate evidence synthesis and\nbiomarker-based subgroup analyses. By automating traditionally manual processes\nand providing a scalable, web-based solution, KM-GPT transforms clinical\nresearch by leveraging reconstructed IPD to enable more informed downstream\nanalyses, supporting evidence-based decision-making.", "AI": {"tldr": "KM-GPT\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u3001AI\u9a71\u52a8\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u9ad8\u51c6\u786e\u3001\u9ad8\u9c81\u68d2\u6027\u5730\u4eceKaplan-Meier\u56fe\u4e2d\u91cd\u5efa\u4e2a\u4f53\u60a3\u8005\u6570\u636e\uff08IPD\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u624b\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4eceKaplan-Meier\u56fe\u91cd\u5efaIPD\u5bf9\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u8bc1\u636e\u5408\u6210\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6570\u5b57\u5316\uff0c\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "KM-GPT\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u5148\u8fdb\u56fe\u50cf\u9884\u5904\u7406\u3001\u57fa\u4e8eGPT-5\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u8fed\u4ee3\u91cd\u5efa\u7b97\u6cd5\u7684\u5168\u81ea\u52a8AI\u7ba1\u9053\u3002\u5b83\u91c7\u7528\u6df7\u5408\u63a8\u7406\u67b6\u6784\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u53cb\u597d\u7684\u7f51\u9875\u754c\u9762\u548cAI\u52a9\u624b\u63d0\u9ad8\u53ef\u8bbf\u95ee\u6027\u3002", "result": "KM-GPT\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3002\u5b83\u88ab\u6210\u529f\u5e94\u7528\u4e8e\u80c3\u764c\u514d\u75ab\u6cbb\u7597\u8bd5\u9a8c\u7684\u835f\u8403\u5206\u6790\uff0c\u4ee5\u4fc3\u8fdb\u8bc1\u636e\u5408\u6210\u548c\u57fa\u4e8e\u751f\u7269\u6807\u5fd7\u7269\u7684\u4e9a\u7ec4\u5206\u6790\u3002", "conclusion": "KM-GPT\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u7edf\u624b\u52a8\u6d41\u7a0b\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7f51\u9875\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u91cd\u5efa\u7684IPD\u8d4b\u80fd\u66f4\u660e\u667a\u7684\u4e0b\u6e38\u5206\u6790\uff0c\u4ece\u800c\u6539\u53d8\u4e34\u5e8a\u7814\u7a76\u5e76\u652f\u6301\u5faa\u8bc1\u51b3\u7b56\u3002"}}
{"id": "2509.18813", "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "MAPEX\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u7b56\u7565\uff08\u9488\u5bf9\u4e0d\u540c\u6587\u6863\u957f\u5ea6\uff09\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u3001\u57fa\u4e8e\u63d0\u793a\u8bcd\u7684LLM\u5173\u952e\u8bcd\u62bd\u53d6\u65b9\u6cd5\u591a\u91c7\u7528\u5355\u4e00\u63a8\u7406\u6d41\u7a0b\u548c\u7edf\u4e00\u63d0\u793a\u8bcd\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u573a\u666f\u4e0b\u5173\u952e\u8bcd\u62bd\u53d6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMAPEX\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5f15\u5165\u5173\u952e\u8bcd\u62bd\u53d6\u3002MAPEX\u901a\u8fc7\u4e13\u5bb6\u62db\u52df\u3001\u5019\u9009\u62bd\u53d6\u3001\u4e3b\u9898\u5f15\u5bfc\u3001\u77e5\u8bc6\u589e\u5f3a\u548c\u540e\u5904\u7406\u6a21\u5757\u534f\u8c03LLM\u667a\u80fd\u4f53\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u53cc\u8def\u5f84\u7b56\u7565\uff1a\u77ed\u6587\u672c\u77e5\u8bc6\u9a71\u52a8\u62bd\u53d6\uff0c\u957f\u6587\u672c\u4e3b\u9898\u5f15\u5bfc\u62bd\u53d6\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAPEX\u6cdb\u5316\u6027\u548c\u901a\u7528\u6027\u5f3a\uff0c\u5e73\u5747F1@5\u6307\u6807\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u65b9\u6cd52.44%\uff0c\u8d85\u8d8a\u6807\u51c6LLM\u57fa\u7ebf4.01%\u3002", "conclusion": "MAPEX\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u52a8\u6001\u9002\u5e94\u6587\u6863\u957f\u5ea6\u7684\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u62bd\u53d6\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.18846", "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u5219\u6027\u6a21\u578b\u9009\u62e9\u3001\u53bb\u5197\u4f59\u6570\u636e\u91c7\u6837\u548c\u7ed3\u6784\u5316\u8f93\u5165\u8bbe\u8ba1\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728ICD-10-CM\u7f16\u7801\u9884\u6d4b\u4e2d\u7684\u6311\u6218\u3002", "motivation": "ICD\u7f16\u7801\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u73b0\u6709LLM\u5728\u57fa\u6a21\u578b\u9009\u62e9\u3001\u4e0a\u4e0b\u6587\u5904\u7406\u548c\u8bad\u7ec3\u6570\u636e\u5197\u4f59\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u81ea\u52a8\u5316\u7f16\u7801\u7684\u6709\u6548\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\uff1a1. \u57fa\u4e8e\u201cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u201d\u534f\u8bae\u548cPlackett-Luce\u805a\u5408\u8bc4\u4f30\u548c\u6392\u540d\u5f00\u6e90LLM\uff1b2. \u5f15\u5165\u57fa\u4e8e\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u53bb\u5197\u4f59\u91c7\u6837\u7b56\u7565\u6765\u6e05\u7406\u6570\u636e\uff1b3. \u5229\u7528\u7ed3\u6784\u5316\u51fa\u9662\u6458\u8981\u8bbe\u8ba1\u8f93\u5165\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u4e34\u5e8a\u90e8\u5206\u7684\u4e0a\u4e0b\u6587\u6548\u5e94\u3002", "result": "\u7ecf\u8fc7\u5fae\u8c03\u7684\u57fa\u6a21\u578b\u5728\u5185\u90e8\u548c\u5916\u90e8\u8bc4\u4f30\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebfLLM\u3002\u7eb3\u5165\u66f4\u591a\u4e34\u5e8a\u90e8\u5206\u80fd\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u539f\u5219\u7684ICD-10-CM\u4ee3\u7801\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u673a\u6784\u5c31\u7eea\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.18493", "pdf": "https://arxiv.org/pdf/2509.18493", "abs": "https://arxiv.org/abs/2509.18493", "authors": ["Md Mostafijur Rahman", "Radu Marculescu"], "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD", "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMK-UNet\uff0c\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u3001\u591a\u6838U\u578b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u6838\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u591a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\uff0c\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u8d44\u6e90\u5b9e\u73b0\u4e86\u8d85\u8d8aSOTA\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\uff08\u5982\u5373\u65f6\u533b\u7597\u8bbe\u5907\uff09\u4e2d\uff0c\u5bf9\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u533b\u5b66\u8bca\u65ad\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u8d85\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165MK-UNet\u7f51\u7edc\uff0c\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\uff1a1) \u591a\u6838\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5757\uff08MKDC\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u591a\u4e2a\u6838\u5904\u7406\u56fe\u50cf\u5e76\u6355\u83b7\u590d\u6742\u7684\u591a\u5206\u8fa8\u7387\u7a7a\u95f4\u5173\u7cfb\uff1b2) \u7cbe\u5bc6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5305\u62ec\u901a\u9053\u3001\u7a7a\u95f4\u548c\u5206\u7ec4\u95e8\u63a7\u6ce8\u610f\u529b\uff0c\u4ee5\u5f3a\u8c03\u56fe\u50cf\u7684\u663e\u8457\u7279\u5f81\u3002", "result": "MK-UNet\u5177\u6709\u6781\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff08\u4ec50.316M\u53c2\u6570\u548c0.314G FLOPs\uff09\uff0c\u4f46\u5728\u516d\u4e2a\u4e8c\u5143\u533b\u5b66\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4SOTA\u65b9\u6cd5\uff08\u5982TransUNet\u3001UNeXt\u3001MedT\u7b49\uff09\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5206\u5272\u7cbe\u5ea6\u3002\u4f8b\u5982\uff0c\u5b83\u4ee5\u8fdc\u5c11\u4e8eTransUNet\u7684\u53c2\u6570\u548cFLOPs\u83b7\u5f97\u66f4\u9ad8\u7684DICE\u5206\u6570\uff0c\u5e76\u5728\u53c2\u6570\u91cf\u5927\u5e45\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0cDICE\u5206\u6570\u6bd4UNeXt\u63d0\u9ad8\u9ad8\u8fbe6.7%\u3002", "conclusion": "MK-UNet\u5728\u6027\u80fd\u4e0a\u7684\u98de\u8dc3\u548c\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u5176\u6210\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8fdb\u884c\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u533b\u5b66\u8bca\u65ad\uff08\u5982\u5373\u65f6\u533b\u7597\u8bbe\u5907\uff09\u7684\u65e0\u4e0e\u4f26\u6bd4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18144", "pdf": "https://arxiv.org/pdf/2509.18144", "abs": "https://arxiv.org/abs/2509.18144", "authors": ["Yubo Yang", "Yichen Zhu", "Bo Jiang"], "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages", "summary": "Spatio-temporal data abounds in domain like traffic and environmental\nmonitoring. However, it often suffers from missing values due to sensor\nmalfunctions, transmission failures, etc. Recent years have seen continued\nefforts to improve spatio-temporal data imputation performance. Recently\ndiffusion models have outperformed other approaches in various tasks, including\nspatio-temporal imputation, showing competitive performance. Extracting and\nutilizing spatio-temporal dependencies as conditional information is vital in\ndiffusion-based methods. However, previous methods introduce error accumulation\nin this process and ignore the variability of the dependencies in the noisy\ndata at different diffusion steps. In this paper, we propose AdaSTI (Adaptive\nDependency Model in Diffusion-based Spatio-Temporal Imputation), a novel\nspatio-temporal imputation approach based on conditional diffusion model.\nInside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model\nfor pre-imputation with the imputed result used to extract conditional\ninformation by our designed Spatio-Temporal Conditionalizer (STC)network. We\nalso propose a Noise-Aware Spatio-Temporal (NAST) network with a gated\nattention mechanism to capture the variant dependencies across diffusion steps.\nExtensive experiments on three real-world datasets show that AdaSTI outperforms\nexisting methods in all the settings, with up to 46.4% reduction in imputation\nerror.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaSTI\uff0c\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165BiS4PI\u8fdb\u884c\u9884\u63d2\u8865\u548c\u8bbe\u8ba1Noise-Aware Spatio-Temporal (NAST)\u7f51\u7edc\u6765\u6355\u6349\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u4e0b\u566a\u58f0\u6570\u636e\u4e2d\u4f9d\u8d56\u5173\u7cfb\u7684\u53d8\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63d2\u8865\u6027\u80fd\u3002", "motivation": "\u65f6\u7a7a\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u7b49\u539f\u56e0\u5b58\u5728\u7f3a\u5931\u503c\u3002\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u53d6\u548c\u5229\u7528\u65f6\u7a7a\u4f9d\u8d56\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u606f\u65f6\uff0c\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u5ffd\u7565\u4e86\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u4e0b\u566a\u58f0\u6570\u636e\u4e2d\u4f9d\u8d56\u5173\u7cfb\u7684\u53d8\u5f02\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86AdaSTI (Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation) \u65b9\u6cd5\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u63d2\u8865\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5305\u542b\uff1a1) \u4e00\u4e2a\u57fa\u4e8e\u53cc\u5411S4\u6a21\u578b\u7684BiS4PI\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u63d2\u8865\uff1b2) \u4e00\u4e2aSpatio-Temporal Conditionalizer (STC) \u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u9884\u63d2\u8865\u7ed3\u679c\u4e2d\u63d0\u53d6\u6761\u4ef6\u4fe1\u606f\uff1b3) \u4e00\u4e2a\u5e26\u6709\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u7684Noise-Aware Spatio-Temporal (NAST) \u7f51\u7edc\uff0c\u7528\u4e8e\u6355\u6349\u8de8\u6269\u6563\u6b65\u9aa4\u7684\u53d8\u5f02\u4f9d\u8d56\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAdaSTI\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d2\u8865\u8bef\u5dee\u6700\u591a\u51cf\u5c11\u4e8646.4%\u3002", "conclusion": "AdaSTI\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5904\u7406\u65f6\u7a7a\u4f9d\u8d56\u548c\u566a\u58f0\u53d8\u5f02\u6027\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u5931\u503c\u63d2\u8865\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18843", "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5f00\u653e\u6743\u91cdLLMs\u4e0e\u4e13\u6709LLMs\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f00\u653e\u6743\u91cd\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u7279\u522b\u662f\u5728\u96c6\u6210\u7b56\u7565\u4e0b\u6709\u65f6\u80fd\u8d85\u8d8a\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5f00\u653e\u6743\u91cdLLMs\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u6027\u80fd\u5df2\u63a5\u8fd1\u4e13\u6709LLMs\uff0c\u56e0\u6b64\u5f15\u53d1\u4e86\u5c0f\u578b\u5f00\u653e\u6743\u91cdLLMs\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u5927\u578b\u95ed\u6e90\u6a21\u578b\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u7279\u522b\u5173\u6ce8\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u3002", "method": "\u53c2\u4e0e\u4e86BioASQ\u6311\u6218\u8d5b\u7684Task 13B Phase B\uff0c\u5c06\u591a\u4e2a\u5f00\u653e\u6743\u91cd\u6a21\u578b\u4e0eGPT-4o\u3001GPT-4.1\u3001Claude 3.5 Sonnet\u548cClaude 3.7 Sonnet\u7b49\u9876\u7ea7\u4e13\u6709\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\u3002\u91c7\u7528\u7684\u6280\u672f\u5305\u62ec\u57fa\u4e8e\u5d4c\u5165\u8ddd\u79bb\u68c0\u7d22\u76f8\u5173\u7247\u6bb5\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u4ee5\u53ca\u9488\u5bf9\u7cbe\u786e\u7b54\u6848\u95ee\u9898\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u653e\u6743\u91cdLLMs\u4e0e\u4e13\u6709\u6a21\u578b\u5177\u6709\u53ef\u6bd4\u6027\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u5e94\u7528\u96c6\u6210\u7b56\u7565\u65f6\uff0c\u5f00\u653e\u6743\u91cdLLMs\u751a\u81f3\u8d85\u8d8a\u4e86\u5b83\u4eec\u7684\u95ed\u6e90\u5bf9\u5e94\u6a21\u578b\u3002", "conclusion": "\u5f00\u653e\u6743\u91cdLLMs\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u80fd\u591f\u4e0e\u4e13\u6709LLMs\u7ade\u4e89\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u7b49\u65b9\u6cd5\uff0c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u66ff\u4ee3\u5927\u578b\u95ed\u6e90\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18849", "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6df7\u5408\u4f18\u52bf\u7b56\u7565\u4f18\u5316\uff08MAPO\uff09\uff0c\u901a\u8fc7\u8003\u8651\u8f68\u8ff9\u786e\u5b9a\u6027\u5e76\u52a8\u6001\u8c03\u6574\u4f18\u52bf\u51fd\u6570\uff0c\u89e3\u51b3\u4e86GRPO\u4e2d\u4f18\u52bf\u51fd\u6570\u5206\u914d\u4e0d\u5408\u7406\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1GRPO\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u5176\u6838\u5fc3\u673a\u5236\u2014\u2014\u4f18\u52bf\u51fd\u6570\u5728\u5206\u914d\u8f68\u8ff9\u91cd\u8981\u6027\u65f6\u5b58\u5728\u201c\u4f18\u52bf\u53cd\u8f6c\u201d\u548c\u201c\u4f18\u52bf\u955c\u50cf\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u4f18\u52bf\u5206\u914d\u4e0d\u5408\u7406\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u672c\u6587\u63d0\u51faMAPO\u7b56\u7565\uff0c\u901a\u8fc7\u63ed\u793a\u8f68\u8ff9\u5177\u6709\u4e0d\u540c\u786e\u5b9a\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u9488\u5bf9\u9ad8\u786e\u5b9a\u6027\u8f68\u8ff9\u6837\u672c\uff0c\u63d0\u51fa\u4f18\u52bf\u767e\u5206\u6bd4\u504f\u5dee\uff1b\u5e76\u6839\u636e\u8f68\u8ff9\u786e\u5b9a\u6027\u52a8\u6001\u5730\u91cd\u65b0\u52a0\u6743\u4f18\u52bf\u51fd\u6570\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u914d\u7f6e\u4f18\u52bf\u51fd\u6570\u6765\u9002\u5e94\u6837\u672c\u7279\u5f02\u6027\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6bd4\u8f83\u4ee5\u53ca\u5bf9\u4e0d\u540c\u4f18\u52bf\u53d8\u4f53\u7684\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86MAPO\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAPO\u901a\u8fc7\u5bf9GRPO\u4e2d\u4f18\u52bf\u51fd\u6570\u7684\u521b\u65b0\u6027\u6539\u8fdb\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f18\u52bf\u5206\u914d\u96be\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.18501", "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "BridgeSplat\u662f\u4e00\u79cd\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c063D\u9ad8\u65af\u51fd\u6570\u7ed1\u5b9a\u5230CT\u7f51\u683c\u5e76\u8fdb\u884c\u5149\u5ea6\u76d1\u7763\u4f18\u5316\uff0c\u5b9e\u73b0\u672f\u4e2d\u89c6\u9891\u4e0e\u672f\u524dCT\u6570\u636e\u7684\u8054\u52a8\uff0c\u4ece\u800c\u4ece\u5355\u76eeRGB\u6570\u636e\u83b7\u5f97CT\u7684\u5408\u7406\u5f62\u53d8\u3002", "motivation": "\u89e3\u51b3\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\u4e2d\uff0c\u672f\u4e2d\u89c6\u9891\u6570\u636e\u4e0e\u672f\u524d\u4f53\u79ef\u60a3\u8005\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u7684\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u4e24\u8005\u6709\u6548\u8026\u5408\u3002", "method": "\u5c063D\u9ad8\u65af\u51fd\u6570\u7ed1\u5b9a\u5230CT\u7f51\u683c\u4e0a\uff0c\u901a\u8fc7\u5149\u5ea6\u76d1\u7763\u8054\u5408\u4f18\u5316\u9ad8\u65af\u53c2\u6570\u548c\u7f51\u683c\u5f62\u53d8\u3002\u6bcf\u4e2a\u9ad8\u65af\u51fd\u6570\u76f8\u5bf9\u4e8e\u5176\u7236\u7f51\u683c\u4e09\u89d2\u5f62\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u786e\u4fdd\u9ad8\u65af\u51fd\u6570\u4e0e\u7f51\u683c\u5bf9\u9f50\uff0c\u5e76\u4f7f\u5f62\u53d8\u80fd\u591f\u53cd\u5411\u4f20\u64ad\u4ee5\u66f4\u65b0CT\u6570\u636e\u3002", "result": "\u5728\u5185\u810f\u732a\u624b\u672f\u548c\u6a21\u62df\u4eba\u7c7b\u809d\u810f\u7684\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86BridgeSplat\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5730\u4ece\u5355\u76eeRGB\u6570\u636e\u4e2d\u83b7\u5f97\u4e86\u672f\u524dCT\u7684\u5408\u7406\u5f62\u53d8\u3002", "conclusion": "BridgeSplat\u901a\u8fc7\u8026\u5408\u672f\u4e2d3D\u91cd\u5efa\u4e0e\u672f\u524dCT\u6570\u636e\uff0c\u5f25\u5408\u4e86\u624b\u672f\u89c6\u9891\u4e0e\u4f53\u79ef\u60a3\u8005\u6570\u636e\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.18145", "pdf": "https://arxiv.org/pdf/2509.18145", "abs": "https://arxiv.org/abs/2509.18145", "authors": ["Syed Ahmad Chan Bukhari", "Amritpal Singh", "Shifath Hossain", "Iram Wajahat"], "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 3 Figure", "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping\nsigns of physiological deterioration that require timely escalation of care.\nTraditional early warning systems, such as SOFA or MEWS, are limited by their\nfocus on single outcomes and fail to capture the multi-dimensional nature of\nclinical decline. This study proposes a multi-label classification framework to\npredict Care Escalation Triggers (CETs), including respiratory failure,\nhemodynamic instability, renal compromise, and neurological deterioration,\nusing the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are\ndefined through rule-based criteria applied to data from hours 24 to 72 (for\nexample, oxygen saturation below 90, mean arterial pressure below 65 mmHg,\ncreatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale\nscore greater than 2). Features are extracted from the first 24 hours and\ninclude vital sign aggregates, laboratory values, and static demographics. We\ntrain and evaluate multiple classification models on a cohort of 85,242 ICU\nstays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation\nmetrics include per-label precision, recall, F1-score, and Hamming loss.\nXGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,\n0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,\noutperforming baseline models. Feature analysis shows that clinically relevant\nparameters such as respiratory rate, blood pressure, and creatinine are the\nmost influential predictors, consistent with the clinical definitions of the\nCETs. The proposed framework demonstrates practical potential for early,\ninterpretable clinical alerts without requiring complex time-series modeling or\nnatural language processing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u5229\u7528ICU\u60a3\u8005\u524d24\u5c0f\u65f6\u6570\u636e\uff0c\u65e9\u671f\u9884\u6d4b\u591a\u79cd\u62a4\u7406\u5347\u7ea7\u89e6\u53d1\u56e0\u7d20\uff08\u5982\u547c\u5438\u8870\u7aed\u3001\u8840\u6d41\u52a8\u529b\u5b66\u4e0d\u7a33\u5b9a\u7b49\uff09\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u9884\u8b66\u7cfb\u7edf\u53ea\u5173\u6ce8\u5355\u4e00\u7ed3\u679c\u7684\u5c40\u9650\u6027\u3002", "motivation": "ICU\u60a3\u8005\u5e38\u8868\u73b0\u51fa\u590d\u6742\u3001\u91cd\u53e0\u7684\u751f\u7406\u6076\u5316\u8ff9\u8c61\uff0c\u9700\u8981\u53ca\u65f6\u5347\u7ea7\u62a4\u7406\u3002\u4f20\u7edf\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff08\u5982SOFA\u6216MEWS\uff09\u56e0\u53ea\u5173\u6ce8\u5355\u4e00\u7ed3\u679c\u800c\u53d7\u5230\u9650\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u6076\u5316\u7684\u591a\u7ef4\u6027\u8d28\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\u9884\u6d4b\u62a4\u7406\u5347\u7ea7\u89e6\u53d1\u56e0\u7d20\uff08CETs\uff09\uff0c\u5305\u62ec\u547c\u5438\u8870\u7aed\u3001\u8840\u6d41\u52a8\u529b\u5b66\u4e0d\u7a33\u5b9a\u3001\u80be\u810f\u635f\u5bb3\u548c\u795e\u7ecf\u529f\u80fd\u6076\u5316\u3002CETs\u901a\u8fc7\u5bf9ICU\u5165\u4f4f\u540e24\u81f372\u5c0f\u65f6\u7684\u6570\u636e\u5e94\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u6807\u51c6\u6765\u5b9a\u4e49\uff08\u4f8b\u5982\uff0c\u8840\u6c27\u9971\u548c\u5ea6\u4f4e\u4e8e90%\uff09\u3002\u7279\u5f81\u4eceICU\u5165\u4f4f\u524d24\u5c0f\u65f6\u63d0\u53d6\uff0c\u5305\u62ec\u751f\u547d\u4f53\u5f81\u805a\u5408\u3001\u5b9e\u9a8c\u5ba4\u503c\u548c\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u3002\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\u4e2d\u768485,242\u4f8bICU\u4f4f\u9662\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u6bcf\u4e2a\u6807\u7b7e\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1-\u5206\u6570\u548c\u6c49\u660e\u635f\u5931\u3002XGBoost\u88ab\u9009\u4e3a\u6700\u4f73\u6a21\u578b\u3002", "result": "XGBoost\u4f5c\u4e3a\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\uff0c\u5728\u547c\u5438\u8870\u7aed\u3001\u8840\u6d41\u52a8\u529b\u5b66\u3001\u80be\u810f\u635f\u5bb3\u548c\u795e\u7ecf\u529f\u80fd\u6076\u5316\u65b9\u9762\u7684F1-\u5206\u6570\u5206\u522b\u4e3a0.66\u30010.72\u30010.76\u548c0.62\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u7279\u5f81\u5206\u6790\u8868\u660e\uff0c\u547c\u5438\u9891\u7387\u3001\u8840\u538b\u548c\u808c\u9150\u7b49\u4e34\u5e8a\u76f8\u5173\u53c2\u6570\u662f\u6700\u5177\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u56e0\u5b50\uff0c\u8fd9\u4e0eCETs\u7684\u4e34\u5e8a\u5b9a\u4e49\u4e00\u81f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5c55\u793a\u4e86\u5728\u65e0\u9700\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u6216\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u65e9\u671f\u3001\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u9884\u8b66\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2509.18862", "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "categories": ["cs.CL", "I.2.7; I.2.1"], "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u7279\u5f81\u96c6\u6210\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u4ec5\u5e26\u6765\u5fae\u5c0f\u6027\u80fd\u63d0\u5347\uff080.4-0.5%\uff09\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff084.2\u500d\uff09\uff0c\u8868\u660e\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5df2\u80fd\u9ad8\u6548\u6355\u83b7\u5927\u90e8\u5206\u68c0\u6d4b\u4fe1\u53f7\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0c\u9700\u4e25\u8c28\u9a8c\u8bc1\u591a\u7279\u5f81\u65b9\u6cd5\u80fd\u5426\u663e\u8457\u8d85\u8d8a\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6765\u63d0\u5347AI\u6587\u672c\u68c0\u6d4b\u6548\u679c\uff0c\u56e0\u76f4\u89c9\u8ba4\u4e3a\u7ed3\u5408\u8bed\u4e49\u3001\u53e5\u6cd5\u548c\u7edf\u8ba1\u7279\u5f81\u5e94\u63d0\u4f9b\u4e92\u8865\u4fe1\u53f7\u3002", "method": "\u5b9e\u73b0MHFD\uff08\u591a\u5c42\u6b21\u7279\u5f81\u68c0\u6d4b\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\uff0c\u6574\u5408\u4e86\u57fa\u4e8eDeBERTa\u7684\u8bed\u4e49\u5206\u6790\u3001\u53e5\u6cd5\u5206\u6790\u548c\u7edf\u8ba1\u6982\u7387\u7279\u5f81\u3002", "result": "\u591a\u7279\u5f81\u96c6\u6210\u4ec5\u5e26\u67650.4-0.5%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5374\u4ea7\u751f4.2\u500d\u7684\u8ba1\u7b97\u5f00\u9500\u3002MHFD\u5728\u57df\u5185\u68c0\u6d4b\u8fbe\u523089.7%\u51c6\u786e\u7387\uff0c\u8de8\u57df\u68c0\u6d4b\u4fdd\u630184.2%\u7a33\u5b9a\u6027\u80fd\uff0c\u8f83\u73b0\u6709\u65b9\u6cd5\u67090.4-2.6%\u7684\u9002\u5ea6\u63d0\u5347\u3002", "conclusion": "\u5c3d\u7ba1\u6709\u7406\u8bba\u9884\u671f\uff0c\u4f46\u591a\u7279\u5f81\u96c6\u6210\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u4e0d\u8db3\u4ee5\u62b5\u6d88\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6697\u793a\u73b0\u4ee3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5df2\u9ad8\u6548\u5730\u6355\u83b7\u4e86\u5927\u591a\u6570\u76f8\u5173\u7684\u68c0\u6d4b\u4fe1\u53f7\u3002"}}
{"id": "2509.18864", "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "categories": ["cs.AI"], "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.", "AI": {"tldr": "\u9488\u5bf9LLMs\u5728\u7528\u6237\u753b\u50cf\u4e2d\u7f3a\u4e4f\u57fa\u51c6\u548c\u6807\u7b7e\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faProfileBench\u57fa\u51c6\u548c\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u6846\u67b6Conf-Profile\uff0c\u901a\u8fc7\u6807\u7b7e\u5408\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u753b\u50cf\u7684\u6027\u80fd\u3002", "motivation": "\u7528\u6237\u753b\u50cf\u662f\u7406\u89e3\u7528\u6237\u7684\u6838\u5fc3\u6280\u672f\uff0cLLMs\u5728\u6b64\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u8fdb\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u7528\u6237\u753b\u50cf\u4efb\u52a1\u9762\u4e34\u96be\u4ee5\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u6807\u7b7e\u3001\u4ee5\u53ca\u5f02\u6784\u548c\u566a\u58f0\u7528\u6237\u4fe1\u606f\u5f71\u54cdLLMs\u53ef\u9760\u6027\u7684\u96be\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faProfileBench\uff0c\u4e00\u4e2a\u6e90\u81ea\u771f\u5b9e\u89c6\u9891\u5e73\u53f0\u7684\u5de5\u4e1a\u7ea7\u57fa\u51c6\uff0c\u5305\u542b\u5f02\u6784\u7528\u6237\u6570\u636e\u548c\u7ed3\u6784\u5316\u7684\u753b\u50cf\u5206\u7c7b\u4f53\u7cfb\u3002\u4e3a\u5b9e\u73b0\u65e0\u6807\u7b7e\u4e14\u53ef\u9760\u7684\u7528\u6237\u753b\u50cf\uff0c\u63d0\u51faConf-Profile\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u9996\u5148\uff0c\u5229\u7528\u9ad8\u7ea7LLM\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u63d0\u793a\u5408\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6295\u7968\u548c\u6821\u51c6\u6765\u63d0\u5347\u51c6\u786e\u6027\u548c\u5e73\u8861\u5206\u5e03\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7LLM\u4e2d\u3002\u5176\u6b21\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u96be\u5ea6\u8fc7\u6ee4\u3001\u51c6\u771f\u5b9e\u6807\u7b7e\u6295\u7968\u548c\u5956\u52b1\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cConf-Profile\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728Qwen3-8B\u6a21\u578b\u4e0aF1\u5206\u6570\u63d0\u9ad8\u4e8613.97\u3002", "conclusion": "Conf-Profile\u6846\u67b6\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6807\u7b7e\u5408\u6210\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u7528\u6237\u753b\u50cf\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6807\u7b7e\u548c\u4fe1\u606f\u566a\u58f0\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e3a\u7528\u6237\u753b\u50cf\u9886\u57df\u63d0\u4f9b\u4e86\u6025\u9700\u7684\u57fa\u51c6ProfileBench\u3002"}}
{"id": "2509.18502", "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "categories": ["cs.CV"], "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.", "AI": {"tldr": "\u9488\u5bf9\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u65e0\u6e90\u57df\u9002\u5e94(SFDA)\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u6269\u6563\u5f15\u5bfc\u6807\u7b7e\u4e30\u5bcc(DGLE)\u7684\u65b0\u9896\u4f2a\u6807\u7b7e\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5c11\u91cf\u9ad8\u8d28\u91cf\u521d\u59cb\u4f2a\u6807\u7b7e\u51fa\u53d1\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f20\u64ad\u751f\u6210\u5b8c\u6574\u4e14\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u96c6\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u65e0\u6e90\u57df\u9002\u5e94(SFDA)\u7814\u7a76\u4e0d\u8db3\u3002SFDA\u4e2d\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u56e0\u4f2a\u6807\u7b7e\u96c6\u4e2d\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u5bfc\u81f4\u540c\u65f6\u4f18\u5316\u6240\u6709\u4f2a\u6807\u7b7e\u6781\u5177\u6311\u6218\uff0c\u8fdb\u800c\u9650\u5236\u4e86\u81ea\u8bad\u7ec3\u7684\u6709\u6548\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u6269\u6563\u5f15\u5bfc\u6807\u7b7e\u4e30\u5bcc(DGLE)\u6846\u67b6\u3002\u9996\u5148\uff0c\u901a\u8fc7\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548c\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u7684\u4f2a\u6807\u7b7e\u878d\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u7ec6\u8282\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u83b7\u53d6\u5c11\u91cf\u9ad8\u8d28\u91cf\u521d\u59cb\u4f2a\u6807\u7b7e\u3002\u5176\u6b21\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5f3a\u5927\u7684\u53bb\u566a\u548c\u590d\u6742\u5206\u5e03\u5efa\u6a21\u80fd\u529b\uff0c\u5c06\u8fd9\u4e9b\u4e0d\u5b8c\u6574\u4e14\u5206\u5e03\u4e0d\u89c4\u5219\u7684\u79cd\u5b50\u4f2a\u6807\u7b7e\u4f20\u64ad\u4e3a\u5b8c\u6574\u4e14\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u89c4\u907f\u4e86\u76f4\u63a5\u4f18\u5316\u5b8c\u6574\u4f2a\u6807\u7b7e\u96c6\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6210\u679c\u662f\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18147", "pdf": "https://arxiv.org/pdf/2509.18147", "abs": "https://arxiv.org/abs/2509.18147", "authors": ["Xinyu Mu", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations.", "AI": {"tldr": "ConceptFlow\u662f\u4e00\u79cd\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8ddf\u8e2a\u6982\u5ff5\u5728CNN\u5c42\u95f4\u7684\u6f14\u53d8\u6765\u6a21\u62df\u6a21\u578b\u5185\u90e8\u201c\u601d\u8003\u8def\u5f84\u201d\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u5fe0\u5b9e\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5ffd\u89c6\u4e86CNN\u4e2d\u5355\u4e2a\u6ee4\u6ce2\u5668\u7684\u8bed\u4e49\u4f5c\u7528\u4ee5\u53ca\u6982\u5ff5\u5728\u5c42\u95f4\u7684\u52a8\u6001\u4f20\u64ad\u3002", "method": "\u63d0\u51faConceptFlow\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(i) \u6982\u5ff5\u6ce8\u610f\u529b\uff08\u5c06\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u4e0e\u76f8\u5173\u9ad8\u5c42\u6982\u5ff5\u5173\u8054\uff09\uff0c\u548c (ii) \u6982\u5ff5\u8def\u5f84\uff08\u57fa\u4e8e\u6982\u5ff5\u8f6c\u6362\u77e9\u9635\u91cf\u5316\u6982\u5ff5\u5728\u6ee4\u6ce2\u5668\u95f4\u7684\u4f20\u64ad\u548c\u8f6c\u6362\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eConceptFlow\u4e3a\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u8bed\u4e49\u610f\u4e49\u7684\u6d1e\u5bdf\uff0c\u9a8c\u8bc1\u4e86\u6982\u5ff5\u6ce8\u610f\u529b\u548c\u6982\u5ff5\u8def\u5f84\u5728\u89e3\u91ca\u51b3\u7b56\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ConceptFlow\u901a\u8fc7\u5efa\u6a21\u5206\u5c42\u6982\u5ff5\u8def\u5f84\uff0c\u63d0\u4f9b\u4e86\u5bf9CNN\u5185\u90e8\u903b\u8f91\u7684\u66f4\u6df1\u5c42\u7406\u89e3\uff0c\u5e76\u652f\u6301\u751f\u6210\u66f4\u5fe0\u5b9e\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u89e3\u91ca\u3002"}}
{"id": "2509.18880", "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.", "AI": {"tldr": "DivEye\u662f\u4e00\u4e2a\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u4e2d\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u6ce2\u52a8\u7279\u6027\u3002\u5b83\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u7279\u5f81\u6355\u6349\u4eba\u7c7b\u6587\u672c\u4e0eLLM\u8f93\u51fa\u5728\u8bcd\u6c47\u548c\u7ed3\u6784\u4e0d\u53ef\u9884\u6d4b\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u68c0\u6d4b\u5668\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u6253\u51fbLLM\u5728\u6559\u80b2\u3001\u5546\u4e1a\u3001\u65b0\u95fb\u548c\u793e\u4ea4\u5a92\u4f53\u7b49\u9886\u57df\u88ab\u6ee5\u7528\uff08\u5982\u6563\u5e03\u865a\u5047\u4fe1\u606f\u6216\u6b3a\u9a97\uff09\uff0c\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u53d8\u5f97\u65e5\u76ca\u5fc5\u8981\u3002\u73b0\u6709\u68c0\u6d4b\u5668\u5e38\u4f9d\u8d56token\u7ea7\u4f3c\u7136\u6216\u4e0d\u900f\u660e\u7684\u9ed1\u76d2\u5206\u7c7b\u5668\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u751f\u6210\u6587\u672c\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86DivEye\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u201c\u60ca\u5947\u5ea6\u201d\uff08surprisal\uff09\u7684\u7279\u5f81\u6765\u6355\u6349\u6587\u672c\u4e2d\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u6ce2\u52a8\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4eba\u7c7b\u521b\u4f5c\u6587\u672c\u5728\u8bcd\u6c47\u548c\u7ed3\u6784\u4e0d\u53ef\u9884\u6d4b\u6027\u4e0a\u6bd4LLM\u8f93\u51fa\u5c55\u73b0\u51fa\u66f4\u4e30\u5bcc\u7684\u53d8\u5f02\u6027\u7684\u89c2\u5bdf\uff0c\u901a\u8fc7\u4e00\u7ec4\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u7279\u5f81\u6765\u6355\u6349\u8fd9\u4e00\u4fe1\u53f7\u3002", "result": "DivEye\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u9ad8\u8fbe33.2%\uff0c\u5e76\u4e0e\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002\u5b83\u5bf9\u91ca\u4e49\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u540c\u9886\u57df\u548c\u6a21\u578b\u4e4b\u95f4\u6cdb\u5316\u826f\u597d\uff0c\u5e76\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u65f6\u80fd\u5c06\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe18.7%\u3002", "conclusion": "DivEye\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u6587\u672c\u88ab\u6807\u8bb0\u539f\u56e0\u7684\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\uff0c\u8fd8\u6307\u51fa\u201c\u8282\u594f\u6027\u4e0d\u53ef\u9884\u6d4b\u6027\u201d\u662fLLM\u68c0\u6d4b\u4e2d\u4e00\u4e2a\u5f3a\u5927\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4fe1\u53f7\u3002\u8fd9\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u4e8e\u7406\u89e3LLM\u751f\u6210\u5185\u5bb9\u7684\u72ec\u7279\u7279\u5f81\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18868", "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "categories": ["cs.AI"], "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bb0\u5fc6\u64cd\u4f5c\u6027\u5b9a\u4e49\u3001\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\u3001\u8bb0\u5fc6\u56db\u5143\u7ec4\u3001\u5206\u5c42\u8bc4\u4f30\u534f\u8bae\u548c\u53ef\u5ba1\u8ba1\u7684\u8bb0\u5fc6\u66f4\u65b0\u4e0e\u9057\u5fd8\u6846\u67b6\uff08DMM Gov\uff09\uff0c\u65e8\u5728\u4e3aLLM\u8bb0\u5fc6\u7814\u7a76\u4e0e\u90e8\u7f72\u63d0\u4f9b\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u907f\u514d\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\u5bf9LLM\u8bb0\u5fc6\u80fd\u529b\u8fdb\u884c\u626d\u66f2\u6bd4\u8f83\uff0c\u5e76\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u8bb0\u5fc6\u64cd\u4f5c\u6027\u5b9a\u4e49\uff0c\u540c\u65f6\u89e3\u51b3LLM\u8bb0\u5fc6\u7684\u8bc4\u4f30\u3001\u6cbb\u7406\u3001\u66f4\u65b0\u548c\u9057\u5fd8\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u4e86LLM\u8bb0\u5fc6\u7684\u7edf\u4e00\u64cd\u4f5c\u6027\u5b9a\u4e49\u30022. \u5efa\u7acb\u4e86\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\uff08\u53c2\u6570\u8bb0\u5fc6\u3001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u3001\u5916\u90e8\u8bb0\u5fc6\u3001\u7a0b\u5e8f/\u60c5\u666f\u8bb0\u5fc6\uff09\u548c\u8bb0\u5fc6\u56db\u5143\u7ec4\u30023. \u63d0\u51fa\u4e09\u8bbe\u7f6e\u534f\u8bae\uff08\u4ec5\u53c2\u6570\u3001\u79bb\u7ebf\u68c0\u7d22\u3001\u5728\u7ebf\u68c0\u7d22\uff09\u4ee5\u89e3\u8026\u80fd\u529b\u4e0e\u4fe1\u606f\u53ef\u7528\u6027\u30024. \u6784\u5efa\u4e86\u5206\u5c42\u8bc4\u4f30\uff08\u6db5\u76d6\u53c2\u6570\u3001\u4e0a\u4e0b\u6587\u3001\u5916\u90e8\u548c\u7a0b\u5e8f/\u60c5\u666f\u8bb0\u5fc6\uff09\u30025. \u96c6\u6210\u4e86\u65f6\u95f4\u6cbb\u7406\u3001\u6cc4\u6f0f\u5ba1\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u30026. \u63d0\u51fa\u4e86DMM Gov\u6846\u67b6\uff0c\u534f\u8c03DAPT/TAPT\u3001PEFT\u3001\u6a21\u578b\u7f16\u8f91\uff08\u5982ROME\u3001MEND\u3001MEMIT\u3001SERAC\uff09\u548cRAG\uff0c\u5f62\u6210\u53ef\u5ba1\u8ba1\u7684\u66f4\u65b0\u4e0e\u9057\u5fd8\u95ed\u73af\u3002", "result": "\u672c\u7814\u7a76\u6210\u679c\u4e3a\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684LLM\u8bb0\u5fc6\u7814\u7a76\u4e0e\u90e8\u7f72\u5750\u6807\u7cfb\u7edf\u3002\u63d0\u51fa\u4e86\u56db\u4e2a\u53ef\u6d4b\u8bd5\u7684\u547d\u9898\uff1a\u6700\u5c0f\u53ef\u8bc6\u522b\u6027\u3001\u6700\u5c0f\u8bc4\u4f30\u5361\u3001\u53ef\u9a8c\u8bc1\u9057\u5fd8\u7684\u56e0\u679c\u7ea6\u675f\u7f16\u8f91\u3001\u4ee5\u53ca\u4f55\u65f6\u5c0f\u7a97\u53e3\u91cd\u653e\u7684\u68c0\u7d22\u4f18\u4e8e\u8d85\u957f\u4e0a\u4e0b\u6587\u9605\u8bfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u7edf\u4e00\u7684LLM\u8bb0\u5fc6\u7406\u89e3\u3001\u8bc4\u4f30\u3001\u6cbb\u7406\u548c\u66f4\u65b0\u673a\u5236\uff0c\u4e3aLLM\u8bb0\u5fc6\u7684\u7406\u8bba\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u786e\u4fdd\u4e86\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3001\u53ef\u6bd4\u8f83\u6027\u548c\u53ef\u6cbb\u7406\u6027\u3002"}}
{"id": "2509.18504", "pdf": "https://arxiv.org/pdf/2509.18504", "abs": "https://arxiv.org/abs/2509.18504", "authors": ["Jiaxin Dai", "Xiang Xiang"], "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u53cc\u66f2\u7a7a\u95f4\uff08Poincar\u00e9\u7403\u6a21\u578b\uff09\u4e2d\u8fdb\u884c\u7c97\u5230\u7ec6\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08C2FSCIL\uff09\uff0c\u901a\u8fc7\u53cc\u66f2\u5bf9\u6bd4\u635f\u5931\u3001\u53cc\u66f2\u5168\u8fde\u63a5\u5c42\u548c\u6700\u5927\u71b5\u5206\u5e03\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u7c97\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u7f13\u89e3\u5c11\u6837\u672c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5bf9\u5206\u5c42\u6570\u636e\u8868\u793a\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u53cc\u66f2\u7a7a\u95f4\u5177\u6709\u4f18\u8d8a\u7684\u8868\u793a\u80fd\u529b\u3002\u4e3a\u66f4\u597d\u5730\u89e3\u91ca\u201c\u7c97\u5230\u7ec6\u201d\u8303\u5f0f\uff0c\u5e76\u63d0\u5347\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4f5c\u8005\u63d0\u51fa\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\u4ee5\u89e3\u51b3C2FSCIL\u4efb\u52a1\u3002", "method": "\u8be5\u65b9\u6cd5\u9075\u5faaKnowe\u8303\u5f0f\uff0c\u5373\u5bf9\u6bd4\u5b66\u4e60\u7c97\u7c7b\u522b\u6807\u7b7e\u5e76\u5f52\u4e00\u5316\u51bb\u7ed3\u7ec6\u7c7b\u522b\u5206\u7c7b\u5668\u6743\u91cd\u3002\u5177\u4f53\u5730\uff0c\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u5230Poincar\u00e9\u7403\u6a21\u578b\u4e2d\uff0c\u4f7f\u8f93\u5165\u56fe\u50cf\u7279\u5f81\u5411\u91cf\u843d\u5165\u53cc\u66f2\u7a7a\u95f4\u3002\u5f15\u5165\u4e86\u53cc\u66f2\u5bf9\u6bd4\u635f\u5931\u548c\u53cc\u66f2\u5168\u8fde\u63a5\u5c42\u8fdb\u884c\u4f18\u5316\u548c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u6700\u5927\u71b5\u5206\u5e03\u4f30\u8ba1\u7ec6\u7c7b\u522b\u7279\u5f81\u5411\u91cf\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u751f\u6210\u589e\u5f3a\u7279\u5f81\u4ee5\u7f13\u89e3\u5c11\u6837\u672c\u8bad\u7ec3\u4e2d\u7684\u8fc7\u62df\u5408\u3002", "result": "\u5728C2FSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u7c97\u7c7b\u522b\u548c\u7ec6\u7c7b\u522b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u53cc\u66f2\u5bf9\u6bd4\u635f\u5931\u3001\u53cc\u66f2\u5168\u8fde\u63a5\u5c42\u548c\u6700\u5927\u71b5\u5206\u5e03\u589e\u5f3a\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7c97\u5230\u7ec6\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18150", "pdf": "https://arxiv.org/pdf/2509.18150", "abs": "https://arxiv.org/abs/2509.18150", "authors": ["Kean Shi", "Liang Chen", "Haozhe Zhao", "Baobao Chang"], "title": "Sparse Training Scheme for Multimodal LLM", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b(MLLMs)\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u7a00\u758f\u8bad\u7ec3\u65b9\u6848(STS)\uff0c\u901a\u8fc7\u89c6\u89c9token\u538b\u7f29\u548c\u52a8\u6001\u8df3\u5c42\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u6a21\u578b(MLLMs)\u56e0\u591a\u6a21\u6001\u6570\u636e\u5f15\u5165\u7684\u8d85\u957f\u8f93\u5165\u5e8f\u5217\u548c\u5c42\u95f4\u8ba1\u7b97\u5229\u7528\u7387\u4f4e\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u8bad\u7ec3\u65b9\u6848(STS)\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u89c6\u89c9Token\u538b\u7f29\u5668(Visual Token Compressor)\uff0c\u7528\u4e8e\u538b\u7f29\u89c6\u89c9token\u4ee5\u51cf\u5c11\u4fe1\u606f\u8d1f\u8377\uff1b2) \u5c42\u52a8\u6001\u8df3\u8fc7\u5668(Layer Dynamic Skipper)\uff0c\u7528\u4e8e\u5728\u6b63\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u52a8\u6001\u8df3\u8fc7\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0d\u5fc5\u8981\u7684\u5c42\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cdMLLM\u67b6\u6784\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684STS\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.18901", "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovi\u010d", "Michael F\u00e4rber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com", "AI": {"tldr": "\u672c\u6587\u63d0\u51faJEDI\uff0c\u4e00\u79cd\u4ec5\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u6267\u884c\u62bd\u53d6\u5f0f\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u65e0\u9700\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728NLI\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u51c6\u786e\u6027\u4e0e\u663e\u8457\u7684\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u5e76\u5229\u7528\u5408\u6210\u7406\u7531\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u53ca\u5176\u76f8\u5173\u4efb\u52a1\uff08\u5982\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff09\u4e2d\uff0c\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u6709\u52a9\u4e8e\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u6765\u6267\u884c\u5206\u89e3\u3002", "method": "\u63d0\u51faJEDI\uff0c\u4e00\u79cd\u4ec5\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u65e0\u9700\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u8054\u5408\u6267\u884c\u62bd\u53d6\u5f0f\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u3002\u4e3a\u4fc3\u8fdb\u8bad\u7ec3\uff0c\u7814\u7a76\u8005\u751f\u6210\u4e86\u4e00\u4e2a\u8986\u76d6\u591a\u4e2aNLI\u57fa\u51c6\u7684\u5927\u578b\u5408\u6210\u7406\u7531\u8bed\u6599\u5e93\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cJEDI\u5728\u5206\u5e03\u5185\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5206\u5e03\u5916\u548c\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u4ec5\u4f9d\u8d56\u62bd\u53d6\u5f0f\u7406\u7531\u76d1\u7763\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0cNLI\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u6cdb\u5316\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u4ec5\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u67b6\u6784\u548c\u5408\u6210\u7406\u7531\u6765\u5b9e\u73b0\u3002"}}
{"id": "2509.18883", "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "categories": ["cs.AI"], "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90MoE\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7CoT\u51b7\u542f\u52a8\u548c\u5927\u89c4\u6a21RL\u8bad\u7ec3\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u51cf\u5c1164.5%\u7684token\u6d88\u8017\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u3001\u5f3a\u5927\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u548c\u667a\u80fd\u4f53AI\u65b9\u9762\uff0c\u4ee5\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u91c7\u7528CoT\u6570\u636e\u51b7\u542f\u52a8\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u51b7\u542f\u52a8\u7b56\u7565\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff1b\u57df\u5e76\u884c\u8bad\u7ec3\u65b9\u6848\uff0c\u5c06\u4e0d\u540c\u9886\u57df\uff08\u5982STEM\u3001\u4ee3\u7801\u3001\u667a\u80fd\u4f53\uff09\u7684\u4f18\u5316\u89e3\u8026\u5e76\u878d\u5408\uff1b\u4f7f\u7528DORA\u7cfb\u7edf\uff0c\u4e00\u4e2a\u5f02\u6b65RL\u6846\u67b6\uff0c\u5b9e\u73b0\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u4e09\u500d\u4ee5\u4e0a\u3002", "result": "LongCat-Flash-Thinking\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684SOTA\u6027\u80fd\u3002\u5728AIME-25\u667a\u80fd\u4f53\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747token\u6d88\u8017\u51cf\u5c1164.5%\uff08\u4ece19,653\u964d\u81f36,965\uff09\uff0c\u540c\u65f6\u672a\u964d\u4f4e\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86LongCat-Flash-Thinking\uff0c\u4e00\u4e2a\u5728\u590d\u6742\u63a8\u7406\u548c\u667a\u80fd\u4f53AI\u65b9\u9762\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u6027\u80fd\u548c\u5353\u8d8a\u6548\u7387\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u65e8\u5728\u4fc3\u8fdb\u63a8\u7406\u7cfb\u7edf\u548c\u667a\u80fd\u4f53AI\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.18538", "pdf": "https://arxiv.org/pdf/2509.18538", "abs": "https://arxiv.org/abs/2509.18538", "authors": ["Zixin Zhu", "Haoxiang Li", "Xuelu Feng", "He Wu", "Chunming Qiao", "Junsong Yuan"], "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts", "categories": ["cs.CV"], "comment": "Accepted as Spotlight at NeurIPS 2025", "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u79fb\u9664\u7269\u4f53\u51e0\u4f55\u4fe1\u606f\u518d\u8fdb\u884c\u5916\u89c2\u6e32\u67d3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u79fb\u9664\u7269\u4f53\u65f6\u65e0\u6cd5\u540c\u65f6\u6d88\u9664\u9634\u5f71\u3001\u53cd\u5c04\u7b49\u56e0\u679c\u89c6\u89c9\u4f2a\u5f71\uff0c\u6216\u7f3a\u4e4f\u63a7\u5236\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u5916\u89c2\u7684\u7269\u4f53\u79fb\u9664\u65b9\u6cd5\uff0c\u8981\u4e48\u56e0\u4e25\u683c\u9075\u5faa\u63a9\u7801\u5bf9\u9f50\u8bad\u7ec3\u800c\u65e0\u6cd5\u79fb\u9664\u672a\u660e\u786e\u63a9\u76d6\u7684\u56e0\u679c\u89c6\u89c9\u4f2a\u5f71\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\uff09\uff0c\u8981\u4e48\u56e0\u91c7\u7528\u677e\u6563\u63a9\u7801\u5bf9\u9f50\u7b56\u7565\u800c\u7f3a\u4e4f\u63a7\u5236\u529b\uff0c\u53ef\u80fd\u8bef\u5220\u5176\u4ed6\u7269\u4f53\u3002\u8fd9\u4e9b\u5c40\u9650\u6027\u6e90\u4e8e\u5ffd\u7565\u4e86\u7269\u4f53\u51e0\u4f55\u5b58\u5728\u4e0e\u5176\u89c6\u89c9\u6548\u679c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u7269\u4f53\u79fb\u9664\u89e3\u8026\u4e3a\uff1a(1) \u51e0\u4f55\u79fb\u9664\uff0c\u4f7f\u7528\u4e25\u683c\u63a9\u7801\u5bf9\u9f50\u7684\u76d1\u7763\u76f4\u63a5\u4ece\u51e0\u4f55\u4fe1\u606f\uff08\u5982\u6df1\u5ea6\uff09\u4e2d\u79fb\u9664\u7269\u4f53\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u7684\u7f16\u8f91\uff1b(2) \u5916\u89c2\u6e32\u67d3\uff0c\u57fa\u4e8e\u66f4\u65b0\u540e\u7684\u51e0\u4f55\u4fe1\u606f\u6e32\u67d3\u903c\u771f\u7684RGB\u56fe\u50cf\uff0c\u9690\u5f0f\u5904\u7406\u56e0\u679c\u89c6\u89c9\u6548\u679c\u3002\u51e0\u4f55\u79fb\u9664\u9636\u6bb5\u5f15\u5165\u4e86\u57fa\u4e8e\u6b63\u8d1f\u6837\u672c\u5bf9\u7684\u504f\u597d\u9a71\u52a8\u76ee\u6807\uff0c\u4ee5\u9f13\u52b1\u79fb\u9664\u7269\u4f53\u53ca\u5176\u56e0\u679c\u4f2a\u5f71\u5e76\u907f\u514d\u7ed3\u6784\u63d2\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u79fb\u9664\u7269\u4f53\u53ca\u5176\u76f8\u5173\u4f2a\u5f71\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u79fb\u9664\u548c\u5916\u89c2\u6e32\u67d3\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u672c\u65b9\u6cd5\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u7269\u4f53\u79fb\u9664\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u667a\u80fd\u3001\u66f4\u5168\u9762\u7684\u56fe\u50cf\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2509.18151", "pdf": "https://arxiv.org/pdf/2509.18151", "abs": "https://arxiv.org/abs/2509.18151", "authors": ["Jindi Lv", "Yuhao Zhou", "Yuxin Tian", "Qing Ye", "Wentao Feng", "Jiancheng Lv"], "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples.", "AI": {"tldr": "HyperNAS\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u9884\u6d4b\u5668\u8303\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40\u7f16\u7801\u548c\u5171\u4eab\u8d85\u7f51\u7edc\u589e\u5f3a\u67b6\u6784\u8868\u793a\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u8bc4\u4f30\u8017\u65f6\u548c\u9884\u6d4b\u5668\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u8017\u65f6\uff0c\u4e14\u73b0\u6709\u795e\u7ecf\u9884\u6d4b\u5668\uff08\u4ee3\u7406\u6a21\u578b\uff09\u56e0\u96be\u4ee5\u6355\u6349\u590d\u6742\u67b6\u6784\u5173\u7cfb\u800c\u6cdb\u5316\u6027\u5dee\uff0c\u9650\u5236\u4e86\u65b0\u67b6\u6784\u7684\u6027\u80fd\u9884\u6d4b\u3002", "method": "HyperNAS\u5305\u542b\uff1a1) \u5168\u5c40\u7f16\u7801\u65b9\u6848\uff0c\u7528\u4e8e\u6355\u6349\u5b8f\u89c2\u7ed3\u6784\u4fe1\u606f\uff1b2) \u5171\u4eab\u8d85\u7f51\u7edc\uff0c\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u67b6\u6784\u95f4\u6a21\u5f0f\u5b66\u4e60\u3002\u4e3a\u7a33\u5b9a\u8bad\u7ec3\uff0c\u8fd8\u5f00\u53d1\u4e86\u52a8\u6001\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u635f\u5931\uff0c\u4fc3\u8fdb\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u4e2a\u6027\u5316\u63a2\u7d22\u3002", "result": "\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u641c\u7d22\u7a7a\u95f4\uff08\u5305\u62ecViTs\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86HyperNAS\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u3002\u4f8b\u5982\uff0c\u5728CIFAR-10\u4e0a\u8fbe\u523097.60%\u7684top-1\u51c6\u786e\u7387\uff0c\u5728ImageNet\u4e0a\u8fbe\u523082.4%\u7684top-1\u51c6\u786e\u7387\uff0c\u4e14\u6240\u9700\u6837\u672c\u91cf\u81f3\u5c11\u51cf\u5c115.0\u500d\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "HyperNAS\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8868\u793a\u5b66\u4e60\u548c\u7a33\u5b9a\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecf\u9884\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9884\u6d4b\u6548\u7387\uff0c\u5728\u5c11\u6837\u672cNAS\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18987", "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "categories": ["cs.CL"], "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\uff08E2E-ST\uff09\u8bad\u7ec3\u671f\u95f4\u5bf9\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u5f25\u5408\u6a21\u6001\u9e3f\u6c9f\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\uff08E2E-ST\uff09\u5b58\u5728\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\u95f4\u7684\u5dee\u5f02\u3002\u73b0\u6709\u901a\u8fc7\u8bcd\u6216token\u7ea7\u5bf9\u9f50\u7684\u65b9\u6cd5\u9700\u8981\u8bed\u8a00\u7279\u5f02\u6027\u5de5\u5177\uff0c\u4e14\u5e76\u975e\u6240\u6709\u8bed\u8a00\u90fd\u53ef\u7528\uff1b\u800c\u57fa\u4e8e\u6700\u8fd1\u90bb\u76f8\u4f3c\u5ea6\u641c\u7d22\u7684\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u6765\u5bf9\u9f50\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5f25\u5408\u6a21\u6001\u9e3f\u6c9f\uff0c\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u5bf9\u9f50\uff0c\u8fbe\u5230\u4e0e\u73b0\u6709\u5de5\u4f5c\u76f8\u5f53\u7684E2E-ST\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u52a0\u5feb\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u57286\u4e2a\u8bed\u8a00\u65b9\u5411\u4e2d\u76845\u4e2a\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eDTW\u7684\u5bf9\u9f50\u65b9\u6cd5\u5728E2E-ST\u4e2d\u6709\u6548\u3001\u51c6\u786e\u4e14\u5feb\u901f\u5730\u5f25\u5408\u4e86\u8bed\u97f3\u548c\u6587\u672c\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.18905", "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "categories": ["cs.AI"], "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u4e86\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u7ea7\u7a7a\u95f4\u667a\u80fd\u5206\u7c7b\u548cSIBench\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86VLM\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u7684\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u662f\u4eba\u7c7b\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u5bf9\u5177\u8eab\u667a\u80fd\u548c\u81ea\u4e3b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u7684\u4e09\u7ef4\u7a7a\u95f4\u8868\u793a\u548c\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u96be\u4ee5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a\u7cfb\u7edf\u56de\u987e\u73b0\u6709VSR\u65b9\u6cd5\uff08\u6db5\u76d6\u8f93\u5165\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u673a\u5236\uff09\uff1b\u5c06\u7a7a\u95f4\u667a\u80fd\u5206\u4e3a\u57fa\u672c\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u7a7a\u95f4\u89c4\u5212\u4e09\u4e2a\u80fd\u529b\u5c42\u7ea7\uff1b\u6784\u5efaSIBench\u7a7a\u95f4\u667a\u80fd\u57fa\u51c6\uff0c\u6574\u5408\u8fd120\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u6db5\u76d623\u79cd\u4efb\u52a1\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684VLMs\u5728\u57fa\u672c\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u89c4\u5212\u4efb\u52a1\uff08\u5c24\u5176\u662f\u5728\u6570\u503c\u4f30\u8ba1\u3001\u591a\u89c6\u89d2\u63a8\u7406\u3001\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u60f3\u8c61\u65b9\u9762\uff09\u4e0a\u6301\u7eed\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524d\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u4ecd\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u4e3a\u672a\u6765\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u8def\u7ebf\u56fe\u548c\u5168\u9762\u7684\u57fa\u51c6\uff0c\u4ee5\u63a8\u52a8\u7a7a\u95f4\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18546", "pdf": "https://arxiv.org/pdf/2509.18546", "abs": "https://arxiv.org/abs/2509.18546", "authors": ["Yujia Liu", "Dingquan Li", "Tiejun Huang"], "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models", "categories": ["cs.CV"], "comment": null, "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.", "AI": {"tldr": "\u9488\u5bf9NR-IQA\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u8fc1\u79fb\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faSEGA\u653b\u51fb\uff0c\u901a\u8fc7\u96c6\u6210\u9ad8\u65af\u5e73\u6ed1\u68af\u5ea6\u548c\u6270\u52a8\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9NR-IQA\u6a21\u578b\u7684\u767d\u76d2\u5bf9\u6297\u653b\u51fb\u5728\u66f4\u771f\u5b9e\u7684\u9ed1\u76d2\u573a\u666f\u4e2d\u8fc1\u79fb\u6027\u5dee\uff0c\u96be\u4ee5\u6709\u6548\u63ed\u793a\u6a21\u578b\u6f0f\u6d1e\u5e76\u6307\u5bfc\u9c81\u68d2\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u9ed1\u76d2\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u3002", "method": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u53ef\u8fc1\u79fb\u7684Signed Ensemble Gaussian\u9ed1\u76d2\u653b\u51fb\uff08SEGA\uff09\uff0c\u5176\u4e3b\u8981\u601d\u60f3\u662f\u901a\u8fc7\u5bf9\u6e90\u6a21\u578b\u5e94\u7528\u9ad8\u65af\u5e73\u6ed1\u5e76\u96c6\u6210\u5176\u5e73\u6ed1\u68af\u5ea6\u6765\u8fd1\u4f3c\u76ee\u6807\u6a21\u578b\u68af\u5ea6\u3002\u6b64\u5916\uff0cSEGA\u8fd8\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6270\u52a8\u8fc7\u6ee4\u63a9\u7801\u6765\u53bb\u9664\u4e0d\u5f53\u6270\u52a8\uff0c\u4ee5\u786e\u4fdd\u5bf9\u6297\u6270\u52a8\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "result": "\u5728CLIVE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEGA\u653b\u51fb\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "SEGA\u653b\u51fb\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9NR-IQA\u6a21\u578b\u7684\u8fc1\u79fb\u5f0f\u9ed1\u76d2\u653b\u51fb\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u63ed\u793a\u6a21\u578b\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18152", "pdf": "https://arxiv.org/pdf/2509.18152", "abs": "https://arxiv.org/abs/2509.18152", "authors": ["Zhenyu Qi", "Qing Yu", "Jichen Wang", "Yun-Bo Zhao", "Zerui Li", "Wenjun Lv"], "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data.", "AI": {"tldr": "\u9488\u5bf9\u6d4b\u4e95\u89e3\u91ca\u4e2d\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86WLFM\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u66f2\u7ebf\u6d4b\u4e95\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u548c\u5ca9\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u8fc1\u79fb\u7684\u7279\u6027\u3002", "motivation": "\u4e95\u6d4b\u89e3\u91ca\u662f\u5730\u4e0b\u8868\u5f81\u7684\u57fa\u7840\uff0c\u4f46\u9762\u4e34\u5de5\u5177\u54cd\u5e94\u5f02\u6784\u3001\u4fe1\u53f7\u566a\u58f0\u548c\u6807\u7b7e\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86WLFM\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u6765\u81ea1200\u53e3\u4e95\u7684\u591a\u66f2\u7ebf\u6d4b\u4e95\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u5c06\u6d4b\u4e95\u5757\u6807\u8bb0\u5316\u4e3a\u5730\u8d28\u6807\u8bb0\uff1b\u901a\u8fc7\u63a9\u7801\u6807\u8bb0\u5efa\u6a21\u548c\u5730\u5c42\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b\u4ee5\u53ca\u901a\u8fc7\u5c11\u6837\u672c\u5fae\u8c03\u8fdb\u884c\u591a\u4efb\u52a1\u9002\u5e94\u3002", "result": "WLFM\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u4e2d\u8fbe\u52300.0041 MSE\uff0c\u5728\u5ca9\u6027\u5206\u7c7b\u4e2d\u8fbe\u523074.13%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002WLFM-Finetune\u8fdb\u4e00\u6b65\u63d0\u5347\u81f30.0038 MSE\u548c78.10%\u51c6\u786e\u7387\u3002WLFM\u5c55\u73b0\u51fa\u5c42\u4f4d\u611f\u77e5\u3001\u5b66\u4e60\u53ef\u590d\u7528\u5730\u8d28\u8bcd\u6c47\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u63a9\u7801\u66f2\u7ebf\u7684\u80fd\u529b\uff0c\u5c3d\u7ba1\u5728\u6d45\u5c42\u548c\u8d85\u6df1\u5c42\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u805a\u7c7b\u5206\u6790\u8868\u660e\u5176\u5728\u8fb9\u754c\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eWLFM\u662f\u5730\u8d28AI\u9886\u57df\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u8fc1\u79fb\u7684\u9aa8\u5e72\u6a21\u578b\uff0c\u5bf9\u6d4b\u4e95\u3001\u5730\u9707\u548c\u6587\u672c\u7b49\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.19020", "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.", "AI": {"tldr": "\u672c\u6587\u5bf9\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u9886\u57df\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u7b56\u7565\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u901a\u8fc7\u6700\u4f73N\u5019\u9009\u6846\u67b6\u53d1\u73b0TTS\u80fd\u6709\u6548\u63d0\u5347\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u8ba1\u7b97\u9884\u7b97\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524dNLP\u7cfb\u7edf\u4e3b\u8981\u901a\u8fc7\u6269\u5927\u6a21\u578b\u53c2\u6570\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u4f5c\u4e3a\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u5728\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5bf9MT\u7684TTS\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u5b9e\u7528\u7684\u6700\u4f73N\u5019\u9009\u6846\u67b6\uff0c\u5e76\u5728WMT24\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e86\u516d\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4e00\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u3001\u4e94\u79cd\u6a21\u578b\u5c3a\u5bf8\uff083B-72B\uff09\uff0c\u4ee5\u53ca\u591a\u79cdTTS\u8ba1\u7b97\u9884\u7b97\uff08N\u6700\u9ad8\u8fbe1024\uff09\u3002", "result": "a) \u5bf9\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0cTTS\u666e\u904d\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u5f97\u5230\u8bc1\u5b9e\uff1bb) \u4f7f\u7528\u8f83\u5927N\u503c\u589e\u5f3a\u7684\u5c0f\u6a21\u578b\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8aN=1\u7684\u5927\u6a21\u578b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff1bc) \u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u5927\u6a21\u578b\u901a\u5e38\u66f4\u9ad8\u6548\uff0c\u4e14TTS\u5728\u4f4e\u8d44\u6e90\u60c5\u51b5\u4e0b\u53ef\u80fd\u56e0\u8bc4\u4f30\u6307\u6807\u76f2\u533a\u800c\u964d\u4f4e\u8d28\u91cf\u3002", "conclusion": "TTS\u662f\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u7684\u6709\u6548\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u3002\u5b83\u63d0\u4f9b\u4e86\u5728\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u8ba1\u7b97\u548c\u6027\u80fd\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u5176\u6548\u7387\u548c\u6548\u679c\u53d7\u8bed\u8a00\u8d44\u6e90\u6c34\u5e73\u548c\u8ba1\u7b97\u9884\u7b97\u5f71\u54cd\uff0c\u5927\u6a21\u578b\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u901a\u5e38\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2509.18942", "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DEAL\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548c\u6301\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3LLM\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u5728\u4ece\u5934\u8bad\u7ec3\u8ba1\u7b97\u91cf\u4e0d\u53ef\u884c\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEAL\u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u4e0e\u6301\u7eed\u5fae\u8c03\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u5e76\u96c6\u6210\u4e86\u77e5\u8bc6\u4fdd\u7559\u548c\u81ea\u9002\u5e94\u53c2\u6570\u66f4\u65b0\u6a21\u5757\u3002", "result": "\u572815\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDEAL\u6846\u67b6\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DEAL\u65b9\u6cd5\u901a\u8fc7\u63d0\u9ad8LLM\u7684\u4efb\u52a1\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63a8\u8fdbLLM\u6301\u7eed\u9002\u5e94\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18550", "pdf": "https://arxiv.org/pdf/2509.18550", "abs": "https://arxiv.org/abs/2509.18550", "authors": ["Mohammad Junayed Hasan", "Nabeel Mohammed", "Shafin Rahman", "Philipp Koehn"], "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles", "categories": ["cs.CV"], "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025.\n  Final version to appear in the conference proceedings", "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHadaSmileNet\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u53c2\u6570\u7684Hadamard\u4e58\u6cd5\u878d\u5408\u5c06Transformer\u7279\u5f81\u4e0eD-Marker\u751f\u7406\u7279\u5f81\u7ed3\u5408\uff0c\u7528\u4e8e\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u88c5\u60c5\u7eea\uff08\u5fae\u7b11\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u88c5\u60c5\u7eea\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6a21\u5f0f\u8bc6\u522b\u6311\u6218\uff0c\u5728\u793e\u4f1a\u79d1\u5b66\u3001\u533b\u7597\u4fdd\u5065\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548cD-Marker\u7279\u5f81\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3001\u8f85\u52a9\u4efb\u52a1\u76d1\u7763\u548c\u590d\u6742\u635f\u5931\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165HadaSmileNet\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u65e0\u53c2\u6570\u7684\u4e58\u6cd5\u4ea4\u4e92\uff08\u7279\u522b\u662fHadamard\u4e58\u6cd5\u878d\u5408\uff09\u76f4\u63a5\u6574\u5408\u57fa\u4e8eTransformer\u7684\u8868\u793a\u4e0e\u751f\u7406\u5b66D-Marker\u7279\u5f81\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e8615\u79cd\u878d\u5408\u7b56\u7565\u4ee5\u786e\u5b9a\u6700\u4f73\u65b9\u6848\u3002", "result": "Hadamard\u4e58\u6cd5\u878d\u5408\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u5e76\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u5728UvA-NEMO (88.7%, +0.8)\u3001MMI (99.7%)\u3001SPOS (98.5%, +0.7) \u548cBBC (100%, +5.0) \u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u65b0SOTA\u7ed3\u679c\u3002\u4e0e\u591a\u4efb\u52a1\u66ff\u4ee3\u65b9\u6848\u76f8\u6bd4\uff0c\u53c2\u6570\u51cf\u5c11\u4e8626%\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u7b80\u5316\uff0c\u7279\u5f81\u53ef\u89c6\u5316\u4e5f\u8bc1\u660e\u4e86\u9886\u57df\u77e5\u8bc6\u76f4\u63a5\u96c6\u6210\u589e\u5f3a\u4e86\u5224\u522b\u529b\u3002", "conclusion": "HadaSmileNet\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u88c5\u60c5\u7eea\u7684\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u60c5\u611f\u8ba1\u7b97\u80fd\u529b\u7684\u591a\u5a92\u4f53\u6570\u636e\u6316\u6398\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.18153", "pdf": "https://arxiv.org/pdf/2509.18153", "abs": "https://arxiv.org/abs/2509.18153", "authors": ["Hanqun Cao", "Marcelo D. T. Torres", "Jingjie Zhang", "Zijun Gao", "Fang Wu", "Chunbin Gu", "Jure Leskovec", "Yejin Choi", "Cesar de la Fuente-Nunez", "Guangyong Chen", "Pheng-Ann Heng"], "title": "A deep reinforcement learning platform for antibiotic discovery", "categories": ["cs.LG", "q-bio.BM"], "comment": "42 pages, 16 figures", "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths\nannually by 2050, underscoring the urgent need for new antibiotics. Here we\npresent ApexAmphion, a deep-learning framework for de novo design of\nantibiotics that couples a 6.4-billion-parameter protein language model with\nreinforcement learning. The model is first fine-tuned on curated peptide data\nto capture antimicrobial sequence regularities, then optimised with proximal\npolicy optimization against a composite reward that combines predictions from a\nlearned minimum inhibitory concentration (MIC) classifier with differentiable\nphysicochemical objectives. In vitro evaluation of 100 designed peptides showed\nlow MIC values (nanomolar range in some cases) for all candidates (100% hit\nrate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial\nactivity against at least two clinically relevant bacteria. The lead molecules\nkilled bacteria primarily by potently targeting the cytoplasmic membrane. By\nunifying generation, scoring and multi-objective optimization with deep\nreinforcement learning in a single pipeline, our approach rapidly produces\ndiverse, potent candidates, offering a scalable route to peptide antibiotics\nand a platform for iterative steering toward potency and developability within\nhours.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aApexAmphion\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u65e8\u5728\u4ece\u5934\u8bbe\u8ba1\u65b0\u578b\u80bd\u7c7b\u6297\u751f\u7d20\u3002\u8be5\u6846\u67b6\u80fd\u5feb\u901f\u751f\u6210\u9ad8\u6548\u3001\u5e7f\u8c31\u7684\u5019\u9009\u836f\u7269\uff0c\u8fd9\u4e9b\u836f\u7269\u4e3b\u8981\u901a\u8fc7\u9776\u5411\u7ec6\u80de\u819c\u53d1\u6325\u4f5c\u7528\uff0c\u4e3a\u5e94\u5bf9\u6297\u83cc\u7d20\u8010\u836f\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6297\u83cc\u7d20\u8010\u836f\u6027\uff08AMR\uff09\u9884\u8ba1\u52302050\u5e74\u6bcf\u5e74\u5c06\u5bfc\u81f41000\u4e07\u4eba\u6b7b\u4ea1\uff0c\u8fd9\u51f8\u663e\u4e86\u5f00\u53d1\u65b0\u6297\u751f\u7d20\u7684\u7d27\u8feb\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86ApexAmphion\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e00\u4e2a64\u4ebf\u53c2\u6570\u7684\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u3002\u6a21\u578b\u9996\u5148\u901a\u8fc7\u7cbe\u9009\u7684\u80bd\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6355\u83b7\u6297\u83cc\u5e8f\u5217\u89c4\u5f8b\uff0c\u7136\u540e\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u5176\u590d\u5408\u5956\u52b1\u7ed3\u5408\u4e86\u5b66\u4e60\u5230\u7684\u6700\u5c0f\u6291\u83cc\u6d53\u5ea6\uff08MIC\uff09\u5206\u7c7b\u5668\u9884\u6d4b\u548c\u53ef\u5fae\u5206\u7684\u7406\u5316\u76ee\u6807\u3002", "result": "\u4f53\u5916\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u6709100\u79cd\u8bbe\u8ba1\u80bd\u5747\u8868\u73b0\u51fa\u4f4eMIC\u503c\uff08\u90e8\u5206\u8fbe\u5230\u7eb3\u6469\u5c14\u7ea7\u522b\uff0c100%\u547d\u4e2d\u7387\uff09\u3002\u5176\u4e2d99\u79cd\u5316\u5408\u7269\u5bf9\u81f3\u5c11\u4e24\u79cd\u4e34\u5e8a\u76f8\u5173\u7ec6\u83cc\u663e\u793a\u51fa\u5e7f\u8c31\u6297\u83cc\u6d3b\u6027\u3002\u4e3b\u8981\u7684\u5148\u5bfc\u5206\u5b50\u901a\u8fc7\u6709\u6548\u9776\u5411\u7ec6\u80de\u8d28\u819c\u6765\u6740\u706d\u7ec6\u83cc\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5c06\u751f\u6210\u3001\u8bc4\u5206\u548c\u591a\u76ee\u6807\u4f18\u5316\u6574\u5408\u5230\u4e00\u4e2a\u5355\u4e00\u6d41\u7a0b\u4e2d\uff0c\u80fd\u591f\u5feb\u901f\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u6548\u7684\u5019\u9009\u836f\u7269\uff0c\u4e3a\u80bd\u7c7b\u6297\u751f\u7d20\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u5728\u6570\u5c0f\u65f6\u5185\u8fed\u4ee3\u8c03\u6574\u6548\u529b\u548c\u53ef\u5f00\u53d1\u6027\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.19033", "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790CLiC-it\u4f1a\u8bae\u8bba\u6587\u96c6\uff0c\u8ffd\u8e2a\u5e76\u5448\u73b0\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u793e\u533a\u5728\u8fc7\u53bb\u5341\u5e74\uff082014-2024\uff09\u7684\u7814\u7a76\u8d8b\u52bf\u548c\u53d1\u5c55\u3002", "motivation": "Transformer-based\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\u8fc5\u901f\u63a8\u52a8\u4e86\u8ba1\u7b97\u8bed\u8a00\u5b66\uff08CL\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u53d1\u5c55\uff0c\u6539\u53d8\u4e86\u7814\u7a76\u76ee\u6807\u548c\u4f18\u5148\u4e8b\u9879\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u610f\u5927\u5229\u548c\u56fd\u9645\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6709\u5173\u8be5\u9886\u57df\u65b0\u5174\u8d8b\u52bf\u548c\u5173\u952e\u53d1\u5c55\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u660e\u667a\u51b3\u7b56\u548c\u65b9\u5411\u3002", "method": "\u6536\u96c6CLiC-it\uff08\u610f\u5927\u5229\u9886\u5148\u7684CL/NLP\u4f1a\u8bae\uff09\u524d10\u5c4a\uff082014-2024\uff09\u7684\u4f1a\u8bae\u8bba\u6587\u96c6\uff0c\u6784\u5efa\u201cCLiC-it\u8bed\u6599\u5e93\u201d\u3002\u5bf9\u8bed\u6599\u5e93\u7684\u5143\u6570\u636e\uff08\u5305\u62ec\u4f5c\u8005\u6765\u6e90\u3001\u6027\u522b\u3001\u5355\u4f4d\u7b49\uff09\u548c\u8bba\u6587\u5185\u5bb9\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8eCLiC-it\u8bed\u6599\u5e93\u5143\u6570\u636e\u548c\u8bba\u6587\u5185\u5bb9\u7684\u5168\u9762\u5206\u6790\uff0c\u63ed\u793a\u4e86\u610f\u5927\u5229CL\u548cNLP\u793e\u533a\u5341\u5e74\u6765\u7684\u65b0\u5174\u8d8b\u52bf\u548c\u5173\u952e\u53d1\u5c55\u60c5\u51b5\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u610f\u5927\u5229\u548c\u56fd\u9645\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4ee5\u652f\u6301\u4ed6\u4eec\u5728CL\u548cNLP\u9886\u57df\u7684\u660e\u667a\u51b3\u7b56\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.18970", "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "categories": ["cs.AI"], "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u63a2\u8ba8\u4e86\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u7f13\u89e3\u4e0e\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u53ef\u9760\u7684\u4ee3\u7406\u7cfb\u7edf\u5f00\u53d1\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u591a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u56fa\u6709\u7684\u5e7b\u89c9\u95ee\u9898\u5bfc\u81f4\u4efb\u52a1\u6267\u884c\u9519\u8bef\uff0c\u4e25\u91cd\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u6df1\u5165\u7406\u89e3\u5e76\u7cfb\u7edf\u6574\u5408\u76f8\u5173\u8fdb\u5c55\u4ee5\u89e3\u51b3\u6b64\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790LLM\u4ee3\u7406\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u5e7b\u89c9\u7c7b\u578b\u7684\u65b0\u5206\u7c7b\u6cd5\u3002\u6b64\u5916\uff0c\u6df1\u5165\u7814\u7a76\u4e8618\u79cd\u5bfc\u81f4\u4ee3\u7406\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u73b0\u6709\u7814\u7a76\u603b\u7ed3\u4e86\u5e7b\u89c9\u7684\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9LLM\u4ee3\u7406\u5e7b\u89c9\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u4e8618\u79cd\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e86\u73b0\u6709\u7f13\u89e3\u548c\u68c0\u6d4b\u5e7b\u89c9\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u89e3\u51b3LLM\u4ee3\u7406\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u5168\u9762\u7406\u89e3\uff0c\u5e76\u5e0c\u671b\u6fc0\u53d1\u672a\u6765\u7684\u7814\u7a76\u52aa\u529b\uff0c\u6700\u7ec8\u4fc3\u8fdb\u5f00\u53d1\u66f4\u7a33\u5065\u548c\u53ef\u9760\u7684LLM\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2509.18566", "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4e8b\u4ef6\u5f15\u5bfc\u6846\u67b6\uff0c\u5229\u7528\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\u8054\u5408\u91cd\u5efa\u52a8\u6001\u4eba\u7269\u4e0e\u9759\u6001\u573a\u666f\u3002\u901a\u8fc7\u7edf\u4e00\u7684\u8bed\u4e49\u9ad8\u65af\u548c\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5feb\u8fd0\u52a8\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u4eba\u7269\u548c\u9759\u6001\u573a\u666f\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5feb\u901f\u8fd0\u52a8\u4e0b\uff0cRGB\u5e27\u5bb9\u6613\u51fa\u73b0\u8fd0\u52a8\u6a21\u7cca\u3002\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4e3a\u52a8\u6001\u4eba\u7269\u91cd\u5efa\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u611f\u77e5\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e8b\u4ef6\u5f15\u5bfc\u4eba\u4f53-\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5229\u7528\u5355\u4e2a\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u5171\u540c\u5efa\u6a21\u4eba\u4f53\u548c\u573a\u666f\u3002\u91c7\u7528\u7edf\u4e00\u76843D\u9ad8\u65af\u96c6\u5408\uff0c\u6bcf\u4e2a\u9ad8\u65af\u5177\u6709\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u4ec5\u5c06\u5c5e\u4e8e\u4eba\u4f53\u7684\u90e8\u5206\u8fdb\u884c\u53d8\u5f62\u52a8\u753b\uff0c\u800c\u573a\u666f\u9ad8\u65af\u4fdd\u6301\u9759\u6001\u3002\u4e3a\u89e3\u51b3\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\uff0c\u5339\u914d\u8fde\u7eed\u6e32\u67d3\u7684\u6a21\u62df\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u4f53\u906e\u7f69\uff0c\u7b80\u5316\u4e86\u9ad8\u65af\u96c6\u5408\u7684\u7ba1\u7406\u3002", "result": "\u5728ZJU-MoCap-Blur\u548cMMHPSD-Blur\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4eba\u4f53-\u573a\u666f\u91cd\u5efa\u3002\u76f8\u5bf9\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5728PSNR/SSIM\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0cLPIPS\u6709\u6240\u964d\u4f4e\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u5ea6\u4e3b\u4f53\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee\u89c6\u9891\u4e2d\u5feb\u8fd0\u52a8\u548c\u8fd0\u52a8\u6a21\u7cca\u4e0b\u52a8\u6001\u4eba\u7269\u4e0e\u9759\u6001\u573a\u666f\u91cd\u5efa\u7684\u96be\u9898\uff0c\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u7b80\u5316\u4e86\u91cd\u5efa\u6d41\u7a0b\u3002"}}
{"id": "2509.18154", "pdf": "https://arxiv.org/pdf/2509.18154", "abs": "https://arxiv.org/abs/2509.18154", "authors": ["Tianyu Yu", "Zefan Wang", "Chongyi Wang", "Fuwei Huang", "Wenshuo Ma", "Zhihui He", "Tianchi Cai", "Weize Chen", "Yuxiang Huang", "Yuanqian Zhao", "Bokai Xu", "Junbo Cui", "Yingjing Xu", "Liqing Ruan", "Luoyuan Zhang", "Hanyu Liu", "Jingkun Tang", "Hongyuan Liu", "Qining Guo", "Wenhao Hu", "Bingxiang He", "Jie Zhou", "Jie Cai", "Ji Qi", "Zonghao Guo", "Chi Chen", "Guoyang Zeng", "Yuxuan Li", "Ganqu Cui", "Ning Ding", "Xu Han", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe", "categories": ["cs.LG", "cs.CV"], "comment": "Project Website: https://github.com/OpenBMB/MiniCPM-V", "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.", "AI": {"tldr": "MiniCPM-V 4.5\u662f\u4e00\u4e2a8B\u53c2\u6570\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u3001\u6570\u636e\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u521b\u65b0\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86GPT-4o-latest\u548c\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u662f\u9650\u5236\u5176\u666e\u53ca\u548c\u6269\u5c55\u6027\u7684\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u5f15\u5165\u4e86MiniCPM-V 4.5\u6a21\u578b\uff088B\u53c2\u6570\uff09\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e09\u9879\u6838\u5fc3\u6539\u8fdb\uff1a\u7edf\u4e00\u76843D-Resampler\u67b6\u6784\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7d27\u51d1\u7f16\u7801\uff1b\u7edf\u4e00\u7684\u5b66\u4e60\u8303\u5f0f\u7528\u4e8e\u6587\u6863\u77e5\u8bc6\u548c\u6587\u672c\u8bc6\u522b\uff0c\u65e0\u9700\u7e41\u91cd\u7684\u6570\u636e\u5de5\u7a0b\uff1b\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4ee5\u63d0\u5347\u77ed\u3001\u957f\u63a8\u7406\u80fd\u529b\u3002", "result": "MiniCPM-V 4.5\u5728OpenCompass\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86GPT-4o-latest\u7b49\u4e13\u6709\u6a21\u578b\u548cQwen2.5-VL 72B\u7b49\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002\u5728VideoMME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u572830B\u4ee5\u4e0b\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14GPU\u5185\u5b58\u6d88\u8017\u4ec5\u4e3aQwen2.5-VL 7B\u768446.7%\uff0c\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a8.7%\u3002", "conclusion": "MiniCPM-V 4.5\u8bc1\u660e\u4e86\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9886\u57df\uff0c\u901a\u8fc7\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u7b56\u7565\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u521b\u65b0\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u548c\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dMLLM\u9762\u4e34\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.19094", "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPathways of Thoughts (PoT) \u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u4e2a\u6027\u5316\u95ee\u7b54\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u62df\u8fed\u4ee3\u8ba4\u77e5\u8fc7\u7a0b\u5e76\u6574\u5408\u591a\u6837\u7684\u63a8\u7406\u8def\u5f84\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "motivation": "\u4e2a\u6027\u5316\u95ee\u7b54\u5bf9\u4e8e\u6ee1\u8db3\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4ece\u5197\u957f\u3001\u5608\u6742\u548c\u9690\u5f0f\u8bed\u5883\u4e2d\u63a8\u65ad\u504f\u597d\u4ee5\u53ca\u751f\u6210\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u5408\u9002\u4e14\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u56de\u7b54\u9762\u4e34\u6311\u6218\uff0c\u76ee\u524d\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51faPathways of Thoughts (PoT)\uff0c\u4e00\u79cd\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55LLM\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002\u8be5\u65b9\u6cd5\u5c06LLM\u7684\u63a8\u7406\u5efa\u6a21\u4e3a\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u63a8\u7406\u3001\u4fee\u8ba2\u3001\u4e2a\u6027\u5316\u548c\u6f84\u6e05\u7b49\u8ba4\u77e5\u64cd\u4f5c\uff0c\u4ee5\u63a2\u7d22\u591a\u4e2a\u63a8\u7406\u8f68\u8ff9\u5e76\u751f\u6210\u591a\u6837\u5316\u7684\u5019\u9009\u54cd\u5e94\u3002PoT\u968f\u540e\u6839\u636e\u63a8\u65ad\u7684\u7528\u6237\u504f\u597d\u805a\u5408\u5e76\u91cd\u65b0\u52a0\u6743\u8fd9\u4e9b\u5019\u9009\u54cd\u5e94\uff0c\u751f\u6210\u6700\u7ec8\u7684\u4e2a\u6027\u5316\u56de\u7b54\u3002", "result": "\u5728LaMP-QA\u4e2a\u6027\u5316\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoT\u6301\u7eed\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe13.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002\u4eba\u5de5\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u6807\u6ce8\u8005\u572866%\u7684\u6848\u4f8b\u4e2d\u504f\u597dPoT\u7684\u8f93\u51fa\uff0c\u4e14\u4ec5\u670915%\u7684\u6848\u4f8b\u4e3a\u5e73\u5c40\u3002", "conclusion": "PoT\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62dfLLM\u7684\u8fed\u4ee3\u8ba4\u77e5\u8fc7\u7a0b\u548c\u6574\u5408\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u95ee\u7b54\u65b9\u9762\u7684\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2509.18980", "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u5426\u4ece\u6570\u5b66\u4e0a\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u6a21\u578b\u751f\u6210\u6709\u6548\u7684\u7528\u6237\u89e3\u91ca\u3002\u4e00\u9879326\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6240\u6709\u89e3\u91ca\u7c7b\u578b\u90fd\u666e\u904d\u53d7\u5230\u597d\u8bc4\uff0c\u7b56\u7565\u95f4\u5b58\u5728\u4e2d\u7b49\u7edf\u8ba1\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u4e3a\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u6a21\u578b\u751f\u6210\u6709\u6548\u7684\u3001\u9762\u5411\u7528\u6237\u7684\u89e3\u91ca\u3002\u73b0\u6709\u53ef\u89e3\u91caAI\u5de5\u4f5c\u591a\u4f9d\u8d56\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u672a\u80fd\u6355\u6349\u7528\u6237\u771f\u5b9e\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ea6\u675f\u77e9\u9635\u5206\u89e3\u7684\u63a8\u8350\u6a21\u578b\uff08\u5177\u6709\u663e\u5f0f\u7528\u6237\u7c7b\u578b\u548c\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u5206\u6570\uff09\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684LLM\u63d0\u793a\uff0c\u5c06\u6a21\u578b\u7ed3\u6784\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u901a\u8fc7\u6539\u53d8LLM\u7684\u8f93\u5165\u4fe1\u606f\u751f\u6210\u591a\u79cd\u89e3\u91ca\u7c7b\u578b\u3002\u5bf9326\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u89e3\u91ca\u5728\u900f\u660e\u5ea6\u3001\u6709\u6548\u6027\u3001\u8bf4\u670d\u529b\u3001\u4fe1\u4efb\u548c\u6ee1\u610f\u5ea6\u4e94\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u8d28\u91cf\u4ee5\u53ca\u63a8\u8350\u672c\u8eab\u3002", "result": "\u6240\u6709\u89e3\u91ca\u7c7b\u578b\u666e\u904d\u53d7\u5230\u7528\u6237\u7684\u597d\u8bc4\u3002\u4e0d\u540c\u89e3\u91ca\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u4e2d\u7b49\u7684\u7edf\u8ba1\u5dee\u5f02\u3002\u7528\u6237\u8bc4\u8bba\u4e3a\u91cf\u5316\u7ed3\u679c\u63d0\u4f9b\u4e86\u8865\u5145\u6027\u89c1\u89e3\uff0c\u8fdb\u4e00\u6b65\u5f3a\u8c03\u4e86\u53c2\u4e0e\u8005\u5bf9\u4e0d\u540c\u7c7b\u578b\u89e3\u91ca\u7684\u53cd\u5e94\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u53ef\u89e3\u91ca\u63a8\u8350\u6a21\u578b\u7684\u9762\u5411\u7528\u6237\u89e3\u91ca\uff0c\u4e14\u8fd9\u4e9b\u89e3\u91ca\u666e\u904d\u53d7\u5230\u7528\u6237\u7684\u597d\u8bc4\u3002\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4e0d\u540c\u89e3\u91ca\u7b56\u7565\u7684\u611f\u77e5\u5dee\u5f02\u503c\u5f97\u8fdb\u4e00\u6b65\u5173\u6ce8\u3002"}}
{"id": "2509.18571", "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "Live-E2T\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u8bed\u4e49\u5143\u7ec4\u3001\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548cLLM\u94fe\u5f0f\u601d\u8003\u63a8\u7406\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u89c6\u9891\u5a01\u80c1\u76d1\u63a7\u7684\u5b9e\u65f6\u6027\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5a01\u80c1\u76d1\u63a7\u65b9\u6cd5\uff08\u65e0\u8bba\u662f\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u8fd8\u662f\u751f\u6210\u6a21\u578b\uff09\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u80fd\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u8fd9\u4e24\u9879\u8981\u6c42\uff0c\u5b58\u5728\u7a7a\u767d\u3002", "method": "Live-E2T\u6846\u67b6\u901a\u8fc7\u4e09\u79cd\u534f\u540c\u673a\u5236\u5b9e\u73b0\uff1a1. \u5c06\u89c6\u9891\u5e27\u89e3\u6784\u4e3a\u7d27\u51d1\u7684\u201c\u4eba-\u7269-\u4ea4\u4e92-\u5730\u70b9\u201d\u8bed\u4e49\u5143\u7ec4\u8868\u793a\u30022. \u63d0\u51fa\u9ad8\u6548\u7684\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548c\u66f4\u65b0\u673a\u5236\uff0c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\u30023. \u4f7f\u7528\u601d\u7ef4\u94fe\u7b56\u7565\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5bf9\u4e8b\u4ef6\u5e8f\u5217\u8fdb\u884c\u900f\u660e\u903b\u8f91\u63a8\u7406\u5e76\u751f\u6210\u5a01\u80c1\u8bc4\u4f30\u62a5\u544a\u3002", "result": "\u5728XD-Violence\u548cUCF-Crime\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLive-E2T\u5728\u5a01\u80c1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5b9e\u65f6\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "Live-E2T\u6210\u529f\u5730\u5f25\u5408\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u89c6\u9891\u5a01\u80c1\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18161", "pdf": "https://arxiv.org/pdf/2509.18161", "abs": "https://arxiv.org/abs/2509.18161", "authors": ["William H Patty"], "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u5316\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u5f62\u72b6\uff0c\u63a2\u7d22\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u4f46\u589e\u52a0\u4e86\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\u9650\u5236\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u5f62\u72b6\uff0c\u4e3a\u795e\u7ecf\u5143\u5206\u914d\u66f4\u4f18\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u66f4\u53c2\u6570\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e869\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63a2\u7d22\u4f7f\u7528\u53c2\u6570\u5316\u7ebf\u6027B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u53cc\u91cd\u4f18\u5316\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8eReLU\u7684\u4f20\u7edf\u6a21\u578b\u76f8\u6bd4\uff0c\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u6a21\u578b\u5728FNN\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe94%\u7684\u7ec8\u7aef\u6a21\u578b\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u5728CNN\u4e2d\u5b9e\u73b0\u4e8651%\u7684\u9519\u8bef\u7387\u964d\u4f4e\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6027\u80fd\u63d0\u5347\u662f\u4ee5\u589e\u52a0\u989d\u5916\u7684\u5f00\u53d1\u548c\u8bad\u7ec3\u590d\u6742\u6027\u4ee5\u53ca\u7ec8\u7aef\u6a21\u578b\u5ef6\u8fdf\u4e3a\u4ee3\u4ef7\u7684\u3002", "conclusion": "\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6fc0\u6d3b\u51fd\u6570\u5f62\u72b6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff08\u964d\u4f4e\u9519\u8bef\u7387\uff09\uff0c\u4f46\u8fd9\u79cd\u6027\u80fd\u63d0\u5347\u4f34\u968f\u7740\u5f00\u53d1\u3001\u8bad\u7ec3\u590d\u6742\u6027\u548c\u6a21\u578b\u5ef6\u8fdf\u7684\u589e\u52a0\u3002"}}
{"id": "2509.19108", "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "categories": ["cs.CL"], "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bed\u6599\u5e93\u5206\u6790\uff0c\u5b9e\u8bc1\u8c03\u67e5\u4e86\u8bed\u8a00\u5b66\u4e2d\u5173\u4e8e\u53e5\u5b50\u72ec\u7279\u6027\u7684\u4e3b\u5f20\uff0c\u53d1\u73b0\u867d\u7136\u72ec\u4e00\u65e0\u4e8c\u7684\u53e5\u5b50\u901a\u5e38\u5360\u591a\u6570\uff0c\u4f46\u91cd\u590d\u53e5\u7684\u5b58\u5728\u4e0d\u5bb9\u5ffd\u89c6\u4e14\u53d7\u8bed\u4f53\u9650\u5236\u3002", "motivation": "\u8bed\u8a00\u5b66\u4e2d\u666e\u904d\u8ba4\u4e3a\u5927\u591a\u6570\u8bed\u8a00\u8868\u8fbe\u662f\u72ec\u4e00\u65e0\u4e8c\u7684\uff08\u5982\u4e54\u59c6\u65af\u57fa\u548c\u5e73\u514b\u7684\u4e3b\u5f20\uff09\u3002\u968f\u7740\u5927\u578b\u8bed\u6599\u5e93\u7684\u51fa\u73b0\uff0c\u8fd9\u4e00\u4e3b\u5f20\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u5b9e\u8bc1\u65b9\u6cd5\u8fdb\u884c\u68c0\u9a8c\u3002", "method": "\u4f7f\u7528NLTK Python\u5e93\u89e3\u6790\u4e0d\u540c\u8bed\u4f53\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u8bed\u6599\u5e93\u4e2d\u7cbe\u786e\u5b57\u7b26\u4e32\u5339\u914d\uff08\u5373\u91cd\u590d\u53e5\u5b50\uff09\u7684\u6570\u91cf\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u5b8c\u5168\u72ec\u7279\u7684\u53e5\u5b50\u5728\u8bed\u6599\u5e93\u4e2d\u901a\u5e38\u5360\u591a\u6570\uff0c\u4f46\u8fd9\u9ad8\u5ea6\u53d7\u8bed\u4f53\u9650\u5236\uff0c\u4e14\u91cd\u590d\u53e5\u5b50\u5728\u4efb\u4f55\u5355\u4e2a\u8bed\u6599\u5e93\u4e2d\u90fd\u5360\u6709\u4e0d\u5bb9\u5ffd\u89c6\u7684\u6bd4\u4f8b\u3002", "conclusion": "\u5173\u4e8e\u8bed\u8a00\u8868\u8fbe\u666e\u904d\u72ec\u7279\u7684\u4f20\u7edf\u89c2\u70b9\u9700\u8981\u4fee\u6b63\uff0c\u72ec\u7279\u6027\u867d\u5e38\u89c1\u4f46\u5e76\u975e\u7edd\u5bf9\uff0c\u91cd\u590d\u73b0\u8c61\u663e\u8457\u4e14\u5177\u6709\u8bed\u4f53\u4f9d\u8d56\u6027\u3002"}}
{"id": "2509.18986", "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "categories": ["cs.AI"], "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\u5728\u4e00\u4e2a\u822a\u7a7a\u7269\u6d41\u516c\u53f8\u7684\u771f\u5b9e\u51fa\u5e93\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u6027\u6700\u9ad8\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5982\u63d0\u5347\u6280\u672f\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u66f4\u5c11\u3002", "motivation": "\u9884\u6d4b\u6d41\u7a0b\u6267\u884c\u7684\u5269\u4f59\u65f6\u95f4\u662f\u6d41\u7a0b\u6316\u6398\u4e2d\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u7684\u4e00\u4e2a\u91cd\u8981\u76ee\u6807\uff0c\u65e8\u5728\u5c55\u671b\u8fdb\u884c\u4e2d\u6d41\u7a0b\u7684\u672a\u6765\u3002", "method": "\u672c\u6587\u5728\u4e00\u4e2a\u822a\u7a7a\u4e1a\u52a1\u7269\u6d41\u516c\u53f8\u7684\u771f\u5b9e\u51fa\u5e93\u4ed3\u5e93\u6d41\u7a0b\u4e2d\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u516c\u53f8\u63d0\u4f9b\u7684\u4e00\u4e2a\u5305\u542b169,523\u6761\u8f68\u8ff9\u7684\u5168\u65b0\u4e8b\u4ef6\u65e5\u5fd7\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u50cf\u4f20\u7edf\u63d0\u5347\u6280\u672f\u8fd9\u6837\u7684\u6d45\u5c42\u65b9\u6cd5\u4e5f\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u663e\u8457\u66f4\u5c11\u3002", "conclusion": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8003\u8651\u5230\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u63d0\u4f9b\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u503c\u5f97\u8003\u8651\u7684\u9009\u62e9\u3002"}}
{"id": "2509.18582", "pdf": "https://arxiv.org/pdf/2509.18582", "abs": "https://arxiv.org/abs/2509.18582", "authors": ["Daiqing Qi", "Handong Zhao", "Jing Shi", "Simon Jenni", "Yifei Fan", "Franck Dernoncourt", "Scott Cohen", "Sheng Li"], "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers", "categories": ["cs.CV"], "comment": null, "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e13\u4e1a\u7f8e\u5b66\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86PhotoCritique\u6570\u636e\u96c6\u3001PhotoEye\u6a21\u578b\uff08\u5177\u5907\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u878d\u5408\u673a\u5236\uff09\u548cPhotoBench\u57fa\u51c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u9ad8\u7ea7\u7f8e\u5b66\u7406\u89e3\uff08\u8d85\u8d8a\u4e8b\u5b9e\u8bc6\u522b\uff0c\u5982\u8272\u5f69\u3001\u6784\u56fe\uff09\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5c24\u5176\u5728\u9700\u8981\u6444\u5f71\u6280\u5de7\u548c\u540e\u671f\u5904\u7406\u77e5\u8bc6\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u5176\u7f3a\u4e4f\u5bf9\u7f8e\u5b66\u5143\u7d20\uff08\u5982\u8272\u5f69\u3001\u5149\u5f71\u3001\u6784\u56fe\uff09\u7684\u6df1\u5c42\u611f\u77e5\u80fd\u529b\u3002", "method": "1. **\u6570\u636e\u96c6**: \u5f15\u5165PhotoCritique\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u4e13\u4e1a\u4e14\u591a\u6837\u5316\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7531\u4e13\u4e1a\u6444\u5f71\u5e08\u548c\u7231\u597d\u8005\u8ba8\u8bba\u751f\u6210\u3002 2. **\u6a21\u578b**: \u63d0\u51faPhotoEye\u6a21\u578b\uff0c\u91c7\u7528\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u878d\u5408\u673a\u5236\uff0c\u4ee5\u591a\u7ef4\u5ea6\u7406\u89e3\u56fe\u50cf\u7f8e\u5b66\u3002 3. **\u57fa\u51c6**: \u5efa\u7acbPhotoBench\uff0c\u4e00\u4e2a\u5168\u9762\u4e14\u4e13\u4e1a\u7684\u56fe\u50cf\u7f8e\u5b66\u7406\u89e3\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5728\u73b0\u6709\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684PhotoBench\u4e0a\uff0c\u6240\u63d0\u51fa\u7684PhotoEye\u6a21\u578b\u5747\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u4e13\u4e1a\u6570\u636e\u96c6PhotoCritique\u3001\u5f00\u53d1\u5177\u6709\u591a\u89c6\u89d2\u7406\u89e3\u80fd\u529b\u7684PhotoEye\u6a21\u578b\u4ee5\u53ca\u8bbe\u7acbPhotoBench\u57fa\u51c6\uff0c\u672c\u6587\u6210\u529f\u5730\u4ece\u6839\u672c\u4e0a\u589e\u5f3a\u4e86MLLMs\u5bf9\u56fe\u50cf\u7f8e\u5b66\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.18162", "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "categories": ["cs.LG"], "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19109", "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "categories": ["cs.CL"], "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u9996\u4e2a\u5409\u5c14\u5409\u65af\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6KyrgyzNER\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u5728\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u6f5c\u529b\u3002", "motivation": "\u5409\u5c14\u5409\u65af\u8bed\u4f5c\u4e3a\u4e00\u79cd\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u624b\u52a8\u6807\u6ce8\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u5176\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "1. \u6784\u5efa\u4e86KyrgyzNER\u6570\u636e\u96c6\uff0c\u5305\u542b1,499\u7bc7\u65b0\u95fb\u6587\u7ae0\uff08\u6765\u81ea24.KG\uff09\uff0c10,900\u4e2a\u53e5\u5b50\u548c39,075\u4e2a\u5b9e\u4f53\u63d0\u53ca\uff0c\u6db5\u76d627\u4e2a\u5b9e\u4f53\u7c7b\u522b\u3002\n2. \u8be6\u7ec6\u4ecb\u7ecd\u4e86\u6807\u6ce8\u65b9\u6848\u3001\u9047\u5230\u7684\u6311\u6218\u548c\u63cf\u8ff0\u6027\u7edf\u8ba1\u6570\u636e\u3002\n3. \u8bc4\u4f30\u4e86\u5305\u62ec\u57fa\u4e8e\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u7684\u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\uff08\u5982RoBERTa\u53d8\u4f53\uff09\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "1. \u6240\u6709\u6a21\u578b\u5728\u5904\u7406\u7a00\u6709\u5b9e\u4f53\u7c7b\u522b\u65f6\u5747\u9047\u5230\u56f0\u96be\u3002\n2. \u7ecf\u5927\u91cf\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00RoBERTa\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u5e73\u8861\uff0c\u8868\u73b0\u6700\u4f73\u3002\n3. \u5176\u4ed6\u591a\u8bed\u8a00\u6a21\u578b\u4e5f\u53d6\u5f97\u4e86\u53ef\u6bd4\u7684\u7ed3\u679c\u3002", "conclusion": "1. \u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u4e3a\u5904\u7406\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u5e26\u6765\u4e86\u6311\u6218\u4e0e\u673a\u9047\u3002\n2. \u5c3d\u7ba1\u591a\u8bed\u8a00RoBERTa\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5176\u4ed6\u591a\u8bed\u8a00\u6a21\u578b\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u3002\n3. \u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u7ec6\u7c92\u5ea6\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u4ee5\u63d0\u4f9b\u5bf9\u5409\u5c14\u5409\u65af\u8bed\u5904\u7406\u7ba1\u9053\u8bc4\u4f30\u7684\u66f4\u6df1\u5c42\u89c1\u89e3\u3002"}}
{"id": "2509.19030", "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Herv\u00e9", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "categories": ["cs.AI"], "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation", "AI": {"tldr": "\u9488\u5bf9\u7a0b\u5e8f\u5316\u751f\u6210\u5185\u5bb9\uff08PCG\uff09\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u4e0d\u7b26\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u201c\u5730\u6807\u201d\u3001\u201c\u7eaa\u5ff5\u7891\u201d\u548c\u201c\u4fe1\u6807\u201d\u7b49\u4ee5\u73a9\u5bb6\u4e3a\u4e2d\u5fc3\u7684\u6982\u5ff5\uff0c\u4ee5\u5b9e\u73b0PCG\u7684\u81ea\u52a8\u5316\u5206\u89e3\u548c\u5b50\u7ec4\u4ef6\u8bc4\u4f30\uff0c\u65e8\u5728\u5f25\u5408\u4eba\u6587\u4e0e\u6280\u672f\u6e38\u620f\u7814\u7a76\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u73b0\u6709\u7a0b\u5e8f\u5316\u751f\u6210\u5185\u5bb9\uff08PCG\uff09\u7684\u7b97\u6cd5\u8bc4\u4f30\u96be\u4ee5\u627e\u5230\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u76f8\u7b26\u7684\u6307\u6807\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u590d\u5408\u578b\u4f5c\u54c1\u3002\u81ea\u52a8\u5206\u89e3\u4f5c\u4e3a\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u4e00\u7cfb\u5217\u7b26\u5408\u7279\u5b9a\u5c5e\u6027\u7684\u6982\u5ff5\u3002", "method": "\u501f\u9274\u6e38\u620f\u7814\u7a76\u548c\u6e38\u620fAI\u7814\u7a76\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u201c\u5730\u6807\u201d\uff08Landmarks\uff09\u3001\u201c\u7eaa\u5ff5\u7891\u201d\uff08Monuments\uff09\u548c\u201c\u4fe1\u6807\u201d\uff08Beacons\uff09\u8fd9\u4e09\u4e2a\u5d4c\u5957\u6982\u5ff5\u3002\u8fd9\u4e9b\u6982\u5ff5\u4ee5\u73a9\u5bb6\u4e3a\u4e2d\u5fc3\uff0c\u57fa\u4e8e\u4f5c\u54c1\u7684\u201c\u53ef\u611f\u77e5\u6027\u201d\u3001\u201c\u5524\u8d77\u6027\u201d\u548c\u201c\u884c\u52a8\u53ec\u5524\u201d\u6765\u5b9a\u4e49\u3002", "result": "\u8fd9\u4e9b\u65b0\u6982\u5ff5\u5b9e\u4f53\u53ef\u4ee5\u4f7f\u7528\u73b0\u6709\u7814\u7a76\u548c\u884c\u4e1a\u6280\u672f\u8fdb\u884c\u53d1\u73b0\u548c\u8bc4\u4f30\uff0c\u4ece\u800c\u4e3aPCG\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u5206\u89e3\u53ca\u5176\u7a81\u51fa\u5b50\u7ec4\u4ef6\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u8fde\u63a5\u4eba\u6587\u4e0e\u6280\u672f\u6e38\u620f\u7814\u7a76\uff0c\u5e76\u5b9e\u73b0\u66f4\u4f18\u7684\u8ba1\u7b97PCG\u8bc4\u4f30\u3002\u5c3d\u7ba1\u5f3a\u8c03\u4e86\u6df7\u5408\u4e3b\u52a8PCG\u548c\u7ec4\u5408PCG\uff0c\u4f46\u8be5\u65b9\u6cd5\u9884\u8ba1\u53ef\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\u3002"}}
{"id": "2509.18591", "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eXMem\u6a21\u578b\u7684\u5b9e\u65f6\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8eMRI\u5f15\u5bfc\u653e\u7597\u3002\u8be5\u6846\u67b6\u5728\u521d\u6b65\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u5408\u7406\u7684\u5206\u5272\u6027\u80fd\u548c\u5b9e\u65f6\u6027\uff0c\u5c3d\u7ba1\u8be6\u7ec6\u91cf\u5316\u7ed3\u679c\u56e0\u8bb0\u5f55\u4e22\u5931\u672a\u80fd\u63d0\u4f9b\u3002", "motivation": "\u63d0\u9ad8MRI\u5f15\u5bfc\u653e\u7597\u4e2d\u80bf\u7624\u8ffd\u8e2a\u7684\u7cbe\u5ea6\uff0c\u4ee5\u589e\u5f3a\u764c\u75c7\u6cbb\u7597\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528XMem\u6a21\u578b\uff08\u4e00\u79cd\u8bb0\u5fc6\u589e\u5f3a\u67b6\u6784\uff09\u5bf9\u957f\u65f6\u95f4\u7535\u5f71MRI\u5e8f\u5217\u4e2d\u7684\u80bf\u7624\u8fdb\u884c\u5206\u5272\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u6709\u6548\u6574\u5408\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u80bf\u7624\u8fd0\u52a8\u7684\u5b9e\u65f6\u8ffd\u8e2a\uff0c\u5e76\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u4ecd\u80fd\u83b7\u5f97\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "result": "\u8be6\u7ec6\u5b9e\u9a8c\u8bb0\u5f55\u5df2\u4e22\u5931\uff0c\u6545\u672a\u80fd\u63d0\u4f9b\u7cbe\u786e\u7684\u5b9a\u91cf\u7ed3\u679c\u3002\u4f46\u521d\u6b65\u5370\u8c61\u663e\u793a\uff0c\u8be5\u57fa\u4e8eXMem\u7684\u6846\u67b6\u5177\u6709\u5408\u7406\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e76\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684XMem\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u5347MRI\u5f15\u5bfc\u653e\u7597\u4e2d\u80bf\u7624\u8ffd\u8e2a\u7684\u7cbe\u5ea6\uff0c\u5bf9\u63d0\u9ad8\u764c\u75c7\u6cbb\u7597\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.18164", "pdf": "https://arxiv.org/pdf/2509.18164", "abs": "https://arxiv.org/abs/2509.18164", "authors": ["Ranfei Chen", "Ming Chen"], "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture\nfollowing auto regressive models. Their denoising process offers a powerful\ngenerative advantage, but they present significant challenges in learning and\nunderstanding numerically sensitive mathematical and order-sensitive logical\ntasks. Current training methods, including pre-training, fine-tuning, and\nreinforcement learning, focus primarily on improving general knowledge\nretention and reasoning abilities, but lack a comprehensive understanding of\nmathematical and logical patterns. We propose DSFT, a simple yet effective\nDiffusion SFT strategy, by adjusting the masking strategy and loss function,\nguiding models to understand mathematical and logical patterns. This strategy\ncan be flexibly combined with pre-training, reinforcement learning, and other\ntraining methods. Validated on models such as LLaDA and Dream series, we prove\nthat DSFT on small-scale data can achieve improvements of 5-10% and\napproximately 2% on mathematical and logical problems, respectively. This\ninspiring masking approach offers insights for future learning of specific\npatterns, which can be easily and efficiently combined with other training\nmethods and applied to various dLLMs. Our code is publicly available at\nhttps://anonymous.4open.science/r/DSFT-0FFB/", "AI": {"tldr": "dLLMs\u5728\u6570\u5b66\u548c\u903b\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51faDSFT\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86dLLMs\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5728\u6570\u503c\u654f\u611f\u7684\u6570\u5b66\u548c\u987a\u5e8f\u654f\u611f\u7684\u903b\u8f91\u4efb\u52a1\u4e0a\u5b66\u4e60\u548c\u7406\u89e3\u80fd\u529b\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u7406\u89e3\u8fd9\u4e9b\u6a21\u5f0f\u3002", "method": "\u63d0\u51faDSFT\uff08Diffusion SFT\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5bfc\u6a21\u578b\u7406\u89e3\u6570\u5b66\u548c\u903b\u8f91\u6a21\u5f0f\u3002\u8be5\u7b56\u7565\u53ef\u4e0e\u9884\u8bad\u7ec3\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u7075\u6d3b\u7ed3\u5408\u3002", "result": "\u5728LLaDA\u548cDream\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cDSFT\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u4f7f\u6570\u5b66\u95ee\u9898\u6027\u80fd\u63d0\u53475-10%\uff0c\u903b\u8f91\u95ee\u9898\u6027\u80fd\u63d0\u5347\u7ea62%\u3002", "conclusion": "DSFT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u542f\u53d1\u6027\u7684\u63a9\u7801\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7279\u5b9a\u6a21\u5f0f\u7684\u5b66\u4e60\uff0c\u6613\u4e8e\u9ad8\u6548\u5730\u4e0e\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\uff0c\u5e76\u9002\u7528\u4e8e\u5404\u79cddLLMs\u3002"}}
{"id": "2509.19125", "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LLM\u591a\u65b9\u9762\u7f16\u7801\u548c\u52a8\u6001\u805a\u7c7b\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u8d2f\u6027\u548c\u7c92\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u8981\u6c42\u9ad8\u6548\u7684\u7ec4\u7ec7\u548c\u7efc\u5408\u65b9\u6cd5\u3002\u73b0\u6709\u5206\u7c7b\u6cd5\u6784\u5efa\u65b9\u6cd5\uff08\u5982\u65e0\u76d1\u7763\u805a\u7c7b\u6216\u76f4\u63a5LLM\u63d0\u793a\uff09\u5728\u8fde\u8d2f\u6027\u548c\u7c92\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\u751f\u6210\u6846\u67b6\uff0c\u6574\u5408LLM\u5f15\u5bfc\u7684\u591a\u65b9\u9762\u7f16\u7801\u548c\u52a8\u6001\u805a\u7c7b\u3002\u8be5\u65b9\u6cd5\u5229\u7528LLM\u8bc6\u522b\u8bba\u6587\u5173\u952e\u65b9\u9762\u5e76\u751f\u6210\u65b9\u9762\u7279\u5b9a\u6458\u8981\uff0c\u7136\u540e\u8fdb\u884c\u7f16\u7801\u548c\u805a\u7c7b\u4ee5\u5f62\u6210\u8fde\u8d2f\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b156\u4e2a\u4e13\u5bb6\u5236\u4f5c\u5206\u7c7b\u6cd5\uff08\u6db5\u76d61.16\u4e07\u7bc7\u8bba\u6587\uff09\u7684\u65b0\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u6cd5\u7684\u8fde\u8d2f\u6027\u3001\u7c92\u5ea6\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u672c\u6846\u67b6\u901a\u8fc7\u6574\u5408LLM\u4f18\u52bf\u548c\u52a8\u6001\u805a\u7c7b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u7c7b\u6cd5\u6784\u5efa\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u79d1\u5b66\u6587\u732e\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19058", "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "categories": ["cs.AI"], "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u53ef\u89c2\u6d4b\u6e90\u4f5c\u4e3a\u5185\u90e8\u8f85\u52a9\u53d8\u91cf\uff0c\u4ee5\u8bc6\u522b\u6f5c\u5728\u56e0\u5b50\u5e76\u63d0\u9ad8\u5176\u53ef\u6062\u590d\u6027\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u8f85\u52a9\u53d8\u91cf\u5b9e\u73b0\u53ef\u8bc6\u522b\u6027\uff0c\u4f46\u672a\u5145\u5206\u5229\u7528\u53ef\u89c2\u6d4b\u5230\u7684\u7cfb\u7edf\u9a71\u52a8\u6f5c\u5728\u56e0\u5b50\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u5185\u90e8\u53ef\u89c2\u6d4b\u6e90\u6765\u589e\u5f3a\u6f5c\u5728\u56e0\u5b50\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5c06\u53ef\u89c2\u6d4b\u6e90\u4f5c\u4e3a\u5185\u90e8\u8f85\u52a9\u53d8\u91cf\u7684\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6709\u6548\u7684\u6761\u4ef6\u53d8\u91cf\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4f53\u79ef\u4fdd\u6301\u7f16\u7801\u5668\u5b9e\u73b0\u6f5c\u5728\u53d8\u91cf\u7684\u8bc6\u522b\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u53d8\u91cf\u9009\u62e9\u65b9\u6848\uff0c\u4ee5\u5728\u5b58\u5728\u591a\u4e2a\u5df2\u77e5\u8f85\u52a9\u53d8\u91cf\u65f6\u6700\u5927\u5316\u6f5c\u5728\u56e0\u5b50\u7684\u53ef\u6062\u590d\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u4f7f\u7528\u4f53\u79ef\u4fdd\u6301\u7f16\u7801\u5668\u8bc6\u522b\u51fa\u6240\u6709\u6f5c\u5728\u53d8\u91cf\uff08\u5728\u5b50\u7a7a\u95f4\u53d8\u6362\u548c\u7f6e\u6362\u7684\u6761\u4ef6\u4e0b\uff09\u3002\u53d8\u91cf\u9009\u62e9\u65b9\u6848\u80fd\u6700\u5927\u5316\u6f5c\u5728\u56e0\u5b50\u7684\u53ef\u6062\u590d\u6027\u3002\u5e76\u5728\u5408\u6210\u56fe\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u5185\u90e8\u53ef\u89c2\u6d4b\u6e90\uff0c\u6269\u5c55\u4e86\u5f53\u524d\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u548c\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2509.18593", "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.", "AI": {"tldr": "Multi-contrast MRI\u8d85\u5206\u8fa8\u7387(MC-MRI SR)\u65e8\u5728\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u589e\u5f3a\u4f4e\u5206\u8fa8\u7387\u5bf9\u6bd4\u5ea6\u3002\u672c\u6587\u63d0\u51faSSCM\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u7a7a\u95f4\u5f62\u53d8\u3001\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u548c\u7a7a\u9891\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\u548c\u4e00\u81f4\u6027\u91cd\u5efa\u3002", "motivation": "Multi-contrast MRI\u8d85\u5206\u8fa8\u7387\u65e8\u5728\u7f29\u77ed\u91c7\u96c6\u65f6\u95f4\u3001\u63d0\u9ad8\u6210\u50cf\u6548\u7387\u5e76\u4fdd\u7559\u89e3\u5256\u7ec6\u8282\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\uff0c\u5c3d\u7ba1\u76ee\u6807\u4e0e\u53c2\u8003\u56fe\u50cf\u5b58\u5728\u7ed3\u6784\u5dee\u5f02\u548c\u8fd0\u52a8\uff0c\u4ecd\u9700\u4fdd\u6301\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u89e3\u5256\u7ed3\u6784\u5bf9\u9f50\u548c\u8fde\u8d2f\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u9891\u7387\u57df\u4fe1\u606f\u5229\u7528\u65b9\u9762\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7cbe\u7ec6\u5bf9\u9f50\u5dee\u548c\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u4e0d\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u578b\uff08Spatial-Semantic Consistent Model, SSCM\uff09\u3002SSCM\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u52a8\u6001\u7a7a\u95f4\u5f62\u53d8\u6a21\u5757\uff0c\u7528\u4e8e\u5bf9\u6bd4\u5ea6\u95f4\u7a7a\u95f4\u5bf9\u9f50\uff1b2) \u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u6a21\u5757\uff0c\u7528\u4e8e\u957f\u7a0b\u8bed\u4e49\u4e00\u81f4\u6027\uff1b3) \u7a7a\u9891\u878d\u5408\u6a21\u5757\uff0c\u7528\u4e8e\u7cbe\u7ec6\u7ed3\u6784\u6062\u590d\u3002", "result": "\u5728\u516c\u5171\u548c\u79c1\u4eba\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSCM\u5728\u53c2\u6570\u91cf\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u786e\u4fdd\u4e86\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u4e00\u81f4\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "SSCM\u6a21\u578b\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86Multi-contrast MRI\u8d85\u5206\u8fa8\u7387\u4e2d\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u7684\u6311\u6218\u3002\u8be5\u6a21\u578b\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u7684\u91cd\u5efa\uff0c\u5c55\u73b0\u51fa\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.18166", "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model.", "AI": {"tldr": "MobiGPT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u540c\u65f6\u9884\u6d4b\u57fa\u7ad9\u6d41\u91cf\u3001\u7528\u6237\u5e94\u7528\u4f7f\u7528\u548c\u4fe1\u9053\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u8f6f\u63d0\u793a\u5b66\u4e60\u548c\u65f6\u95f4\u63a9\u7801\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u5f3a\u6cdb\u5316\u548c\u5353\u8d8a\u7684\u96f6/\u5c11\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u7c7b\u578b\u7684\u5b9a\u5236\u6a21\u578b\uff0c\u5bfc\u81f4\u5728\u5927\u89c4\u6a21\u5f02\u6784\u7f51\u7edc\u4e2d\u590d\u6742\u6027\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u9884\u6d4b\u8303\u5f0f\u6765\u63d0\u9ad8\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aMobiGPT\u7684\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u7edf\u4e00\u7ed3\u6784\u6765\u9884\u6d4b\u57fa\u7ad9\u6d41\u91cf\u3001\u7528\u6237\u5e94\u7528\u4f7f\u7528\u548c\u4fe1\u9053\u8d28\u91cf\u3002\u63d0\u51fa\u4e86\u8f6f\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4ee5\u7406\u89e3\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u63a9\u7801\u673a\u5236\u4ee5\u5904\u7406\u77ed\u671f\u9884\u6d4b\u3001\u957f\u671f\u9884\u6d4b\u548c\u5206\u5e03\u751f\u6210\u7b49\u4efb\u52a1\u3002", "result": "MobiGPT\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u591a\u7c7b\u578b\u9884\u6d4b\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u5206\u522b\u63d0\u5347\u4e8627.37%\u300120.08%\u548c7.27%\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0cMobiGPT\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6/\u5c11\u6837\u672c\u6027\u80fd\uff0c\u63d0\u5347\u8d85\u8fc721.51%\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "MobiGPT\u4f5c\u4e3a\u4e00\u79cd\u7edf\u4e00\u7684\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u5b9a\u5236\u5316\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u79fb\u52a8\u6570\u636e\u7c7b\u578b\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u548c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.19143", "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201canecdoctoring\u201d\u7684\u65b0\u578b\u8de8\u8bed\u8a00\u3001\u8de8\u6587\u5316\u7ea2\u961f\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ee5\u89e3\u51b3\u751f\u6210\u5f0fAI\u7684\u5168\u7403\u865a\u5047\u4fe1\u606f\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u6ee5\u7528\u5e26\u6765\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u7684\u5de8\u5927\u98ce\u9669\u3002\u73b0\u6709\u7684\u7ea2\u961f\u8bc4\u4f30\u6570\u636e\u96c6\u4e3b\u8981\u4ee5\u7f8e\u56fd\u548c\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u8de8\u8bed\u8a00\u548c\u8de8\u6587\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u201canecdoctoring\u201d\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u8de8\u8bed\u8a00\u548c\u6587\u5316\u7684\u5bf9\u6297\u6027\u63d0\u793a\u3002\u4ece\u4e09\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5370\u5730\u8bed\uff09\u548c\u4e24\u4e2a\u5730\u533a\uff08\u7f8e\u56fd\u3001\u5370\u5ea6\uff09\u7684\u4e8b\u5b9e\u6838\u67e5\u7f51\u7ad9\u6536\u96c6\u865a\u5047\u4fe1\u606f\uff0c\u5c06\u5176\u805a\u7c7b\u6210\u66f4\u5e7f\u6cdb\u7684\u53d9\u4e8b\uff0c\u5e76\u7528\u77e5\u8bc6\u56fe\u8c31\u8868\u5f81\u8fd9\u4e9b\u96c6\u7fa4\uff0c\u6700\u540e\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u653b\u51fb\u8005LLM\u3002", "result": "\u76f8\u8f83\u4e8e\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5168\u7403\u8303\u56f4\u5185\u6709\u6548\u4e14\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u5bf9\u6297\u6027\u6ee5\u7528\u7684\u865a\u5047\u4fe1\u606f\u7f13\u89e3\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.19077", "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "\u9488\u5bf9LLM\u4f5c\u4e3aAI\u4ee3\u7406\u89c4\u5212\u5668\u9762\u4e34\u7684\u73af\u5883\u9002\u5e94\u6027\u5dee\u3001\u67e5\u8be2\u6210\u672c\u9ad8\u548c\u65e0\u6cd5\u4f18\u5316\u957f\u671f\u5956\u52b1\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86CoPiC\u3002\u8be5\u65b9\u6cd5\u5229\u7528LLM\u751f\u6210\u9ad8\u5c42\u89c4\u5212\u7a0b\u5e8f\u6765\u8fed\u4ee3\u751f\u6210\u548c\u7cbe\u70bc\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u8bba\u5668\u8bc4\u4f30\u957f\u671f\u5956\u52b1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6210\u529f\u7387\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u67e5\u8be2\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u89c4\u5212\u5668\u7531\u4e8e\u901a\u7528\u77e5\u8bc6\u4e0e\u7279\u5b9a\u73af\u5883\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5bfc\u81f4\u8ba1\u5212\u4e0d\u51c6\u786e\u3002\u4e3a\u7ea0\u6b63\u8ba1\u5212\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684LLM\u67e5\u8be2\uff0c\u5bfc\u81f4\u9ad8\u6602\u6210\u672c\uff0c\u4e14\u8fd9\u79cd\u57fa\u4e8e\u77ed\u671f\u53cd\u9988\u7684\u4fee\u6b63\u9650\u5236\u4e86LLM\u5236\u5b9a\u7b26\u5408\u957f\u671f\u5956\u52b1\u8ba1\u5212\u7684\u80fd\u529b\u3002", "method": "CoPiC\u4e0d\u4f9d\u8d56\u9891\u7e41\u67e5\u8be2\uff0c\u800c\u662f\u8ba9LLM\u751f\u6210\u4e00\u5957\u591a\u6837\u5316\u7684\u9ad8\u5c42\u89c4\u5212\u7a0b\u5e8f\uff0c\u8fd9\u4e9b\u7a0b\u5e8f\u80fd\u8fed\u4ee3\u5730\u751f\u6210\u548c\u7cbe\u70bc\u5019\u9009\u8ba1\u5212\u3002\u968f\u540e\uff0c\u4e00\u4e2a\u8bad\u7ec3\u8fc7\u7684\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u8bba\u5668\u8bc4\u4f30\u8fd9\u4e9b\u5019\u9009\u8ba1\u5212\uff0c\u5e76\u9009\u62e9\u6700\u7b26\u5408\u957f\u671f\u5956\u52b1\u7684\u8ba1\u5212\u8fdb\u884c\u6267\u884c\u3002", "result": "\u5728ALFWorld\u3001NetHack\u548cStarCraft II Unit Building\u4efb\u52a1\u4e2d\uff0cCoPiC\u7684\u8868\u73b0\u4f18\u4e8eAdaPlanner\u548cReflexion\u7b49\u5148\u8fdb\u7684LLM\u57fa\u7ebf\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8623.33%\uff0c\u67e5\u8be2\u6210\u672c\u964d\u4f4e\u4e8691.27%\u3002", "conclusion": "CoPiC\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u89c4\u5212\u7a0b\u5e8f\u4f5c\u4e3a\u89c4\u5212\u5668\u548c\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u8bba\u5668\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u6709\u6548\u6539\u5584\u4e86LLM\u5728\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u67e5\u8be2\u6210\u672c\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u957f\u671f\u5956\u52b1\u7684\u4f18\u5316\u3002"}}
{"id": "2509.18600", "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOraPO\u548cFactS\u6846\u67b6\uff0c\u5728\u8d44\u6e90\u6709\u9650\u4e0b\u5b9e\u73b0\u9ad8\u6548\u533b\u5b66\u62a5\u544a\u751f\u6210\uff0c\u5237\u65b0CheXpert Plus SOTA\uff0c\u4e14\u663e\u8457\u964d\u4f4e\u6570\u636e\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u5927\u89c4\u6a21\u914d\u5bf9\u8bed\u6599\u5e93\u548c\u8d85\u5927\u9aa8\u5e72\u7f51\u7edc\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u6570\u636e\u5bc6\u96c6\uff0c\u96be\u4ee5\u5728\u9884\u7b97\u53d7\u9650\u60c5\u51b5\u4e0b\u5e94\u7528\u3002", "method": "\u63d0\u51faOracle-educated GRPO (OraPO) \u7ed3\u5408FactScore-based reward (FactS)\u3002OraPO\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u8a00\u673a\u5c06GRPO\u5931\u8d25\u63a2\u7d22\u8f6c\u5316\u4e3a\u76f4\u63a5\u504f\u597d\u76d1\u7763\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5\u3001\u7eaf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002FactS\u901a\u8fc7\u63d0\u53d6\u539f\u5b50\u4e34\u5e8a\u4e8b\u5b9e\u5e76\u4e0e\u771f\u5b9e\u6807\u7b7e\u6838\u5bf9\uff0c\u63d0\u4f9b\u5bc6\u96c6\u3001\u53ef\u89e3\u91ca\u7684\u53e5\u5b50\u7ea7\u5956\u52b1\uff0c\u5c06\u5b66\u4e60\u4e0e\u8bca\u65ad\u8bc1\u636e\u76f8\u7ed3\u5408\u3002", "result": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u6311\u6218\u6027\u75c5\u4f8b\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5728CheXpert Plus\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff08F1\u503c\u4e3a0.341\uff09\uff0c\u5e76\u4e14\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c11\u4e862-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u4ec5\u4f7f\u7528\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u666e\u901a\u786c\u4ef6\u3002", "conclusion": "OraPO\u548cFactS\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u7d27\u51d1\u800c\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\u3002"}}
{"id": "2509.18169", "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PiMoE\uff08Physically-isolated Mixture of Experts\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u751f\u6574\u5408\u8ba1\u7b97\u80fd\u529b\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u4e0e\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u7684\u4ea4\u66ff\u8fdb\u884c\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e0\u6cd5\u5c06\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u4f5c\u4e3a\u5185\u5728\u53ef\u89e3\u91ca\u7684\u80fd\u529b\uff0c\u800c\u4e3b\u6d41\u591a\u4ee3\u7406\u65b9\u6cd5\u867d\u80fd\u5229\u7528\u5916\u90e8\u4e13\u5bb6\uff0c\u4f46\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u3001\u591a\u6a21\u6001\u80fd\u529b\u4f4e\u6548\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "PiMoE\u662f\u4e00\u79cd\u8bad\u7ec3\u548c\u63a8\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u72ec\u8bad\u7ec3\u4e13\u5bb6\u3001\u4e00\u4e2a\u6587\u672c\u5230\u8ba1\u7b97\u6a21\u5757\u4ee5\u53ca\u4e00\u4e2a\u8def\u7531\u5668\uff0c\u5c06\u8ba1\u7b97\u80fd\u529b\u5185\u751f\u5730\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8def\u7531\u5668\u5728token\u7ea7\u522b\u5f15\u5bfc\u8ba1\u7b97\u548c\u63a8\u7406\u7684\u8fed\u4ee3\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u4e0e\u76f4\u63a5\u5fae\u8c03LLMs\u76f8\u6bd4\uff0cPiMoE\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff1b\u4e0e\u4e3b\u6d41\u591a\u4ee3\u7406\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u54cd\u5e94\u5ef6\u8fdf\u3001token\u4f7f\u7528\u548cGPU\u80fd\u8017\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "PiMoE\u4e3a\u4e0b\u4e00\u4ee3\u79d1\u5b66\u6216\u5de5\u4e1a\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u6574\u5408\u8ba1\u7b97\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.19163", "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "categories": ["cs.CL"], "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u6784\u5efa\u4e86AI\u4f4e\u8d28\u91cf\u6587\u672c\uff08'slop'\uff09\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u53d1\u73b0\u5176\u4e3b\u89c2\u6027\u4e0e\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u6709\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30AI\u751f\u6210\u6587\u672c\u8d28\u91cf\u7684\u6846\u67b6\u3002", "motivation": "AI\u751f\u6210\u6587\u672c\u7684\u4f4e\u8d28\u91cf\u73b0\u8c61\uff08'slop'\uff09\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u516c\u8ba4\u7684\u5b9a\u4e49\u548c\u8861\u91cf\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u91c7\u8bbfNLP\u3001\u5199\u4f5c\u548c\u54f2\u5b66\u4e13\u5bb6\uff0c\u5f00\u53d1\u4e86'slop'\u7684\u5206\u7c7b\u6cd5\u5e76\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff1b\u91c7\u7528\u8de8\u5ea6\u7ea7\u6807\u6ce8\uff08span-level annotation\uff09\u6765\u5206\u6790\u4e8c\u5143\u5224\u65ad\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u7684\u5173\u8054\u3002", "result": "\u53d1\u73b0\u5bf9'slop'\u7684\u4e8c\u5143\u5224\u65ad\u5177\u6709\u4e00\u5b9a\u4e3b\u89c2\u6027\uff0c\u4f46\u8fd9\u4e9b\u5224\u65ad\u4e0e\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u7b49\u6f5c\u5728\u7ef4\u5ea6\u5b58\u5728\u5173\u8054\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684\u6846\u67b6\u53ef\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u548c\u504f\u597d\u4efb\u52a1\uff0c\u5e76\u6709\u671b\u4e3a\u7406\u89e3\u5f71\u54cd\u8d28\u91cf\u5224\u65ad\u7684\u8bed\u8a00\u548c\u98ce\u683c\u56e0\u7d20\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.19236", "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "categories": ["cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u667a\u80fd\u4f53\u56e2\u961f\u7ed3\u6784\u6765\u4fc3\u8fdb\u534f\u4f5c\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u663e\u8457\u964d\u4f4e\u4e86token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u751f\u6210\u667a\u80fd\u4f53\u5728\u540e\u7eed\u9636\u6bb5\u7684\u534f\u4f5c\u9700\u6c42\uff0c\u5bfc\u81f4\u56e2\u961f\u7ec4\u6210\u4e0d\u4f73\uff0c\u5f71\u54cd\u7cfb\u7edf\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86AgentInit\u65b9\u6cd5\u3002\u5b83\u5728\u667a\u80fd\u4f53\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u591a\u8f6e\u4ea4\u4e92\u548c\u53cd\u601d\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\uff08Natural Language to Format\uff09\u673a\u5236\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u6807\u51c6\u5316\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5e15\u7d2f\u6258\u539f\u5219\u7684\u5e73\u8861\u56e2\u961f\u9009\u62e9\u7b56\u7565\uff0c\u7efc\u5408\u8003\u8651\u56e2\u961f\u591a\u6837\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentInit\u5728\u4e0d\u540c\u6846\u67b6\u548c\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u521d\u59cb\u5316\u65b9\u6cd5\u548c\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u6027\u80fd\u63d0\u5347\u5206\u522b\u9ad8\u8fbe1.2\u500d\u548c1.6\u500d\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86token\u6d88\u8017\u3002\u6b64\u5916\uff0c\u5b83\u663e\u793a\u51fa\u5f3a\u5927\u7684\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9274\u4e8e\u5176\u5353\u8d8a\u7684\u6027\u80fd\u63d0\u5347\u3001\u8d44\u6e90\u6d88\u8017\u964d\u4f4e\u4ee5\u53ca\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\uff0cAgentInit\u662f\u4e00\u79cd\u80fd\u529b\u5f3a\u3001\u9002\u5e94\u6027\u5e7f\u4e14\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.18602", "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "categories": ["cs.CV"], "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAMSF\uff0c\u4e00\u4e2a\u514d\u8bad\u7ec3\u3001\u57fa\u4e8e\u53c2\u8003\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u591a\u4e2a\u53c2\u8003\u98ce\u683c\u7684\u53ef\u63a7\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684\u98ce\u683c\u878d\u5408\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e00\u662f\u901a\u5e38\u53ea\u80fd\u63a5\u53d7\u4e00\u5f20\u98ce\u683c\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u6df7\u5408\u7f8e\u5b66\u548c\u591a\u98ce\u683c\u6269\u5c55\u6027\uff1b\u4e8c\u662f\u7f3a\u4e4f\u5e73\u8861\u591a\u79cd\u98ce\u683c\u5f71\u54cd\u7684\u6709\u6548\u673a\u5236\u3002", "method": "AMSF\u901a\u8fc7\u8bed\u4e49token\u5206\u89e3\u6a21\u5757\u7f16\u7801\u6240\u6709\u98ce\u683c\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u81ea\u9002\u5e94\u5730\u6ce8\u5165\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u7684\u6bcf\u4e2a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u76f8\u4f3c\u5ea6\u611f\u77e5\u91cd\u52a0\u6743\u6a21\u5757\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u52a8\u6001\u8c03\u6574\u5206\u914d\u7ed9\u6bcf\u4e2a\u98ce\u683c\u7ec4\u4ef6\u7684\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u5e73\u8861\u4e14\u7528\u6237\u53ef\u63a7\u7684\u98ce\u683c\u878d\u5408\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6216\u5916\u90e8\u9002\u914d\u5668\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u8868\u660e\uff0cAMSF\u751f\u6210\u7684\u591a\u98ce\u683c\u878d\u5408\u7ed3\u679c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u5176\u8bbe\u8ba1\u53ef\u4ee5\u65e0\u7f1d\u6269\u5c55\u5230\u4e24\u79cd\u6216\u66f4\u591a\u98ce\u683c\u7684\u878d\u5408\u3002", "conclusion": "AMSF\u7684\u8fd9\u4e9b\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u5bcc\u6709\u8868\u73b0\u529b\u7684\u591a\u98ce\u683c\u751f\u6210\u65b9\u9762\u8fc8\u51fa\u7684\u5b9e\u7528\u4e00\u6b65\u3002"}}
{"id": "2509.18171", "pdf": "https://arxiv.org/pdf/2509.18171", "abs": "https://arxiv.org/abs/2509.18171", "authors": ["Zhanting Zhou", "KaHou Tam", "Zeqin Wu", "Pengzhao Sun", "Jinbo Wang", "Fengli Zhang"], "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification", "categories": ["cs.LG"], "comment": null, "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19170", "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CoT\uff09\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u96be\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u9650\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u8f6f\u201dtoken\u548c\u8f93\u5165\u5d4c\u5165\u566a\u58f0\u5b9e\u73b0\u63a2\u7d22\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4e0e\u79bb\u6563CoT\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u5728\u591a\u6837\u6027\u4e0a\u66f4\u4f18\uff0c\u5e76\u80fd\u517c\u5bb9\u6807\u51c6\u90e8\u7f72\u3002", "motivation": "\u8fde\u7eedtoken\u5728LLMs\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u56e0\u5176\u66f4\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u6a21\u62df\u591a\u6761\u63a8\u7406\u8def\u5f84\u7684\u6f5c\u529b\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u8bad\u7ec3\u56f0\u96be\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u9700\u8981\u4ece\u79bb\u6563CoT\u84b8\u998f\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u957f\u601d\u7ef4\u94fe\u573a\u666f\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u9996\u6b21\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u6765\u5b66\u4e60\u8fde\u7eedCoT\uff0c\u65e0\u9700\u4ece\u53c2\u8003\u79bb\u6563CoT\u8fdb\u884c\u84b8\u998f\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u201c\u8f6f\u201dtoken\uff08token\u6df7\u5408\u7269\uff09\u5e76\u7ed3\u5408\u8f93\u5165\u5d4c\u5165\u566a\u58f0\u6765\u63d0\u4f9bRL\u63a2\u7d22\u3002\u6b64\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff0c\u80fd\u591f\u5b66\u4e60\u5305\u542b\u6570\u767e\u4e2atoken\u7684\u8fde\u7eedCoT\u3002", "result": "\u5728Llama\u548cQwen\u7b498B\u6a21\u578b\u4e0a\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u8fde\u7eedCoT\u8bad\u7ec3\u7684\u6a21\u578b\u5728pass@1\u4e0a\u4e0e\u79bb\u6563token CoT\u8868\u73b0\u6301\u5e73\uff0c\u4f46\u5728pass@32\u4e0a\u8d85\u8d8a\u4e86\u540e\u8005\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684CoT\u591a\u6837\u6027\u3002\u6700\u4f73\u65b9\u6848\u662f\u4f7f\u7528\u8fde\u7eedCoT token\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u79bb\u6563token\u8fdb\u884c\u63a8\u7406\u3002\u6b64\u5916\uff0c\u8fde\u7eedCoT RL\u8bad\u7ec3\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u5728\u57df\u5916\u4efb\u52a1\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u5f71\u54cd\u66f4\u5c0f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u8bad\u7ec3\u8fde\u7eed\u601d\u7ef4\u94fe\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548cCoT\u591a\u6837\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u4ee5\u6807\u51c6\u65b9\u5f0f\u90e8\u7f72\uff0c\u540c\u65f6\u5bf9\u57fa\u7840\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.19265", "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u4e16\u754c\u7684\u8de8\u6587\u5316\u5e38\u8bc6\u63a8\u7406\u8f6c\u79fb\uff0c\u53d1\u73b0\u5c11\u91cf\u6587\u5316\u7279\u5b9a\u793a\u4f8b\u6216\u8de8\u6587\u5316\u793a\u4f8b\u5747\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u9ad8\u6548\u8de8\u6587\u5316\u5bf9\u9f50\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u4f4e\u8d44\u6e90\u6587\u5316\u73af\u5883\u4e0b\u7684LLM\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u666e\u904d\u5b58\u5728\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6837\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002\u5c3d\u7ba1\u5df2\u6709\u4e00\u4e9b\u6587\u5316\u5bf9\u9f50\u7814\u7a76\uff0c\u4f46\u5229\u7528\u4e00\u79cd\u6587\u5316\u80cc\u666f\u7684\u5bf9\u9f50\u6765\u63d0\u5347\u5176\u4ed6\u6587\u5316\u80cc\u666f\u6027\u80fd\u7684\u8de8\u6587\u5316\u8f6c\u79fb\u6f5c\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6db5\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u7684\u6587\u5316\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u60c5\u5883\u5b66\u4e60\u548c\u57fa\u4e8e\u6f14\u793a\u7684\u5f3a\u5316\uff08DITTO\uff09\u7b49\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u4e0e\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ec5\u752812\u4e2a\u6765\u81ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u6587\u5316\u7279\u5b9a\u793a\u4f8b\uff0c\u5373\u53ef\u5e73\u5747\u63d0\u5347\u5176\u4ed6\u56fd\u5bb6\u6027\u80fd10%\u3002\u6b64\u5916\uff0c\u6765\u81ea\u5370\u5ea6\u5c3c\u897f\u4e9a\u548c\u7f8e\u56fd\u7b49\u975e\u672c\u6587\u5316\u80cc\u666f\u7684\u6f14\u793a\uff0c\u5728\u591a\u9879\u9009\u62e9\u9898\u63a8\u7406\u4e2d\u80fd\u4e0e\u672c\u6587\u5316\u5bf9\u9f50\u6548\u679c\u6301\u5e73\u6216\u8d85\u8d8a\uff0c\u51f8\u663e\u4e86\u6587\u5316\u5e38\u8bc6\u7684\u8de8\u6587\u5316\u53ef\u8f6c\u79fb\u6027\u4e0d\u4ec5\u9650\u4e8e\u963f\u62c9\u4f2f\u4e16\u754c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u6548\u7684\u8de8\u6587\u5316\u5bf9\u9f50\u662f\u53ef\u80fd\u5b9e\u73b0\u7684\uff0c\u4e3a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u5230\u8d44\u6e90\u532e\u4e4f\u7684\u6587\u5316\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18613", "pdf": "https://arxiv.org/pdf/2509.18613", "abs": "https://arxiv.org/abs/2509.18613", "authors": ["Yuzhi Wu", "Li Xiao", "Jun Liu", "Guangfeng Jiang", "XiangGen Xia"], "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMLF-4DRCNet\uff0c\u4e00\u4e2a\u7528\u4e8e3D\u76ee\u6807\u68c0\u6d4b\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u3001\u573a\u666f\u548c\u63d0\u8bae\u5c42\u9762\u7684\u591a\u7ea7\u878d\u54084D\u96f7\u8fbe\u548c\u76f8\u673a\u56fe\u50cf\uff0c\u89e3\u51b3\u4e864D\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u751a\u81f3\u53ef\u4e0eLiDAR-based\u6a21\u578b\u5ab2\u7f8e\u3002", "motivation": "4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u70b9\u4e91\u7a00\u758f\u4e14\u566a\u58f0\u5927\uff0c\u9650\u5236\u4e86\u5176\u72ec\u7acb\u8fdb\u884c3D\u76ee\u6807\u68c0\u6d4b\u3002\u73b0\u6709\u7684\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u65b9\u6cd5\u591a\u91c7\u7528\u4e3aLiDAR-\u76f8\u673a\u8bbe\u8ba1\u7684BEV\u878d\u5408\u8303\u5f0f\uff0c\u5ffd\u7565\u4e86\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u548c\u4e0d\u5b8c\u6574\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4e14\u878d\u5408\u4ec5\u9650\u4e8e\u7c97\u7cd9\u7684\u573a\u666f\u5c42\u9762\u3002", "method": "\u672c\u6587\u63d0\u51faMLF-4DRCNet\uff0c\u4e00\u4e2a\u7528\u4e8e4D\u96f7\u8fbe\u548c\u76f8\u673a\u56fe\u50cf\u591a\u7ea7\u878d\u5408\u76843D\u76ee\u6807\u68c0\u6d4b\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u70b9\u3001\u573a\u666f\u548c\u63d0\u8bae\u5c42\u9762\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\n1.  **Enhanced Radar Point Encoder (ERPE)**\uff1a\u5728\u70b9\u5c42\u9762\uff0c\u5229\u75282D\u56fe\u50cf\u5b9e\u4f8b\u5bc6\u96c6\u5316\u96f7\u8fbe\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7Triple-Attention Voxel Feature Encoder\u5c06\u5176\u7f16\u7801\u4e3a\u4f53\u7d20\u3002\n2.  **Hierarchical Scene Fusion Pooling (HSFP)**\uff1a\u4f7f\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u52a8\u6001\u6574\u5408\u591a\u5c3a\u5ea6\u4f53\u7d20\u7279\u5f81\u548c2D\u56fe\u50cf\u7279\u5f81\uff0c\u4ee5\u6355\u83b7\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u5e76\u5bf9\u878d\u5408\u7279\u5f81\u8fdb\u884c\u6c60\u5316\u3002\n3.  **Proposal-Level Fusion Enhancement (PLFE)**\uff1a\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u7279\u5f81\u6765\u7ec6\u5316\u533a\u57df\u63d0\u8bae\uff0c\u5e76\u8fdb\u4e00\u6b65\u4e0eHSFP\u7684\u6c60\u5316\u7279\u5f81\u6574\u5408\u3002", "result": "MLF-4DRCNet\u5728View-of-Delft (VoD) \u548c TJ4DRadSet \u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5728VoD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u57fa\u4e8eLiDAR\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "MLF-4DRCNet\u901a\u8fc7\u521b\u65b0\u7684\u591a\u7ea7\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u514b\u670d\u4e864D\u96f7\u8fbe\u70b9\u4e91\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e3a3D\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u5176\u6027\u80fd\u53ef\u4e0eLiDAR\u7cfb\u7edf\u5ab2\u7f8e\uff0c\u5c55\u73b0\u4e864D\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18172", "pdf": "https://arxiv.org/pdf/2509.18172", "abs": "https://arxiv.org/abs/2509.18172", "authors": ["Wonjun Bang", "Jongseok Park", "Hongseung Yu", "Kyungmin Bin", "Kyunghan Lee"], "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization", "categories": ["cs.LG"], "comment": "9 pages, 4 figures", "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSBVR\uff0c\u4e00\u79cd\u65b0\u578bLLM\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6\u53cb\u597d\u7684\u9ad8\u65af\u5206\u5e03\u7f16\u7801\u548c\u5b9a\u5236CUDA\u6838\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u3001\u9ad8\u7cbe\u5ea6\u53ca\u5feb\u901f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53c2\u6570\u91cf\u5de8\u5927\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u57fa\u4e8eRTN\u7684\u65b9\u6cd5\u672a\u8003\u8651LLM\u6743\u91cd\u7684\u9ad8\u65af\u5206\u5e03\uff1b\u57fa\u4e8e\u7801\u672c\u7684\u65b9\u6cd5\u867d\u8003\u8651\u5206\u5e03\uff0c\u4f46\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51faSBVR\uff08Summation of BitVector Representation\uff09\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u503c\u6620\u5c04\u5230\u7b26\u5408LLM\u6743\u91cd\u5b9e\u9645\u9ad8\u65af\u5206\u5e03\u7684\u975e\u5747\u5300\u8868\u793a\u70b9\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u538b\u7f29\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u5b9a\u5236CUDA\u6838\uff0c\u76f4\u63a5\u5728SBVR\u683c\u5f0f\u4e0b\u8fdb\u884c\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\uff0c\u65e0\u9700\u89e3\u538b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u6267\u884c\u3002", "result": "SBVR\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u56f0\u60d1\u5ea6\u548c\u51c6\u786e\u6027\u57fa\u51c6\u6027\u80fd\u3002\u57284\u6bd4\u7279\u91cf\u5316\u4e0b\uff0c\u76f8\u8f83\u4e8e\u539f\u751fFP16\u6a21\u578b\uff0c\u7aef\u5230\u7aeftoken\u751f\u6210\u901f\u5ea6\u63d0\u5347\u4e862.21\u500d\u81f33.04\u500d\u3002", "conclusion": "SBVR\u901a\u8fc7\u5b9e\u73b0\u9ad8\u65af\u5206\u5e03\u7684\u4ee3\u7801\u8868\u793a\u548c\u786c\u4ef6\u53cb\u597d\u7684\u5b9a\u5236CUDA\u6838\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709LLM\u91cf\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b9e\u73b0\u51c6\u786e\u538b\u7f29\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19199", "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPRL\u7684\u5728\u7ebf\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4f5c\u4e3a\u667a\u80fd\u4f53\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u8fdb\u884c\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u6362\u4e3a\u9690\u5f0f\u6b65\u5956\u52b1\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "LLM\u4f5c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u671f\u63a8\u7406\u548c\u884c\u52a8\u65f6\uff0c\u9762\u4e34\u7a00\u758f\u4e14\u6709\u65f6\u96be\u4ee5\u9a8c\u8bc1\u7684\u5956\u52b1\u5e26\u6765\u7684\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u96be\u9898\u3002\u73b0\u6709\u5c06\u8fc7\u7a0b\u76d1\u7763\u6574\u5408\u5230\u667a\u80fd\u4f53\u5b66\u4e60\u4e2d\u7684\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u504f\u5dee\u3001\u5956\u52b1\u64cd\u7eb5\u3001\u4fe1\u53f7\u8fc7\u4e8e\u7ec6\u7c92\u5ea6\u5bfc\u81f4\u9ad8\u65b9\u5dee\u6216\u72b6\u6001\u91cd\u53e0\u7a00\u5c11\u65f6\u7684\u5931\u6548\u95ee\u9898\u3002", "method": "OPRL\u662f\u4e00\u79cd\u901a\u7528\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6on-policy\u7b97\u6cd5\u4e2d\uff0c\u65e0\u9700\u989d\u5916rollouts\u6216\u663e\u5f0f\u6b65\u9aa4\u6807\u7b7e\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u8f68\u8ff9\u7684DPO\u76ee\u6807\uff0c\u4ea4\u66ff\u4f18\u5316\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u548c\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u5956\u52b1\u3002\u8fd9\u4e9b\u6b65\u5956\u52b1\u4e0e\u60c5\u8282\u7ea7\u7ed3\u679c\u5956\u52b1\u7ed3\u5408\uff0c\u7528\u4e8e\u7b56\u7565\u66f4\u65b0\uff0c\u5f62\u6210\u4e00\u4e2a\u81ea\u6211\u5f3a\u5316\u7684\u5faa\u73af\u3002", "result": "OPRL\u5728WebShop\u3001VisualSokoban\u548cSOTOPIA\u7b49\u4e09\u4e2a\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u4f18\u4e8e\u9886\u5148\u7684LLMs\u548c\u5f3a\u5927\u7684RL\u57fa\u7ebf\u3002\u5b83\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u4f4e\u7684\u8bad\u7ec3\u65b9\u5dee\u3002\u6b64\u5916\uff0cOPRL\u8fd8\u901a\u8fc7\u66f4\u5c11\u7684\u64cd\u4f5c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a2\u7d22\u3002", "conclusion": "OPRL\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6548\u7387\uff0c\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u9884\u793a\u7740\u5176\u5728\u5b9e\u9645\u667a\u80fd\u4f53\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18619", "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "categories": ["cs.CV"], "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "PDLS\u6846\u67b6\u901a\u8fc7\u53cc\u6f5c\u5728\u7a7a\u95f4\u5f15\u5bfc\uff08\u7ed3\u6784\u4e0e\u8bed\u4e49\uff09\uff0c\u5229\u7528LQR\u4f18\u5316\u63a7\u5236\uff0c\u5b9e\u73b0\u5bf9\u635f\u574f\u56fe\u50cf\u7684\u7a33\u5b9a\u53cd\u6f14\uff0c\u5728\u4fdd\u6301\u7ec6\u8282\u7684\u540c\u65f6\u907f\u514d\u8bed\u4e49\u6f02\u79fb\uff0c\u65e0\u9700\u6bcf\u56fe\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u635f\u574f\u56fe\u50cf\u53cd\u6f14\u5230\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\uff0c\u5982\u7ec6\u8282\u6a21\u7cca\u6216\u5c5e\u6027\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u65e0\u8bad\u7ec3\u7684Prompt-Guided Dual Latent Steering (PDLS) \u6846\u67b6\uff0c\u57fa\u4e8eRectified Flow\u6a21\u578b\u3002PDLS\u5c06\u53cd\u6f14\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7ed3\u6784\u8def\u5f84\u548c\u8bed\u4e49\u8def\u5f84\u4e24\u4e2a\u4e92\u8865\u6d41\uff0c\u5e76\u5c06\u5176\u5efa\u6a21\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\uff0c\u4ece\u800c\u5728\u6bcf\u4e00\u6b65\u52a8\u6001\u5f15\u5bfc\u751f\u6210\u8f68\u8ff9\uff0c\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\u5e76\u4fdd\u7559\u7ec6\u8282\u3002", "result": "\u5728FFHQ-1K\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u81ea\u7531\u5f62\u5f0f\u4fee\u590d\u7b49\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660ePDLS\u751f\u6210\u7684\u91cd\u5efa\u56fe\u50cf\u6bd4\u5355\u4e00\u6f5c\u5728\u7a7a\u95f4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5fe0\u5b9e\u4e8e\u539f\u59cb\u56fe\u50cf\uff0c\u4e14\u4e0e\u8bed\u4e49\u4fe1\u606f\u66f4\u4e00\u81f4\u3002", "conclusion": "PDLS\u901a\u8fc7\u6709\u6548\u7684\u53cc\u91cd\u5f15\u5bfc\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u635f\u574f\u56fe\u50cf\u53cd\u6f14\u4e2d\u7684\u7ed3\u6784\u4e0e\u8bed\u4e49\u5e73\u8861\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u65e0\u8bed\u4e49\u6f02\u79fb\u7684\u91cd\u5efa\uff0c\u4f18\u4e8e\u73b0\u6709\u5355\u4e00\u6f5c\u5728\u7a7a\u95f4\u65b9\u6cd5\u3002"}}
{"id": "2509.18173", "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5730\u7406\u7a7a\u95f4\u8def\u5f84\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u8def\u5f84\u53cd\u8f6c\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5305\u62ec\u65e0\u6cd5\u8fd4\u56de\u8d77\u70b9\u3001\u975e\u6700\u4f18\u89e3\u3001\u4f4e\u9c81\u68d2\u6027\u548c\u5bf9\u9519\u8bef\u7b54\u6848\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "motivation": "\u4eba\u7c7b\u80fd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff0c\u4f46LLMs\u7684\u5730\u7406\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u5c1a\u5f85\u63a2\u7d22\u3002\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u975e\u91cf\u5316\u6307\u6807\u3001\u6709\u9650\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u4e0d\u6e05\u6670\u7684\u7814\u7a76\u5c42\u6b21\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6765\u81ea\u5168\u740312\u4e2a\u5927\u90fd\u5e02\u768436000\u6761\u8def\u5f84\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\u3002\u5f15\u5165\u4e86PathBuilder\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u5bfc\u822a\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u4e92\u8f6c\u6362\u3002\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6307\u6807\uff0c\u4e25\u683c\u8bc4\u4f30\u4e8611\u4e2a\u6700\u5148\u8fdb\u7684LLMs\u5728\u8def\u5f84\u53cd\u8f6c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cLLMs\u5728\u53cd\u8f6c\u8def\u5f84\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5927\u591a\u6570\u53cd\u8f6c\u8def\u5f84\u65e2\u4e0d\u80fd\u8fd4\u56de\u8d77\u70b9\uff0c\u4e5f\u4e0d\u63a5\u8fd1\u6700\u4f18\u8def\u5f84\u3002\u6b64\u5916\uff0cLLMs\u5728\u8def\u5f84\u751f\u6210\u65b9\u9762\u9c81\u68d2\u6027\u8f83\u4f4e\uff0c\u5e76\u4e14\u5bf9\u5176\u9519\u8bef\u7b54\u6848\u8868\u73b0\u51fa\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "LLMs\u5728\u5730\u7406\u7a7a\u95f4\u8def\u5f84\u8ba4\u77e5\uff0c\u7279\u522b\u662f\u8def\u5f84\u53cd\u8f6c\u4efb\u52a1\u4e0a\uff0c\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u4e9f\u9700\u6539\u8fdb\u5176\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.19212", "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "categories": ["cs.CL", "cs.AI"], "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSafeCoDe\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u52a8\u6001\u8c03\u6574token\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u5b89\u5168\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u5e73\u8861\u8fc7\u654f\u548c\u4e0d\u654f\u611f\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u5b89\u5168\u51b3\u7b56\u7684\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fc7\u5ea6\u654f\u611f\uff08\u4e0d\u5408\u7406\u62d2\u7edd\u826f\u6027\u67e5\u8be2\uff09\u548c\u4e0d\u654f\u611f\uff08\u9057\u6f0f\u89c6\u89c9\u98ce\u9669\uff09\uff0c\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u6301\u7eed\u7a7a\u767d\u3002", "method": "\u5f15\u5165Safety-aware Contrastive Decoding (SafeCoDe)\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\u52a8\u6001\u8c03\u6574token\u751f\u6210\uff1a1) \u5bf9\u6bd4\u89e3\u7801\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u56fe\u50cf\u548c\u9ad8\u65af\u566a\u58f0\u56fe\u50cf\u6765\u7a81\u51fa\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\u654f\u611f\u7684token\uff1b2) \u5168\u5c40\u611f\u77e5token\u8c03\u5236\u7b56\u7565\uff0c\u5c06\u573a\u666f\u7ea7\u63a8\u7406\u4e0etoken\u7ea7\u8c03\u6574\u76f8\u7ed3\u5408\uff0c\u6839\u636e\u9884\u6d4b\u7684\u5b89\u5168\u7ed3\u8bba\u8c03\u6574\u62d2\u7edd\u884c\u4e3a\u3002", "result": "\u5728\u591a\u6837\u5316\u7684MLLM\u67b6\u6784\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u4e0d\u654f\u611f\u3001\u8fc7\u5ea6\u654f\u611f\u548c\u4e00\u822c\u5b89\u5168\u8bc4\u4f30\uff09\u4e2d\uff0cSafeCoDe\u6301\u7eed\u6539\u8fdb\u4e86\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff08helpfulnes\uff09\u3002", "conclusion": "SafeCoDe\u901a\u8fc7\u52a8\u6001\u8c03\u6574token\u751f\u6210\u548c\u96c6\u6210\u573a\u666f\u7ea7\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728\u5b89\u5168\u5bf9\u9f50\u4e2d\u8fc7\u654f\u548c\u4e0d\u654f\u611f\u7684\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u529f\u80fd\u6027\u3002"}}
{"id": "2509.18638", "pdf": "https://arxiv.org/pdf/2509.18638", "abs": "https://arxiv.org/abs/2509.18638", "authors": ["Yiwei Lyu", "Samir Harake", "Asadur Chowdury", "Soumyanil Banerjee", "Rachel Gologorsky", "Shixuan Liu", "Anna-Katharina Meissner", "Akshay Rao", "Chenhui Zhao", "Akhil Kondepudi", "Cheng Jiang", "Xinhai Hou", "Rushikesh S. Joshi", "Volker Neuschmelting", "Ashok Srinivasan", "Dawn Kleindorfer", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Learning neuroimaging models from health system-scale data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPrima\uff0c\u4e00\u4e2a\u57fa\u4e8e22\u4e07\u4f59\u4efdMRI\u7814\u7a76\u8bad\u7ec3\u7684\u9996\u4e2a\u795e\u7ecf\u5f71\u50cf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u7f13\u89e3\u533b\u7597\u7cfb\u7edf\u538b\u529b\uff0c\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u5e76\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709AI\u6a21\u578b\u3002", "motivation": "\u795e\u7ecf\u5f71\u50cf\uff08MRI\uff09\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u5bfc\u81f4\u533b\u7597\u7cfb\u7edf\u8d1f\u62c5\u52a0\u91cd\u3001\u5468\u8f6c\u65f6\u95f4\u5ef6\u957f\u548c\u533b\u751f\u804c\u4e1a\u5026\u6020\uff0c\u5c24\u5176\u5bf9\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u5f71\u54cd\u663e\u8457\u3002\u8feb\u5207\u9700\u8981AI\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5229\u7528\u5927\u578b\u5b66\u672f\u533b\u7597\u7cfb\u7edf\u4f5c\u4e3a\u6570\u636e\u5f15\u64ce\uff0c\u5f00\u53d1\u4e86Prima\u6a21\u578b\uff0c\u5b83\u662f\u9996\u4e2a\u652f\u6301\u771f\u5b9e\u4e34\u5e8aMRI\u7814\u7a76\u8f93\u5165\u7684\u795e\u7ecf\u5f71\u50cfAI\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002Prima\u91c7\u7528\u5206\u5c42\u89c6\u89c9\u67b6\u6784\uff0c\u8bad\u7ec3\u6570\u636e\u8d85\u8fc722\u4e07\u4efdMRI\u7814\u7a76\u3002\u8be5\u6a21\u578b\u5728\u4e00\u4e2a\u4e3a\u671f\u4e00\u5e74\u7684\u3001\u5305\u542b3\u4e07\u4efdMRI\u7814\u7a76\u7684\u533b\u7597\u7cfb\u7edf\u8303\u56f4\u7814\u7a76\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "Prima\u572852\u79cd\u4e3b\u8981\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff08\u5305\u62ec\u80bf\u7624\u3001\u708e\u75c7\u3001\u611f\u67d3\u548c\u53d1\u80b2\u6027\u75c5\u53d8\uff09\u7684\u653e\u5c04\u8bca\u65ad\u4e2d\uff0c\u5e73\u5747\u8bca\u65adROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u8fbe\u523092.0\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u901a\u7528\u548c\u533b\u5b66AI\u6a21\u578b\u3002\u5b83\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9274\u522b\u8bca\u65ad\u3001\u653e\u5c04\u79d1\u533b\u751f\u5de5\u4f5c\u5217\u8868\u4f18\u5148\u7ea7\u548c\u4e34\u5e8a\u8f6c\u8bca\u5efa\u8bae\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u654f\u611f\u7fa4\u4f53\u7684\u7b97\u6cd5\u516c\u5e73\u6027\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u533b\u7597\u7cfb\u7edf\u504f\u89c1\uff0c\u5982\u7f29\u77ed\u4f4e\u8d44\u6e90\u4eba\u7fa4\u7684\u5468\u8f6c\u65f6\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u533b\u7597\u7cfb\u7edf\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982Prima\uff09\u7684\u53d8\u9769\u6f5c\u529b\u53ca\u5176\u5728\u63a8\u52a8AI\u9a71\u52a8\u533b\u7597\u4fdd\u5065\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u7684\u6311\u6218\u5e76\u4fc3\u8fdb\u533b\u7597\u516c\u5e73\u6027\u3002"}}
{"id": "2509.18200", "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u6846\u67b6\u548c\u5bf9\u8bdd\u65b9\u4f4d\u63a8\u7406\uff08COR\uff09\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u5bf9\u8bdd\u667a\u80fd\u4f53\u5c06\u81ea\u6211\u4e2d\u5fc3\u8bdd\u8bed\uff08\u5982\u201c\u5728\u6211\u53f3\u8fb9\u201d\uff09\u8f6c\u5316\u4e3a\u7edd\u5bf9\u65b9\u5411\uff08\u5982\u201cN/E/S/W\u201d\uff09\u7684\u6311\u6218\u3002MCoT\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\u8fc7\u7a0b\u6574\u5408\u8bed\u97f3\u548c\u5730\u6807\u5750\u6807\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u548c\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u53caASR\u8f6c\u5f55\u73af\u5883\u4e2d\u3002", "motivation": "\u5bf9\u8bdd\u667a\u80fd\u4f53\u5728GPS\u4fe1\u53f7\u5f31\u3001\u5730\u56fe\u4fe1\u606f\u7f3a\u4e4f\u7684\u5ba4\u5185\u6216\u590d\u6742\u73af\u5883\u4e2d\uff0c\u9700\u8981\u5c06\u7528\u6237\u81ea\u6211\u4e2d\u5fc3\uff08egocentric\uff09\u7684\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7edd\u5bf9\u65b9\u4f4d\uff08allocentric\uff09\u3002\u73b0\u6709\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u591a\u6a21\u6001\u7a7a\u95f4\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\u5c1a\u4e0d\u5145\u5206\uff0c\u5c24\u5176\u5728\u975e\u82f1\u8bed\u548cASR\u8f6c\u5f55\u573a\u666f\u4e0b\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u5f15\u5165\u4e86\u201c\u5bf9\u8bdd\u65b9\u4f4d\u63a8\u7406\uff08COR\uff09\u201d\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eASR\u8f6c\u5f55\u7684\u7e41\u4f53\u4e2d\u6587\u5bf9\u8bdd\u5bfc\u822a\u4e2d\u7684\u81ea\u6211\u4e2d\u5fc3\u5230\u7edd\u5bf9\u65b9\u4f4d\u63a8\u7406\u95ee\u9898\u3002\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4e09\u6b65\u63a8\u7406\u8fc7\u7a0b\uff081.\u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb\uff1b2.\u5c06\u5750\u6807\u6620\u5c04\u5230\u7edd\u5bf9\u65b9\u5411\uff1b3.\u63a8\u65ad\u7528\u6237\u65b9\u4f4d\uff09\u6574\u5408ASR\u8f6c\u5f55\u8bed\u97f3\u4e0e\u5730\u6807\u5750\u6807\u3002\u5728\u53f0\u6e7e\u5927\u6a21\u578bTaiwan-LLM-13B-v2.0-Chat\u4e0a\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "MCoT\u5728\u5e72\u51c0\u8f6c\u5f55\u6587\u672c\u4e0a\u8fbe\u5230100%\u7684\u65b9\u4f4d\u51c6\u786e\u7387\uff0c\u5728ASR\u8f6c\u5f55\u6587\u672c\u4e0a\u8fbe\u523098.1%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u975e\u7ed3\u6784\u5316\u57fa\u7ebf\u3002\u8be5\u6a21\u578b\u5728\u5608\u6742\u5bf9\u8bdd\u6761\u4ef6\u4e0b\uff08\u5305\u62ecASR\u8bc6\u522b\u9519\u8bef\u548c\u591a\u8bed\u8a00\u4ee3\u7801\u5207\u6362\uff09\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8de8\u9886\u57df\u8bc4\u4f30\u4e2d\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u5bf9\u8bed\u8a00\u53d8\u5f02\u3001\u9886\u57df\u6f02\u79fb\u548c\u6307\u4ee3\u6b67\u4e49\u4e5f\u5177\u6709\u97e7\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u7a7a\u95f4\u63a8\u7406\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u8def\u5f84\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u5177\u8eab\u5bfc\u822a\u3002"}}
{"id": "2509.19224", "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u5728EHR\u836f\u7269\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e34\u5e8a\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u836f\u7269\u68c0\u6d4b\u4e0a\u66f4\u4f18\uff0c\u800c\u901a\u7528\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u5728\u6355\u6349\u8bed\u8a00\u4e0a\u4e0b\u6587\u8868\u793a\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u5df2\u6210\u4e3a\u533b\u5b66\u8bed\u8a00NLP\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u5bf9\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "method": "\u9009\u53d6Bert Base\u3001BioBert\u3001\u4e24\u79cdBio+Clinical Bert\u3001RoBerta\u548cClinical Longformer\u7b49\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5728\u54c8\u4f5b\u533b\u5b66\u96622022\u5e74n2c2\u6311\u6218\u8d5b\uff08Track 1\uff09\u7684CMED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u6a21\u578b\u7528\u4e8e\u6267\u884c\u836f\u7269\u63d0\u53d6\u3001\u533b\u7597\u4e8b\u4ef6\u68c0\u6d4b\u548c\u591a\u7ef4\u5ea6\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\u3002\u6027\u80fd\u8bc4\u4f30\u91c7\u7528\u53ec\u56de\u7387\u3001\u7cbe\u786e\u5ea6\u548cF1-\u5206\u6570\u7b49\u6307\u6807\u3002", "result": "\u9884\u8bad\u7ec3\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u836f\u7269\u548c\u836f\u7269\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u7136\u800c\uff0c\u9884\u8bad\u7ec3\u5728\u901a\u7528\u9886\u57df\u6570\u636e\u4e0a\u7684Bert Base\u6a21\u578b\u5728\u5206\u7c7b\u836f\u7269\u76f8\u5173\u4e8b\u4ef6\u7684\u4e0a\u4e0b\u6587\u65b9\u9762\u6700\u4e3a\u6709\u6548\u3002", "conclusion": "\u5728EHR\u836f\u7269\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\uff08\u5982\u836f\u7269\u68c0\u6d4b\u4e0e\u4e0a\u4e0b\u6587\u5206\u7c7b\uff09\uff0c\u57fa\u4e8e\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u57fa\u4e8e\u901a\u7528\u9886\u57df\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5404\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.18639", "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUiG\uff08Understanding-in-Generation\uff09\u7684\u65b0\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7edf\u4e00\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u4ee5\u56fe\u50cf\u7f16\u8f91\u4f5c\u4e3a\u6865\u6881\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684Chain-of-Thought (CoT) \u63a8\u7406\u65b9\u6cd5\u5c06\u7406\u89e3\u548c\u751f\u6210\u8fc7\u7a0b\u5206\u79bb\uff0c\u9650\u5236\u4e86\u5176\u6307\u5bfc\u7edf\u4e00\u6a21\u578b\u63a8\u7406\u4ee5\u5f25\u8865\u751f\u6210\u80fd\u529b\u4e0d\u8db3\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUiG\uff08Understanding-in-Generation\uff09\u7684\u63a8\u7406\u6846\u67b6\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5927\u7684\u7406\u89e3\u80fd\u529b\u6765\u6574\u5408\u751f\u6210\u6307\u5bfc\u3002\u5177\u4f53\u5730\uff0c\u5f15\u5165\u201c\u56fe\u50cf\u7f16\u8f91\u201d\u4f5c\u4e3a\u6865\u6881\uff0c\u9996\u5148\u9a8c\u8bc1\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5e76\u5c06\u7edf\u4e00\u6a21\u578b\u7684\u7406\u89e3\u878d\u5165\u7f16\u8f91\u6307\u4ee4\u4e2d\uff0c\u968f\u540e\u9010\u6b65\u589e\u5f3a\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5c06\u7406\u89e3\u9010\u6b65\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u3002", "result": "UiG\u6846\u67b6\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728TIIF\u57fa\u51c6\u6d4b\u8bd5\u7684\u957f\u63d0\u793a\u8bbe\u7f6e\u4e0a\u83b7\u5f97\u4e863.92%\u7684\u589e\u76ca\u3002", "conclusion": "UiG\u6846\u67b6\u901a\u8fc7\u5c06\u7edf\u4e00\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u5206\u79bb\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2509.18208", "pdf": "https://arxiv.org/pdf/2509.18208", "abs": "https://arxiv.org/abs/2509.18208", "authors": ["Boyuan Zhang", "Yingjun Du", "Xiantong Zhen", "Ling Shao"], "title": "Variational Task Vector Composition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u95e8\u63a7\u91c7\u6837\uff0c\u9009\u62e9\u6027\u5730\u6574\u5408\u4efb\u52a1\u5411\u91cf\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u77e5\u8bc6\u96c6\u6210\u6027\u80fd\u3002", "motivation": "\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u5b58\u5728\u7ed3\u6784\u5197\u4f59\uff0c\u5bfc\u81f4\u9ad8\u65b9\u5dee\u548c\u91c7\u6837\u4f4e\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u4efb\u52a1\u7ea7\u522b\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u6837\u672c\u7279\u5f02\u6027\uff0c\u9650\u5236\u4e86\u591a\u4efb\u52a1\u77e5\u8bc6\u6574\u5408\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\uff0c\u5c06\u7ec4\u5408\u7cfb\u6570\u89c6\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\u8fdb\u884c\u4f30\u8ba1\u3002\u5f15\u5165Spike-and-Slab\u5148\u9a8c\u4ee5\u4fc3\u8fdb\u7a00\u758f\u6027\u5e76\u4fdd\u7559\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5206\u91cf\u3002\u5f00\u53d1\u95e8\u63a7\u91c7\u6837\u673a\u5236\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u548c\u91cd\u8981\u6027\u8fc7\u6ee4\u7ec4\u5408\u7cfb\u6570\uff0c\u6784\u5efa\u53ef\u63a7\u540e\u9a8c\uff0c\u5b9e\u73b0\u6837\u672c\u7ea7\u3001\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5229\u7528\u4efb\u52a1\u5411\u91cf\u4e2d\u6700\u53ef\u9760\u548c\u4fe1\u606f\u91cf\u5927\u7684\u5206\u91cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u77e5\u8bc6\u96c6\u6210\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u9ad8\u6548\u3001\u6709\u6548\u7684\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6574\u5408\u591a\u4efb\u52a1\u77e5\u8bc6\u7684\u80fd\u529b\u3002"}}
{"id": "2509.19228", "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LLM\u957f\u4e0a\u4e0b\u6587\u8ba1\u7b97\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faCompLLM\uff0c\u4e00\u79cd\u5206\u6bb5\u72ec\u7acb\u538b\u7f29\u4e0a\u4e0b\u6587\u7684\u8f6f\u538b\u7f29\u6280\u672f\u3002\u5b83\u5b9e\u73b0\u4e86\u7ebf\u6027\u6548\u7387\u3001\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u663e\u8457\u52a0\u901fTTFT\u3001\u51cf\u5c11KV\u7f13\u5b58\uff0c\u5e76\u4fdd\u6301\u6216\u8d85\u8d8a\u672a\u538b\u7f29\u4e0a\u4e0b\u6587\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\uff0c\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9762\u4e34\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002\u73b0\u6709\u8f6f\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u5b9e\u7528\u6027\u53d7\u9650\uff0c\u56e0\u5176\u6574\u4f53\u538b\u7f29\u5bfc\u81f4\u4e8c\u6b21\u590d\u6742\u5ea6\u4e14\u65e0\u6cd5\u91cd\u7528\u8ba1\u7b97\u3002", "method": "\u672c\u6587\u63d0\u51faCompLLM\uff0c\u4e00\u79cd\u9488\u5bf9\u5b9e\u9645\u90e8\u7f72\u8bbe\u8ba1\u7684\u8f6f\u538b\u7f29\u6280\u672f\u3002\u5b83\u5c06\u957f\u4e0a\u4e0b\u6587\u5206\u5272\u6210\u591a\u4e2a\u6bb5\uff0c\u5e76\u72ec\u7acb\u538b\u7f29\u6bcf\u4e2a\u6bb5\uff0c\u800c\u975e\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u8fdb\u884c\u5904\u7406\u3002", "result": "CompLLM\u5177\u6709\u4e09\u5927\u7279\u6027\uff1a\u538b\u7f29\u6b65\u9aa4\u4e0e\u4e0a\u4e0b\u6587\u957f\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\uff08\u6548\u7387\uff09\uff1b\u80fd\u5c06\u77ed\u5e8f\u5217\u8bad\u7ec3\u7684\u6a21\u578b\u6cdb\u5316\u81f3100k tokens\u957f\u4e0a\u4e0b\u6587\uff08\u53ef\u6269\u5c55\u6027\uff09\uff1b\u5141\u8bb8\u538b\u7f29\u6bb5\u7f13\u5b58\u548c\u91cd\u7528\uff08\u53ef\u91cd\u7528\u6027\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57282\u500d\u538b\u7f29\u7387\u4e0b\uff0c\u9ad8\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\uff0c\u9996\u6b21\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u52a0\u901f\u9ad8\u8fbe4\u500d\uff0cKV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c1150%\uff0c\u540c\u65f6\u6027\u80fd\u4e0e\u672a\u538b\u7f29\u4e0a\u4e0b\u6587\u76f8\u5f53\uff0c\u751a\u81f3\u5728\u6781\u957f\u5e8f\u5217\u4e0a\u8d85\u8d8a\u3002", "conclusion": "CompLLM\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u8f6f\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6bb5\u72ec\u7acb\u538b\u7f29\u89e3\u51b3\u4e86LLM\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u3001\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18642", "pdf": "https://arxiv.org/pdf/2509.18642", "abs": "https://arxiv.org/abs/2509.18642", "authors": ["Nicolas Toussaint", "Emanuele Colleoni", "Ricardo Sanchez-Matilla", "Joshua Sutcliffe", "Vanessa Thompson", "Muhammad Asad", "Imanol Luengo", "Danail Stoyanov"], "title": "Zero-shot Monocular Metric Depth for Endoscopic Images", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025 DEMI Workshop", "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7f3a\u4e4f\u57fa\u51c6\u548c\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6EndoSynth\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5355\u76ee\u76f8\u5bf9\u548c\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u5728\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662f\u57fa\u4e8eTransformer\u7684\u7f51\u7edc\uff09\u7684\u63a8\u52a8\u4e0b\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u5185\u7aa5\u955c\u56fe\u50cf\u5e94\u7528\u9886\u57df\u4ecd\u7f3a\u4e4f\u9c81\u68d2\u7684\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "1) \u5bf9\u6700\u5148\u8fdb\u7684\uff08\u5ea6\u91cf\u548c\u76f8\u5bf9\uff09\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u771f\u5b9e\u7684\u3001\u672a\u89c1\u8fc7\u7684\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u30022) \u5f15\u5165\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6EndoSynth\uff0c\u5176\u4e2d\u5305\u542b\u5185\u7aa5\u955c\u624b\u672f\u5668\u68b0\u3001\u771f\u5b9e\u7684\u5ea6\u91cf\u6df1\u5ea6\u548c\u5206\u5272\u63a9\u7801\u30023) \u4f7f\u7528EndoSynth\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u7684\u5173\u952e\u89c1\u89e3\u3002\u4f7f\u7528EndoSynth\u6570\u636e\u96c6\u5fae\u8c03\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5927\u591a\u6570\u672a\u89c1\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u5185\u7aa5\u955c\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2509.18353", "pdf": "https://arxiv.org/pdf/2509.18353", "abs": "https://arxiv.org/abs/2509.18353", "authors": ["Jakub Adamczyk", "Jakub Poziemski", "Franciszek Job", "Mateusz Kr\u00f3l", "Maciej Makowski"], "title": "MolPILE - large-scale, diverse dataset for molecular representation learning", "categories": ["cs.LG"], "comment": null, "summary": "The size, diversity, and quality of pretraining datasets critically determine\nthe generalization ability of foundation models. Despite their growing\nimportance in chemoinformatics, the effectiveness of molecular representation\nlearning has been hindered by limitations in existing small molecule datasets.\nTo address this gap, we present MolPILE, large-scale, diverse, and rigorously\ncurated collection of 222 million compounds, constructed from 6 large-scale\ndatabases using an automated curation pipeline. We present a comprehensive\nanalysis of current pretraining datasets, highlighting considerable\nshortcomings for training ML models, and demonstrate how retraining existing\nmodels on MolPILE yields improvements in generalization performance. This work\nprovides a standardized resource for model training, addressing the pressing\nneed for an ImageNet-like dataset in molecular chemistry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MolPILE\uff0c\u4e00\u4e2a\u5305\u542b2.22\u4ebf\u5316\u5408\u7269\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7cbe\u5fc3\u6574\u7406\u7684\u5206\u5b50\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u5e76\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5c0f\u5206\u5b50\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u6709\u9650\uff0c\u963b\u788d\u4e86\u5206\u5b50\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u6027\u4ee5\u53ca\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5bfc\u81f4\u5316\u5b66\u4fe1\u606f\u5b66\u9886\u57df\u7f3a\u4e4f\u7c7b\u4f3cImageNet\u7684\u6807\u51c6\u5316\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6574\u7406\u6d41\u7a0b\uff0c\u4ece6\u4e2a\u5927\u578b\u6570\u636e\u5e93\u4e2d\u6784\u5efa\u4e86MolPILE\uff0c\u4e00\u4e2a\u5305\u542b2.22\u4ebf\u5316\u5408\u7269\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u4e25\u683c\u7b5b\u9009\u7684\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u5bf9\u73b0\u6709\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728MolPILE\u4e0a\u91cd\u65b0\u8bad\u7ec3\u73b0\u6709\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u5176\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "MolPILE\u4e3a\u5206\u5b50\u5316\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6025\u9700\u7684\u3001\u7c7b\u4f3cImageNet\u7684\u6807\u51c6\u5316\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\uff0c\u6709\u6548\u5f25\u8865\u4e86\u5f53\u524d\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.19249", "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "\u63d0\u51faRLPT\uff08\u57fa\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u76f4\u63a5\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3LLM\u8bad\u7ec3\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8ba1\u7b97\u8d44\u6e90\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f46\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u589e\u957f\u6709\u9650\uff0c\u5bfc\u81f4\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6269\u5c55\u65b9\u6cd5\u53d7\u5230\u9650\u5236\u3002", "method": "\u5f15\u5165RLPT\uff08Reinforcement Learning on Pre-Training data\uff09\uff0c\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65f6\u6269\u5c55\u8303\u5f0f\u3002RLPT\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7f\u7b56\u7565\u81ea\u4e3b\u63a2\u7d22\u9884\u8bad\u7ec3\u6570\u636e\u3002\u4e0e\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u7684RLHF\u548cRLVR\u4e0d\u540c\uff0cRLPT\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u83b7\u53d6\u5956\u52b1\u4fe1\u53f7\uff0c\u5177\u4f53\u91c7\u7528\u201c\u4e0b\u4e00\u7247\u6bb5\u63a8\u7406\u76ee\u6807\u201d\uff0c\u5956\u52b1\u6a21\u578b\u51c6\u786e\u9884\u6d4b\u540e\u7eed\u6587\u672c\u7247\u6bb5\uff0c\u4ece\u800c\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u6269\u5c55RL\uff0c\u57f9\u517b\u66f4\u901a\u7528\u7684\u63a8\u7406\u6280\u80fd\u3002", "result": "RLPT\u5728\u901a\u7528\u9886\u57df\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6709\u6548\u3002\u4f8b\u5982\uff0c\u5e94\u7528\u4e8eQwen3-4B-Base\u6a21\u578b\u65f6\uff0c\u5728MMLU\u3001MMLU-Pro\u3001GPQA-Diamond\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u7edd\u5bf9\u63d0\u5347\uff08\u5982MMLU +3.0\uff0cGPQA-Diamond +8.1\uff09\u3002\u7814\u7a76\u7ed3\u679c\u8fd8\u663e\u793a\u51fa\u826f\u597d\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u5e76\u80fd\u589e\u5f3aRLVR\u6027\u80fd\u3002", "conclusion": "RLPT\u4e3a\u6269\u5c55LLM\u7684\u63a8\u7406\u8fb9\u754c\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c55\u73b0\u4e86\u5728\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6301\u7eed\u63d0\u5347\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u80fd\u589e\u5f3aRLVR\u6027\u80fd\u3002"}}
{"id": "2509.18683", "pdf": "https://arxiv.org/pdf/2509.18683", "abs": "https://arxiv.org/abs/2509.18683", "authors": ["Lanhu Wu", "Zilin Gao", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.", "AI": {"tldr": "\u63d0\u51faLEAF-Mamba\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u8c03\u5c40\u90e8\u8bed\u4e49\u548c\u81ea\u9002\u5e94\u878d\u5408\uff0c\u5229\u7528Mamba\u89e3\u51b3RGB-D\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "motivation": "\u73b0\u6709RGB-D\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08CNNs\u6216Vision Transformers\uff09\u5728\u5c40\u90e8\u611f\u53d7\u91ce\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\u3002\u76f4\u63a5\u5e94\u7528SSM\u6a21\u578b\uff08Mamba\uff09\u5219\u4f1a\u7f3a\u4e4f\u5c40\u90e8\u8bed\u4e49\u548c\u4e0d\u8db3\u7684\u8de8\u6a21\u6001\u878d\u5408\u3002", "method": "\u63d0\u51faLocal Emphatic and Adaptive Fusion State Space Model (LEAF-Mamba)\uff0c\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7ec4\u4ef6\uff1a1) \u5c40\u90e8\u5f3a\u8c03\u72b6\u6001\u7a7a\u95f4\u6a21\u5757 (LE-SSM) \u7528\u4e8e\u6355\u83b7\u591a\u5c3a\u5ea6\u5c40\u90e8\u4f9d\u8d56\uff1b2) \u57fa\u4e8eSSM\u7684\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757 (AFM) \u7528\u4e8e\u4e92\u8865\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6574\u5408\u3002", "result": "LEAF-Mamba\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e16\u79cd\u6700\u5148\u8fdb\u7684RGB-D SOD\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728RGB-T SOD\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LEAF-Mamba\u901a\u8fc7\u521b\u65b0\u7684\u5c40\u90e8\u8bed\u4e49\u5f3a\u8c03\u548c\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86RGB-D SOD\u9886\u57df\u4e2d\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6743\u8861\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6548\u679c\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18362", "pdf": "https://arxiv.org/pdf/2509.18362", "abs": "https://arxiv.org/abs/2509.18362", "authors": ["Yuxuan Cai", "Xiaozhuan Liang", "Xinghua Wang", "Jin Ma", "Haijin Liang", "Jinwen Luo", "Xinyu Zuo", "Lisheng Duan", "Yuyang Yin", "Xi Chen"], "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference.", "AI": {"tldr": "FastMTP\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u8bad\u7ec3\u4f7f\u5176\u4e0e\u63a8\u7406\u6a21\u5f0f\u5bf9\u9f50\uff0c\u663e\u8457\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\uff0c\u5b9e\u73b0\u65e0\u635f\u8f93\u51fa\u8d28\u91cf\u4e0b\u7684\u5e73\u57472.03\u500d\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u751f\u6210LLM\u7684\u987a\u5e8f\u6027\u5bfc\u81f4\u541e\u5410\u91cf\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u867d\u6709\u8bad\u7ec3\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5728\u63a8\u7406\u52a0\u901f\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u672c\u6587\u63d0\u51faFastMTP\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u63d0\u9ad8\u591a\u6b65\u8349\u7a3f\u8d28\u91cf\u548c\u63a8\u6d4b\u89e3\u7801\u6027\u80fd\uff1a1) \u4f7f\u7528\u4f4d\u7f6e\u5171\u4eab\u6743\u91cd\u7684MTP\u5934\u5728\u81ea\u84b8\u998f\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6355\u83b7\u8fde\u7eed\u672a\u6765\u4ee4\u724c\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b2) \u5c06\u8bed\u8a00\u611f\u77e5\u52a8\u6001\u8bcd\u6c47\u538b\u7f29\u96c6\u6210\u5230MTP\u5934\u4e2d\uff0c\u4ee5\u51cf\u5c11\u8349\u7a3f\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u4e03\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFastMTP\u4e0e\u6807\u51c6\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u5e73\u57472.03\u500d\u7684\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\uff1b\u6bd4\u666e\u901aMTP\u6027\u80fd\u63d0\u534782%\u3002", "conclusion": "FastMTP\u4ec5\u9700\u8f7b\u91cf\u7ea7\u8bad\u7ec3\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u4e3a\u52a0\u901fLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u5feb\u901f\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19269", "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5b66\u4e60\u6982\u5ff5\u7a7a\u95f4\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u5d4c\u5165\u539f\u578b\u63cf\u8ff0\u6765\u7f16\u7801\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03LLM\u5c06\u539f\u578b\u5d4c\u5165\u4e0e\u6982\u5ff5\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u9f50\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6b64\u65b9\u6cd5\u9ad8\u6548\u3002", "motivation": "\u6982\u5ff5\u7a7a\u95f4\u5728\u8ba4\u77e5\u79d1\u5b66\u548c\u53ef\u89e3\u91caAI\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u96be\u4ee5\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5728\u4eceLLMs\u4e2d\u63d0\u53d6\u65f6\uff0c\u867d\u7136LLMs\u6355\u83b7\u4e86\u611f\u77e5\u7279\u5f81\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u63d0\u53d6\u6982\u5ff5\u7a7a\u95f4\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7b56\u7565\uff0c\u901a\u8fc7\u5d4c\u5165\u76f8\u5e94\u539f\u578b\uff08\u5982\u201c\u975e\u5e38\u751c\u7684\u98df\u7269\u201d\uff09\u7684\u63cf\u8ff0\u6765\u7f16\u7801\u7279\u5f81\uff08\u5982\u751c\u5ea6\uff09\u3002\u4e3a\u6539\u8fdb\u6b64\u7b56\u7565\uff0c\u7814\u7a76\u8005\u5fae\u8c03\u4e86LLM\uff0c\u4f7f\u539f\u578b\u5d4c\u5165\u4e0e\u5bf9\u5e94\u7684\u6982\u5ff5\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u9f50\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u8be5\u65b9\u6cd5\u9ad8\u5ea6\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u4eceLLMs\u4e2d\u6709\u6548\u63d0\u53d6\u6982\u5ff5\u7a7a\u95f4\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18692", "pdf": "https://arxiv.org/pdf/2509.18692", "abs": "https://arxiv.org/abs/2509.18692", "authors": ["Xinle Gao", "Linghui Ye", "Zhiyong Xiao"], "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u98df\u54c1\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86Vision Transformer\u6a21\u578b\u7684\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u98df\u54c1\u5de5\u4e1a\u5bf9\u751f\u4ea7\u8d28\u91cf\u548c\u6548\u7387\u8981\u6c42\u63d0\u9ad8\uff0c\u98df\u54c1\u56fe\u50cf\u5206\u7c7b\u5728\u81ea\u52a8\u5316\u8d28\u68c0\u3001\u98df\u54c1\u5b89\u5168\u76d1\u7ba1\u548c\u667a\u80fd\u519c\u4e1a\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709Vision Transformer\u6a21\u578b\u56e0\u53c2\u6570\u91cf\u5927\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u98df\u54c1\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\uff0c\u878d\u5408\u4e86\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08WMHAM\uff09\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\u3002WMHAM\u901a\u8fc7\u9ad8\u6548\u7a97\u53e3\u5212\u5206\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0cSAM\u5219\u81ea\u9002\u5e94\u5f3a\u8c03\u5173\u952e\u7a7a\u95f4\u533a\u57df\u4ee5\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728Food-101\u548cVireo Food-172\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5206\u522b\u8fbe\u5230\u4e8695.24%\u548c94.33%\u7684\u51c6\u786e\u7387\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53c2\u6570\u91cf\u548cFLOPs\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2509.18367", "pdf": "https://arxiv.org/pdf/2509.18367", "abs": "https://arxiv.org/abs/2509.18367", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faM-DSL\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165non-i.i.d.\u5ea6\u91cf\u6307\u6807\u548c\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u8702\u7fa4\u5b66\u4e60(DSL)\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03(non-i.i.d.)\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u8702\u7fa4\u5b66\u4e60(DSL)\u5728\u8fb9\u7f18\u7269\u8054\u7f51\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4f46\u975e\u72ec\u7acb\u540c\u5206\u5e03(non-i.i.d.)\u6570\u636e\u4f1a\u4e25\u91cd\u964d\u4f4e\u5b66\u4e60\u6027\u80fd\u5e76\u5bfc\u81f4\u8bad\u7ec3\u884c\u4e3a\u53d1\u6563\u3002\u6b64\u5916\uff0c\u76ee\u524d\u7f3a\u4e4f\u6570\u636e\u5f02\u6784\u6027\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u6027\u7684\u7406\u8bba\u6307\u5bfc\u3002", "method": ["\u7814\u7a76\u4e86DSL\u6846\u67b6\u4e0b\u975ei.i.d.\u6570\u636e\u96c6\u7684\u6570\u636e\u5f02\u6784\u6027\u5f71\u54cd\u3002", "\u63d0\u51faM-DSL\u7b97\u6cd5\uff0c\u4e00\u79cd\u65b0\u7684\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u8bbe\u8ba1\uff0c\u4ee5\u6709\u6548\u5904\u7406\u5206\u5e03\u5f0f\u5f02\u6784\u6570\u636e\u3002", "\u5f15\u5165\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7684non-i.i.d.\u5ea6\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u5c40\u90e8\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u5dee\u5f02\uff0c\u5e76\u5c06\u6570\u636e\u5f02\u6784\u6027\u4e0eDSL\u6027\u80fd\u8bc4\u4f30\u8054\u7cfb\u8d77\u6765\u3002", "M-DSL\u7b97\u6cd5\u5229\u7528\u8be5\u6307\u6807\u6307\u5bfc\u9009\u62e9\u5bf9\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u8d21\u732e\u663e\u8457\u7684\u5de5\u4f5c\u8005\u3002", "\u63d0\u4f9b\u4e86M-DSL\u6536\u655b\u884c\u4e3a\u7684\u7406\u8bba\u5206\u6790\u3002", "\u5728\u4e0d\u540c\u5f02\u6784\u6570\u636e\u96c6\u548cnon-i.i.d.\u6570\u636e\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002"], "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86M-DSL\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u7f51\u7edc\u667a\u80fd\u65b9\u9762\u7684\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "M-DSL\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u8702\u7fa4\u5b66\u4e60\u4e2d\u975ei.i.d.\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5de5\u4f5c\u8005\u9009\u62e9\u673a\u5236\u548c\u5f02\u6784\u5ea6\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u5f02\u6784\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u6027\u80fd\u548c\u7f51\u7edc\u667a\u80fd\u3002"}}
{"id": "2509.19270", "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Bo\u017e\u00edk", "Marek \u0160uppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65af\u6d1b\u4f10\u514b\u8bed\u4f4e\u8d44\u6e90ASR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u672c\u6587\u63a8\u51fa\u4e86SloPalSpeech\u6570\u636e\u96c6\uff082806\u5c0f\u65f6\u8bae\u4f1a\u8bed\u97f3\uff09\uff0c\u5e76\u7528\u5176\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86WER\uff0c\u4e14\u5df2\u516c\u5f00\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "motivation": "\u65af\u6d1b\u4f10\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u53d7\u963b\u3002", "method": "\u5f15\u5165SloPalSpeech\u5927\u578b\u65af\u6d1b\u4f10\u514b\u8bedASR\u6570\u636e\u96c6\uff082806\u5c0f\u65f6\u8bae\u4f1a\u8bed\u97f3\uff09\uff0c\u5f00\u53d1\u9c81\u68d2\u7684\u5904\u7406\u6d41\u7a0b\u5c06\u957f\u5f55\u97f3\u5bf9\u9f50\u5e76\u5206\u6bb5\u4e3a\u5e72\u51c0\u7684\u97f3\u9891-\u6587\u672c\u5bf9\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u591a\u4e2aOpenAI Whisper\u6a21\u578b\uff08small, medium, large-v3, large-v3-turbo\uff09\u3002", "result": "\u5728Common Voice\u548cFLEURS\u7b49\u6807\u51c6\u65af\u6d1b\u4f10\u514b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5fae\u8c03\u540e\u7684Whisper\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3002\u4f8b\u5982\uff0c\u5fae\u8c03\u540e\u7684Whisper-small\u6a21\u578bWER\u964d\u4f4e\u9ad8\u8fbe70%\uff0c\u63a5\u8fd1\u66f4\u5927\u7684Whisper-large-v3\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u4e3a\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7684\u672a\u6765\u7814\u7a76\uff0c\u4f5c\u8005\u516c\u5f00\u4e86\u5b8c\u6574\u7684SloPalSpeech\u6570\u636e\u96c6\u3001\u5206\u6bb5\u8f6c\u5f55\u6587\u672c\u548c\u6240\u6709\u5fae\u8c03\u6a21\u578b\u3002"}}
{"id": "2509.18693", "pdf": "https://arxiv.org/pdf/2509.18693", "abs": "https://arxiv.org/abs/2509.18693", "authors": ["Siyi Chen", "Kai Wang", "Weicong Pang", "Ruiming Yang", "Ziru Chen", "Renjun Gao", "Alexis Kai Hon Lau", "Dasa Gu", "Chenchen Zhang", "Cheng Li"], "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "Project is available at\n  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md", "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.", "AI": {"tldr": "\u63d0\u51faOSDA\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408SAM\u548cMLLM\uff0c\u5b9e\u73b0\u65e0\u9700\u6807\u6ce8\u7684\u5f00\u653e\u96c6\u5730\u7269\u8986\u76d6\u53d1\u73b0\u3001\u5206\u5272\u548c\u63cf\u8ff0\u3002", "motivation": "\u9065\u611f\u5f00\u653e\u96c6\u5730\u7269\u8986\u76d6\u5206\u6790\u9700\u8981\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5b9a\u4f4d\u548c\u5f00\u653e\u8bed\u4e49\u5206\u7c7b\u80fd\u529b\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u5206\u5272\u65b0\u5bf9\u8c61\u5e76\u8d4b\u4e88\u53ef\u89e3\u91ca\u8bed\u4e49\u6807\u7b7e\u3002", "method": "OSDA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1. \u4f7f\u7528\u53ef\u63d0\u793a\u5fae\u8c03\u5206\u5272\u6a21\u578b(SAM)\u8fdb\u884c\u7cbe\u786e\u53d1\u73b0\u548c\u63a9\u819c\u63d0\u53d6\uff1b2. \u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u8fdb\u884c\u8bed\u4e49\u5f52\u56e0\u548c\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b3. \u91c7\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5e76\u7ed3\u5408\u4eba\u5de5\u8bc4\u5206\u8bc4\u4f30MLLM\u3002\u8be5\u6846\u67b6\u4e0e\u67b6\u6784\u65e0\u5173\u4e14\u65e0\u9700\u6807\u7b7e\u3002", "result": "OSDA\u7ed3\u5408\u4e86\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u9065\u611f\u89e3\u91ca\u7684\u5173\u952e\u6311\u6218\u3002\u5b83\u652f\u6301\u5728\u591a\u6837\u5316\u536b\u661f\u56fe\u50cf\u4e0a\u8fdb\u884c\u9c81\u68d2\u8bc4\u4f30\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u52a8\u6001\u5730\u7269\u8986\u76d6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u81ea\u52a8\u5316\u5236\u56fe\u66f4\u65b0\u548c\u5927\u89c4\u6a21\u5730\u7403\u89c2\u6d4b\u5206\u6790\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18376", "pdf": "https://arxiv.org/pdf/2509.18376", "abs": "https://arxiv.org/abs/2509.18376", "authors": ["Burouj Armgaan", "Eshan Jain", "Harsh Pandey", "Mahesh Chandran", "Sayan Ranu"], "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability", "categories": ["cs.LG", "cs.SI"], "comment": "31 pages, 20 figures, NeurIPS 2025 (Oral)", "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.", "AI": {"tldr": "GnnXemplar\u662f\u4e00\u79cd\u65b0\u7684\u5168\u5c40GNN\u89e3\u91ca\u5668\uff0c\u5b83\u901a\u8fc7\u8bc6\u522b\u4ee3\u8868\u6027\u8282\u70b9\u5e76\u5229\u7528LLMs\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u578b\u56fe\u89e3\u91ca\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fe0\u5b9e\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8282\u70b9\u5206\u7c7b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u4fe1\u4efb\u548c\u91c7\u7528\u3002\u5c3d\u7ba1\u5c40\u90e8\u89e3\u91ca\u65b9\u6cd5\u5df2\u5b58\u5728\uff0c\u4f46\u8868\u5f81\u6574\u4e2a\u7c7b\u522b\u7684\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u4ecd\u4e0d\u6210\u719f\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u3001\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u4e2d\uff0c\u73b0\u6709\u57fa\u4e8emotif\u53d1\u73b0\u7684\u65b9\u6cd5\u56e0\u5b50\u56fe\u91cd\u590d\u7a00\u5c11\u3001\u9ad8\u7ef4\u5c5e\u6027\u548c\u590d\u6742\u7ed3\u6784-\u5c5e\u6027\u4ea4\u4e92\u800c\u5931\u6548\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GnnXemplar\uff0c\u4e00\u4e2a\u53d7\u8ba4\u77e5\u79d1\u5b66\u4e2d\u8303\u4f8b\u7406\u8bba\u542f\u53d1\u7684\u5168\u5c40\u89e3\u91ca\u5668\u3002\u5b83\u5728GNN\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bc6\u522b\u4ee3\u8868\u6027\u8282\u70b9\uff08\u8303\u4f8b\uff09\uff0c\u5e76\u5229\u7528\u4ece\u8fd9\u4e9b\u8303\u4f8b\u90bb\u57df\u5bfc\u51fa\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u6765\u89e3\u91ca\u9884\u6d4b\u3002\u8303\u4f8b\u9009\u62e9\u88ab\u5efa\u6a21\u4e3a\u57fa\u4e8e\u53cd\u5411k\u8fd1\u90bb\u7684\u8986\u76d6\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u7684\u8d2a\u5a6a\u8fd1\u4f3c\u3002\u4e3a\u4e86\u751f\u6210\u53ef\u89e3\u91ca\u89c4\u5219\uff0cGnnXemplar\u91c7\u7528\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u7cbe\u70bc\u63d0\u793a\u7b56\u7565\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGnnXemplar\u5728\u5fe0\u5b9e\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4e00\u9879\u670960\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "GnnXemplar\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u5168\u5c40GNN\u89e3\u91ca\u6846\u67b6\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u578b\u590d\u6742\u56fe\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u4e86GNN\u7684\u53ef\u4fe1\u8d56\u5ea6\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19271", "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Fr\u00e9d\u00e9ric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u9488\u5bf9\u6c83\u6d1b\u592b\u8bed\uff08\u4e00\u79cd\u4f4e\u8d44\u6e90\u53e3\u8bed\uff09\u7684\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6 (WolBanking77)\uff0c\u4ee5\u89e3\u51b3\u9ad8\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u5b9e\u9a8c\uff0c\u83b7\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u610f\u56fe\u5206\u7c7b\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u9ad8\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u7279\u522b\u662f\u6587\u76f2\u7387\u9ad8\u3001\u53e3\u8bed\u4f7f\u7528\u591a\u4e8e\u4e66\u9762\u8bed\u7684\u5730\u533a\uff09\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002\u4f8b\u5982\uff0c\u6c83\u6d1b\u592b\u8bed\u5728\u585e\u5185\u52a0\u5c14\u88ab\u5e7f\u6cdb\u4f7f\u7528\u4f46\u6587\u76f2\u7387\u9ad8\uff0c\u6025\u9700\u76f8\u5173\u7814\u7a76\u3002", "method": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6c83\u6d1b\u592b\u8bed\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6WolBanking77\uff0c\u5305\u542b9,791\u6761\u94f6\u884c\u9886\u57df\u7684\u6587\u672c\u53e5\u5b50\u548c\u8d85\u8fc74\u5c0f\u65f6\u7684\u53e3\u8bed\u53e5\u5b50\u3002\u5728\u6b64\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6587\u672c\u548c\u8bed\u97f3\u9886\u57df\u7684\u6700\u65b0\uff08SOTA\uff09\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6570\u636e\u5185\u5bb9\u5206\u6790\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728WolBanking77\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002\u62a5\u544a\u4e86NLP\u6a21\u578b\u7684f1-score\u548cASR\u6a21\u578b\u7684\u8bcd\u9519\u8bef\u7387\u6307\u6807\uff0c\u5e76\u8fdb\u884c\u4e86\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u6bd4\u8f83\u3002", "conclusion": "WolBanking77\u6570\u636e\u96c6\u7684\u53d1\u5e03\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u53e3\u8bed\u610f\u56fe\u5206\u7c7b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\u3002\u4f5c\u8005\u8ba1\u5212\u5171\u4eab\u6570\u636e\u96c6\u3001\u8fdb\u884c\u7ef4\u62a4\u66f4\u65b0\u5e76\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u76f8\u5173\u5b66\u672f\u7814\u7a76\u3002"}}
{"id": "2509.18697", "pdf": "https://arxiv.org/pdf/2509.18697", "abs": "https://arxiv.org/abs/2509.18697", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2021: cross-domain plant identification", "categories": ["cs.CV"], "comment": "15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation\n  Forum, September 21 to 24, 2021, Bucharest, Romania", "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.", "AI": {"tldr": "PlantCLEF 2021\u6311\u6218\u8d5b\u65e8\u5728\u8bc4\u4f30\u5982\u4f55\u5229\u7528\u690d\u7269\u6807\u672c\u9986\u6570\u636e\uff0c\u63d0\u5347\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\u4f46\u6570\u636e\u532e\u4e4f\u5730\u533a\uff08\u5982\u572d\u4e9a\u90a3\u5730\u76fe\uff09\u7684\u690d\u7269\u81ea\u52a8\u5316\u8bc6\u522b\u80fd\u529b\uff0c\u91c7\u7528\u8de8\u57df\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u690d\u7269\u8bc6\u522b\u4e3b\u8981\u4f9d\u8d56\u91ce\u5916\u7167\u7247\uff0c\u4f46\u6570\u636e\u96c6\u4e2d\u4e8e\u5c11\u6570\u7269\u79cd\u548c\u5730\u533a\uff08\u5317\u7f8e\u3001\u897f\u6b27\uff09\u3002\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\u4f46\u6570\u636e\u532e\u4e4f\u7684\u70ed\u5e26\u5730\u533a\u6025\u9700\u6539\u8fdb\u8bc6\u522b\u6280\u672f\u3002\u6570\u5b57\u5316\u690d\u7269\u6807\u672c\u9986\u85cf\u54c1\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u5927\u91cf\u8865\u5145\u6570\u636e\u3002", "method": "\u91c7\u7528PlantCLEF 2021\u6311\u6218\u8d5b\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5305\u542b\u7ea61,000\u79cd\u572d\u4e9a\u90a3\u5730\u76fe\u690d\u7269\u7684\u6570\u636e\u96c6\u3002\u4efb\u52a1\u4e3a\u8de8\u57df\u5206\u7c7b\uff0c\u8bad\u7ec3\u96c6\u7ed3\u5408\u4e86\u6570\u5341\u4e07\u4efd\u6807\u672c\u9986\u6807\u672c\u548c\u6570\u5343\u5f20\u91ce\u5916\u7167\u7247\uff0c\u4ee5\u5b66\u4e60\u4e24\u57df\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5305\u542b5\u4e2a\u5f62\u6001/\u529f\u80fd\u6027\u72b6\u53ca\u5143\u6570\u636e\u3002\u6d4b\u8bd5\u96c6\u4ec5\u4e3a\u91ce\u5916\u7167\u7247\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6311\u6218\u8d5b\u7684\u8d44\u6e90\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u53c2\u4e0e\u7814\u7a76\u56e2\u961f\u4f7f\u7528\u7684\u8bc6\u522b\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u4e86\u4e3b\u8981\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u65e8\u5728\u8bc4\u4f30\u5229\u7528\u690d\u7269\u6807\u672c\u9986\u6570\u636e\u6539\u5584\u6570\u636e\u532e\u4e4f\u5730\u533a\u690d\u7269\u8bc6\u522b\u7684\u6f5c\u529b\uff0c\u6587\u7ae0\u5bf9\u6311\u6218\u8d5b\u7684\u6210\u679c\u548c\u7ed3\u679c\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002"}}
{"id": "2509.18386", "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GETAD\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9053\u8def\u7f51\u7edc\u62d3\u6251\u3001\u8bed\u4e49\u548c\u5386\u53f2\u6a21\u5f0f\uff0c\u5229\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cTransformer\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u52a0\u6743\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u5728\u9053\u8def\u53d7\u9650\u73af\u5883\u4e2d\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5c06\u8f68\u8ff9\u89c6\u4e3a\u91c7\u6837\u4f4d\u7f6e\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u79fb\u52a8\u7f51\u7edc\u7684\u7ea6\u675f\u548c\u8fde\u63a5\u4fe1\u606f\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u53d7\u9053\u8def\u7f51\u7edc\u9650\u5236\u7684\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e2d\u7684\u5f02\u5e38\u3002", "method": "GETAD\u6846\u67b6\u6574\u5408\u4e86\u9053\u8def\u7f51\u7edc\u62d3\u6251\u3001\u8def\u6bb5\u8bed\u4e49\u548c\u5386\u53f2\u51fa\u884c\u6a21\u5f0f\u3002\u5b83\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u878d\u5408\u7269\u7406\u5c5e\u6027\u548c\u8f6c\u6362\u884c\u4e3a\u7684\u9053\u8def\u611f\u77e5\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u4f4d\u7f6e\u7f16\u7801\u589e\u5f3a\u3002\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u7528\u4e8e\u5efa\u6a21\u5e8f\u5217\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u81ea\u56de\u5f52\u9884\u6d4b\u548c\u76d1\u7763\u94fe\u63a5\u9884\u6d4b\u7684\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u52a0\u6743\u8d1f\u5bf9\u6570\u4f3c\u7136\uff08CW NLL\uff09\u4f5c\u4e3a\u5f02\u5e38\u8bc4\u5206\u51fd\u6570\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGETAD\u5728\u68c0\u6d4b\u5f02\u5e38\u65b9\u9762\uff08\u5c24\u5176\u662f\u5728\u9053\u8def\u53d7\u9650\u73af\u5883\u4e2d\u68c0\u6d4b\u7ec6\u5fae\u5f02\u5e38\u65f6\uff09\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06\u56fe\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u878d\u5165\u8f68\u8ff9\u5efa\u6a21\u7684\u76ca\u5904\uff0c\u8fd9\u4f7f\u5f97\u5f02\u5e38\u68c0\u6d4b\u66f4\u52a0\u7cbe\u786e\u548c\u60c5\u5883\u611f\u77e5\u3002"}}
{"id": "2509.19274", "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "categories": ["cs.CL", "cs.MM"], "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86DRISHTIKON\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5370\u5ea6\u6587\u5316\u7684\u9996\u6b21\u591a\u6a21\u6001\u591a\u8bed\u8a00\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u6587\u5316\u80cc\u666f\u8f93\u5165\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4f20\u7edf\u65b9\u9762\u3002", "motivation": "\u73b0\u6709AI\u57fa\u51c6\u591a\u4e3a\u901a\u7528\u6216\u5168\u7403\u8303\u56f4\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u6587\u5316\uff08\u5982\u5370\u5ea6\u6587\u5316\uff09\u7684\u6df1\u5165\u7ec6\u81f4\u8bc4\u4f30\u3002\u4e3a\u4e86\u8861\u91cf\u548c\u63d0\u5347\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u5904\u7406\u6587\u5316\u80cc\u666f\u3001\u591a\u6a21\u6001\u8f93\u5165\u65f6\u7684\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u4e14\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\u3002", "method": "\u5f15\u5165\u4e86DRISHTIKON\uff0c\u4e00\u4e2a\u6db5\u76d6\u5370\u5ea615\u79cd\u8bed\u8a00\u3001\u6240\u6709\u90a6\u548c\u8054\u90a6\u5c5e\u5730\u3001\u5305\u542b\u8d85\u8fc764,000\u4e2a\u5bf9\u9f50\u6587\u672c-\u56fe\u50cf\u5bf9\u7684\u591a\u6a21\u6001\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5176\u5185\u5bb9\u6d89\u53ca\u8282\u65e5\u3001\u670d\u9970\u3001\u7f8e\u98df\u3001\u827a\u672f\u5f62\u5f0f\u548c\u5386\u53f2\u9057\u4ea7\u7b49\u3002\u4f5c\u8005\u5229\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5305\u62ec\u5f00\u6e90\u3001\u4e13\u6709\u3001\u63a8\u7406\u4e13\u4e1a\u53ca\u5370\u5ea6\u8bed\u7cfb\u6a21\u578b\uff0c\u8bc4\u4f30\u8bbe\u7f6e\u6db5\u76d6\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u6587\u5316\u80cc\u666f\u7684\u591a\u6a21\u6001\u8f93\u5165\u65f6\uff0c\u5176\u63a8\u7406\u80fd\u529b\u5b58\u5728\u5173\u952e\u5c40\u9650\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u8bb0\u8f7d\u8f83\u5c11\u7684\u4f20\u7edf\uff0c\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u5c24\u4e3a\u4e0d\u8db3\u3002", "conclusion": "DRISHTIKON\u586b\u8865\u4e86\u5305\u5bb9\u6027AI\u7814\u7a76\u7684\u4e00\u4e2a\u91cd\u8981\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5f00\u53d1\u66f4\u5177\u6587\u5316\u610f\u8bc6\u548c\u591a\u6a21\u6001\u80fd\u529b\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2509.18699", "pdf": "https://arxiv.org/pdf/2509.18699", "abs": "https://arxiv.org/abs/2509.18699", "authors": ["Zedong Zhang", "Ying Tai", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping", "categories": ["cs.CV"], "comment": null, "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAGSwap\u65b9\u6cd5\u548cCOF\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\u65f6\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u4e0d\u4e00\u81f4\u7ed3\u679c\u4ee5\u53ca\u7f3a\u4e4f\u7efc\u5408\u6027\u57fa\u51c6\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u4ea7\u751f\u6709\u504f\u3001\u89c6\u89c9\u6df7\u4e71\u6216\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u4e14\u7f3a\u5c11\u5168\u9762\u7684\u57fa\u51c6\u6570\u636e\u96c6\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faAGSwap\uff08\u81ea\u9002\u5e94\u7ec4\u4ea4\u6362\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1. \u7ec4\u7ea7\u5d4c\u5165\u4ea4\u6362\uff0c\u901a\u8fc7\u7279\u5f81\u64cd\u4f5c\u878d\u5408\u4e0d\u540c\u6982\u5ff5\u7684\u8bed\u4e49\u5c5e\u6027\uff1b2. \u81ea\u9002\u5e94\u7ec4\u66f4\u65b0\uff0c\u901a\u8fc7\u5e73\u8861\u8bc4\u4f30\u5206\u6570\u6307\u5bfc\u52a8\u6001\u4f18\u5316\u4ee5\u786e\u4fdd\u5408\u6210\u4e00\u81f4\u6027\u3002\u540c\u65f6\u5f15\u5165COF\uff08\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\uff09\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u57fa\u4e8eImageNet-1K\u548cWordNet\u6784\u5efa\u7684\u5927\u89c4\u6a21\u3001\u5206\u5c42\u7ed3\u6784\u6570\u636e\u96c6\uff0c\u5305\u542b95\u4e2a\u8d85\u7c7b\uff0c10\u4e2a\u5b50\u7c7b\uff0c\u5171451,250\u4e2a\u72ec\u7279\u878d\u5408\u5bf9\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAGSwap\u5728\u7b80\u5355\u548c\u590d\u6742\u63d0\u793a\u4e0b\u5747\u4f18\u4e8e\u5305\u62ecGPT-Image-1\u5728\u5185\u7684\u73b0\u6709\u6700\u5148\u8fdb\u7ec4\u5408\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "AGSwap\u53ca\u5176\u914d\u5957\u7684COF\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.18389", "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "categories": ["cs.LG"], "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.", "AI": {"tldr": "\u73b0\u6709RL\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u89e3\u51b3\u65b0\u4efb\u52a1\uff08ICRL\uff09\uff0c\u4f46\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4f7f\u7528\u6807\u51c6RL\u3002\u672c\u6587\u63a2\u8ba8\u6807\u51c6RL\u9884\u8bad\u7ec3\u4e3a\u4f55\u80fd\u4ea7\u751f\u652f\u6301ICRL\u7684\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u5047\u8bbe\uff1aICRL\u53c2\u6570\u662f\u9884\u8bad\u7ec3\u635f\u5931\u7684\u6700\u5c0f\u5316\u5668\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u521d\u6b65\u652f\u6301\u3002", "motivation": "\u73b0\u4ee3RL\u4ee3\u7406\u5728\u9884\u8bad\u7ec3\u540e\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u4ec5\u901a\u8fc7\u4e0a\u4e0b\u6587\u8f93\u5165\u5373\u53ef\u89e3\u51b3\u65b0\u4efb\u52a1\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff08ICRL\uff09\u3002\u7136\u800c\uff0c\u8bb8\u591aICRL\u5de5\u4f5c\u4ecd\u91c7\u7528\u6807\u51c6RL\u7b97\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u65e8\u5728\u56de\u7b54\u6838\u5fc3\u95ee\u9898\uff1a\u4e3a\u4ec0\u4e48RL\u9884\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u652f\u6301ICRL\u7684\u7f51\u7edc\u53c2\u6570\uff1f", "method": "\u672c\u6587\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u4e3a\u63d0\u51fa\u7684\u5047\u8bbe\u63d0\u4f9b\u521d\u6b65\u652f\u6301\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5bf9\u4e00\u4e2a\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u7684Transformer\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u5f53\u4e00\u4e2aTransformer\u4e3a\u7b56\u7565\u8bc4\u4f30\u8fdb\u884c\u9884\u8bad\u7ec3\u65f6\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5176\u4e2d\u4e00\u4e2a\u5168\u5c40\u6700\u5c0f\u5316\u5668\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002\u8fd9\u4e3a\u4f5c\u8005\u7684\u5047\u8bbe\u63d0\u4f9b\u4e86\u521d\u6b65\u652f\u6301\u3002", "conclusion": "\u672c\u6587\u521d\u6b65\u8bc1\u660e\u4e86\u652f\u6301ICRL\u7684\u53c2\u6570\u53ef\u4ee5\u662f\u9884\u8bad\u7ec3\u635f\u5931\u7684\u6700\u5c0f\u5316\u5668\uff0c\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u7b56\u7565\u8bc4\u4f30\u9884\u8bad\u7ec3\u7684Transformer\u53ef\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002"}}
{"id": "2509.18705", "pdf": "https://arxiv.org/pdf/2509.18705", "abs": "https://arxiv.org/abs/2509.18705", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries", "categories": ["cs.CV"], "comment": "13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2019, Lugano, Switzerland", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u533a\u57df\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u96be\u9898\uff0cPlantCLEF 2019\u6311\u6218\u57fa\u4e8e\u572d\u4e9a\u90a3\u548c\u5317\u4e9a\u9a6c\u900a\u5730\u533a\u76841\u4e07\u79cd\u690d\u7269\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u6bd4\u3002\u672c\u8bba\u6587\u62a5\u544a\u4e86\u8be5\u6311\u6218\u7684\u8d44\u6e90\u3001\u65b9\u6cd5\u548c\u4e3b\u8981\u6210\u679c\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u4f7f\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u73b0\u6709\u6570\u636e\u4ec5\u8986\u76d6\u5c11\u6570\u7269\u79cd\u3002\u5168\u7403\u5927\u90e8\u5206\u7269\u79cd\uff08\u5982\u6570\u636e\u7a00\u7f3a\u4f46\u591a\u6837\u6027\u9ad8\u7684\u5730\u533a\uff09\u7f3a\u4e4f\u8db3\u591f\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u8bc6\u522b\u56f0\u96be\u3002", "method": "\u7ec4\u7ec7\u4e86LifeCLEF 2019\u690d\u7269\u8bc6\u522b\u6311\u6218\uff08PlantCLEF 2019\uff09\u3002\u8be5\u6311\u6218\u5229\u7528\u5305\u542b1\u4e07\u4e2a\u7269\u79cd\u7684\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5173\u6ce8\u572d\u4e9a\u90a3\u5730\u76fe\u548c\u5317\u4e9a\u9a6c\u900a\u96e8\u6797\u5730\u533a\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\u5c06\u7cfb\u7edf\u6027\u80fd\u4e0e\u9876\u7ea7\u70ed\u5e26\u690d\u7269\u4e13\u5bb6\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u672c\u8bba\u6587\u5448\u73b0\u4e86\u6311\u6218\u7684\u8d44\u6e90\u548c\u8bc4\u4f30\uff0c\u603b\u7ed3\u4e86\u5404\u53c2\u4e0e\u7814\u7a76\u56e2\u961f\u91c7\u7528\u7684\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u5e76\u5bf9\u4e3b\u8981\u6210\u679c\u8fdb\u884c\u4e86\u5206\u6790\u3002\uff08\u5177\u4f53\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u7ed9\u51fa\uff09", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7\u62a5\u544a\u548c\u5206\u6790PlantCLEF 2019\u6311\u6218\uff0c\u65e8\u5728\u6df1\u5165\u4e86\u89e3\u81ea\u52a8\u5316\u690d\u7269\u8bc6\u522b\u5728\u6570\u636e\u7a00\u7f3a\u9ad8\u591a\u6837\u6027\u533a\u57df\u7684\u73b0\u72b6\u3001\u6311\u6218\u53ca\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8868\u73b0\u3002"}}
{"id": "2509.18396", "pdf": "https://arxiv.org/pdf/2509.18396", "abs": "https://arxiv.org/abs/2509.18396", "authors": ["Do\u011fay Alt\u0131nel"], "title": "Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules", "categories": ["cs.LG"], "comment": "24 pages", "summary": "Deep learning optimizers are optimization algorithms that enable deep neural\nnetworks to learn. The effectiveness of learning is highly dependent on the\noptimizer employed in the training process. Alongside the rapid advancement of\ndeep learning, a wide range of optimizers with different approaches have been\ndeveloped. This study aims to provide a review of various optimizers that have\nbeen proposed and received attention in the literature. From Stochastic\ngradient descent to the most recent ones such as Momentum, AdamW, Sophia, and\nMuon in chronological order, optimizers are examined individually, and their\ndistinctive features are highlighted in the study. The update rule of each\noptimizer is presented in detail, with an explanation of the associated\nconcepts and variables. The techniques applied by these optimizers, their\ncontributions to the optimization process, and their default hyperparameter\nsettings are also discussed. In addition, insights are offered into the open\nchallenges encountered in the optimization of deep learning models. Thus, a\ncomprehensive resource is provided both for understanding the current state of\noptimizers and for identifying potential areas of future development.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5176\u6f14\u53d8\u3001\u5de5\u4f5c\u539f\u7406\u3001\u7279\u70b9\u53ca\u9762\u4e34\u7684\u6311\u6218\uff0c\u65e8\u5728\u63d0\u4f9b\u7406\u89e3\u73b0\u72b6\u548c\u6307\u660e\u672a\u6765\u65b9\u5411\u7684\u8d44\u6e90\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u5bf9\u6a21\u578b\u5b66\u4e60\u6548\u679c\u81f3\u5173\u91cd\u8981\u4e14\u53d1\u5c55\u8fc5\u901f\uff0c\u79cd\u7c7b\u7e41\u591a\uff0c\u6025\u9700\u4e00\u4efd\u7cfb\u7edf\u6027\u7efc\u8ff0\u6765\u68b3\u7406\u5176\u53d1\u5c55\u5386\u7a0b\u3001\u673a\u5236\u53ca\u5f53\u524d\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u65f6\u95f4\u987a\u5e8f\u5ba1\u67e5\u65b9\u6cd5\uff0c\u4eceSGD\u5230\u6700\u65b0\u4f18\u5316\u5668\uff08\u5982Momentum, AdamW, Sophia, Muon\uff09\u9010\u4e00\u5206\u6790\uff0c\u8be6\u7ec6\u9610\u8ff0\u5176\u66f4\u65b0\u89c4\u5219\u3001\u76f8\u5173\u6982\u5ff5\u3001\u6280\u672f\u5e94\u7528\u3001\u8d21\u732e\u53ca\u9ed8\u8ba4\u8d85\u53c2\u6570\u8bbe\u7f6e\uff0c\u5e76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u5f00\u653e\u6311\u6218\u3002", "result": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4efd\u5168\u9762\u7684\u8d44\u6e90\uff0c\u65e2\u6709\u52a9\u4e8e\u7406\u89e3\u4f18\u5316\u5668\u7684\u5f53\u524d\u72b6\u6001\uff0c\u4e5f\u80fd\u8bc6\u522b\u672a\u6765\u6f5c\u5728\u7684\u53d1\u5c55\u9886\u57df\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u7684\u6f14\u8fdb\u4e0e\u73b0\u72b6\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u7efc\u5408\u6027\u53c2\u8003\u3002"}}
{"id": "2509.18711", "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "RSVG-ZeroOV\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\uff0c\u65e0\u9700\u6602\u8d35\u6570\u636e\u6216\u8017\u65f6\u5fae\u8c03\uff0c\u5e76\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\uff08RSVG\uff09\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c01\u95ed\u8bcd\u6c47\u6216\u8fc7\u5ea6\u4f9d\u8d56\u6602\u8d35\u6570\u636e\u548c\u8017\u65f6\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u514d\u8bad\u7ec3\u6846\u67b6RSVG-ZeroOV\u3002\u5b83\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a(i) \u6982\u8ff0\uff1a\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u83b7\u53d6\u6587\u672c\u67e5\u8be2\u4e0e\u89c6\u89c9\u533a\u57df\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u3002(ii) \u805a\u7126\uff1a\u5229\u7528\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u8865\u5145VLM\u53ef\u80fd\u5ffd\u7565\u7684\u5bf9\u8c61\u7ed3\u6784\u548c\u5f62\u72b6\u4fe1\u606f\u3002(iii) \u6f14\u5316\uff1a\u5f15\u5165\u6ce8\u610f\u529b\u6f14\u5316\u6a21\u5757\u4ee5\u6291\u5236\u4e0d\u76f8\u5173\u6fc0\u6d3b\uff0c\u751f\u6210\u7eaf\u5316\u7684\u76ee\u6807\u5206\u5272\u63a9\u7801\u3002", "result": "RSVG-ZeroOV\u65e0\u9700\u7e41\u7410\u7684\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u5f31\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "RSVG-ZeroOV\u6210\u529f\u5730\u5229\u7528\u51bb\u7ed3\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.18408", "pdf": "https://arxiv.org/pdf/2509.18408", "abs": "https://arxiv.org/abs/2509.18408", "authors": ["Sarwan Ali"], "title": "Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations", "categories": ["cs.LG", "q-bio.QM"], "comment": "Accepted to CIKM 2025 as Short paper", "summary": "We present a novel information-preserving Chaos Game Representation (CGR)\nmethod, also called Reverse-CGR (R-CGR), for biological sequence analysis that\naddresses the fundamental limitation of traditional CGR approaches - the loss\nof sequence information during geometric mapping. Our method introduces\ncomplete sequence recovery through explicit path encoding combined with\nrational arithmetic precision control, enabling perfect sequence reconstruction\nfrom stored geometric traces. Unlike purely geometric approaches, our\nreversibility is achieved through comprehensive path storage that maintains\nboth positional and character information at each step. We demonstrate the\neffectiveness of R-CGR on biological sequence classification tasks, achieving\ncompetitive performance compared to traditional sequence-based methods while\nproviding interpretable geometric visualizations. The approach generates\nfeature-rich images suitable for deep learning while maintaining complete\nsequence information through explicit encoding, opening new avenues for\ninterpretable bioinformatics analysis where both accuracy and sequence recovery\nare essential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReverse-CGR (R-CGR)\u7684\u65b0\u578b\u4fe1\u606f\u4fdd\u5b58\u6df7\u6c8c\u535a\u5f08\u8868\u793a\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCGR\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u751f\u7269\u5e8f\u5217\u7684\u5b8c\u5168\u91cd\u5efa\u548c\u53ef\u89e3\u91ca\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u6df7\u6c8c\u535a\u5f08\u8868\u793a\u6cd5\uff08CGR\uff09\u5728\u51e0\u4f55\u6620\u5c04\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5e8f\u5217\u4fe1\u606f\u4e22\u5931\u8fd9\u4e00\u6839\u672c\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u660e\u786e\u7684\u8def\u5f84\u7f16\u7801\u7ed3\u5408\u6709\u7406\u7b97\u672f\u7cbe\u5ea6\u63a7\u5236\uff0c\u5b9e\u73b0\u5b8c\u5168\u7684\u5e8f\u5217\u6062\u590d\u548c\u4ece\u51e0\u4f55\u8f68\u8ff9\u7684\u5b8c\u7f8e\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u5728\u6bcf\u4e00\u6b65\u90fd\u7ef4\u62a4\u4f4d\u7f6e\u548c\u5b57\u7b26\u4fe1\u606f\uff0c\u5b9e\u73b0\u53ef\u9006\u6027\u3002", "result": "\u5728\u751f\u7269\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u4f20\u7edf\u57fa\u4e8e\u5e8f\u5217\u65b9\u6cd5\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u53ef\u89c6\u5316\u3002\u751f\u6210\u4e86\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u4e30\u5bcc\u56fe\u50cf\uff0c\u540c\u65f6\u901a\u8fc7\u660e\u786e\u7f16\u7801\u4fdd\u6301\u4e86\u5b8c\u6574\u7684\u5e8f\u5217\u4fe1\u606f\u3002", "conclusion": "R-CGR\u4e3a\u53ef\u89e3\u91ca\u7684\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5728\u9700\u8981\u51c6\u786e\u6027\u548c\u5e8f\u5217\u6062\u590d\u7684\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.18715", "pdf": "https://arxiv.org/pdf/2509.18715", "abs": "https://arxiv.org/abs/2509.18715", "authors": ["Yingquan Wang", "Pingping Zhang", "Chong Sun", "Dong Wang", "Huchuan Lu"], "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by TCSVT2025", "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a\u5c5e\u6027\u63d0\u793a\u7ec4\u5408\uff08APC\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u76ee\u6807\u91cd\u8bc6\u522b\uff08ReID\uff09\u6a21\u578b\u7684\u5224\u522b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ReID\u6a21\u578b\u5728\u5355\u57df\u6216\u8de8\u57df\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\uff1a\u5355\u57df\u6a21\u578b\u6613\u8fc7\u62df\u5408\u4e8e\u7279\u5b9a\u9886\u57df\u7279\u5f81\uff0c\u800c\u8de8\u57df\u6a21\u578b\u7684\u5f52\u4e00\u5316\u7b56\u7565\u53ef\u80fd\u6291\u5236\u8eab\u4efd\u5224\u522b\u6027\u7ebf\u7d22\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3ReID\u6a21\u578b\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faAttribute Prompt Composition (APC) \u6846\u67b6\u3002\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86Attribute Prompt Generator (APG)\uff0c\u5176\u5305\u542b\u4e00\u4e2a\u63d0\u4f9b\u4e30\u5bcc\u8bed\u4e49\u63cf\u8ff0\u7684Semantic Attribute Dictionary (SAD) \u548c\u4e00\u4e2a\u81ea\u9002\u5e94\u7ec4\u5408\u5c5e\u6027\u751f\u6210\u5224\u522b\u6027\u7279\u5f81\u7684Prompt Composition Module (PCM)\uff0c\u4ee5\u5229\u7528\u6587\u672c\u8bed\u4e49\u3002\u5176\u6b21\uff0c\u501f\u9274\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51faFast-Slow Training Strategy (FSTS)\uff0c\u901a\u8fc7Fast Update Stream (FUS) \u5feb\u901f\u5b66\u4e60ReID\u7279\u6709\u7684\u5224\u522b\u77e5\u8bc6\uff0c\u540c\u65f6\u901a\u8fc7Slow Update Stream (SUS) \u4fdd\u7559\u9884\u8bad\u7ec3VLM\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u901a\u8fc7\u76f8\u4e92\u4f5c\u7528\u5e73\u8861\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\u5e76\u51cf\u8f7b\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4f20\u7edf\u548c\u57df\u6cdb\u5316ReID\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "APC\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5730\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548c\u65b0\u9896\u7684\u5feb\u901f-\u6162\u901f\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86ReID\u9886\u57df\u4e2d\u5224\u522b\u6027\u4e0e\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18433", "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "categories": ["cs.LG"], "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare.", "AI": {"tldr": "\u9488\u5bf9\u533b\u7597AI\u4e2d\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6311\u6218\uff08\u5956\u52b1\u5b9a\u4e49\u96be\u3001IRL\u4e0d\u51c6\u786e\u3001\u7b56\u7565\u5bf9\u9f50\u5dee\uff09\uff0c\u672c\u6587\u63d0\u51faKANDI\uff08Kolmogorov-Arnold Networks and Diffusion Policies for Offline IRL\uff09\uff0c\u7ed3\u5408KAN\u4f30\u8ba1\u4e13\u5bb6\u5956\u52b1\u51fd\u6570\u548c\u6269\u6563\u7b56\u7565\u4f18\u5316\u52a8\u4f5c\uff0c\u4ee5\u4fc3\u8fdb\u9ad8\u8dcc\u5012\u98ce\u9669\u8001\u5e74\u4eba\u8eab\u4f53\u6d3b\u52a8\uff0c\u5e76\u5728\u4e34\u5e8a\u6570\u636e\u548cD4RL\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u7597AI\u4e2d\u53d7\u5173\u6ce8\uff0c\u4f46\u9762\u4e34\u6311\u6218\uff1a\u76f4\u63a5\u5956\u52b1\u96be\u4ee5\u5b9a\u4e49\uff1b\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u96be\u4ee5\u4ece\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e13\u5bb6\u884c\u4e3a\u63a8\u65ad\u51c6\u786e\u5956\u52b1\u51fd\u6570\uff1b\u79bb\u7ebfRL\u7b56\u7565\u96be\u4ee5\u4e0e\u533b\u7597\u5e94\u7528\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u3002\u5177\u4f53\u9700\u6c42\u662f\u901a\u8fc7\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u76d1\u6d4b\u6d3b\u52a8\uff0c\u4fc3\u8fdb\u9ad8\u8dcc\u5012\u98ce\u9669\u8001\u5e74\u4eba\u8eab\u4f53\u6d3b\u52a8\u3002", "method": "\u672c\u6587\u5f15\u5165KANDI\uff08Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning\uff09\u3002KANDI\u5229\u7528Kolmogorov-Arnold Networks\uff08KANs\uff09\u7684\u7075\u6d3b\u51fd\u6570\u903c\u8fd1\u80fd\u529b\uff0c\u901a\u8fc7\u5b66\u4e60\u4f4e\u8dcc\u5012\u98ce\u9669\u8001\u5e74\u4eba\uff08\u4e13\u5bb6\uff09\u7684\u81ea\u7531\u751f\u6d3b\u884c\u4e3a\u6765\u4f30\u8ba1\u5956\u52b1\u51fd\u6570\u3002\u540c\u65f6\uff0c\u5c06\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u96c6\u6210\u5230Actor-Critic\u6846\u67b6\u4e2d\uff0c\u4e3a\u79bb\u7ebfRL\u4e2d\u7684\u52a8\u4f5c\u4f18\u5316\u548c\u6548\u7387\u63d0\u4f9b\u751f\u6210\u5f0f\u65b9\u6cd5\u3002", "result": "KANDI\u5728PEER\u7814\u7a76\uff08\u4e00\u9879\u4e24\u81c2\u4e34\u5e8a\u8bd5\u9a8c\uff09\u7684\u53ef\u7a7f\u6234\u6d3b\u52a8\u76d1\u6d4b\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u8ba1\u5212\u4e2d\u4fc3\u8fdb\u8001\u5e74\u4eba\u8eab\u4f53\u6d3b\u52a8\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6b64\u5916\uff0cKANDI\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "KANDI\u5c55\u73b0\u51fa\u89e3\u51b3\u533b\u7597\u5e94\u7528\u4e2d\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5173\u952e\u6311\u6218\u7684\u6f5c\u529b\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u6d3b\u52a8\u63a8\u5e7f\u5e72\u9884\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18717", "pdf": "https://arxiv.org/pdf/2509.18717", "abs": "https://arxiv.org/abs/2509.18717", "authors": ["Tong Zhang", "Kuofeng Gao", "Jiawang Bai", "Leo Yu Zhang", "Xin Yin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOTCCLIP\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u5339\u914d\u548c\u5bf9\u9f50\u91cd\u5efa\u56fe\u6587\u5bf9\uff0c\u4ee5\u9632\u5fa1CLIP\u6a21\u578b\u514d\u53d7\u6295\u6bd2\u653b\u51fb\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u53d7\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u6e90\u81ea\u4e92\u8054\u7f51\uff0c\u6613\u53d7\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u6295\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5168\u5c40\u8868\u793a\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u53ef\u80fd\u5f15\u5165\u9519\u8bef\u5339\u914d\u5e76\u635f\u5bb3\u9884\u8bad\u7ec3\u3002", "method": "\u63d0\u51faOTCCLIP\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff08Optimal Transport\uff09\u91cd\u5efa\u56fe\u50cf-\u6587\u672c\u5bf9\u3002\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4f18\u4f20\u8f93\u8ddd\u79bb\u5ea6\u91cf\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u96c6\u4e4b\u95f4\u7684\u5339\u914d\uff0c\u5e76\u636e\u6b64\u91cd\u65b0\u5206\u914d\u6587\u672c\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u76ee\u6807\u51fd\u6570\uff0c\u9f13\u52b1\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u4ee5\u51cf\u5c11\u4e0d\u5339\u914d\u5bf9\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOTCCLIP\u80fd\u6709\u6548\u964d\u4f4e\u6295\u6bd2\u653b\u51fb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cOTCCLIP\u663e\u8457\u63d0\u5347\u4e86\u5728\u6295\u6bd2\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u7684\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u7ebf\u6027\u63a2\u6d4b\uff08linear probing\uff09\u6027\u80fd\u3002", "conclusion": "OTCCLIP\u901a\u8fc7\u5229\u7528\u6700\u4f18\u4f20\u8f93\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7279\u5f81\u5339\u914d\u548c\u5bf9\u9f50\uff0c\u6210\u529f\u9632\u5fa1\u4e86\u9488\u5bf9CLIP\u6a21\u578b\u7684\u6295\u6bd2\u653b\u51fb\uff0c\u5e76\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u53d7\u6c61\u67d3\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u7279\u5f81\u5229\u7528\u4e0a\u7684\u5c40\u9650\u3002"}}
{"id": "2509.18445", "pdf": "https://arxiv.org/pdf/2509.18445", "abs": "https://arxiv.org/abs/2509.18445", "authors": ["Kangzheng Liu", "Leixin Ma"], "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems", "categories": ["cs.LG", "physics.app-ph"], "comment": "9 pages, 7 figures", "summary": "The simulation of complex physical systems using a discretized mesh is a\ncornerstone of applied mechanics, but traditional numerical solvers are often\ncomputationally prohibitive for many-query tasks. While Graph Neural Networks\n(GNNs) have emerged as powerful surrogate models for mesh-based data, their\nstandard autoregressive application for long-term prediction is often plagued\nby error accumulation and instability. To address this, we introduce\nMeshODENet, a general framework that synergizes the spatial reasoning of GNNs\nwith the continuous-time modeling of Neural Ordinary Differential Equations. We\ndemonstrate the framework's effectiveness and versatility on a series of\nchallenging structural mechanics problems, including one- and two-dimensional\nelastic bodies undergoing large, non-linear deformations. The results\ndemonstrate that our approach significantly outperforms baseline models in\nlong-term predictive accuracy and stability, while achieving substantial\ncomputational speed-ups over traditional solvers. This work presents a powerful\nand generalizable approach for developing data-driven surrogates to accelerate\nthe analysis and modeling of complex structural systems.", "AI": {"tldr": "MeshODENet\u6846\u67b6\u7ed3\u5408GNN\u4e0e\u795e\u7ecfODE\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u548cGNN\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u8ba1\u7b97\u6602\u8d35\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u7a33\u5b9a\u6027\u548c\u5927\u5e45\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u591a\u67e5\u8be2\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff1b\u73b0\u6709GNN\u4ee3\u7406\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165MeshODENet\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08Neural ODE\uff09\u7684\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u3002\u5728\u5305\u62ec\u4e00\u7ef4\u548c\u4e8c\u7ef4\u5f39\u6027\u4f53\u5927\u975e\u7ebf\u6027\u53d8\u5f62\u5728\u5185\u7684\u4e00\u7cfb\u5217\u7ed3\u6784\u529b\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "MeshODENet\u5728\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u6570\u636e\u9a71\u52a8\u4ee3\u7406\u65b9\u6cd5\uff0c\u53ef\u52a0\u901f\u590d\u6742\u7ed3\u6784\u7cfb\u7edf\u7684\u5206\u6790\u548c\u5efa\u6a21\u3002"}}
{"id": "2509.18733", "pdf": "https://arxiv.org/pdf/2509.18733", "abs": "https://arxiv.org/abs/2509.18733", "authors": ["Yilin Gao", "Kangyi Chen", "Zhongxing Peng", "Hengjie Lu", "Shugong Xu"], "title": "Knowledge Transfer from Interaction Learning", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLFI\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u9ad8\u6548\u5229\u7528VLM\u7684\u4ea4\u4e92\u6a21\u5f0f\u77e5\u8bc6\uff0c\u89e3\u51b3VFM\u5728\u77e5\u8bc6\u8fc1\u79fb\u4e0a\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86VFM\u5728\u591a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fc1\u79fb\u77e5\u8bc6\u65f6\u9762\u4e34\u6839\u672c\u6027\u9650\u5236\u3002VLMs\u64c5\u957f\u5efa\u6a21\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u800cVFMs\u5ffd\u89c6\u5e95\u5c42\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u91c7\u7528\u9762\u5411\u7ed3\u679c\u7684\u8303\u5f0f\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8fc1\u79fb\u53d7\u963b\uff0c\u5e76\u9650\u5236\u4e86\u5728\u591a\u6837\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u5f0f\u6846\u67b6LFI\uff08Learning from Interactions\uff09\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u7406\u89e3\u660e\u786e\u5efa\u6a21\u4e3a\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5f25\u8865\u73b0\u6709VFMs\u7684\u4e0d\u8db3\u3002\u6838\u5fc3\u5728\u4e8e\u6355\u83b7\u9884\u8bad\u7ec3VLM\u4e2d\u7f16\u7801\u7684\u52a8\u6001\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u9879\u6280\u672f\u521b\u65b0\uff1a\u7ef4\u62a4\u8de8\u7f51\u7edc\u5c42\u6301\u4e45\u5173\u7cfb\u7ed3\u6784\u7684\u201c\u4ea4\u4e92\u67e5\u8be2\u201d\uff08Interaction Queries\uff09\uff0c\u4ee5\u53ca\u6e90\u81eaVLM\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7684\u201c\u57fa\u4e8e\u4ea4\u4e92\u7684\u76d1\u7763\u201d\uff08interaction-based supervision\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u6301\u7eed\u6539\u8fdb\uff0c\u5728TinyImageNet\u5206\u7c7b\u4e0a\u5b9e\u73b03.3\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u5728COCO\u68c0\u6d4b/\u5206\u5272\u4e0a\u5b9e\u73b01.6mAP/2.4AP\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u4e14\u53c2\u6570\u5f00\u9500\u5c0f\u3001\u6536\u655b\u5feb\u3002\u5728\u8de8\u57df\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728PACS\u548cVLCS\u4e0a\u5b9e\u73b02.4\u548c9.3\u7684\u96f6\u6837\u672c\u6539\u8fdb\u3002\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9e\u5176\u8ba4\u77e5\u5bf9\u9f50\u6027\uff0c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad82.7\u500d\u3002", "conclusion": "LFI\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u89c6\u89c9\u7406\u89e3\u7684\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86VFMs\u5728\u77e5\u8bc6\u8fc1\u79fb\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u548c\u8de8\u57df\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3001\u6548\u7387\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u8ba4\u77e5\u5bf9\u9f50\u6027\u3002"}}
{"id": "2509.18452", "pdf": "https://arxiv.org/pdf/2509.18452", "abs": "https://arxiv.org/abs/2509.18452", "authors": ["Anton Lebedev", "Won Kyung Lee", "Soumyadip Ghosh", "Olha I. Yaman", "Vassilis Kalantzis", "Yingdong Lu", "Tomasz Nowicki", "Shashanka Ubaru", "Lior Horesh", "Vassil Alexandrov"], "title": "Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "D.2.0; G.4; B.8.2"], "comment": "8 pages, 3 figures, 1 algorithm, 1 table of experiment cases", "summary": "Large, sparse linear systems are pervasive in modern science and engineering,\nand Krylov subspace solvers are an established means of solving them. Yet\nconvergence can be slow for ill-conditioned matrices, so practical deployments\nusually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix\ninversion can generate such preconditioners and accelerate Krylov iterations,\nbut its effectiveness depends on parameters whose optima vary across matrices;\nmanual or grid search is costly. We present an AI-driven framework recommending\nMCMC parameters for a given linear system. A graph neural surrogate predicts\npreconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition\nfunction then chooses the parameter sets most likely to minimise iterations. On\na previously unseen ill-conditioned system, the framework achieves better\npreconditioning with 50\\% of the search budget of conventional methods,\nyielding about a 10\\% reduction in iterations to convergence. These results\nsuggest a route for incorporating MCMC-based preconditioners into large-scale\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2aAI\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u6765\u9ad8\u6548\u63a8\u8350MCMC\u9884\u5904\u7406\u5668\u53c2\u6570\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5927\u578b\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u7684\u6c42\u89e3\u8fed\u4ee3\u6b21\u6570\u548c\u53c2\u6570\u641c\u7d22\u6210\u672c\u3002", "motivation": "Krylov\u5b50\u7a7a\u95f4\u6c42\u89e3\u5668\u5728\u5904\u7406\u75c5\u6001\u5927\u578b\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u65f6\u6536\u655b\u7f13\u6162\uff0c\u9700\u9884\u5904\u7406\u5668\u3002MCMC\u57fa\u77e9\u9635\u6c42\u9006\u53ef\u751f\u6210\u9884\u5904\u7406\u5668\uff0c\u4f46\u5176\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u53c2\u6570\uff0c\u800c\u624b\u52a8\u6216\u7f51\u683c\u641c\u7d22\u8fd9\u4e9b\u53c2\u6570\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u6846\u67b6\u6765\u63a8\u8350\u7ed9\u5b9a\u7ebf\u6027\u7cfb\u7edf\u7684MCMC\u53c2\u6570\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u56fe\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u6839\u636e\u7cfb\u7edf\u77e9\u9635A\u548cMCMC\u53c2\u6570\u9884\u6d4b\u9884\u5904\u7406\u901f\u5ea6\uff1b\u7136\u540e\u5229\u7528\u8d1d\u53f6\u65af\u91c7\u96c6\u51fd\u6570\u9009\u62e9\u6700\u6709\u53ef\u80fd\u6700\u5c0f\u5316\u8fed\u4ee3\u6b21\u6570\u7684\u53c2\u6570\u96c6\u3002", "result": "\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u75c5\u6001\u7cfb\u7edf\u4e0a\uff0c\u8be5\u6846\u67b6\u4ee5\u4f20\u7edf\u65b9\u6cd550%\u7684\u641c\u7d22\u9884\u7b97\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u5904\u7406\u6548\u679c\uff0c\u4f7f\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e86\u7ea610%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u8be5AI\u9a71\u52a8\u6846\u67b6\u4e3a\u5c06MCMC\u57fa\u9884\u5904\u7406\u5668\u6574\u5408\u5230\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u9014\u5f84\uff0c\u63d0\u9ad8\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18738", "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyPSAM\uff0c\u4e00\u4e2a\u6df7\u5408\u63d0\u793a\u9a71\u52a8\u7684SAM\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u8fb9\u754c\u4e0d\u7cbe\u786e\u548c\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u878d\u5408\u7f51\u7edc\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u5229\u7528\u5373\u63d2\u5373\u7528\u7ec6\u5316\u7f51\u7edc\u7ed3\u5408\u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u63d0\u793a\u5f15\u5bfcSAM\u8fdb\u884c\u663e\u8457\u6027\u56fe\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u548c\u51fa\u8272\u7684\u901a\u7528\u6027\u3002", "motivation": "RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u4e2d\uff0c\u7531\u4e8e\u56fa\u6709\u7684\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u548c\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u5b66\u4e60\u7cbe\u786e\u7684\u7269\u4f53\u8fb9\u754c\u548c\u5b8c\u6574\u7684\u663e\u8457\u76ee\u6807\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u63d0\u793a\u9a71\u52a8\u7684Segment Anything Model (HyPSAM)\u3002\u9996\u5148\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u6001\u878d\u5408\u7f51\u7edc\uff08DFNet\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u663e\u8457\u56fe\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u52a8\u6001\u5377\u79ef\u548c\u591a\u5206\u652f\u89e3\u7801\u4fc3\u8fdb\u81ea\u9002\u5e94\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7ec6\u5316\u7f51\u7edc\uff08P2RNet\uff09\u4f5c\u4e3a\u901a\u7528\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u6df7\u5408\u63d0\u793a\uff08\u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u63d0\u793a\uff09\u5f15\u5bfcSAM\u7ec6\u5316\u663e\u8457\u56fe\u3002", "result": "HyPSAM\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u663e\u8457\u7684\u901a\u7528\u6027\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u4e0d\u540c\u7684RGB-T SOD\u65b9\u6cd5\u4e2d\uff0c\u5e76\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HyPSAM\u901a\u8fc7\u5229\u7528SAM\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u521b\u65b0\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86RGB-T SOD\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u7a81\u51fa\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u8be5\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18457", "pdf": "https://arxiv.org/pdf/2509.18457", "abs": "https://arxiv.org/abs/2509.18457", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Velarie Yaa Ansu-Baidoo", "Eric Kim", "Gautham Krishna Gudur", "Mohit Malu", "Owen Krueger", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGluMind\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u7eed\u3001\u957f\u671f\u7684\u8840\u7cd6\u9884\u6d4b\uff0c\u901a\u8fc7\u7279\u6b8a\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u77e5\u8bc6\u4fdd\u7559\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8840\u7cd6\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u6709\u6548\u6574\u5408\u91c7\u6837\u7387\u4e0d\u540c\u7684\u591a\u6a21\u6001\u751f\u7406\u548c\u884c\u4e3a\u6570\u636e\u3001\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u53ca\u5728\u6301\u7eed\u5b66\u4e60\u65b0\u6570\u636e\u65f6\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u548c\u51c6\u786e\u7684\u9884\u6d4b\u3002", "method": "\u672c\u6587\u63d0\u51faGluMind\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6846\u67b6\u3002\u5b83\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5e76\u884c\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1a\u4ea4\u53c9\u6ce8\u610f\u529b\uff08\u6574\u5408\u8840\u7cd6\u4e0e\u751f\u7406\u884c\u4e3a\u4fe1\u53f7\uff0c\u89e3\u51b3\u91c7\u6837\u7387\u5dee\u5f02\uff09\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\uff08\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\uff09\u3002\u6b64\u5916\uff0cGluMind\u8fd8\u6574\u5408\u4e86\u77e5\u8bc6\u4fdd\u7559\u6280\u672f\uff0c\u4ee5\u7f13\u89e3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u5347\u6574\u4f53\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728AIREADI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGluMind\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6SOTA\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u4e0a\u7ea6\u670915%\u7684\u6539\u8fdb\uff0c\u5728\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e0a\u7ea6\u67099%\u7684\u6539\u8fdb\u3002", "conclusion": "GluMind\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6a21\u6001\u6846\u67b6\u3001\u5e76\u884c\u6ce8\u610f\u529b\u673a\u5236\u548c\u77e5\u8bc6\u4fdd\u7559\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u3001\u957f\u671f\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u3001\u957f\u671f\u4f9d\u8d56\u6355\u6349\u53ca\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2509.18743", "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "categories": ["cs.CV"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTriFusion-AE\uff0c\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u5148\u9a8c\u3001\u6df1\u5ea6\u56fe\u548cLiDAR\u70b9\u4e91\u7684\u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5f3a\u566a\u58f0\u548c\u5bf9\u6297\u6027\u653b\u51fb\u4e0bLiDAR\u70b9\u4e91\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u3002", "motivation": "LiDAR\u70b9\u4e91\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u9886\u57df\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u3001\u906e\u6321\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5f71\u54cd\u3002\u73b0\u6709\u81ea\u52a8\u7f16\u7801\u5668\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff0c\u5176\u53bb\u566a\u548c\u91cd\u5efa\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51faTriFusion-AE\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u5148\u9a8c\u3001\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u5355\u76ee\u6df1\u5ea6\u56fe\u4ee5\u53caLiDAR\u70b9\u4e91\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9\u9f50\u6587\u672c\u7684\u8bed\u4e49\u7ebf\u7d22\u3001\u56fe\u50cf\u7684\u51e0\u4f55\uff08\u6df1\u5ea6\uff09\u7279\u5f81\u548cLiDAR\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u5b66\u4e60\u5bf9\u968f\u673a\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6270\u52a8\u5177\u6709\u5f39\u6027\u7684\u8868\u793a\u3002\u8be5\u6846\u67b6\u88ab\u8bbe\u8ba1\u4e3a\u6a21\u578b\u65e0\u5173\u7684\uff0c\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8eCNN\u7684\u70b9\u4e91\u81ea\u52a8\u7f16\u7801\u5668\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u5728nuScenes-mini\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8be5\u6a21\u578b\u5728\u8f7b\u5ea6\u6270\u52a8\u4e0b\u589e\u76ca\u6709\u9650\uff0c\u4f46\u5728\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u548c\u91cd\u566a\u58f0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9c81\u68d2\u7684\u91cd\u5efa\uff0c\u800c\u57fa\u4e8eCNN\u7684\u81ea\u52a8\u7f16\u7801\u5668\u5728\u8fd9\u79cd\u6761\u4ef6\u4e0b\u4f1a\u5931\u6548\u3002", "conclusion": "TriFusion-AE\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LiDAR\u70b9\u4e91\u5728\u6781\u7aef\u6311\u6218\u6761\u4ef6\u4e0b\u7684\u91cd\u5efa\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5176\u6a21\u578b\u65e0\u5173\u6027\u4f7f\u5176\u80fd\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8054\u5408\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2509.18469", "pdf": "https://arxiv.org/pdf/2509.18469", "abs": "https://arxiv.org/abs/2509.18469", "authors": ["Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Probabilistic Geometric Principal Component Analysis with application to neural data", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2025. Code is available at GitHub\n  https://github.com/ShanechiLab/PGPCA.git", "summary": "Dimensionality reduction is critical across various domains of science\nincluding neuroscience. Probabilistic Principal Component Analysis (PPCA) is a\nprominent dimensionality reduction method that provides a probabilistic\napproach unlike the deterministic approach of PCA and serves as a connection\nbetween PCA and Factor Analysis (FA). Despite their power, PPCA and its\nextensions are mainly based on linear models and can only describe the data in\na Euclidean coordinate system. However, in many neuroscience applications, data\nmay be distributed around a nonlinear geometry (i.e., manifold) rather than\nlying in the Euclidean space. We develop Probabilistic Geometric Principal\nComponent Analysis (PGPCA) for such datasets as a new dimensionality reduction\nalgorithm that can explicitly incorporate knowledge about a given nonlinear\nmanifold that is first fitted from these data. Further, we show how in addition\nto the Euclidean coordinate system, a geometric coordinate system can be\nderived for the manifold to capture the deviations of data from the manifold\nand noise. We also derive a data-driven EM algorithm for learning the PGPCA\nmodel parameters. As such, PGPCA generalizes PPCA to better describe data\ndistributions by incorporating a nonlinear manifold geometry. In simulations\nand brain data analyses, we show that PGPCA can effectively model the data\ndistribution around various given manifolds and outperforms PPCA for such data.\nMoreover, PGPCA provides the capability to test whether the new geometric\ncoordinate system better describes the data than the Euclidean one. Finally,\nPGPCA can perform dimensionality reduction and learn the data distribution both\naround and on the manifold. These capabilities make PGPCA valuable for\nenhancing the efficacy of dimensionality reduction for analysis of\nhigh-dimensional data that exhibit noise and are distributed around a nonlinear\nmanifold.", "AI": {"tldr": "\u63d0\u51fa\u6982\u7387\u51e0\u4f55\u4e3b\u6210\u5206\u5206\u6790(PGPCA)\u7b97\u6cd5\uff0c\u5c06PPCA\u6269\u5c55\u81f3\u975e\u7ebf\u6027\u6d41\u5f62\u51e0\u4f55\uff0c\u80fd\u6709\u6548\u5904\u7406\u56f4\u7ed5\u975e\u7ebf\u6027\u6d41\u5f62\u5206\u5e03\u7684\u566a\u58f0\u9ad8\u7ef4\u6570\u636e\uff0c\u5e76\u5728\u6a21\u62df\u548c\u8111\u6570\u636e\u5206\u6790\u4e2d\u4f18\u4e8ePPCA\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u7387\u4e3b\u6210\u5206\u5206\u6790(PPCA)\u53ca\u5176\u6269\u5c55\u4e3b\u8981\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b\u548c\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\uff0c\u65e0\u6cd5\u6709\u6548\u63cf\u8ff0\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u4e2d\u5e38\u89c1\u4e8e\u975e\u7ebf\u6027\u6d41\u5f62\u5468\u56f4\u7684\u6570\u636e\u5206\u5e03\u3002", "method": "\u5f00\u53d1\u4e86\u6982\u7387\u51e0\u4f55\u4e3b\u6210\u5206\u5206\u6790(PGPCA)\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u663e\u5f0f\u5730\u5c06\u9884\u5148\u4ece\u6570\u636e\u4e2d\u62df\u5408\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u77e5\u8bc6\u7eb3\u5165\u6a21\u578b\uff0c\u5e76\u4e3a\u6d41\u5f62\u5bfc\u51fa\u4e86\u4e00\u4e2a\u51e0\u4f55\u5750\u6807\u7cfb\u6765\u6355\u6349\u6570\u636e\u504f\u79bb\u6d41\u5f62\u548c\u566a\u58f0\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684EM\u7b97\u6cd5\u6765\u5b66\u4e60PGPCA\u6a21\u578b\u53c2\u6570\u3002", "result": "PGPCA\u80fd\u591f\u6709\u6548\u5730\u5efa\u6a21\u56f4\u7ed5\u5404\u79cd\u6d41\u5f62\u7684\u6570\u636e\u5206\u5e03\uff0c\u5728\u6a21\u62df\u548c\u8111\u6570\u636e\u5206\u6790\u4e2d\u5747\u4f18\u4e8ePPCA\u3002\u5b83\u8fd8\u80fd\u6d4b\u8bd5\u65b0\u7684\u51e0\u4f55\u5750\u6807\u7cfb\u662f\u5426\u6bd4\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\u80fd\u66f4\u597d\u5730\u63cf\u8ff0\u6570\u636e\uff0c\u5e76\u6267\u884c\u964d\u7ef4\u53ca\u5b66\u4e60\u6d41\u5f62\u4e0a\u548c\u6d41\u5f62\u5468\u56f4\u7684\u6570\u636e\u5206\u5e03\u3002", "conclusion": "PGPCA\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u6d41\u5f62\u51e0\u4f55\uff0c\u6cdb\u5316\u4e86PPCA\uff0c\u80fd\u591f\u66f4\u597d\u5730\u63cf\u8ff0\u6570\u636e\u5206\u5e03\u3002\u8fd9\u4e9b\u80fd\u529b\u4f7f\u5176\u5728\u589e\u5f3a\u5206\u6790\u90a3\u4e9b\u5305\u542b\u566a\u58f0\u5e76\u5206\u5e03\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u5468\u56f4\u7684\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u7684\u6548\u7387\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18531", "pdf": "https://arxiv.org/pdf/2509.18531", "abs": "https://arxiv.org/abs/2509.18531", "authors": ["Seungyoun Shin", "Dongha Ahn", "Jiwoo Kim", "Sungwook Jeon"], "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "submitted to ICASSP 2026", "summary": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative\nPolicy Optimization (GRPO). However, in the absence of a verifiable reward for\n\\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)\nlowers error rates yet collapses prosody into monotone, unnatural speech;\nadding speaker-similarity further destabilizes training and degrades CER. We\naddress this with an \\textit{iterative Direct Preference Optimization (DPO)}\nscheme that uses only a few hundred human-labeled preference pairs per round to\ndirectly optimize prosodic naturalness while regularizing to the current model.\nOn \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center\ninteractions capturing task-oriented dialogues, our method attains the highest\nhuman preference (ELO) with competitive CER, outperforming GRPO and strong\ncommercial baselines. These results suggest that when prosody cannot be\nrewarded automatically, \\textit{human preference optimization} offers a\npractical and data-efficient path to natural and robust TTS. The demo page is\navailable at \\href{https://tts.ch.dev}", "AI": {"tldr": "\u9488\u5bf9GRPO\u5728TTS\u4e2d\u5bfc\u81f4\u97f5\u5f8b\u4e0d\u81ea\u7136\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u8fed\u4ee3DPO\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u4eba\u5de5\u504f\u597d\u6570\u636e\u76f4\u63a5\u4f18\u5316\u97f5\u5f8b\uff0c\u5728\u97e9\u8bed\u5ba2\u670d\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u4eba\u4f53\u504f\u597d\u548c\u7ade\u4e89\u6027CER\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u5728TTS\u4e2d\u867d\u7136\u80fd\u964d\u4f4e\u9519\u8bef\u7387\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u97f5\u5f8b\u5956\u52b1\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8bed\u97f3\u5355\u8c03\u3001\u4e0d\u81ea\u7136\uff1b\u540c\u65f6\uff0c\u589e\u52a0\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u4f1a\u8fdb\u4e00\u6b65\u7834\u574f\u8bad\u7ec3\u5e76\u964d\u4f4e\u9519\u8bef\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u65b9\u6848\u3002\u8be5\u65b9\u6848\u6bcf\u8f6e\u4ec5\u4f7f\u7528\u51e0\u767e\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u504f\u597d\u5bf9\uff0c\u76f4\u63a5\u4f18\u5316\u97f5\u5f8b\u7684\u81ea\u7136\u5ea6\uff0c\u5e76\u5bf9\u5f53\u524d\u6a21\u578b\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5728KoCC-TTS\uff08\u4e00\u4e2a\u7cbe\u9009\u7684\u97e9\u8bed\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u6570\u636e\u96c6\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u4eba\u4f53\u504f\u597d\uff08ELO\uff09\u5206\u6570\u548c\u6709\u7ade\u4e89\u529b\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u4f18\u4e8eGRPO\u548c\u5f3a\u5927\u7684\u5546\u4e1a\u57fa\u7ebf\u3002", "conclusion": "\u5f53\u97f5\u5f8b\u65e0\u6cd5\u81ea\u52a8\u5956\u52b1\u65f6\uff0c\u4eba\u5de5\u504f\u597d\u4f18\u5316\u4e3a\u5b9e\u73b0\u81ea\u7136\u3001\u9c81\u68d2\u7684TTS\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u4e14\u6570\u636e\u9ad8\u6548\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18754", "pdf": "https://arxiv.org/pdf/2509.18754", "abs": "https://arxiv.org/abs/2509.18754", "authors": ["Yuyang Liu", "Xinyuan Shi", "Bang Yang", "Peilin Zhou", "Jiahua Dong", "Long Chen", "Ian Reid", "Xiaondan Liang"], "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages", "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCOLT\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u53d8\u5316\u7684\u5de5\u5177\u6d41\u4e2d\u5b66\u4e60\u548c\u4f7f\u7528\u5de5\u5177\u65f6\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002COLT\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u7801\u672c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u52a8\u6001\u5de5\u5177\u9009\u62e9\uff0c\u5e76\u5728\u65b0\u6536\u96c6\u7684VideoToolBench\u6570\u636e\u96c6\u548c\u73b0\u6709\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video LLMs\uff09\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\uff0c\u4f9d\u8d56\u56fa\u5b9a\u7684\u5de5\u5177\u5e93\uff0c\u96be\u4ee5\u9002\u5e94\u5de5\u5177\u6570\u636e\u6301\u7eed\u6f14\u53d8\u548c\u6d41\u5f0f\u66f4\u65b0\u7684\u771f\u5b9e\u73af\u5883\uff0c\u4e14\u9762\u4e34\u5bf9\u8fc7\u53bb\u5b66\u4e60\u5de5\u5177\u7684\u201c\u707e\u96be\u6027\u9057\u5fd8\u201d\u95ee\u9898\u3002", "method": "\u63d0\u51faCOntinuaL Tool usage (COLT) \u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5f00\u6e90\u89c6\u9891LLM\u7684\u6301\u7eed\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002COLT\u6838\u5fc3\u5728\u4e8e\u5f15\u5165\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u7801\u672c\u4f5c\u4e3a\u5de5\u5177\u7279\u6709\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5e76\u57fa\u4e8e\u7528\u6237\u6307\u4ee4\u4e0e\u7801\u672c\u4e2d\u5de5\u5177\u7279\u5f81\u7684\u76f8\u4f3c\u6027\u52a8\u6001\u9009\u62e9\u76f8\u5173\u5de5\u5177\u3002\u6b64\u5916\uff0c\u4e3a\u91ca\u653e\u89c6\u9891LLM\u7684\u5de5\u5177\u4f7f\u7528\u6f5c\u529b\uff0c\u6536\u96c6\u4e86\u4e00\u4e2a\u4ee5\u89c6\u9891\u4e3a\u4e2d\u5fc3\u7684\u5de5\u5177\u4f7f\u7528\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6VideoToolBench\u3002", "result": "\u5728\u73b0\u6709\u89c6\u9891LLM\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e13\u95e8\u7684VideoToolBench\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684COLT\u6846\u67b6\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "COLT\u6210\u529f\u5730\u4f7f\u5f00\u6e90\u89c6\u9891LLMs\u80fd\u591f\u5728\u8fde\u7eed\u7684\u5de5\u5177\u6d41\u4e2d\u81ea\u52a8\u83b7\u53d6\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u5df2\u5b66\u4e60\u5de5\u5177\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u5de5\u5177\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18470", "pdf": "https://arxiv.org/pdf/2509.18470", "abs": "https://arxiv.org/abs/2509.18470", "authors": ["Xiaozhou Tan", "Minghui Zhao", "Mattias Cross", "Anton Ragni"], "title": "Discrete-time diffusion-like models for speech synthesis", "categories": ["cs.LG", "eess.AS"], "comment": null, "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u79bb\u6563\u65f6\u95f4\u6269\u6563\u7c7b\u8fc7\u7a0b\uff0c\u7528\u4e8e\u8bed\u97f3\u751f\u6210\uff0c\u65e8\u5728\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u5176\u5728\u6548\u7387\u548c\u4e00\u81f4\u6027\u65b9\u9762\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u8bed\u97f3\u8d28\u91cf\u3002", "motivation": "\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5728\u8bed\u97f3\u751f\u6210\u4e2d\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u53d7\u9650\u4e8e\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u548c\u8bad\u7ec3/\u63a8\u7406\u6761\u4ef6\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\u548c\u8bad\u7ec3/\u63a8\u7406\u5b8c\u5168\u4e00\u81f4\u6027\u3002", "method": "\u7814\u7a76\u4e86\u6269\u6563\u7c7b\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u53d8\u4f53\uff0c\u5305\u62ec\u5e94\u7528\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3001\u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3001\u6a21\u7cca\u566a\u58f0\u4ee5\u53ca\u6a21\u7cca\u566a\u58f0\u4e0e\u9ad8\u65af\u566a\u58f0\u6df7\u5408\u7684\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u5728\u8bed\u97f3\u4e3b\u89c2\u548c\u5ba2\u89c2\u8d28\u91cf\u4e0a\u4e0e\u5e7f\u6cdb\u6d41\u884c\u7684\u8fde\u7eed\u65f6\u95f4\u5bf9\u5e94\u7269\u76f8\u5f53\u3002\u540c\u65f6\uff0c\u5b83\u4eec\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u4e00\u81f4\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u65b9\u6848\u3002", "conclusion": "\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u662f\u8bed\u97f3\u751f\u6210\u4e2d\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u7684\u4e00\u4e2a\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u5728\u4fdd\u6301\u8bed\u97f3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6548\u7387\u4e0e\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.18570", "pdf": "https://arxiv.org/pdf/2509.18570", "abs": "https://arxiv.org/abs/2509.18570", "authors": ["Yuke Si", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "5 pages; submitted to ICASSP 2026", "summary": "Recent advances in large language models have facilitated the development of\nunified speech language models (SLMs) capable of supporting multiple speech\ntasks within a shared architecture. However, tasks such as automatic speech\nrecognition (ASR) and speech emotion recognition (SER) rely on distinct types\nof information: ASR primarily depends on linguistic content, whereas SER\nrequires the integration of both linguistic and paralinguistic cues. Existing\nmultitask SLMs typically adopt naive parameter sharing or prompt-based\nconditioning without explicitly modeling the differences in information\ncomposition required by each task. Such designs risk task interference and\nperformance degradation, especially under limited data conditions. To address\nthese limitations, we propose HarmoniFuse, a component-selective and\nprompt-adaptive framework for multi-task speech language modeling. HarmoniFuse\nis designed to harmonize heterogeneous task demands by selecting and fusing\ntask-relevant components of speech representations. Specifically, it integrates\na gated speech encoder to extract task-specific acoustic features and a\nprompt-adaptive dynamic fusion module to aggregate transformer layers based on\ntask characteristics. In addition, a batch-interleaved training strategy\nenables leveraging separate ASR and SER datasets without requiring joint\nannotation. Experimental results demonstrate that HarmoniFuse improves both ASR\nand SER performance, offering a scalable and robust solution for multitask\nspeech understanding under realistic data constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHarmoniFuse\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u4ef6\u9009\u62e9\u548c\u63d0\u793a\u81ea\u9002\u5e94\u878d\u5408\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e2d\u5f02\u6784\u4efb\u52a1\uff08\u5982ASR\u548cSER\uff09\u7684\u4fe1\u606f\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u5904\u7406ASR\u548cSER\u7b49\u9700\u8981\u4e0d\u540c\u7c7b\u578b\u4fe1\u606f\u7684\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u663e\u5f0f\u5efa\u6a21\u4fe1\u606f\u6784\u6210\u5dee\u5f02\uff0c\u5bfc\u81f4\u4efb\u52a1\u5e72\u6270\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u3002", "method": "\u672c\u6587\u63d0\u51faHarmoniFuse\uff0c\u4e00\u4e2a\u7ec4\u4ef6\u9009\u62e9\u548c\u63d0\u793a\u81ea\u9002\u5e94\u7684\u591a\u4efb\u52a1\u8bed\u97f3\u8bed\u8a00\u5efa\u6a21\u6846\u67b6\u3002\u5b83\u96c6\u6210\u4e86\u95e8\u63a7\u8bed\u97f3\u7f16\u7801\u5668\u6765\u63d0\u53d6\u4efb\u52a1\u7279\u5b9a\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u4ee5\u53ca\u4e00\u4e2a\u63d0\u793a\u81ea\u9002\u5e94\u52a8\u6001\u878d\u5408\u6a21\u5757\u6765\u6839\u636e\u4efb\u52a1\u7279\u6027\u805a\u5408Transformer\u5c42\u3002\u6b64\u5916\uff0c\u91c7\u7528\u6279\u6b21\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\u4ee5\u5229\u7528\u72ec\u7acb\u7684ASR\u548cSER\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHarmoniFuse\u663e\u8457\u63d0\u9ad8\u4e86ASR\u548cSER\u7684\u6027\u80fd\u3002", "conclusion": "HarmoniFuse\u4e3a\u5728\u5b9e\u9645\u6570\u636e\u7ea6\u675f\u4e0b\u8fdb\u884c\u591a\u4efb\u52a1\u8bed\u97f3\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18759", "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.", "AI": {"tldr": "FixingGS\u63d0\u51fa\u4e00\u79cd\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u6269\u6563\u6a21\u578b\u84b8\u998f\uff0c\u589e\u5f3a\u7a00\u758f\u89c6\u89d2\u4e0b\u76843DGS\u91cd\u5efa\uff0c\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u5e76\u4fee\u590d\u5185\u5bb9\uff0c\u540c\u65f6\u786e\u4fdd\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "motivation": "3DGS\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\u65f6\uff0c\u7531\u4e8e\u89c6\u89c9\u4fe1\u606f\u4e0d\u8db3\uff0c\u6613\u4ea7\u751f\u660e\u663e\u4f2a\u5f71\u3002\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u5148\u9a8c\u7684\u65b9\u6cd5\u867d\u80fd\u53bb\u9664\u4f2a\u5f71\uff0c\u4f46\u96be\u4ee5\u4fdd\u8bc1\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7ed3\u6784\u6a21\u7cca\u548c\u7ec6\u8282\u4e0d\u771f\u5b9e\u3002", "method": "\u672c\u6587\u63d0\u51faFixingGS\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5145\u5206\u5229\u7528\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u6765\u589e\u5f3a\u7a00\u758f\u89c6\u89d23DGS\u91cd\u5efa\u3002\u6838\u5fc3\u662f\u5176\u84b8\u998f\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u6269\u6563\u5148\u9a8c\uff0c\u4ece\u800c\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u548c\u8fdb\u884c\u5185\u5bb9\u4fee\u590d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6e10\u8fdb\u589e\u5f3a\u65b9\u6848\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u7ea6\u675f\u4e0d\u8db3\u533a\u57df\u7684\u91cd\u5efa\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cFixingGS\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u6027\u80fd\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FixingGS\u901a\u8fc7\u514d\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d23DGS\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6027\u80fd\u3002"}}
{"id": "2509.18471", "pdf": "https://arxiv.org/pdf/2509.18471", "abs": "https://arxiv.org/abs/2509.18471", "authors": ["Mariano Tepper", "Ted Willke"], "title": "Individualized non-uniform quantization for vector search", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Embedding vectors are widely used for representing unstructured data and\nsearching through it for semantically similar items. However, the large size of\nthese vectors, due to their high-dimensionality, creates problems for modern\nvector search techniques: retrieving large vectors from memory/storage is\nexpensive and their footprint is costly. In this work, we present NVQ\n(non-uniform vector quantization), a new vector compression technique that is\ncomputationally and spatially efficient in the high-fidelity regime. The core\nin NVQ is to use novel parsimonious and computationally efficient\nnonlinearities for building non-uniform vector quantizers. Critically, these\nquantizers are \\emph{individually} learned for each indexed vector. Our\nexperimental results show that NVQ exhibits improved accuracy compared to the\nstate of the art with a minimal computational cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNVQ\uff08\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\uff09\u6280\u672f\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u975e\u7ebf\u6027\u538b\u7f29\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u5b58\u50a8\u548c\u68c0\u7d22\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u4fdd\u771f\u5ea6\u4e0b\u63d0\u5347\u4e86\u5411\u91cf\u641c\u7d22\u7cbe\u5ea6\u3002", "motivation": "\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u56e0\u5176\u5e9e\u5927\u5c3a\u5bf8\uff0c\u5bfc\u81f4\u73b0\u4ee3\u5411\u91cf\u641c\u7d22\u6280\u672f\u4e2d\u68c0\u7d22\u6210\u672c\u9ad8\u6602\u4e14\u5360\u7528\u5927\u91cf\u5185\u5b58/\u5b58\u50a8\u7a7a\u95f4\u3002", "method": "\u5f15\u5165NVQ\uff08\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\uff09\uff0c\u4e00\u79cd\u65b0\u7684\u5411\u91cf\u538b\u7f29\u6280\u672f\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u4f7f\u7528\u65b0\u9896\u3001\u7cbe\u7b80\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u975e\u7ebf\u6027\u51fd\u6570\u6765\u6784\u5efa\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\u5668\uff0c\u5e76\u4e14\u8fd9\u4e9b\u91cf\u5316\u5668\u662f\u4e3a\u6bcf\u4e2a\u7d22\u5f15\u5411\u91cf\u5355\u72ec\u5b66\u4e60\u7684\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNVQ\u5728\u8ba1\u7b97\u6210\u672c\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u76f8\u6bd4\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "NVQ\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u548c\u7a7a\u95f4\u4e0a\u9ad8\u6548\u7684\u5411\u91cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5411\u91cf\u641c\u7d22\u7684\u6210\u672c\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u641c\u7d22\u51c6\u786e\u6027\u3002"}}
