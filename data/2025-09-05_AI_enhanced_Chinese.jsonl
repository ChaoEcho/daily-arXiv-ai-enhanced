{"id": "2509.03525", "pdf": "https://arxiv.org/pdf/2509.03525", "abs": "https://arxiv.org/abs/2509.03525", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u7b56\u7565\u5728DementiaBank\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u8bed\u97f3\u75f4\u5446\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u9002\u5e94\u7b56\u7565\u5bf9\u68c0\u6d4b\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u7ecf\u8fc7\u9002\u5f53\u8c03\u6574\u7684\u5f00\u6e90\u6a21\u578b\u53ef\u5ab2\u7f8e\u5546\u7528\u7cfb\u7edf\u3002", "motivation": "\u7f8e\u56fd\u8d85\u8fc7\u4e00\u534a\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53ca\u76f8\u5173\u75f4\u5446\u75c7\u60a3\u8005\u672a\u88ab\u8bca\u65ad\uff0c\u800c\u57fa\u4e8e\u8bed\u97f3\u7684\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5e76\u6539\u8fdb\u57fa\u4e8e\u8bed\u97f3\u7684\u75f4\u5446\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86DementiaBank\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u6bd4\u8f83\u4e86\u4e5d\u79cd\u7eaf\u6587\u672c\u6a21\u578b\u548c\u4e09\u79cd\u591a\u6a21\u6001\u97f3\u9891-\u6587\u672c\u6a21\u578b\u3002\u91c7\u7528\u7684\u9002\u5e94\u7b56\u7565\u5305\u62ec\uff1a\u4e0d\u540c\u793a\u8303\u9009\u62e9\u7b56\u7565\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u63a8\u7406\u589e\u5f3a\u63d0\u793a\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4ee5\u53ca\u591a\u6a21\u6001\u6574\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u6dfb\u52a0\u5206\u7c7b\u5934\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7c7b\u8d28\u5fc3\u793a\u8303\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff1b\u63a8\u7406\u80fd\u529b\u63d0\u9ad8\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u8868\u73b0\uff1b\u4ee4\u724c\u7ea7\u5fae\u8c03\u901a\u5e38\u4ea7\u751f\u4e86\u6700\u4f73\u5206\u6570\uff1b\u6dfb\u52a0\u5206\u7c7b\u5934\u663e\u8457\u6539\u5584\u4e86\u8868\u73b0\u4e0d\u4f73\u7684\u6a21\u578b\u3002\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u97f3\u9891-\u6587\u672c\u7cfb\u7edf\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u80fd\u8d85\u8d8a\u8868\u73b0\u6700\u597d\u7684\u7eaf\u6587\u672c\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u793a\u8303\u9009\u62e9\u3001\u63a8\u7406\u8bbe\u8ba1\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u5bf9\u57fa\u4e8e\u8bed\u97f3\u7684\u75f4\u5446\u68c0\u6d4b\u6027\u80fd\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002\u7ecf\u8fc7\u9002\u5f53\u8c03\u6574\u7684\u5f00\u6e90\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8d8a\u5546\u4e1a\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03526", "pdf": "https://arxiv.org/pdf/2509.03526", "abs": "https://arxiv.org/abs/2509.03526", "authors": ["Yansong Liu", "Jiateng Li", "Yuan Liu"], "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "The recent advancements of Large Language Models (LLMs) have spurred\nconsiderable research interest in extending their linguistic capabilities\nbeyond text to other modalities, which leads to emergence of speech-based LLMs\n(SpeechLMs) with capability of processing user request in either speech or\ntextual formats. However, owing to inter-modal discrepancies, these SpeechLMs\nstill exhibit a significant performance gap compared to their text-based LLM\ncounterparts in instruction-following, particularly when confronted with the\ndynamic and variable nature of user speech. To address this challenge, this\npaper introduces a framework termed Reinforced Behavior Alignment (RBA),\ndesigned to bolster the language generation proficiency of SpeechLMs. Instead\nof relying on supervised fine-tuning from human annotations, RBA employs a\nself-synthesis methodology to generate extensive, high-fidelity alignment data\nby a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of\na teacher using a reinforcement learning-based approach. Experimental results\ndemonstrate that this method effectively enhances the instruction-following\ncapabilities of SpeechLMs that outperform conventional distillation baselines.\nCrucially, we demonstrate that RBA can be seamlessly extended to tasks such\nincluding spoken question answering and speech-to-text translation, attaining\nstate-of-the-art performance on open benchmarks with only self-generated data.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u8bed\u97f3\u5927\u6a21\u578b\uff08SpeechLMs\uff09\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u843d\u540e\u4e8e\u6587\u672c\u5927\u6a21\u578b\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u5f3a\u5316\u884c\u4e3a\u5bf9\u9f50\uff08RBA\uff09\u6846\u67b6\u3002RBA\u901a\u8fc7\u5f3a\u5927\u7684\u6559\u5e08LLM\u81ea\u5408\u6210\u6570\u636e\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5c06SpeechLMs\u884c\u4e3a\u4e0e\u6559\u5e08LLM\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86SpeechLMs\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u5728\u5176\u4ed6\u8bed\u97f3\u4efb\u52a1\u4e2d\u4ec5\u51ed\u81ea\u751f\u6210\u6570\u636e\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u8bed\u97f3\u5927\u6a21\u578b\uff08SpeechLMs\uff09\u4e0e\u7eaf\u6587\u672c\u5927\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u52a8\u6001\u591a\u53d8\u7684\u7528\u6237\u8bed\u97f3\u65f6\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u6a21\u6001\u95f4\u5dee\u5f02\u9020\u6210\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5f3a\u5316\u884c\u4e3a\u5bf9\u9f50\uff08RBA\uff09\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3aSpeechLMs\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u3002RBA\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u800c\u662f\u91c7\u7528\u81ea\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5927\u7684\u6559\u5e08LLM\u751f\u6210\u5927\u91cf\u9ad8\u4fdd\u771f\u5bf9\u9f50\u6570\u636e\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4f7fSpeechLMs\u7684\u884c\u4e3a\u4e0e\u6559\u5e08LLM\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRBA\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86SpeechLMs\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u84b8\u998f\u57fa\u7ebf\u3002\u6b64\u5916\uff0cRBA\u53ef\u4ee5\u65e0\u7f1d\u6269\u5c55\u5230\u5305\u62ec\u8bed\u97f3\u95ee\u7b54\u548c\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u5728\u5185\u7684\u4efb\u52a1\uff0c\u4ec5\u4f7f\u7528\u81ea\u751f\u6210\u6570\u636e\u5c31\u5728\u5f00\u653e\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RBA\u6846\u67b6\u901a\u8fc7\u81ea\u5408\u6210\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u63d0\u5347\u4e86SpeechLMs\u5728\u6307\u4ee4\u9075\u5faa\u548c\u591a\u79cd\u8bed\u97f3\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5f25\u5408\u8bed\u97f3\u5927\u6a21\u578b\u4e0e\u6587\u672c\u5927\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.03527", "pdf": "https://arxiv.org/pdf/2509.03527", "abs": "https://arxiv.org/abs/2509.03527", "authors": ["Bohdan M. Pavlyshenko"], "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the paper, we consider multilevel multitask analysis of cryptocurrency\nnews using a fine-tuned Mistral 7B large language model with\nretrieval-augmented generation (RAG).\n  On the first level of analytics, the fine-tuned model generates graph and\ntext summaries with sentiment scores as well as JSON representations of\nsummaries. Higher levels perform hierarchical stacking that consolidates sets\nof graph-based and text-based summaries as well as summaries of summaries into\ncomprehensive reports. The combination of graph and text summaries provides\ncomplementary views of cryptocurrency news. The model is fine-tuned with 4-bit\nquantization using the PEFT/LoRA approach. The representation of cryptocurrency\nnews as knowledge graph can essentially eliminate problems with large language\nmodel hallucinations.\n  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM\nmodels for multilevel cryptocurrency news analysis can conduct informative\nqualitative and quantitative analytics, providing important insights.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u57fa\u4e8eRAG\u5fae\u8c03\u7684Mistral 7B\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u52a0\u5bc6\u8d27\u5e01\u65b0\u95fb\u8fdb\u884c\u591a\u7ea7\u591a\u4efb\u52a1\u5206\u6790\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u6587\u6458\u8981\u548c\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u6709\u6548\u63d0\u4f9b\u5b9a\u6027\u548c\u5b9a\u91cf\u6d1e\u5bdf\u5e76\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5bf9\u52a0\u5bc6\u8d27\u5e01\u65b0\u95fb\u8fdb\u884c\u6709\u6548\u3001\u6df1\u5165\u7684\u591a\u7ea7\u591a\u4efb\u52a1\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u56fe\u6587\u6458\u8981\u548c\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u89e3\u51b3\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u5fae\u8c03\u7684Mistral 7B\u5927\u8bed\u8a00\u6a21\u578b\u3002\u6a21\u578b\u901a\u8fc7PEFT/LoRA\u65b9\u6cd5\u8fdb\u884c4\u4f4d\u91cf\u5316\u5fae\u8c03\u3002\u5206\u6790\u5206\u4e3a\u591a\u4e2a\u5c42\u7ea7\uff1a\u7b2c\u4e00\u7ea7\u751f\u6210\u5e26\u6709\u60c5\u611f\u8bc4\u5206\u7684\u56fe\u8868\u548c\u6587\u672c\u6458\u8981\uff0c\u4ee5\u53caJSON\u8868\u793a\uff1b\u66f4\u9ad8\u5c42\u7ea7\u901a\u8fc7\u5206\u5c42\u5806\u53e0\u6574\u5408\u56fe\u8868\u548c\u6587\u672c\u6458\u8981\uff0c\u5f62\u6210\u7efc\u5408\u62a5\u544a\u3002\u5c06\u52a0\u5bc6\u8d27\u5e01\u65b0\u95fb\u8868\u793a\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4ee5\u6d88\u9664\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5fae\u8c03\u7684Mistral 7B\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u7ea7\u52a0\u5bc6\u8d27\u5e01\u65b0\u95fb\u5206\u6790\uff0c\u80fd\u591f\u8fdb\u884c\u4fe1\u606f\u4e30\u5bcc\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u91cd\u8981\u6d1e\u5bdf\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5fae\u8c03\u7684Mistral 7B\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u591a\u7ea7\u5206\u6790\u548c\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u9760\u7684\u5de5\u5177\uff0c\u80fd\u591f\u4e3a\u52a0\u5bc6\u8d27\u5e01\u65b0\u95fb\u63d0\u4f9b\u6df1\u5165\u7684\u5206\u6790\u548c\u91cd\u8981\u6d1e\u5bdf\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2509.03667", "pdf": "https://arxiv.org/pdf/2509.03667", "abs": "https://arxiv.org/abs/2509.03667", "authors": ["Vivek Vasan", "Alexander Nico-Katz", "Boulat A. Bash", "Daniel C. Kilper", "Marco Ruffini"], "title": "Entanglement Purification With Finite Latency Classical Communication in Quantum Networks", "categories": ["cs.NI"], "comment": null, "summary": "Quantum networks rely on high fidelity entangled pairs distributed to nodes,\nbut maintaining their fidelity is challenged by environmental decoherence\nduring storage. Entanglement purification is used to restore fidelity, but the\nidle periods imposed by the associated classical communication delays\ncounteract this goal by exposing the states to further decoherence. In this\nwork, we analyze the practical viability of entanglement purification protocols\n(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet\nprotocol (IP) communications networks. We present a comprehensive performance\nevaluation of these protocols in various network conditions for a range of\nquantum memory technologies. We employ a microscopic Lindblad treatment of the\nunderlying quantum dynamics, and use current-generation metropolitan IP network\nlatency statistics and parameters drawn from quantum memory testbeds. In doing\nso we identify the regions in which entanglement purification succeeds and\nfails, delineated by break-even iso-fidelity contours in the phase space. We\nthen determine the total number of entangled pairs required to complete a\nmulti-round purification protocol, and the steady-state throughput of entangled\npairs with purified fidelities that exceed application-specific thresholds.\nThis provides latency budgets, memory quality targets, and resource-overhead\nestimates for deploying purification on current and near-future networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5728IP\u7f51\u7edc\u5ef6\u8fdf\u4e0b\u7ea0\u7f20\u7eaf\u5316\u534f\u8bae\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u786e\u5b9a\u4e86\u5176\u6210\u529f\u4e0e\u5931\u8d25\u6761\u4ef6\u53ca\u8d44\u6e90\u9700\u6c42\uff0c\u4ee5\u514b\u670d\u91cf\u5b50\u7f51\u7edc\u4e2d\u7ecf\u5178\u901a\u4fe1\u5ef6\u8fdf\u5bfc\u81f4\u7684\u989d\u5916\u9000\u76f8\u5e72\u95ee\u9898\u3002", "motivation": "\u91cf\u5b50\u7f51\u7edc\u4f9d\u8d56\u9ad8\u4fdd\u771f\u5ea6\u7ea0\u7f20\u5bf9\uff0c\u4f46\u5b58\u50a8\u671f\u95f4\u7684\u9000\u76f8\u5e72\u4f1a\u964d\u4f4e\u4fdd\u771f\u5ea6\u3002\u7ea0\u7f20\u7eaf\u5316\u867d\u80fd\u6062\u590d\u4fdd\u771f\u5ea6\uff0c\u4f46\u7ecf\u5178\u901a\u4fe1\u5ef6\u8fdf\u9020\u6210\u7684\u7a7a\u95f2\u671f\u4f1a\u52a0\u5267\u9000\u76f8\u5e72\uff0c\u62b5\u6d88\u7eaf\u5316\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8bc4\u4f30\u5728\u5b9e\u9645IP\u7f51\u7edc\u5ef6\u8fdf\u4e0b\u7ea0\u7f20\u7eaf\u5316\u534f\u8bae\uff08\u5982BBPSSW, DEJMPS\uff09\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u5fae\u89c2Lindblad\u65b9\u7a0b\u63cf\u8ff0\u5e95\u5c42\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u7ed3\u5408\u5f53\u524d\u57ce\u57dfIP\u7f51\u7edc\u5ef6\u8fdf\u7edf\u8ba1\u6570\u636e\u4ee5\u53ca\u91cf\u5b50\u5b58\u50a8\u5668\u6d4b\u8bd5\u5e73\u53f0\u53c2\u6570\u3002\u5bf9BBPSSW\u548cDEJMPS\u7b49\u7ea0\u7f20\u7eaf\u5316\u534f\u8bae\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u548c\u91cf\u5b50\u5b58\u50a8\u6280\u672f\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u7ea0\u7f20\u7eaf\u5316\u6210\u529f\u4e0e\u5931\u8d25\u7684\u533a\u57df\uff0c\u5e76\u4ee5\u76c8\u4e8f\u5e73\u8861\u7684\u7b49\u4fdd\u771f\u5ea6\u7b49\u503c\u7ebf\u5728\u76f8\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5212\u5206\u3002\u786e\u5b9a\u4e86\u5b8c\u6210\u591a\u8f6e\u7eaf\u5316\u534f\u8bae\u6240\u9700\u7684\u7ea0\u7f20\u5bf9\u603b\u6570\uff0c\u4ee5\u53ca\u8fbe\u5230\u5e94\u7528\u6307\u5b9a\u4fdd\u771f\u5ea6\u9608\u503c\u7684\u7eaf\u5316\u7ea0\u7f20\u5bf9\u7684\u7a33\u6001\u541e\u5410\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u5f53\u524d\u548c\u672a\u6765\u7f51\u7edc\u4e2d\u90e8\u7f72\u7ea0\u7f20\u7eaf\u5316\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u9884\u7b97\u3001\u5b58\u50a8\u5668\u8d28\u91cf\u76ee\u6807\u548c\u8d44\u6e90\u5f00\u9500\u4f30\u7b97\uff0c\u4e3a\u5b9e\u9645\u91cf\u5b50\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.03528", "pdf": "https://arxiv.org/pdf/2509.03528", "abs": "https://arxiv.org/abs/2509.03528", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6cd5\u5f8b\u9886\u57df\u6d41\u7a0b\u6316\u6398\uff08PM\uff09\u7684\u6570\u636e\u9650\u5236\uff0c\u5f15\u5165\u4e86ProLiFIC\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u7684\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5e76\u63d0\u8bae\u5176\u4f5c\u4e3a\u6cd5\u5f8bPM\u7684\u57fa\u51c6\u3002", "motivation": "\u6d41\u7a0b\u6316\u6398\uff08PM\uff09\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u6570\u636e\u96c6\u53ef\u8bbf\u95ee\u6027\u548c\u8d28\u91cf\u7684\u9650\u5236\u3002", "method": "\u4eceNormattiva\u95e8\u6237\u7684\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u5176\u8fdb\u884c\u7ed3\u6784\u5316\u5904\u7406\uff0c\u6784\u5efa\u4e86\u6db5\u76d61987\u5e74\u81f32022\u5e74\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u7684ProLiFIC\u7efc\u5408\u4e8b\u4ef6\u65e5\u5fd7\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86ProLiFIC\uff0c\u4e00\u4e2a\u4ece\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6570\u636e\u4e2d\u7ed3\u6784\u5316\u800c\u6765\u7684\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u7efc\u5408\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5e76\u5c55\u793a\u4e86\u521d\u6b65\u5206\u6790\u793a\u4f8b\u3002", "conclusion": "ProLiFIC\u53ef\u4f5c\u4e3a\u6cd5\u5f8b\u9886\u57df\u6d41\u7a0b\u6316\u6398\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u65b0\u53d1\u5c55\uff0c\u5e76\u9a8c\u8bc1\u4e86PM\u4e0eLLM\u96c6\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.03762", "pdf": "https://arxiv.org/pdf/2509.03762", "abs": "https://arxiv.org/abs/2509.03762", "authors": ["Sathwik Chadaga", "Eytan Modiano"], "title": "Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "We consider the problem of joint routing and scheduling in queueing networks,\nwhere the edge transmission costs are unknown. At each time-slot, the network\ncontroller receives noisy observations of transmission costs only for those\nedges it selects for transmission. The network controller's objective is to\nmake routing and scheduling decisions so that the total expected cost is\nminimized. This problem exhibits an exploration-exploitation trade-off,\nhowever, previous bandit-style solutions cannot be directly applied to this\nproblem due to the queueing dynamics. In order to ensure network stability, the\nnetwork controller needs to optimize throughput and cost simultaneously. We\nshow that the best achievable cost is lower bounded by the solution to a static\noptimization problem, and develop a network control policy using techniques\nfrom Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show\nthat the policy achieves a sub-linear regret of order $O(\\sqrt{T}\\log T)$, as\ncompared to the best policy that has complete knowledge of arrivals and costs.\nFinally, we evaluate the proposed policy using simulations and show that its\nregret is indeed sub-linear.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fb9\u4f20\u8f93\u6210\u672c\u672a\u77e5\u6392\u961f\u7f51\u7edc\u4e2d\u8054\u5408\u8def\u7531\u4e0e\u8c03\u5ea6\u95ee\u9898\u7684\u7f51\u7edc\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408Lyapunov\u6f02\u79fb\u52a0\u60e9\u7f5a\u4f18\u5316\u548c\u591a\u81c2\u8001\u864e\u673a\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be$O(\\sqrt{T}\\log T)$\u3002", "motivation": "\u5728\u6392\u961f\u7f51\u7edc\u4e2d\uff0c\u8054\u5408\u8def\u7531\u548c\u8c03\u5ea6\u65f6\u8fb9\u4f20\u8f93\u6210\u672c\u672a\u77e5\uff0c\u63a7\u5236\u5668\u4ec5\u80fd\u83b7\u53d6\u6240\u9009\u8fb9\u7684\u566a\u58f0\u89c2\u6d4b\u3002\u76ee\u6807\u662f\u6700\u5c0f\u5316\u603b\u9884\u671f\u6210\u672c\uff0c\u540c\u65f6\u9700\u786e\u4fdd\u7f51\u7edc\u7a33\u5b9a\u6027\uff08\u4f18\u5316\u541e\u5410\u91cf\u548c\u6210\u672c\uff09\u3002\u4f20\u7edf\u591a\u81c2\u8001\u864e\u673a\u65b9\u6848\u56e0\u961f\u5217\u52a8\u6001\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\uff0c\u5b58\u5728\u63a2\u7d22-\u5229\u7528\u6743\u8861\u6311\u6218\u3002", "method": "\u9996\u5148\u786e\u5b9a\u4e86\u6700\u4f73\u53ef\u5b9e\u73b0\u6210\u672c\u7684\u4e0b\u754c\u3002\u7136\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7f51\u7edc\u63a7\u5236\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u4e86Lyapunov\u6f02\u79fb\u52a0\u60e9\u7f5a\u4f18\u5316\u548c\u591a\u81c2\u8001\u864e\u673a\u6280\u672f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5b9e\u73b0\u4e86$O(\\sqrt{T}\\log T)$\u7684\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u4e0e\u5b8c\u5168\u4e86\u89e3\u5230\u8fbe\u548c\u6210\u672c\u7684\u6700\u4f73\u7b56\u7565\u76f8\u6bd4\u3002\u4eff\u771f\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u9057\u61be\u6027\u80fd\u786e\u5b9e\u662f\u6b21\u7ebf\u6027\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7f51\u7edc\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u672a\u77e5\u6210\u672c\u6392\u961f\u7f51\u7edc\u4e2d\u7684\u8054\u5408\u8def\u7531\u4e0e\u8c03\u5ea6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u4f18\u79c0\u7684\u6b21\u7ebf\u6027\u9057\u61be\u6027\u80fd\u3002"}}
{"id": "2509.03609", "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGFP\u7684\u65b0\u578b\u901a\u7528\u7279\u5f81\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u9ad8\u5c42\u7279\u5f81\u800c\u975e\u4f4e\u5c42\u5750\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u76d1\u7763\u9aa8\u9abc\u884c\u4e3a\u8bc6\u522b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u8303\u5f0f\u5728\u9aa8\u9abc\u884c\u4e3a\u8bc6\u522b\u4e2d\u901a\u5e38\u5c06\u91cd\u5efa\u76ee\u6807\u9650\u5236\u5728\u539f\u59cb\u5173\u8282\u5750\u6807\u6216\u5176\u7b80\u5355\u53d8\u4f53\u4e0a\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u8bed\u4e49\u8868\u793a\u53d7\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u901a\u7528\u7279\u5f81\u9884\u6d4b\uff08GFP\uff09\u6846\u67b6\uff0c\u7528\u9ad8\u5c42\u7279\u5f81\u9884\u6d4b\uff08\u4ece\u5c40\u90e8\u8fd0\u52a8\u6a21\u5f0f\u5230\u5168\u5c40\u8bed\u4e49\u8868\u793a\uff09\u53d6\u4ee3\u4f20\u7edf\u7684\u4f4e\u5c42\u91cd\u5efa\u3002\u8be5\u6846\u67b6\u5f15\u5165\u534f\u4f5c\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u76ee\u6807\u751f\u6210\u7f51\u7edc\u52a8\u6001\u4ea7\u751f\u8de8\u65f6\u7a7a\u5c42\u6b21\u7684\u591a\u6837\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u7ea6\u675f\u4f18\u5316\u4ee5\u786e\u4fdd\u7279\u5f81\u591a\u6837\u6027\u5e76\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGFP\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u8bad\u7ec3\u901f\u5ea6\u63d0\u53476.2\u500d\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u8868\u793a\u8d28\u91cf\uff0c\u5728NTU RGB+D 60\u3001NTU RGB+D 120\u548cPKU-MMD\u7b49\u6570\u636e\u96c6\u7684\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GFP\u6846\u67b6\u901a\u8fc7\u9ad8\u5c42\u7279\u5f81\u9884\u6d4b\u548c\u534f\u4f5c\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u63a9\u7801\u9aa8\u9abc\u5efa\u6a21\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u8868\u793a\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u81ea\u76d1\u7763\u9aa8\u9abc\u884c\u4e3a\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03594", "pdf": "https://arxiv.org/pdf/2509.03594", "abs": "https://arxiv.org/abs/2509.03594", "authors": ["Thomas R. Harvey"], "title": "The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "https://github.com/harveyThomas4692/Induced-Metric-Optimiser", "summary": "We present a class of novel optimisers for training neural networks that\nmakes use of the Riemannian metric naturally induced when the loss landscape is\nembedded in higher-dimensional space. This is the same metric that underlies\ncommon visualisations of loss landscapes. By taking this geometric perspective\nliterally and using the induced metric, we develop a new optimiser and compare\nit to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of\ntasks and architectures. Empirically, we conclude that this new class of\noptimisers is highly effective in low dimensional examples, and provides slight\nimprovement over state-of-the-art methods for training neural networks. These\nnew optimisers have theoretically desirable properties. In particular, the\neffective learning rate is automatically decreased in regions of high curvature\nacting as a smoothed out form of gradient clipping. Similarly, one variant of\nthese optimisers can also be viewed as inducing an effective scheduled learning\nrate and decoupled weight decay is the natural choice from our geometric\nperspective. The basic method can be used to modify any existing\npreconditioning method. The new optimiser has a computational complexity\ncomparable to that of Adam.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u7c7b\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u9ece\u66fc\u5ea6\u91cf\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5668\uff0c\u5728\u4f4e\u7ef4\u793a\u4f8b\u4e2d\u9ad8\u6548\uff0c\u5bf9\u73b0\u6709SOTA\u65b9\u6cd5\u7565\u6709\u6539\u8fdb\uff0c\u5177\u6709\u826f\u597d\u7406\u8bba\u7279\u6027\uff08\u5982\u9ad8\u66f2\u7387\u533a\u57df\u5b66\u4e60\u7387\u81ea\u52a8\u964d\u4f4e\uff09\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0eAdam\u76f8\u5f53\u3002", "motivation": "\u5229\u7528\u635f\u5931\u51fd\u6570\u5728\u66f4\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5d4c\u5165\u65f6\u81ea\u7136\u4ea7\u751f\u7684\u9ece\u66fc\u5ea6\u91cf\uff08\u901a\u5e38\u7528\u4e8e\u53ef\u89c6\u5316\u635f\u5931\u51fd\u6570\u666f\u89c2\uff09\uff0c\u4ee5\u51e0\u4f55\u89c6\u89d2\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5668\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u4f18\u5316\u5668\u5728\u5904\u7406\u635f\u5931\u666f\u89c2\u51e0\u4f55\u7279\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u7c7b\u65b0\u578b\u4f18\u5316\u5668\uff0c\u5176\u6838\u5fc3\u662f\u5229\u7528\u5c06\u635f\u5931\u666f\u89c2\u5d4c\u5165\u9ad8\u7ef4\u7a7a\u95f4\u65f6\u81ea\u7136\u8bf1\u5bfc\u7684\u9ece\u66fc\u5ea6\u91cf\u3002\u4f5c\u8005\u5c06\u8fd9\u79cd\u51e0\u4f55\u89c6\u89d2\u5177\u4f53\u5316\u5e76\u5e94\u7528\u5230\u4f18\u5316\u5668\u8bbe\u8ba1\u4e2d\uff0c\u7136\u540e\u5c06\u5176\u4e0eSGD\u3001Adam\u3001AdamW\u548cMuon\u7b49\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\u3002\u8be5\u57fa\u672c\u65b9\u6cd5\u53ef\u7528\u4e8e\u4fee\u6539\u4efb\u4f55\u73b0\u6709\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0eAdam\u76f8\u5f53\u3002", "result": "\u7ecf\u9a8c\u6027\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u7c7b\u65b0\u578b\u4f18\u5316\u5668\u5728\u4f4e\u7ef4\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u5e76\u4e14\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f7b\u5fae\u7684\u6539\u8fdb\u3002\u7406\u8bba\u4e0a\uff0c\u8fd9\u4e9b\u4f18\u5316\u5668\u5177\u6709 desirable \u7684\u7279\u6027\uff0c\u4f8b\u5982\uff0c\u5728\u9ad8\u66f2\u7387\u533a\u57df\uff0c\u6709\u6548\u5b66\u4e60\u7387\u4f1a\u81ea\u52a8\u964d\u4f4e\uff0c\u8d77\u5230\u4e86\u5e73\u6ed1\u68af\u5ea6\u88c1\u526a\u7684\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u5176\u4e2d\u4e00\u4e2a\u53d8\u4f53\u80fd\u591f\u8bf1\u5bfc\u6709\u6548\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5e76\u4e14\u4ece\u51e0\u4f55\u89c6\u89d2\u6765\u770b\uff0c\u89e3\u8026\u6743\u91cd\u8870\u51cf\u662f\u81ea\u7136\u7684\u9009\u62e9\u3002", "conclusion": "\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u9ece\u66fc\u5ea6\u91cf\u7684\u65b0\u578b\u4f18\u5316\u5668\u662f\u6709\u6548\u7684\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7ef4\u573a\u666f\u4e2d\uff0c\u5e76\u4e14\u80fd\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5e26\u6765\u8f7b\u5fae\u7684\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u4e9b\u4f18\u5316\u5668\u4e0d\u4ec5\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u7279\u6027\uff08\u5982\u5b66\u4e60\u7387\u7684\u81ea\u52a8\u8c03\u6574\uff09\uff0c\u800c\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4e0eAdam\u76f8\u5f53\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u9014\u5f84\uff0c\u5e76\u80fd\u7075\u6d3b\u5730\u4e0e\u73b0\u6709\u9884\u5904\u7406\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2509.03536", "pdf": "https://arxiv.org/pdf/2509.03536", "abs": "https://arxiv.org/abs/2509.03536", "authors": ["Weizhi Chen", "Ziwei Wang", "Leyang Yang", "Sheng Zhou", "Xiaoxuan Tang", "Jiajun Bu", "Yong Li", "Wei Jiang"], "title": "PG-Agent: An Agent Powered by Page Graph", "categories": ["cs.AI", "cs.HC"], "comment": "Paper accepted to ACM MM 2025", "summary": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.", "AI": {"tldr": "\u4e3a\u89e3\u51b3GUI\u667a\u80fd\u4f53\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u672c\u6587\u5c06\u64cd\u4f5c\u5e8f\u5217\u8f6c\u5316\u4e3a\u9875\u9762\u56fe\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u4e0e\u591a\u667a\u80fd\u4f53\u6846\u67b6PG-Agent\uff0c\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u964c\u751f\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u4e3b\u8981\u4f9d\u8d56\u591a\u6b65\u64cd\u4f5c\u7684\u987a\u5e8f\u7247\u6bb5\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u672a\u80fd\u6355\u6349\u9875\u9762\u95f4\u590d\u6742\u7684\u8f6c\u6362\u5173\u7cfb\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u96be\u4ee5\u6df1\u5165\u611f\u77e5GUI\u73af\u5883\u5e76\u6cdb\u5316\u5230\u65b0\u573a\u666f\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5c06\u987a\u5e8f\u64cd\u4f5c\u7247\u6bb5\u8f6c\u6362\u4e3a\u663e\u5f0f\u5efa\u6a21\u9875\u9762\u95f4\u56fe\u7ed3\u6784\u7684\u201c\u9875\u9762\u56fe\u201d\uff08page graphs\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u4ece\u9875\u9762\u56fe\u4e2d\u68c0\u7d22\u53ef\u9760\u7684GUI\u611f\u77e5\u6307\u5357\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6PG-Agent\uff0c\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u7b56\u7565\uff0c\u5e76\u5c06\u8fd9\u4e9b\u611f\u77e5\u6307\u5357\u6ce8\u5165\u5176\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u672a\u77e5\u573a\u666f\u7684\u6cdb\u5316\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPG-Agent\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u7528\u4e8e\u6784\u5efa\u9875\u9762\u56fe\u7684\u7247\u6bb5\u6570\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "PG-Agent\u901a\u8fc7\u521b\u65b0\u7684\u9875\u9762\u56fe\u6784\u5efa\u3001\u7ed3\u5408RAG\u6280\u672f\u4ee5\u53ca\u5b9a\u5236\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709GUI\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u611f\u77e5\u548c\u65b0\u573a\u666f\u6cdb\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.03529", "pdf": "https://arxiv.org/pdf/2509.03529", "abs": "https://arxiv.org/abs/2509.03529", "authors": ["Alejandro \u00c1lvarez Castro", "Joaqu\u00edn Ordieres-Mer\u00e9"], "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Presented at NLMLT2025 (https://airccse.org/csit/V15N16.html), 15\n  pages, 5 figures", "summary": "Earnings calls represent a uniquely rich and semi-structured source of\nfinancial communication, blending scripted managerial commentary with\nunscripted analyst dialogue. Although recent advances in financial sentiment\nanalysis have integrated multi-modal signals, such as textual content and vocal\ntone, most systems rely on flat document-level or sentence-level models,\nfailing to capture the layered discourse structure of these interactions. This\npaper introduces a novel multi-modal framework designed to generate\nsemantically rich and structurally aware embeddings of earnings calls, by\nencoding them as hierarchical discourse trees. Each node, comprising either a\nmonologue or a question-answer pair, is enriched with emotional signals derived\nfrom text, audio, and video, as well as structured metadata including coherence\nscores, topic labels, and answer coverage assessments. A two-stage transformer\narchitecture is proposed: the first encodes multi-modal content and discourse\nmetadata at the node level using contrastive learning, while the second\nsynthesizes a global embedding for the entire conference. Experimental results\nreveal that the resulting embeddings form stable, semantically meaningful\nrepresentations that reflect affective tone, structural logic, and thematic\nalignment. Beyond financial reporting, the proposed system generalizes to other\nhigh-stakes unscripted communicative domains such as tele-medicine, education,\nand political discourse, offering a robust and explainable approach to\nmulti-modal discourse representation. This approach offers practical utility\nfor downstream tasks such as financial forecasting and discourse evaluation,\nwhile also providing a generalizable method applicable to other domains\ninvolving high-stakes communication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u7f16\u7801\u4e3a\u5206\u5c42\u8bdd\u8bed\u6811\uff0c\u751f\u6210\u7ed3\u6784\u611f\u77e5\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u5d4c\u5165\uff0c\u5e76\u5c55\u793a\u5176\u5728\u91d1\u878d\u5206\u6790\u53ca\u5176\u4ed6\u9ad8\u98ce\u9669\u6c9f\u901a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u91d1\u878d\u60c5\u611f\u5206\u6790\u5df2\u6574\u5408\u591a\u6a21\u6001\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u6241\u5e73\u7684\u6587\u6863\u6216\u53e5\u5b50\u7ea7\u6a21\u578b\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u4e2d\u56fa\u6709\u7684\u5206\u5c42\u8bdd\u8bed\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u7f16\u7801\u4e3a\u5206\u5c42\u8bdd\u8bed\u6811\uff0c\u6bcf\u4e2a\u8282\u70b9\uff08\u72ec\u767d\u6216\u95ee\u7b54\u5bf9\uff09\u901a\u8fc7\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u60c5\u611f\u4fe1\u53f7\u53ca\u7ed3\u6784\u5316\u5143\u6570\u636e\uff08\u5982\u8fde\u8d2f\u6027\u3001\u4e3b\u9898\u3001\u56de\u7b54\u8986\u76d6\u7387\uff09\u8fdb\u884c\u4e30\u5bcc\u3002\u91c7\u7528\u4e24\u9636\u6bb5Transformer\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u8282\u70b9\u7ea7\u591a\u6a21\u6001\u5185\u5bb9\u548c\u5143\u6570\u636e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5408\u6210\u6574\u4e2a\u4f1a\u8bae\u7684\u5168\u5c40\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u751f\u6210\u7684\u5d4c\u5165\u5f62\u6210\u4e86\u7a33\u5b9a\u3001\u8bed\u4e49\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u80fd\u6709\u6548\u53cd\u6620\u60c5\u611f\u57fa\u8c03\u3001\u7ed3\u6784\u903b\u8f91\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91d1\u878d\u9884\u6d4b\u548c\u8bdd\u8bed\u8bc4\u4f30\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u5e76\u53ef\u6cdb\u5316\u5230\u8fdc\u7a0b\u533b\u7597\u3001\u6559\u80b2\u548c\u653f\u6cbb\u8bdd\u8bed\u7b49\u5176\u4ed6\u9ad8\u98ce\u9669\u3001\u975e\u811a\u672c\u5316\u6c9f\u901a\u9886\u57df\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u8bdd\u8bed\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2509.03818", "pdf": "https://arxiv.org/pdf/2509.03818", "abs": "https://arxiv.org/abs/2509.03818", "authors": ["Sherwan Jalal Abdullah", "Sravan Reddy Chintareddy", "Victor S. Frost", "Shawn Keshmiri", "Morteza Hashemi"], "title": "A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "In this work, we develop a measurement platform to capture mobile network\nperformance metrics including coverage and quality of service in regions where\nconventional coverage testing approaches are frequently time-intensive,\nlabor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing\nmethods are used to collect cellular network performance metrics. However,\nthese approaches are inadequate in rural areas due to low-density population,\nand difficult terrain. The platform described here is a UAV-based and is\ndesigned to investigate the mobile network performance through aerial\noperations and gather Radio Access Network (RAN) signal alongside end-to-end\nnetwork performance metrics. Our platform gathers metrics through the\nintegration of an onboard computation unit and commercial off-the-shelf\ncellular modem. The gathered data are subsequently analyzed and displayed using\ngeospatial mapping utilities and statistical techniques to deliver key\nobservations on cellular network performance. Experimental results showed that\nthe received signal power improves at higher altitudes due to enhanced\nline-of-sight (LoS) conditions as expected. However, the signal quality\ndegrades as a result of increased interference from neighboring cells. The\nanalysis reveals that for most of the geographic area covered in the initial\nexperiments the system maintained acceptable signal quality, with adequate\nthroughput performance for both uplink and downlink communications, while\nmaintaining satisfactory round-trip time characteristics. Notably, the\nexperiment showed that a strong radio signal metric for a given cell does not\nnecessarily translate to consistent spatial coverage across the tested region.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u6d4b\u91cf\u5e73\u53f0\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u504f\u8fdc\u5730\u533a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5206\u6790\u4e86\u9ad8\u7a7a\u4fe1\u53f7\u7279\u6027\u53ca\u8986\u76d6\u4e0e\u4fe1\u53f7\u5f3a\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u79fb\u52a8\u7f51\u7edc\u8986\u76d6\u6d4b\u8bd5\u65b9\u6cd5\uff08\u5982\u4f17\u5305\uff09\u8017\u65f6\u3001\u8d39\u529b\u3001\u5371\u9669\uff0c\u4e14\u5728\u4eba\u53e3\u5bc6\u5ea6\u4f4e\u3001\u5730\u5f62\u590d\u6742\u7684\u519c\u6751\u5730\u533a\u56e0\u6548\u7387\u4f4e\u4e0b\u800c\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u6d4b\u91cf\u5e73\u53f0\uff0c\u901a\u8fc7\u7a7a\u4e2d\u4f5c\u4e1a\u6536\u96c6\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u4fe1\u53f7\u548c\u7aef\u5230\u7aef\u7f51\u7edc\u6027\u80fd\u6307\u6807\u3002\u5e73\u53f0\u96c6\u6210\u4e86\u677f\u8f7d\u8ba1\u7b97\u5355\u5143\u548c\u5546\u7528\u73b0\u6210\u8702\u7a9d\u8c03\u5236\u89e3\u8c03\u5668\uff0c\u6536\u96c6\u7684\u6570\u636e\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u6620\u5c04\u5de5\u5177\u548c\u7edf\u8ba1\u6280\u672f\u8fdb\u884c\u5206\u6790\u548c\u5c55\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u6d77\u62d4\u5904\u63a5\u6536\u4fe1\u53f7\u529f\u7387\u56e0\u89c6\u8ddd\u6539\u5584\u800c\u589e\u5f3a\uff0c\u4f46\u4fe1\u53f7\u8d28\u91cf\u56e0\u90bb\u8fd1\u5c0f\u533a\u5e72\u6270\u589e\u52a0\u800c\u4e0b\u964d\u3002\u5728\u5927\u591a\u6570\u8986\u76d6\u533a\u57df\uff0c\u7cfb\u7edf\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u4fe1\u53f7\u8d28\u91cf\u3001\u8db3\u591f\u7684\u4e0a\u4e0b\u884c\u541e\u5410\u91cf\u548c\u6ee1\u610f\u7684\u5f80\u8fd4\u65f6\u95f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f3a\u7684\u65e0\u7ebf\u7535\u4fe1\u53f7\u6307\u6807\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u6d4b\u8bd5\u533a\u57df\u5185\u4e00\u81f4\u7684\u7a7a\u95f4\u8986\u76d6\u3002", "conclusion": "\u8be5\u65e0\u4eba\u673a\u5e73\u53f0\u80fd\u6709\u6548\u6d4b\u91cf\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6311\u6218\u6027\u533a\u57df\u3002\u7814\u7a76\u63ed\u793a\u4e86\u9ad8\u7a7a\u4fe1\u53f7\u5f3a\u5ea6\u4e0e\u8d28\u91cf\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f3a\u7684\u65e0\u7ebf\u7535\u4fe1\u53f7\u6307\u6807\u4e0d\u4e00\u5b9a\u80fd\u786e\u4fdd\u4e00\u81f4\u7a7a\u95f4\u8986\u76d6\u7684\u5173\u952e\u53d1\u73b0\u3002"}}
{"id": "2509.03614", "pdf": "https://arxiv.org/pdf/2509.03614", "abs": "https://arxiv.org/abs/2509.03614", "authors": ["Seungho Choe", "Xiaoli Qin", "Abubakr Shafique", "Amanda Dy", "Dimitri Androutsos", "Susan Done", "April Khademi"], "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge", "categories": ["cs.CV"], "comment": "4 pages, 1 figures, final submission for MIDOG 2025 challenge", "summary": "Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.", "AI": {"tldr": "\u75c5\u7406\u5b66\u5bb6\u4eba\u5de5\u8ba1\u6570\u6709\u4e1d\u5206\u88c2\u8017\u65f6\u4e14\u53d8\u5f02\u6027\u5927\u3002AI\u9762\u4e34\u57df\u6f02\u79fb\u548c\u6570\u636e\u4e0d\u5e73\u8861\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eUNet\u7684\u5e08\u751f\u6a21\u578b\uff0c\u7ed3\u5408\u57df\u6cdb\u5316\u6a21\u5757\uff08\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff09\uff0c\u5b9e\u73b0\u4e86\u50cf\u7d20\u7ea7\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff0c\u5e76\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4eba\u5de5\u8ba1\u6570\u6709\u4e1d\u5206\u88c2\u8017\u65f6\u4e14\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u6027\u5927\u3002\u73b0\u6709AI\u5de5\u5177\u5728\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u4e2d\u5b58\u5728\u57df\u6f02\u79fb\u95ee\u9898\uff08\u6e90\u4e8e\u5f62\u6001\u591a\u6837\u6027\u3001\u67d3\u8272\u65b9\u6848\u5dee\u5f02\u7b49\uff09\u548c\u6570\u636e\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5c06\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u8868\u8ff0\u4e3a\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e08\u751f\u6a21\u578b\uff0c\u540c\u65f6\u89e3\u51b3\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\uff08Track 1\uff09\u548c\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff08Track 2\uff09\u3002\u8be5\u65b9\u6cd5\u4ee5UNet\u5206\u5272\u9aa8\u5e72\u7f51\u7edc\u4e3a\u57fa\u7840\uff0c\u96c6\u6210\u4e86\u57df\u6cdb\u5316\u6a21\u5757\uff08\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u548c\u57df\u5bf9\u6297\u8bad\u7ec3\uff09\u3002\u91c7\u7528\u5e08\u751f\u7b56\u7565\u751f\u6210\u50cf\u7d20\u7ea7\u4f2a\u63a9\u819c\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u8fa8\u522b\u80fd\u529b\u548c\u5bf9\u6297\u57df\u6f02\u79fb\u7684\u9c81\u68d2\u6027\u3002\u5206\u7c7b\u4efb\u52a1\u5219\u901a\u8fc7\u4e00\u4e2a\u5229\u7528\u5206\u5272\u6a21\u578b\u7279\u5f81\u56fe\u7684\u591a\u5c3a\u5ea6CNN\u5206\u7c7b\u5668\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u8303\u5f0f\u4e0b\u5b8c\u6210\u3002", "result": "\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u7b97\u6cd5\u5728Track 1\uff08\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\uff09\u4e2dF1\u5206\u6570\u8fbe\u52300.7660\uff0c\u5728Track 2\uff08\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\uff09\u4e2d\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u52300.8414\u3002", "conclusion": "\u5c06\u57fa\u4e8e\u5206\u5272\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\u96c6\u6210\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u9c81\u68d2\u7684\u6709\u4e1d\u5206\u88c2\u5206\u6790\u3002"}}
{"id": "2509.03643", "pdf": "https://arxiv.org/pdf/2509.03643", "abs": "https://arxiv.org/abs/2509.03643", "authors": ["Chao Pang", "Jiheum Park", "Xinzhuo Jiang", "Nishanth Parameshwar Pavinkurve", "Krishna S. Kalluri", "Shalmali Joshi", "No\u00e9mie Elhadad", "Karthik Natarajan"], "title": "CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Electronic Health Records (EHRs) provide a rich, longitudinal view of patient\nhealth and hold significant potential for advancing clinical decision support,\nrisk prediction, and data-driven healthcare research. However, most artificial\nintelligence (AI) models for EHRs are designed for narrow, single-purpose\ntasks, limiting their generalizability and utility in real-world settings.\nHere, we present CEHR-GPT, a general-purpose foundation model for EHR data that\nunifies three essential capabilities - feature representation, zero-shot\nprediction, and synthetic data generation - within a single architecture. To\nsupport temporal reasoning over clinical sequences, \\cehrgpt{} incorporates a\nnovel time-token-based learning framework that explicitly encodes patients'\ndynamic timelines into the model structure. CEHR-GPT demonstrates strong\nperformance across all three tasks and generalizes effectively to external\ndatasets through vocabulary expansion and fine-tuning. Its versatility enables\nrapid model development, cohort discovery, and patient outcome forecasting\nwithout the need for task-specific retraining.", "AI": {"tldr": "CEHR-GPT\u662f\u4e00\u79cd\u901a\u7528\u7684EHR\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u5728\u4e00\u4e2a\u67b6\u6784\u4e2d\u7edf\u4e00\u4e86\u7279\u5f81\u8868\u793a\u3001\u96f6\u6837\u672c\u9884\u6d4b\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u4ee4\u724c\u5b66\u4e60\u6846\u67b6\u652f\u6301\u65f6\u95f4\u63a8\u7406\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5177\u6709\u5de8\u5927\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5927\u591a\u6570AI\u6a21\u578b\u90fd\u5c40\u9650\u4e8e\u72ed\u7a84\u7684\u5355\u7528\u9014\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CEHR-GPT\uff0c\u4e00\u4e2a\u901a\u7528\u7684EHR\u6570\u636e\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u5c06\u7279\u5f81\u8868\u793a\u3001\u96f6\u6837\u672c\u9884\u6d4b\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u4e09\u79cd\u80fd\u529b\u6574\u5408\u5728\u4e00\u4e2a\u67b6\u6784\u4e2d\u3002\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u65b0\u9896\u7684\u65f6\u95f4\u4ee4\u724c\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u660e\u786e\u7f16\u7801\u60a3\u8005\u7684\u52a8\u6001\u65f6\u95f4\u7ebf\u3002", "result": "CEHR-GPT\u5728\u6240\u6709\u4e09\u9879\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u8bcd\u6c47\u6269\u5c55\u548c\u5fae\u8c03\u6709\u6548\u5730\u6cdb\u5316\u5230\u5916\u90e8\u6570\u636e\u96c6\u3002", "conclusion": "CEHR-GPT\u7684\u591a\u529f\u80fd\u6027\u4f7f\u5176\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u6a21\u578b\u5f00\u53d1\u3001\u961f\u5217\u53d1\u73b0\u548c\u60a3\u8005\u7ed3\u5c40\u9884\u6d4b\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.03548", "pdf": "https://arxiv.org/pdf/2509.03548", "abs": "https://arxiv.org/abs/2509.03548", "authors": ["Jo\u00e3o P. Arroyo", "Jo\u00e3o G. Rodrigues", "Daniel Lawand", "Denis D. Mau\u00e1", "Junkyu Lee", "Radu Marinescu", "Alex Gray", "Eduardo R. Laurentino", "Fabio G. Cozman"], "title": "Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models", "categories": ["cs.AI"], "comment": "Accepted at the Causal Abstractions and Representations (CAR)\n  workshop of the 41st Conference on Uncertainty in Artificial Intelligence\n  (UAI 2025)", "summary": "We investigate partially identifiable queries in a class of causal models. We\nfocus on acyclic Structural Causal Models that are quasi-Markovian (that is,\neach endogenous variable is connected with at most one exogenous confounder).\nWe look into scenarios where endogenous variables are observed (and a\ndistribution over them is known), while exogenous variables are not fully\nspecified. This leads to a representation that is in essence a Bayesian network\nwhere the distribution of root variables is not uniquely determined. In such\ncircumstances, it may not be possible to precisely compute a probability value\nof interest. We thus study the computation of tight probability bounds, a\nproblem that has been solved by multilinear programming in general, and by\nlinear programming when a single confounded component is intervened upon. We\npresent a new algorithm to simplify the construction of such programs by\nexploiting input probabilities over endogenous variables. For scenarios with a\nsingle intervention, we apply column generation to compute a probability bound\nthrough a sequence of auxiliary linear integer programs, thus showing that a\nrepresentation with polynomial cardinality for exogenous variables is possible.\nExperiments show column generation techniques to be superior to existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u51c6\u9a6c\u5c14\u53ef\u592b\u56e0\u679c\u6a21\u578b\u4e2d\u5916\u751f\u53d8\u91cf\u672a\u5b8c\u5168\u6307\u5b9a\u65f6\u7684\u90e8\u5206\u53ef\u8bc6\u522b\u67e5\u8be2\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7a0b\u5e8f\u6784\u5efa\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u5bf9\u5355\u6b21\u5e72\u9884\u573a\u666f\u5e94\u7528\u5217\u751f\u6210\u6280\u672f\u8ba1\u7b97\u7d27\u5bc6\u6982\u7387\u8fb9\u754c\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5217\u751f\u6210\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u51c6\u9a6c\u5c14\u53ef\u592b\u56e0\u679c\u6a21\u578b\u4e2d\uff0c\u5f53\u5916\u751f\u53d8\u91cf\u672a\u5b8c\u5168\u6307\u5b9a\u65f6\uff0c\u65e0\u6cd5\u7cbe\u786e\u8ba1\u7b97\u611f\u5174\u8da3\u7684\u6982\u7387\u503c\uff0c\u56e0\u6b64\u9700\u8981\u8ba1\u7b97\u7d27\u5bc6\u7684\u6982\u7387\u8fb9\u754c\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5185\u751f\u53d8\u91cf\u7684\u8f93\u5165\u6982\u7387\u6765\u7b80\u5316\u591a\u7ebf\u6027\u6216\u7ebf\u6027\u89c4\u5212\u7a0b\u5e8f\u7684\u6784\u5efa\u3002\n2. \u5bf9\u4e8e\u5355\u6b21\u5e72\u9884\u573a\u666f\uff0c\u5e94\u7528\u5217\u751f\u6210\uff08column generation\uff09\u6280\u672f\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u8f85\u52a9\u7ebf\u6027\u6574\u6570\u89c4\u5212\u6765\u8ba1\u7b97\u6982\u7387\u8fb9\u754c\u3002", "result": "1. \u8bc1\u660e\u4e86\u5916\u751f\u53d8\u91cf\u53ef\u4ee5\u7528\u591a\u9879\u5f0f\u57fa\u6570\u8868\u793a\u3002\n2. \u5b9e\u9a8c\u8868\u660e\uff0c\u5217\u751f\u6210\u6280\u672f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5217\u751f\u6210\u6280\u672f\u4e3a\u51c6\u9a6c\u5c14\u53ef\u592b\u56e0\u679c\u6a21\u578b\u4e2d\u90e8\u5206\u53ef\u8bc6\u522b\u67e5\u8be2\u7684\u6982\u7387\u8fb9\u754c\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03530", "pdf": "https://arxiv.org/pdf/2509.03530", "abs": "https://arxiv.org/abs/2509.03530", "authors": ["Paul Blum", "Enrico Liscio", "Ruixuan Zhang", "Caroline Figueroa", "Pradeep K. Murukannaiah"], "title": "Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts", "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a leading cause of death among adolescents (12-18), yet predicting\nit remains a significant challenge. Many cases go undetected due to a lack of\ncontact with mental health services. Social media, however, offers a unique\nopportunity, as young people often share their thoughts and struggles online in\nreal time. In this work, we propose a novel task and method to approach it:\npredicting suicidal ideation and behavior (SIB) from forum posts before an\nadolescent explicitly expresses suicidal ideation on an online forum. This\npredictive framing, where no self-disclosure is used as input at any stage,\nremains largely unexplored in the suicide prediction literature. To this end,\nwe introduce Early-SIB, a transformer-based model that sequentially processes\nthe posts a user writes and engages with to predict whether they will write a\nSIB post. Our model achieves a balanced accuracy of 0.73 for predicting future\nSIB on a Dutch youth forum, demonstrating that such tools can offer a\nmeaningful addition to traditional methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578bEarly-SIB\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u9752\u5c11\u5e74\u5728\u7ebf\u8bba\u575b\u5e16\u5b50\uff0c\u5728\u5176\u660e\u786e\u8868\u8fbe\u81ea\u6740\u610f\u5ff5\u4e4b\u524d\uff0c\u9884\u6d4b\u672a\u6765\u53ef\u80fd\u51fa\u73b0\u7684\u81ea\u6740\u610f\u5ff5\u548c\u884c\u4e3a\uff08SIB\uff09\u3002", "motivation": "\u9752\u5c11\u5e74\u81ea\u6740\u662f\u4e3b\u8981\u6b7b\u56e0\uff0c\u4f46\u9884\u6d4b\u6781\u5177\u6311\u6218\uff0c\u8bb8\u591a\u6848\u4f8b\u56e0\u7f3a\u4e4f\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u63a5\u89e6\u800c\u672a\u88ab\u53d1\u73b0\u3002\u793e\u4ea4\u5a92\u4f53\u4e3a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\uff0c\u56e0\u4e3a\u9752\u5c11\u5e74\u5e38\u5728\u7ebf\u5206\u4eab\u60f3\u6cd5\u3002\u73b0\u6709\u7814\u7a76\u9c9c\u5c11\u63a2\u7d22\u5728\u6ca1\u6709\u660e\u786e\u81ea\u6740\u8868\u9732\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u9884\u6d4b\uff0c\u8fd9\u4fc3\u4f7f\u672c\u7814\u7a76\u63d0\u51fa\u65b0\u7684\u4efb\u52a1\u548c\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86Early-SIB\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u6309\u65f6\u95f4\u987a\u5e8f\u5904\u7406\u7528\u6237\u64b0\u5199\u548c\u4e92\u52a8\u7684\u5e16\u5b50\uff0c\u4ee5\u6b64\u9884\u6d4b\u7528\u6237\u662f\u5426\u4f1a\u5728\u672a\u6765\u53d1\u5e03\u542b\u6709\u81ea\u6740\u610f\u5ff5\u548c\u884c\u4e3a\uff08SIB\uff09\u7684\u5185\u5bb9\u3002", "result": "\u5728\u8377\u5170\u4e00\u4e2a\u9752\u5c11\u5e74\u8bba\u575b\u4e0a\uff0cEarly-SIB\u6a21\u578b\u5728\u9884\u6d4b\u672a\u6765SIB\u65b9\u9762\u8fbe\u5230\u4e860.73\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cEarly-SIB\u8fd9\u7c7b\u5de5\u5177\u80fd\u591f\u5bf9\u4f20\u7edf\u7684\u81ea\u6740\u9884\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u8865\u5145\uff0c\u4e3a\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.03901", "pdf": "https://arxiv.org/pdf/2509.03901", "abs": "https://arxiv.org/abs/2509.03901", "authors": ["Katarzyna Kosek-Szott", "Szymon Szott", "Wojciech Ciezobka", "Maksymilian Wojnar", "Krzysztof Rusek", "Jonathan Segev"], "title": "Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research", "categories": ["cs.NI"], "comment": "30 pages, survey paper", "summary": "Indoor positioning is an enabling technology for home, office, and industrial\nnetwork users because it provides numerous information and communication\ntechnology (ICT) and Internet of things (IoT) functionalities such as indoor\nnavigation, smart meter localization, asset tracking, support for emergency\nservices, and detection of hazardous situations. The IEEE 802.11mc fine timing\nmeasurement (FTM) protocol (commercially known as Wi-Fi Location) has great\npotential to enable indoor positioning in future generation devices, primarily\nbecause of the high availability of Wi-Fi networks, FTM's high accuracy and\ndevice support. Furthermore, new FTM enhancements are available in the released\n(802.11az) and recently completed (802.11bk) amendments. Despite the multitude\nof literature reviews on indoor positioning, a survey dedicated to FTM and its\nrecent enhancements has so far been lacking. We fill this gap by classifying\nand reviewing over 180 research papers related to the practical accuracy\nachieved with FTM, methods for improving its accuracy (also with machine\nlearning), combining FTM with other indoor positioning systems, FTM-based\napplications, and security issues. Based on the conducted survey, we summarize\nthe most important research achievements and formulate open areas for further\nresearch.", "AI": {"tldr": "\u672c\u6587\u5bf9IEEE 802.11mc FTM\u534f\u8bae\uff08Wi-Fi\u5b9a\u4f4d\uff09\u53ca\u5176\u6700\u65b0\u589e\u5f3a\u529f\u80fd\u5728\u5ba4\u5185\u5b9a\u4f4d\u9886\u57df\u7684\u6587\u732e\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5176\u51c6\u786e\u6027\u3001\u6539\u8fdb\u65b9\u6cd5\u3001\u7ec4\u5408\u5e94\u7528\u3001\u5b89\u5168\u95ee\u9898\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5ba4\u5185\u5b9a\u4f4d\u5bf9\u591a\u79cdICT\u548cIoT\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0cIEEE 802.11mc FTM\u534f\u8bae\u56e0Wi-Fi\u7f51\u7edc\u7684\u9ad8\u53ef\u7528\u6027\u3001\u9ad8\u7cbe\u5ea6\u548c\u8bbe\u5907\u652f\u6301\uff0c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5c3d\u7ba1\u6709\u5927\u91cf\u5ba4\u5185\u5b9a\u4f4d\u6587\u732e\u7efc\u8ff0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9FTM\u53ca\u5176\u6700\u65b0\u589e\u5f3a\u529f\u80fd\u7684\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u56de\u987e180\u591a\u7bc7\u76f8\u5173\u7814\u7a76\u8bba\u6587\uff0c\u6db5\u76d6FTM\u7684\u5b9e\u9645\u7cbe\u5ea6\u3001\u7cbe\u5ea6\u6539\u8fdb\u65b9\u6cd5\uff08\u5305\u62ec\u673a\u5668\u5b66\u4e60\uff09\u3001FTM\u4e0e\u5176\u4ed6\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u7684\u7ed3\u5408\u3001\u57fa\u4e8eFTM\u7684\u5e94\u7528\u4ee5\u53ca\u5b89\u5168\u95ee\u9898\u3002", "result": "\u603b\u7ed3\u4e86FTM\u5b9a\u4f4d\u9886\u57df\u6700\u91cd\u8981\u7684\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u57fa\u4e8e\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u9886\u57df\u3002"}}
{"id": "2509.03616", "pdf": "https://arxiv.org/pdf/2509.03616", "abs": "https://arxiv.org/abs/2509.03616", "authors": ["Rajeev Ranjan Dwivedi", "Ankur Kumar", "Vinod K Kurmi"], "title": "Multi Attribute Bias Mitigation via Representation Learning", "categories": ["cs.CV"], "comment": "ECAI 2025 (28th European Conference on Artificial Intelligence)", "summary": "Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5e7f\u4e49\u591a\u504f\u7f6e\u7f13\u89e3\uff08GMBM\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u7cbe\u7b80\u65b9\u6848\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u4e2d\u591a\u91cd\u91cd\u53e0\u504f\u7f6e\u5bf9\u89c6\u89c9\u6a21\u578b\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u7684\u635f\u5bb3\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ec5\u9700\u5206\u7ec4\u6807\u7b7e\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u504f\u7f6e\u5ea6\u91cf\u6807\u51c6SBA\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u666e\u904d\u5b58\u5728\u591a\u79cd\u91cd\u53e0\u504f\u7f6e\uff08\u5982\u7eb9\u7406\u3001\u6c34\u5370\u3001\u6027\u522b\u5316\u5986\u3001\u573a\u666f\u5bf9\u8c61\u914d\u5bf9\u7b49\uff09\uff0c\u8fd9\u4e9b\u504f\u7f6e\u5171\u540c\u635f\u5bb3\u4e86\u73b0\u4ee3\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002\u5355\u72ec\u89e3\u51b3\u8fd9\u4e9b\u504f\u7f6e\u662f\u4e0d\u8db3\u7684\uff0c\u56e0\u4e3a\u7f13\u89e3\u4e00\u4e2a\u504f\u7f6e\u5f80\u5f80\u4f1a\u5141\u8bb8\u6216\u52a0\u5267\u5176\u4ed6\u504f\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u5904\u7406\u591a\u504f\u7f6e\u95ee\u9898\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002", "method": "GMBM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\n1.  **\u81ea\u9002\u5e94\u504f\u7f6e\u96c6\u6210\u5b66\u4e60\uff08ABIL\uff09**\uff1a\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5c5e\u6027\u8bad\u7ec3\u7f16\u7801\u5668\u5e76\u5c06\u5176\u4e0e\u4e3b\u9aa8\u5e72\u7f51\u7edc\u96c6\u6210\uff0c\u6545\u610f\u8bc6\u522b\u5df2\u77e5\u6377\u5f84\u7684\u5f71\u54cd\uff0c\u8feb\u4f7f\u5206\u7c7b\u5668\u660e\u786e\u8bc6\u522b\u8fd9\u4e9b\u504f\u7f6e\u3002\n2.  **\u68af\u5ea6\u6291\u5236\u5fae\u8c03**\uff1a\u4ece\u9aa8\u5e72\u7f51\u7edc\u7684\u68af\u5ea6\u4e2d\u4fee\u526a\u6389\u8fd9\u4e9b\u504f\u7f6e\u65b9\u5411\uff0c\u7559\u4e0b\u4e00\u4e2a\u7d27\u51d1\u7684\u7f51\u7edc\uff0c\u5ffd\u7565\u5176\u521a\u521a\u5b66\u4f1a\u8bc6\u522b\u7684\u6240\u6709\u6377\u5f84\u3002\n\u6b64\u5916\uff0c\u4e3a\u89e3\u51b3\u73b0\u6709\u504f\u7f6e\u5ea6\u91cf\u5728\u5b50\u7ec4\u4e0d\u5e73\u8861\u548c\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5931\u6548\u95ee\u9898\uff0c\u7814\u7a76\u5f15\u5165\u4e86**\u5c3a\u5ea6\u504f\u7f6e\u653e\u5927\uff08SBA\uff09**\uff1a\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u8861\u91cf\u6a21\u578b\u5f15\u8d77\u7684\u504f\u7f6e\u653e\u5927\u4e0e\u5206\u5e03\u5dee\u5f02\u5206\u79bb\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "result": "GMBM\u5728FB CMNIST\u3001CelebA\u548cCOCO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7ec4\u51c6\u786e\u7387\uff0c\u5c06\u591a\u5c5e\u6027\u504f\u7f6e\u653e\u5927\u51cf\u534a\uff0c\u5e76\u5728\u504f\u7f6e\u590d\u6742\u6027\u548c\u5206\u5e03\u504f\u79fb\u52a0\u5267\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7fSBA\u8fbe\u5230\u4e86\u65b0\u4f4e\u3002\u8fd9\u8868\u660eGMBM\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u8bc6\u522b\u7684\u5b9e\u7528\u3001\u7aef\u5230\u7aef\u591a\u504f\u7f6e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GMBM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u8bc6\u522b\u4e2d\u591a\u91cd\u91cd\u53e0\u504f\u7f6e\u95ee\u9898\uff0c\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u65b9\u6cd5\u548c\u65b0\u5f15\u5165\u7684SBA\u5ea6\u91cf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002\u5b83\u88ab\u8ba4\u4e3a\u662f\u7b2c\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u7aef\u5230\u7aef\u7684\u591a\u504f\u7f6e\u89c6\u89c9\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03652", "pdf": "https://arxiv.org/pdf/2509.03652", "abs": "https://arxiv.org/abs/2509.03652", "authors": ["E. Khalafyan", "A. E. Allahverdyan", "A. Hovhannisyan"], "title": "Nonnegative matrix factorization and the principle of the common cause", "categories": ["cs.LG"], "comment": null, "summary": "Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction\nmethod. The principle of the common cause (PCC) is a basic methodological\napproach in probabilistic causality, which seeks an independent mixture model\nfor the joint probability of two dependent random variables. It turns out that\nthese two concepts are closely related. This relationship is explored\nreciprocally for several datasets of gray-scale images, which are conveniently\nmapped into probability models. On one hand, PCC provides a predictability tool\nthat leads to a robust estimation of the effective rank of NMF. Unlike other\nestimates (e.g., those based on the Bayesian Information Criteria), our\nestimate of the rank is stable against weak noise. We show that NMF implemented\naround this rank produces features (basis images) that are also stable against\nnoise and against seeds of local optimization, thereby effectively resolving\nthe NMF nonidentifiability problem. On the other hand, NMF provides an\ninteresting possibility of implementing PCC in an approximate way, where larger\nand positively correlated joint probabilities tend to be explained better via\nthe independent mixture model. We work out a clustering method, where data\npoints with the same common cause are grouped into the same cluster. We also\nshow how NMF can be employed for data denoising.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u4e0e\u5171\u540c\u539f\u56e0\u539f\u5219\uff08PCC\uff09\u4e4b\u95f4\u7684\u4e92\u60e0\u5173\u7cfb\u3002PCC\u80fd\u63d0\u4f9b\u9c81\u68d2\u7684NMF\u6709\u6548\u79e9\u4f30\u8ba1\uff0c\u5e76\u7a33\u5b9a\u5176\u7279\u5f81\uff1bNMF\u80fd\u8fd1\u4f3c\u5b9e\u73b0PCC\uff0c\u8fdb\u800c\u5e94\u7528\u4e8e\u805a\u7c7b\u548c\u6570\u636e\u53bb\u566a\uff0c\u6709\u6548\u89e3\u51b3\u4e86NMF\u7684\u4e0d\u53ef\u8bc6\u522b\u6027\u95ee\u9898\u3002", "motivation": "\u53d1\u73b0\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u4e0e\u6982\u7387\u56e0\u679c\u5173\u7cfb\u4e2d\u7684\u5171\u540c\u539f\u56e0\u539f\u5219\uff08PCC\uff09\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u5173\u8054\uff0c\u56e0\u6b64\u65e8\u5728\u6df1\u5165\u63a2\u7d22\u8fd9\u79cd\u4e92\u60e0\u5173\u7cfb\uff0c\u4ee5\u89e3\u51b3NMF\u7684\u79e9\u4f30\u8ba1\u548c\u975e\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u5e76\u4e3aPCC\u63d0\u4f9b\u65b0\u7684\u5b9e\u73b0\u4e0e\u5e94\u7528\u53ef\u80fd\u6027\u3002", "method": ["\u5c06\u7070\u5ea6\u56fe\u50cf\u6570\u636e\u96c6\u6620\u5c04\u4e3a\u6982\u7387\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u4e92\u60e0\u63a2\u7d22NMF\u4e0ePCC\u7684\u5173\u7cfb\u3002", "\u5229\u7528PCC\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\uff0c\u5bf9NMF\u7684\u6709\u6548\u79e9\u8fdb\u884c\u9c81\u68d2\u4f30\u8ba1\u3002", "\u5229\u7528NMF\u8fd1\u4f3c\u5b9e\u73b0PCC\u3002", "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u540c\u539f\u56e0\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "\u5c55\u793a\u4e86NMF\u5728\u6570\u636e\u53bb\u566a\u4e2d\u7684\u5e94\u7528\u3002"], "result": ["PCC\u80fd\u4e3aNMF\u63d0\u4f9b\u9c81\u68d2\u4e14\u5bf9\u5f31\u566a\u58f0\u7a33\u5b9a\u7684\u6709\u6548\u79e9\u4f30\u8ba1\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eBIC\u7684\u4f30\u8ba1\uff09\u3002", "\u57fa\u4e8ePCC\u4f30\u8ba1\u7684\u79e9\uff0cNMF\u751f\u6210\u7684\u7279\u5f81\uff08\u57fa\u56fe\u50cf\uff09\u5bf9\u566a\u58f0\u548c\u5c40\u90e8\u4f18\u5316\u79cd\u5b50\u90fd\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86NMF\u7684\u975e\u8bc6\u522b\u6027\u95ee\u9898\u3002", "NMF\u4e3aPCC\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fd1\u4f3c\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\uff0c\u5c24\u5176\u80fd\u66f4\u597d\u5730\u89e3\u91ca\u8f83\u5927\u4e14\u6b63\u76f8\u5173\u7684\u8054\u5408\u6982\u7387\u3002", "\u5f00\u53d1\u4e86\u4e00\u79cd\u5c06\u5177\u6709\u76f8\u540c\u5171\u540c\u539f\u56e0\u7684\u6570\u636e\u70b9\u5206\u7ec4\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "\u5c55\u793a\u4e86NMF\u53ef\u6709\u6548\u7528\u4e8e\u6570\u636e\u53bb\u566a\u3002"], "conclusion": "NMF\u4e0ePCC\u4e4b\u95f4\u7d27\u5bc6\u7684\u4e92\u60e0\u5173\u7cfb\u5e26\u6765\u4e86\u663e\u8457\u7684\u534f\u540c\u6548\u5e94\u3002PCC\u63d0\u5347\u4e86NMF\u7684\u7a33\u5b9a\u6027\u548c\u79e9\u4f30\u8ba1\uff0c\u800cNMF\u5219\u4e3aPCC\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8fd1\u4f3c\u5b9e\u73b0\u3002\u8fd9\u79cd\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86NMF\u7684\u975e\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86NMF\u5728\u805a\u7c7b\u548c\u6570\u636e\u53bb\u566a\u7b49\u65b9\u9762\u7684\u5e94\u7528\u3002"}}
{"id": "2509.03550", "pdf": "https://arxiv.org/pdf/2509.03550", "abs": "https://arxiv.org/abs/2509.03550", "authors": ["Tonghe Li", "Jixin Liu", "Weili Zeng", "Hao Jiang"], "title": "Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method", "categories": ["cs.AI"], "comment": "59 pages,13 figures, 3 tables", "summary": "In the context of continuously rising global air traffic, efficient and safe\nConflict Detection and Resolution (CD&R) is paramount for air traffic\nmanagement. Although Deep Reinforcement Learning (DRL) offers a promising\npathway for CD&R automation, existing approaches commonly suffer from a\n\"unimodal bias\" in their policies. This leads to a critical lack of\ndecision-making flexibility when confronted with complex and dynamic\nconstraints, often resulting in \"decision deadlocks.\" To overcome this\nlimitation, this paper pioneers the integration of diffusion probabilistic\nmodels into the safety-critical task of CD&R, proposing a novel autonomous\nconflict resolution framework named Diffusion-AC. Diverging from conventional\nmethods that converge to a single optimal solution, our framework models its\npolicy as a reverse denoising process guided by a value function, enabling it\nto generate a rich, high-quality, and multimodal action distribution. This core\narchitecture is complemented by a Density-Progressive Safety Curriculum (DPSC),\na training mechanism that ensures stable and efficient learning as the agent\nprogresses from sparse to high-density traffic environments. Extensive\nsimulation experiments demonstrate that the proposed method significantly\noutperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the\nmost challenging high-density scenarios, Diffusion-AC not only maintains a high\nsuccess rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions\n(NMACs) by approximately 59% compared to the next-best-performing baseline,\nsignificantly enhancing the system's safety margin. This performance leap stems\nfrom its unique multimodal decision-making capability, which allows the agent\nto flexibly switch to effective alternative maneuvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusion-AC\u7684\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\uff08CD&R\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6269\u6563\u6982\u7387\u6a21\u578b\u548c\u5bc6\u5ea6\u6e10\u8fdb\u5f0f\u5b89\u5168\u8bfe\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u4e2d\u7b56\u7565\u7684\u201c\u5355\u5cf0\u504f\u7f6e\u201d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u51b3\u7b56\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5bc6\u5ea6\u7a7a\u57df\u4e0b\u7684\u51b2\u7a81\u89e3\u51b3\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u5168\u7403\u822a\u7a7a\u4ea4\u901a\u91cf\u7684\u6301\u7eed\u589e\u957f\uff0c\u9ad8\u6548\u5b89\u5168\u7684\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\uff08CD&R\uff09\u5bf9\u4e8e\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u5728CD&R\u4e2d\u5e38\u53d7\u201c\u5355\u5cf0\u504f\u7f6e\u201d\u56f0\u6270\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u52a8\u6001\u7ea6\u675f\u4e0b\u7f3a\u4e4f\u51b3\u7b56\u7075\u6d3b\u6027\uff0c\u6613\u9677\u5165\u201c\u51b3\u7b56\u50f5\u5c40\u201d\u3002", "method": "\u672c\u6587\u521b\u65b0\u6027\u5730\u5c06\u6269\u6563\u6982\u7387\u6a21\u578b\u5e94\u7528\u4e8eCD&R\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u540d\u4e3aDiffusion-AC\u7684\u81ea\u4e3b\u51b2\u7a81\u89e3\u51b3\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u7b56\u7565\u5efa\u6a21\u4e3a\u7531\u4ef7\u503c\u51fd\u6570\u5f15\u5bfc\u7684\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\uff0c\u4ee5\u751f\u6210\u4e30\u5bcc\u3001\u9ad8\u8d28\u91cf\u548c\u591a\u6a21\u6001\u7684\u52a8\u4f5c\u5206\u5e03\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u201c\u5bc6\u5ea6\u6e10\u8fdb\u5f0f\u5b89\u5168\u8bfe\u7a0b\uff08DPSC\uff09\u201d\u8bad\u7ec3\u673a\u5236\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u5728\u4ece\u7a00\u758f\u5230\u9ad8\u5bc6\u5ea6\u4ea4\u901a\u73af\u5883\u8fc7\u6e21\u65f6\u80fd\u7a33\u5b9a\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684DRL\u57fa\u7ebf\u3002\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\uff0cDiffusion-AC\u4e0d\u4ec5\u4fdd\u6301\u4e8694.1%\u7684\u9ad8\u6210\u529f\u7387\uff0c\u800c\u4e14\u4e0e\u6b21\u4f18\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5c06\u8fd1\u8ddd\u7a7a\u4e2d\u76f8\u649e\uff08NMACs\uff09\u7684\u53d1\u751f\u7387\u964d\u4f4e\u4e86\u7ea659%\u3002", "conclusion": "Diffusion-AC\u901a\u8fc7\u5176\u72ec\u7279\u7684\u591a\u6a21\u6001\u51b3\u7b56\u80fd\u529b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u7075\u6d3b\u5207\u6362\u5230\u6709\u6548\u7684\u66ff\u4ee3\u673a\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0a\u7684\u663e\u8457\u98de\u8dc3\uff0c\u6781\u5927\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u88d5\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709DRL\u5728CD&R\u4e2d\u9762\u4e34\u7684\u51b3\u7b56\u7075\u6d3b\u6027\u95ee\u9898\u3002"}}
{"id": "2509.03531", "pdf": "https://arxiv.org/pdf/2509.03531", "abs": "https://arxiv.org/abs/2509.03531", "authors": ["Oscar Obeso", "Andy Arditi", "Javier Ferrando", "Joshua Freeman", "Cameron Holmes", "Neel Nanda"], "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models are now routinely used in high-stakes applications\nwhere hallucinations can cause serious harm, such as medical consultations or\nlegal advice. Existing hallucination detection methods, however, are\nimpractical for real-world use, as they are either limited to short factual\nqueries or require costly external verification. We present a cheap, scalable\nmethod for real-time identification of hallucinated tokens in long-form\ngenerations, and scale it effectively to 70B parameter models. Our approach\ntargets \\emph{entity-level hallucinations} -- e.g., fabricated names, dates,\ncitations -- rather than claim-level, thereby naturally mapping to token-level\nlabels and enabling streaming detection. We develop an annotation methodology\nthat leverages web search to annotate model responses with grounded labels\nindicating which tokens correspond to fabricated entities. This dataset enables\nus to train effective hallucination classifiers with simple and efficient\nmethods such as linear probes. Evaluating across four model families, our\nclassifiers consistently outperform baselines on long-form responses, including\nmore expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for\nLlama-3.3-70B), and are also an improvement in short-form question-answering\nsettings. Moreover, despite being trained only with entity-level labels, our\nprobes effectively detect incorrect answers in mathematical reasoning tasks,\nindicating generalization beyond entities. While our annotation methodology is\nexpensive, we find that annotated responses from one model can be used to train\neffective classifiers on other models; accordingly, we publicly release our\ndatasets to facilitate reuse. Overall, our work suggests a promising new\napproach for scalable, real-world hallucination detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5b9e\u4f53\u7ea7\u5e7b\u89c9\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u4efb\u52a1\uff0c\u540c\u65f6\u516c\u5f00\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff08\u5982\u533b\u7597\u54a8\u8be2\u3001\u6cd5\u5f8b\u5efa\u8bae\uff09\u4f7f\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5e7b\u89c9\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u5371\u5bb3\u3002\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u77ed factual \u67e5\u8be2\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ecf\u6d4e\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u68c0\u6d4b\u957f\u6587\u672c\u751f\u6210\u4e2d\u5e7b\u89c9\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u9488\u5bf9\u5b9e\u4f53\u7ea7\u5e7b\u89c9\uff08\u5982\u865a\u6784\u7684\u59d3\u540d\u3001\u65e5\u671f\u3001\u5f15\u7528\uff09\uff0c\u5c06\u5176\u6620\u5c04\u4e3a token \u7ea7\u522b\u6807\u7b7e\u4ee5\u5b9e\u73b0\u6d41\u5f0f\u68c0\u6d4b\u3002\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u7f51\u7edc\u641c\u7d22\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u7528\u4e8e\u6807\u8bb0\u6a21\u578b\u54cd\u5e94\u4e2d\u865a\u6784\u7684\u5b9e\u4f53\uff0c\u5e76\u4f7f\u7528\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u4e86\u6709\u6548\u7684\u5e7b\u89c9\u5206\u7c7b\u5668\uff0c\u5982\u7b80\u5355\u9ad8\u6548\u7684\u7ebf\u6027\u63a2\u9488\uff08linear probes\uff09\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3 70B \u53c2\u6570\u6a21\u578b\u3002", "result": "\u5206\u7c7b\u5668\u5728\u957f\u6587\u672c\u54cd\u5e94\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u4f8b\u5982\uff0cLlama-3.3-70B \u7684 AUC 0.90 vs. 0.71\uff0c\u4f18\u4e8e\u8bed\u4e49\u71b5\uff09\uff0c\u5728\u77ed\u6587\u672c\u95ee\u7b54\u8bbe\u7f6e\u4e2d\u4e5f\u8868\u73b0\u51fa\u6539\u8fdb\u3002\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u5b9e\u4f53\u7ea7\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6240\u5f00\u53d1\u7684\u63a2\u9488\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4e5f\u80fd\u6709\u6548\u68c0\u6d4b\u9519\u8bef\u7b54\u6848\uff0c\u8868\u660e\u5176\u5177\u5907\u8d85\u8d8a\u5b9e\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002\u867d\u7136\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u4e00\u4e2a\u6a21\u578b\u6807\u6ce8\u7684\u54cd\u5e94\u53ef\u7528\u4e8e\u8bad\u7ec3\u5176\u4ed6\u6a21\u578b\u7684\u6709\u6548\u5206\u7c7b\u5668\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u771f\u5b9e\u4e16\u754c\u7684\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\u3002\u4e3a\u4fc3\u8fdb\u590d\u7528\uff0c\u7814\u7a76\u56e2\u961f\u516c\u5f00\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3002"}}
{"id": "2509.03935", "pdf": "https://arxiv.org/pdf/2509.03935", "abs": "https://arxiv.org/abs/2509.03935", "authors": ["Sungho Cho", "Sung Il Choi", "Seung Hyun Oh", "Ian P. Roberts", "Sang Hyun Lee"], "title": "Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues", "categories": ["cs.NI"], "comment": null, "summary": "This work considers a parallel task execution strategy in vehicular edge\ncomputing (VEC) networks, where edge servers are deployed along the roadside to\nprocess offloaded computational tasks of vehicular users. To minimize the\noverall waiting delay among vehicular users, a novel task offloading solution\nis implemented based on the network cooperation balancing resource\nunder-utilization and load congestion. Dual evaluation through theoretical and\nnumerical ways shows that the developed solution achieves a globally optimal\ndelay reduction performance compared to existing methods, which is also\napproved by the feasibility test over a real-map virtual environment. The\nin-depth analysis reveals that predicting the instantaneous processing power of\nedge servers facilitates the identification of overloaded servers, which is\ncritical for determining network delay. By considering discrete variables of\nthe queue, the proposed technique's precise estimation can effectively address\nthese combinatorial challenges to achieve optimal performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6848\uff0c\u901a\u8fc7\u7f51\u7edc\u534f\u4f5c\u5728\u8f66\u8f7d\u8fb9\u7f18\u8ba1\u7b97\uff08VEC\uff09\u7f51\u7edc\u4e2d\u6700\u5c0f\u5316\u7528\u6237\u7b49\u5f85\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u4f18\u7684\u5ef6\u8fdf\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u5728\u8f66\u8f7d\u8fb9\u7f18\u8ba1\u7b97\uff08VEC\uff09\u7f51\u7edc\u4e2d\uff0c\u8fb9\u7f18\u670d\u52a1\u5668\u5904\u7406\u8f66\u8f86\u7528\u6237\u5378\u8f7d\u7684\u8ba1\u7b97\u4efb\u52a1\u3002\u7814\u7a76\u65e8\u5728\u6700\u5c0f\u5316\u8f66\u8f86\u7528\u6237\u7684\u6574\u4f53\u7b49\u5f85\u5ef6\u8fdf\u3002", "method": "\u5b9e\u65bd\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u534f\u4f5c\u7684\u65b0\u578b\u4efb\u52a1\u5378\u8f7d\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5e73\u8861\u4e86\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548c\u8d1f\u8f7d\u62e5\u5835\u3002\u901a\u8fc7\u9884\u6d4b\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u77ac\u65f6\u5904\u7406\u80fd\u529b\u6765\u8bc6\u522b\u8fc7\u8f7d\u670d\u52a1\u5668\uff0c\u5e76\u8003\u8651\u961f\u5217\u7684\u79bb\u6563\u53d8\u91cf\u8fdb\u884c\u7cbe\u786e\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u548c\u6570\u503c\u8bc4\u4f30\u4ee5\u53ca\u771f\u5b9e\u5730\u56fe\u865a\u62df\u73af\u5883\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u5f00\u53d1\u7684\u89e3\u51b3\u65b9\u6848\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u4f18\u7684\u5ef6\u8fdf\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6280\u672f\u901a\u8fc7\u7cbe\u786e\u4f30\u8ba1\u961f\u5217\u7684\u79bb\u6563\u53d8\u91cf\u5e76\u9884\u6d4b\u670d\u52a1\u5668\u5904\u7406\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7ec4\u5408\u6311\u6218\uff0c\u4ece\u800c\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u5ef6\u8fdf\u3002"}}
{"id": "2509.03631", "pdf": "https://arxiv.org/pdf/2509.03631", "abs": "https://arxiv.org/abs/2509.03631", "authors": ["Anders Kjelsrud", "Lasse L\u00f8vstakken", "Erik Smistad", "H\u00e5vard Dalen", "Gilles Van De Vyver"], "title": "Lightweight image segmentation for echocardiography", "categories": ["cs.CV"], "comment": "4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium", "summary": "Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7U-Net\u6a21\u578b\u7528\u4e8e\u8d85\u58f0\u5fc3\u52a8\u56fe\u5de6\u5fc3\u5ba4\u5206\u5272\uff0c\u5176\u6027\u80fd\u4e0ennU-Net\u76f8\u5f53\uff0c\u4f46\u4f53\u79ef\u66f4\u5c0f\u3001\u901f\u5ea6\u66f4\u5feb\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u5de6\u5fc3\u5ba4\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e8e\u81ea\u52a8\u63d0\u53d6\u4e34\u5e8a\u6d4b\u91cf\u6307\u6807\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1nnU-Net\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u4f53\u79ef\u5e9e\u5927\u4e14\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u5bf9nnU-Net\u7684\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u6d88\u878d\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6570\u636e\u589e\u5f3a\u65b9\u6848\u3001\u67b6\u6784\u4fee\u6539\u3001\u635f\u5931\u51fd\u6570\u548c\u540e\u5904\u7406\u6280\u672f\u5728\u5fc3\u810f\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7U-Net\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u4eff\u5c04\u589e\u5f3a\u548c\u6df1\u5ea6\u76d1\u7763\u662f\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\uff0c\u800c\u590d\u6742\u589e\u5f3a\u548c\u5927\u578b\u6a21\u578b\u5bb9\u91cf\u7684\u56de\u62a5\u9012\u51cf\u3002\u5f00\u53d1\u7684\u8f7b\u91cf\u7ea7U-Net\uff082M\u53c2\u6570\uff09\u5728CAMUS\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0ennU-Net\uff0833M\u53c2\u6570\uff09\u7edf\u8ba1\u5b66\u4e0a\u7b49\u6548\u7684\u6027\u80fd\uff08LV/MYO/LA\u7684Dice\u5206\u6570\u5206\u522b\u4e3a0.93/0.85/0.89 vs 0.93/0.86/0.89, p>0.05\uff09\uff0c\u540c\u65f6\u6a21\u578b\u4f53\u79ef\u7f29\u5c0f16\u500d\uff0c\u901f\u5ea6\u63d0\u53474\u500d\uff08\u6bcf\u5e271.35ms vs 5.40ms\uff09\u3002\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u53ef\u6bd4\u8f83\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u51cf\u5c0f\u6a21\u578b\u4f53\u79ef\uff0c\u4ece\u800c\u6ee1\u8db3\u4e86\u5b9e\u65f6\u5fc3\u810f\u5206\u5272\u7684\u9700\u6c42\u3002"}}
{"id": "2509.03660", "pdf": "https://arxiv.org/pdf/2509.03660", "abs": "https://arxiv.org/abs/2509.03660", "authors": ["Yunkai Bao", "Reza Safarzadeh", "Xin Wang", "Steve Drew"], "title": "Semi-decentralized Federated Time Series Prediction with Client Availability Budgets", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning (FL) effectively promotes collaborative training among\ndistributed clients with privacy considerations in the Internet of Things (IoT)\nscenarios. Despite of data heterogeneity, FL clients may also be constrained by\nlimited energy and availability budgets. Therefore, effective selection of\nclients participating in training is of vital importance for the convergence of\nthe global model and the balance of client contributions. In this paper, we\ndiscuss the performance impact of client availability with time-series data on\nfederated learning. We set up three different scenarios that affect the\navailability of time-series data and propose FedDeCAB, a novel,\nsemi-decentralized client selection method applying probabilistic rankings of\navailable clients. When a client is disconnected from the server, FedDeCAB\nallows obtaining partial model parameters from the nearest neighbor clients for\njoint optimization, improving the performance of offline models and reducing\ncommunication overhead. Experiments based on real-world large-scale taxi and\nvessel trajectory datasets show that FedDeCAB is effective under highly\nheterogeneous data distribution, limited communication budget, and dynamic\nclient offline or rejoining.", "AI": {"tldr": "\u9488\u5bf9\u7269\u8054\u7f51\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u548c\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFedDeCAB\uff0c\u4e00\u79cd\u534a\u53bb\u4e2d\u5fc3\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6982\u7387\u6392\u540d\u548c\u90bb\u5c45\u5ba2\u6237\u7aef\u53c2\u6570\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u5e94\u5bf9\u5ba2\u6237\u7aef\u65ad\u5f00\u8fde\u63a5\uff0c\u63d0\u5347\u79bb\u7ebf\u6a21\u578b\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u7269\u8054\u7f51\u573a\u666f\u4e2d\uff0c\u9664\u4e86\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u6311\u6218\u5916\uff0c\u5ba2\u6237\u7aef\u8fd8\u53d7\u9650\u4e8e\u6709\u9650\u7684\u80fd\u91cf\u548c\u53ef\u7528\u6027\u9884\u7b97\u3002\u56e0\u6b64\uff0c\u6709\u6548\u9009\u62e9\u53c2\u4e0e\u8bad\u7ec3\u7684\u5ba2\u6237\u7aef\u5bf9\u4e8e\u786e\u4fdd\u5168\u5c40\u6a21\u578b\u6536\u655b\u548c\u5e73\u8861\u5ba2\u6237\u7aef\u8d21\u732e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FedDeCAB\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u534a\u53bb\u4e2d\u5fc3\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u53ef\u7528\u5ba2\u6237\u7aef\u7684\u6982\u7387\u6392\u540d\u673a\u5236\uff0c\u5e76\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bbe\u7f6e\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u53ef\u7528\u6027\u573a\u666f\u3002\u5f53\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u65ad\u5f00\u8fde\u63a5\u65f6\uff0cFedDeCAB\u5141\u8bb8\u4ece\u6700\u8fd1\u7684\u90bb\u5c45\u5ba2\u6237\u7aef\u83b7\u53d6\u90e8\u5206\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u5927\u89c4\u6a21\u7684\u58eb\u548c\u8239\u8236\u8f68\u8ff9\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedDeCAB\u5728\u9ad8\u5ea6\u5f02\u6784\u7684\u6570\u636e\u5206\u5e03\u3001\u6709\u9650\u7684\u901a\u4fe1\u9884\u7b97\u4ee5\u53ca\u5ba2\u6237\u7aef\u52a8\u6001\u79bb\u7ebf\u6216\u91cd\u65b0\u52a0\u5165\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "FedDeCAB\u901a\u8fc7\u5176\u534a\u53bb\u4e2d\u5fc3\u5316\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u548c\u90bb\u5c45\u53c2\u6570\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u4f4e\u548c\u6570\u636e\u9ad8\u5ea6\u5f02\u6784\u7684\u6311\u6218\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u79bb\u7ebf\u6a21\u578b\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2509.03581", "pdf": "https://arxiv.org/pdf/2509.03581", "abs": "https://arxiv.org/abs/2509.03581", "authors": ["Davide Paglieri", "Bart\u0142omiej Cupia\u0142", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim Rockt\u00e4schel"], "title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Training large language models (LLMs) to reason via reinforcement learning\n(RL) significantly improves their problem-solving capabilities. In agentic\nsettings, existing methods like ReAct prompt LLMs to explicitly plan before\nevery action; however, we demonstrate that always planning is computationally\nexpensive and degrades performance on long-horizon tasks, while never planning\nfurther limits performance. To address this, we introduce a conceptual\nframework formalizing dynamic planning for LLM agents, enabling them to\nflexibly decide when to allocate test-time compute for planning. We propose a\nsimple two-stage training pipeline: (1) supervised fine-tuning on diverse\nsynthetic data to prime models for dynamic planning, and (2) RL to refine this\ncapability in long-horizon environments. Experiments on the Crafter environment\nshow that dynamic planning agents trained with this approach are more\nsample-efficient and consistently achieve more complex objectives.\nAdditionally, we demonstrate that these agents can be effectively steered by\nhuman-written plans, surpassing their independent capabilities. To our\nknowledge, this work is the first to explore training LLM agents for dynamic\ntest-time compute allocation in sequential decision-making tasks, paving the\nway for more efficient, adaptive, and controllable agentic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u7684\u52a8\u6001\u89c4\u5212\u6846\u67b6\u53ca\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u89c4\u5212\uff0c\u4ece\u800c\u5728\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u9996\u6b21\u63a2\u7d22\u4e86\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u52a8\u6001\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\uff08\u5982ReAct\uff09\u8981\u6c42LLM\u5728\u6bcf\u6b21\u884c\u52a8\u524d\u90fd\u8fdb\u884c\u660e\u786e\u89c4\u5212\uff0c\u8fd9\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u5728\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u4f1a\u964d\u4f4e\u6027\u80fd\uff1b\u800c\u4ece\u4e0d\u89c4\u5212\u5219\u4f1a\u9650\u5236\u5176\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u8ba9LLM\u667a\u80fd\u4f53\u80fd\u591f\u7075\u6d3b\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u89c4\u5212\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\u6765\u5f62\u5f0f\u5316LLM\u667a\u80fd\u4f53\u7684\u52a8\u6001\u89c4\u5212\uff0c\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u51b3\u5b9a\u4f55\u65f6\u5206\u914d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u89c4\u5212\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a(1) \u5728\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4ee5\u521d\u6b65\u57f9\u517b\u6a21\u578b\u7684\u52a8\u6001\u89c4\u5212\u80fd\u529b\uff1b(2) \u5728\u957f\u5468\u671f\u73af\u5883\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u4ee5\u8fdb\u4e00\u6b65\u5b8c\u5584\u548c\u4f18\u5316\u6b64\u80fd\u529b\u3002", "result": "\u5728Crafter\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6b64\u65b9\u6cd5\u8bad\u7ec3\u7684\u52a8\u6001\u89c4\u5212\u667a\u80fd\u4f53\u66f4\u5177\u6837\u672c\u6548\u7387\uff0c\u5e76\u80fd\u6301\u7eed\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u76ee\u6807\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u88ab\u4eba\u7c7b\u7f16\u5199\u7684\u8ba1\u5212\u6709\u6548\u5f15\u5bfc\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u72ec\u7acb\u8fd0\u4f5c\u65f6\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u63a2\u7d22\u4e86\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u8fdb\u884c\u52a8\u6001\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u548c\u53ef\u63a7\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.03533", "pdf": "https://arxiv.org/pdf/2509.03533", "abs": "https://arxiv.org/abs/2509.03533", "authors": ["Igor Halperin"], "title": "Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck", "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are prone to critical failure modes, including\n\\textit{intrinsic faithfulness hallucinations} (also known as confabulations),\nwhere a response deviates semantically from the provided context. Frameworks\ndesigned to detect this, such as Semantic Divergence Metrics (SDM), rely on\nidentifying latent topics shared between prompts and responses, typically by\napplying geometric clustering to their sentence embeddings. This creates a\ndisconnect, as the topics are optimized for spatial proximity, not for the\ndownstream information-theoretic analysis. In this paper, we bridge this gap by\ndeveloping a principled topic identification method grounded in the\nDeterministic Information Bottleneck (DIB) for geometric clustering. Our key\ncontribution is to transform the DIB method into a practical algorithm for\nhigh-dimensional data by substituting its intractable KL divergence term with a\ncomputationally efficient upper bound. The resulting method, which we dub UDIB,\ncan be interpreted as an entropy-regularized and robustified version of K-means\nthat inherently favors a parsimonious number of informative clusters. By\napplying UDIB to the joint clustering of LLM prompt and response embeddings, we\ngenerate a shared topic representation that is not merely spatially coherent\nbut is fundamentally structured to be maximally informative about the\nprompt-response relationship. This provides a superior foundation for the SDM\nframework and offers a novel, more sensitive tool for detecting confabulations.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u6d4b\u6846\u67b6\uff08SDM\uff09\u5728\u4e3b\u9898\u8bc6\u522b\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u63d0\u51faUDIB\uff0c\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbDIB\u7684\u805a\u7c7b\u7b97\u6cd5\uff0c\u80fd\u751f\u6210\u4fe1\u606f\u91cf\u66f4\u4e30\u5bcc\u7684\u5171\u4eab\u4e3b\u9898\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347SDM\u68c0\u6d4bLLM\u5e7b\u89c9\u7684\u654f\u611f\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u4ea7\u751f\u201c\u5185\u5728\u5fe0\u5b9e\u5ea6\u5e7b\u89c9\u201d\uff08confabulations\uff09\uff0c\u5373\u54cd\u5e94\u4e0e\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u8bed\u4e49\u4e0d\u7b26\u3002\u73b0\u6709\u7684\u68c0\u6d4b\u6846\u67b6\uff08\u5982\u8bed\u4e49\u5dee\u5f02\u5ea6\u91cfSDM\uff09\u901a\u8fc7\u5bf9\u53e5\u5b50\u5d4c\u5165\u8fdb\u884c\u51e0\u4f55\u805a\u7c7b\u6765\u8bc6\u522b\u6f5c\u5728\u4e3b\u9898\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f18\u5316\u7684\u662f\u7a7a\u95f4\u90bb\u8fd1\u6027\uff0c\u800c\u975e\u540e\u7eed\u4fe1\u606f\u7406\u8bba\u5206\u6790\u6240\u9700\u7684\u4fe1\u606f\u91cf\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u53d7\u9650\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u786e\u5b9a\u6027\u4fe1\u606f\u74f6\u9888\uff08DIB\uff09\u7684\u539f\u5219\u6027\u4e3b\u9898\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u547d\u540d\u4e3aUDIB\u3002\u901a\u8fc7\u7528\u8ba1\u7b97\u9ad8\u6548\u7684\u4e0a\u754c\u66ff\u6362DIB\u4e2d\u96be\u4ee5\u5904\u7406\u7684KL\u6563\u5ea6\u9879\uff0c\u5c06DIB\u65b9\u6cd5\u8f6c\u5316\u4e3a\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u7684\u5b9e\u7528\u7b97\u6cd5\u3002UDIB\u53ef\u88ab\u89c6\u4e3aK-means\u7684\u71b5\u6b63\u5219\u5316\u548c\u9c81\u68d2\u5316\u7248\u672c\uff0c\u5176\u7279\u70b9\u662f\u5929\u7136\u503e\u5411\u4e8e\u751f\u6210\u6570\u91cf\u9002\u4e2d\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u805a\u7c7b\u3002\u8be5\u65b9\u6cd5\u88ab\u5e94\u7528\u4e8eLLM\u63d0\u793a\u548c\u54cd\u5e94\u5d4c\u5165\u7684\u8054\u5408\u805a\u7c7b\uff0c\u4ee5\u751f\u6210\u5171\u4eab\u4e3b\u9898\u8868\u793a\u3002", "result": "UDIB\u751f\u6210\u7684\u5171\u4eab\u4e3b\u9898\u8868\u793a\u4e0d\u4ec5\u5177\u6709\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u800c\u4e14\u5728\u6839\u672c\u4e0a\u88ab\u6784\u5efa\u4e3a\u5173\u4e8e\u63d0\u793a-\u54cd\u5e94\u5173\u7cfb\u7684\u6700\u5927\u4fe1\u606f\u91cf\u8868\u793a\u3002\u8fd9\u4e3a\u8bed\u4e49\u5dee\u5f02\u5ea6\u91cf\uff08SDM\uff09\u6846\u67b6\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u57fa\u7840\u3002", "conclusion": "UDIB\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u66f4\u654f\u611f\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4bLLM\u7684\u865a\u6784\uff08\u5e7b\u89c9\uff09\u95ee\u9898\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u73b0\u6709SDM\u6846\u67b6\u5728\u8bc6\u522b\u4e3b\u9898\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2509.04219", "pdf": "https://arxiv.org/pdf/2509.04219", "abs": "https://arxiv.org/abs/2509.04219", "authors": ["Leandro M\u00e1rcio Bertholdo", "Renan Barreto Paredes", "Gabriela de Lima Marin", "Cesar A. H. Loureiro", "Milton Kaoru Kashiwakura Pedro de Botelho Marcos"], "title": "Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding", "categories": ["cs.NI"], "comment": "32 pages, 15 figures. To appear in Passive and Active Measurement\n  Conference (PAM) 2025, published in Lecture Notes in Computer Science (LNCS),\n  Springer. The final authenticated version is available at\n  https://doi.org/10.1007/978-3-031-85960-1_12", "summary": "In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused\nwidespread damage to infrastructure, impacting over 400 cities and 2.3 million\npeople. This study presents the construction of comprehensive\ntelecommunications datasets during this climatic event, encompassing Internet\nmeasurements, fiber cut reports, and Internet Exchange routing data. By\ncorrelating network disruptions with hydrological and operational factors, the\ndataset offers insights into the resilience of fiber networks, data centers,\nand Internet traffic during critical events. For each scenario, we investigate\nfailures related to the Information and Communication Technology infrastructure\nand highlight the challenges faced when its resilience is critically tested.\nPreliminary findings reveal trends in connectivity restoration, infrastructure\nvulnerabilities, and user behavior changes. These datasets and pre-analysis aim\nto support future research on disaster recovery strategies and the development\nof robust telecommunications systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5df4\u897f2024\u5e74\u6d2a\u707e\u671f\u95f4\u7684\u7535\u4fe1\u6570\u636e\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u97e7\u6027\u3001\u8106\u5f31\u6027\u53ca\u7528\u6237\u884c\u4e3a\u53d8\u5316\uff0c\u65e8\u5728\u652f\u6301\u707e\u5bb3\u6062\u590d\u7b56\u7565\u7814\u7a76\u3002", "motivation": "\u5df4\u897f2024\u5e745\u6708\u6d2a\u707e\u5bf9\u57fa\u7840\u8bbe\u65bd\u9020\u6210\u5e7f\u6cdb\u7834\u574f\uff0c\u5f71\u54cd\u4e86400\u591a\u4e2a\u57ce\u5e02\u548c230\u4e07\u4eba\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u548c\u5206\u6790\u7535\u4fe1\u6570\u636e\u96c6\uff0c\u6df1\u5165\u7406\u89e3\u5149\u7ea4\u7f51\u7edc\u3001\u6570\u636e\u4e2d\u5fc3\u548c\u4e92\u8054\u7f51\u6d41\u91cf\u5728\u5173\u952e\u4e8b\u4ef6\u4e2d\u7684\u97e7\u6027\uff0c\u5e76\u63ed\u793a\u4fe1\u606f\u901a\u4fe1\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u5728\u4e25\u5cfb\u8003\u9a8c\u4e0b\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u6db5\u76d6\u4e92\u8054\u7f51\u6d4b\u91cf\u3001\u5149\u7ea4\u4e2d\u65ad\u62a5\u544a\u548c\u4e92\u8054\u7f51\u4ea4\u6362\u8def\u7531\u6570\u636e\u7684\u7efc\u5408\u7535\u4fe1\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5c06\u7f51\u7edc\u4e2d\u65ad\u4e0e\u6c34\u6587\u548c\u8fd0\u884c\u56e0\u7d20\u8fdb\u884c\u5173\u8054\u5206\u6790\uff0c\u5e76\u8c03\u67e5\u4fe1\u606f\u901a\u4fe1\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u7684\u76f8\u5173\u6545\u969c\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u63ed\u793a\u4e86\u8fde\u63a5\u6062\u590d\u8d8b\u52bf\u3001\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u6027\u4ee5\u53ca\u7528\u6237\u884c\u4e3a\u53d8\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u521d\u6b65\u5206\u6790\u65e8\u5728\u4e3a\u672a\u6765\u5173\u4e8e\u707e\u5bb3\u6062\u590d\u7b56\u7565\u7684\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u4fc3\u8fdb\u5065\u58ee\u7535\u4fe1\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.03633", "pdf": "https://arxiv.org/pdf/2509.03633", "abs": "https://arxiv.org/abs/2509.03633", "authors": ["Josafat-Mattias Burmeister", "Andreas Tockner", "Stefan Reder", "Markus Engel", "Rico Richter", "Jan-Peter Mund", "J\u00fcrgen D\u00f6llner"], "title": "treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds", "categories": ["cs.CV", "cs.AI", "I.4.6; I.5.2; I.2.10"], "comment": null, "summary": "Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u8ba2\u7248\u7684\u65e0\u76d1\u7763treeX\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u8fd1\u8ddd\u79bb\u6fc0\u5149\u626b\u63cf\u70b9\u4e91\u4e2d\u9ad8\u6548\u5730\u8fdb\u884c\u5355\u6728\u5206\u5272\u3002\u8be5\u7b97\u6cd5\u9488\u5bf9\u4e0d\u540c\u6fc0\u5149\u626b\u63cf\u6570\u636e\uff08TLS, PLS, ULS\uff09\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u8d44\u6e90\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u53ef\u7528\u4e8e\u534a\u81ea\u52a8\u6807\u7b7e\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5355\u6728\u5206\u5272\u9700\u8981\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u6709\u6548\u5904\u7406\u8fd1\u8ddd\u79bb\u6fc0\u5149\u626b\u63cf\u83b7\u53d6\u7684\u68ee\u6797\u70b9\u4e91\u6570\u636e\u3002", "method": "\u672c\u6587\u63d0\u51fatreeX\u7b97\u6cd5\u7684\u4fee\u8ba2\u7248\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u805a\u7c7b\u7684\u6811\u5e72\u68c0\u6d4b\u548c\u533a\u57df\u589e\u957f\u7684\u6811\u51a0\u63cf\u7ed8\u3002\u8be5\u65b9\u6cd5\u4e3a\u5730\u9762\u6fc0\u5149\u626b\u63cf\uff08TLS\u548cPLS\uff09\u548c\u65e0\u4eba\u673a\u6fc0\u5149\u626b\u63cf\uff08ULS\uff09\u6570\u636e\u63d0\u4f9b\u4e86\u4e24\u5957\u53c2\u6570\u9884\u8bbe\u3002\u4f5c\u8005\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5bf9\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5305\u62ec\u539f\u7248treeX\u5728\u5185\u7684\u516d\u79cd\u5f00\u6e90\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u4e0e\u539f\u7248treeX\u7b97\u6cd5\u76f8\u6bd4\uff0c\u4fee\u8ba2\u7248\u663e\u8457\u7f29\u77ed\u4e86\u8fd0\u884c\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff1a\u5bf9\u4e8e\u5730\u9762\u6570\u636e\uff0c\u5355\u6728\u68c0\u6d4bF1\u5206\u6570\u63d0\u9ad8\u4e86+0.11\u81f3+0.49\u3002\u5bf9\u4e8eULS\u6570\u636e\uff0c\u8be5\u9884\u8bbe\u5b9e\u73b0\u4e860.58\u7684F1\u5206\u6570\uff0c\u800c\u539f\u7248\u7b97\u6cd5\u672a\u80fd\u5206\u5272\u4efb\u4f55\u6b63\u786e\u5b9e\u4f8b\u3002\u5bf9\u4e8eTLS\u548cPLS\u6570\u636e\uff0c\u8be5\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u4e0e\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u5728\u5185\u7684\u6700\u65b0\u5f00\u6e90\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7279\u6027\u5339\u914d\uff08\u6811\u5e72\u53ef\u89c1\u5ea6\u9ad8\u3001\u70b9\u5bc6\u5ea6\u8db3\u591f\uff09\u573a\u666f\u4e0b\u7684\u8d44\u6e90\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4e5f\u53ef\u7528\u4e8e\u534a\u81ea\u52a8\u751f\u6210\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6807\u7b7e\u3002\u4e3a\u4fc3\u8fdb\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f5c\u8005\u63d0\u4f9b\u4e86\u5f00\u653e\u6e90\u4ee3\u7801\u7684Python\u5b9e\u73b0\u3002"}}
{"id": "2509.03666", "pdf": "https://arxiv.org/pdf/2509.03666", "abs": "https://arxiv.org/abs/2509.03666", "authors": ["Kenny Guo", "Nicholas Eckhert", "Krish Chhajer", "Luthira Abeykoon", "Lorne Schell"], "title": "AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management", "categories": ["cs.LG"], "comment": "IEEE (International Conference on Smart Energy Grid Engineering\n  (SEGE)) 2025, 6 pages", "summary": "We present a deep reinforcement learning-based framework for autonomous\nmicrogrid management. tailored for remote communities. Using deep reinforcement\nlearning and time-series forecasting models, we optimize microgrid energy\ndispatch strategies to minimize costs and maximize the utilization of renewable\nenergy sources such as solar and wind. Our approach integrates the transformer\narchitecture for forecasting of renewable generation and a proximal-policy\noptimization (PPO) agent to make decisions in a simulated environment. Our\nexperimental results demonstrate significant improvements in both energy\nefficiency and operational resilience when compared to traditional rule-based\nmethods. This work contributes to advancing smart-grid technologies in pursuit\nof zero-carbon energy systems. We finally provide an open-source framework for\nsimulating several microgrid environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u6cbb\u5fae\u7535\u7f51\u7ba1\u7406\u6846\u67b6\uff0c\u4e13\u4e3a\u504f\u8fdc\u793e\u533a\u8bbe\u8ba1\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548cPPO\u667a\u80fd\u4f53\u4f18\u5316\u80fd\u6e90\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u6210\u672c\u5e76\u6700\u5927\u5316\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u80fd\u6548\u548c\u8fd0\u884c\u97e7\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f18\u5316\u504f\u8fdc\u793e\u533a\u5fae\u7535\u7f51\u7684\u80fd\u6e90\u8c03\u5ea6\u7b56\u7565\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u8fd0\u8425\u6210\u672c\u3001\u6700\u5927\u5316\u592a\u9633\u80fd\u548c\u98ce\u80fd\u7b49\u53ef\u518d\u751f\u80fd\u6e90\u7684\u5229\u7528\u7387\uff0c\u5e76\u63a8\u52a8\u667a\u80fd\u7535\u7f51\u6280\u672f\u53d1\u5c55\uff0c\u4ee5\u5b9e\u73b0\u96f6\u78b3\u80fd\u6e90\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e0e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528Transformer\u67b6\u6784\u8fdb\u884c\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u9884\u6d4b\uff0c\u4ee5\u53ca\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u667a\u80fd\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u80fd\u6e90\u6548\u7387\u548c\u8fd0\u884c\u97e7\u6027\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u7535\u7f51\u7ba1\u7406\u95ee\u9898\uff0c\u5bf9\u667a\u80fd\u7535\u7f51\u6280\u672f\u53d1\u5c55\u548c\u96f6\u78b3\u80fd\u6e90\u7cfb\u7edf\u6784\u5efa\u505a\u51fa\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u5fae\u7535\u7f51\u6a21\u62df\u6846\u67b6\u3002"}}
{"id": "2509.03626", "pdf": "https://arxiv.org/pdf/2509.03626", "abs": "https://arxiv.org/abs/2509.03626", "authors": ["Zahra Zehtabi Sabeti Moghaddam", "Zeinab Dehghani", "Maneeha Rani", "Koorosh Aslansefat", "Bhupesh Kumar Mishra", "Rameez Raja Kureshi", "Dhavalkumar Thakker"], "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE", "categories": ["cs.AI"], "comment": null, "summary": "Generative AI, such as Large Language Models (LLMs), has achieved impressive\nprogress but still produces hallucinations and unverifiable claims, limiting\nreliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves\naccuracy by grounding outputs in external knowledge, especially in domains like\nhealthcare, where precision is vital. However, RAG remains opaque and\nessentially a black box, heavily dependent on data quality. We developed a\nmethod-agnostic, perturbation-based framework that provides token and\ncomponent-level interoperability for Graph RAG using SMILE and named it as\nKnowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing\nsimilarities, and training weighted linear surrogates, KG-SMILE identifies the\ngraph entities and relations most influential to generated outputs, thereby\nmaking RAG more transparent. We evaluate KG-SMILE using comprehensive\nattribution metrics, including fidelity, faithfulness, consistency, stability,\nand accuracy. Our findings show that KG-SMILE produces stable, human-aligned\nexplanations, demonstrating its capacity to balance model effectiveness with\ninterpretability and thereby fostering greater transparency and trust in\nmachine learning technologies.", "AI": {"tldr": "KG-SMILE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6270\u52a8\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u77e5\u8bc6\u56fe\u8c31RAG\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u901a\u8fc7\u89e3\u91ca\u751f\u6210\u8f93\u51fa\u4e2d\u6700\u5177\u5f71\u54cd\u529b\u7684\u56fe\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u4fe1\u4efb\u3002", "motivation": "\u5c3d\u7ba1RAG\u6539\u5584\u4e86LLMs\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5176\u4f5c\u4e3a\u201c\u9ed1\u7bb1\u201d\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\uff0c\u5c24\u5176\u5728\u533b\u7597\u7b49\u5bf9\u7cbe\u786e\u5ea6\u8981\u6c42\u9ad8\u7684\u654f\u611f\u9886\u57df\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3RAG\uff08\u7279\u522b\u662f\u56feRAG\uff09\u7684\u8fd9\u4e00\u4e0d\u900f\u660e\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u4fe1\u4efb\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aKG-SMILE\u7684\u3001\u4e0e\u65b9\u6cd5\u65e0\u5173\u7684\u57fa\u4e8e\u6270\u52a8\u6846\u67b6\uff0c\u5229\u7528SMILE\u4e3a\u56feRAG\u63d0\u4f9btoken\u548c\u7ec4\u4ef6\u7ea7\u522b\u7684\u4e92\u64cd\u4f5c\u6027\u3002\u901a\u8fc7\u5e94\u7528\u53d7\u63a7\u6270\u52a8\u3001\u8ba1\u7b97\u76f8\u4f3c\u6027\u5e76\u8bad\u7ec3\u52a0\u6743\u7ebf\u6027\u66ff\u4ee3\u6a21\u578b\uff0cKG-SMILE\u8bc6\u522b\u5bf9\u751f\u6210\u8f93\u51fa\u5f71\u54cd\u6700\u5927\u7684\u56fe\u5b9e\u4f53\u548c\u5173\u7cfb\u3002\u901a\u8fc7\u5fe0\u5b9e\u5ea6\u3001\u771f\u5b9e\u6027\u3001\u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u7b49\u7efc\u5408\u5f52\u56e0\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "KG-SMILE\u80fd\u591f\u751f\u6210\u7a33\u5b9a\u4e14\u4e0e\u4eba\u7c7b\u7406\u89e3\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u8868\u660e\u5176\u5728\u6a21\u578b\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "KG-SMILE\u901a\u8fc7\u63d0\u9ad8RAG\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u66f4\u5927\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2509.03535", "pdf": "https://arxiv.org/pdf/2509.03535", "abs": "https://arxiv.org/abs/2509.03535", "authors": ["Ahmed Mubarak", "Amna Ahmed", "Amira Nasser", "Aya Mohamed", "Fares El-Sadek", "Mohammed Ahmed", "Ahmed Salah", "Youssef Sobhy"], "title": "QuesGenie: Intelligent Multimodal Question Generation", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 8 figures, 12 tables. Supervised by Dr. Ahmed Salah and TA\n  Youssef Sobhy", "summary": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u95ee\u9898\u751f\u6210\u7cfb\u7edf\uff0c\u5229\u7528RLHF\u4ece\u591a\u6e90\u5185\u5bb9\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u4f9b\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u4e14\u667a\u80fd\u7684\u7ec3\u4e60\u6750\u6599\u3002", "motivation": "\u5728\u4fe1\u606f\u4e30\u5bcc\u7684\u65f6\u4ee3\uff0c\u5b66\u4e60\u8005\u867d\u80fd\u83b7\u53d6\u5927\u91cf\u6559\u80b2\u8d44\u6e90\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e9b\u8d44\u6e90\u91cf\u8eab\u5b9a\u5236\u7684\u7ec3\u4e60\u6750\u6599\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u95ee\u9898\u751f\u6210\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u4ece\u591a\u79cd\u5185\u5bb9\u683c\u5f0f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\u3002\u5176\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u3001\u95ee\u9898\u751f\u6210\u3001\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4ee5\u53ca\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u4ea4\u4e92\u5f0f\u754c\u9762\u3002", "result": "\u8be5\u9879\u76ee\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5e73\u8861\u8d44\u6e90\u6548\u7387\u3001\u529f\u80fd\u9c81\u68d2\u6027\u548c\u6d41\u7545\u7528\u6237\u4f53\u9a8c\u7684\u591a\u6a21\u6001\u95ee\u9898\u751f\u6210\u7cfb\u7edf\uff0c\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u548c\u667a\u80fd\u7684\u95ee\u9898\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u548c\u667a\u80fd\u7684\u95ee\u9898\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec3\u4e60\u6750\u6599\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5728\u8d44\u6e90\u6548\u7387\u3001\u529f\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.03635", "pdf": "https://arxiv.org/pdf/2509.03635", "abs": "https://arxiv.org/abs/2509.03635", "authors": ["Hongpei Zheng", "Lintao Xiang", "Qijun Yang", "Qian Lin", "Hujun Yin"], "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding", "categories": ["cs.CV"], "comment": "16 pages, 6 figures", "summary": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Reg3D\uff0c\u4e00\u4e2a\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u76d1\u7763\u548c\u91cd\u6784\u4efb\u52a1\u6765\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u51e0\u4f55\u7ea6\u675f\u7f3a\u5931\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u57282D\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u6269\u5c55\u52303D\u573a\u666f\u7406\u89e3\u4ecd\u662f\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u76d1\u7763\uff0c\u7f3a\u4e4f\u5b66\u4e60\u9c81\u68d23D\u7a7a\u95f4\u8868\u793a\u6240\u9700\u7684\u51e0\u4f55\u7ea6\u675f\u3002", "method": "\u5f15\u5165Reg3D\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u91cd\u6784\u51e0\u4f55\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\u3002\u5176\u6838\u5fc3\u662f\u901a\u8fc7\u91cd\u5efa\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u800c\u975e\u4ec5\u63cf\u8ff0\u6765\u589e\u5f3a3D\u7406\u89e3\u3002Reg3D\u91c7\u7528\u53cc\u91cd\u76d1\u7763\u8303\u5f0f\uff0c\u5c063D\u51e0\u4f55\u4fe1\u606f\u540c\u65f6\u4f5c\u4e3a\u8f93\u5165\u548c\u663e\u5f0f\u5b66\u4e60\u76ee\u6807\u3002\u5177\u4f53\u8bbe\u8ba1\u4e86\u4e92\u8865\u7684\u7269\u4f53\u7ea7\u548c\u5e27\u7ea7\u91cd\u5efa\u4efb\u52a1\uff0c\u5e76\u5728\u53cc\u7f16\u7801\u5668\u67b6\u6784\u4e2d\u5f3a\u5236\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4ee5\u53d1\u5c55\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728ScanQA\u3001Scan2Cap\u3001ScanRefer\u548cSQA3D\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cReg3D\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Reg3D\u4e3a\u7a7a\u95f4\u611f\u77e5\u7684\u591a\u6a21\u6001\u6a21\u578b\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.03672", "pdf": "https://arxiv.org/pdf/2509.03672", "abs": "https://arxiv.org/abs/2509.03672", "authors": ["Arpan Mukherjee", "Marcello Bullo", "Deniz G\u00fcnd\u00fcz"], "title": "SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Uniform-reward reinforcement learning from human feedback (RLHF), which\ntrains a single reward model to represent the preferences of all annotators,\nfails to capture the diversity of opinions across sub-populations,\ninadvertently favoring dominant groups. The state-of-the-art, MaxMin-RLHF,\naddresses this by learning group-specific reward models, and by optimizing for\nthe group receiving the minimum reward, thereby promoting fairness. However, we\nidentify that a key limitation of MaxMin-RLHF is its poor performance when the\nminimum-reward group is a minority. To mitigate this drawback, we introduce a\nnovel framework, termed {\\em SharedRep-RLHF}. At its core, SharedRep-RLHF\nlearns and leverages {\\em shared traits} in annotations among various groups,\nin contrast to learning separate reward models across groups. We first show\nthat MaxMin-RLHF is provably suboptimal in learning shared traits, and then\nquantify the sample complexity of SharedRep-RLHF. Experiments across diverse\nnatural language tasks showcase the effectiveness of SharedRep-RLHF compared to\nMaxMin-RLHF with a gain of up to 20% in win rate.", "AI": {"tldr": "\u4f20\u7edf\u7684RLHF\u672a\u80fd\u6355\u83b7\u7fa4\u4f53\u504f\u597d\u591a\u6837\u6027\u3002MaxMin-RLHF\u901a\u8fc7\u5173\u6ce8\u6700\u5c0f\u5956\u52b1\u7ec4\u63d0\u5347\u516c\u5e73\u6027\uff0c\u4f46\u5728\u6700\u5c0f\u5956\u52b1\u7ec4\u4e3a\u5c11\u6570\u7fa4\u4f53\u65f6\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51faSharedRep-RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5e76\u5229\u7528\u7fa4\u4f53\u95f4\u5171\u4eab\u7279\u5f81\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7edf\u4e00\u5956\u52b1\u7684RLHF\u672a\u80fd\u6355\u83b7\u4e0d\u540c\u7fa4\u4f53\u7684\u610f\u89c1\u591a\u6837\u6027\u5e76\u504f\u8892\u591a\u6570\u7fa4\u4f53\u3002MaxMin-RLHF\u867d\u7136\u901a\u8fc7\u4f18\u5316\u6700\u5c0f\u5956\u52b1\u7ec4\u63d0\u5347\u4e86\u516c\u5e73\u6027\uff0c\u4f46\u5f53\u6700\u5c0f\u5956\u52b1\u7ec4\u4e3a\u5c11\u6570\u7fa4\u4f53\u65f6\uff0c\u5176\u6027\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SharedRep-RLHF\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u5b66\u4e60\u5e76\u5229\u7528\u5404\u7fa4\u4f53\u6ce8\u91ca\u4e2d\u7684\u201c\u5171\u4eab\u7279\u5f81\u201d\uff0c\u800c\u975e\u4e3a\u6bcf\u4e2a\u7fa4\u4f53\u5355\u72ec\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u3002\u7814\u7a76\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86MaxMin-RLHF\u5728\u5b66\u4e60\u5171\u4eab\u7279\u5f81\u65b9\u9762\u7684\u6b21\u4f18\u6027\uff0c\u5e76\u91cf\u5316\u4e86SharedRep-RLHF\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "SharedRep-RLHF\u5728\u591a\u6837\u5316\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6bd4MaxMin-RLHF\u66f4\u9ad8\u7684\u6548\u7387\u3002\u4e0eMaxMin-RLHF\u76f8\u6bd4\uff0cSharedRep-RLHF\u7684\u80dc\u7387\uff08win rate\uff09\u6700\u9ad8\u63d0\u5347\u4e8620%\u3002", "conclusion": "SharedRep-RLHF\u901a\u8fc7\u6709\u6548\u5229\u7528\u7fa4\u4f53\u95f4\u5171\u4eab\u7279\u5f81\uff0c\u6210\u529f\u514b\u670d\u4e86MaxMin-RLHF\u5728\u5904\u7406\u5c11\u6570\u6700\u5c0f\u5956\u52b1\u7ec4\u65f6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u516c\u5e73\u6027\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03636", "pdf": "https://arxiv.org/pdf/2509.03636", "abs": "https://arxiv.org/abs/2509.03636", "authors": ["Jacqueline Maasch", "John Kalantari", "Kia Khezeli"], "title": "CausalARC: Abstract Reasoning with Causal World Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning requires adaptation to novel problem settings under limited data\nand distribution shift. This work introduces CausalARC: an experimental testbed\nfor AI reasoning in low-data and out-of-distribution regimes, modeled after the\nAbstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is\nsampled from a fully specified causal world model, formally expressed as a\nstructural causal model. Principled data augmentations provide observational,\ninterventional, and counterfactual feedback about the world model in the form\nof few-shot, in-context learning demonstrations. As a proof-of-concept, we\nillustrate the use of CausalARC for four language model evaluation settings:\n(1) abstract reasoning with test-time training, (2) counterfactual reasoning\nwith in-context learning, (3) program synthesis, and (4) causal discovery with\nlogical reasoning.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165CausalARC\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56e0\u679c\u4e16\u754c\u6a21\u578b\u7684AI\u63a8\u7406\u65b0\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u6570\u636e\u548c\u57df\u5916\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u6f14\u793a\u4e86\u5176\u5728\u8bed\u8a00\u6a21\u578b\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709AI\u63a8\u7406\u5728\u6709\u9650\u6570\u636e\u548c\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u9896\u95ee\u9898\u8bbe\u7f6e\u3002", "method": "\u672c\u6587\u5f15\u5165CausalARC\uff0c\u4e00\u4e2a\u501f\u9274\u62bd\u8c61\u63a8\u7406\u8bed\u6599\u5e93(ARC)\u6784\u5efa\u7684\u5b9e\u9a8c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u4f4e\u6570\u636e\u548c\u57df\u5916AI\u63a8\u7406\u3002\u5176\u4efb\u52a1\u5747\u6e90\u4e8e\u7ed3\u6784\u5316\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u539f\u5219\u6027\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u89c2\u6d4b\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u53cd\u9988\uff0c\u4ee5\u5c11\u91cf\u6837\u672c\u548c\u60c5\u5883\u5b66\u4e60\u6f14\u793a\u5f62\u5f0f\u5448\u73b0\u3002", "result": "\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u672c\u6587\u5c55\u793a\u4e86CausalARC\u5728\u56db\u79cd\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u7684\u5e94\u7528\uff1a\u5305\u62ec\u5e26\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u7684\u62bd\u8c61\u63a8\u7406\u3001\u5e26\u60c5\u5883\u5b66\u4e60\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u7a0b\u5e8f\u5408\u6210\uff0c\u4ee5\u53ca\u5e26\u903b\u8f91\u63a8\u7406\u7684\u56e0\u679c\u53d1\u73b0\u3002", "conclusion": "CausalARC\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347AI\uff08\u7279\u522b\u662f\u8bed\u8a00\u6a21\u578b\uff09\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u6570\u636e\u548c\u57df\u5916\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5176\u6548\u7528\u5df2\u5728\u591a\u79cd\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5f97\u5230\u8bc1\u660e\u3002"}}
{"id": "2509.03537", "pdf": "https://arxiv.org/pdf/2509.03537", "abs": "https://arxiv.org/abs/2509.03537", "authors": ["Cheng-Kai Yeh", "Hsing-Wang Lee", "Chung-Hung Kuo", "Hen-Hsen Huang"], "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "7 pages, accepted by CIKM 2025 as a short paper", "summary": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAR$^2$\uff08Adversarial Reinforcement Learning for Abstract Reasoning\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u5f0f\u5f3a\u5316\u5b66\u4e60\u660e\u786e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u62bd\u8c61\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728\u672a\u89c1\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\uff0c\u5ffd\u89c6\u4e86\u5bf9\u62bd\u8c61\u80fd\u529b\u7684\u663e\u5f0f\u8bad\u7ec3\uff0c\u800c\u62bd\u8c61\u80fd\u529b\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548cLLM\u95ee\u9898\u89e3\u51b3\u7684\u5173\u952e\u57fa\u7840\u6280\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51faAR$^2$\u6846\u67b6\uff0c\u5229\u7528\u4e00\u4e2a\u6559\u5e08\u6a21\u578b\u5c06\u6838\u5fc3\u95ee\u9898\u8f6c\u5316\u4e3a\u590d\u6742\u3001\u53d9\u8ff0\u4e30\u5bcc\u7684\u63cf\u8ff0\uff0c\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2a\u5b66\u751f\u7f16\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u53d6\u5e95\u5c42\u8ba1\u7b97\u5185\u6838\u6765\u89e3\u51b3\u8fd9\u4e9b\u590d\u6742\u7684\u53d9\u8ff0\u6027\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5176\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAR$^2$\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u6a21\u578b\u5728\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u62bd\u8c61\u80fd\u529b\u662f\u589e\u5f3aLLM\u6cdb\u5316\u6027\u7684\u5173\u952e\u6280\u80fd\u3002"}}
{"id": "2509.03704", "pdf": "https://arxiv.org/pdf/2509.03704", "abs": "https://arxiv.org/abs/2509.03704", "authors": ["Seth Z. Zhao", "Huizhi Zhang", "Zhaowei Li", "Juntong Peng", "Anthony Chui", "Zewei Zhou", "Zonglin Meng", "Hao Xiang", "Zhiyu Huang", "Fujia Wang", "Ran Tian", "Chenfeng Xu", "Bolei Zhou", "Jiaqi Ma"], "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QuantV2X\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u91cf\u5316\u591a\u667a\u80fd\u4f53V2X\u534f\u540c\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u91cf\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u4f20\u8f93\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709V2X\u534f\u540c\u611f\u77e5\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7cbe\u5ea6\uff0c\u5374\u5ffd\u7565\u4e86\u6548\u7387\u3001\u5ef6\u8fdf\u548c\u5b9e\u9645\u90e8\u7f72\u6027\u7b49\u7cfb\u7edf\u7ea7\u5173\u952e\u8003\u91cf\u3002\u5f53\u524d\u7cfb\u7edf\u591a\u4f9d\u8d56\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u4f20\u8f93\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u5f15\u5165QuantV2X\uff0c\u8be5\u7cfb\u7edf\u9488\u5bf9\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53V2X\u534f\u540c\u611f\u77e5\uff0c\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u91cf\u5316\u7b56\u7565\uff0c\u540c\u65f6\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c\u4f20\u8f93\u6d88\u606f\u8868\u793a\uff0c\u4ee5\u540c\u6b65\u51cf\u5c11\u8ba1\u7b97\u8d1f\u8f7d\u548c\u4f20\u8f93\u5e26\u5bbd\u3002", "result": "QuantV2X\u5728\u4f4e\u6bd4\u7279\u7ea6\u675f\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5168\u7cbe\u5ea6\u7cfb\u7edf\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002\u5728\u9762\u5411\u90e8\u7f72\u7684\u6307\u6807\u8bc4\u4f30\u4e0b\uff0c\u7cfb\u7edf\u7ea7\u5ef6\u8fdf\u964d\u4f4e\u4e863.2\u500d\uff0cmAP30\u76f8\u6bd4\u5168\u7cbe\u5ea6\u57fa\u7ebf\u63d0\u5347\u4e869.5\u3002\u6b64\u5916\uff0cQuantV2X\u6269\u5c55\u6027\u66f4\u5f3a\uff0c\u80fd\u4f7f\u66f4\u5927\u3001\u66f4\u5f3a\u7684\u6a21\u578b\u5728\u4e25\u683c\u7684\u5185\u5b58\u9884\u7b97\u5185\u8fd0\u884c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5168\u91cf\u5316\u591a\u667a\u80fd\u4f53\u4e2d\u95f4\u878d\u5408\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u4e3aV2X\u534f\u540c\u611f\u77e5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u7cfb\u7edf\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2509.03673", "pdf": "https://arxiv.org/pdf/2509.03673", "abs": "https://arxiv.org/abs/2509.03673", "authors": ["Hang Wang", "Huijie Tang", "Ningai Leng", "Zhoufan Yu"], "title": "A Machine Learning-Based Study on the Synergistic Optimization of Supply Chain Management and Financial Supply Chains from an Economic Perspective", "categories": ["cs.LG"], "comment": "Accepted by the 2025 IEEE 8th International Conference on Information\n  Systems and Computer Aided Education (ICISCAE 2025)", "summary": "Based on economic theories and integrated with machine learning technology,\nthis study explores a collaborative Supply Chain Management and Financial\nSupply Chain Management (SCM - FSCM) model to solve issues like efficiency\nloss, financing constraints, and risk transmission. We combine Transaction Cost\nand Information Asymmetry theories and use algorithms such as random forests to\nprocess multi-dimensional data and build a data-driven, three-dimensional\n(cost-efficiency-risk) analysis framework. We then apply an FSCM model of \"core\nenterprise credit empowerment plus dynamic pledge financing.\" We use Long\nShort-Term Memory (LSTM) networks for demand forecasting and\nclustering/regression algorithms for benefit allocation. The study also\ncombines Game Theory and reinforcement learning to optimize the\ninventory-procurement mechanism and uses eXtreme Gradient Boosting (XGBoost)\nfor credit assessment to enable rapid monetization of inventory. Verified with\n20 core and 100 supporting enterprises, the results show a 30\\% increase in\ninventory turnover, an 18\\%-22\\% decrease in SME financing costs, a stable\norder fulfillment rate above 95\\%, and excellent model performance (demand\nforecasting error <= 8\\%, credit assessment accuracy >= 90\\%). This SCM-FSCM\nmodel effectively reduces operating costs, alleviates financing constraints,\nand supports high-quality supply chain development.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u7ecf\u6d4e\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u4f5c\u5f0f\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e0e\u91d1\u878d\u4f9b\u5e94\u94fe\u7ba1\u7406\uff08SCM-FSCM\uff09\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u6548\u7387\u635f\u5931\u3001\u878d\u8d44\u7ea6\u675f\u548c\u98ce\u9669\u4f20\u5bfc\u7b49\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u548c\u4f9b\u5e94\u94fe\u9ad8\u8d28\u91cf\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u4f9b\u5e94\u94fe\u4e2d\u5b58\u5728\u7684\u6548\u7387\u635f\u5931\u3001\u878d\u8d44\u7ea6\u675f\u548c\u98ce\u9669\u4f20\u5bfc\u7b49\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u4f9b\u5e94\u94fe\u7684\u8fd0\u8425\u6548\u7387\u548c\u91d1\u878d\u652f\u6301\u80fd\u529b\u3002", "method": "\u6574\u5408\u4ea4\u6613\u6210\u672c\u7406\u8bba\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7406\u8bba\uff0c\u5229\u7528\u968f\u673a\u68ee\u6797\u7b49\u7b97\u6cd5\u5904\u7406\u591a\u7ef4\u6570\u636e\u5e76\u6784\u5efa\u6210\u672c-\u6548\u7387-\u98ce\u9669\u4e09\u7ef4\u5206\u6790\u6846\u67b6\u3002\u5e94\u7528\u201c\u6838\u5fc3\u4f01\u4e1a\u4fe1\u7528\u8d4b\u80fd+\u52a8\u6001\u8d28\u62bc\u878d\u8d44\u201d\u7684FSCM\u6a21\u578b\u3002\u91c7\u7528LSTM\u8fdb\u884c\u9700\u6c42\u9884\u6d4b\uff0c\u805a\u7c7b/\u56de\u5f52\u7b97\u6cd5\u8fdb\u884c\u6536\u76ca\u5206\u914d\u3002\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5e93\u5b58\u91c7\u8d2d\u673a\u5236\uff0c\u5e76\u4f7f\u7528XGBoost\u8fdb\u884c\u4fe1\u7528\u8bc4\u4f30\u3002", "result": "\u7ecf20\u5bb6\u6838\u5fc3\u4f01\u4e1a\u548c100\u5bb6\u914d\u5957\u4f01\u4e1a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5e93\u5b58\u5468\u8f6c\u7387\u63d0\u534730%\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u878d\u8d44\u6210\u672c\u964d\u4f4e18%-22%\uff0c\u8ba2\u5355\u5c65\u884c\u7387\u7a33\u5b9a\u572895%\u4ee5\u4e0a\u3002\u6a21\u578b\u6027\u80fd\u4f18\u5f02\uff0c\u5176\u4e2d\u9700\u6c42\u9884\u6d4b\u8bef\u5dee\u4e0d\u9ad8\u4e8e8%\uff0c\u4fe1\u7528\u8bc4\u4f30\u51c6\u786e\u7387\u4e0d\u4f4e\u4e8e90%\u3002", "conclusion": "\u8be5SCM-FSCM\u6a21\u578b\u80fd\u6709\u6548\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u7f13\u89e3\u878d\u8d44\u7ea6\u675f\uff0c\u5e76\u652f\u6301\u4f9b\u5e94\u94fe\u9ad8\u8d28\u91cf\u53d1\u5c55\u3002"}}
{"id": "2509.03644", "pdf": "https://arxiv.org/pdf/2509.03644", "abs": "https://arxiv.org/abs/2509.03644", "authors": ["Fran\u00e7ois Olivier", "Zied Bouraoui"], "title": "Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations", "categories": ["cs.AI", "cs.CL"], "comment": "To appear in Proceedings of Machine Learning Research, 19th\n  Conference on Neurosymbolic Learning and Reasoning, 2025", "summary": "Despite significant progress in natural language understanding, Large\nLanguage Models (LLMs) remain error-prone when performing logical reasoning,\noften lacking the robust mental representations that enable human-like\ncomprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that\ngrounds understanding and logical reasoning in schematic representations based\non image schemas-recurring patterns derived from sensorimotor experience that\nstructure human cognition. Our system operationalizes the spatial foundations\nof these cognitive structures using declarative spatial reasoning within Answer\nSet Programming. Through evaluation on logical deduction problems, we\ndemonstrate that LLMs can be guided to interpret scenarios through embodied\ncognitive structures, that these structures can be formalized as executable\nprograms, and that the resulting representations support effective logical\nreasoning with enhanced interpretability. While our current implementation\nfocuses on spatial primitives, it establishes the computational foundation for\nincorporating more complex and dynamic representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edfEmbodied-LM\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u56fe\u50cf\u6a21\u5f0f\u7684\u793a\u610f\u6027\u8868\u5f81\u548cAnswer Set Programming\u4e2d\u7684\u58f0\u660e\u5f0f\u7a7a\u95f4\u63a8\u7406\uff0c\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7f3a\u4e4f\u7c7b\u4eba\u8ba4\u77e5\u8868\u5f81\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6267\u884c\u903b\u8f91\u63a8\u7406\u65f6\u4ecd\u7136\u5bb9\u6613\u51fa\u9519\uff0c\u5e76\u4e14\u901a\u5e38\u7f3a\u4e4f\u652f\u6301\u7c7b\u4eba\u7406\u89e3\u7684\u7a33\u5065\u5fc3\u7406\u8868\u5f81\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u539f\u578b\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edfEmbodied-LM\uff0c\u8be5\u7cfb\u7edf\u5c06\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\u57fa\u4e8e\u56fe\u50cf\u6a21\u5f0f\uff08\u6e90\u4e8e\u611f\u89c9\u8fd0\u52a8\u7ecf\u9a8c\u5e76\u6784\u5efa\u4eba\u7c7b\u8ba4\u77e5\u7684\u91cd\u590d\u6a21\u5f0f\uff09\u7684\u793a\u610f\u6027\u8868\u5f81\u3002\u7cfb\u7edf\u901a\u8fc7\u5728Answer Set Programming\u4e2d\u8fd0\u7528\u58f0\u660e\u5f0f\u7a7a\u95f4\u63a8\u7406\uff0c\u6765\u64cd\u4f5c\u8fd9\u4e9b\u8ba4\u77e5\u7ed3\u6784\u7684\u7a7a\u95f4\u57fa\u7840\u3002", "result": "\u901a\u8fc7\u5bf9\u903b\u8f91\u6f14\u7ece\u95ee\u9898\u7684\u8bc4\u4f30\uff0c\u7814\u7a76\u8868\u660eLLMs\u53ef\u4ee5\u901a\u8fc7\u5177\u8eab\u8ba4\u77e5\u7ed3\u6784\u6765\u89e3\u91ca\u573a\u666f\uff0c\u8fd9\u4e9b\u7ed3\u6784\u53ef\u4ee5\u88ab\u5f62\u5f0f\u5316\u4e3a\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u4e14\u7531\u6b64\u4ea7\u751f\u7684\u8868\u5f81\u80fd\u591f\u6709\u6548\u652f\u6301\u903b\u8f91\u63a8\u7406\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Embodied-LM\u901a\u8fc7\u5177\u8eab\u8ba4\u77e5\u7ed3\u6784\u4e3aLLMs\u7684\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5c3d\u7ba1\u5f53\u524d\u5b9e\u73b0\u96c6\u4e2d\u4e8e\u7a7a\u95f4\u539f\u8bed\uff0c\u4f46\u5b83\u4e3a\u6574\u5408\u66f4\u590d\u6742\u548c\u52a8\u6001\u7684\u8868\u5f81\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.03540", "pdf": "https://arxiv.org/pdf/2509.03540", "abs": "https://arxiv.org/abs/2509.03540", "authors": ["Shanglin Wu", "Lihui Liu", "Jinho D. Choi", "Kai Shu"], "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u52a8\u6001\u6784\u5efa\u548c\u6269\u5c55\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408LLM\u5185\u90e8\u77e5\u8bc6\u548c\u5916\u90e8\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u7b54\u6848\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u7b54\u6848\u65f6\u5e38\u56e0\u53c2\u6570\u8bb0\u5fc6\u9650\u5236\u800c\u51fa\u73b0\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u867d\u7136\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u6709\u6240\u5e2e\u52a9\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5c06\u77e5\u8bc6\u89c6\u4e3a\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u652f\u6301\u7ec4\u5408\u63a8\u7406\u548c\u8bc6\u522b\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6784\u5efa\u548c\u6269\u5c55\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a\u9996\u5148\u901a\u8fc7\u63d0\u793a\u4ece\u95ee\u9898\u4e2d\u63d0\u53d6\u4e00\u4e2a\u79cd\u5b50KG\uff1b\u7136\u540e\u5229\u7528LLM\u7684\u6f5c\u5728\u77e5\u8bc6\u8fdb\u884c\u8fed\u4ee3\u6269\u5c55\uff1b\u6700\u540e\u901a\u8fc7\u5916\u90e8\u68c0\u7d22\u5bf9\u56fe\u8c31\u8fdb\u884c\u9009\u62e9\u6027\u7cbe\u70bc\uff0c\u4ee5\u589e\u5f3a\u4e8b\u5b9e\u8986\u76d6\u5e76\u7ea0\u6b63\u4e0d\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u4e8b\u5b9e\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u63d0\u793a\u548c\u9759\u6001KG\u589e\u5f3a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u7b54\u6848\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u5c55\u73b0\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u63a8\u7406\u65f6\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u662f\u589e\u5f3aLLM\u4e8b\u5b9e\u6027\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5b83\u4ee5\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002"}}
{"id": "2509.03729", "pdf": "https://arxiv.org/pdf/2509.03729", "abs": "https://arxiv.org/abs/2509.03729", "authors": ["Bandita Bharadwaj", "Ankur Mishra", "Saurav Bharadwaj"], "title": "Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns", "categories": ["cs.CV"], "comment": null, "summary": "This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86ResNet50\u3001MobileNetV2\u548cEfficientNetB0\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u57fa\u4e8e\u53f6\u8109\u6a21\u5f0f\u7684\u690d\u7269\u7269\u79cd\u81ea\u52a8\u5206\u7c7b\u4e2d\u7684\u6548\u679c\u3002EfficientNetB0\u8868\u73b0\u6700\u4f73\uff08\u6d4b\u8bd5\u51c6\u786e\u738794.67%\uff09\uff0c\u7a81\u663e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u690d\u7269\u5206\u7c7b\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u57fa\u4e8e\u5177\u6709\u9ad8\u5206\u7c7b\u5b66\u76f8\u5173\u6027\u7684\u53f6\u8109\u6a21\u5f0f\uff0c\u5b9e\u73b0\u690d\u7269\u7269\u79cd\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u3002\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6548\u7528\u3002", "method": "\u8bc4\u4f30\u4e86ResNet50\u3001MobileNetV2\u548cEfficientNetB0\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u4f7f\u7528\u5305\u542b15\u79cd\u690d\u7269\uff08\u51711125\u5f20\u56fe\u50cf\uff09\u7684\u745e\u5178\u53f6\u7247\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u7684\u6807\u51c6\u6027\u80fd\u6307\u6807\uff08\u51c6\u786e\u7387\u3001F1\u5206\u6570\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\uff09\u6765\u8861\u91cf\u6a21\u578b\u8868\u73b0\u3002", "result": "ResNet50\u8bad\u7ec3\u51c6\u786e\u738794.11%\u4f46\u51fa\u73b0\u8fc7\u62df\u5408\uff0c\u6d4b\u8bd5\u51c6\u786e\u738788.45%\uff0cF1\u5206\u657087.82%\u3002MobileNetV2\u6cdb\u5316\u80fd\u529b\u66f4\u4f73\uff0c\u6d4b\u8bd5\u51c6\u786e\u738793.34%\uff0cF1\u5206\u657093.23%\uff0c\u9002\u7528\u4e8e\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u5e94\u7528\u3002EfficientNetB0\u8868\u73b0\u6700\u4f18\uff0c\u6d4b\u8bd5\u51c6\u786e\u738794.67%\uff0c\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc794.6%\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\uff08\u7279\u522b\u662fEfficientNetB0\uff09\u5728\u5229\u7528\u53f6\u8109\u7279\u5f81\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u690d\u7269\u5206\u7c7b\u5de5\u5177\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.03677", "pdf": "https://arxiv.org/pdf/2509.03677", "abs": "https://arxiv.org/abs/2509.03677", "authors": ["Vincent-Daniel Yun"], "title": "Insights from Gradient Dynamics: Gradient Autoscaled Normalization", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Gradient dynamics play a central role in determining the stability and\ngeneralization of deep neural networks. In this work, we provide an empirical\nanalysis of how variance and standard deviation of gradients evolve during\ntraining, showing consistent changes across layers and at the global scale in\nconvolutional networks. Motivated by these observations, we propose a\nhyperparameter-free gradient normalization method that aligns gradient scaling\nwith their natural evolution. This approach prevents unintended amplification,\nstabilizes optimization, and preserves convergence guarantees. Experiments on\nthe challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN\ndemonstrate that our method maintains or improves test accuracy even under\nstrong generalization. Beyond practical performance, our study highlights the\nimportance of directly tracking gradient dynamics, aiming to bridge the gap\nbetween theoretical expectations and empirical behaviors, and to provide\ninsights for future optimization research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u68af\u5ea6\u52a8\u6001\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u68af\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u80fd\u7a33\u5b9a\u4f18\u5316\u5e76\u63d0\u9ad8\u6df1\u5ea6\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89c2\u5bdf\u5230\u68af\u5ea6\u65b9\u5dee\u548c\u6807\u51c6\u5dee\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u6301\u7eed\u4e14\u4e00\u81f4\u7684\u6f14\u53d8\u6a21\u5f0f\uff0c\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u52a8\u6001\u6765\u9632\u6b62\u68af\u5ea6\u4e0d\u5f53\u653e\u5927\uff0c\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u63d0\u9ad8\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u9996\u5148\u5bf9\u5377\u79ef\u7f51\u7edc\u4e2d\u68af\u5ea6\u65b9\u5dee\u548c\u6807\u51c6\u5dee\u7684\u6f14\u53d8\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u68af\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u68af\u5ea6\u7f29\u653e\u4e0e\u5b83\u4eec\u7684\u81ea\u7136\u6f14\u53d8\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9632\u6b62\u68af\u5ea6\u610f\u5916\u653e\u5927\uff0c\u7a33\u5b9a\u4f18\u5316\uff0c\u5e76\u4fdd\u7559\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u5728CIFAR-100\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff08\u4f7f\u7528ResNet-20, ResNet-56, VGG-16-BN\uff09\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f3a\u6cdb\u5316\u80fd\u529b\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6216\u63d0\u9ad8\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u76f4\u63a5\u8ddf\u8e2a\u68af\u5ea6\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u7406\u8bba\u4e0e\u7ecf\u9a8c\u884c\u4e3a\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u7f51\u7edc\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03646", "pdf": "https://arxiv.org/pdf/2509.03646", "abs": "https://arxiv.org/abs/2509.03646", "authors": ["Haozhe Wang", "Qixin Xu", "Che Liu", "Junhong Wu", "Fangzhen Lin", "Wenhu Chen"], "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u7684\u673a\u5236\u4e0d\u900f\u660e\u3002\u672c\u7814\u7a76\u63ed\u793a\u4e86LLM\u5b66\u4e60\u5b58\u5728\u63a8\u7406\u5c42\u7ea7\u548c\u4e24\u9636\u6bb5\u52a8\u6001\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faHICRA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f18\u5316\u96c6\u4e2d\u5728\u9ad8\u5f71\u54cd\u7684\u89c4\u5212\u4ee4\u724c\u4e0a\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u3002\u540c\u65f6\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u71b5\u662f\u8861\u91cf\u6218\u7565\u63a2\u7d22\u7684\u4f18\u8d8a\u6307\u6807\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u6709\u6548\u63d0\u5347LLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\uff08\u5982\u201c\u987f\u609f\u65f6\u523b\u201d\u3001\u957f\u5ea6\u6269\u5c55\u7b49\uff09\u4ecd\u4e0d\u660e\u786e\u3002\u73b0\u6709RL\u7b97\u6cd5\uff08\u5982GRPO\uff09\u4f18\u5316\u538b\u529b\u5206\u6563\uff0c\u5b66\u4e60\u4fe1\u53f7\u7a00\u91ca\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u5b66\u4e60\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u5c42\u7ea7\uff08\u9ad8\u5c42\u6218\u7565\u89c4\u5212\u4e0e\u4f4e\u5c42\u7a0b\u5e8f\u6267\u884c\uff09\u548c\u4e24\u9636\u6bb5\u52a8\u6001\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u201c\u5c42\u7ea7\u611f\u77e5\u4fe1\u7528\u5206\u914d\u201d\uff08HICRA\uff09\u7b97\u6cd5\uff0c\u65e8\u5728\u5c06\u4f18\u5316\u7cbe\u529b\u96c6\u4e2d\u5728\u9ad8\u5f71\u54cd\u529b\u7684\u89c4\u5212\u4ee4\u724c\u4e0a\u3002\u6b64\u5916\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u71b5\u4f5c\u4e3a\u8861\u91cf\u6218\u7565\u63a2\u7d22\u7684\u4f18\u8d8a\u6027\u3002", "result": "\u63ed\u793a\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4e2d\u5b58\u5728\u4ece\u7a0b\u5e8f\u6b63\u786e\u6027\u5230\u9ad8\u5c42\u6218\u7565\u89c4\u5212\u7684\u4e8c\u9636\u6bb5\u5b66\u4e60\u52a8\u6001\u548c\u74f6\u9888\u8f6c\u79fb\u3002\u6240\u63d0\u51fa\u7684HICRA\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u7b97\u6cd5\u3002\u8bc1\u660e\u4e86\u5173\u6ce8\u6218\u7565\u74f6\u9888\u662f\u89e3\u9501\u9ad8\u7ea7\u63a8\u7406\u7684\u5173\u952e\u3002\u6b64\u5916\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u71b5\u662f\u8861\u91cf\u6218\u7565\u63a2\u7d22\u7684\u66f4\u4f18\u6307\u6807\u3002", "conclusion": "LLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347\u9075\u5faa\u5c42\u7ea7\u52a8\u6001\uff0c\u4ece\u4f4e\u7ea7\u6280\u80fd\u5230\u9ad8\u7ea7\u6218\u7565\u89c4\u5212\u3002\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u5730\u5c06\u4f18\u5316\u8d44\u6e90\u5206\u914d\u7ed9\u9ad8\u5f71\u54cd\u529b\u7684\u89c4\u5212\u4ee4\u724c\uff08\u5982HICRA\u6240\u793a\uff09\uff0c\u80fd\u591f\u6709\u6548\u7a81\u7834\u73b0\u6709RL\u7b97\u6cd5\u7684\u6548\u7387\u74f6\u9888\uff0c\u89e3\u9501LLMs\u7684\u66f4\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002\u8bed\u4e49\u71b5\u662f\u8861\u91cf\u6218\u7565\u63a2\u7d22\u7684\u66f4\u6709\u6548\u6307\u6807\u3002"}}
{"id": "2509.03565", "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "categories": ["cs.CL", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u9879\u540d\u4e3a\u201c\u591a\u6587\u6863\u79d1\u5b66\u63a8\u7406\u201d\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u8de8\u6587\u6863\u63d0\u53d6\u548c\u5bf9\u9f50\u4fe1\u606f\u6765\u91cd\u5efa\u7814\u7a76\u53d1\u5c55\u94fe\u3002\u4e3a\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aResearchPulse\u7684\u4ee3\u7406\u6846\u67b6\u548cResearchPulse-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8eGPT-4o\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u79d1\u5b66\u601d\u60f3\u7684\u6f14\u53d8\uff0c\u9700\u8981\u8d85\u8d8a\u5355\u7bc7\u8bba\u6587\u6458\u8981\uff0c\u8fdb\u884c\u7ed3\u6784\u5316\u7684\u3001\u8de8\u6587\u6863\u7684\u63a8\u7406\u3002\u5f53\u524d\u7f3a\u4e4f\u4e00\u79cd\u80fd\u63d0\u53d6\u548c\u5bf9\u9f50\u76f8\u5173\u8bba\u6587\u7684\u52a8\u673a\u3001\u65b9\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4ee5\u91cd\u5efa\u7814\u7a76\u53d1\u5c55\u94fe\u7684\u7cfb\u7edf\uff0c\u8fd9\u5f15\u5165\u4e86\u65f6\u95f4\u5bf9\u9f50\u548c\u5f02\u6784\u5b9e\u9a8c\u6570\u636e\u6807\u51c6\u5316\u7684\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u6b63\u5f0f\u5316\u4e86\u201c\u591a\u6587\u6863\u79d1\u5b66\u63a8\u7406\u201d\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86ResearchPulse\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u8c03\u4ee3\u7406\uff1a\u4e00\u4e2a\u7528\u4e8e\u4efb\u52a1\u5206\u89e3\u7684\u8ba1\u5212\u4ee3\u7406\uff08Plan Agent\uff09\u3001\u4e00\u4e2a\u6784\u5efa\u52a8\u673a-\u65b9\u6cd5\u601d\u7ef4\u5bfc\u56fe\u7684Mmap-Agent\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5408\u6210\u5b9e\u9a8c\u6298\u7ebf\u56fe\u7684Lchart-Agent\u3002\u540c\u65f6\uff0c\u521b\u5efa\u4e86ResearchPulse-Bench\uff0c\u4e00\u4e2a\u5f15\u7528\u611f\u77e5\uff08citation-aware\uff09\u7684\u5e26\u6ce8\u91ca\u8bba\u6587\u805a\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cResearchPulse\u7cfb\u7edf\uff08\u5c3d\u7ba1\u4f7f\u75287B\u89c4\u6a21\u7684\u4ee3\u7406\uff09\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u6301\u7eed\u4f18\u4e8e\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5f3a\u5927\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u901a\u8fc7\u63d0\u51fa\u591a\u6587\u6863\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u3001ResearchPulse\u4ee3\u7406\u6846\u67b6\u548cResearchPulse-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u63a8\u8fdb\u4e86\u5bf9\u79d1\u5b66\u601d\u60f3\u6f14\u53d8\u7406\u89e3\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.03737", "pdf": "https://arxiv.org/pdf/2509.03737", "abs": "https://arxiv.org/abs/2509.03737", "authors": ["Casper van Engelenburg", "Jan van Gemert", "Seyran Khademi"], "title": "LayoutGKN: Graph Similarity Learning of Floor Plans", "categories": ["cs.CV"], "comment": "BMVC (2025)", "summary": "Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.", "AI": {"tldr": "LayoutGKN\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u6bd4\u8f83\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ef6\u8fdf\u8de8\u56fe\u8282\u70b9\u4ea4\u4e92\u5e76\u7ed3\u5408\u53ef\u5fae\u5206\u56fe\u6838\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u3002", "motivation": "\u697c\u5c42\u5e73\u9762\u56fe\u7684\u56fe\u6bd4\u8f83\u5bf9\u641c\u7d22\u3001\u805a\u7c7b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u6700\u6210\u529f\u7684\u56fe\u5339\u914d\u7f51\u7edc\u65b9\u6cd5\u56e0\u4f9d\u8d56\u6602\u8d35\u7684\u4e2d\u95f4\u8de8\u56fe\u8282\u70b9\u7ea7\u4ea4\u4e92\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u7f13\u6162\u3002", "method": "\u5f15\u5165LayoutGKN\uff0c\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u56fe\u6bd4\u8f83\u65b9\u6cd5\u3002\u5b83\u5c06\u8de8\u56fe\u8282\u70b9\u7ea7\u4ea4\u4e92\u63a8\u8fdf\u5230\u8054\u5408\u5d4c\u5165\u67b6\u6784\u7684\u672b\u7aef\u3002\u5177\u4f53\u5b9e\u73b0\u662f\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u53ef\u5fae\u5206\u56fe\u6838\u4f5c\u4e3a\u8ddd\u79bb\u51fd\u6570\uff0c\u4f5c\u7528\u4e8e\u6700\u7ec8\u5b66\u4e60\u5230\u7684\u8282\u70b9\u7ea7\u5d4c\u5165\u3002", "result": "LayoutGKN\u5728\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u65b9\u9762\u8868\u73b0\u4e0e\u56fe\u5339\u914d\u7f51\u7edc\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u540c\u65f6\uff0c\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "LayoutGKN\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u56fe\u6bd4\u8f83\u7684\u6548\u7387\u548c\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u697c\u5c42\u5e73\u9762\u56fe\u7b49\u5e94\u7528\u7684\u56fe\u6bd4\u8f83\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03682", "pdf": "https://arxiv.org/pdf/2509.03682", "abs": "https://arxiv.org/abs/2509.03682", "authors": ["Zhengyang Li", "Qijin Ji", "Xinghong Ling", "Quan Liu"], "title": "A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games", "categories": ["cs.LG"], "comment": "IEEE Transactions on Games, 2025", "summary": "Recent advancements in multi-agent reinforcement learning (MARL) have\ndemonstrated its application potential in modern games. Beginning with\nfoundational work and progressing to landmark achievements such as AlphaStar in\nStarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving\nsuperhuman performance across diverse game environments through techniques like\nself-play, supervised learning, and deep reinforcement learning. With its\ngrowing impact, a comprehensive review has become increasingly important in\nthis field. This paper aims to provide a thorough examination of MARL's\napplication from turn-based two-agent games to real-time multi-agent video\ngames including popular genres such as Sports games, First-Person Shooter (FPS)\ngames, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena\n(MOBA) games. We further analyze critical challenges posed by MARL in video\ngames, including nonstationary, partial observability, sparse rewards, team\ncoordination, and scalability, and highlight successful implementations in\ngames like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2,\nHonor of Kings, etc. This paper offers insights into MARL in video game AI\nsystems, proposes a novel method to estimate game complexity, and suggests\nfuture research directions to advance MARL and its applications in game\ndevelopment, inspiring further innovation in this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u5404\u79cd\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u3001\u9762\u4e34\u7684\u6311\u6218\u3001\u6210\u529f\u6848\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u6e38\u620f\u590d\u6742\u6027\u4f30\u7b97\u65b9\u6cd5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9274\u4e8eMARL\u5728\u73b0\u4ee3\u6e38\u620f\u4e2d\u5c55\u73b0\u51fa\u7684\u5de8\u5927\u6f5c\u529b\u548c\u5f71\u54cd\u529b\uff0c\u5bf9\u5176\u5e94\u7528\u8fdb\u884c\u5168\u9762\u3001\u6df1\u5165\u7684\u5ba1\u67e5\u5df2\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9MARL\u5728\u4e0d\u540c\u7c7b\u578b\u6e38\u620f\uff08\u5305\u62ec\u56de\u5408\u5236\u3001\u5b9e\u65f6\u3001\u4f53\u80b2\u3001FPS\u3001RTS\u548cMOBA\u6e38\u620f\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u5f7b\u5e95\u5ba1\u67e5\u548c\u5206\u6790\uff0c\u63a2\u8ba8\u5176\u9762\u4e34\u7684\u975e\u5e73\u7a33\u6027\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u7a00\u758f\u5956\u52b1\u3001\u56e2\u961f\u534f\u8c03\u548c\u53ef\u6269\u5c55\u6027\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6e38\u620f\u590d\u6742\u6027\u4f30\u7b97\u65b9\u6cd5\u3002", "result": "\u6587\u7ae0\u63d0\u4f9b\u4e86MARL\u5728\u89c6\u9891\u6e38\u620fAI\u7cfb\u7edf\u4e2d\u7684\u89c1\u89e3\uff0c\u5206\u6790\u4e86\u5173\u952e\u6311\u6218\uff0c\u7a81\u51fa\u4e86\u300a\u706b\u7bad\u8054\u76df\u300b\u3001\u300a\u6211\u7684\u4e16\u754c\u300b\u3001\u300a\u96f7\u795e\u4e4b\u9524III\u7ade\u6280\u573a\u300b\u3001\u300a\u661f\u9645\u4e89\u9738II\u300b\u3001\u300aDota 2\u300b\u3001\u300a\u738b\u8005\u8363\u8000\u300b\u7b49\u6e38\u620f\u7684\u6210\u529f\u5e94\u7528\u6848\u4f8b\u3002", "conclusion": "\u672c\u6587\u4e3a\u89c6\u9891\u6e38\u620fAI\u7cfb\u7edf\u4e2d\u7684MARL\u63d0\u4f9b\u4e86\u6df1\u523b\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u7b97\u6e38\u620f\u590d\u6742\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u4e3a\u63a8\u8fdbMARL\u53ca\u5176\u5728\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.03649", "pdf": "https://arxiv.org/pdf/2509.03649", "abs": "https://arxiv.org/abs/2509.03649", "authors": ["Davide Italo Serramazza", "Nikos Papadeas", "Zahraa Abdallah", "Georgiana Ifrim"], "title": "An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification", "categories": ["cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has become an increasingly important topic for\nunderstanding and attributing the predictions made by complex Time Series\nClassification (TSC) models. Among attribution methods, SHapley Additive\nexPlanations (SHAP) is widely regarded as an excellent attribution method; but\nits computational complexity, which scales exponentially with the number of\nfeatures, limits its practicality for long time series. To address this, recent\nstudies have shown that aggregating features via segmentation, to compute a\nsingle attribution value for a group of consecutive time points, drastically\nreduces SHAP running time. However, the choice of the optimal segmentation\nstrategy remains an open question. In this work, we investigated eight\ndifferent Time Series Segmentation algorithms to understand how segment\ncompositions affect the explanation quality. We evaluate these approaches using\ntwo established XAI evaluation methodologies: InterpretTime and AUC Difference.\nThrough experiments on both Multivariate (MTS) and Univariate Time Series\n(UTS), we find that the number of segments has a greater impact on explanation\nquality than the specific segmentation method. Notably, equal-length\nsegmentation consistently outperforms most of the custom time series\nsegmentation algorithms. Furthermore, we introduce a novel attribution\nnormalisation technique that weights segments by their length and we show that\nit consistently improves attribution quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u516b\u79cd\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7b97\u6cd5\u5bf9SHAP\u89e3\u91ca\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5206\u5272\u6570\u91cf\u6bd4\u5177\u4f53\u65b9\u6cd5\u66f4\u91cd\u8981\uff0c\u7b49\u957f\u5206\u5272\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u56e0\u5f52\u4e00\u5316\u6280\u672f\u4ee5\u63d0\u9ad8\u5f52\u56e0\u8d28\u91cf\u3002", "motivation": "\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5bf9\u4e8e\u7406\u89e3\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff08TSC\uff09\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002SHAP\u662f\u4e00\u79cd\u4f18\u79c0\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u65f6\u5e8f\u6570\u636e\u4e0a\u7684\u5b9e\u7528\u6027\u3002\u867d\u7136\u5206\u5272\u53ef\u4ee5\u964d\u4f4eSHAP\u8fd0\u884c\u65f6\u95f4\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u6700\u4f18\u7684\u5206\u5272\u7b56\u7565\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u516b\u79cd\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7b97\u6cd5\uff0c\u4ee5\u7406\u89e3\u7247\u6bb5\u7ec4\u6210\u5982\u4f55\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\u3002\u8bc4\u4f30\u4f7f\u7528\u4e86InterpretTime\u548cAUC Difference\u4e24\u79cdXAI\u8bc4\u4f30\u65b9\u6cd5\u3002\u5b9e\u9a8c\u5728\u591a\u5143\uff08MTS\uff09\u548c\u5355\u53d8\u91cf\uff08UTS\uff09\u65f6\u95f4\u5e8f\u5217\u4e0a\u8fdb\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u56e0\u5f52\u4e00\u5316\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u7247\u6bb5\u957f\u5ea6\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u5272\u7684\u6570\u91cf\u5bf9\u89e3\u91ca\u8d28\u91cf\u7684\u5f71\u54cd\u5927\u4e8e\u5177\u4f53\u7684\u5206\u5272\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7b49\u957f\u5206\u5272\u59cb\u7ec8\u4f18\u4e8e\u5927\u591a\u6570\u81ea\u5b9a\u4e49\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u65b0\u5f15\u5165\u7684\u5f52\u56e0\u5f52\u4e00\u5316\u6280\u672f\uff08\u6839\u636e\u7247\u6bb5\u957f\u5ea6\u52a0\u6743\uff09\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u5f52\u56e0\u8d28\u91cf\u3002", "conclusion": "\u5bf9\u4e8e\u57fa\u4e8eSHAP\u7684\u65f6\u95f4\u5e8f\u5217\u89e3\u91ca\uff0c\u5206\u5272\u7684\u6570\u91cf\u662f\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7b49\u957f\u5206\u5272\u901a\u5e38\u662f\u6700\u4f73\u9009\u62e9\u3002\u540c\u65f6\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u957f\u5ea6\u52a0\u6743\u5f52\u56e0\u5f52\u4e00\u5316\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u5f52\u56e0\u8d28\u91cf\u3002"}}
{"id": "2509.03610", "pdf": "https://arxiv.org/pdf/2509.03610", "abs": "https://arxiv.org/abs/2509.03610", "authors": ["Josh Wisoff", "Yao Tang", "Zhengyu Fang", "Jordan Guzman", "YuTang Wang", "Alex Yu"], "title": "NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management", "categories": ["cs.CL"], "comment": null, "summary": "Note-taking is a critical practice for capturing, organizing, and reflecting\non information in both academic and professional settings. The recent success\nof large language models has accelerated the development of AI-assisted tools,\nyet existing solutions often struggle with efficiency. We present NoteBar, an\nAI-assisted note-taking tool that leverages persona information and efficient\nlanguage models to automatically organize notes into multiple categories and\nbetter support user workflows. To support research and evaluation in this\nspace, we further introduce a novel persona-conditioned dataset of 3,173 notes\nand 8,494 annotated concepts across 16 MBTI personas, offering both diversity\nand semantic richness for downstream tasks. Finally, we demonstrate that\nNoteBar can be deployed in a practical and cost-effective manner, enabling\ninteractive use without reliance on heavy infrastructure. Together, NoteBar and\nits accompanying dataset provide a scalable and extensible foundation for\nadvancing AI-assisted personal knowledge management.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aNoteBar\u7684AI\u8f85\u52a9\u7b14\u8bb0\u5de5\u5177\uff0c\u5b83\u5229\u7528\u4eba\u683c\u4fe1\u606f\u548c\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u7ec4\u7ec7\u7b14\u8bb0\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u578b\u4eba\u683c\u6761\u4ef6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709AI\u7b14\u8bb0\u5de5\u5177\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9\u7b14\u8bb0\u5de5\u5177\u5728\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u652f\u6301\u7528\u6237\u5de5\u4f5c\u6d41\u7a0b\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684AI\u8f85\u52a9\u5de5\u5177\u6765\u6355\u6349\u3001\u7ec4\u7ec7\u548c\u53cd\u601d\u4fe1\u606f\u3002", "method": "1. \u5f00\u53d1NoteBar\uff1a\u4e00\u4e2aAI\u8f85\u52a9\u7b14\u8bb0\u5de5\u5177\uff0c\u5229\u7528\u4eba\u683c\u4fe1\u606f\u548c\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u7b14\u8bb0\u5206\u7c7b\u5230\u591a\u4e2a\u7c7b\u522b\uff0c\u4ee5\u652f\u6301\u7528\u6237\u5de5\u4f5c\u6d41\u7a0b\u30022. \u6784\u5efa\u6570\u636e\u96c6\uff1a\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b3,173\u6761\u7b14\u8bb0\u548c8,494\u4e2a\u5e26\u6ce8\u91ca\u6982\u5ff5\u7684\u65b0\u578b\u4eba\u683c\u6761\u4ef6\u6570\u636e\u96c6\uff0c\u6db5\u76d616\u79cdMBTI\u4eba\u683c\uff0c\u65e8\u5728\u63d0\u4f9b\u591a\u6837\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u4ee5\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "1. NoteBar\u80fd\u591f\u81ea\u52a8\u5c06\u7b14\u8bb0\u7ec4\u7ec7\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d\uff0c\u5e76\u66f4\u597d\u5730\u652f\u6301\u7528\u6237\u5de5\u4f5c\u6d41\u7a0b\u30022. NoteBar\u53ef\u4ee5\u4ee5\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u5f0f\u90e8\u7f72\uff0c\u65e0\u9700\u5927\u91cf\u57fa\u7840\u8bbe\u65bd\u5373\u53ef\u5b9e\u73b0\u4ea4\u4e92\u4f7f\u7528\u30023. \u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u591a\u6837\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u7684\u4eba\u683c\u6761\u4ef6\u6570\u636e\u96c6\u3002", "conclusion": "NoteBar\u53ca\u5176\u914d\u5957\u6570\u636e\u96c6\u4e3a\u63a8\u8fdbAI\u8f85\u52a9\u4e2a\u4eba\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2509.03740", "pdf": "https://arxiv.org/pdf/2509.03740", "abs": "https://arxiv.org/abs/2509.03740", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and\n\\textit{parameter-efficient} adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only \\textbf{0.04\\%} of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.", "AI": {"tldr": "CLIP-SVD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u9ad8\u6548\u591a\u6a21\u6001\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u901a\u8fc7\u4ec5\u5fae\u8c03CLIP\u6a21\u578b\u53c2\u6570\u77e9\u9635\u7684\u5947\u5f02\u503c\uff0c\u65e0\u9700\u989d\u5916\u6a21\u5757\uff0c\u5728\u7ec6\u7c92\u5ea6\u9886\u57df\u9002\u5e94\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u7ec6\u7c92\u5ea6\u9886\u57df\u9002\u5e94\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u8868\u73b0\u4e3a\u5bf9\u63d0\u793a\u5de5\u7a0b\u7684\u4f9d\u8d56\u3001\u5168\u6a21\u578b\u5fae\u8c03\u6210\u672c\u9ad8\uff0c\u4ee5\u53ca\u73b0\u6709\u57fa\u4e8e\u989d\u5916\u7ec4\u4ef6\u7684\u9002\u5e94\u65b9\u6cd5\u53ef\u80fd\u9650\u5236\u9002\u5e94\u8d28\u91cf\u3001\u7834\u574f\u6a21\u578b\u7a33\u5b9a\u6027\u5e76\u635f\u5bb3\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CLIP-SVD\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u3001\u53c2\u6570\u9ad8\u6548\u7684\u9002\u5e94\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u76f4\u63a5\u4fee\u6539CLIP\u7684\u5185\u90e8\u53c2\u6570\u7a7a\u95f4\uff0c\u800c\u65e0\u9700\u6ce8\u5165\u989d\u5916\u7684\u6a21\u5757\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u4ec5\u5fae\u8c03CLIP\u53c2\u6570\u77e9\u9635\u7684\u5947\u5f02\u503c\uff0c\u4ee5\u91cd\u65b0\u7f29\u653e\u57fa\u5411\u91cf\u8fdb\u884c\u9886\u57df\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "CLIP-SVD\u4ec5\u4f7f\u7528\u6a21\u578b\u603b\u53c2\u6570\u76840.04%\u5c31\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u9002\u5e94\u6027\u80fd\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u572811\u4e2a\u81ea\u7136\u6570\u636e\u96c6\u548c10\u4e2a\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5176\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u65b9\u6cd5\u5206\u6790\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86CLIP-SVD\u9002\u5e94\u6548\u679c\u548c\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CLIP-SVD\u901a\u8fc7\u5176\u72ec\u7279\u7684\u5947\u5f02\u503c\u5fae\u8c03\u7b56\u7565\uff0c\u514b\u670d\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u53c2\u6570\u6548\u7387\u3001\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03691", "pdf": "https://arxiv.org/pdf/2509.03691", "abs": "https://arxiv.org/abs/2509.03691", "authors": ["Matthew Zhang", "Jihao Andreas Lin", "Adrian Weller", "Richard E. Turner", "Isaac Reid"], "title": "Graph Random Features for Scalable Gaussian Processes", "categories": ["cs.LG"], "comment": null, "summary": "We study the application of graph random features (GRFs) - a recently\nintroduced stochastic estimator of graph node kernels - to scalable Gaussian\nprocesses on discrete input spaces. We prove that (under mild assumptions)\nBayesian inference with GRFs enjoys $O(N^{3/2})$ time complexity with respect\nto the number of nodes $N$, compared to $O(N^3)$ for exact kernels. Substantial\nwall-clock speedups and memory savings unlock Bayesian optimisation on graphs\nwith over $10^6$ nodes on a single computer chip, whilst preserving competitive\nperformance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u968f\u673a\u7279\u5f81\uff08GRFs\uff09\u5728\u79bb\u6563\u8f93\u5165\u7a7a\u95f4\u4e0a\u53ef\u6269\u5c55\u9ad8\u65af\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(N^{3/2})$\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684$O(N^3)$\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5728\u767e\u4e07\u8282\u70b9\u56fe\u4e0a\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u5728\u79bb\u6563\u8f93\u5165\u7a7a\u95f4\uff08\u5982\u56fe\uff09\u4e0a\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\u65f6\uff0c\u7cbe\u786e\u6838\u51fd\u6570\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(N^3)$\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56fe\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u56fe\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u56fe\u968f\u673a\u7279\u5f81\uff08GRFs\uff09\u4f5c\u4e3a\u56fe\u8282\u70b9\u6838\u7684\u968f\u673a\u4f30\u8ba1\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u3002", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\uff0c\u4f7f\u7528GRFs\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(N^{3/2})$\uff0c\u76f8\u6bd4\u7cbe\u786e\u6838\u7684$O(N^3)$\u6709\u663e\u8457\u63d0\u5347\u3002\u5b9e\u73b0\u4e86\u5927\u5e45\u5ea6\u7684\u8fd0\u884c\u65f6\u52a0\u901f\u548c\u5185\u5b58\u8282\u7701\uff0c\u4f7f\u5f97\u5728\u5355\u4e2a\u8ba1\u7b97\u82af\u7247\u4e0a\u5bf9\u8d85\u8fc7$10^6$\u4e2a\u8282\u70b9\u7684\u56fe\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u6210\u4e3a\u53ef\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u56fe\u968f\u673a\u7279\u5f81\uff08GRFs\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8fdb\u884c\u9ad8\u65af\u8fc7\u7a0b\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u4f18\u5316\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2509.03728", "pdf": "https://arxiv.org/pdf/2509.03728", "abs": "https://arxiv.org/abs/2509.03728", "authors": ["Wesley Hanwen Deng", "Sunnie S. Y. Kim", "Akshita Jha", "Ken Holstein", "Motahhare Eslami", "Lauren Wilcox", "Leon A Gatys"], "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPersonaTeaming\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u8eab\u4efd\u548c\u80cc\u666f\u5f15\u5165\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u7684\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u4fdd\u6301\u4e86\u63d0\u793a\u591a\u6837\u6027\u3002", "motivation": "AI\u6cbb\u7406\u548c\u5b89\u5168\u7814\u7a76\u5f3a\u8c03\u7ea2\u961f\u6d4b\u8bd5\u4eba\u5458\u7684\u8eab\u4efd\u548c\u80cc\u666f\u4f1a\u5f71\u54cd\u5176\u98ce\u9669\u53d1\u73b0\u7b56\u7565\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u81ea\u52a8\u5316\u7ea2\u961f\u65b9\u6cd5\u672a\u8003\u8651\u8eab\u4efd\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u5176\u53d1\u73b0\u6f5c\u5728\u98ce\u9669\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86PersonaTeaming\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u4e2d\u5f15\u5165\u201c\u89d2\u8272\u201d\uff08personas\uff09\u6765\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u5bf9\u6297\u7b56\u7565\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u57fa\u4e8e\u201c\u7ea2\u961f\u4e13\u5bb6\u201d\u6216\u201c\u666e\u901aAI\u7528\u6237\u201d\u89d2\u8272\u53d8\u5f02\u63d0\u793a\uff1b2) \u8bbe\u8ba1\u52a8\u6001\u89d2\u8272\u751f\u6210\u7b97\u6cd5\uff0c\u81ea\u9002\u5e94\u751f\u6210\u4e0d\u540c\u89d2\u8272\u7c7b\u578b\uff1b3) \u63d0\u51fa\u4e86\u65b0\u7684\u201c\u53d8\u5f02\u8ddd\u79bb\u201d\u5ea6\u91cf\u6807\u51c6\uff0c\u4ee5\u8865\u5145\u73b0\u6709\u7684\u63d0\u793a\u591a\u6837\u6027\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0eRainbowPlus\uff08\u4e00\u79cd\u5148\u8fdb\u7684\u81ea\u52a8\u5316\u7ea2\u961f\u65b9\u6cd5\uff09\u76f8\u6bd4\uff0cPersonaTeaming\u901a\u8fc7\u89d2\u8272\u53d8\u5f02\u4f7f\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe144.1%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u591a\u6837\u6027\u3002", "conclusion": "\u7814\u7a76\u8ba8\u8bba\u4e86\u4e0d\u540c\u89d2\u8272\u7c7b\u578b\u548c\u53d8\u5f02\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u81ea\u52a8\u5316\u7ea2\u961f\u548c\u4eba\u7c7b\u7ea2\u961f\u65b9\u6cd5\u7684\u4e92\u8865\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u673a\u4f1a\u3002"}}
{"id": "2509.03615", "pdf": "https://arxiv.org/pdf/2509.03615", "abs": "https://arxiv.org/abs/2509.03615", "authors": ["Aryan Gupta", "Anupam Purwar"], "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition", "categories": ["cs.CL", "cs.AI"], "comment": "Sprinklr OCR provides a fast and compute light way of performing OCR", "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u7684\u8fb9\u7f18OCR\u7cfb\u7edfSprinklr-Edge-OCR\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709LVLM\u53ca\u4f20\u7edfOCR\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cSprinklr-Edge-OCR\u5728\u6548\u7387\u548c\u6210\u672c\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u8bc1\u660e\u4f20\u7edfOCR\u5728\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u4e0b\u4ecd\u5177\u4f18\u52bf\u3002", "motivation": "\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u5728\u591a\u8bed\u8a00\u3001\u5608\u6742\u548c\u591a\u6837\u5316\u7684\u771f\u5b9e\u56fe\u50cf\u4e2d\u4ecd\u9762\u4e34\u663e\u8457\u6311\u6218\u3002\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5174\u8d77\u5e76\u5c55\u73b0\u51fa\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u5bf9OCR\u7cfb\u7edf\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6210\u672c\u8981\u6c42\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e13\u95e8\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u4f18\u5316\u7684\u65b0\u578bOCR\u7cfb\u7edfSprinklr-Edge-OCR\u3002\u7814\u7a76\u4eba\u5458\u5728\u4e00\u4e2a\u4e13\u6709\u7684\u3001\u53cc\u91cd\u4eba\u5de5\u6807\u6ce8\u7684\u591a\u8bed\u8a00\uff0854\u79cd\u8bed\u8a00\uff09\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u4e94\u79cd\u5148\u8fdb\u7684LVLM\uff08InternVL, Qwen, GOT OCR, LLaMA, MiniCPM\uff09\u548c\u4e24\u79cd\u4f20\u7edfOCR\u7cfb\u7edf\uff08Sprinklr-Edge-OCR, SuryaOCR\uff09\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u6bd4\u8f83\u8bc4\u4f30\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u8bed\u8a00\u8986\u76d6\u7387\u3001\u8ba1\u7b97\u6548\u7387\uff08\u5ef6\u8fdf\u3001\u5185\u5b58\u3001GPU\u4f7f\u7528\uff09\u548c\u90e8\u7f72\u6210\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u6a21\u578b\u5728\u4ec5CPU\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u8fb9\u7f18\u6848\u4f8b\u90e8\u7f72\u5206\u6790\u3002", "result": "Qwen\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u7cbe\u5ea6\uff080.54\uff09\u3002Sprinklr-Edge-OCR\u5219\u83b7\u5f97\u4e86\u6700\u4f73\u7684\u6574\u4f53F1\u5206\u6570\uff080.46\uff09\uff0c\u5e76\u5728\u6548\u7387\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5904\u7406\u56fe\u50cf\u901f\u5ea6\u6bd4LVLM\u5feb35\u500d\uff08\u5e73\u5747\u6bcf\u5f20\u56fe\u50cf0.17\u79d2\uff09\uff0c\u540c\u65f6\u6210\u672c\u4e5f\u8fdc\u4f4e\u4e8eLVLM\u76840.01\uff08\u6bcf1000\u5f20\u56fe\u50cf0.006\u7f8e\u5143\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u7531\u4e8e\u5176\u4f4e\u8ba1\u7b97\u8981\u6c42\u3001\u4f4e\u5ef6\u8fdf\u548c\u6781\u9ad8\u7684\u7ecf\u6d4e\u6027\uff0c\u4f20\u7edfOCR\u7cfb\u7edf\u4ecd\u7136\u662f\u8fb9\u7f18\u90e8\u7f72\u6700\u7406\u60f3\u7684\u9009\u62e9\u3002"}}
{"id": "2509.03754", "pdf": "https://arxiv.org/pdf/2509.03754", "abs": "https://arxiv.org/abs/2509.03754", "authors": ["Zongsen Qiu"], "title": "STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.", "AI": {"tldr": "\u63d0\u51faSTA-Net\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u8bad\u7ec3NAS\u548c\u5f62\u72b6-\u7eb9\u7406\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7279\u5f81\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u3002", "motivation": "\u5168\u7403\u7cae\u98df\u5b89\u5168\u9700\u6c42\u4fc3\u4f7f\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u6210\u4e3a\u5173\u952e\uff0c\u4f46\u73b0\u6709\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\uff0c\u5176\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\u96be\u4ee5\u6709\u6548\u6355\u6349\u4e0d\u89c4\u5219\u75c5\u6591\u5f62\u72b6\u548c\u590d\u6742\u7eb9\u7406\u7b49\u7ec6\u5fae\u75c5\u7406\u7279\u5f81\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u53cc\u91cd\u89e3\u51b3\u65b9\u6848\uff1a1) \u4f7f\u7528\u514d\u8bad\u7ec3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5(DeepMAD)\u6784\u5efa\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u9ad8\u6548\u7f51\u7edc\u9aa8\u5e72\uff1b2) \u5f15\u5165\u5f62\u72b6-\u7eb9\u7406\u6ce8\u610f\u529b\u6a21\u5757(STAM)\uff0c\u5c06\u6ce8\u610f\u529b\u673a\u5236\u89e3\u8026\u4e3a\u4e24\u4e2a\u5206\u652f\u2014\u2014\u4e00\u4e2a\u4f7f\u7528\u53ef\u53d8\u5f62\u5377\u79ef(DCNv4)\u6355\u83b7\u5f62\u72b6\u4fe1\u606f\uff0c\u53e6\u4e00\u4e2a\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\u7ec4\u6355\u83b7\u7eb9\u7406\u4fe1\u606f\u3002", "result": "\u5728\u516c\u5f00\u7684CCMT\u690d\u7269\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\uff0cSTA-Net\u6a21\u578b\uff08\u62e5\u6709401K\u53c2\u6570\u548c51.1M FLOPs\uff09\u5b9e\u73b0\u4e8689.00%\u7684\u51c6\u786e\u7387\u548c88.96%\u7684F1\u5206\u6570\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0cSTAM\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u6807\u51c6\u6ce8\u610f\u529b\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\u5e76\u878d\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u7684\u7cbe\u51c6\u519c\u4e1aAI\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2509.03695", "pdf": "https://arxiv.org/pdf/2509.03695", "abs": "https://arxiv.org/abs/2509.03695", "authors": ["Payam Abdisarabshali", "Fardis Nadimi", "Kasra Borazjani", "Naji Khosravan", "Minghui Liwang", "Wei Ni", "Dusit Niyato", "Michael Langberg", "Seyyedali Hosseinalipour"], "title": "Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 2 figures, 1 table", "summary": "The rise of foundation models (FMs) has reshaped the landscape of machine\nlearning. As these models continued to grow, leveraging geo-distributed data\nfrom wireless devices has become increasingly critical, giving rise to\nfederated foundation models (FFMs). More recently, FMs have evolved into\nmulti-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse\nmodalities across multiple tasks, which motivates a new underexplored paradigm:\nM3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by\nproposing hierarchical federated foundation models (HF-FMs), which in turn\nexpose two overlooked heterogeneity dimensions to fog/edge networks that have a\ndirect impact on these emerging models: (i) heterogeneity in collected\nmodalities and (ii) heterogeneity in executed tasks across fog/edge nodes.\nHF-FMs strategically align the modular structure of M3T FMs, comprising\nmodality encoders, prompts, mixture-of-experts (MoEs), adapters, and task\nheads, with the hierarchical nature of fog/edge infrastructures. Moreover,\nHF-FMs enable the optional usage of device-to-device (D2D) communications,\nenabling horizontal module relaying and localized cooperative training among\nnodes when feasible. Through delving into the architectural design of HF-FMs,\nwe highlight their unique capabilities along with a series of tailored future\nresearch directions. Finally, to demonstrate their potential, we prototype\nHF-FMs in a wireless network setting and release the open-source code for the\ndevelopment of HF-FMs with the goal of fostering exploration in this untapped\nfield (GitHub: https://github.com/payamsiabd/M3T-FFM).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5206\u5c42\u8054\u90a6\u57fa\u7840\u6a21\u578b\uff08HF-FMs\uff09\uff0c\u5c06\u591a\u6a21\u6001\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b\u4e0e\u96fe/\u8fb9\u7f18\u7f51\u7edc\u7684\u5206\u5c42\u67b6\u6784\u7ed3\u5408\uff0c\u89e3\u51b3\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\u6267\u884c\u7684\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u53d1\u5c55\u548c\u591a\u6a21\u6001\u591a\u4efb\u52a1\uff08M3T\uff09FMs\u7684\u51fa\u73b0\uff0c\u5229\u7528\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u53d8\u5f97\u5173\u952e\u3002\u5f53\u524d\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22M3T FFMs\uff0c\u5c24\u5176\u5ffd\u7565\u4e86\u96fe/\u8fb9\u7f18\u7f51\u7edc\u4e2d\u6536\u96c6\u6a21\u6001\u548c\u6267\u884c\u4efb\u52a1\u7684\u5f02\u6784\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8054\u90a6\u57fa\u7840\u6a21\u578b\uff08HF-FMs\uff09\uff0c\u5c06M3T FMs\u7684\u6a21\u5757\u5316\u7ed3\u6784\uff08\u5982\u6a21\u6001\u7f16\u7801\u5668\u3001MoEs\u3001\u9002\u914d\u5668\u3001\u4efb\u52a1\u5934\uff09\u4e0e\u96fe/\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u7684\u5206\u5c42\u6027\u8d28\u5bf9\u9f50\u3002HF-FMs\u8fd8\u652f\u6301\u53ef\u9009\u7684\u8bbe\u5907\u5230\u8bbe\u5907\uff08D2D\uff09\u901a\u4fe1\uff0c\u5b9e\u73b0\u6a21\u5757\u4e2d\u7ee7\u548c\u5c40\u90e8\u534f\u4f5c\u8bad\u7ec3\u3002\u901a\u8fc7\u539f\u578b\u9a8c\u8bc1\u5176\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u3002", "result": "HF-FMs\u63ed\u793a\u4e86\u96fe/\u8fb9\u7f18\u7f51\u7edc\u4e2d\u6a21\u6001\u6536\u96c6\u548c\u4efb\u52a1\u6267\u884c\u7684\u4e24\u79cd\u5f02\u6784\u7ef4\u5ea6\u3002\u6587\u7ae0\u5f3a\u8c03\u4e86HF-FMs\u7684\u72ec\u7279\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\u539f\u578b\u5b9e\u73b0\u5c55\u793a\u4e86HF-FMs\u5728\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "HF-FMs\u4e3a\u591a\u6a21\u6001\u591a\u4efb\u52a1\u8054\u90a6\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u672a\u88ab\u63a2\u7d22\u7684\u8303\u5f0f\u3002\u901a\u8fc7\u5176\u67b6\u6784\u8bbe\u8ba1\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u65e8\u5728\u4fc3\u8fdb\u8be5\u65b0\u5174\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2509.03730", "pdf": "https://arxiv.org/pdf/2509.03730", "abs": "https://arxiv.org/abs/2509.03730", "authors": ["Pengrui Han", "Rafal Kocielnik", "Peiyang Song", "Ramit Debnath", "Dean Mobbs", "Anima Anandkumar", "R. Michael Alvarez"], "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "stat.ML"], "comment": "We make public all code and source data at\n  https://github.com/psychology-of-AI/Personality-Illusion", "summary": "Personality traits have long been studied as predictors of human\nbehavior.Recent advances in Large Language Models (LLMs) suggest similar\npatterns may emerge in artificial systems, with advanced LLMs displaying\nconsistent behavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u683c\u7279\u5f81\uff0c\u53d1\u73b0\u6307\u4ee4\u5bf9\u9f50\u80fd\u7a33\u5b9aLLM\u7684\u81ea\u6211\u62a5\u544a\u6027\u683c\uff0c\u4f46\u8fd9\u4e9b\u62a5\u544a\u5e76\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u5176\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u4eba\u683c\u6ce8\u5165\u867d\u80fd\u5f71\u54cd\u81ea\u6211\u62a5\u544a\uff0c\u4f46\u5bf9\u5b9e\u9645\u884c\u4e3a\u5f71\u54cd\u751a\u5fae\u3002", "motivation": "\u4eba\u683c\u7279\u8d28\u5728\u4eba\u7c7b\u884c\u4e3a\u9884\u6d4b\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0cLLMs\u4e5f\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7279\u8d28\u7684\u7a33\u5b9a\u884c\u4e3a\u503e\u5411\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7814\u7a76\u591a\u4f9d\u8d56\u7b80\u5316\u81ea\u6211\u62a5\u544a\u548c\u542f\u53d1\u5f0f\u63d0\u793a\uff0c\u7f3a\u4e4f\u884c\u4e3a\u9a8c\u8bc1\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7cfb\u7edf\u6027\u5730\u523b\u753bLLM\u6027\u683c\u5e76\u6df1\u5165\u7406\u89e3\u5176\u6a21\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u523b\u753b\u4e86LLM\u7684\u6027\u683c\uff1a1) \u8bad\u7ec3\u9636\u6bb5\u4e2d\u7279\u8d28\u753b\u50cf\u7684\u52a8\u6001\u51fa\u73b0\u4e0e\u6f14\u53d8\uff1b2) \u81ea\u6211\u62a5\u544a\u7279\u8d28\u5728\u884c\u4e3a\u4efb\u52a1\u4e2d\u7684\u9884\u6d4b\u6548\u5ea6\uff1b3) \u76ee\u6807\u5e72\u9884\uff08\u5982\u4eba\u683c\u6ce8\u5165\uff09\u5bf9\u81ea\u6211\u62a5\u544a\u548c\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6307\u4ee4\u5bf9\u9f50\uff08\u5982RLHF\u3001\u6307\u4ee4\u8c03\u4f18\uff09\u80fd\u663e\u8457\u7a33\u5b9a\u7279\u8d28\u8868\u8fbe\u5e76\u589e\u5f3a\u7279\u8d28\u76f8\u5173\u6027\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u6570\u636e\u76f8\u4f3c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u81ea\u6211\u62a5\u544a\u7684\u7279\u8d28\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u9884\u6d4b\u884c\u4e3a\uff0c\u89c2\u5bdf\u5230\u7684\u5173\u8054\u4e5f\u5e38\u4e0e\u4eba\u7c7b\u6a21\u5f0f\u4e0d\u540c\u3002\u5c3d\u7ba1\u4eba\u683c\u6ce8\u5165\u80fd\u6210\u529f\u5f15\u5bfc\u81ea\u6211\u62a5\u544a\u671d\u9884\u671f\u65b9\u5411\u53d1\u5c55\uff0c\u4f46\u5bf9\u5b9e\u9645\u884c\u4e3a\u7684\u5f71\u54cd\u5374\u5f88\u5c0f\u6216\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u672c\u7814\u7a76\u533a\u5206\u4e86\u8868\u9762\u7279\u8d28\u8868\u8fbe\u4e0e\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u6311\u6218\u4e86\u5173\u4e8eLLM\u6027\u683c\u7684\u5047\u8bbe\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8fdb\u884c\u66f4\u6df1\u5c42\u6b21\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.03647", "pdf": "https://arxiv.org/pdf/2509.03647", "abs": "https://arxiv.org/abs/2509.03647", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Jou Barzdukas", "Simon Fu", "Narmeen Oozeer"], "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5668\u5b58\u5728\u201c\u81ea\u6211\u504f\u597d\u504f\u5dee\u201d\u3002\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f6c\u5411\u5411\u91cf\uff0c\u8be5\u7814\u7a76\u5728\u63a8\u7406\u65f6\u6210\u529f\u5c06\u4e0d\u5408\u7406\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\u51cf\u5c11\u4e86\u9ad8\u8fbe97%\uff0c\u4f46\u5bf9\u5408\u7406\u504f\u597d\u548c\u65e0\u504f\u4e00\u81f4\u6027\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u8868\u660e\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5668\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u201c\u81ea\u6211\u504f\u597d\u504f\u5dee\u201d\uff0c\u5373\u503e\u5411\u4e8e\u504f\u7231\u81ea\u5df1\u7684\u8f93\u51fa\u800c\u975e\u5176\u4ed6\u6a21\u578b\u7684\u3002\u8fd9\u79cd\u504f\u89c1\u635f\u5bb3\u4e86\u8bc4\u4f30\u7ba1\u9053\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u504f\u597d\u8c03\u4f18\u548c\u6a21\u578b\u8def\u7531\u7b49\u5173\u952e\u4efb\u52a1\u4e2d\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u533a\u5206\u5408\u7406\u548c\u4e0d\u5408\u7406\u81ea\u6211\u504f\u597d\u7684\u5b9a\u5236\u6570\u636e\u96c6\u3002\u7136\u540e\uff0c\u4f7f\u7528\u5bf9\u6bd4\u6fc0\u6d3b\u6dfb\u52a0\uff08CAA\uff09\u548c\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u8f7b\u91cf\u7ea7\u8f6c\u5411\u5411\u91cf\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5e94\u7528\u8fd9\u4e9b\u5411\u91cf\u4ee5\u671f\u7f13\u89e3\u504f\u5dee\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8f6c\u5411\u5411\u91cf\u80fd\u5c06\u4e0d\u5408\u7406\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\u51cf\u5c11\u9ad8\u8fbe97%\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8f6c\u5411\u5411\u91cf\u5728\u5904\u7406\u5408\u6cd5\u81ea\u6211\u504f\u597d\u548c\u65e0\u504f\u4e00\u81f4\u6027\u65f6\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u6697\u793a\u81ea\u6211\u504f\u597d\u53ef\u80fd\u6d89\u53ca\u591a\u4e2a\u6216\u975e\u7ebf\u6027\u65b9\u5411\u3002", "conclusion": "\u8f6c\u5411\u5411\u91cf\u4f5c\u4e3aLLM\u8bc4\u4f30\u5668\uff08LLM-as-judges\uff09\u7684\u5b89\u5168\u4fdd\u969c\u5177\u6709\u663e\u8457\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002\u5c3d\u7ba1\u5b83\u4eec\u80fd\u6709\u6548\u51cf\u5c11\u4e0d\u5408\u7406\u7684\u81ea\u6211\u504f\u597d\uff0c\u4f46\u5176\u5728\u590d\u6742\u60c5\u5883\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u8868\u660e\uff0c\u81ea\u6211\u504f\u597d\u7684\u673a\u5236\u53ef\u80fd\u66f4\u4e3a\u590d\u6742\uff0c\u9700\u8981\u672a\u6765\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u9c81\u68d2\u7684\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2509.03786", "pdf": "https://arxiv.org/pdf/2509.03786", "abs": "https://arxiv.org/abs/2509.03786", "authors": ["Xinxin Wang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Yinan Yao"], "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection", "categories": ["cs.CV"], "comment": "14pages, accepted by PRCV2025", "summary": "Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6c34\u4e0b\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\uff08UCOD\uff09\u4efb\u52a1\uff0c\u53d1\u5e03\u4e86DeepCamo\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86SLENet\u6846\u67b6\u3002SLENet\u901a\u8fc7\u7279\u5f81\u589e\u5f3a\u548c\u5b9a\u4f4d\u5f15\u5bfc\uff0c\u5728UCOD\u548c\u901a\u7528COD\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u5bf9\u6d77\u6d0b\u751f\u6001\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u5149\u5b66\u7578\u53d8\u3001\u6c34\u4f53\u6d51\u6d4a\u548c\u6d77\u6d0b\u751f\u7269\u590d\u6742\u6027\u7b49\u56e0\u7d20\uff0c\u8be5\u4efb\u52a1\u7814\u7a76\u4e0d\u8db3\u4e14\u8bc6\u522b\u7cbe\u5ea6\u53d7\u9650\uff0c\u6025\u9700\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u8005\u9996\u5148\u5b9a\u4e49\u4e86UCOD\u4efb\u52a1\u5e76\u6784\u5efa\u4e86DeepCamo\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u8bed\u4e49\u5b9a\u4f4d\u4e0e\u589e\u5f3a\u7f51\u7edc\uff08SLENet\uff09\uff0c\u8be5\u7f51\u7edc\u5305\u542bGamma-Asymmetric Enhancement (GAE) \u6a21\u5757\u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u53ca\u5b9a\u4f4d\u5f15\u5bfc\u5206\u652f(LGB)\u751f\u6210\u5305\u542b\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u7684\u5b9a\u4f4d\u56fe\uff0c\u8be5\u56fe\u6307\u5bfc\u591a\u5c3a\u5ea6\u76d1\u7763\u89e3\u7801\u5668(MSSD)\u751f\u6210\u66f4\u7cbe\u786e\u7684\u9884\u6d4b\u3002", "result": "SLENet\u5728DeepCamo\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u73b0\u6709\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SLENet\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u7ed3\u6784\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.03703", "pdf": "https://arxiv.org/pdf/2509.03703", "abs": "https://arxiv.org/abs/2509.03703", "authors": ["Tristan Luca Saidi", "Abigail Hickok", "Bastian Rieck", "Andrew J. Blumberg"], "title": "EmbedOR: Provable Cluster-Preserving Visualizations with Curvature-Based Stochastic Neighbor Embeddings", "categories": ["cs.LG"], "comment": null, "summary": "Stochastic Neighbor Embedding (SNE) algorithms like UMAP and tSNE often\nproduce visualizations that do not preserve the geometry of noisy and high\ndimensional data. In particular, they can spuriously separate connected\ncomponents of the underlying data submanifold and can fail to find clusters in\nwell-clusterable data. To address these limitations, we propose EmbedOR, a SNE\nalgorithm that incorporates discrete graph curvature. Our algorithm\nstochastically embeds the data using a curvature-enhanced distance metric that\nemphasizes underlying cluster structure. Critically, we prove that the EmbedOR\ndistance metric extends consistency results for tSNE to a much broader class of\ndatasets. We also describe extensive experiments on synthetic and real data\nthat demonstrate the visualization and geometry-preservation capabilities of\nEmbedOR. We find that, unlike other SNE algorithms and UMAP, EmbedOR is much\nless likely to fragment continuous, high-density regions of the data. Finally,\nwe demonstrate that the EmbedOR distance metric can be used as a tool to\nannotate existing visualizations to identify fragmentation and provide deeper\ninsight into the underlying geometry of the data.", "AI": {"tldr": "EmbedOR\u662f\u4e00\u79cd\u65b0\u7684SNE\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u56fe\u66f2\u7387\uff0c\u89e3\u51b3\u4e86UMAP\u548ctSNE\u7b49\u7b97\u6cd5\u5728\u53ef\u89c6\u5316\u9ad8\u7ef4\u6570\u636e\u65f6\u51e0\u4f55\u7ed3\u6784\u4fdd\u5b58\u4e0d\u4f73\u3001\u6613\u4ea7\u751f\u865a\u5047\u5206\u79bb\u7684\u95ee\u9898\uff0c\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u6570\u636e\u7c07\u5e76\u51cf\u5c11\u8fde\u7eed\u533a\u57df\u7684\u788e\u7247\u5316\u3002", "motivation": "\u73b0\u6709\u7684SNE\u7b97\u6cd5\uff08\u5982UMAP\u548ctSNE\uff09\u5728\u5904\u7406\u566a\u58f0\u5927\u3001\u9ad8\u7ef4\u5ea6\u6570\u636e\u65f6\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u672a\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u6570\u636e\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e38\u5c06\u5e95\u5c42\u6570\u636e\u6d41\u5f62\u4e2d\u7684\u8fde\u901a\u5206\u91cf\u9519\u8bef\u5730\u5206\u79bb\uff0c\u5e76\u4e14\u5728\u53ef\u826f\u597d\u805a\u7c7b\u7684\u6570\u636e\u4e2d\u4e5f\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u805a\u7c7b\u3002", "method": "\u63d0\u51faEmbedOR\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u79bb\u6563\u56fe\u66f2\u7387\u878d\u5165SNE\u6846\u67b6\u4e2d\u3002\u5b83\u4f7f\u7528\u4e00\u79cd\u66f2\u7387\u589e\u5f3a\u7684\u8ddd\u79bb\u5ea6\u91cf\u8fdb\u884c\u968f\u673a\u5d4c\u5165\uff0c\u4ee5\u5f3a\u8c03\u6f5c\u5728\u7684\u7c07\u7ed3\u6784\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u8bc1\u660eEmbedOR\u7684\u8ddd\u79bb\u5ea6\u91cf\u5c06tSNE\u7684\u4e00\u81f4\u6027\u7ed3\u679c\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEmbedOR\u5728\u53ef\u89c6\u5316\u548c\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u4e0e\u5176\u4ed6SNE\u7b97\u6cd5\u548cUMAP\u4e0d\u540c\uff0cEmbedOR\u5927\u5927\u964d\u4f4e\u4e86\u8fde\u7eed\u3001\u9ad8\u5bc6\u5ea6\u6570\u636e\u533a\u57df\u788e\u7247\u5316\u7684\u53ef\u80fd\u6027\u3002\u6b64\u5916\uff0cEmbedOR\u8ddd\u79bb\u5ea6\u91cf\u53ef\u7528\u4e8e\u6807\u6ce8\u73b0\u6709\u53ef\u89c6\u5316\uff0c\u4ee5\u8bc6\u522b\u788e\u7247\u5316\u5e76\u63d0\u4f9b\u5bf9\u6570\u636e\u5e95\u5c42\u51e0\u4f55\u7684\u66f4\u6df1\u5165\u6d1e\u5bdf\u3002", "conclusion": "EmbedOR\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u56fe\u66f2\u7387\uff0c\u663e\u8457\u6539\u5584\u4e86SNE\u7b97\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u4fdd\u7559\u80fd\u529b\uff0c\u6709\u6548\u907f\u514d\u4e86\u8fde\u7eed\u533a\u57df\u7684\u788e\u7247\u5316\uff0c\u5e76\u80fd\u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\u6765\u589e\u5f3a\u73b0\u6709\u53ef\u89c6\u5316\u7684\u6d1e\u5bdf\u529b\u3002"}}
{"id": "2509.03736", "pdf": "https://arxiv.org/pdf/2509.03736", "abs": "https://arxiv.org/abs/2509.03736", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "categories": ["cs.AI"], "comment": "25 pages, 9 figures, 7 tables", "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u53c2\u4e0e\u8005\u65f6\uff0c\u5c3d\u7ba1\u53ef\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8868\u9762\u54cd\u5e94\uff0c\u4f46\u5728\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u5185\u90e8\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u4f5c\u4e3a\u4eba\u7c7b\u7814\u7a76\u66ff\u4ee3\u54c1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9274\u4e8eLLM\u5728\u4eba\u7c7b\u7814\u7a76\u4e2d\u66ff\u4ee3\u771f\u5b9e\u53c2\u4e0e\u8005\u7684\u6f5c\u529b\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u751f\u6210\u6570\u636e\u4e0e\u4eba\u7c7b\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002\u672c\u7814\u7a76\u5219\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u6839\u672c\u7684\u95ee\u9898\uff1aLLM\u4ee3\u7406\u5728\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u80fd\u5426\u4fdd\u6301\u5185\u90e8\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u9879\u7814\u7a76\uff0c\u65e8\u5728(a)\u63ed\u793a\u4ee3\u7406\u7684\u5185\u90e8\u72b6\u6001\uff0c\u4ee5\u53ca(b)\u5728\u57fa\u672c\u5bf9\u8bdd\u573a\u666f\u4e2d\u68c0\u9a8c\u4ee3\u7406\u884c\u4e3a\u3002\u8be5\u8bbe\u8ba1\u7528\u4e8e\u63a2\u7d22\u4e00\u7cfb\u5217\u884c\u4e3a\u5047\u8bbe\uff0c\u4ee5\u8bc4\u4f30\u4ee3\u7406\u7684\u5bf9\u8bdd\u884c\u4e3a\u662f\u5426\u4e0e\u5176\u63ed\u793a\u7684\u5185\u90e8\u72b6\u6001\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u4e0d\u540c\u89c4\u6a21\u7684LLM\u90fd\u5b58\u5728\u663e\u8457\u7684\u5185\u90e8\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u5c3d\u7ba1LLM\u4ee3\u7406\u53ef\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u5bf9\u5e94\u7684\u54cd\u5e94\uff0c\u4f46\u5b83\u4eec\u672a\u80fd\u4fdd\u6301\u5185\u90e8\u4e00\u81f4\u6027\u3002\u8fd9\u8868\u660e\uff0c\u5728\u4eba\u7c7b\u4e3b\u4f53\u7814\u7a76\u4e2d\uff0cLLM\u4f5c\u4e3a\u771f\u5b9e\u53c2\u4e0e\u8005\u7684\u66ff\u4ee3\u54c1\uff0c\u5176\u80fd\u529b\u5b58\u5728\u5173\u952e\u7f3a\u9677\u3002"}}
{"id": "2509.03662", "pdf": "https://arxiv.org/pdf/2509.03662", "abs": "https://arxiv.org/abs/2509.03662", "authors": ["Ali Noori", "Somya Mohanty", "Prashanti Manda"], "title": "Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV", "categories": ["cs.CL"], "comment": null, "summary": "Clinical notes contain rich clinical narratives but their unstructured format\nposes challenges for large-scale analysis. Standardized terminologies such as\nSNOMED CT improve interoperability, yet understanding how concepts relate\nthrough co-occurrence and semantic similarity remains underexplored. In this\nstudy, we leverage the MIMIC-IV database to investigate the relationship\nbetween SNOMED CT concept co-occurrence patterns and embedding-based semantic\nsimilarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained\nembeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently\nco-occurring concepts are also semantically close, whether embeddings can\nsuggest missing concepts, and how these relationships evolve temporally and\nacross specialties. Our analyses reveal that while co-occurrence and semantic\nsimilarity are weakly correlated, embeddings capture clinically meaningful\nassociations not always reflected in documentation frequency. Embedding-based\nsuggestions frequently matched concepts later documented, supporting their\nutility for augmenting clinical annotations. Clustering of concept embeddings\nyielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular\nconditions) that map to patient phenotypes and care patterns. Finally,\nco-occurrence patterns linked to outcomes such as mortality and readmission\ndemonstrate the practical utility of this approach. Collectively, our findings\nhighlight the complementary value of co-occurrence statistics and semantic\nembeddings in improving documentation completeness, uncovering latent clinical\nrelationships, and informing decision support and phenotyping applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528MIMIC-IV\u6570\u636e\uff0c\u901a\u8fc7\u5171\u73b0\u6a21\u5f0f\uff08NPMI\uff09\u548c\u8bed\u4e49\u5d4c\u5165\uff08ClinicalBERT, BioBERT\uff09\u5206\u6790SNOMED CT\u6982\u5ff5\u95f4\u7684\u5173\u7cfb\u3002\u53d1\u73b0\u5171\u73b0\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u5173\u8054\u8f83\u5f31\uff0c\u4f46\u5d4c\u5165\u80fd\u6355\u83b7\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u5173\u8054\u5e76\u9884\u6d4b\u7f3a\u5931\u6982\u5ff5\uff0c\u5bf9\u6539\u5584\u6587\u6863\u3001\u63ed\u793a\u6f5c\u5728\u5173\u7cfb\u548c\u652f\u6301\u51b3\u7b56\u5177\u6709\u4e92\u8865\u4ef7\u503c\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u7684\u975e\u7ed3\u6784\u5316\u7279\u6027\u963b\u788d\u4e86\u5927\u89c4\u6a21\u5206\u6790\u3002\u5c3d\u7ba1SNOMED CT\u7b49\u6807\u51c6\u5316\u672f\u8bed\u63d0\u9ad8\u4e86\u4e92\u64cd\u4f5c\u6027\uff0c\u4f46\u6982\u5ff5\u95f4\u7684\u5171\u73b0\u6a21\u5f0f\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u8fd9\u4e24\u79cd\u5173\u7cfb\u3002", "method": "\u7814\u7a76\u5229\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u6807\u51c6\u5316\u9010\u70b9\u4e92\u4fe1\u606f\uff08NPMI\uff09\u5206\u6790SNOMED CT\u6982\u5ff5\u7684\u5171\u73b0\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\uff08\u5982ClinicalBERT\u3001BioBERT\uff09\u8ba1\u7b97\u6982\u5ff5\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u7814\u7a76\u8003\u5bdf\u4e86\u9891\u7e41\u5171\u73b0\u6982\u5ff5\u4e0e\u8bed\u4e49\u63a5\u8fd1\u5ea6\u7684\u5173\u7cfb\u3001\u5d4c\u5165\u6a21\u578b\u5bf9\u7f3a\u5931\u6982\u5ff5\u7684\u5efa\u8bae\u80fd\u529b\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5173\u7cfb\u5982\u4f55\u968f\u65f6\u95f4\u6216\u4e13\u4e1a\u9886\u57df\u6f14\u53d8\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u6982\u5ff5\u5d4c\u5165\u805a\u7c7b\u5206\u6790\u548c\u5171\u73b0\u6a21\u5f0f\u4e0e\u4e34\u5e8a\u7ed3\u5c40\uff08\u5982\u6b7b\u4ea1\u7387\u548c\u518d\u5165\u9662\uff09\u7684\u5173\u8054\u6027\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5171\u73b0\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u4e4b\u95f4\u5b58\u5728\u5f31\u76f8\u5173\u6027\uff0c\u4f46\u5d4c\u5165\u80fd\u591f\u6355\u6349\u5230\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u5173\u8054\uff0c\u800c\u8fd9\u4e9b\u5173\u8054\u4e0d\u603b\u662f\u901a\u8fc7\u6587\u6863\u9891\u7387\u4f53\u73b0\u3002\u57fa\u4e8e\u5d4c\u5165\u7684\u5efa\u8bae\u7ecf\u5e38\u4e0e\u540e\u7eed\u8bb0\u5f55\u7684\u6982\u5ff5\u76f8\u7b26\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u589e\u5f3a\u4e34\u5e8a\u6807\u6ce8\u65b9\u9762\u7684\u6548\u7528\u3002\u6982\u5ff5\u5d4c\u5165\u7684\u805a\u7c7b\u4ea7\u751f\u4e86\u8fde\u8d2f\u7684\u4e34\u5e8a\u4e3b\u9898\uff08\u5982\u75c7\u72b6\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u8bca\u65ad\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\uff09\uff0c\u8fd9\u4e9b\u4e3b\u9898\u4e0e\u60a3\u8005\u8868\u578b\u548c\u62a4\u7406\u6a21\u5f0f\u76f8\u5bf9\u5e94\u3002\u6700\u540e\uff0c\u4e0e\u6b7b\u4ea1\u7387\u548c\u518d\u5165\u9662\u7b49\u4e34\u5e8a\u7ed3\u5c40\u76f8\u5173\u7684\u5171\u73b0\u6a21\u5f0f\u4e5f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5171\u73b0\u7edf\u8ba1\u548c\u8bed\u4e49\u5d4c\u5165\u5728\u63d0\u9ad8\u6587\u6863\u5b8c\u6574\u6027\u3001\u63ed\u793a\u6f5c\u5728\u4e34\u5e8a\u5173\u7cfb\u4ee5\u53ca\u4e3a\u51b3\u7b56\u652f\u6301\u548c\u8868\u578b\u5e94\u7528\u63d0\u4f9b\u4fe1\u606f\u65b9\u9762\u7684\u4e92\u8865\u4ef7\u503c\u3002"}}
{"id": "2509.03794", "pdf": "https://arxiv.org/pdf/2509.03794", "abs": "https://arxiv.org/abs/2509.03794", "authors": ["Juhun Lee", "Simon S. Woo"], "title": "Fitting Image Diffusion Models on Video Datasets", "categories": ["cs.CV"], "comment": "ICCV25 Workshop", "summary": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u89c6\u9891\u5e27\u65f6\u95f4\u4fe1\u606f\u6539\u8fdb\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u6536\u655b\u3001\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u57fa\u4e8e\u72ec\u7acb\u91c7\u6837\u7684\u9759\u6001\u56fe\u50cf\u8bad\u7ec3\uff0c\u5728\u6355\u6349\u65f6\u95f4\u4e16\u754c\u65f6\u5b58\u5728\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3001\u5206\u5e03\u8986\u76d6\u6709\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u8fde\u7eed\u89c6\u9891\u5e27\u4e2d\u56fa\u6709\u7684\u65f6\u95f4\u5f52\u7eb3\u504f\u5dee\u6765\u6539\u8fdb\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u6b64\u65b9\u6cd5\u65e0\u9700\u5bf9\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u6269\u6563\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728HandCo\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e862\u500d\u4ee5\u4e0a\uff0c\u5e76\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u66f4\u4f4e\u7684FID\u3002\u5b83\u8fd8\u901a\u8fc7\u9f13\u52b1\u6a21\u578b\u6355\u6349\u6709\u610f\u4e49\u7684\u65f6\u95f4\u53d8\u5316\u6765\u63d0\u9ad8\u751f\u6210\u591a\u6837\u6027\u3002\u4f18\u5316\u5206\u6790\u8868\u660e\uff0c\u8be5\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8fde\u7eed\u89c6\u9891\u5e27\u4e2d\u7684\u65f6\u95f4\u5f52\u7eb3\u504f\u5dee\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u3001\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6355\u6349\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u4efb\u52a1\u3002"}}
{"id": "2509.03707", "pdf": "https://arxiv.org/pdf/2509.03707", "abs": "https://arxiv.org/abs/2509.03707", "authors": ["Qiyuan Chen", "Raed Al Kontar"], "title": "Online Learning of Optimal Sequential Testing Policies", "categories": ["cs.LG"], "comment": null, "summary": "This paper studies an online learning problem that seeks optimal testing\npolicies for a stream of subjects, each of whom can be evaluated through a\nsequence of candidate tests drawn from a common pool. We refer to this problem\nas the Online Testing Problem (OTP). Although conducting every candidate test\nfor a subject provides more information, it is often preferable to select only\na subset when tests are correlated and costly, and make decisions with partial\ninformation. If the joint distribution of test outcomes were known, the problem\ncould be cast as a Markov Decision Process (MDP) and solved exactly. In\npractice, this distribution is unknown and must be learned online as subjects\nare tested. When a subject is not fully tested, the resulting missing data can\nbias estimates, making the problem fundamentally harder than standard episodic\nMDPs. We prove that the minimax regret must scale at least as\n$\\Omega(T^{\\frac{2}{3}})$, in contrast to the $\\Theta(\\sqrt{T})$ rate in\nepisodic MDPs, revealing the difficulty introduced by missingness. This\nelevated lower bound is then matched by an Explore-Then-Commit algorithm whose\ncumulative regret is $\\tilde{O}(T^{\\frac{2}{3}})$ for both discrete and\nGaussian distributions. To highlight the consequence of missingness-dependent\nrewards in OTP, we study a variant called the Online Cost-sensitive Maximum\nEntropy Sampling Problem, where rewards are independent of missing data. This\nstructure enables an iterative-elimination algorithm that achieves\n$\\tilde{O}(\\sqrt{T})$ regret, breaking the $\\Omega(T^{\\frac{2}{3}})$ lower\nbound for OTP. Numerical results confirm our theory in both settings. Overall,\nthis work deepens the understanding of the exploration--exploitation trade-off\nunder missing data and guides the design of efficient sequential testing\npolicies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u7ebf\u6d4b\u8bd5\u95ee\u9898\uff08OTP\uff09\uff0c\u65e8\u5728\u5728\u6d4b\u8bd5\u7ed3\u679c\u5206\u5e03\u672a\u77e5\u4e14\u5b58\u5728\u7f3a\u5931\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8bbe\u8ba1\u6700\u4f18\u7684\u5728\u7ebf\u6d4b\u8bd5\u7b56\u7565\u3002\u7814\u7a76\u53d1\u73b0\u7f3a\u5931\u6570\u636e\u663e\u8457\u589e\u52a0\u4e86\u5b66\u4e60\u96be\u5ea6\uff0c\u5bfc\u81f4\u6bd4\u6807\u51c6MDP\u66f4\u9ad8\u7684\u540e\u6094\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u5339\u914d\u8be5\u4e0b\u754c\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u7279\u5b9a\u7ed3\u6784\u4e0b\u53ef\u8fbe\u5230\u7684\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u6d41\u5f0f\u4e3b\u4f53\u6d4b\u8bd5\u4e2d\uff0c\u9700\u8981\u6709\u6548\u9009\u62e9\u5e76\u5b66\u4e60\u6700\u4f18\u6d4b\u8bd5\u7b56\u7565\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u5177\u6709\u76f8\u5173\u6027\u548c\u6210\u672c\uff0c\u4e14\u6d4b\u8bd5\u7ed3\u679c\u7684\u8054\u5408\u5206\u5e03\u901a\u5e38\u672a\u77e5\u3002\u5f53\u4e3b\u4f53\u672a\u88ab\u5b8c\u5168\u6d4b\u8bd5\u65f6\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u7f3a\u5931\u6570\u636e\u4f1a\u4f7f\u4f30\u8ba1\u4ea7\u751f\u504f\u5dee\uff0c\u4f7f\u5f97\u95ee\u9898\u6bd4\u6807\u51c6\u7684\u5206\u96c6\u5f0f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u63a2\u7d22-\u5229\u7528\u6743\u8861\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7b56\u7565\u3002", "method": "\u672c\u6587\u9996\u5148\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u7ebf\u6d4b\u8bd5\u95ee\u9898\uff08OTP\uff09\u5728\u7f3a\u5931\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u6700\u5c0f\u6700\u5927\u540e\u6094\u4e0b\u754c\u3002\u63a5\u7740\uff0c\u9488\u5bf9\u4e00\u822cOTP\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5148\u63a2\u7d22\u540e\u63d0\u4ea4\u201d\uff08Explore-Then-Commit\uff09\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u7a81\u51fa\u7f3a\u5931\u6570\u636e\u5bf9\u5956\u52b1\u4f9d\u8d56\u6027\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u4e86\u4e00\u4e2a\u5956\u52b1\u72ec\u7acb\u4e8e\u7f3a\u5931\u6570\u636e\u7684\u53d8\u4f53\u95ee\u9898\uff08\u5728\u7ebf\u6210\u672c\u654f\u611f\u6700\u5927\u71b5\u91c7\u6837\u95ee\u9898\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fed\u4ee3\u6d88\u9664\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u7531\u4e8e\u7f3a\u5931\u6570\u636e\u7684\u5b58\u5728\uff0cOTP\u7684\u6700\u5c0f\u6700\u5927\u540e\u6094\u81f3\u5c11\u4e3a $\\Omega(T^{\\frac{2}{3}})$\uff0c\u8fdc\u9ad8\u4e8e\u6807\u51c6\u5206\u96c6\u5f0fMDP\u7684 $\\Theta(\\sqrt{T})$ \u901f\u7387\u3002\u63d0\u51fa\u7684\u201c\u5148\u63a2\u7d22\u540e\u63d0\u4ea4\u201d\u7b97\u6cd5\u5b9e\u73b0\u4e86 $\\tilde{O}(T^{\\frac{2}{3}})$ \u7684\u7d2f\u79ef\u540e\u6094\u7387\uff0c\u5339\u914d\u4e86\u8be5\u4e0b\u754c\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u5956\u52b1\u72ec\u7acb\u4e8e\u7f3a\u5931\u6570\u636e\u7684\u53d8\u4f53\u95ee\u9898\uff0c\u8fed\u4ee3\u6d88\u9664\u7b97\u6cd5\u8fbe\u5230\u4e86 $\\tilde{O}(\\sqrt{T})$ \u7684\u540e\u6094\u7387\uff0c\u6210\u529f\u7a81\u7834\u4e86OTP\u7684 $\\Omega(T^{\\frac{2}{3}})$ \u4e0b\u754c\uff0c\u8868\u660e\u7279\u5b9a\u95ee\u9898\u7ed3\u6784\u53ef\u4ee5\u7f13\u89e3\u7f3a\u5931\u6570\u636e\u7684\u5f71\u54cd\u3002\u6570\u503c\u7ed3\u679c\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7406\u8bba\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u7f3a\u5931\u6570\u636e\u60c5\u5883\u4e0b\u63a2\u7d22-\u5229\u7528\u6743\u8861\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684\u5e8f\u5217\u6d4b\u8bd5\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u5b83\u63ed\u793a\u4e86\u7f3a\u5931\u6570\u636e\u5bf9\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\u7684\u6839\u672c\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5176\u5f71\u54cd\u4f30\u8ba1\u548c\u5956\u52b1\u65f6\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u95ee\u9898\u7ed3\u6784\u4e0b\uff08\u4f8b\u5982\u5956\u52b1\u72ec\u7acb\u4e8e\u7f3a\u5931\u6570\u636e\uff09\u5982\u4f55\u80fd\u663e\u8457\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2509.03768", "pdf": "https://arxiv.org/pdf/2509.03768", "abs": "https://arxiv.org/abs/2509.03768", "authors": ["Connor Walker", "Koorosh Aslansefat", "Mohammad Naveed Akram", "Yiannis Papadopoulos"], "title": "RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet\nconventional Large Language Models (LLMs) often fail when confronted with\nhighly specialised or unexpected scenarios. We introduce RAGuard, an enhanced\nRetrieval-Augmented Generation (RAG) framework that explicitly integrates\nsafety-critical documents alongside technical manuals.By issuing parallel\nqueries to two indices and allocating separate retrieval budgets for knowledge\nand safety, RAGuard guarantees both technical depth and safety coverage. We\nfurther develop a SafetyClamp extension that fetches a larger candidate pool,\n\"hard-clamping\" exact slot guarantees to safety. We evaluate across sparse\n(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,\nmeasuring Technical Recall@K and Safety Recall@K. Both proposed extensions of\nRAG show an increase in Safety Recall@K from almost 0\\% in RAG to more than\n50\\% in RAGuard, while maintaining Technical Recall above 60\\%. These results\ndemonstrate that RAGuard and SafetyClamp have the potential to establish a new\nstandard for integrating safety assurance into LLM-powered decision support in\ncritical maintenance contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAGuard\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5b89\u5168\u5173\u952e\u6587\u6863\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6d77\u4e0a\u98ce\u7535\u7ef4\u62a4\u4e2d\u5b89\u5168\u53ec\u56de\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6280\u672f\u53ec\u56de\u7387\u3002", "motivation": "\u5728\u6d77\u4e0a\u98ce\u7535(OSW)\u7ef4\u62a4\u4e2d\uff0c\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u9762\u5bf9\u9ad8\u5ea6\u4e13\u4e1a\u5316\u6216\u610f\u5916\u60c5\u51b5\u65f6\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u4e86RAGuard\uff0c\u4e00\u4e2a\u589e\u5f3a\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u5c06\u5b89\u5168\u5173\u952e\u6587\u6863\u4e0e\u6280\u672f\u624b\u518c\u96c6\u6210\u3002\u901a\u8fc7\u5bf9\u4e24\u4e2a\u7d22\u5f15\uff08\u77e5\u8bc6\u548c\u5b89\u5168\uff09\u8fdb\u884c\u5e76\u884c\u67e5\u8be2\uff0c\u5e76\u4e3a\u77e5\u8bc6\u548c\u5b89\u5168\u5206\u914d\u72ec\u7acb\u7684\u68c0\u7d22\u9884\u7b97\uff0c\u4ee5\u786e\u4fdd\u6280\u672f\u6df1\u5ea6\u548c\u5b89\u5168\u8986\u76d6\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86SafetyClamp\u6269\u5c55\uff0c\u4ee5\u83b7\u53d6\u66f4\u5927\u7684\u5019\u9009\u6c60\uff0c\u5e76\u201c\u786c\u6027\u56fa\u5b9a\u201d\u5b89\u5168\u65b9\u9762\u7684\u7cbe\u786e\u63d2\u69fd\u4fdd\u969c\u3002\u901a\u8fc7\u7a00\u758f(BM25)\u3001\u5bc6\u96c6(Dense Passage Retrieval)\u548c\u6df7\u5408\u68c0\u7d22\u8303\u5f0f\u8fdb\u884c\u8bc4\u4f30\uff0c\u6d4b\u91cf\u6280\u672f\u53ec\u56de\u7387@K\u548c\u5b89\u5168\u53ec\u56de\u7387@K\u3002", "result": "RAGuard\u548cSafetyClamp\u8fd9\u4e24\u4e2aRAG\u7684\u6269\u5c55\u63d0\u6848\uff0c\u5c06\u5b89\u5168\u53ec\u56de\u7387@K\u4eceRAG\u7684\u51e0\u4e4e0%\u63d0\u9ad8\u523050%\u4ee5\u4e0a\uff0c\u540c\u65f6\u5c06\u6280\u672f\u53ec\u56de\u7387\u4fdd\u6301\u572860%\u4ee5\u4e0a\u3002", "conclusion": "RAGuard\u548cSafetyClamp\u6709\u6f5c\u529b\u5728\u5173\u952e\u7ef4\u62a4\u573a\u666f\u4e2d\u4e3aLLM\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6574\u5408\u5b89\u5168\u4fdd\u969c\u5efa\u7acb\u65b0\u6807\u51c6\u3002"}}
{"id": "2509.03725", "pdf": "https://arxiv.org/pdf/2509.03725", "abs": "https://arxiv.org/abs/2509.03725", "authors": ["Parush Gera", "Tempestt Neal"], "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLSD\u7684\u57fa\u4e8e\u5ea6\u91cf\u5b66\u4e60\u7684\u5c11\u6837\u672c\u8de8\u9886\u57df\u8de8\u76ee\u6807\u7acb\u573a\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u5dee\u5f02\u6765\u589e\u5f3a\u9886\u57df\u9002\u5e94\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7acb\u573a\u68c0\u6d4b\u6a21\u578b\u5728\u8de8\u9886\u57df\u548c\u8de8\u76ee\u6807\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u4f7f\u6a21\u578b\u6709\u6548\u9002\u5e94\u65b0\u7684\u76ee\u6807\u9886\u57df\u5e76\u4ece\u6709\u9650\u7684\u4f8b\u5b50\u4e2d\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86MLSD\uff08Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5ea6\u91cf\u5b66\u4e60\uff08Metric Learning\uff09\u7ed3\u5408\u4e09\u5143\u7ec4\u635f\u5931\uff08Triplet Loss\uff09\u6765\u6355\u83b7\u7acb\u573a\u76ee\u6807\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u3002\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5224\u522b\u6027\u5d4c\u5165\u7a7a\u95f4\uff0cMLSD\u4f7f\u8de8\u76ee\u6807\u6216\u8de8\u9886\u57df\u7acb\u573a\u68c0\u6d4b\u6a21\u578b\u80fd\u591f\u4ece\u65b0\u76ee\u6807\u57df\u4e2d\u83b7\u53d6\u6709\u7528\u7684\u793a\u4f8b\uff0c\u4ece\u800c\u589e\u5f3a\u9886\u57df\u9002\u5e94\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u591a\u4e2a\u8de8\u76ee\u6807\u548c\u8de8\u9886\u57df\u573a\u666f\u4e2d\u5bf9MLSD\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cMLSD\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u7acb\u573a\u68c0\u6d4b\u6a21\u578b\u4e0a\u5747\u5b9e\u73b0\u4e86\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MLSD\u901a\u8fc7\u5176\u57fa\u4e8e\u5ea6\u91cf\u5b66\u4e60\u7684\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u548c\u8de8\u76ee\u6807\u7acb\u573a\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2509.03800", "pdf": "https://arxiv.org/pdf/2509.03800", "abs": "https://arxiv.org/abs/2509.03800", "authors": ["Yuheng Li", "Yenho Chen", "Yuxiang Lai", "Jike Zhong", "Vanessa Wildman", "Xiaofeng Yang"], "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting", "categories": ["cs.CV"], "comment": null, "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MedVista3D\uff0c\u4e00\u4e2a\u591a\u5c3a\u5ea6\u8bed\u4e49\u589e\u5f3a\u76843D CT\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u653e\u5c04\u8bca\u65ad\u4e2d\u7684\u9519\u8bef\u3002\u901a\u8fc7\u5c40\u90e8\u4e0e\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u53ca\u8bed\u4e49\u5339\u914d\uff0c\u8be5\u6a21\u578b\u5728\u591a\u79cd\u533b\u5b66\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u653e\u5c04\u8bca\u65ad\u9519\u8bef\uff08\u5982\u6f0f\u8bca\u3001\u6ce8\u610f\u529b\u76f2\u533a\u3001\u6c9f\u901a\u5931\u8d25\uff09\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5c24\u5176\u5728\u9700\u8981\u68c0\u67e5\u6570\u767e\u5207\u7247\u76843D\u6210\u50cf\u4e2d\u95ee\u9898\u66f4\u7a81\u51fa\uff0c\u6839\u6e90\u5728\u4e8e\u5c40\u90e8\u5f02\u5e38\u7684\u9057\u6f0f\u3001\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u62a5\u544a\u8bed\u8a00\u7684\u53d8\u5f02\u6027\u3002\u73b0\u67093D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7cbe\u51c6\u5c40\u90e8\u68c0\u6d4b\u3001\u5168\u5c40\u63a8\u7406\u548c\u8bed\u4e49\u4e00\u81f4\u62a5\u544a\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86MedVista3D\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e3D CT\u5206\u6790\u7684\u591a\u5c3a\u5ea6\u8bed\u4e49\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u4e3a\u5b9e\u73b0\u75be\u75c5\u8054\u5408\u68c0\u6d4b\u548c\u6574\u4f53\u89e3\u8bfb\uff0cMedVista3D\u5728\u5168\u5bb9\u79ef\u4e0a\u4e0b\u6587\u5185\u8fdb\u884c\u5c40\u90e8\u548c\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u4ee5\u5b66\u4e60\u7ec6\u7c92\u5ea6\u8868\u793a\u3002\u4e3a\u89e3\u51b3\u62a5\u544a\u53d8\u5f02\u6027\uff0c\u6a21\u578b\u5e94\u7528\u8bed\u8a00\u6a21\u578b\u91cd\u5199\u5e76\u5f15\u5165\u653e\u5c04\u8bed\u4e49\u5339\u914d\u5e93\u4ee5\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u5bf9\u9f50\u3002", "result": "MedVista3D\u5728\u96f6\u6837\u672c\u75be\u75c5\u5206\u7c7b\u3001\u62a5\u544a\u68c0\u7d22\u548c\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u5668\u5b98\u5206\u5272\u548c\u9884\u540e\u9884\u6d4b\u7b49\u4efb\u52a1\u3002", "conclusion": "MedVista3D\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u8bed\u4e49\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u514b\u670d\u4e863D\u653e\u5c04\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3001\u62a5\u544a\u4e00\u81f4\u6027\u53ca\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2509.03709", "pdf": "https://arxiv.org/pdf/2509.03709", "abs": "https://arxiv.org/abs/2509.03709", "authors": ["Allan Salihovic", "Payam Abdisarabshali", "Michael Langberg", "Seyyedali Hosseinalipour"], "title": "From Federated Learning to $\\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks", "categories": ["cs.LG", "cs.AI"], "comment": "6 figures, 12 pages", "summary": "We provide our perspective on $\\mathbb{X}$-Learning ($\\mathbb{X}$L), a novel\ndistributed learning architecture that generalizes and extends the concept of\ndecentralization. Our goal is to present a vision for $\\mathbb{X}$L,\nintroducing its unexplored design considerations and degrees of freedom. To\nthis end, we shed light on the intuitive yet non-trivial connections between\n$\\mathbb{X}$L, graph theory, and Markov chains. We also present a series of\nopen research directions to stimulate further research.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03811", "pdf": "https://arxiv.org/pdf/2509.03811", "abs": "https://arxiv.org/abs/2509.03811", "authors": ["Yongzhi Qi", "Jiaheng Yin", "Jianshen Zhang", "Dongyang Geng", "Zhengyu Chen", "Hao Hu", "Wei Qi", "Zuo-Jun Max Shen"], "title": "Leveraging LLM-Based Agents for Intelligent Supply Chain Planning", "categories": ["cs.AI"], "comment": null, "summary": "In supply chain management, planning is a critical concept. The movement of\nphysical products across different categories, from suppliers to warehouse\nmanagement, to sales, and logistics transporting them to customers, entails the\ninvolvement of many entities. It covers various aspects such as demand\nforecasting, inventory management, sales operations, and replenishment. How to\ncollect relevant data from an e-commerce platform's perspective, formulate\nlong-term plans, and dynamically adjust them based on environmental changes,\nwhile ensuring interpretability, efficiency, and reliability, is a practical\nand challenging problem. In recent years, the development of AI technologies,\nespecially the rapid progress of large language models, has provided new tools\nto address real-world issues. In this work, we construct a Supply Chain\nPlanning Agent (SCPA) framework that can understand domain knowledge,\ncomprehend the operator's needs, decompose tasks, leverage or create new tools,\nand return evidence-based planning reports. We deploy this framework in\nJD.com's real-world scenario, demonstrating the feasibility of LLM-agent\napplications in the supply chain. It effectively reduced labor and improved\naccuracy, stock availability, and other key metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4f9b\u5e94\u94fe\u89c4\u5212\u4ee3\u7406\uff08SCPA\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u7535\u5546\u5e73\u53f0\u4e2d\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5728\u4eac\u4e1c\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "motivation": "\u4f9b\u5e94\u94fe\u89c4\u5212\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5173\u952e\u7684\u95ee\u9898\uff0c\u6d89\u53ca\u6570\u636e\u6536\u96c6\u3001\u957f\u671f\u89c4\u5212\u3001\u52a8\u6001\u8c03\u6574\uff0c\u5e76\u9700\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u800cAI\u6280\u672f\uff08\u7279\u522b\u662fLLM\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4f9b\u5e94\u94fe\u89c4\u5212\u4ee3\u7406\uff08SCPA\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7406\u89e3\u9886\u57df\u77e5\u8bc6\u548c\u64cd\u4f5c\u8005\u9700\u6c42\uff0c\u5206\u89e3\u4efb\u52a1\uff0c\u5229\u7528\u6216\u521b\u5efa\u5de5\u5177\uff0c\u5e76\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u89c4\u5212\u62a5\u544a\u3002", "result": "\u8be5\u6846\u67b6\u5df2\u5728\u4eac\u4e1c\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u90e8\u7f72\uff0c\u8bc1\u660e\u4e86LLM-agent\u5728\u4f9b\u5e94\u94fe\u4e2d\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002\u5b83\u6709\u6548\u51cf\u5c11\u4e86\u52b3\u52a8\u529b\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u5e93\u5b58\u53ef\u7528\u6027\u53ca\u5176\u4ed6\u5173\u952e\u6307\u6807\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff08\u5982SCPA\uff09\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u89c4\u5212\u6311\u6218\uff0c\u4e3a\u7535\u5546\u5e73\u53f0\u7684\u5b9e\u9645\u8fd0\u8425\u5e26\u6765\u4e86\u663e\u8457\u7684\u6539\u8fdb\u548c\u6548\u76ca\u3002"}}
{"id": "2509.03791", "pdf": "https://arxiv.org/pdf/2509.03791", "abs": "https://arxiv.org/abs/2509.03791", "authors": ["Saki Imai", "Mert \u0130nan", "Anthony Sicilia", "Malihe Alikhani"], "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSiLVERScore\u7684\u65b0\u578b\u8bed\u4e49\u611f\u77e5\u5d4c\u5165\u5f0f\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u624b\u8bed\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56de\u8bd1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u8bed\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u56de\u8bd1\uff09\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u624b\u8bed\u7684\u591a\u6a21\u6001\u7279\u6027\uff08\u5982\u9762\u90e8\u8868\u60c5\u3001\u7a7a\u95f4\u8bed\u6cd5\u548c\u97f5\u5f8b\uff09\uff0c\u4e14\u96be\u4ee5\u533a\u5206\u8bc4\u4f30\u9519\u8bef\u662f\u6765\u6e90\u4e8e\u751f\u6210\u6a21\u578b\u8fd8\u662f\u7ffb\u8bd1\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u63d0\u51faSiLVERScore\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u8bed\u4e49\u611f\u77e5\u7684\u5d4c\u5165\u5f0f\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u5728\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bc4\u4f30\u624b\u8bed\u751f\u6210\u6765\u89e3\u51b3\u73b0\u6709\u95ee\u9898\u3002\u5176\u8d21\u732e\u5305\u62ec\uff1a\u8bc6\u522b\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\u3001\u5f15\u5165SiLVERScore\u8fdb\u884c\u8bed\u4e49\u611f\u77e5\u8bc4\u4f30\u3001\u5c55\u793a\u5176\u5bf9\u8bed\u4e49\u548c\u97f5\u5f8b\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u63a2\u8ba8\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6311\u6218\u3002", "result": "\u5728PHOENIX-14T\u548cCSL-Daily\u6570\u636e\u96c6\u4e0a\uff0cSiLVERScore\u5728\u6b63\u786e\u5bf9\u548c\u968f\u673a\u5bf9\u4e4b\u95f4\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u533a\u5206\uff08ROC AUC = 0.99\uff0c\u91cd\u53e0 < 7%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u3002", "conclusion": "SiLVERScore\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u8bed\u4e49\u611f\u77e5\u4e14\u80fd\u6355\u6349\u624b\u8bed\u591a\u6a21\u6001\u7279\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56de\u8bd1\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.03803", "pdf": "https://arxiv.org/pdf/2509.03803", "abs": "https://arxiv.org/abs/2509.03803", "authors": ["Mengyu Gao", "Qiulei Dong"], "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation", "categories": ["cs.CV"], "comment": "ICCV 2025 Accepted", "summary": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaPL\u7684\u56e0\u679c\u6307\u5bfc\u6587\u672c\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u7c92\u5316\u6280\u672f\u589e\u5f3aCLIP\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCLIP\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u65f6\u8868\u73b0\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CaPL\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u5c5e\u6027\u89e3\u8026\u6a21\u5757\uff0c\u5229\u7528\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\u5c06\u89c6\u89c9\u7279\u5f81\u5206\u89e3\u4e3a\u975e\u4e2a\u4f53\u5316\u548c\u4e2a\u4f53\u5316\u5c5e\u6027\uff1b2) \u7c92\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u6574\u5408\u4e0a\u8ff0\u5c5e\u6027\u5e76\u5728\u4e24\u79cd\u56e0\u679c\u63a8\u65ad\u7b56\u7565\u4e0b\u6784\u5efa\u89c6\u89c9\u7c92\u5ea6\uff0c\u4ece\u800c\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u6587\u672c\u63d0\u793a\u3002", "result": "CaPL\u65b9\u6cd5\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "CaPL\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7c92\u5316\u548c\u56e0\u679c\u63a8\u65ad\uff0c\u6709\u6548\u63d0\u5347\u4e86CLIP\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u63d0\u793a\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2509.03733", "pdf": "https://arxiv.org/pdf/2509.03733", "abs": "https://arxiv.org/abs/2509.03733", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Differentiable Entropy Regularization for Geometry and Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a differentiable estimator of range-partition entropy, a recent\nconcept from computational geometry that enables algorithms to adapt to the\n\"sortedness\" of their input. While range-partition entropy provides strong\nguarantees in algorithm design, it has not yet been made accessible to deep\nlearning. In this work, we (i) propose the first differentiable approximation\nof range-partition entropy, enabling its use as a trainable loss or\nregularizer; (ii) design EntropyNet, a neural module that restructures data\ninto low-entropy forms to accelerate downstream instance-optimal algorithms;\nand (iii) extend this principle beyond geometry by applying entropy\nregularization directly to Transformer attention. Across tasks, we demonstrate\nthat differentiable entropy improves efficiency without degrading correctness:\nin geometry, our method achieves up to $4.1\\times$ runtime speedups with\nnegligible error ($<0.2%$); in deep learning, it induces structured attention\npatterns that yield 6% higher accuracy at 80% sparsity compared to L1\nbaselines. Our theoretical analysis provides approximation bounds for the\nestimator, and extensive ablations validate design choices. These results\nsuggest that entropy-bounded computation is not only theoretically elegant but\nalso a practical mechanism for adaptive learning, efficiency, and structured\nrepresentation.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u8303\u56f4\u5212\u5206\u71b5\u7684\u53ef\u5fae\u5206\u4f30\u8ba1\u5668\uff0c\u4f7f\u5176\u80fd\u591f\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u635f\u5931\u6216\u6b63\u5219\u5316\u9879\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u51e0\u4f55\uff0c\u4ee5\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u548c\u5b9e\u73b0\u81ea\u9002\u5e94\u5b66\u4e60\u3002", "motivation": "\u8303\u56f4\u5212\u5206\u71b5\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u5c1a\u672a\u88ab\u5f15\u5165\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\uff0c\u9650\u5236\u4e86\u5176\u5728\u63d0\u9ad8\u6548\u7387\u548c\u81ea\u9002\u5e94\u6027\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": ["\u63d0\u51fa\u4e86\u9996\u4e2a\u53ef\u5fae\u5206\u7684\u8303\u56f4\u5212\u5206\u71b5\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f7f\u5176\u53ef\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u7684\u635f\u5931\u51fd\u6570\u6216\u6b63\u5219\u5316\u9879\u4f7f\u7528\u3002", "\u8bbe\u8ba1\u4e86EntropyNet\u795e\u7ecf\u6a21\u5757\uff0c\u7528\u4e8e\u5c06\u6570\u636e\u91cd\u6784\u4e3a\u4f4e\u71b5\u5f62\u5f0f\uff0c\u4ece\u800c\u52a0\u901f\u4e0b\u6e38\u7684\u5b9e\u4f8b\u6700\u4f18\u7b97\u6cd5\u3002", "\u5c06\u71b5\u6b63\u5219\u5316\u539f\u5219\u6269\u5c55\u5230\u51e0\u4f55\u9886\u57df\u4e4b\u5916\uff0c\u76f4\u63a5\u5e94\u7528\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002"], "result": ["\u5728\u51e0\u4f55\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.1\u500d\u7684\u8fd0\u884c\u65f6\u52a0\u901f\uff0c\u4e14\u8bef\u5dee\u53ef\u5ffd\u7565\uff08<0.2%\uff09\u3002", "\u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u5b83\u8bf1\u5bfc\u4e86\u7ed3\u6784\u5316\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u572880%\u7a00\u758f\u5ea6\u4e0b\u6bd4L1\u57fa\u7ebf\u63d0\u9ad8\u4e866%\u7684\u51c6\u786e\u6027\u3002", "\u63d0\u4f9b\u4e86\u4f30\u8ba1\u5668\u7684\u8fd1\u4f3c\u754c\u9650\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u3002"], "conclusion": "\u71b5\u7ea6\u675f\u8ba1\u7b97\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u4f18\u96c5\uff0c\u800c\u4e14\u662f\u5b9e\u73b0\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u63d0\u9ad8\u6548\u7387\u548c\u83b7\u5f97\u7ed3\u6784\u5316\u8868\u793a\u7684\u4e00\u79cd\u5b9e\u7528\u673a\u5236\u3002"}}
{"id": "2509.03817", "pdf": "https://arxiv.org/pdf/2509.03817", "abs": "https://arxiv.org/abs/2509.03817", "authors": ["Wei Yang", "Jesse Thomason"], "title": "Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems of large language models (LLMs) show promise for complex\nreasoning, but their effectiveness is often limited by fixed collaboration\nprotocols. These frameworks typically focus on macro-level orchestration while\noverlooking agents' internal deliberative capabilities. This critical\nmeta-cognitive blindspot treats agents as passive executors unable to adapt\ntheir strategy based on internal cognitive states like uncertainty or\nconfidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where\nagents learn a decentralized policy over a set of high-level meta-cognitive\nactions: Persist, Refine, and Concede. To overcome the instability of\ntraditional policy gradients in this setting, we develop SoftRankPO, a novel\nreinforcement learning algorithm. SoftRankPO stabilizes training by shaping\nadvantages based on the rank of rewards mapped through smooth normal quantiles,\nmaking the learning process robust to reward variance. Experiments show that\nMPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across\nfive mathematical and general reasoning benchmarks compared to six\nstate-of-the-art heuristic and learning-based multi-agent reasoning algorithms.\nOur work presents a paradigm for learning adaptive, meta-cognitive policies for\nmulti-agent LLM systems, shifting the focus from designing fixed protocols to\nlearning dynamic, deliberative strategies.", "AI": {"tldr": "\u63d0\u51fa\u5143\u7b56\u7565\u5ba1\u8bae\u6846\u67b6\uff08MPDF\uff09\uff0c\u4f7fLLM\u591a\u667a\u80fd\u4f53\u80fd\u5b66\u4e60\u52a8\u6001\u3001\u5143\u8ba4\u77e5\u7684\u534f\u4f5c\u7b56\u7565\uff0c\u514b\u670d\u56fa\u5b9a\u534f\u8bae\u9650\u5236\u3002\u901a\u8fc7\u65b0\u578bRL\u7b97\u6cd5SoftRankPO\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u76f8\u8f83\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53474-5%\u3002", "motivation": "\u5f53\u524dLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u50f5\u5316\u7684\u534f\u4f5c\u534f\u8bae\uff0c\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u5185\u90e8\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u6216\u4fe1\u5fc3\u7684\u5143\u8ba4\u77e5\u5ba1\u8bae\u80fd\u529b\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u8c03\u6574\u7b56\u7565\u3002", "method": "\u5f15\u5165\u5143\u7b56\u7565\u5ba1\u8bae\u6846\u67b6\uff08MPDF\uff09\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u7531\u201c\u575a\u6301\u201d\u3001\u201c\u4f18\u5316\u201d\u548c\u201c\u653e\u5f03\u201d\u7b49\u5143\u8ba4\u77e5\u9ad8\u7ea7\u52a8\u4f5c\u7ec4\u6210\u7684\u5206\u6563\u7b56\u7565\u3002\u4e3a\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5f00\u53d1\u4e86\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5SoftRankPO\uff0c\u901a\u8fc7\u5c06\u5956\u52b1\u6620\u5c04\u4e3a\u5e73\u6ed1\u6b63\u6001\u5206\u4f4d\u6570\u540e\uff0c\u57fa\u4e8e\u5176\u6392\u540d\u6765\u5851\u9020\u4f18\u52bf\uff0c\u589e\u5f3a\u8bad\u7ec3\u5bf9\u5956\u52b1\u65b9\u5dee\u7684\u9c81\u68d2\u6027\u3002", "result": "MPDF\u7ed3\u5408SoftRankPO\u5728\u4e94\u9879\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u516d\u79cd\u6700\u5148\u8fdb\u7684\u542f\u53d1\u5f0f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u9ad8\u4e864-5%\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5b66\u4e60\u81ea\u9002\u5e94\u7684\u5143\u8ba4\u77e5\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u9884\u8bbe\u56fa\u5b9a\u534f\u8bae\u8f6c\u5411\u5b66\u4e60\u52a8\u6001\u3001\u5ba1\u8bae\u6027\u7684\u667a\u80fd\u4f53\u7b56\u7565\u3002"}}
{"id": "2509.03805", "pdf": "https://arxiv.org/pdf/2509.03805", "abs": "https://arxiv.org/abs/2509.03805", "authors": ["Saki Imai", "Mert \u0130nan", "Anthony Sicilia", "Malihe Alikhani"], "title": "Measuring How (Not Just Whether) VLMs Build Common Ground", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u5305\u542b\u56db\u4e2a\u6307\u6807\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4ea4\u4e92\u5f0f\u63a5\u5730\u8bed\u5883\u4e2d\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6d4b\u8bd5\u7684VLMs\u4e0e\u4eba\u7c7b\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5373\u4f7f\u4efb\u52a1\u6210\u529f\u4e5f\u4e0d\u610f\u5473\u7740\u6210\u529f\u7684\u63a5\u5730\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u591a\u9650\u4e8e\u5355\u8f6e\u6216\u95ee\u7b54\u8bbe\u7f6e\uff0c\u672a\u80fd\u6355\u6349\u201c\u63a5\u5730\u201d\uff08grounding\uff09\u8fd9\u4e00\u4e92\u52a8\u6027\u8fc7\u7a0b\u3002\u63a5\u5730\u6d89\u53ca\u901a\u8fc7\u6301\u7eed\u6c9f\u901a\u9010\u6b65\u5efa\u7acb\u5171\u4eab\u7406\u89e3\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30VLM\u5728\u6b64\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u5957\u5305\u542b\u56db\u4e2a\u6307\u6807\u7684\u8bc4\u4f30\u4f53\u7cfb\uff08\u63a5\u5730\u6548\u7387\u3001\u5185\u5bb9\u5bf9\u9f50\u3001\u8bcd\u6c47\u9002\u5e94\u548c\u7c7b\u4eba\u6027\uff09\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30VLM\u5728\u4ea4\u4e92\u5f0f\u63a5\u5730\u8bed\u5883\u4e0b\u7684\u6027\u80fd\u3002\u8be5\u4f53\u7cfb\u88ab\u5e94\u7528\u4e8e\u4e09\u4e2a\u4e13\u6709VLM\u7684150\u6b21\u4e92\u52a8\u6307\u79f0\u6e38\u620f\uff08referential games\uff09\u81ea\u73a9\u4f1a\u8bdd\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5bf9\u8bdd\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u6240\u6709\u4e09\u4e2aVLM\u5728\u81f3\u5c11\u4e09\u4e2a\u6307\u6807\u4e0a\u90fd\u504f\u79bb\u4e86\u4eba\u7c7b\u6a21\u5f0f\uff0c\u5176\u4e2dGPT4o-mini\u5728\u603b\u4f53\u4e0a\u6700\u63a5\u8fd1\u4eba\u7c7b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4efb\u52a1\u6210\u529f\u5f97\u5206\u5e76\u4e0d\u80fd\u5b8c\u5168\u6307\u793a\u6210\u529f\u7684\u63a5\u5730\uff0c\u4e14\u9ad8\u56fe\u50cf-\u8bdd\u8bed\u5bf9\u9f50\u4e5f\u5e76\u4e0d\u4e00\u5b9a\u9884\u793a\u7740\u4efb\u52a1\u6210\u529f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6307\u6807\u5957\u4ef6\u548c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765VLM\u63a5\u5730\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7a81\u51fa\u4e86\u5f53\u524dVLM\u5728\u4ea4\u4e92\u5f0f\u63a5\u5730\u65b9\u9762\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.03808", "pdf": "https://arxiv.org/pdf/2509.03808", "abs": "https://arxiv.org/abs/2509.03808", "authors": ["Huanan Li", "Rui Fan", "Juntao Guan", "Weidong Hao", "Lai Rui", "Tong Wu", "Yikai Wang", "Lin Gu"], "title": "EGTM: Event-guided Efficient Turbulence Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65b0\u578b\u6e4d\u6d41\u7f13\u89e3\uff08TM\uff09\u65b9\u6cd5EGTM\uff0c\u901a\u8fc7\u201c\u4e8b\u4ef6\u5e78\u8fd0\u6d1e\u5bdf\u201d\u548c\u4e8b\u4ef6\u6d41\u878d\u5408\uff0c\u5728\u5927\u5e45\u63d0\u9ad8\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u56fe\u50cf\u6062\u590d\u8d28\u91cf\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u9a71\u52a8TM\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e27\u76f8\u673a\u7684\u6df1\u5ea6\u5b66\u4e60TM\u65b9\u6cd5\u56e0\u5e27\u7387\u9650\u5236\u548c\u7c97\u7c92\u5ea6\u52a8\u6001\u5b66\u4e60\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u4f4e\u4e0b\u3002\u4e8b\u4ef6\u76f8\u673a\u51ed\u501f\u5176\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7a00\u758f\u5f02\u6b65\u6210\u50cf\u673a\u5236\uff0c\u6709\u671b\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "1. \u63d0\u51fa\u4e86\u201c\u4e8b\u4ef6\u5e78\u8fd0\u6d1e\u5bdf\u201d\uff0c\u63ed\u793a\u6e4d\u6d41\u7578\u53d8\u4e0e\u4e8b\u4ef6\u6d41\u9006\u65f6\u7a7a\u5206\u5e03\u7684\u76f8\u5173\u6027\u30022. \u57fa\u4e8e\u6b64\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86EGTM\u6846\u67b6\uff0c\u4ece\u5608\u6742\u7684\u6e4d\u6d41\u4e8b\u4ef6\u4e2d\u63d0\u53d6\u50cf\u7d20\u7ea7\u53ef\u9760\u7684\u65e0\u6e4d\u6d41\u5f15\u5bfc\u4fe1\u606f\uff0c\u7528\u4e8e\u65f6\u95f4\u5e78\u8fd0\u878d\u5408\u30023. \u6784\u5efa\u4e86\u9996\u4e2a\u6e4d\u6d41\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\uff0c\u8d21\u732e\u4e86\u7b2c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u9a71\u52a8TM\u6570\u636e\u96c6\u3002", "result": "\u76f8\u6bd4\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u5728\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u590d\u6742\u5ea6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86710\u500d\u3001214\u500d\u548c224\u500d\u7684\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u6062\u590d\u8d28\u91cf\u4e0a\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\uff08PSNR\u63d0\u53470.94\uff0cSSIM\u63d0\u53470.08\uff09\u3002", "conclusion": "\u5c06\u4e8b\u4ef6\u6a21\u6001\u5f15\u5165\u6e4d\u6d41\u7f13\u89e3\u4efb\u52a1\u5177\u6709\u5de8\u5927\u7684\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u80fd\u5b9e\u73b0\u9886\u5148\u7684\u56fe\u50cf\u6062\u590d\u8d28\u91cf\u3002"}}
{"id": "2509.03738", "pdf": "https://arxiv.org/pdf/2509.03738", "abs": "https://arxiv.org/abs/2509.03738", "authors": ["Bahareh Tolooshams", "Ailsa Shen", "Anima Anandkumar"], "title": "Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "comment": "Tolooshams and Shen has equal contribution. preprint", "summary": "We frame the problem of unifying representations in neural models as one of\nsparse model recovery and introduce a framework that extends sparse\nautoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,\nenabling mechanistic interpretability of large neural operators (NO). While the\nPlatonic Representation Hypothesis suggests that neural networks converge to\nsimilar representations across architectures, the representational properties\nof neural operators remain underexplored despite their growing importance in\nscientific computing. We compare the inference and training dynamics of SAEs,\nlifted-SAE, and SAE neural operators. We highlight how lifting and operator\nmodules introduce beneficial inductive biases, enabling faster recovery,\nimproved recovery of smooth concepts, and robust inference across varying\nresolutions, a property unique to neural operators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6269\u5c55\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5176\u5e94\u7528\u4e8e\u63d0\u5347\u7a7a\u95f4\u548c\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u795e\u7ecf\u7b97\u5b50\uff08NO\uff09\u7684\u673a\u68b0\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u53d1\u73b0\u8be5\u6846\u67b6\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u5177\u6709\u66f4\u5feb\u7684\u6062\u590d\u901f\u5ea6\u3001\u66f4\u597d\u7684\u5e73\u6ed1\u6982\u5ff5\u6062\u590d\u80fd\u529b\u4ee5\u53ca\u8de8\u5206\u8fa8\u7387\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7b97\u5b50\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u8868\u793a\u5c5e\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u963b\u788d\u4e86\u5bf9\u5927\u578b\u795e\u7ecf\u7b97\u5b50\u8fdb\u884c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u8868\u793a\u95ee\u9898\uff0c\u89e3\u51b3\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5c06\u795e\u7ecf\u6a21\u578b\u4e2d\u7edf\u4e00\u8868\u793a\u7684\u95ee\u9898\u6846\u5b9a\u4e3a\u7a00\u758f\u6a21\u578b\u6062\u590d\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5c06\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u6269\u5c55\u5230\u63d0\u5347\u7a7a\u95f4\u548c\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u7684\u6846\u67b6\u3002\u901a\u8fc7\u6bd4\u8f83SAEs\u3001lifted-SAE\u548cSAE\u795e\u7ecf\u7b97\u5b50\u7684\u63a8\u7406\u548c\u8bad\u7ec3\u52a8\u6001\u6765\u8bc4\u4f30\u8be5\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u5347\uff08lifting\uff09\u548c\u7b97\u5b50\u6a21\u5757\u5f15\u5165\u4e86\u6709\u76ca\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u5f97\u6a21\u578b\u6062\u590d\u901f\u5ea6\u66f4\u5feb\uff0c\u5bf9\u5e73\u6ed1\u6982\u5ff5\u7684\u6062\u590d\u80fd\u529b\u66f4\u5f3a\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u5177\u6709\u9c81\u68d2\u7684\u63a8\u7406\u80fd\u529b\u3002\u9c81\u68d2\u63a8\u7406\u662f\u795e\u7ecf\u7b97\u5b50\u72ec\u6709\u7684\u7279\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5c06\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6269\u5c55\u5230\u63d0\u5347\u7a7a\u95f4\u548c\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u7684\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u795e\u7ecf\u7b97\u5b50\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u548c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u6062\u590d\u901f\u5ea6\u3001\u5e73\u6ed1\u6982\u5ff5\u6062\u590d\u548c\u8de8\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u8868\u660e\u5176\u5728\u7edf\u4e00\u795e\u7ecf\u6a21\u578b\u8868\u793a\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.03827", "pdf": "https://arxiv.org/pdf/2509.03827", "abs": "https://arxiv.org/abs/2509.03827", "authors": ["Pierre Le Coz", "Jia An Liu", "Debarun Bhattacharjya", "Georgina Curto", "Serge Stinckwich"], "title": "What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in high-stakes\ndomains. Their capacity to process vast amounts of unstructured data, explore\nflexible scenarios, and handle a diversity of contextual factors can make them\nuniquely suited to provide new insights for the complexity of social\npolicymaking. This article evaluates whether LLMs' are aligned with domain\nexperts (and among themselves) to inform social policymaking on the subject of\nhomelessness alleviation - a challenge affecting over 150 million people\nworldwide. We develop a novel benchmark comprised of decision scenarios with\npolicy choices across four geographies (South Bend, USA; Barcelona, Spain;\nJohannesburg, South Africa; Macau SAR, China). The policies in scope are\ngrounded in the conceptual framework of the Capability Approach for human\ndevelopment. We also present an automated pipeline that connects the\nbenchmarked policies to an agent-based model, and we explore the social impact\nof the recommended policies through simulated social scenarios. The paper\nresults reveal promising potential to leverage LLMs for social policy making.\nIf responsible guardrails and contextual calibrations are introduced in\ncollaboration with local domain experts, LLMs can provide humans with valuable\ninsights, in the form of alternative policies at scale.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65e0\u5bb6\u53ef\u5f52\u95ee\u9898\u793e\u4f1a\u653f\u7b56\u5236\u5b9a\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u4e0e\u9886\u57df\u4e13\u5bb6\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7406\u6a21\u578b\u6a21\u62df\uff0c\u7ed3\u679c\u663e\u793aLLMs\u5728\u5f15\u5165\u8d1f\u8d23\u4efb\u7684\u9632\u62a4\u63aa\u65bd\u548c\u4e13\u5bb6\u534f\u4f5c\u540e\u5177\u6709\u63d0\u4f9b\u5927\u89c4\u6a21\u653f\u7b56\u89c1\u89e3\u7684\u826f\u597d\u524d\u666f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u6d77\u91cf\u975e\u7ed3\u6784\u5316\u6570\u636e\u548c\u590d\u6742\u573a\u666f\u65b9\u9762\u5177\u6709\u72ec\u7279\u80fd\u529b\uff0c\u53ef\u80fd\u4e3a\u590d\u6742\u7684\u793e\u4f1a\u653f\u7b56\u5236\u5b9a\uff08\u5982\u89e3\u51b3\u5168\u7403\u65e0\u5bb6\u53ef\u5f52\u95ee\u9898\uff09\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u4e3a\u793e\u4f1a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4fe1\u606f\u65f6\uff0c\u662f\u5426\u4e0e\u9886\u57df\u4e13\u5bb6\uff08\u4ee5\u53caLLMs\u4e4b\u95f4\uff09\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8de8\u56db\u4e2a\u5730\u7406\u533a\u57df\uff08\u7f8e\u56fd\u5357\u672c\u5fb7\u3001\u897f\u73ed\u7259\u5df4\u585e\u7f57\u90a3\u3001\u5357\u975e\u7ea6\u7ff0\u5185\u65af\u5821\u3001\u4e2d\u56fd\u6fb3\u95e8\u7279\u533a\uff09\u7684\u65e0\u5bb6\u53ef\u5f52\u653f\u7b56\u51b3\u7b56\u573a\u666f\uff0c\u653f\u7b56\u57fa\u4e8e\u4eba\u7c7b\u53d1\u5c55\u7684\u201c\u80fd\u529b\u65b9\u6cd5\u201d\u6846\u67b6\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5c06\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u653f\u7b56\u4e0e\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u793e\u4f1a\u573a\u666f\u63a2\u7d22\u63a8\u8350\u653f\u7b56\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u793e\u4f1a\u653f\u7b56\u5236\u5b9a\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u5982\u679c\u5728\u4e0e\u5f53\u5730\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u7684\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u8d1f\u8d23\u4efb\u7684\u9632\u62a4\u63aa\u65bd\u548c\u60c5\u5883\u6821\u51c6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4ee5\u5927\u89c4\u6a21\u66ff\u4ee3\u653f\u7b56\u7684\u5f62\u5f0f\uff0c\u4e3a\u4eba\u7c7b\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.03809", "pdf": "https://arxiv.org/pdf/2509.03809", "abs": "https://arxiv.org/abs/2509.03809", "authors": ["Jiaxin Guo", "Daimeng Wei", "Yuanchang Luo", "Xiaoyu Chen", "Zhanglin Wu", "Huan Yang", "Hengchao Shang", "Zongyao Li", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": "under preview", "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u7684\u5bf9\u9f50\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u201cAlign-then-Slide\u201d\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u80fd\u6709\u6548\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\uff08doc-mt\uff09\u5e26\u6765\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u5176\u6574\u6587\u6863\u8f93\u51fa\u4f7f\u5f97\u4f9d\u8d56\u9010\u53e5\u5bf9\u9f50\u7684\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5e38\u5e38\u51fa\u73b0\u53e5\u5b50\u9057\u6f0f\u3001\u591a\u5bf9\u4e00\u6216\u4e00\u5bf9\u591a\u6620\u5c04\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8d85\u957f\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\u2014\u2014\u201cAlign-then-Slide\u201d\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\n1.  **\u5bf9\u9f50\u9636\u6bb5\uff08Align stage\uff09**\uff1a\u81ea\u52a8\u63a8\u65ad\u53e5\u5b50\u7ea7\u522b\u7684\u6e90-\u76ee\u6807\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u91cd\u5efa\u76ee\u6807\u6587\u672c\u4ee5\u5339\u914d\u6e90\u6587\u672c\u7684\u53e5\u5b50\u6570\u91cf\uff0c\u4ece\u800c\u89e3\u51b3\u9057\u6f0f\u3001\u591a\u5bf9\u4e00/\u4e00\u5bf9\u591a\u6620\u5c04\u7b49\u95ee\u9898\u3002\n2.  **N\u5757\u6ed1\u52a8\u8bc4\u4f30\u9636\u6bb5\uff08n-Chunk Sliding Evaluate stage\uff09**\uff1a\u57281\u30012\u30013\u548c4\u4e2a\u5757\uff08chunk\uff09\u4e0b\u8ba1\u7b97\u5e73\u5747\u6307\u6807\u5206\u6570\uff0c\u4ee5\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "1.  \u5728WMT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4e13\u5bb6MQM\u6392\u540d\u4e4b\u95f4\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.929\u3002\n2.  \u5728\u4e00\u4e2a\u65b0\u6574\u7406\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u518d\u6b21\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002\n3.  Align-then-Slide\u4ea7\u751f\u7684\u504f\u597d\u6570\u636e\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884cCPO\u8bad\u7ec3\uff0c\u5e76\u53ef\u76f4\u63a5\u7528\u4f5cGRPO\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4e24\u8005\u90fd\u751f\u6210\u4e86\u4f18\u4e8e\u9999\u8349SFT\u57fa\u7ebf\uff08vanilla SFT baseline\uff09\u7684\u7ffb\u8bd1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u201cAlign-then-Slide\u201d\u6846\u67b6\u662f\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e00\u4e2a\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u53ef\u64cd\u4f5c\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2509.03872", "pdf": "https://arxiv.org/pdf/2509.03872", "abs": "https://arxiv.org/abs/2509.03872", "authors": ["Nan Yang", "Yang Wang", "Zhanwen Liu", "Yuchao Dai", "Yang Liu", "Xiangmo Zhao"], "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.", "AI": {"tldr": "FocusMamba\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u7a00\u758f\u5316\u548c\u8de8\u6a21\u6001\u805a\u7126\u878d\u5408\uff0c\u5b9e\u73b0\u4e86RGB-\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u591a\u6a21\u6001\u7279\u5f81\u7684\u81ea\u9002\u5e94\u534f\u540c\u5904\u7406\u548c\u9ad8\u6548\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709RGB-\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u8fc7\u7a0b\u4e2d\u7edf\u4e00\u5904\u7406\u5404\u6a21\u6001\uff08\u56fe\u50cf\u80cc\u666f\u548c\u4e8b\u4ef6\u6570\u636e\u975e\u4e8b\u4ef6\u533a\u57df\uff09\u7684\u4f4e\u4fe1\u606f\u533a\u57df\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6b21\u4f18\u6027\u80fd\u3002\u5f53\u524d\u4ee4\u724c\u7a00\u758f\u5316\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u6570\u91cf\u6216\u9608\u503c\u9009\u62e9\u4ee4\u724c\uff0c\u96be\u4ee5\u4fdd\u7559\u590d\u6742\u6837\u672c\u4e2d\u7684\u4fe1\u606f\u6027\u4ee4\u724c\u3002", "method": "\u63d0\u51faFocusMamba\uff0c\u8be5\u65b9\u6cd5\u6267\u884c\u591a\u6a21\u6001\u7279\u5f81\u7684\u81ea\u9002\u5e94\u534f\u540c\u7a00\u758f\u5316\u5e76\u9ad8\u6548\u96c6\u6210\u4e92\u8865\u4fe1\u606f\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u4e8b\u4ef6\u5f15\u5bfc\u591a\u6a21\u6001\u7a00\u758f\u5316\uff08EGMS\uff09\u7b56\u7565\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u611f\u77e5\u7684\u573a\u666f\u5185\u5bb9\u53d8\u5316\uff0c\u8bc6\u522b\u5e76\u81ea\u9002\u5e94\u4e22\u5f03\u5404\u6a21\u6001\u4e2d\u7684\u4f4e\u4fe1\u606f\u533a\u57df\u30022. \u8de8\u6a21\u6001\u805a\u7126\u878d\u5408\uff08CMFF\uff09\u6a21\u5757\uff0c\u57fa\u4e8e\u7a00\u758f\u5316\u7ed3\u679c\uff0c\u6709\u6548\u6355\u83b7\u548c\u6574\u5408\u6765\u81ea\u4e24\u4e2a\u6a21\u6001\u7684\u4e92\u8865\u7279\u5f81\u3002", "result": "\u5728DSEC-Det\u548cPKU-DAVIS-SOD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FocusMamba\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u540c\u7a00\u758f\u5316\u548c\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RGB-\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u4f4e\u4fe1\u606f\u533a\u57df\u7edf\u4e00\u5904\u7406\u5bfc\u81f4\u7684\u8ba1\u7b97\u5197\u4f59\u548c\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861\u3002"}}
{"id": "2509.03749", "pdf": "https://arxiv.org/pdf/2509.03749", "abs": "https://arxiv.org/abs/2509.03749", "authors": ["Livia Betti", "Farooq Sanni", "Gnouyaro Sogoyou", "Togbe Agbagla", "Cullen Molitor", "Tamma Carleton", "Esther Rolf"], "title": "Mapping on a Budget: Optimizing Spatial Data Collection for ML", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In applications across agriculture, ecology, and human development, machine\nlearning with satellite imagery (SatML) is limited by the sparsity of labeled\ntraining data. While satellite data cover the globe, labeled training datasets\nfor SatML are often small, spatially clustered, and collected for other\npurposes (e.g., administrative surveys or field measurements). Despite the\npervasiveness of this issue in practice, past SatML research has largely\nfocused on new model architectures and training algorithms to handle scarce\ntraining data, rather than modeling data conditions directly. This leaves\nscientists and policymakers who wish to use SatML for large-scale monitoring\nuncertain about whether and how to collect additional data to maximize\nperformance. Here, we present the first problem formulation for the\noptimization of spatial training data in the presence of heterogeneous data\ncollection costs and realistic budget constraints, as well as novel methods for\naddressing this problem. In experiments simulating different problem settings\nacross three continents and four tasks, our strategies reveal substantial gains\nfrom sample optimization. Further experiments delineate settings for which\noptimized sampling is particularly effective. The problem formulation and\nmethods we introduce are designed to generalize across application domains for\nSatML; we put special emphasis on a specific problem setting where our\ncoauthors can immediately use our findings to augment clustered agricultural\nsurveys for SatML monitoring in Togo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u536b\u661f\u673a\u5668\u5b66\u4e60\uff08SatML\uff09\u7a7a\u95f4\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u758f\u548c\u96c6\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u519c\u4e1a\u3001\u751f\u6001\u548c\u4eba\u7c7b\u53d1\u5c55\u7b49SatML\u5e94\u7528\u4e2d\uff0c\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u758f\u6027\u3001\u7a7a\u95f4\u96c6\u805a\u6027\u4ee5\u53ca\u6570\u636e\u6536\u96c6\u76ee\u7684\u7684\u591a\u6837\u6027\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b97\u6cd5\uff0c\u800c\u975e\u6570\u636e\u6761\u4ef6\u672c\u8eab\uff0c\u5bfc\u81f4\u4f7f\u7528\u8005\u4e0d\u786e\u5b9a\u5982\u4f55\u6709\u6548\u6536\u96c6\u989d\u5916\u6570\u636e\u4ee5\u6700\u5927\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5f02\u6784\u6570\u636e\u6536\u96c6\u6210\u672c\u548c\u5b9e\u9645\u9884\u7b97\u7ea6\u675f\u4e0b\u7a7a\u95f4\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u7684\u95ee\u5f00\u9898\u8868\u8ff0\uff0c\u5e76\u5f15\u5165\u4e86\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u666e\u904d\u9002\u7528\u4e8eSatML\u7684\u5404\u4e2a\u5e94\u7528\u9886\u57df\u3002", "result": "\u5728\u6a21\u62df\u4e0d\u540c\u95ee\u9898\u8bbe\u7f6e\uff08\u6db5\u76d6\u4e09\u5927\u6d32\u3001\u56db\u9879\u4efb\u52a1\uff09\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4ed6\u4eec\u7684\u7b56\u7565\u663e\u793a\u51fa\u901a\u8fc7\u6837\u672c\u4f18\u5316\u83b7\u5f97\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u4e5f\u660e\u786e\u4e86\u4f18\u5316\u91c7\u6837\u7279\u522b\u6709\u6548\u7684\u5177\u4f53\u573a\u666f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u95ee\u9898\u8868\u8ff0\u548c\u65b9\u6cd5\u5177\u6709\u8de8SatML\u5e94\u7528\u9886\u57df\u7684\u666e\u9002\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u589e\u5f3a\u7279\u5b9a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u7684\u96c6\u4e2d\u5f0f\u519c\u4e1a\u8c03\u67e5\uff0c\u4ee5\u6539\u8fdbSatML\u5728\u591a\u54e5\u7b49\u5730\u7684\u76d1\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.03828", "pdf": "https://arxiv.org/pdf/2509.03828", "abs": "https://arxiv.org/abs/2509.03828", "authors": ["Jaerong Ahn", "Andrew Wen", "Nan Wang", "Heling Jia", "Zhiyi Yue", "Sunyang Fu", "Hongfang Liu"], "title": "An Agentic Model Context Protocol Framework for Medical Concept Standardization", "categories": ["cs.AI"], "comment": null, "summary": "The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)\nprovides a standardized representation of heterogeneous health data to support\nlarge-scale, multi-institutional research. One critical step in data\nstandardization using OMOP CDM is the mapping of source medical terms to OMOP\nstandard concepts, a procedure that is resource-intensive and error-prone.\nWhile large language models (LLMs) have the potential to facilitate this\nprocess, their tendency toward hallucination makes them unsuitable for clinical\ndeployment without training and expert validation. Here, we developed a\nzero-training, hallucination-preventive mapping system based on the Model\nContext Protocol (MCP), a standardized and secure framework allowing LLMs to\ninteract with external resources and tools. The system enables explainable\nmapping and significantly improves efficiency and accuracy with minimal effort.\nIt provides real-time vocabulary lookups and structured reasoning outputs\nsuitable for immediate use in both exploratory and production environments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u96f6\u8bad\u7ec3\u3001\u9632\u5e7b\u89c9OMOP\u6982\u5ff5\u6620\u5c04\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u90e8\u7f72\u3002", "motivation": "OMOP CDM\u4e2d\u5c06\u6e90\u533b\u5b66\u672f\u8bed\u6620\u5c04\u5230\u6807\u51c6\u6982\u5ff5\u7684\u8fc7\u7a0b\u8017\u8d39\u8d44\u6e90\u4e14\u6613\u51fa\u9519\uff1b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5e7b\u89c9\u95ee\u9898\u4f7f\u5176\u4e0d\u9002\u5408\u76f4\u63a5\u7528\u4e8e\u4e34\u5e8a\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5f00\u53d1\u4e86\u4e00\u4e2a\u96f6\u8bad\u7ec3\u3001\u9632\u5e7b\u89c9\u7684\u6620\u5c04\u7cfb\u7edf\u3002MCP\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u5b89\u5168\u7684\u6846\u67b6\uff0c\u5141\u8bb8LLMs\u4e0e\u5916\u90e8\u8d44\u6e90\u548c\u5de5\u5177\u4ea4\u4e92\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u6620\u5c04\uff0c\u4ee5\u6700\u5c0f\u7684\u52aa\u529b\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u5b83\u63d0\u4f9b\u5b9e\u65f6\u8bcd\u6c47\u67e5\u8be2\u548c\u7ed3\u6784\u5316\u63a8\u7406\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u63a2\u7d22\u6027\u548c\u751f\u4ea7\u73af\u5883\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aOMOP\u6570\u636e\u6807\u51c6\u5316\u4e2d\u7684\u672f\u8bed\u6620\u5c04\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u65f6\u53ef\u7528\u7684\u3001\u514b\u670dLLM\u5e7b\u89c9\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6620\u5c04\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2509.03829", "pdf": "https://arxiv.org/pdf/2509.03829", "abs": "https://arxiv.org/abs/2509.03829", "authors": ["Huhong Xian", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.", "AI": {"tldr": "\u9488\u5bf9\u9700\u8981\u5e27\u7ea7\u5b9a\u4f4d\u4f2a\u9020\u8bed\u97f3\u7684\u5c40\u90e8\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08PADD\uff09\uff0c\u672c\u6587\u63d0\u51faNE-PADD\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u547d\u540d\u5b9e\u4f53\u77e5\u8bc6\uff08SpeechNER\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86PADD\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08ADD\uff09\u662f\u53e5\u5b50\u7ea7\u7684\uff0c\u800c\u5c40\u90e8\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08PADD\uff09\u8981\u6c42\u5e27\u7ea7\u5b9a\u4f4d\u4f2a\u9020\u8bed\u97f3\u3002\u73b0\u6709PADD\u7814\u7a76\u5c1a\u672a\u5145\u5206\u5229\u7528\u97f3\u9891\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u547d\u540d\u5b9e\u4f53\u4fe1\u606f\uff0c\u8fd9\u4e3a\u63d0\u5347PADD\u6027\u80fd\u63d0\u4f9b\u4e86\u672a\u88ab\u63a2\u7d22\u7684\u65b9\u5411\u3002", "method": "\u63d0\u51faNE-PADD\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u5e76\u884c\u5206\u652f\uff08\u8bed\u97f3\u547d\u540d\u5b9e\u4f53\u8bc6\u522bSpeechNER\u548cPADD\uff09\u5229\u7528\u547d\u540d\u5b9e\u4f53\u77e5\u8bc6\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u79cd\u6ce8\u610f\u529b\u805a\u5408\u673a\u5236\uff1a\u6ce8\u610f\u529b\u878d\u5408\uff08AF\uff09\u7528\u4e8e\u7ed3\u5408\u6ce8\u610f\u529b\u6743\u91cd\uff1b\u6ce8\u610f\u529b\u8fc1\u79fb\uff08AT\uff09\u901a\u8fc7\u8f85\u52a9\u635f\u5931\u6307\u5bfcPADD\u5229\u7528\u547d\u540d\u5b9e\u4f53\u8bed\u4e49\u3002\u8be5\u65b9\u6cd5\u5728PartialSpoof-NER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728PartialSpoof-NER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684NE-PADD\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6574\u5408\u547d\u540d\u5b9e\u4f53\u77e5\u8bc6\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c40\u90e8\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff08PADD\uff09\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03873", "pdf": "https://arxiv.org/pdf/2509.03873", "abs": "https://arxiv.org/abs/2509.03873", "authors": ["Jiajun Song", "Xiaoou Liu"], "title": "SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "34th International Conference on Artificial Neural Networks - ICANN\n  2025", "summary": "Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.", "AI": {"tldr": "\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u98df\u7269\u7c7b\u522b\u8bc6\u522b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u98df\u7269\u8bc6\u522b(CZSFR)\u4efb\u52a1\uff0c\u5e76\u63d0\u51faSalientFusion\u65b9\u6cd5\uff0c\u901a\u8fc7SalientFormer\u53bb\u9664\u80cc\u666f\u5197\u4f59\u548c\u89e3\u51b3\u4e3b\u914d\u83dc\u6df7\u6dc6\uff0cDebiasAT\u51cf\u5c11\u8bed\u4e49\u504f\u5dee\uff0c\u5728\u65b0\u57fa\u51c6\u548c\u901a\u7528CZSL\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u98df\u7269\u8bc6\u522b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u51fa\u73b0\u7684\u65b0\u83dc\u54c1\u3002\u4e3a\u89e3\u51b3\u8bc6\u522b\u672a\u89c1\u98df\u7269\u7c7b\u522b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u88ab\u96f6\u6837\u672c\u98df\u7269\u5b66\u4e60(ZSFL)\u6240\u9a71\u52a8\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u7ec4\u5408\u96f6\u6837\u672c\u98df\u7269\u8bc6\u522b(CZSFR)\u4efb\u52a1\u3002\u7136\u800c\uff0cCZSFR\u9762\u4e34\u80cc\u666f\u5197\u4f59\u3001\u4e3b\u914d\u83dc\u89d2\u8272\u6df7\u6dc6\u4ee5\u53ca\u5355\u4e00\u5c5e\u6027\u8bed\u4e49\u504f\u5dee\u4e09\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684CZSFR\u65b9\u6cd5\u2014\u2014SalientFusion\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1. SalientFormer\uff0c\u7528\u4e8e\u53bb\u9664\u5197\u4f59\u80cc\u666f\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u7279\u5f81\u89e3\u51b3\u4e3b\u914d\u83dc\u7684\u89d2\u8272\u6df7\u6dc6\u95ee\u9898\uff1b2. DebiasAT\uff0c\u901a\u8fc7\u5bf9\u9f50\u63d0\u793a\u8bed\u4e0e\u89c6\u89c9\u7279\u5f81\u6765\u51cf\u5c11\u8bed\u4e49\u504f\u5dee\u3002", "result": "SalientFusion\u65b9\u6cd5\u5728\u5176\u63d0\u51fa\u7684CZSFood-90\u548cCZSFood-164\u65b0\u57fa\u51c6\u4e0a\uff0c\u4ee5\u53ca\u6700\u6d41\u884c\u7684\u901a\u7528CZSL\u6570\u636e\u96c6\u4e0a\uff0c\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u7ed3\u679c\u3002", "conclusion": "SalientFusion\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u98df\u7269\u8bc6\u522b\uff08CZSFR\uff09\u4e2d\u7684\u80cc\u666f\u5197\u4f59\u3001\u4e3b\u914d\u83dc\u89d2\u8272\u6df7\u6dc6\u548c\u8bed\u4e49\u504f\u5dee\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u76f8\u5173\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u9886\u5148\u6027\u80fd\uff0c\u4e3a\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u98df\u7269\u7c7b\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.03758", "pdf": "https://arxiv.org/pdf/2509.03758", "abs": "https://arxiv.org/abs/2509.03758", "authors": ["Alvaro Almeida Gomez"], "title": "Learning functions through Diffusion Maps", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "Comments are welcome", "summary": "We propose a data-driven method for approximating real-valued functions on\nsmooth manifolds, building on the Diffusion Maps framework under the manifold\nhypothesis. Given pointwise evaluations of a function, the method constructs a\nsmooth extension to the ambient space by exploiting diffusion geometry and its\nconnection to the heat equation and the Laplace-Beltrami operator.\n  To address the computational challenges of high-dimensional data, we\nintroduce a dimensionality reduction strategy based on the low-rank structure\nof the distance matrix, revealed via singular value decomposition (SVD). In\naddition, we develop an online updating mechanism that enables efficient\nincorporation of new data, thereby improving scalability and reducing\ncomputational cost.\n  Numerical experiments, including applications to sparse CT reconstruction,\ndemonstrate that the proposed methodology outperforms classical feedforward\nneural networks and interpolation methods in terms of both accuracy and\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u56fe\u7684\u6d41\u5f62\u51fd\u6570\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u964d\u7ef4\u548c\u5728\u7ebf\u66f4\u65b0\u673a\u5236\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u7a00\u758fCT\u91cd\u5efa\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5728\u5e73\u6ed1\u6d41\u5f62\u4e0a\u8fd1\u4f3c\u5b9e\u503c\u51fd\u6570\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6269\u6563\u56fe\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u51e0\u4f55\u3001\u70ed\u65b9\u7a0b\u548c\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\u6784\u5efa\u51fd\u6570\u7684\u5e73\u6ed1\u6269\u5c55\u3002\u4e3a\u5e94\u5bf9\u9ad8\u7ef4\u6570\u636e\u6311\u6218\uff0c\u5f15\u5165\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u7684\u8ddd\u79bb\u77e9\u9635\u4f4e\u79e9\u7ed3\u6784\u964d\u7ef4\u7b56\u7565\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u5728\u7ebf\u66f4\u65b0\u673a\u5236\u4ee5\u9ad8\u6548\u6574\u5408\u65b0\u6570\u636e\uff0c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\uff08\u5305\u62ec\u7a00\u758fCT\u91cd\u5efa\uff09\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u7ecf\u5178\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u63d2\u503c\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u56fe\u7684\u6d41\u5f62\u51fd\u6570\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u964d\u7ef4\u548c\u5728\u7ebf\u66f4\u65b0\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u65f6\u8fd1\u4f3c\u5e73\u6ed1\u6d41\u5f62\u4e0a\u5b9e\u503c\u51fd\u6570\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.03830", "pdf": "https://arxiv.org/pdf/2509.03830", "abs": "https://arxiv.org/abs/2509.03830", "authors": ["Kaizhen Tan", "Yufan Wu", "Yuxuan Liu", "Haoran Zeng"], "title": "A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai", "categories": ["cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Historic urban quarters play a vital role in preserving cultural heritage\nwhile serving as vibrant spaces for tourism and everyday life. Understanding\nhow tourists perceive these environments is essential for sustainable,\nhuman-centered urban planning. This study proposes a multidimensional\nAI-powered framework for analyzing tourist perception in historic urban\nquarters using multimodal data from social media. Applied to twelve historic\nquarters in central Shanghai, the framework integrates focal point extraction,\ncolor theme analysis, and sentiment mining. Visual focus areas are identified\nfrom tourist-shared photos using a fine-tuned semantic segmentation model. To\nassess aesthetic preferences, dominant colors are extracted using a clustering\nmethod, and their spatial distribution across quarters is analyzed. Color\nthemes are further compared between social media photos and real-world street\nviews, revealing notable shifts. This divergence highlights potential gaps\nbetween visual expectations and the built environment, reflecting both\nstylistic preferences and perceptual bias. Tourist reviews are evaluated\nthrough a hybrid sentiment analysis approach combining a rule-based method and\na multi-task BERT model. Satisfaction is assessed across four dimensions:\ntourist activities, built environment, service facilities, and business\nformats. The results reveal spatial variations in aesthetic appeal and\nemotional response. Rather than focusing on a single technical innovation, this\nframework offers an integrated, data-driven approach to decoding tourist\nperception and contributes to informed decision-making in tourism, heritage\nconservation, and the design of aesthetically engaging public spaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u7ef4\u6846\u67b6\uff0c\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u5386\u53f2\u57ce\u533a\u6e38\u5ba2\u611f\u77e5\uff0c\u63ed\u793a\u4e86\u5ba1\u7f8e\u504f\u597d\u548c\u60c5\u611f\u53cd\u5e94\u7684\u7a7a\u95f4\u5dee\u5f02\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5386\u53f2\u57ce\u533a\u5bf9\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548c\u65c5\u6e38\u81f3\u5173\u91cd\u8981\u3002\u7406\u89e3\u6e38\u5ba2\u611f\u77e5\u662f\u5b9e\u73b0\u53ef\u6301\u7eed\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u57ce\u5e02\u89c4\u5212\u7684\u5173\u952e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4AI\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5386\u53f2\u57ce\u533a\u7684\u6e38\u5ba2\u611f\u77e5\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u7126\u70b9\u533a\u57df\u63d0\u53d6\uff08\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff09\u3001\u8272\u5f69\u4e3b\u9898\u5206\u6790\uff08\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u6bd4\u8f83\u793e\u4ea4\u5a92\u4f53\u7167\u7247\u4e0e\u771f\u5b9e\u8857\u666f\uff09\uff0c\u4ee5\u53ca\u60c5\u611f\u6316\u6398\uff08\u7ed3\u5408\u89c4\u5219\u65b9\u6cd5\u548c\u591a\u4efb\u52a1BERT\u6a21\u578b\u7684\u6df7\u5408\u60c5\u611f\u5206\u6790\uff09\u3002\u7814\u7a76\u5e94\u7528\u4e8e\u4e0a\u6d77\u7684\u5341\u4e8c\u4e2a\u5386\u53f2\u57ce\u533a\uff0c\u5e76\u5728\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6e38\u5ba2\u6ee1\u610f\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5386\u53f2\u57ce\u533a\u5ba1\u7f8e\u5438\u5f15\u529b\u548c\u60c5\u611f\u53cd\u5e94\u7684\u7a7a\u95f4\u5dee\u5f02\u3002\u793e\u4ea4\u5a92\u4f53\u7167\u7247\u4e0e\u771f\u5b9e\u8857\u666f\u7684\u8272\u5f69\u4e3b\u9898\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u6e38\u5ba2\u89c6\u89c9\u9884\u671f\u4e0e\u5efa\u6210\u73af\u5883\u4e4b\u95f4\u5b58\u5728\u6f5c\u5728\u5dee\u8ddd\uff0c\u53cd\u6620\u51fa\u98ce\u683c\u504f\u597d\u548c\u611f\u77e5\u504f\u5dee\u3002", "conclusion": "\u672c\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7efc\u5408\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u89e3\u7801\u6e38\u5ba2\u611f\u77e5\uff0c\u6709\u52a9\u4e8e\u5728\u65c5\u6e38\u3001\u9057\u4ea7\u4fdd\u62a4\u548c\u7f8e\u5b66\u516c\u5171\u7a7a\u95f4\u8bbe\u8ba1\u9886\u57df\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002"}}
{"id": "2509.03867", "pdf": "https://arxiv.org/pdf/2509.03867", "abs": "https://arxiv.org/abs/2509.03867", "authors": ["Yang Wang", "Chenghao Xiao", "Chia-Yi Hsiao", "Zi Yan Chang", "Chi-Li Chen", "Tyler Loakman", "Chenghua Lin"], "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth", "categories": ["cs.CL"], "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference", "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u201cDrivelology\u201d\u6982\u5ff5\uff0c\u6307\u8bed\u6cd5\u8fde\u8d2f\u4f46\u8bed\u7528\u77db\u76fe\u3001\u60c5\u611f\u8d1f\u8f7d\u6216\u4fee\u8f9e\u98a0\u8986\u7684\u6df1\u5c42\u201c\u5e9f\u8bdd\u201d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96be\u4ee5\u7406\u89e3\u5176\u5206\u5c42\u8bed\u4e49\uff0c\u5e76\u6784\u5efa\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u63ed\u793aLLMs\u5728\u8bed\u7528\u7406\u89e3\u4e0a\u7684\u6df1\u5c42\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591aNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53ef\u80fd\u65e0\u6cd5\u7406\u89e3\u201cDrivelology\u201d\u8fd9\u79cd\u5177\u6709\u6df1\u5c42\u9690\u6027\u610f\u4e49\u7684\u590d\u6742\u8bed\u8a00\u73b0\u8c61\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u5728\u5904\u7406\u8fd9\u7c7b\u201c\u6709\u6df1\u5ea6\u7684\u5e9f\u8bdd\u201d\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6311\u6218\u5176\u7edf\u8ba1\u6d41\u5229\u5ea6\u662f\u5426\u7b49\u540c\u4e8e\u8ba4\u77e5\u7406\u89e3\u7684\u5047\u8bbe\u3002", "method": "1. \u5b9a\u4e49\u5e76\u5f15\u5165\u201cDrivelology\u201d\u6982\u5ff5\uff0c\u5373\u201c\u6709\u6df1\u5ea6\u7684\u5e9f\u8bdd\u201d\u30022. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1200\u591a\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u8bed\u8a00\uff08\u82f1\u3001\u666e\u3001\u897f\u3001\u6cd5\u3001\u65e5\u3001\u97e9\uff09Drivelological\u6587\u672c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u8fdb\u884c\u4e25\u8c28\u6807\u6ce8\u30023. \u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9Drivelology\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660eLLMs\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027\uff1a\u6a21\u578b\u7ecf\u5e38\u5c06Drivelology\u4e0e\u6d45\u5c42\u5e9f\u8bdd\u6df7\u6dc6\uff0c\u751f\u6210\u4e0d\u8fde\u8d2f\u7684\u89e3\u91ca\uff0c\u6216\u5b8c\u5168\u672a\u80fd\u6355\u6349\u5176\u9690\u542b\u7684\u4fee\u8f9e\u529f\u80fd\u3002\u8fd9\u63ed\u793a\u4e86LLMs\u5728\u8bed\u7528\u7406\u89e3\u4e0a\u5b58\u5728\u6df1\u5c42\u6b21\u7684\u8868\u5f81\u5dee\u8ddd\u3002", "conclusion": "Drivelology\u73b0\u8c61\u7a81\u663e\u4e86LLMs\u5728\u7406\u89e3\u8d85\u8d8a\u8868\u9762\u8fde\u8d2f\u6027\u7684\u8bed\u8a00\u6df1\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u6311\u6218\u4e86\u7edf\u8ba1\u6d41\u5229\u5ea6\u5373\u8ba4\u77e5\u7406\u89e3\u7684\u5047\u8bbe\u3002\u672c\u7814\u7a76\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7a76\u548c\u5efa\u6a21\u8bed\u8a00\u6df1\u5c42\u542b\u4e49\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002"}}
{"id": "2509.03883", "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u5168\u9762\u6df1\u5165\u5730\u5206\u6790\u4e86\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\uff0c\u6db5\u76d6\u4e86\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u3001\u5b50\u4efb\u52a1\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e76\u5ba1\u9605\u4e86\u4e24\u767e\u591a\u7bc7\u8bba\u6587\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u6587\u7ae0\u7f3a\u4e4f\u5bf9\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u6574\u4e2a\u8fc7\u7a0b\u7684\u5168\u9762\u6982\u8ff0\uff0c\u4ec5\u5173\u6ce8\u5355\u4e00\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63ed\u793a\u8be5\u9886\u57df\u524d\u666f\uff0c\u5e76\u4e3a\u6570\u5b57\u4eba\u5e94\u7528\u7684\u8fdb\u5c55\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u4e00\u9879\u6df1\u5165\u7684\u7efc\u8ff0\u7814\u7a76\u3002\u5b83\u6db5\u76d6\u4e86\u5341\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u4e94\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u8f93\u5165\u3001\u8fd0\u52a8\u89c4\u5212\u3001\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u3001\u7cbe\u70bc\u548c\u8f93\u51fa\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u9996\u4e2a\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u65b9\u9762\u6f5c\u529b\u7684\u7efc\u8ff0\u3002\u8be5\u7efc\u8ff0\u56de\u987e\u4e86\u89c6\u89c9\u3001\u6587\u672c\u548c\u97f3\u9891\u4e09\u79cd\u4e3b\u8981\u6a21\u6001\u4e0b\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6280\u672f\u8d8b\u52bf\uff0c\u5e76\u5ba1\u9605\u4e86\u4e24\u767e\u591a\u7bc7\u8bba\u6587\uff0c\u5f3a\u8c03\u4e86\u91cc\u7a0b\u7891\u5f0f\u7684\u5de5\u4f5c\u3002", "result": "\u672c\u6587\u5bf9\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u5176\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u3001\u591a\u4e2a\u5b50\u4efb\u52a1\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002\u5b83\u56de\u987e\u4e86\u6700\u65b0\u7684\u53d1\u5c55\u548c\u6280\u672f\u8d8b\u52bf\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u91cc\u7a0b\u7891\u5f0f\u7684\u5de5\u4f5c\u3002\u8bba\u6587\u4e2d\u5ba1\u67e5\u7684\u6a21\u578b\u5b8c\u6574\u5217\u8868\u53ef\u5728\u5176\u4ed3\u5e93\u4e2d\u83b7\u53d6\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u63ed\u793a\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u524d\u666f\uff0c\u5e76\u4e3a\u63a8\u52a8\u6570\u5b57\u4eba\u7efc\u5408\u5e94\u7528\u63d0\u4f9b\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.03771", "pdf": "https://arxiv.org/pdf/2509.03771", "abs": "https://arxiv.org/abs/2509.03771", "authors": ["Brennen Hill"], "title": "Learning an Adversarial World Model for Automated Curriculum Generation in MARL", "categories": ["cs.LG", "cs.AI", "cs.MA", "68T05, 91A26, 90C40", "I.2.6; I.2.11"], "comment": null, "summary": "World models that infer and predict environmental dynamics are foundational\nto embodied intelligence. However, their potential is often limited by the\nfinite complexity and implicit biases of hand-crafted training environments. To\ndevelop truly generalizable and robust agents, we need environments that scale\nin complexity alongside the agents learning within them. In this work, we\nreframe the challenge of environment generation as the problem of learning a\ngoal-conditioned, generative world model. We propose a system where a\ngenerative **Attacker** agent learns an implicit world model to synthesize\nincreasingly difficult challenges for a team of cooperative **Defender**\nagents. The Attacker's objective is not passive prediction, but active,\ngoal-driven interaction: it models and generates world states (i.e.,\nconfigurations of enemy units) specifically to exploit the Defenders'\nweaknesses. Concurrently, the embodied Defender team learns a cooperative\npolicy to overcome these generated worlds. This co-evolutionary dynamic creates\na self-scaling curriculum where the world model continuously adapts to\nchallenge the decision-making policy of the agents, providing an effectively\ninfinite stream of novel and relevant training scenarios. We demonstrate that\nthis framework leads to the emergence of complex behaviors, such as the world\nmodel learning to generate flanking and shielding formations, and the defenders\nlearning coordinated focus-fire and spreading tactics. Our findings position\nadversarial co-evolution as a powerful method for learning instrumental world\nmodels that drive agents toward greater strategic depth and robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\uff08\u653b\u51fb\u8005\uff09\u4e3a\u5408\u4f5c\u667a\u80fd\u4f53\uff08\u9632\u5fa1\u8005\uff09\u751f\u6210\u9ad8\u96be\u5ea6\u73af\u5883\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u5b66\u4e60\u590d\u6742\u884c\u4e3a\u5e76\u63d0\u5347\u7b56\u7565\u6df1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u53d7\u9650\u4e8e\u4eba\u5de5\u73af\u5883\u7684\u6709\u9650\u590d\u6742\u6027\u548c\u9690\u6027\u504f\u89c1\uff0c\u963b\u788d\u4e86\u901a\u7528\u548c\u9c81\u68d2\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u968f\u667a\u80fd\u4f53\u5b66\u4e60\u800c\u6269\u5c55\u590d\u6742\u5ea6\u7684\u73af\u5883\uff0c\u5e76\u4e14\u4e16\u754c\u6a21\u578b\u5e94\u4ece\u88ab\u52a8\u9884\u6d4b\u8f6c\u5411\u4e3b\u52a8\u3001\u76ee\u6807\u9a71\u52a8\u7684\u73af\u5883\u751f\u6210\u3002", "method": "\u5c06\u73af\u5883\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b66\u4e60\u4e00\u4e2a\u76ee\u6807\u6761\u4ef6\u3001\u751f\u6210\u5f0f\u7684\u4e16\u754c\u6a21\u578b\u3002\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u751f\u6210\u5f0f\u201c\u653b\u51fb\u8005\u201d\u667a\u80fd\u4f53\u548c\u4e00\u4e2a\u5408\u4f5c\u7684\u201c\u9632\u5fa1\u8005\u201d\u667a\u80fd\u4f53\u56e2\u961f\u3002\u653b\u51fb\u8005\u5b66\u4e60\u9690\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u5408\u6210\u65e5\u76ca\u56f0\u96be\u7684\u6311\u6218\uff08\u5982\u654c\u65b9\u5355\u4f4d\u914d\u7f6e\uff09\uff0c\u65e8\u5728\u5229\u7528\u9632\u5fa1\u8005\u7684\u5f31\u70b9\u3002\u9632\u5fa1\u8005\u56e2\u961f\u5219\u5b66\u4e60\u5408\u4f5c\u7b56\u7565\u4ee5\u514b\u670d\u8fd9\u4e9b\u751f\u6210\u7684\u4e16\u754c\u3002\u8fd9\u79cd\u534f\u540c\u8fdb\u5316\u52a8\u6001\u521b\u9020\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u4e16\u754c\u6a21\u578b\u4e0d\u65ad\u8c03\u6574\u4ee5\u6311\u6218\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u4fc3\u6210\u4e86\u590d\u6742\u884c\u4e3a\u7684\u6d8c\u73b0\u3002\u4e16\u754c\u6a21\u578b\u5b66\u4f1a\u751f\u6210\u4fa7\u7ffc\u548c\u63a9\u62a4\u9635\u578b\uff0c\u800c\u9632\u5fa1\u8005\u5219\u5b66\u4f1a\u4e86\u534f\u8c03\u96c6\u706b\u548c\u5206\u6563\u6218\u672f\u3002", "conclusion": "\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u662f\u5b66\u4e60\u5177\u6709\u5de5\u5177\u6027\u4e16\u754c\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u63a8\u52a8\u667a\u80fd\u4f53\u53d1\u5c55\u51fa\u66f4\u6df1\u5c42\u6b21\u7684\u6218\u7565\u6df1\u5ea6\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.03857", "pdf": "https://arxiv.org/pdf/2509.03857", "abs": "https://arxiv.org/abs/2509.03857", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Hasmot Ali", "Marufa Kamal", "Syed Bahauddin Alam", "Mohammad Ashiqur Rahman"], "title": "Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures", "categories": ["cs.AI"], "comment": null, "summary": "Generative AI (GEN AI) models have revolutionized diverse application domains\nbut present substantial challenges due to reliability concerns, including\nhallucinations, semantic drift, and inherent biases. These models typically\noperate as black-boxes, complicating transparent and objective evaluation.\nCurrent evaluation methods primarily depend on subjective human assessment,\nlimiting scalability, transparency, and effectiveness. This research proposes a\nsystematic methodology using deterministic and Large Language Model\n(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN\nAI reliability. We construct two parallel KGs: (i) a deterministic KG built\nusing explicit rule-based methods, predefined ontologies, domain-specific\ndictionaries, and structured entity-relation extraction rules, and (ii) an\nLLM-generated KG dynamically derived from real-time textual data streams such\nas live news articles. Utilizing real-time news streams ensures authenticity,\nmitigates biases from repetitive training, and prevents adaptive LLMs from\nbypassing predefined benchmarks through feedback memorization. To quantify\nstructural deviations and semantic discrepancies, we employ several established\nKG metrics, including Instantiated Class Ratio (ICR), Instantiated Property\nRatio (IPR), and Class Instantiation (CI). An automated real-time monitoring\nframework continuously computes deviations between deterministic and\nLLM-generated KGs. By establishing dynamic anomaly thresholds based on\nhistorical structural metric distributions, our method proactively identifies\nand flags significant deviations, thus promptly detecting semantic anomalies or\nhallucinations. This structured, metric-driven comparison between deterministic\nand dynamically generated KGs delivers a robust and scalable evaluation\nframework.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u786e\u5b9a\u6027\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548cLLM\u751f\u6210KG\uff0c\u901a\u8fc7\u5b9e\u65f6\u6bd4\u8f83\u4e24\u8005\u6765\u6301\u7eed\u76d1\u63a7\u548c\u8bc4\u4f30\u751f\u6210\u5f0fAI\u53ef\u9760\u6027\uff0c\u5e76\u53ca\u65f6\u68c0\u6d4b\u8bed\u4e49\u5f02\u5e38\u6216\u5e7b\u89c9\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u8bed\u4e49\u6f02\u79fb\u548c\u56fa\u6709\u504f\u89c1\u7b49\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e14\u5176\u9ed1\u7bb1\u7279\u6027\u4f7f\u5f97\u8bc4\u4f30\u590d\u6742\u3002\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u5224\u65ad\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3001\u900f\u660e\u5ea6\u548c\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u786e\u5b9a\u6027KG\u548cLLM\u751f\u6210KG\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002\u6784\u5efa\u4e24\u4e2a\u5e76\u884c\u7684KG\uff1a\u4e00\u4e2a\u662f\u7531\u663e\u5f0f\u89c4\u5219\u3001\u9884\u5b9a\u4e49\u672c\u4f53\u7b49\u6784\u5efa\u7684\u786e\u5b9a\u6027KG\uff1b\u53e6\u4e00\u4e2a\u662f\u4ece\u5b9e\u65f6\u6587\u672c\u6570\u636e\uff08\u5982\u65b0\u95fb\u6587\u7ae0\uff09\u52a8\u6001\u751f\u6210\u7684LLM KG\u3002\u901a\u8fc7ICR\u3001IPR\u7b49KG\u6307\u6807\u91cf\u5316\u4e24\u8005\u4e4b\u95f4\u7684\u7ed3\u6784\u504f\u5dee\u548c\u8bed\u4e49\u5dee\u5f02\uff0c\u5e76\u5efa\u7acb\u81ea\u52a8\u5316\u5b9e\u65f6\u76d1\u63a7\u6846\u67b6\uff0c\u4f9d\u636e\u5386\u53f2\u6307\u6807\u5206\u5e03\u7684\u52a8\u6001\u5f02\u5e38\u9608\u503c\u6765\u8bc6\u522b\u548c\u6807\u8bb0\u663e\u8457\u504f\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3b\u52a8\u8bc6\u522b\u5e76\u6807\u8bb0\u786e\u5b9a\u6027KG\u4e0eLLM\u751f\u6210KG\u4e4b\u95f4\u7684\u663e\u8457\u504f\u5dee\uff0c\u4ece\u800c\u53ca\u65f6\u68c0\u6d4b\u751f\u6210\u5f0fAI\u4e2d\u7684\u8bed\u4e49\u5f02\u5e38\u6216\u5e7b\u89c9\u3002", "conclusion": "\u8fd9\u79cd\u7ed3\u6784\u5316\u3001\u6307\u6807\u9a71\u52a8\u7684\u786e\u5b9a\u6027KG\u4e0e\u52a8\u6001\u751f\u6210KG\u7684\u6bd4\u8f83\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u5f0fAI\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.03871", "pdf": "https://arxiv.org/pdf/2509.03871", "abs": "https://arxiv.org/abs/2509.03871", "authors": ["Yanbo Wang", "Yongcan Yu", "Jian Liang", "Ran He"], "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress", "summary": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.", "AI": {"tldr": "CoT\u63a8\u7406\u63d0\u5347\u4e86LLM\u6027\u80fd\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u7efc\u8ff0\u4e86CoT\u5728\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u9690\u79c1\u6027\u65b9\u9762\u7684\u6700\u65b0\u7814\u7a76\uff0c\u53d1\u73b0CoT\u867d\u7136\u6709\u6f5c\u529b\u63d0\u5347\u53ef\u4fe1\u5ea6\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u3001\u9c81\u68d2\u6027\u548c\u9690\u79c1\u65b9\u9762\u5b58\u5728\u66f4\u5927\u6f0f\u6d1e\u3002", "motivation": "CoT\u63a8\u7406\u5c3d\u7ba1\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u76ee\u524d\u5bf9CoT\u5982\u4f55\u5168\u9762\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\uff08\u5305\u62ec\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u9690\u79c1\u6027\uff09\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u63a8\u7406\u6a21\u578b\u548cCoT\u6280\u672f\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u884c\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u4fe1\u63a8\u7406\u7684\u4e94\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff1a\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u9690\u79c1\u6027\u3002\u5bf9\u6bcf\u4e2a\u7ef4\u5ea6\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u63d0\u4f9b\u4e86\u7814\u7a76\u6982\u8ff0\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u5176\u65b9\u6cd5\u3001\u53d1\u73b0\u548c\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u63a8\u7406\u6280\u672f\u5728\u7f13\u89e3\u5e7b\u89c9\u3001\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u63d0\u9ad8\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6f5c\u529b\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\uff0c\u4f46\u5c16\u7aef\u63a8\u7406\u6a21\u578b\u672c\u8eab\u5728\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u9690\u79c1\u65b9\u9762\u5f80\u5f80\u9762\u4e34\u76f8\u4f3c\u751a\u81f3\u66f4\u5927\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u672c\u5de5\u4f5c\u7efc\u5408\u4e86\u5bf9\u63a8\u7406\u53ef\u4fe1\u5ea6\u7684\u89c1\u89e3\uff0c\u65e8\u5728\u4e3aAI\u5b89\u5168\u793e\u533a\u63d0\u4f9b\u4e00\u4efd\u6709\u4ef7\u503c\u4e14\u53ca\u65f6\u7684\u8d44\u6e90\uff0c\u4ee5\u5e2e\u52a9\u5176\u4e86\u89e3\u63a8\u7406\u53ef\u4fe1\u5ea6\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2509.03887", "pdf": "https://arxiv.org/pdf/2509.03887", "abs": "https://arxiv.org/abs/2509.03887", "authors": ["Bu Jin", "Songen Gu", "Xiaotao Hu", "Yupeng Zheng", "Xiaoyang Guo", "Qian Zhang", "Xiaoxiao Long", "Wei Yin"], "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOccTENS\uff0c\u4e00\u79cd\u751f\u6210\u5f0f\u5360\u7528\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u65f6\u95f4\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\uff08TENS\uff09\u4efb\u52a1\u5e76\u5f15\u5165TensFormer\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u3001\u957f\u671f\u751f\u6210\u9000\u5316\u548c\u53ef\u63a7\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u4e14\u53ef\u63a7\u7684\u957f\u671f\u5360\u7528\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u81ea\u56de\u5f52\uff08AR\uff09\u7684\u5360\u7528\u4e16\u754c\u6a21\u578b\u867d\u7136\u80fd\u9884\u6d4b\u8f66\u8f86\u8fd0\u52a8\u548c\u672a\u6765\u5360\u7528\u573a\u666f\uff0c\u4f46\u666e\u904d\u5b58\u5728**\u6548\u7387\u4f4e\u4e0b**\u3001**\u957f\u671f\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u9000\u5316**\u4ee5\u53ca**\u7f3a\u4e4f\u53ef\u63a7\u6027**\u7b49\u95ee\u9898\uff0c\u4e14\u5360\u7528\u4e16\u754c\u6a21\u578b\u9700\u6355\u6349\u7cbe\u7ec6\u76843D\u51e0\u4f55\u548c\u52a8\u6001\u6f14\u5316\uff0c\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u5c06\u5360\u7528\u4e16\u754c\u6a21\u578b\u91cd\u6784\u4e3a\u65f6\u95f4\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\uff08TENS\uff09\u4efb\u52a1\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u5206\u89e3\u4e3a\u7a7a\u95f4\u9010\u5c3a\u5ea6\u751f\u6210\u548c\u65f6\u95f4\u9010\u573a\u666f\u9884\u6d4b\u3002\u901a\u8fc7**TensFormer**\u6709\u6548\u7ba1\u7406\u5360\u7528\u5e8f\u5217\u7684\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u4e3a\u589e\u5f3a\u59ff\u6001\u53ef\u63a7\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e00\u79cd\u6574\u4f53\u59ff\u6001\u805a\u5408\u7b56\u7565\uff0c\u7edf\u4e00\u5efa\u6a21\u5360\u7528\u548c\u81ea\u6211\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOccTENS\u5728\u5360\u7528\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "OccTENS\u901a\u8fc7\u521b\u65b0\u7684TENS\u4efb\u52a1\u91cd\u6784\u548cTensFormer\u6a21\u578b\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u957f\u671f\u5360\u7528\u751f\u6210\uff0c\u4e3a3D\u573a\u666f\u7684\u52a8\u6001\u6f14\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.03790", "pdf": "https://arxiv.org/pdf/2509.03790", "abs": "https://arxiv.org/abs/2509.03790", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "What fundamental properties of reward functions enable efficient\nsparse-reward reinforcement learning? We address this question through the lens\nof low-rank structure in reward matrices, showing that such structure induces a\nsharp transition from exponential to polynomial sample complexity, the first\nresult of this kind for sparse-reward RL. We introduce Policy-Aware Matrix\nCompletion (PAMC), which connects matrix completion theory with reinforcement\nlearning via a new analysis of policy-dependent sampling. Our framework\nprovides: (i) impossibility results for general sparse reward observation, (ii)\nreward-free representation learning from dynamics, (iii) distribution-free\nconfidence sets via conformal prediction, and (iv) robust completion guarantees\nthat degrade gracefully when low-rank structure is only approximate.\nEmpirically, we conduct a pre-registered evaluation across 100 systematically\nsampled domains, finding exploitable structure in over half. PAMC improves\nsample efficiency by factors between 1.6 and 2.1 compared to strong\nexploration, structured, and representation-learning baselines, while adding\nonly about 20 percent computational overhead.These results establish structural\nreward learning as a promising new paradigm, with immediate implications for\nrobotics, healthcare, and other safety-critical, sample-expensive applications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u5956\u52b1\u77e9\u9635\u7684\u4f4e\u79e9\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2d\u6837\u672c\u6548\u7387\u4ece\u6307\u6570\u7ea7\u5230\u591a\u9879\u5f0f\u7ea7\u7684\u8f6c\u53d8\uff0c\u5e76\u63d0\u51fa\u4e86\u7b56\u7565\u611f\u77e5\u77e9\u9635\u8865\u5168\uff08PAMC\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u63a2\u7a76\u4f55\u79cd\u5956\u52b1\u51fd\u6570\u7684\u6839\u672c\u5c5e\u6027\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7b56\u7565\u611f\u77e5\u77e9\u9635\u8865\u5168\uff08Policy-Aware Matrix Completion, PAMC\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u77e9\u9635\u8865\u5168\u7406\u8bba\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5206\u6790\u4f9d\u8d56\u7b56\u7565\u7684\u91c7\u6837\uff0c\u5229\u7528\u5956\u52b1\u77e9\u9635\u7684\u4f4e\u79e9\u7ed3\u6784\u3002PAMC\u6846\u67b6\u8fd8\u63d0\u4f9b\u4e86\uff1a(i) \u4e00\u822c\u7a00\u758f\u5956\u52b1\u89c2\u6d4b\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff1b(ii) \u57fa\u4e8e\u52a8\u6001\u7684\u65e0\u5956\u52b1\u8868\u793a\u5b66\u4e60\uff1b(iii) \u57fa\u4e8e\u4e00\u81f4\u6027\u9884\u6d4b\u7684\u65e0\u5206\u5e03\u7f6e\u4fe1\u96c6\uff1b(iv) \u5f53\u4f4e\u79e9\u7ed3\u6784\u8fd1\u4f3c\u65f6\uff0c\u4ecd\u80fd\u4f18\u96c5\u964d\u7ea7\u7684\u9c81\u68d2\u8865\u5168\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u8bc1\u660e\u4e86\u5956\u52b1\u77e9\u9635\u7684\u4f4e\u79e9\u7ed3\u6784\u80fd\u4f7f\u6837\u672c\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u6025\u5267\u8f6c\u53d8\u4e3a\u591a\u9879\u5f0f\u7ea7\u3002\u5b9e\u8bc1\u4e0a\uff0c\u5728100\u4e2a\u6d4b\u8bd5\u57df\u4e2d\u8d85\u8fc7\u4e00\u534a\u53d1\u73b0\u4e86\u53ef\u5229\u7528\u7684\u7ed3\u6784\u3002PAMC\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5c06\u6837\u672c\u6548\u7387\u63d0\u9ad8\u4e861.6\u52302.1\u500d\uff0c\u800c\u8ba1\u7b97\u5f00\u9500\u4ec5\u589e\u52a0\u7ea620%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u786e\u7acb\u4e86\u7ed3\u6784\u5316\u5956\u52b1\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0c\u5bf9\u673a\u5668\u4eba\u3001\u533b\u7597\u4fdd\u5065\u7b49\u5b89\u5168\u5173\u952e\u548c\u6837\u672c\u6210\u672c\u9ad8\u7684\u5e94\u7528\u5177\u6709\u76f4\u63a5\u610f\u4e49\u3002"}}
{"id": "2509.03863", "pdf": "https://arxiv.org/pdf/2509.03863", "abs": "https://arxiv.org/abs/2509.03863", "authors": ["Sina Khajehabdollahi", "Gautier Hamon", "Marko Cvjetko", "Pierre-Yves Oudeyer", "Cl\u00e9ment Moulin-Frier", "C\u00e9dric Colas"], "title": "Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata", "categories": ["cs.AI"], "comment": null, "summary": "Discovering diverse visual patterns in continuous cellular automata (CA) is\nchallenging due to the vastness and redundancy of high-dimensional behavioral\nspaces. Traditional exploration methods like Novelty Search (NS) expand locally\nby mutating known novel solutions but often plateau when local novelty is\nexhausted, failing to reach distant, unexplored regions. We introduce\nExpedition and Expansion (E&E), a hybrid strategy where exploration alternates\nbetween local novelty-driven expansions and goal-directed expeditions. During\nexpeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic\ngoals--descriptions of interesting but hypothetical patterns that drive\nexploration toward uncharted regions. By operating in semantic spaces that\nalign with human perception, E&E both evaluates novelty and generates goals in\nconceptually meaningful ways, enhancing the interpretability and relevance of\ndiscovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,\nemergent behaviors, E&E consistently uncovers more diverse solutions than\nexisting exploration methods. A genealogical analysis further reveals that\nsolutions originating from expeditions disproportionately influence long-term\nexploration, unlocking new behavioral niches that serve as stepping stones for\nsubsequent search. These findings highlight E&E's capacity to break through\nlocal novelty boundaries and explore behavioral landscapes in human-aligned,\ninterpretable ways, offering a promising template for open-ended exploration in\nartificial life and beyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faE&E\uff08Expedition and Expansion\uff09\u6df7\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u5c40\u90e8\u65b0\u9896\u6027\u9a71\u52a8\u6269\u5c55\u548cVLM\uff08\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff09\u5f15\u5bfc\u7684\u63a2\u9669\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u8fde\u7eed\u5143\u80de\u81ea\u52a8\u673a\uff08CA\uff09\u4e2d\u63a2\u7d22\u9ad8\u7ef4\u884c\u4e3a\u7a7a\u95f4\u65f6\u9047\u5230\u7684\u5c40\u90e8\u505c\u6ede\u95ee\u9898\uff0c\u4ece\u800c\u53d1\u73b0\u66f4\u591a\u6837\u5316\u3001\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u7684\u89c6\u89c9\u6a21\u5f0f\u3002", "motivation": "\u5728\u8fde\u7eed\u5143\u80de\u81ea\u52a8\u673a\u4e2d\u53d1\u73b0\u591a\u6837\u5316\u7684\u89c6\u89c9\u6a21\u5f0f\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u884c\u4e3a\u7a7a\u95f4\u5de8\u5927\u4e14\u5197\u4f59\u3002\u4f20\u7edf\u7684\u63a2\u7d22\u65b9\u6cd5\uff08\u5982\u65b0\u9896\u6027\u641c\u7d22\uff09\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u65e0\u6cd5\u63a2\u7d22\u5230\u9065\u8fdc\u3001\u672a\u77e5\u7684\u533a\u57df\u3002", "method": "\u5f15\u5165E&E\u6df7\u5408\u7b56\u7565\uff0c\u4ea4\u66ff\u8fdb\u884c\u5c40\u90e8\u65b0\u9896\u6027\u9a71\u52a8\u7684\u6269\u5c55\u548c\u76ee\u6807\u5bfc\u5411\u7684\u63a2\u9669\u3002\u63a2\u9669\u9636\u6bb5\u5229\u7528VLM\u751f\u6210\u63cf\u8ff0\u5047\u8bbe\u6709\u8da3\u6a21\u5f0f\u7684\u8bed\u8a00\u76ee\u6807\uff0c\u5f15\u5bfc\u63a2\u7d22\u8fdb\u5165\u672a\u5f00\u53d1\u533a\u57df\u3002\u901a\u8fc7\u5728\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0cE&E\u4ee5\u6982\u5ff5\u4e0a\u6709\u610f\u4e49\u7684\u65b9\u5f0f\u8bc4\u4f30\u65b0\u9896\u6027\u5e76\u751f\u6210\u76ee\u6807\u3002", "result": "\u5728Flow Lenia\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0cE&E\u6bd4\u73b0\u6709\u63a2\u7d22\u65b9\u6cd5\u53d1\u73b0\u66f4\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u56e0\u5206\u6790\u663e\u793a\uff0c\u63a2\u9669\u4ea7\u751f\u7684\u89e3\u51b3\u65b9\u6848\u5bf9\u957f\u671f\u63a2\u7d22\u5177\u6709\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u5f00\u542f\u4e86\u65b0\u7684\u884c\u4e3a\u751f\u6001\u4f4d\uff0c\u4e3a\u540e\u7eed\u641c\u7d22\u63d0\u4f9b\u4e86\u8df3\u677f\u3002", "conclusion": "E&E\u80fd\u591f\u7a81\u7834\u5c40\u90e8\u65b0\u9896\u6027\u8fb9\u754c\uff0c\u4ee5\u4eba\u7c7b\u5bf9\u9f50\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u63a2\u7d22\u884c\u4e3a\u666f\u89c2\uff0c\u4e3a\u4eba\u5de5\u751f\u547d\u53ca\u5176\u4ed6\u9886\u57df\u7684\u5f00\u653e\u5f0f\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u672c\u3002"}}
{"id": "2509.03888", "pdf": "https://arxiv.org/pdf/2509.03888", "abs": "https://arxiv.org/abs/2509.03888", "authors": ["Cheng Wang", "Zeming Wei", "Qin Liu", "Muhao Chen"], "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u63a2\u9488\u7684LLM\u5b89\u5168\u63a2\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u53ef\u80fd\u5b66\u4e60\u8868\u5c42\u6a21\u5f0f\u800c\u975e\u8bed\u4e49\u6709\u5bb3\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u865a\u5047\u7684\u5b89\u5168\u611f\uff0c\u5e76\u5f3a\u8c03\u9700\u91cd\u65b0\u8bbe\u8ba1\u6a21\u578b\u4e0e\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5177\u6709\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5bf9\u6709\u5bb3\u6307\u4ee4\u7684\u4f9d\u4ece\u6027\u5f15\u53d1\u5b89\u5168\u62c5\u5fe7\u3002\u73b0\u6709\u57fa\u4e8e\u63a2\u9488\u7684\u65b9\u6cd5\u88ab\u7528\u4e8e\u5b89\u5168\u68c0\u6d4b\uff0c\u4f46\u56e0\u5176\u5728\u5206\u5e03\u5916\u6027\u80fd\u4e0d\u4f73\uff0c\u7814\u7a76\u8005\u6000\u7591\u63a2\u9488\u5b66\u4e60\u7684\u662f\u8868\u5c42\u6a21\u5f0f\u800c\u975e\u8bed\u4e49\u4e0a\u7684\u6709\u5bb3\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a\u5c55\u793a\u7b80\u5355n-gram\u65b9\u6cd5\u7684\u76f8\u8fd1\u6027\u80fd\uff1b\u4f7f\u7528\u8bed\u4e49\u6e05\u6d17\u7684\u6570\u636e\u96c6\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u5047\u8bbe\uff1b\u8be6\u7ec6\u5206\u6790\u6a21\u5f0f\u4f9d\u8d56\u6027\u3002", "result": "\u8bc1\u5b9e\u4e86\u63a2\u9488\u5b66\u4e60\u7684\u662f\u6307\u4ee4\u6a21\u5f0f\u548c\u89e6\u53d1\u8bcd\u7b49\u8868\u5c42\u6a21\u5f0f\uff0c\u800c\u975e\u8bed\u4e49\u4e0a\u7684\u6709\u5bb3\u6027\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8e\u63a2\u9488\u7684\u65b9\u6cd5\u6240\u5e26\u6765\u7684\u865a\u5047\u5b89\u5168\u611f\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u63a2\u9488\u7684LLM\u5b89\u5168\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u65e0\u6cd5\u63d0\u4f9b\u771f\u6b63\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u91cd\u65b0\u8bbe\u8ba1\u6a21\u578b\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u63a8\u52a8\u66f4\u8d1f\u8d23\u4efb\u7684\u7814\u7a76\u3002"}}
{"id": "2509.03893", "pdf": "https://arxiv.org/pdf/2509.03893", "abs": "https://arxiv.org/abs/2509.03893", "authors": ["Stefan Stojanov", "Linan Zhao", "Yunzhi Zhang", "Daniel L. K. Yamins", "Jiajun Wu"], "title": "Weakly-Supervised Learning of Dense Functional Correspondences", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/", "summary": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8de8\u7c7b\u522b\u56fe\u50cf\u7684\u5bc6\u96c6\u5bf9\u5e94\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u529f\u80fd\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f2a\u6807\u6ce8\u529f\u80fd\u6027\u90e8\u4f4d\uff0c\u7ed3\u5408\u5bc6\u96c6\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u5efa\u7acb\u5bc6\u96c6\u529f\u80fd\u6027\u5bf9\u5e94\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5173\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5bc6\u96c6\u5bf9\u5e94\u5bf9\u4e8e\u5f62\u72b6\u91cd\u5efa\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u5728\u8de8\u7c7b\u522b\u56fe\u50cf\u5339\u914d\u7684\u6311\u6218\u6027\u8bbe\u7f6e\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u7acb\u5bf9\u5e94\u3002\u7814\u7a76\u52a8\u673a\u662f\u57fa\u4e8e\u5bf9\u8c61\u529f\u80fd\uff08\u5373\u5bf9\u8c61\u5bf9\u5176\u4ed6\u5bf9\u8c61\u4ea7\u751f\u7684\u5f71\u54cd\uff09\u53ef\u4ee5\u6307\u5bfc\u5bf9\u5e94\u5efa\u7acb\u7684\u89c2\u5bdf\uff0c\u56e0\u4e3a\u5b9e\u73b0\u7279\u5b9a\u529f\u80fd\u7684\u90e8\u4f4d\u901a\u5e38\u5728\u5f62\u72b6\u548c\u5916\u89c2\u4e0a\u76f8\u4f3c\u3002", "method": "\u57fa\u4e8e\u5bf9\u8c61\u529f\u80fd\u7684\u89c2\u5bdf\uff0c\u5b9a\u4e49\u4e86\u5bc6\u96c6\u529f\u80fd\u6027\u5bf9\u5e94\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u6765\u89e3\u51b3\u9884\u6d4b\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u6838\u5fc3\u5728\u4e8e\uff1a\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c\u4f2a\u6807\u6ce8\uff0c\u4ee5\u83b7\u53d6\u529f\u80fd\u6027\u90e8\u4f4d\uff1b\u7136\u540e\u5c06\u5176\u4e0e\u6765\u81ea\u50cf\u7d20\u5bf9\u5e94\u7684\u5bc6\u96c6\u5bf9\u6bd4\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5c06\u529f\u80fd\u6027\u548c\u7a7a\u95f4\u77e5\u8bc6\u84b8\u998f\u5230\u4e00\u4e2a\u65b0\u6a21\u578b\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u5408\u6210\u548c\u771f\u5b9e\u8bc4\u4f30\u6570\u636e\u96c6\u4f5c\u4e3a\u4efb\u52a1\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7531\u73b0\u6210\u7684\u81ea\u76d1\u7763\u56fe\u50cf\u8868\u793a\u548c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\u7684\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u8de8\u7c7b\u522b\u5bc6\u96c6\u5bf9\u5e94\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5bf9\u8c61\u529f\u80fd\u548c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5e94\u5efa\u7acb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2509.03810", "pdf": "https://arxiv.org/pdf/2509.03810", "abs": "https://arxiv.org/abs/2509.03810", "authors": ["Xiannan Huang", "Shuhan Qiu", "Jiayuan Du", "Chao Yang"], "title": "Online time series prediction using feature adjustment", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is of significant importance across various domains.\nHowever, it faces significant challenges due to distribution shift. This issue\nbecomes particularly pronounced in online deployment scenarios where data\narrives sequentially, requiring models to adapt continually to evolving\npatterns. Current time series online learning methods focus on two main\naspects: selecting suitable parameters to update (e.g., final layer weights or\nadapter modules) and devising suitable update strategies (e.g., using recent\nbatches, replay buffers, or averaged gradients). We challenge the conventional\nparameter selection approach, proposing that distribution shifts stem from\nchanges in underlying latent factors influencing the data. Consequently,\nupdating the feature representations of these latent factors may be more\neffective. To address the critical problem of delayed feedback in multi-step\nforecasting (where true values arrive much later than predictions), we\nintroduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in\nZ-space). ADAPT-Z utilizes an adapter module that leverages current feature\nrepresentations combined with historical gradient information to enable robust\nparameter updates despite the delay. Extensive experiments demonstrate that our\nmethod consistently outperforms standard base models without adaptation and\nsurpasses state-of-the-art online learning approaches across multiple datasets.\nThe code is available at https://github.com/xiannanhuang/ADAPT-Z.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faADAPT-Z\uff0c\u4e00\u79cd\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u65b0\u6f5c\u5728\u56e0\u5b50\u7279\u5f81\u8868\u793a\u548c\u5229\u7528\u5386\u53f2\u68af\u5ea6\u89e3\u51b3\u5206\u5e03\u6f02\u79fb\u548c\u591a\u6b65\u9884\u6d4b\u4e2d\u7684\u5ef6\u8fdf\u53cd\u9988\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u56e0\u5206\u5e03\u6f02\u79fb\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7ebf\u90e8\u7f72\u4e2d\u9700\u6301\u7eed\u9002\u5e94\u3002\u73b0\u6709\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u5728\u53c2\u6570\u9009\u62e9\u548c\u66f4\u65b0\u7b56\u7565\u4e0a\u6709\u9650\u3002\u672c\u6587\u8ba4\u4e3a\u5206\u5e03\u6f02\u79fb\u6e90\u4e8e\u6f5c\u5728\u56e0\u5b50\u53d8\u5316\uff0c\u66f4\u65b0\u7279\u5f81\u8868\u793a\u53ef\u80fd\u66f4\u6709\u6548\u3002\u591a\u6b65\u9884\u6d4b\u4e2d\u7684\u5ef6\u8fdf\u53cd\u9988\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faADAPT-Z\uff08Z\u7a7a\u95f4\u6301\u7eed\u8ffd\u8e2a\u7684\u81ea\u52a8\u589e\u91cf\u8c03\u6574\uff09\u3002\u6311\u6218\u4f20\u7edf\u53c2\u6570\u9009\u62e9\uff0c\u5efa\u8bae\u66f4\u65b0\u6f5c\u5728\u56e0\u5b50\u7279\u5f81\u8868\u793a\u3002\u5229\u7528\u9002\u914d\u5668\u6a21\u5757\uff0c\u7ed3\u5408\u5f53\u524d\u7279\u5f81\u8868\u793a\u548c\u5386\u53f2\u68af\u5ea6\u4fe1\u606f\uff0c\u5b9e\u73b0\u5ef6\u8fdf\u53cd\u9988\u4e0b\u7684\u9c81\u68d2\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cADAPT-Z\u6301\u7eed\u4f18\u4e8e\u672a\u9002\u5e94\u7684\u6807\u51c6\u57fa\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "ADAPT-Z\u901a\u8fc7\u65b0\u9896\u7684\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff08\u66f4\u65b0\u6f5c\u5728\u56e0\u5b50\u7279\u5f81\u8868\u793a\uff09\u548c\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\u5e94\u5bf9\u5ef6\u8fdf\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03890", "pdf": "https://arxiv.org/pdf/2509.03890", "abs": "https://arxiv.org/abs/2509.03890", "authors": ["Yineng Yan", "Xidong Wang", "Jin Seng Cheng", "Ran Hu", "Wentao Guan", "Nahid Farahmand", "Hengte Lin", "Yue Li"], "title": "FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of agentic AI, powered by Large Language Models (LLMs), marks a\nparadigm shift from reactive generative systems to proactive, goal-oriented\nautonomous agents capable of sophisticated planning, memory, and tool use. This\nevolution presents a novel opportunity to address long-standing challenges in\ncomplex digital environments. Core tasks on Consumer-to-Consumer (C2C)\ne-commerce platforms often require users to navigate complex Graphical User\nInterfaces (GUIs), making the experience time-consuming for both buyers and\nsellers. This paper introduces a novel approach to simplify these interactions\nthrough an LLM-powered agentic assistant. This agent functions as a new,\nconversational entry point to the marketplace, shifting the primary interaction\nmodel from a complex GUI to an intuitive AI agent. By interpreting natural\nlanguage commands, the agent automates key high-friction workflows. For\nsellers, this includes simplified updating and renewal of listings, and the\nability to send bulk messages. For buyers, the agent facilitates a more\nefficient product discovery process through conversational search. We present\nthe architecture for Facebook Marketplace Assistant (FaMA), arguing that this\nagentic, conversational paradigm provides a lightweight and more accessible\nalternative to traditional app interfaces, allowing users to manage their\nmarketplace activities with greater efficiency. Experiments show FaMA achieves\na 98% task success rate on solving complex tasks on the marketplace and enables\nup to a 2x speedup on interaction time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u4ecb\u7ecd\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u52a9\u624bFaMA\uff0c\u7528\u4e8e\u7b80\u5316C2C\u7535\u5546\u5e73\u53f0\uff08\u5982Facebook Marketplace\uff09\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\uff0c\u63d0\u9ad8\u7528\u6237\u6548\u7387\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u548c\u4ea4\u4e92\u65f6\u95f4\u52a0\u901f\u3002", "motivation": "C2C\u7535\u5546\u5e73\u53f0\u7684\u7528\u6237\u754c\u9762\uff08GUI\uff09\u590d\u6742\uff0c\u5bfc\u81f4\u4e70\u5356\u53cc\u65b9\u5b8c\u6210\u6838\u5fc3\u4efb\u52a1\u8017\u65f6\u3002\u65b0\u5174\u7684\u667a\u80fdAI\uff08\u7531LLM\u9a71\u52a8\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u957f\u671f\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u5f15\u5165\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4ee3\u7406\u52a9\u624b\uff08FaMA\uff09\uff0c\u4f5c\u4e3a\u5e02\u573a\u7684\u65b0\u578b\u5bf9\u8bdd\u5f0f\u5165\u53e3\u3002\u901a\u8fc7\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u8be5\u52a9\u624b\u81ea\u52a8\u5316\u4e86\u9ad8\u6469\u64e6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u7b80\u5316\u5356\u5bb6\u7684\u5546\u54c1\u66f4\u65b0\u548c\u6279\u91cf\u6d88\u606f\u53d1\u9001\uff0c\u4ee5\u53ca\u4e3a\u4e70\u5bb6\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u5bf9\u8bdd\u5f0f\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFaMA\u5728\u89e3\u51b3\u5e02\u573a\u4e0a\u7684\u590d\u6742\u4efb\u52a1\u65f6\u8fbe\u5230\u4e8698%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5c06\u4ea4\u4e92\u65f6\u95f4\u52a0\u5feb\u4e86\u591a\u8fbe2\u500d\u3002", "conclusion": "\u667a\u80fd\u7684\u5bf9\u8bdd\u5f0f\u8303\u5f0f\u4e3a\u4f20\u7edf\u5e94\u7528\u754c\u9762\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u66f4\u6613\u4e8e\u8bbf\u95ee\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u7ba1\u7406\u5176\u5e02\u573a\u6d3b\u52a8\u3002"}}
{"id": "2509.03891", "pdf": "https://arxiv.org/pdf/2509.03891", "abs": "https://arxiv.org/abs/2509.03891", "authors": ["Gowen Loo", "Chang Liu", "Qinghong Yin", "Xiang Chen", "Jiawei Chen", "Jingyuan Zhang", "Yu Tian"], "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709LLM\u79fb\u52a8\u4ee3\u7406\u5728\u7406\u89e3\u3001\u5916\u90e8\u4ea4\u4e92\u548c\u8bb0\u5fc6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51faMobileRAG\u6846\u67b6\uff08\u57fa\u4e8eRAG\uff09\u53ca\u5176\u8bc4\u4f30\u57fa\u51c6MobileRAG-Eval\u3002MobileRAG\u5728\u771f\u5b9e\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\u6027\u80fd\u63d0\u534710.3%\uff0c\u4e14\u64cd\u4f5c\u6b65\u9aa4\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709LLM\u79fb\u52a8\u4ee3\u7406\u4e3b\u8981\u9762\u4e34\u4e09\u4e2a\u95ee\u9898\uff1a1) \u8fc7\u5ea6\u4f9d\u8d56LLM\u7406\u89e3\u80fd\u529b\u5bfc\u81f4\u9519\u8bef\u64cd\u4f5c\u6216\u9057\u6f0f\u6b65\u9aa4\uff1b2) \u7f3a\u4e4f\u4e0e\u5916\u90e8\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5f53\u5e94\u7528\u65e0\u6cd5\u6ee1\u8db3\u67e5\u8be2\u65f6\u4efb\u52a1\u7ec8\u6b62\uff1b3) \u7f3a\u4e4f\u8bb0\u5fc6\u80fd\u529b\uff0c\u6bcf\u6b21\u6307\u4ee4\u9700\u91cd\u5efa\u754c\u9762\u4e14\u65e0\u6cd5\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faMobileRAG\u6846\u67b6\uff0c\u4e00\u4e2a\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6539\u8fdb\u7684\u79fb\u52a8\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542bInterRAG\u3001LocalRAG\u548cMemRAG\u3002MobileRAG\u5229\u7528RAG\u66f4\u5feb\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u7528\u6237\u67e5\u8be2\uff0c\u5e76\u5b8c\u6210\u590d\u6742\u3001\u957f\u5e8f\u5217\u7684\u79fb\u52a8\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u5f15\u5165MobileRAG-Eval\uff0c\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u590d\u6742\u3001\u9700\u5916\u90e8\u77e5\u8bc6\u8f85\u52a9\u7684\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u4efb\u52a1\u7684\u6311\u6218\u6027\u57fa\u51c6\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30MobileRAG\u7684\u6027\u80fd\u3002", "result": "\u5728MobileRAG-Eval\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMobileRAG\u80fd\u591f\u8f7b\u677e\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u79fb\u52a8\u4efb\u52a1\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u534710.3%\uff0c\u4e14\u64cd\u4f5c\u6b65\u9aa4\u66f4\u5c11\u3002", "conclusion": "MobileRAG\u901a\u8fc7RAG\u589e\u5f3a\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709LLM\u79fb\u52a8\u4ee3\u7406\u5728\u7406\u89e3\u3001\u5916\u90e8\u4ea4\u4e92\u548c\u8bb0\u5fc6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5904\u7406\u590d\u6742\u771f\u5b9e\u79fb\u52a8\u4efb\u52a1\u65f6\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.03895", "pdf": "https://arxiv.org/pdf/2509.03895", "abs": "https://arxiv.org/abs/2509.03895", "authors": ["Phuoc-Nguyen Bui", "Khanh-Binh Nguyen", "Hyunseung Choo"], "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model", "categories": ["cs.CV"], "comment": "ICCV 2025 - LIMIT Workshop", "summary": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.", "AI": {"tldr": "\u4e3a\u89e3\u51b3CLIP\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faAttn-Adapter\u5728\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5176\u9002\u5e94\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u5ea7\u6a21\u578b\uff0c\u5e76\u5728\u6cdb\u5316\u6027\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u79bb\u7ebf\u5fae\u8c03\u65b9\u6848\u8ba1\u7b97\u5bc6\u96c6\u4e14\u6613\u8fc7\u62df\u5408\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faAttn-Adapter\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u5728\u7ebf\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3aCLIP\u7684\u9002\u5e94\u6027\uff0c\u4e14\u65e0\u9700\u91cd\u8bad\u7ec3\u57fa\u5ea7\u6a21\u578b\u3002\u5176\u6838\u5fc3\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1aMemory Attn-Adapter\uff08\u5229\u7528\u652f\u6301\u6837\u672c\u7ec6\u5316\u7c7b\u522b\u5d4c\u5165\uff09\u548cLocal-Global Attn-Adapter\uff08\u6574\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u4ee5\u4e30\u5bcc\u56fe\u50cf\u5d4c\u5165\uff09\u3002", "result": "Attn-Adapter\u5728\u8de8\u7c7b\u522b\u548c\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u80fd\u826f\u597d\u5730\u6269\u5c55\u5230\u4e0d\u540c\u7684CLIP\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "Attn-Adapter\u901a\u8fc7\u5176\u521b\u65b0\u7684\u53cc\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86CLIP\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u57fa\u5ea7\u6a21\u578b\u3001\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
