{"id": "2508.03712", "pdf": "https://arxiv.org/pdf/2508.03712", "abs": "https://arxiv.org/abs/2508.03712", "authors": ["Agrima Seth", "Monojit Choudhary", "Sunayana Sitaram", "Kentaro Toyama", "Aditya Vashistha", "Kalika Bali"], "title": "How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion", "categories": ["cs.CL"], "comment": "Accepted to AIES 2025", "summary": "Representational bias in large language models (LLMs) has predominantly been\nmeasured through single-response interactions and has focused on Global\nNorth-centric identities like race and gender. We expand on that research by\nconducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded\nrepresentational biases are and how they extend to less-explored dimensions of\nidentity. We prompt GPT-4 Turbo to generate over 7,200 stories about\nsignificant life events (such as weddings) in India, using prompts designed to\nencourage diversity to varying extents. Comparing the diversity of religious\nand caste representation in the outputs against the actual population\ndistribution in India as recorded in census data, we quantify the presence and\n\"stickiness\" of representational bias in the LLM for religion and caste. We\nfind that GPT-4 responses consistently overrepresent culturally dominant groups\nfar beyond their statistical representation, despite prompts intended to\nencourage representational diversity. Our findings also suggest that\nrepresentational bias in LLMs has a winner-take-all quality that is more biased\nthan the likely distribution bias in their training data, and repeated\nprompt-based nudges have limited and inconsistent efficacy in dislodging these\nbiases. These results suggest that diversifying training data alone may not be\nsufficient to correct LLM bias, highlighting the need for more fundamental\nchanges in model development. Dataset and Codebook:\nhttps://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4 Turbo\u5728\u751f\u6210\u5370\u5ea6\u76f8\u5173\u6545\u4e8b\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u3001\u96be\u4ee5\u7ea0\u6b63\u7684\u5b97\u6559\u548c\u79cd\u59d3\u4ee3\u8868\u6027\u504f\u5dee\uff0c\u5373\u4f7f\u4f7f\u7528\u65e8\u5728\u9f13\u52b1\u591a\u6837\u6027\u7684\u63d0\u793a\uff0c\u6a21\u578b\u4e5f\u503e\u5411\u4e8e\u8fc7\u5ea6\u4ee3\u8868\u4e3b\u5bfc\u7fa4\u4f53\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8868\u5f81\u504f\u5dee\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u6b21\u4ea4\u4e92\u53ca\u5317\u7f8e\u4e2d\u5fc3\u8eab\u4efd\uff08\u5982\u79cd\u65cf\u3001\u6027\u522b\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u6b64\u8303\u56f4\uff0c\u63a2\u7a76LLM\u4e2d\u8868\u5f81\u504f\u5dee\u7684\u6df1\u5ea6\u53ca\u5176\u5bf9\u5370\u5ea6\u5b97\u6559\u548c\u79cd\u59d3\u7b49\u8f83\u5c11\u63a2\u7d22\u7684\u8eab\u4efd\u7ef4\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u5bf9GPT-4 Turbo\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5ba1\u8ba1\u3002\u901a\u8fc7\u8bbe\u8ba1\u4e0d\u540c\u7a0b\u5ea6\u9f13\u52b1\u591a\u6837\u6027\u7684\u63d0\u793a\uff0c\u751f\u6210\u4e86\u8d85\u8fc77,200\u4e2a\u5173\u4e8e\u5370\u5ea6\u91cd\u8981\u751f\u6d3b\u4e8b\u4ef6\uff08\u5982\u5a5a\u793c\uff09\u7684\u6545\u4e8b\u3002\u5c06\u8f93\u51fa\u4e2d\u5b97\u6559\u548c\u79cd\u59d3\u4ee3\u8868\u7684\u591a\u6837\u6027\u4e0e\u5370\u5ea6\u5b9e\u9645\u4eba\u53e3\u666e\u67e5\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\uff0c\u91cf\u5316LLM\u4e2d\u8868\u5f81\u504f\u5dee\u7684\u5b58\u5728\u548c\u201c\u7c98\u6027\u201d\u3002", "result": "GPT-4\u7684\u54cd\u5e94\u6301\u7eed\u8fc7\u5ea6\u4ee3\u8868\u6587\u5316\u4e3b\u5bfc\u7fa4\u4f53\uff0c\u8fdc\u8fdc\u8d85\u51fa\u5176\u7edf\u8ba1\u6bd4\u4f8b\uff0c\u5373\u4f7f\u63d0\u793a\u65e8\u5728\u9f13\u52b1\u4ee3\u8868\u6027\u591a\u6837\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cLLM\u4e2d\u7684\u8868\u5f81\u504f\u5dee\u5177\u6709\u201c\u8d62\u8005\u901a\u5403\u201d\u7684\u7279\u70b9\uff0c\u6bd4\u8bad\u7ec3\u6570\u636e\u4e2d\u53ef\u80fd\u7684\u5206\u5e03\u504f\u5dee\u66f4\u4e3a\u4e25\u91cd\uff0c\u4e14\u91cd\u590d\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u5e72\u9884\u5728\u6d88\u9664\u8fd9\u4e9b\u504f\u5dee\u65b9\u9762\u6548\u679c\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u4ec5\u901a\u8fc7\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u4e0d\u8db3\u4ee5\u7ea0\u6b63LLM\u504f\u5dee\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u5f00\u53d1\u9700\u8981\u8fdb\u884c\u66f4\u6839\u672c\u7684\u6539\u53d8\u3002"}}
{"id": "2508.03716", "pdf": "https://arxiv.org/pdf/2508.03716", "abs": "https://arxiv.org/abs/2508.03716", "authors": ["Paul Richmond", "Prarit Agarwal", "Borun Chowdhury", "Vasilis Niarchos", "Constantinos Papageorgakis"], "title": "FeynTune: Large Language Models for High-Energy Theory", "categories": ["cs.CL", "cs.LG", "hep-th"], "comment": "16 pages", "summary": "We present specialized Large Language Models for theoretical High-Energy\nPhysics, obtained as 20 fine-tuned variants of the 8-billion parameter\nLlama-3.1 model. Each variant was trained on arXiv abstracts (through August\n2024) from different combinations of hep-th, hep-ph and gr-qc. For a\ncomparative study, we also trained models on datasets that contained abstracts\nfrom disparate fields such as the q-bio and cs categories. All models were\nfine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and\nvarying dataset sizes, and outperformed the base model on hep-th abstract\ncompletion tasks. We compare performance against leading commercial LLMs\n(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing\nspecialized language models for High-Energy Theoretical Physics.", "AI": {"tldr": "\u672c\u6587\u5bf9Llama-3.1\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u521b\u5efa\u4e8620\u4e2a\u4e13\u95e8\u7528\u4e8e\u9ad8\u80fd\u7406\u8bba\u7269\u7406\u7684LLM\u53d8\u4f53\uff0c\u5e76\u5728\u76f8\u5173\u6458\u8981\u8865\u5168\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u57fa\u7840\u6a21\u578b\u548c\u5546\u4e1aLLM\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e13\u95e8\u670d\u52a1\u4e8e\u9ad8\u80fd\u7406\u8bba\u7269\u7406\u9886\u57df\u7684LLM\uff0c\u5e76\u63a2\u7d22\u5176\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u671f\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "method": "\u57fa\u4e8e80\u4ebf\u53c2\u6570\u7684Llama-3.1\u6a21\u578b\uff0c\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u548c\u4e0d\u540c\u6570\u636e\u96c6\u5927\u5c0f\u8fdb\u884c\u5fae\u8c03\u3002\u8bad\u7ec3\u6570\u636e\u6e90\u81eaarXiv\u4e2dhep-th\u3001hep-ph\u3001gr-qc\u7b49\u9ad8\u80fd\u7269\u7406\u76f8\u5173\u7c7b\u522b\u6458\u8981\uff0c\u5e76\u52a0\u5165\u4e86q-bio\u548ccs\u7b49\u8de8\u9886\u57df\u6458\u8981\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002\u6a21\u578b\u6027\u80fd\u5728hep-th\u6458\u8981\u8865\u5168\u4efb\u52a1\u4e0a\u4e0e\u57fa\u7840\u6a21\u578b\u53ca\u4e3b\u6d41\u5546\u4e1aLLM\uff08ChatGPT\u3001Claude\u3001Gemini\u3001DeepSeek\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u6240\u6709\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728hep-th\u6458\u8981\u8865\u5168\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7840Llama-3.1\u6a21\u578b\u3002\u7814\u7a76\u8fd8\u4e3a\u9ad8\u80fd\u7406\u8bba\u7269\u7406\u9886\u57df\u4e13\u4e1a\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u901a\u7528LLM\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u6458\u8981\u7684\u5fae\u8c03\uff0c\u80fd\u591f\u6709\u6548\u6784\u5efa\u9ad8\u80fd\u7406\u8bba\u7269\u7406\u9886\u57df\u7684\u4e13\u4e1a\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u672a\u6765\u8be5\u9886\u57df\u4e13\u4e1a\u6a21\u578b\u7684\u4f18\u5316\u4e0e\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.03719", "pdf": "https://arxiv.org/pdf/2508.03719", "abs": "https://arxiv.org/abs/2508.03719", "authors": ["Abhay Vijayvargia", "Ajay Nagpal", "Kundeshwar Pundalik", "Atharva Savarkar", "Smita Gautam", "Pankaj Singh", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Indian farmers often lack timely, accessible, and language-friendly\nagricultural advice, especially in rural areas with low literacy. To address\nthis gap in accessibility, this paper presents a novel AI-powered agricultural\nchatbot, Krishi Sathi, designed to support Indian farmers by providing\npersonalized, easy-to-understand answers to their queries through both text and\nspeech. The system's intelligence stems from an IFT model, subsequently refined\nthrough fine-tuning on Indian agricultural knowledge across three curated\ndatasets. Unlike traditional chatbots that respond to one-off questions, Krishi\nSathi follows a structured, multi-turn conversation flow to gradually collect\nthe necessary details from the farmer, ensuring the query is fully understood\nbefore generating a response. Once the intent and context are extracted, the\nsystem performs Retrieval-Augmented Generation (RAG) by first fetching\ninformation from a curated agricultural database and then generating a tailored\nresponse using the IFT model. The chatbot supports both English and Hindi\nlanguages, with speech input and output features (via ASR and TTS) to make it\naccessible for users with low literacy or limited digital skills. This work\ndemonstrates how combining intent-driven dialogue flows, instruction-tuned\nmodels, and retrieval-based generation can improve the quality and\naccessibility of digital agricultural support in India.\n  This approach yielded strong results, with the system achieving a query\nresponse accuracy of 97.53%, 91.35% contextual relevance and personalization,\nand a query completion rate of 97.53%. The average response time remained under\n6 seconds, ensuring timely support for users across both English and Hindi\ninteractions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aKrishi Sathi\u7684AI\u519c\u4e1a\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u901a\u8fc7\u4e2a\u6027\u5316\u3001\u6613\u4e8e\u7406\u89e3\u7684\u6587\u672c\u548c\u8bed\u97f3\u56de\u590d\uff0c\u4e3a\u5370\u5ea6\u519c\u6c11\u63d0\u4f9b\u53ca\u65f6\u3001\u53ef\u53ca\u7684\u519c\u4e1a\u5efa\u8bae\u3002", "motivation": "\u5370\u5ea6\u519c\u6c11\uff0c\u7279\u522b\u662f\u519c\u6751\u5730\u533a\u6587\u5316\u6c34\u5e73\u8f83\u4f4e\u7684\u519c\u6c11\uff0c\u7f3a\u4e4f\u53ca\u65f6\u3001\u53ef\u53ca\u4e14\u8bed\u8a00\u53cb\u597d\u7684\u519c\u4e1a\u5efa\u8bae\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eIFT\u6a21\u578b\u5e76\u7ecf\u8fc7\u5370\u5ea6\u519c\u4e1a\u77e5\u8bc6\u5fae\u8c03\u7684AI\u804a\u5929\u673a\u5668\u4ebaKrishi Sathi\u3002\u7cfb\u7edf\u91c7\u7528\u7ed3\u6784\u5316\u7684\u591a\u8f6e\u5bf9\u8bdd\u6d41\u7a0b\u6765\u5145\u5206\u7406\u89e3\u7528\u6237\u67e5\u8be2\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u4ece\u519c\u4e1a\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u4fe1\u606f\u5e76\u751f\u6210\u5b9a\u5236\u5316\u56de\u590d\u3002\u4e3a\u63d0\u9ad8\u53ef\u53ca\u6027\uff0c\u652f\u6301\u82f1\u8bed\u548c\u5370\u5730\u8bed\uff0c\u5e76\u63d0\u4f9b\u8bed\u97f3\u8f93\u5165/\u8f93\u51fa\uff08ASR\u548cTTS\uff09\u529f\u80fd\u3002", "result": "Krishi Sathi\u5728\u67e5\u8be2\u54cd\u5e94\u51c6\u786e\u7387\u548c\u5b8c\u6210\u7387\u4e0a\u5747\u8fbe\u523097.53%\uff0c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u8fbe\u523091.35%\u3002\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e6\u79d2\uff0c\u786e\u4fdd\u4e86\u53ca\u65f6\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u610f\u56fe\u9a71\u52a8\u7684\u5bf9\u8bdd\u6d41\u3001\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5370\u5ea6\u6570\u5b57\u519c\u4e1a\u652f\u6301\u7684\u8d28\u91cf\u548c\u53ef\u53ca\u6027\u3002"}}
{"id": "2508.03726", "pdf": "https://arxiv.org/pdf/2508.03726", "abs": "https://arxiv.org/abs/2508.03726", "authors": ["Jaydip Sen", "Harshitha Puvvala", "Subhasis Dasgupta"], "title": "Hierarchical Verification of Speculative Beams for Accelerating LLM Inference", "categories": ["cs.CL"], "comment": "This paper was accepted for oral presentation and publication in the\n  3rd International Conference on Data Science and Network Engineering (ICDSNE\n  2025), organized at NIT, Agartala, India, from July 25 to 26, 2025. The paper\n  is 12 pages long, and it contains 3 tables and 4 figures. This is NOT the\n  final paper, which will be published in the Springer-published proceedings", "summary": "Large language models (LLMs) have achieved remarkable success across diverse\nnatural language processing tasks but face persistent challenges in inference\nefficiency due to their autoregressive nature. While speculative decoding and\nbeam sampling offer notable improvements, traditional methods verify draft\nsequences sequentially without prioritization, leading to unnecessary\ncomputational overhead. This work proposes the Hierarchical Verification Tree\n(HVT), a novel framework that restructures speculative beam decoding by\nprioritizing high-likelihood drafts and enabling early pruning of suboptimal\ncandidates. Theoretical foundations and a formal verification-pruning algorithm\nare developed to ensure correctness and efficiency. Integration with standard\nLLM inference pipelines is achieved without requiring retraining or\narchitecture modification. Experimental evaluations across multiple datasets\nand models demonstrate that HVT consistently outperforms existing speculative\ndecoding schemes, achieving substantial reductions in inference time and energy\nconsumption while maintaining or enhancing output quality. The findings\nhighlight the potential of hierarchical verification strategies as a new\ndirection for accelerating large language model inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u9a8c\u8bc1\u6811\uff08HVT\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u9ad8\u53ef\u80fd\u6027\u8349\u7a3f\u5e76\u65e9\u671f\u526a\u679d\u6b21\u4f18\u5019\u9009\uff0c\u91cd\u6784\u4e86\u63a8\u6d4b\u6027\u6ce2\u675f\u89e3\u7801\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7531\u4e8e\u5176\u81ea\u56de\u5f52\u7279\u6027\uff0c\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u9762\u4e34\u6301\u7eed\u6311\u6218\u3002\u4f20\u7edf\u7684\u63a8\u6d4b\u89e3\u7801\u548c\u6ce2\u675f\u91c7\u6837\u65b9\u6cd5\u6309\u987a\u5e8f\u9a8c\u8bc1\u8349\u7a3f\u5e8f\u5217\uff0c\u7f3a\u4e4f\u4f18\u5148\u7ea7\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5206\u5c42\u9a8c\u8bc1\u6811\uff08HVT\uff09\uff0c\u4e00\u79cd\u91cd\u6784\u63a8\u6d4b\u6027\u6ce2\u675f\u89e3\u7801\u7684\u65b0\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u4f18\u5148\u5904\u7406\u9ad8\u53ef\u80fd\u6027\u8349\u7a3f\u548c\u65e9\u671f\u526a\u679d\u6b21\u4f18\u5019\u9009\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u5f00\u53d1\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5f62\u5f0f\u5316\u7684\u9a8c\u8bc1-\u526a\u679d\u7b97\u6cd5\uff0c\u4ee5\u786e\u4fdd\u6b63\u786e\u6027\u548c\u6548\u7387\u3002HVT\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u5373\u53ef\u4e0e\u6807\u51c6LLM\u63a8\u7406\u7ba1\u7ebf\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHVT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\u3002\u5b83\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5206\u5c42\u9a8c\u8bc1\u7b56\u7565\u4f5c\u4e3a\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b0\u65b9\u5411\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03862", "pdf": "https://arxiv.org/pdf/2508.03862", "abs": "https://arxiv.org/abs/2508.03862", "authors": ["Abdul Saboor", "Zhuangzhuang Cui", "Achiel Colpaert", "Evgenii Vinogradov", "Sofie Pollin"], "title": "CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors", "categories": ["cs.NI", "eess.SP"], "comment": "Accepted at IEEE Globecom 2025", "summary": "Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial\nVehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,\nsuch as air taxis. A key challenge in these high-mobility aerial corridors is\nensuring reliable connectivity, where frequent handovers can degrade network\nperformance. To resolve this, we present a Context-Aware Smart Handover (CASH)\nprotocol that uses a forward-looking scoring mechanism based on UAV trajectory\nto make proactive handover decisions. We evaluate the performance of the\nproposed CASH against existing handover protocols in a custom-built simulator.\nResults show that CASH reduces handover frequency by up to 78% while\nmaintaining low outage probability. We then investigate the impact of base\nstation density and safety margin on handover performance, where their optimal\nsetups are empirically obtained to ensure reliable UAM communication.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u4e2d\u65e0\u4eba\u673a\u9891\u7e41\u5207\u6362\u5bfc\u81f4\u7684\u8fde\u63a5\u95ee\u9898\uff0c\u63d0\u51faCASH\u534f\u8bae\uff0c\u901a\u8fc7\u524d\u77bb\u6027\u51b3\u7b56\u663e\u8457\u964d\u4f4e\u5207\u6362\u9891\u7387\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u4f9d\u8d56\u7a7a\u4e2d\u8d70\u5eca\u652f\u6301\u65e0\u4eba\u673a\u4e09\u7ef4\u79fb\u52a8\uff0c\u4f46\u9ad8\u79fb\u52a8\u6027\u5bfc\u81f4\u9891\u7e41\u5207\u6362\uff0c\u4ece\u800c\u964d\u4f4e\u7f51\u7edc\u6027\u80fd\uff0c\u786e\u4fdd\u53ef\u9760\u8fde\u63a5\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u667a\u80fd\u5207\u6362\uff08CASH\uff09\u534f\u8bae\u3002\u8be5\u534f\u8bae\u5229\u7528\u57fa\u4e8e\u65e0\u4eba\u673a\u8f68\u8ff9\u7684\u524d\u77bb\u6027\u8bc4\u5206\u673a\u5236\uff0c\u505a\u51fa\u4e3b\u52a8\u5207\u6362\u51b3\u7b56\u3002\u901a\u8fc7\u5b9a\u5236\u6a21\u62df\u5668\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5e76\u4e0e\u73b0\u6709\u5207\u6362\u534f\u8bae\u8fdb\u884c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u57fa\u7ad9\u5bc6\u5ea6\u548c\u5b89\u5168\u88d5\u5ea6\u5bf9\u5207\u6362\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "CASH\u534f\u8bae\u5c06\u5207\u6362\u9891\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe78%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u4e2d\u65ad\u6982\u7387\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u7ecf\u9a8c\u83b7\u5f97\u4e86\u57fa\u7ad9\u5bc6\u5ea6\u548c\u5b89\u5168\u88d5\u5ea6\u7684\u6700\u4f73\u8bbe\u7f6e\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u7684UAM\u901a\u4fe1\u3002", "conclusion": "CASH\u534f\u8bae\u901a\u8fc7\u667a\u80fd\u524d\u77bb\u6027\u5207\u6362\u6709\u6548\u89e3\u51b3\u4e86UAM\u4e2d\u65e0\u4eba\u673a\u9891\u7e41\u5207\u6362\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fde\u63a5\u53ef\u9760\u6027\uff0c\u5e76\u4e3aUAM\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u5173\u952e\u53c2\u6570\u4f18\u5316\u5efa\u8bae\u3002"}}
{"id": "2508.03858", "pdf": "https://arxiv.org/pdf/2508.03858", "abs": "https://arxiv.org/abs/2508.03858", "authors": ["Charles L. Wang", "Trisha Singhal", "Ameya Kelkar", "Jason Tuo"], "title": "MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems", "categories": ["cs.AI", "cs.ET", "cs.MA"], "comment": null, "summary": "Agentic AI systems capable of reasoning, planning, and executing actions\npresent fundamentally distinct governance challenges compared to traditional AI\nmodels. Unlike conventional AI, these systems exhibit emergent and unexpected\nbehaviors during runtime, introducing novel agent-related risks that cannot be\nfully anticipated through pre-deployment governance alone. To address this\ncritical gap, we introduce MI9, the first fully integrated runtime governance\nframework designed specifically for safety and alignment of agentic AI systems.\nMI9 introduces real-time controls through six integrated components:\nagency-risk index, agent-semantic telemetry capture, continuous authorization\nmonitoring, Finite-State-Machine (FSM)-based conformance engines,\ngoal-conditioned drift detection, and graduated containment strategies.\nOperating transparently across heterogeneous agent architectures, MI9 enables\nthe systematic, safe, and responsible deployment of agentic systems in\nproduction environments where conventional governance approaches fall short,\nproviding the foundational infrastructure for safe agentic AI deployment at\nscale. Detailed analysis through a diverse set of scenarios demonstrates MI9's\nsystematic coverage of governance challenges that existing approaches fail to\naddress, establishing the technical foundation for comprehensive agentic AI\noversight.", "AI": {"tldr": "\u4ee3\u7406AI\u7cfb\u7edf\u56e0\u8fd0\u884c\u65f6\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u5e26\u6765\u72ec\u7279\u6cbb\u7406\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86MI9\uff0c\u4e00\u4e2a\u96c6\u6210\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u7ec4\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e0d\u8db3\u4e4b\u5904\uff0c\u5b9e\u73b0\u4ee3\u7406AI\u7cfb\u7edf\u7684\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "motivation": "\u4e0e\u4f20\u7edfAI\u4e0d\u540c\uff0c\u4ee3\u7406AI\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u4f1a\u5c55\u73b0\u51fa\u7d27\u6025\u548c\u610f\u60f3\u4e0d\u5230\u7684\u884c\u4e3a\uff0c\u5f15\u5165\u4e86\u90e8\u7f72\u524d\u6cbb\u7406\u65e0\u6cd5\u5b8c\u5168\u9884\u89c1\u7684\u4ee3\u7406\u76f8\u5173\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86MI9\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u4e3a\u4ee3\u7406AI\u7cfb\u7edf\u5b89\u5168\u548c\u5bf9\u9f50\u8bbe\u8ba1\u7684\u5b8c\u5168\u96c6\u6210\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\u3002MI9\u901a\u8fc7\u516d\u4e2a\u96c6\u6210\u7ec4\u4ef6\u63d0\u4f9b\u5b9e\u65f6\u63a7\u5236\uff1a\u4ee3\u7406\u98ce\u9669\u6307\u6570\u3001\u4ee3\u7406\u8bed\u4e49\u9065\u6d4b\u6355\u83b7\u3001\u6301\u7eed\u6388\u6743\u76d1\u63a7\u3001\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u7684\u4e00\u81f4\u6027\u5f15\u64ce\u3001\u76ee\u6807\u6761\u4ef6\u6f02\u79fb\u68c0\u6d4b\u548c\u5206\u7ea7\u904f\u5236\u7b56\u7565\u3002", "result": "MI9\u80fd\u591f\u5728\u4f20\u7edf\u6cbb\u7406\u65b9\u6cd5\u4e0d\u8db3\u7684\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u5316\u3001\u5b89\u5168\u3001\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u4ee3\u7406\u7cfb\u7edf\uff0c\u4e3a\u5927\u89c4\u6a21\u5b89\u5168\u4ee3\u7406AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u57fa\u7840\u67b6\u6784\u3002\u901a\u8fc7\u591a\u6837\u5316\u573a\u666f\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u8bc1\u660e\u4e86MI9\u80fd\u7cfb\u7edf\u5730\u8986\u76d6\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u7684\u6cbb\u7406\u6311\u6218\u3002", "conclusion": "MI9\u6846\u67b6\u4e3a\u5168\u9762\u7684\u4ee3\u7406AI\u76d1\u7ba1\u5960\u5b9a\u4e86\u6280\u672f\u57fa\u7840\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4ee3\u7406AI\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u9762\u4e34\u7684\u6cbb\u7406\u96be\u9898\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4ee3\u7406AI\u7cfb\u7edf\u5b89\u5168\u3001\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u548c\u6269\u5c55\u3002"}}
{"id": "2508.03729", "pdf": "https://arxiv.org/pdf/2508.03729", "abs": "https://arxiv.org/abs/2508.03729", "authors": ["Kosmas Pinitas", "Konstantinos Makantasis", "Georgios N. Yannakakis"], "title": "Privileged Contrastive Pretraining for Multimodal Affect Modelling", "categories": ["cs.LG", "cs.HC", "cs.MM"], "comment": null, "summary": "Affective Computing (AC) has made significant progress with the advent of\ndeep learning, yet a persistent challenge remains: the reliable transfer of\naffective models from controlled laboratory settings (in-vitro) to uncontrolled\nreal-world environments (in-vivo). To address this challenge we introduce the\nPrivileged Contrastive Pretraining (PriCon) framework according to which models\nare first pretrained via supervised contrastive learning (SCL) and then act as\nteacher models within a Learning Using Privileged Information (LUPI) framework.\nPriCon both leverages privileged information during training and enhances the\nrobustness of derived affect models via SCL. Experiments conducted on two\nbenchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained\nusing PriCon consistently outperform LUPI and end to end models. Remarkably, in\nmany cases, PriCon models achieve performance comparable to models trained with\naccess to all modalities during both training and testing. The findings\nunderscore the potential of PriCon as a paradigm towards further bridging the\ngap between in-vitro and in-vivo affective modelling, offering a scalable and\npractical solution for real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPrivileged Contrastive Pretraining (PriCon) \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u4ece\u5b9e\u9a8c\u5ba4\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u8fc1\u79fb\u6311\u6218\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u6743\u4fe1\u606f\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5e76\u6709\u671b\u5f25\u5408in-vitro\u548cin-vivo\u60c5\u611f\u5efa\u6a21\u7684\u5dee\u8ddd\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\uff08AC\uff09\u6a21\u578b\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u63a8\u52a8\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u60c5\u611f\u6a21\u578b\u4ece\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u73af\u5883\uff08in-vitro\uff09\u53ef\u9760\u5730\u8fc1\u79fb\u5230\u4e0d\u53d7\u63a7\u771f\u5b9e\u4e16\u754c\u73af\u5883\uff08in-vivo\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86Privileged Contrastive Pretraining (PriCon) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff08SCL\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u5b66\u4e60\uff08LUPI\uff09\u6846\u67b6\u4e2d\u7684\u6559\u5e08\u6a21\u578b\u3002PriCon\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u7279\u6743\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7SCL\u589e\u5f3a\u4e86\u6d3e\u751f\u60c5\u611f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728RECOLA\u548cAGAIN\u4e24\u4e2a\u57fa\u51c6\u60c5\u611f\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528PriCon\u8bad\u7ec3\u7684\u6a21\u578b\u6301\u7eed\u4f18\u4e8eLUPI\u548c\u7aef\u5230\u7aef\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0cPriCon\u6a21\u578b\u751a\u81f3\u5b9e\u73b0\u4e86\u4e0e\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u80fd\u8bbf\u95ee\u6240\u6709\u6a21\u6001\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86PriCon\u4f5c\u4e3a\u4e00\u79cd\u65b0\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u5f25\u5408in-vitro\u548cin-vivo\u60c5\u611f\u5efa\u6a21\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03699", "pdf": "https://arxiv.org/pdf/2508.03699", "abs": "https://arxiv.org/abs/2508.03699", "authors": ["Subin Raj Peter"], "title": "Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task", "categories": ["cs.CV", "cs.HC", "cs.MM"], "comment": "7 pages, 7 figures, conference", "summary": "Virtual Reality (VR) has emerged as a powerful tool for workforce training,\noffering immersive, interactive, and risk-free environments that enhance skill\nacquisition, decision-making, and confidence. Despite its advantages,\ndeveloping VR applications for training remains a significant challenge due to\nthe time, expertise, and resources required to create accurate and engaging\ninstructional content. To address these limitations, this paper proposes a\nnovel approach that leverages Large Language Models (LLMs) to automate the\ngeneration of virtual instructions from textual input. The system comprises two\ncore components: an LLM module that extracts task-relevant information from the\ntext, and an intelligent module that transforms this information into animated\ndemonstrations and visual cues within a VR environment. The intelligent module\nreceives input from the LLM module and interprets the extracted information.\nBased on this, an instruction generator creates training content using relevant\ndata from a database. The instruction generator generates the instruction by\nchanging the color of virtual objects and creating animations to illustrate\ntasks. This approach enhances training effectiveness and reduces development\noverhead, making VR-based training more scalable and adaptable to evolving\nindustrial needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u4ece\u6587\u672c\u751f\u6210VR\u865a\u62df\u6307\u4ee4\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3VR\u57f9\u8bad\u5185\u5bb9\u5f00\u53d1\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u5e76\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1VR\u5728\u52b3\u52a8\u529b\u57f9\u8bad\u4e2d\u5177\u6709\u6c89\u6d78\u3001\u4e92\u52a8\u3001\u65e0\u98ce\u9669\u7b49\u4f18\u52bf\uff0c\u4f46\u5f00\u53d1VR\u57f9\u8bad\u5e94\u7528\u9762\u4e34\u521b\u5efa\u51c6\u786e\u4e14\u5f15\u4eba\u5165\u80dc\u7684\u6559\u5b66\u5185\u5bb9\u6240\u9700\u7684\u65f6\u95f4\u3001\u4e13\u4e1a\u77e5\u8bc6\u548c\u8d44\u6e90\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) LLM\u6a21\u5757\uff0c\u8d1f\u8d23\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff1b2) \u667a\u80fd\u6a21\u5757\uff0c\u5c06LLM\u63d0\u53d6\u7684\u4fe1\u606f\u8f6c\u6362\u4e3aVR\u73af\u5883\u4e2d\u7684\u52a8\u753b\u6f14\u793a\u548c\u89c6\u89c9\u63d0\u793a\u3002\u667a\u80fd\u6a21\u5757\u4e2d\u7684\u6307\u4ee4\u751f\u6210\u5668\u5229\u7528\u6570\u636e\u5e93\u6570\u636e\uff0c\u901a\u8fc7\u6539\u53d8\u865a\u62df\u5bf9\u8c61\u989c\u8272\u548c\u521b\u5efa\u52a8\u753b\u6765\u751f\u6210\u8bad\u7ec3\u5185\u5bb9\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u5e76\u51cf\u5c11\u5f00\u53d1\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u57fa\u4e8eVR\u7684\u57f9\u8bad\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6ee1\u8db3\u4e0d\u65ad\u53d8\u5316\u7684\u5de5\u4e1a\u9700\u6c42\u3002"}}
{"id": "2508.03728", "pdf": "https://arxiv.org/pdf/2508.03728", "abs": "https://arxiv.org/abs/2508.03728", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "categories": ["cs.CL"], "comment": null, "summary": "Wikipedia, a vast and continuously consulted knowledge base, faces\nsignificant challenges in maintaining up-to-date content due to its reliance on\nmanual human editors. Inspired by the vision of continuous knowledge\nacquisition in NELL and fueled by advances in LLM-based agents, this paper\nintroduces WiNELL, an agentic framework for continuously updating Wikipedia\narticles. Our approach employs a multi-agent framework to aggregate online\ninformation, select new and important knowledge for a target entity in\nWikipedia, and then generate precise edit suggestions for human review. Our\nfine-grained editing models, trained on Wikipedia's extensive history of human\nedits, enable incorporating updates in a manner consistent with human editing\nbehavior. Our editor models outperform both open-source instruction-following\nbaselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and\nediting efficiency. End-to-end evaluation on high-activity Wikipedia pages\ndemonstrates WiNELL's ability to identify and suggest timely factual updates.\nThis opens up a promising research direction in LLM agents for automatically\nupdating knowledge bases in a never-ending fashion.", "AI": {"tldr": "WiNELL\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u591a\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u66f4\u65b0Wikipedia\u5185\u5bb9\uff0c\u901a\u8fc7\u805a\u5408\u4fe1\u606f\u3001\u7b5b\u9009\u77e5\u8bc6\u5e76\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u7f16\u8f91\u4e60\u60ef\u7684\u4fee\u6539\u5efa\u8bae\u3002", "motivation": "Wikipedia\u4f9d\u8d56\u4eba\u5de5\u7f16\u8f91\uff0c\u5185\u5bb9\u66f4\u65b0\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u672c\u6587\u53d7NELL\u6301\u7eed\u77e5\u8bc6\u83b7\u53d6\u613f\u666f\u548cLLM\u4ee3\u7406\u6280\u672f\u8fdb\u6b65\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u77e5\u8bc6\u5e93\u5185\u5bb9\u5b9e\u65f6\u66f4\u65b0\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8d1f\u8d23\u805a\u5408\u5728\u7ebf\u4fe1\u606f\u3001\u7b5b\u9009Wikipedia\u76ee\u6807\u5b9e\u4f53\u7684\u65b0\u77e5\u8bc6\u548c\u91cd\u8981\u77e5\u8bc6\uff0c\u5e76\u751f\u6210\u7cbe\u786e\u7684\u7f16\u8f91\u5efa\u8bae\u4f9b\u4eba\u5de5\u5ba1\u6838\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u5728Wikipedia\u4e30\u5bcc\u7f16\u8f91\u5386\u53f2\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u7ec6\u7c92\u5ea6\u7f16\u8f91\u6a21\u578b\uff0c\u786e\u4fdd\u66f4\u65b0\u65b9\u5f0f\u7b26\u5408\u4eba\u7c7b\u7f16\u8f91\u884c\u4e3a\u3002", "result": "\u7f16\u8f91\u6a21\u578b\u5728\u5173\u952e\u4fe1\u606f\u8986\u76d6\u7387\u548c\u7f16\u8f91\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5f00\u6e90\u6307\u4ee4\u9075\u5faa\u57fa\u7ebf\u548c\u95ed\u6e90LLM\uff08\u5982GPT-4o\uff09\u3002\u5728\u5bf9\u6d3b\u8dc3Wikipedia\u9875\u9762\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u4e2d\uff0cWiNELL\u5c55\u793a\u4e86\u8bc6\u522b\u5e76\u63d0\u51fa\u53ca\u65f6\u4e8b\u5b9e\u66f4\u65b0\u7684\u80fd\u529b\u3002", "conclusion": "WiNELL\u4e3aLLM\u667a\u80fd\u4f53\u81ea\u52a8\u3001\u6301\u7eed\u66f4\u65b0\u77e5\u8bc6\u5e93\u5f00\u8f9f\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.03891", "pdf": "https://arxiv.org/pdf/2508.03891", "abs": "https://arxiv.org/abs/2508.03891", "authors": ["Eun Hun Choi", "Jasleen Kaur", "Vladas Pipiras", "Nelson Gomes Rodrigues Antunes", "Brendan Massey"], "title": "Confidence Driven Classification of Application Types in the Presence of Background Network", "categories": ["cs.NI"], "comment": "10 pages", "summary": "Accurately classifying the application types of network traffic using deep\nlearning models has recently gained popularity. However, we find that these\nclassifiers do not perform well on real-world traffic data due to the presence\nof non-application-specific generic background traffic originating from\nadvertisements, analytics, shared APIs, and trackers. Unfortunately,\nstate-of-the-art application classifiers overlook such traffic in curated\ndatasets and only classify relevant application traffic. To address this issue,\nwhen we label and train using an additional class for background traffic, it\nleads to additional confusion between application and background traffic, as\nthe latter is heterogeneous and encompasses all traffic that is not relevant to\nthe application sessions. To avoid falsely classifying background traffic as\none of the relevant application types, a reliable confidence measure is\nwarranted, such that we can refrain from classifying uncertain samples.\nTherefore, we design a Gaussian Mixture Model-based classification framework\nthat improves the indication of the deep learning classifier's confidence to\nallow more reliable classification.", "AI": {"tldr": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6d41\u91cf\u5206\u7c7b\u5668\u5728\u771f\u5b9e\u6d41\u91cf\u4e2d\u56e0\u80cc\u666f\u6d41\u91cf\u8868\u73b0\u4e0d\u4f73\uff0c\u6dfb\u52a0\u80cc\u666f\u7c7b\u522b\u4f1a\u52a0\u5267\u6df7\u6dc6\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u7f6e\u4fe1\u5ea6\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u5668\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u56e0\u975e\u5e94\u7528\u7279\u5b9a\u80cc\u666f\u6d41\u91cf\uff08\u5982\u5e7f\u544a\u3001\u5206\u6790\u6d41\u91cf\uff09\u7684\u5b58\u5728\u800c\u6027\u80fd\u4e0d\u4f73\u3002\u5148\u8fdb\u5206\u7c7b\u5668\u5728\u8bad\u7ec3\u65f6\u5ffd\u7565\u4e86\u6b64\u7c7b\u6d41\u91cf\uff0c\u800c\u7b80\u5355\u6dfb\u52a0\u80cc\u666f\u6d41\u91cf\u7c7b\u522b\u5219\u56e0\u5176\u5f02\u6784\u6027\u5bfc\u81f4\u5e94\u7528\u6d41\u91cf\u4e0e\u80cc\u666f\u6d41\u91cf\u4e4b\u95f4\u7684\u989d\u5916\u6df7\u6dc6\u3002\u4e3a\u907f\u514d\u8bef\u5c06\u80cc\u666f\u6d41\u91cf\u5206\u7c7b\u4e3a\u5e94\u7528\u7c7b\u578b\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u6765\u5904\u7406\u4e0d\u786e\u5b9a\u6837\u672c\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u7f6e\u4fe1\u5ea6\u6307\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u7f51\u7edc\u6d41\u91cf\u5e94\u7528\u7c7b\u578b\u8bc6\u522b\u3002", "result": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7684\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u9488\u5bf9\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6d41\u91cf\u5206\u7c7b\u5668\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u80cc\u666f\u6d41\u91cf\u65f6\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u5668\u7684\u7f6e\u4fe1\u5ea6\u5e76\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u7f51\u7edc\u6d41\u91cf\u5e94\u7528\u7c7b\u578b\u8bc6\u522b\u3002"}}
{"id": "2508.03864", "pdf": "https://arxiv.org/pdf/2508.03864", "abs": "https://arxiv.org/abs/2508.03864", "authors": ["Zhenyu Pan", "Yiting Zhang", "Yutong Zhang", "Jianshu Zhang", "Haozheng Luo", "Yuwei Han", "Dennis Wu", "Hong-Yu Chen", "Philip S. Yu", "Manling Li", "Han Liu"], "title": "Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety", "categories": ["cs.AI"], "comment": null, "summary": "Multi-agent systems (MAS) built on multimodal large language models exhibit\nstrong collaboration and performance. However, their growing openness and\ninteraction complexity pose serious risks, notably jailbreak and adversarial\nattacks. Existing defenses typically rely on external guard modules, such as\ndedicated safety agents, to handle unsafe behaviors. Unfortunately, this\nparadigm faces two challenges: (1) standalone agents offer limited protection,\nand (2) their independence leads to single-point failure-if compromised,\nsystem-wide safety collapses. Naively increasing the number of guard agents\nfurther raises cost and complexity. To address these challenges, we propose\nEvo-MARL, a novel multi-agent reinforcement learning (MARL) framework that\nenables all task agents to jointly acquire defensive capabilities. Rather than\nrelying on external safety modules, Evo-MARL trains each agent to\nsimultaneously perform its primary function and resist adversarial threats,\nensuring robustness without increasing system overhead or single-node failure.\nFurthermore, Evo-MARL integrates evolutionary search with parameter-sharing\nreinforcement learning to co-evolve attackers and defenders. This adversarial\ntraining paradigm internalizes safety mechanisms and continually enhances MAS\nperformance under co-evolving threats. Experiments show that Evo-MARL reduces\nattack success rates by up to 22% while boosting accuracy by up to 5% on\nreasoning tasks-demonstrating that safety and utility can be jointly improved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEvo-MARL\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f7f\u6240\u6709\u4efb\u52a1\u667a\u80fd\u4f53\u5171\u540c\u83b7\u5f97\u9632\u5fa1\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u8d8a\u72f1\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5b89\u5168\u6a21\u5757\uff0c\u5b58\u5728\u4fdd\u62a4\u6709\u9650\u3001\u5355\u70b9\u6545\u969c\u53ca\u589e\u52a0\u6210\u672c\u548c\u590d\u6742\u6027\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faEvo-MARL\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u6bcf\u4e2a\u4efb\u52a1\u667a\u80fd\u4f53\u540c\u65f6\u5b66\u4e60\u5176\u4e3b\u8981\u529f\u80fd\u5e76\u62b5\u6297\u5bf9\u6297\u6027\u5a01\u80c1\uff0c\u4ece\u800c\u5185\u5316\u9632\u5fa1\u80fd\u529b\u3002Evo-MARL\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u4e0e\u53c2\u6570\u5171\u4eab\u5f3a\u5316\u5b66\u4e60\uff0c\u5171\u540c\u8fdb\u5316\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\uff0c\u4ee5\u6301\u7eed\u589e\u5f3a\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cEvo-MARL\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u9ad8\u8fbe22%\uff0c\u5e76\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u9ad8\u8fbe5%\uff0c\u8868\u660e\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u53ef\u4ee5\u5171\u540c\u6539\u5584\u3002", "conclusion": "Evo-MARL\u901a\u8fc7\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5185\u5316\u9632\u5fa1\u673a\u5236\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5b89\u5168\u6311\u6218\uff0c\u5e76\u5728\u4e0d\u589e\u52a0\u7cfb\u7edf\u5f00\u9500\u6216\u5f15\u5165\u5355\u70b9\u6545\u969c\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.03730", "pdf": "https://arxiv.org/pdf/2508.03730", "abs": "https://arxiv.org/abs/2508.03730", "authors": ["Kefei Wu", "Baihua Zheng", "Weiwei Sun"], "title": "PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression", "categories": ["cs.LG"], "comment": null, "summary": "Location-aware devices continuously generate massive volumes of trajectory\ndata, creating demand for efficient compression. Line simplification is a\ncommon solution but typically assumes 2D trajectories and ignores time\nsynchronization and motion continuity. We propose PILOT-C, a novel trajectory\ncompression framework that integrates frequency-domain physics modeling with\nerror-bounded optimization. Unlike existing line simplification methods,\nPILOT-C supports trajectories in arbitrary dimensions, including 3D, by\ncompressing each spatial axis independently. Evaluated on four real-world\ndatasets, PILOT-C achieves superior performance across multiple dimensions. In\nterms of compression ratio, PILOT-C outperforms CISED-W, the current\nstate-of-the-art SED-based line simplification algorithm, by an average of\n19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction\nin error compared to CISED-W. Additionally, PILOT-C seamlessly extends to\nthree-dimensional trajectories while maintaining the same computational\ncomplexity, achieving a 49% improvement in compression ratios over SQUISH-E,\nthe most efficient line simplification algorithm on 3D datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPILOT-C\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8f68\u8ff9\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9891\u57df\u7269\u7406\u5efa\u6a21\u548c\u8bef\u5dee\u7ea6\u675f\u4f18\u5316\uff0c\u5728\u591a\u7ef4\u8f68\u8ff9\u538b\u7f29\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u4f4d\u7f6e\u611f\u77e5\u8bbe\u5907\u4ea7\u751f\u5927\u91cf\u8f68\u8ff9\u6570\u636e\uff0c\u9700\u8981\u9ad8\u6548\u538b\u7f29\u3002\u73b0\u6709\u76f4\u7ebf\u7b80\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe2D\u8f68\u8ff9\uff0c\u5e76\u5ffd\u7565\u65f6\u95f4\u540c\u6b65\u548c\u8fd0\u52a8\u8fde\u7eed\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u7ef4\u8f68\u8ff9\u538b\u7f29\u9700\u6c42\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86PILOT-C\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9891\u57df\u7269\u7406\u5efa\u6a21\u4e0e\u8bef\u5dee\u7ea6\u675f\u4f18\u5316\u76f8\u7ed3\u5408\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cPILOT-C\u901a\u8fc7\u72ec\u7acb\u538b\u7f29\u6bcf\u4e2a\u7a7a\u95f4\u8f74\u6765\u652f\u6301\u4efb\u610f\u7ef4\u5ea6\uff08\u5305\u62ec3D\uff09\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cPILOT-C\u8868\u73b0\u51fa\u8272\u3002\u5728\u538b\u7f29\u6bd4\u65b9\u9762\uff0cPILOT-C\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684SED\u57fa\u76f4\u7ebf\u7b80\u5316\u7b97\u6cd5CISED-W\u5e73\u5747\u9ad819.2%\u3002\u5728\u8f68\u8ff9\u4fdd\u771f\u5ea6\u65b9\u9762\uff0cPILOT-C\u6bd4CISED-W\u5e73\u5747\u51cf\u5c1132.6%\u7684\u8bef\u5dee\u3002\u6b64\u5916\uff0cPILOT-C\u80fd\u65e0\u7f1d\u6269\u5c55\u52303D\u8f68\u8ff9\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0d\u53d8\uff0c\u57283D\u6570\u636e\u96c6\u4e0a\u6bd4\u6700\u6709\u6548\u7684\u76f4\u7ebf\u7b80\u5316\u7b97\u6cd5SQUISH-E\u63d0\u534749%\u7684\u538b\u7f29\u6bd4\u3002", "conclusion": "PILOT-C\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9002\u7528\u4e8e\u4efb\u610f\u7ef4\u5ea6\u7684\u8f68\u8ff9\u538b\u7f29\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u538b\u7f29\u7b97\u6cd5\uff0c\u7279\u522b\u5728\u591a\u7ef4\u8f68\u8ff9\u5904\u7406\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.03720", "pdf": "https://arxiv.org/pdf/2508.03720", "abs": "https://arxiv.org/abs/2508.03720", "authors": ["Ahmet G\u00f6khan Poyraz"], "title": "Outlier Detection Algorithm for Circle Fitting", "categories": ["cs.CV", "eess.IV"], "comment": "Preprint, not peer-reviewed", "summary": "Circle fitting methods are extensively utilized in various industries,\nparticularly in quality control processes and design applications. The\neffectiveness of these algorithms can be significantly compromised when the\npoint sets to be predicted are noisy. To mitigate this issue, outlier detection\nand removal algorithms are often applied before the circle fitting procedure.\nThis study introduces the Polar Coordinate-Based Outlier Detection (PCOD)\nalgorithm, which can be effectively employed in circle fitting applications. In\nthe proposed approach, the point set is first transformed into polar\ncoordinates, followed by the calculation of both local and global standard\ndeviations. Outliers are then identified by comparing local mean values with\nthe global standard deviation. The practicality and efficiency of the proposed\nmethod are demonstrated by focusing on the high-precision diameter measurement\nof industrial washer parts. Images from a machine vision system are processed\nthrough preprocessing steps, including sub-pixel edge detection. The resulting\nsub-pixel edge points are then cleaned using the proposed outlier detection and\nremoval algorithm, after which circle fitting is performed. A comparison is\nmade using ten different circle fitting algorithms and five distinct outlier\ndetection methods. The results indicate that the proposed method outperforms\nthe other approaches, delivering the best performance in terms of accuracy\nwithin the dataset, thereby demonstrating its potential for enhancing circle\nfitting applications in industrial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6781\u5750\u6807\u7684\u5f02\u5e38\u70b9\u68c0\u6d4b\uff08PCOD\uff09\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u566a\u58f0\u6570\u636e\u4e0b\u5706\u62df\u5408\u7684\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u5de5\u4e1a\u5e94\u7528\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5706\u62df\u5408\u65b9\u6cd5\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u5904\u7406\u566a\u58f0\u70b9\u96c6\u65f6\u6548\u679c\u4f1a\u663e\u8457\u964d\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5f02\u5e38\u70b9\u68c0\u6d4b\u548c\u79fb\u9664\u7b97\u6cd5\u6765\u63d0\u5347\u62df\u5408\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u6781\u5750\u6807\u5f02\u5e38\u70b9\u68c0\u6d4b\uff08PCOD\uff09\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5c06\u70b9\u96c6\u8f6c\u6362\u4e3a\u6781\u5750\u6807\uff0c\u7136\u540e\u8ba1\u7b97\u5c40\u90e8\u548c\u5168\u5c40\u6807\u51c6\u5dee\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u5c40\u90e8\u5747\u503c\u4e0e\u5168\u5c40\u6807\u51c6\u5dee\u6765\u8bc6\u522b\u5f02\u5e38\u70b9\u3002\u7814\u7a76\u4ee5\u5de5\u4e1a\u57ab\u7247\u9ad8\u7cbe\u5ea6\u76f4\u5f84\u6d4b\u91cf\u4e3a\u4f8b\uff0c\u901a\u8fc7\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u83b7\u53d6\u56fe\u50cf\uff0c\u7ecf\u8fc7\u4e9a\u50cf\u7d20\u8fb9\u7f18\u68c0\u6d4b\u540e\uff0c\u5229\u7528PCOD\u7b97\u6cd5\u6e05\u6d17\u8fb9\u7f18\u70b9\uff0c\u518d\u8fdb\u884c\u5706\u62df\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5341\u79cd\u4e0d\u540c\u7684\u5706\u62df\u5408\u7b97\u6cd5\u548c\u4e94\u79cd\u5f02\u5e38\u70b9\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684PCOD\u7b97\u6cd5\u5728\u6570\u636e\u96c6\u7684\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "PCOD\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u5de5\u4e1a\u73af\u5883\u4e0b\u5706\u62df\u5408\u5e94\u7528\u7684\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u566a\u58f0\u6570\u636e\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.03737", "pdf": "https://arxiv.org/pdf/2508.03737", "abs": "https://arxiv.org/abs/2508.03737", "authors": ["Ashutosh Bandooni", "Brindha Subburaj"], "title": "GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "6 pages, 3 figures. Accepted, Presented and Published as part of\n  Proceedings of the 6th International Conference on Recent Advantages in\n  Information Technology (RAIT) 2025", "summary": "Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on\nseveral fields and domains are being curated more frequently over the last few\nyears. However these are often monolingual, mostly available in English.\nAdditionally there also is a lack of datasets available in Hindi on tasks apart\nfrom comprehension and translation. We introduce GanitBench, a tough benchmark\nconsisting of 1527 vision-only questions covering several topics in Mathematics\n- available in languages English and Hindi. Collected from two major\nexaminations from India, the JEE Advanced and the CBSE Boards examinations,\nthis benchmark includes questions in the form of images comprising of figures\nessential to a question as well as text. We evaluate two closed source models\nfor the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.\nGPT-4o mini is found to be the more dominant model on the benchmark, with it's\nhighest average accuracy being 38.15%. We also evaluate models through a\n\"Double Lock\" constraint, which brings down the performance of the models by\nconsiderable margins. We observe that two-shot CoT appears to be a more\neffective setting under this environment. Performance of the two VLMs also\ndecreases when answering the same questions in the Hindi language. We hope to\nfacilitate the inclusion of languages like Hindi in research through our work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a GanitBench \u7684\u53cc\u8bed\uff08\u82f1\u8bed\u548c\u5370\u5730\u8bed\uff09\u6570\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u56fe\u50cf\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5370\u5730\u8bed\u548c\u590d\u6742\u7ea6\u675f\u4e0b\u7684\u8868\u73b0\u6709\u5f85\u63d0\u9ad8\u3002", "motivation": "\u73b0\u6709VLM\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\u591a\u4e3a\u5355\u8bed\uff08\u4e3b\u8981\u662f\u82f1\u8bed\uff09\uff0c\u4e14\u5728\u5370\u5730\u8bed\u7b49\u5176\u4ed6\u8bed\u8a00\u4e2d\uff0c\u9664\u4e86\u7406\u89e3\u548c\u7ffb\u8bd1\u4efb\u52a1\u5916\uff0c\u7f3a\u4e4f\u6570\u636e\u96c6\u3002\u5c24\u5176\u662f\u5728\u6570\u5b66\u63a8\u7406\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\uff0c\u5370\u5730\u8bed\u6570\u636e\u96c6\u66f4\u662f\u7a00\u7f3a\u3002", "method": "\u7814\u7a76\u8005\u521b\u5efa\u4e86 GanitBench\uff0c\u4e00\u4e2a\u5305\u542b1527\u4e2a\u89c6\u89c9\u56fe\u50cf\u6570\u5b66\u95ee\u9898\u7684\u96be\u5ea6\u57fa\u51c6\uff0c\u8fd9\u4e9b\u95ee\u9898\u6765\u81ea\u5370\u5ea6JEE Advanced\u548cCBSE Boards\u8003\u8bd5\u3002\u8be5\u57fa\u51c6\u63d0\u4f9b\u82f1\u8bed\u548c\u5370\u5730\u8bed\u7248\u672c\u3002\u4ed6\u4eec\u4f7f\u7528\u96f6\u6837\u672c\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u4e24\u6837\u672c\u601d\u7ef4\u94fe\uff08CoT\uff09\u8bbe\u7f6e\uff0c\u8bc4\u4f30\u4e86GPT-4o mini\u7b49\u4e24\u4e2a\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u53cc\u91cd\u9501\u5b9a\u201d\u7ea6\u675f\u8fdb\u884c\u989d\u5916\u8bc4\u4f30\u3002", "result": "GPT-4o mini\u5728GanitBench\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u4e3a38.15%\u3002\u201c\u53cc\u91cd\u9501\u5b9a\u201d\u7ea6\u675f\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u4e24\u6837\u672cCoT\u5728\u8be5\u73af\u5883\u4e0b\u66f4\u4e3a\u6709\u6548\u3002\u6a21\u578b\u5728\u56de\u7b54\u5370\u5730\u8bed\u95ee\u9898\u65f6\uff0c\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524dVLM\u5728\u5904\u7406\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\uff09\u548c\u590d\u6742\u7ea6\u675f\u6761\u4ef6\u4e0b\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u5370\u5730\u8bed\u7b49\u8bed\u8a00\u5728VLM\u7814\u7a76\u4e2d\u7684\u5305\u5bb9\u6027\uff0c\u5e76\u6307\u51fa\u9700\u8981\u66f4\u591a\u52aa\u529b\u6765\u63d0\u9ad8\u6a21\u578b\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.04004", "pdf": "https://arxiv.org/pdf/2508.04004", "abs": "https://arxiv.org/abs/2508.04004", "authors": ["Tanguy Ropitault", "Matteo Bordin", "Paolo Testolina", "Michele Polese", "Pedram Johari", "Nada Golmie", "Tommaso Melodia"], "title": "Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is\nchallenging because the performance emerges from the tight coupling of\npropagation, beam management, scheduling, and higher-layer interactions.\nSystem-level simulation is therefore indispensable, yet the vast majority of\nstudies rely on the statistical 3GPP channel models. These are well suited to\ncapture average behavior across many statistical realizations, but cannot\nreproduce site-specific phenomena such as corner diffraction, street-canyon\nblockage, or deterministic line-of-sight conditions and\nangle-of-departure/arrival relationships that drive directional links. This\npaper extends 5G-LENA, an NR module for the system-level Network Simulator 3\n(ns-3), with a trace-based channel model that processes the Multipath\nComponents (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer\n(RT)) or measurement campaigns. Our module constructs frequency-domain channel\nmatrices and feeds them to the existing Physical (PHY)/Medium Access Control\n(MAC) stack without any further modifications. The result is a geometry-based\nchannel model that remains fully compatible with the standard 3GPP\nimplementation in 5G-LENA, while delivering site-specific geometric fidelity.\nThis new module provides a key building block toward Digital Twin (DT)\ncapabilities by offering realistic site-specific channel modeling, unlocking\nstudies that require site awareness, including beam management, blockage\nmitigation, and environment-aware sensing. We demonstrate its capabilities for\nprecise beam-steering validation and end-to-end metric analysis. In both cases,\nthe trace-driven engine exposes performance inflections that the statistical\nmodel does not exhibit, confirming its value for high-fidelity system-level\ncellular networks research and as a step toward DT applications.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u54115G-LENA\u6269\u5c55\u4e00\u4e2a\u57fa\u4e8e\u8ff9\u7ebf\u7684\u65b0\u4fe1\u9053\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u7279\u5b9a\u7ad9\u70b9\u7684\u8702\u7a9d\u7f51\u7edc\u4eff\u771f\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u7684\u6027\u80fd\u7ec6\u8282\uff0c\u5e76\u4e3a\u6570\u5b57\u5b6a\u751f\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u8702\u7a9d\u7cfb\u7edf\u8bc4\u4f30\uff085G NR\u52306G\uff09\u590d\u6742\u4e14\u6027\u80fd\u4f9d\u8d56\u591a\u56e0\u7d20\u8026\u5408\u3002\u73b0\u6709\u7cfb\u7edf\u7ea7\u4eff\u771f\u591a\u7528\u7edf\u8ba13GPP\u4fe1\u9053\u6a21\u578b\uff0c\u4f46\u5176\u65e0\u6cd5\u91cd\u73b0\u7279\u5b9a\u7ad9\u70b9\u7684\u51e0\u4f55\u73b0\u8c61\uff08\u5982\u884d\u5c04\u3001\u963b\u585e\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u7ad9\u70b9\u611f\u77e5\u7814\u7a76\uff08\u5982\u6ce2\u675f\u7ba1\u7406\uff09\u3002", "method": "\u5c065G-LENA\uff08ns-3\u7684NR\u6a21\u5757\uff09\u6269\u5c55\u4e3a\u57fa\u4e8e\u8ff9\u7ebf\u7684\u4fe1\u9053\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5904\u7406\u6765\u81ea\u5916\u90e8\u5c04\u7ebf\u8ffd\u8e2a\u5668\u6216\u6d4b\u91cf\u6d3b\u52a8\u7684\u591a\u5f84\u5206\u91cf\uff08MPCs\uff09\uff0c\u6784\u5efa\u9891\u57df\u4fe1\u9053\u77e9\u9635\uff0c\u5e76\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709PHY/MAC\u5806\u6808\u3002\u901a\u8fc7\u6ce2\u675f\u8d4b\u5f62\u9a8c\u8bc1\u548c\u7aef\u5230\u7aef\u6307\u6807\u5206\u6790\u8fdb\u884c\u529f\u80fd\u5c55\u793a\u3002", "result": "\u5f00\u53d1\u51fa\u517c\u5bb93GPP\u7684\u51e0\u4f55\u57fa\u4fe1\u9053\u6a21\u578b\uff0c\u63d0\u4f9b\u7279\u5b9a\u7ad9\u70b9\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002\u8be5\u6a21\u578b\u5728\u6ce2\u675f\u8d4b\u5f62\u9a8c\u8bc1\u548c\u7aef\u5230\u7aef\u6307\u6807\u5206\u6790\u4e2d\uff0c\u63ed\u793a\u4e86\u7edf\u8ba1\u6a21\u578b\u65e0\u6cd5\u5c55\u73b0\u7684\u6027\u80fd\u53d8\u5316\u3002", "conclusion": "\u65b0\u6a21\u5757\u662f\u9ad8\u4fdd\u771f\u8702\u7a9d\u7f51\u7edc\u7cfb\u7edf\u7ea7\u7814\u7a76\u7684\u5173\u952e\u6784\u5efa\u5757\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u80fd\u529b\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u80fd\u652f\u6301\u6ce2\u675f\u7ba1\u7406\u3001\u963b\u585e\u7f13\u89e3\u7b49\u9700\u8981\u7ad9\u70b9\u611f\u77e5\u7684\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2508.03929", "pdf": "https://arxiv.org/pdf/2508.03929", "abs": "https://arxiv.org/abs/2508.03929", "authors": ["Nguyen Viet Tuan Kiet", "Dao Van Tung", "Tran Cong Dao", "Huynh Thi Thanh Binh"], "title": "MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework", "categories": ["cs.AI"], "comment": "24 pages, 4 figures", "summary": "Designing effective algorithmic components remains a fundamental obstacle in\ntackling NP-hard combinatorial optimization problems (COPs), where solvers\noften rely on carefully hand-crafted strategies. Despite recent advances in\nusing large language models (LLMs) to synthesize high-quality components, most\napproaches restrict the search to a single element - commonly a heuristic\nscoring function - thus missing broader opportunities for innovation. In this\npaper, we introduce a broader formulation of solver design as a multi-strategy\noptimization problem, which seeks to jointly improve a set of interdependent\ncomponents under a unified objective. To address this, we propose\nMulti-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a\nnovel framework based on Monte Carlo Tree Search that facilitates turn-based\noptimization between two LLM agents. At each turn, an agent improves one\ncomponent by leveraging the history of both its own and its opponent's prior\nupdates, promoting both competitive pressure and emergent cooperation. This\nstructured interaction broadens the search landscape and encourages the\ndiscovery of diverse, high-performing solutions. Experiments across multiple\nCOP domains show that MOTIF consistently outperforms state-of-the-art methods,\nhighlighting the promise of turn-based, multi-agent prompting for fully\nautomated solver design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMOTIF\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u53ccLLM\u667a\u80fd\u4f53\u8f6e\u6d41\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3NP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u591a\u7b56\u7565\u6c42\u89e3\u5668\u8bbe\u8ba1\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "NP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u5668\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u7b97\u6cd5\u7ec4\u4ef6\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5728\u6c42\u89e3\u5668\u8bbe\u8ba1\u4e2d\u901a\u5e38\u5c40\u9650\u4e8e\u4f18\u5316\u5355\u4e00\u7ec4\u4ef6\uff08\u5982\u542f\u53d1\u5f0f\u8bc4\u5206\u51fd\u6570\uff09\uff0c\u9650\u5236\u4e86\u521b\u65b0\u3002\u7814\u7a76\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u591a\u7b56\u7565\u4f18\u5316\u6846\u67b6\u6765\u540c\u65f6\u6539\u8fdb\u76f8\u4e92\u4f9d\u8d56\u7684\u7ec4\u4ef6\u3002", "method": "\u5f15\u5165MOTIF\uff08Multi-strategy Optimization via Turn-based Interactive Framework\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u4fc3\u8fdb\u4e24\u4e2aLLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8f6e\u6d41\u4f18\u5316\u3002\u6bcf\u4e2a\u56de\u5408\uff0c\u4e00\u4e2a\u667a\u80fd\u4f53\u5229\u7528\u81ea\u8eab\u548c\u5bf9\u624b\u7684\u5386\u53f2\u66f4\u65b0\u6765\u6539\u8fdb\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u4ece\u800c\u4fc3\u8fdb\u7ade\u4e89\u538b\u529b\u548c\u534f\u4f5c\uff0c\u62d3\u5bbd\u641c\u7d22\u7a7a\u95f4\u5e76\u53d1\u73b0\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMOTIF\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8f6e\u6d41\u5f0f\u3001\u591a\u667a\u80fd\u4f53\u63d0\u793a\u65b9\u6cd5\u5728\u5168\u81ea\u52a8\u6c42\u89e3\u5668\u8bbe\u8ba1\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.03733", "pdf": "https://arxiv.org/pdf/2508.03733", "abs": "https://arxiv.org/abs/2508.03733", "authors": ["Wenjie Li", "Yujie Zhang", "Haoran Sun", "Yueqi Li", "Fanrui Zhang", "Mengzhe Xu", "Victoria Borja Clausich", "Sade Mellin", "Renhao Yang", "Chenrun Wang", "Jethro Zih-Shuo Wang", "Shiyi Yao", "Gen Li", "Yidong Xu", "Hanyu Wang", "Yilin Huang", "Angela Lin Wang", "Chen Shi", "Yin Zhang", "Jianan Guo", "Luqi Yang", "Renxuan Li", "Yang Xu", "Jiawei Liu", "Yao Zhang", "Lei Liu", "Carlos Guti\u00e9rrez SanRom\u00e1n", "Lei Wang"], "title": "CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Chest X-ray (CXR) imaging is one of the most widely used diagnostic\nmodalities in clinical practice, encompassing a broad spectrum of diagnostic\ntasks. Recent advancements have seen the extensive application of\nreasoning-based multimodal large language models (MLLMs) in medical imaging to\nenhance diagnostic efficiency and interpretability. However, existing\nmultimodal models predominantly rely on \"one-time\" diagnostic approaches,\nlacking verifiable supervision of the reasoning process. This leads to\nchallenges in multi-task CXR diagnosis, including lengthy reasoning, sparse\nrewards, and frequent hallucinations. To address these issues, we propose\nCX-Mind, the first generative model to achieve interleaved \"think-answer\"\nreasoning for CXR tasks, driven by curriculum-based reinforcement learning and\nverifiable process rewards (CuRL-VPR). Specifically, we constructed an\ninstruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148\nsamples, and generated 42,828 high-quality interleaved reasoning data points\nsupervised by clinical reports. Optimization was conducted in two stages under\nthe Group Relative Policy Optimization framework: initially stabilizing basic\nreasoning with closed-domain tasks, followed by transfer to open-domain\ndiagnostics, incorporating rule-based conditional process rewards to bypass the\nneed for pretrained reward models. Extensive experimental results demonstrate\nthat CX-Mind significantly outperforms existing medical and general-domain\nMLLMs in visual understanding, text generation, and spatiotemporal alignment,\nachieving an average performance improvement of 25.1% over comparable\nCXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves\na mean recall@1 across 14 diseases that substantially surpasses the second-best\nresults, with multi-center expert evaluations further confirming its clinical\nutility across multiple dimensions.", "AI": {"tldr": "\u63d0\u51faCX-Mind\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u5956\u52b1\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u9519\u5f0f\u201c\u601d\u8003-\u56de\u7b54\u201d\u63a8\u7406\u663e\u8457\u63d0\u5347\u80f8\u90e8X\u5149\uff08CXR\uff09\u8bca\u65ad\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u63a8\u7406\u5197\u957f\u548c\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7528\u4e8e\u80f8\u90e8X\u5149\uff08CXR\uff09\u8bca\u65ad\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u666e\u904d\u91c7\u7528\u201c\u4e00\u6b21\u6027\u201d\u8bca\u65ad\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u9a8c\u8bc1\u76d1\u7763\uff0c\u5bfc\u81f4\u5728\u591a\u4efb\u52a1CXR\u8bca\u65ad\u4e2d\u51fa\u73b0\u63a8\u7406\u5197\u957f\u3001\u5956\u52b1\u7a00\u758f\u53ca\u9891\u7e41\u5e7b\u89c9\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faCX-Mind\uff0c\u9996\u4e2a\u5b9e\u73b0CXR\u4efb\u52a1\u4ea4\u9519\u5f0f\u201c\u601d\u8003-\u56de\u7b54\u201d\u63a8\u7406\u7684\u751f\u6210\u6a21\u578b\uff0c\u7531\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u5956\u52b1\uff08CuRL-VPR\uff09\u9a71\u52a8\u3002\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b70\u4e07+\u56fe\u50cf\u548c260\u4e07+\u6837\u672c\u7684\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6CX-Set\uff0c\u5e76\u751f\u6210\u4e864\u4e07+\u9ad8\u8d28\u91cf\u4ea4\u9519\u63a8\u7406\u6570\u636e\u3002\u6a21\u578b\u4f18\u5316\u5728Group Relative Policy Optimization\u6846\u67b6\u4e0b\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a\u5148\u7528\u5c01\u95ed\u57df\u4efb\u52a1\u7a33\u5b9a\u57fa\u672c\u63a8\u7406\uff0c\u540e\u8fc1\u79fb\u81f3\u5f00\u653e\u57df\u8bca\u65ad\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u89c4\u5219\u7684\u6761\u4ef6\u8fc7\u7a0b\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCX-Mind\u5728\u89c6\u89c9\u7406\u89e3\u3001\u6587\u672c\u751f\u6210\u548c\u65f6\u7a7a\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u533b\u7597\u548c\u901a\u7528\u9886\u57dfMLLMs\uff0c\u6bd4\u540c\u7c7bCXR\u4e13\u7528\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u534725.1%\u3002\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6Rui-CXR\u4e0a\uff0cCX-Mind\u572814\u79cd\u75be\u75c5\u4e0a\u7684\u5e73\u5747\u53ec\u56de\u7387@1\u5927\u5e45\u8d85\u8d8a\u6b21\u4f18\u7ed3\u679c\uff0c\u5e76\u83b7\u5f97\u591a\u4e2d\u5fc3\u4e13\u5bb6\u8bc4\u4f30\u7684\u9ad8\u5ea6\u80af\u5b9a\u3002", "conclusion": "CX-Mind\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4ea4\u9519\u63a8\u7406\u673a\u5236\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MLLMs\u5728CXR\u8bca\u65ad\u4e2d\u7684\u6311\u6218\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u88ab\u8bc1\u660e\u5177\u6709\u663e\u8457\u7684\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.03721", "pdf": "https://arxiv.org/pdf/2508.03721", "abs": "https://arxiv.org/abs/2508.03721", "authors": ["Ahmet Gokhan Poyraz", "Ahmet Emir Dirik", "Hakan Gurkan", "Mehmet Kacmaz"], "title": "Enhancing Diameter Measurement Accuracy in Machine Vision Applications", "categories": ["cs.CV", "eess.IV"], "comment": "Preprint", "summary": "In camera measurement systems, specialized equipment such as telecentric\nlenses is often employed to measure parts with narrow tolerances. However,\ndespite the use of such equipment, measurement errors can occur due to\nmechanical and software-related factors within the system. These errors are\nparticularly evident in applications where parts of different diameters are\nmeasured using the same setup. This study proposes two innovative approaches to\nenhance measurement accuracy using multiple known reference parts: a conversion\nfactor-based method and a pixel-based method. In the first approach, the\nconversion factor is estimated from known references to calculate the diameter\n(mm) of the unknown part. In the second approach, the diameter (mm) is directly\nestimated using pixel-based diameter information from the references. The\nexperimental setup includes an industrial-grade camera and telecentric lenses.\nTests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show\nthat measurement errors, which originally ranged from 13-114 micrometers, were\nreduced to 1-2 micrometers using the proposed methods. By utilizing only a few\nknown reference parts, the proposed approach enables high-accuracy measurement\nof all parts within the camera's field of view. Additionally, this method\nenhances the existing diameter measurement literature by significantly reducing\nerror rates and improving measurement reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5c11\u91cf\u5df2\u77e5\u53c2\u8003\u90e8\u4ef6\uff0c\u5c06\u76f8\u673a\u6d4b\u91cf\u7cfb\u7edf\u4e2d\u4e0d\u540c\u76f4\u5f84\u90e8\u4ef6\u7684\u6d4b\u91cf\u8bef\u5dee\u4ece13-114\u5fae\u7c73\u663e\u8457\u964d\u4f4e\u81f31-2\u5fae\u7c73\uff0c\u4ece\u800c\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u76f8\u673a\u6d4b\u91cf\u7cfb\u7edf\u4f7f\u7528\u4e86\u8fdc\u5fc3\u955c\u5934\u7b49\u4e13\u4e1a\u8bbe\u5907\uff0c\u4f46\u7531\u4e8e\u673a\u68b0\u548c\u8f6f\u4ef6\u56e0\u7d20\uff0c\u4ecd\u4f1a\u4ea7\u751f\u6d4b\u91cf\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u5728\u540c\u4e00\u8bbe\u7f6e\u4e0b\u6d4b\u91cf\u4e0d\u540c\u76f4\u5f84\u90e8\u4ef6\u65f6\uff0c\u8bef\u5dee\u66f4\u4e3a\u660e\u663e\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5747\u5229\u7528\u591a\u4e2a\u5df2\u77e5\u53c2\u8003\u90e8\u4ef6\uff1a1. \u57fa\u4e8e\u8f6c\u6362\u56e0\u5b50\u7684\u65b9\u6cd5\uff1a\u4ece\u5df2\u77e5\u53c2\u8003\u90e8\u4ef6\u4f30\u7b97\u8f6c\u6362\u56e0\u5b50\uff0c\u8fdb\u800c\u8ba1\u7b97\u672a\u77e5\u90e8\u4ef6\u7684\u76f4\u5f84\u30022. \u57fa\u4e8e\u50cf\u7d20\u7684\u65b9\u6cd5\uff1a\u76f4\u63a5\u5229\u7528\u53c2\u8003\u90e8\u4ef6\u7684\u50cf\u7d20\u76f4\u5f84\u4fe1\u606f\u4f30\u7b97\u76f4\u5f84\u3002\u5b9e\u9a8c\u91c7\u7528\u5de5\u4e1a\u7ea7\u76f8\u673a\u548c\u8fdc\u5fc3\u955c\u5934\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5bf9\u73bb\u7483\u6837\u672c\uff081-12\u6beb\u7c73\uff09\u548c\u91d1\u5c5e\u5de5\u4ef6\uff083-24\u6beb\u7c73\uff09\u7684\u6d4b\u91cf\u8bef\u5dee\u4ece\u539f\u6765\u768413-114\u5fae\u7c73\u6210\u529f\u964d\u4f4e\u81f31-2\u5fae\u7c73\u3002", "conclusion": "\u901a\u8fc7\u4ec5\u4f7f\u7528\u5c11\u91cf\u5df2\u77e5\u53c2\u8003\u90e8\u4ef6\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u76f8\u673a\u89c6\u573a\u5185\u6240\u6709\u90e8\u4ef6\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u7387\u5e76\u63d0\u9ad8\u6d4b\u91cf\u53ef\u9760\u6027\uff0c\u4ece\u800c\u4e30\u5bcc\u4e86\u73b0\u6709\u76f4\u5f84\u6d4b\u91cf\u6280\u672f\u3002"}}
{"id": "2508.03793", "pdf": "https://arxiv.org/pdf/2508.03793", "abs": "https://arxiv.org/abs/2508.03793", "authors": ["Yanting Wang", "Runpeng Geng", "Ying Chen", "Jinyuan Jia"], "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs", "categories": ["cs.CL", "cs.CR"], "comment": "The code is available at https://github.com/Wang-Yanting/AttnTrace.\n  The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace", "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAttnTrace\uff0c\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u6eaf\u6e90\u65b9\u6cd5\uff0c\u5b83\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u63d0\u793a\u6ce8\u5165\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7ea7AI\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u4e0a\u4e0b\u6587\u6eaf\u6e90\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff08\u5982TracLLM\u8017\u65f6\u6570\u767e\u79d2\uff09\uff0c\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u6eaf\u6e90\u5bf9\u4e8e\u4e8b\u540e\u53d6\u8bc1\u5206\u6790\u3001\u63d0\u5347LLM\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faAttnTrace\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u751f\u6210\u7684\u6ce8\u610f\u529b\u6743\u91cd\u7684\u4e0a\u4e0b\u6587\u6eaf\u6e90\u65b0\u65b9\u6cd5\u3002\u4e3a\u6709\u6548\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u6587\u4e2d\u5f15\u5165\u4e86\u4e24\u9879\u65e8\u5728\u589e\u5f3aAttnTrace\u6709\u6548\u6027\u7684\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u5176\u8bbe\u8ba1\u9009\u62e9\u7684\u7406\u8bba\u89c1\u89e3\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff0cAttnTrace\u5728\u4e0a\u4e0b\u6587\u6eaf\u6e90\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0cAttnTrace\u80fd\u901a\u8fc7\u201c\u5f52\u56e0-\u68c0\u6d4b\u524d\u201d\u8303\u5f0f\uff0c\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u4e0b\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u7684\u80fd\u529b\u3002\u4f5c\u4e3a\u4e00\u4e2a\u5b9e\u9645\u5e94\u7528\uff0cAttnTrace\u80fd\u6709\u6548\u8bc6\u522b\u65e8\u5728\u64cd\u7eb5LLM\u751f\u6210\u8bc4\u8bba\u7684\u8bba\u6587\u4e2d\u6ce8\u5165\u7684\u6307\u4ee4\u3002", "conclusion": "AttnTrace\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u6eaf\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u63d0\u5347LLM\u8f93\u51fa\u53ef\u4fe1\u5ea6\u3001\u8fdb\u884c\u63d0\u793a\u6ce8\u5165\u68c0\u6d4b\u548c\u6cd5\u8bc1\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.04015", "pdf": "https://arxiv.org/pdf/2508.04015", "abs": "https://arxiv.org/abs/2508.04015", "authors": ["Haoxiang Luo", "Kun Yang", "Qi Huang", "Schahram Dustdar"], "title": "A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks", "categories": ["cs.NI"], "comment": null, "summary": "The proliferation of large-scale artificial intelligence and data-intensive\napplications has spurred the development of Computing Power Networks (CPNs),\nwhich promise to deliver ubiquitous and on-demand computational resources.\nHowever, the immense energy consumption of these networks poses a significant\nsustainability challenge. Simultaneously, power grids are grappling with the\ninstability introduced by the high penetration of intermittent renewable energy\nsources (RES). This paper addresses these dual challenges through a novel\nTwo-Stage Co-Optimization (TSCO) framework that synergistically manages power\nsystem dispatch and CPN task scheduling to achieve low-carbon operations. The\nframework decomposes the complex, large-scale problem into a day-ahead\nstochastic unit commitment (SUC) stage and a real-time operational stage. The\nformer is solved using Benders decomposition for computational tractability,\nwhile in the latter, economic dispatch of generation assets is coupled with an\nadaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)\nagent. This agent makes intelligent, carbon-aware decisions by responding to\ndynamic grid conditions, including real-time electricity prices and marginal\ncarbon intensity. Through extensive simulations on an IEEE 30-bus system\nintegrated with a CPN, the TSCO framework is shown to significantly outperform\nbaseline approaches. Results demonstrate that the proposed framework reduces\ntotal carbon emissions and operational costs, while simultaneously decreasing\nRES curtailment by more than 60% and maintaining stringent Quality of Service\n(QoS) for computational tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u534f\u540c\u4f18\u5316\uff08TSCO\uff09\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u534f\u540c\u7ba1\u7406\u7535\u529b\u7cfb\u7edf\u8c03\u5ea6\u548c\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\uff08CPN\uff09\u4efb\u52a1\u8c03\u5ea6\uff0c\u89e3\u51b3CPN\u9ad8\u80fd\u8017\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u5bfc\u81f4\u7535\u7f51\u4e0d\u7a33\u5b9a\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u78b3\u8fd0\u8425\u3002", "motivation": "\u5927\u89c4\u6a21\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u666e\u53ca\u63a8\u52a8\u4e86\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\uff08CPN\uff09\u7684\u53d1\u5c55\uff0c\u4f46\u5176\u5de8\u5927\u7684\u80fd\u8017\u5e26\u6765\u4e86\u53ef\u6301\u7eed\u6027\u6311\u6218\u3002\u540c\u65f6\uff0c\u9ad8\u6e17\u900f\u7387\u7684\u95f4\u6b47\u6027\u53ef\u518d\u751f\u80fd\u6e90\uff08RES\uff09\u5bfc\u81f4\u7535\u7f51\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5e94\u5bf9\u8fd9\u4e9b\u53cc\u91cd\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u534f\u540c\u4f18\u5316\uff08TSCO\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u65e5\u524d\u968f\u673a\u673a\u7ec4\u7ec4\u5408\uff08SUC\uff09\u9636\u6bb5\uff08\u901a\u8fc7Benders\u5206\u89e3\u6c42\u89e3\uff09\u548c\u5b9e\u65f6\u8fd0\u884c\u9636\u6bb5\u3002\u5728\u5b9e\u65f6\u9636\u6bb5\uff0c\u53d1\u7535\u8d44\u4ea7\u7684\u7ecf\u6d4e\u8c03\u5ea6\u4e0e\u7531\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4ee3\u7406\u7ba1\u7406\u7684\u81ea\u9002\u5e94CPN\u4efb\u52a1\u8c03\u5ea6\u76f8\u7ed3\u5408\uff0c\u8be5\u4ee3\u7406\u80fd\u6839\u636e\u5b9e\u65f6\u7535\u4ef7\u548c\u8fb9\u9645\u78b3\u5f3a\u5ea6\u505a\u51fa\u78b3\u611f\u77e5\u51b3\u7b56\u3002", "result": "\u5728\u96c6\u6210CPN\u7684IEEE 30-bus\u7cfb\u7edf\u4e0a\u7684\u5e7f\u6cdb\u6a21\u62df\u8868\u660e\uff0cTSCO\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u51cf\u5c11\u4e86\u603b\u78b3\u6392\u653e\u548c\u8fd0\u8425\u6210\u672c\uff0c\u540c\u65f6\u5c06\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7528\u91cf\u51cf\u5c11\u4e8660%\u4ee5\u4e0a\uff0c\u5e76\u4fdd\u6301\u4e86\u8ba1\u7b97\u4efb\u52a1\u4e25\u683c\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684TSCO\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\u80fd\u8017\u548c\u7535\u7f51\u4e0d\u7a33\u5b9a\u7684\u53cc\u91cd\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4f4e\u78b3\u8fd0\u8425\uff0c\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u5e76\u63d0\u5347\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u7684\u5229\u7528\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2508.03963", "pdf": "https://arxiv.org/pdf/2508.03963", "abs": "https://arxiv.org/abs/2508.03963", "authors": ["Zewen Liu", "Juntong Ni", "Xianfeng Tang", "Max S. Y. Lau", "Wei Jin"], "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?", "categories": ["cs.AI"], "comment": null, "summary": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165SymbolBench\u57fa\u51c6\u548cLLM-\u9057\u4f20\u7f16\u7a0b\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u53d1\u73b0\u548c\u63a8\u7406\u9690\u85cf\u7b26\u53f7\u89c4\u5f8b\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u5f3a\u8c03\u4e86\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u53d1\u73b0\u9690\u85cf\u7684\u7b26\u53f7\u89c4\u5f8b\u662f\u79d1\u5b66\u53d1\u73b0\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5176\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63a8\u65ad\u53ef\u89e3\u91ca\u3001\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u7b26\u53f7\u7ed3\u6784\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86SymbolBench\u8fd9\u4e00\u7efc\u5408\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u7b26\u53f7\u63a8\u7406\u80fd\u529b\uff0c\u6db5\u76d6\u591a\u53d8\u91cf\u7b26\u53f7\u56de\u5f52\u3001\u5e03\u5c14\u7f51\u7edc\u63a8\u65ad\u548c\u56e0\u679c\u53d1\u73b0\u4e09\u9879\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u9057\u4f20\u7f16\u7a0b\u76f8\u7ed3\u5408\u7684\u7edf\u4e00\u95ed\u73af\u7b26\u53f7\u63a8\u7406\u6846\u67b6\uff0c\u5176\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u517c\u4f5c\u9884\u6d4b\u5668\u548c\u8bc4\u4f30\u5668\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u3001\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u63a8\u7406\u7ed3\u6784\u5bf9\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u4e2d\u8868\u73b0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.03741", "pdf": "https://arxiv.org/pdf/2508.03741", "abs": "https://arxiv.org/abs/2508.03741", "authors": ["Xin Liu", "Qiyang Song", "Shaowen Xu", "Kerou Zhou", "Wenbo Jiang", "Xiaoqi Jia", "Weijuan Zhang", "Heqing Huang", "Yakai Li"], "title": "Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ECAI 2025 - 28th European Conference on Artificial\n  Intelligence", "summary": "Large Language Models (LLMs) often retain inaccurate or outdated information\nfrom pre-training, leading to incorrect predictions or biased outputs during\ninference. While existing model editing methods can address this challenge,\nthey struggle with editing large amounts of factual information simultaneously\nand may compromise the general capabilities of the models. In this paper, our\nempirical study demonstrates that it is feasible to edit the internal\nrepresentations of LLMs and replace the entities in a manner similar to editing\nnatural language inputs. Based on this insight, we introduce the Latent\nKnowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of\nspecific entities via a lightweight hypernetwork to enable precise and\nlarge-scale editing. Experiments conducted on Llama-2 and Mistral show even\nwith the number of simultaneous edits reaching 10,000, LKS effectively performs\nknowledge editing while preserving the general abilities of the edited LLMs.\nCode is available at: https://github.com/Linuxin-xxx/LKS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLatent Knowledge Scalpel (LKS)\uff0c\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u7684LLM\u7f16\u8f91\u5de5\u5177\uff0c\u901a\u8fc7\u64cd\u7eb5\u6f5c\u5728\u8868\u793a\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u7cbe\u786e\u7684\u77e5\u8bc6\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u4fdd\u7559\u9884\u8bad\u7ec3\u4e2d\u4e0d\u51c6\u786e\u6216\u8fc7\u65f6\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u9519\u8bef\u9884\u6d4b\u6216\u504f\u89c1\u8f93\u51fa\u3002\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u7f16\u8f91\u5927\u91cf\u4e8b\u5b9e\u4fe1\u606f\uff0c\u4e14\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u7f16\u8f91LLM\u5185\u90e8\u8868\u793a\u5e76\u66ff\u6362\u5b9e\u4f53\u662f\u53ef\u884c\u7684\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faLatent Knowledge Scalpel (LKS)\uff0c\u5b83\u5229\u7528\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u64cd\u7eb5\u7279\u5b9a\u5b9e\u4f53\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u548c\u5927\u89c4\u6a21\u7684\u77e5\u8bc6\u7f16\u8f91\u3002", "result": "\u5728Llama-2\u548cMistral\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u540c\u65f6\u7f16\u8f91\u6570\u91cf\u8fbe\u523010,000\u6761\uff0cLKS\u4ecd\u80fd\u6709\u6548\u6267\u884c\u77e5\u8bc6\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u7559\u5df2\u7f16\u8f91LLM\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "LKS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u4e2d\u5927\u89c4\u6a21\u3001\u7cbe\u786e\u7684\u77e5\u8bc6\u7f16\u8f91\u6311\u6218\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u80fd\u529b\u4fdd\u6301\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.03722", "pdf": "https://arxiv.org/pdf/2508.03722", "abs": "https://arxiv.org/abs/2508.03722", "authors": ["Zhepeng Wang", "Yingjian Zhu", "Guanghao Dong", "Hongzhu Yi", "Feng Chen", "Xinming Wang", "Jun Xie"], "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors", "categories": ["cs.CV", "cs.AI"], "comment": "preprint", "summary": "This study investigates the integration of trustworthy prior reasoning\nknowledge from MLLMs into multimodal emotion recognition. We employ Gemini to\ngenerate fine-grained, modality-separable reasoning traces, which are injected\nas priors during the fusion stage to enrich cross-modal interactions. To\nmitigate the pronounced class-imbalance in multimodal emotion recognition, we\nintroduce Balanced Dual-Contrastive Learning, a loss formulation that jointly\nbalances inter-class and intra-class distributions. Applied to the MER2024\nbenchmark, our prior-enhanced framework yields substantial performance gains,\ndemonstrating that the reliability of MLLM-derived reasoning can be\nsynergistically combined with the domain adaptability of lightweight fusion\nnetworks for robust, scalable emotion recognition.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u53ef\u4fe1\u5148\u9a8c\u63a8\u7406\u77e5\u8bc6\u548c\u5f15\u5165\u5e73\u8861\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u5c06MLLMs\u7684\u53ef\u4fe1\u5148\u9a8c\u63a8\u7406\u77e5\u8bc6\u878d\u5165\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\uff0c\u5e76\u89e3\u51b3\u8be5\u9886\u57df\u666e\u904d\u5b58\u5728\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5229\u7528Gemini\u751f\u6210\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u53ef\u5206\u79bb\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u5728\u878d\u5408\u9636\u6bb5\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u6ce8\u5165\uff0c\u4ee5\u589e\u5f3a\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u5f15\u5165\u5e73\u8861\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff08\u4e00\u79cd\u635f\u5931\u51fd\u6570\uff09\u6765\u5171\u540c\u5e73\u8861\u7c7b\u95f4\u548c\u7c7b\u5185\u5206\u5e03\uff0c\u4ee5\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728MER2024\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u5148\u9a8c\u589e\u5f3a\u6846\u67b6\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MLLM\u5bfc\u51fa\u7684\u63a8\u7406\u53ef\u9760\u6027\u53ef\u4ee5\u4e0e\u8f7b\u91cf\u7ea7\u878d\u5408\u7f51\u7edc\u7684\u9886\u57df\u9002\u5e94\u6027\u534f\u540c\u7ed3\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u60c5\u611f\u8bc6\u522b\u3002"}}
{"id": "2508.03829", "pdf": "https://arxiv.org/pdf/2508.03829", "abs": "https://arxiv.org/abs/2508.03829", "authors": ["Jiahao Xu", "Rui Hu", "Zikai Zhang"], "title": "Majority Bit-Aware Watermarking For Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": "Preprint", "summary": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.", "AI": {"tldr": "\u9488\u5bf9LLM\u591a\u6bd4\u7279\u6c34\u5370\u6280\u672f\u4e2d\u5185\u5bb9\u8d28\u91cf\u4e0e\u89e3\u7801\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86MajorMark\u53ca\u5176\u589e\u5f3a\u7248MajorMark+\uff0c\u901a\u8fc7\u57fa\u4e8e\u591a\u6570\u6bd4\u7279\u7684\u7f16\u7801\u548c\u805a\u7c7b\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u6587\u672c\u7684\u751f\u6210\u8d28\u91cf\u548c\u89e3\u7801\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u751f\u6210\u6709\u5bb3\u6216\u6b3a\u9a97\u6027\u5185\u5bb9\u7684\u6f5c\u5728\u6ee5\u7528\u5f15\u8d77\u62c5\u5fe7\u3002\u6c34\u5370\u6280\u672f\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u5d4c\u5165\u53ef\u8bc6\u522b\u4fe1\u606f\u6765\u9a8c\u8bc1\u5185\u5bb9\u6765\u6e90\u548c\u8ffd\u8e2a\u6ee5\u7528\u7684\u65b9\u6cd5\uff0c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6848\u666e\u904d\u9762\u4e34\u5185\u5bb9\u8d28\u91cf\u4e0e\u89e3\u7801\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u4e3a\u786e\u4fdd\u53ef\u9760\u89e3\u7801\uff0c\u5fc5\u987b\u9650\u5236\u4f18\u9009\u8bcd\u5143\u96c6\u5927\u5c0f\uff0c\u4f46\u8fd9\u4f1a\u964d\u4f4e\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MajorMark\uff0c\u4e00\u79cd\u65b0\u578b\u6c34\u5370\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u591a\u6570\u6bd4\u7279\u611f\u77e5\u7684\u7f16\u7801\u6765\u6539\u8fdb\u6743\u8861\uff0c\u6839\u636e\u6d88\u606f\u7684\u591a\u6570\u6bd4\u7279\u9009\u62e9\u4f18\u9009\u8bcd\u5143\u96c6\uff0c\u4ece\u800c\u5141\u8bb8\u66f4\u5927\u3001\u66f4\u7075\u6d3b\u7684\u8bcd\u5143\u91c7\u6837\u3002\u4e0e\u4f9d\u8d56\u8bcd\u5143\u9891\u7387\u5206\u6790\u7684\u5148\u524d\u65b9\u6cd5\u4e0d\u540c\uff0cMajorMark\u91c7\u7528\u805a\u7c7b\u89e3\u7801\u7b56\u7565\uff0c\u5373\u4f7f\u4f18\u9009\u8bcd\u5143\u96c6\u8f83\u5927\u4e5f\u80fd\u4fdd\u6301\u9ad8\u89e3\u7801\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f15\u5165\u4e86MajorMark$^+$\uff0c\u5b83\u5c06\u6d88\u606f\u5212\u5206\u4e3a\u591a\u4e2a\u5757\uff0c\u72ec\u7acb\u7f16\u7801\u5e76\u786e\u5b9a\u6027\u89e3\u7801\u6bcf\u4e2a\u5757\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6c34\u5370\u6587\u672c\u8d28\u91cf\u548c\u89e3\u7801\u7cbe\u5ea6\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684LLMs\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u89e3\u7801\u7cbe\u5ea6\u548c\u6587\u672c\u751f\u6210\u8d28\u91cf\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6bd4\u7279\u6c34\u5370\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MajorMark\u53ca\u5176\u6539\u8fdb\u7248MajorMark$^+$\u6210\u529f\u89e3\u51b3\u4e86LLM\u591a\u6bd4\u7279\u6c34\u5370\u6280\u672f\u4e2d\u5185\u5bb9\u8d28\u91cf\u4e0e\u89e3\u7801\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6838\u5fc3\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7f16\u7801\u548c\u89e3\u7801\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u6587\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u6c34\u5370\u4fe1\u606f\u89e3\u7801\uff0c\u4e3aLLM\u5185\u5bb9\u6eaf\u6e90\u548c\u6ee5\u7528\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04150", "pdf": "https://arxiv.org/pdf/2508.04150", "abs": "https://arxiv.org/abs/2508.04150", "authors": ["Ilias Chrysovergis", "Alexandros-Apostolos A. Boulogeorgos", "Theodoros A. Tsiftsis", "Dusit Niyato"], "title": "Metaverse Framework for Wireless Systems Management", "categories": ["cs.NI"], "comment": "9 pages, 5 figures, 1 algorithm", "summary": "This article introduces a comprehensive metaverse framework, which is\ndesigned for the simulation, emulation, and interaction with wireless systems.\nThe proposed framework integrates core metaverse technologies such as extended\nreality (XR), digital twins (DTs), artificial intelligence (AI), internet of\nthings (IoT), blockchain, and advanced 6G networking solutions to create a\ndynamic, immersive platform for both system development and management. By\nleveraging XR, users can visualize and engage with complex systems, while DTs\nenable real-time monitoring and optimization. AI generates the\nthree-dimensional (3D) content, enhances decision-making and system\nperformance, whereas IoT devices provide real-time sensor data for boosting the\nsimulation accuracy. Additionally, blockchain ensures secure, decentralized\ninteractions, and 5G/6G networks offer the necessary infrastructure for\nseamless, low-latency communication. This framework serves as a robust tool for\nexploring, developing, and optimizing wireless systems, aiming to provide\nvaluable insights into the future of networked environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5143\u5b87\u5b99\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u7ebf\u7cfb\u7edf\u7684\u6a21\u62df\u3001\u4eff\u771f\u548c\u4ea4\u4e92\uff0c\u6574\u5408\u4e86XR\u3001\u6570\u5b57\u5b6a\u751f\u3001AI\u3001IoT\u3001\u533a\u5757\u94fe\u548c5G/6G\u7b49\u6838\u5fc3\u6280\u672f\u3002", "motivation": "\u65e8\u5728\u4e3a\u65e0\u7ebf\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u7ba1\u7406\u521b\u5efa\u4e00\u4e2a\u52a8\u6001\u3001\u6c89\u6d78\u5f0f\u7684\u5143\u5b87\u5b99\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7\u63d0\u4f9b\u5bf9\u672a\u6765\u7f51\u7edc\u73af\u5883\u7684\u6df1\u5165\u89c1\u89e3\u6765\u63a2\u7d22\u3001\u53d1\u5c55\u548c\u4f18\u5316\u65e0\u7ebf\u7cfb\u7edf\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4ee5\u4e0b\u6838\u5fc3\u5143\u5b87\u5b99\u6280\u672f\u5b9e\u73b0\uff1a\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u7528\u4e8e\u53ef\u89c6\u5316\u548c\u4ea4\u4e92\uff1b\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u548c\u4f18\u5316\uff1b\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7528\u4e8e\u751f\u62103D\u5185\u5bb9\u3001\u589e\u5f3a\u51b3\u7b56\u548c\u7cfb\u7edf\u6027\u80fd\uff1b\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u63d0\u4f9b\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u4ee5\u63d0\u9ad8\u6a21\u62df\u7cbe\u5ea6\uff1b\u533a\u5757\u94fe\u786e\u4fdd\u5b89\u5168\u3001\u53bb\u4e2d\u5fc3\u5316\u4ea4\u4e92\uff1b\u4ee5\u53ca5G/6G\u7f51\u7edc\u63d0\u4f9b\u65e0\u7f1d\u3001\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u65e0\u7ebf\u7cfb\u7edf\u7684\u53ef\u89c6\u5316\u548c\u4ea4\u4e92\uff0c\u63d0\u4f9b\u5b9e\u65f6\u76d1\u63a7\u548c\u4f18\u5316\u80fd\u529b\uff0c\u901a\u8fc7AI\u751f\u62103D\u5185\u5bb9\u5e76\u63d0\u5347\u51b3\u7b56\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u5229\u7528IoT\u6570\u636e\u63d0\u9ad8\u6a21\u62df\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u533a\u5757\u94fe\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\u3002\u5b83\u4f5c\u4e3a\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u652f\u6301\u65e0\u7ebf\u7cfb\u7edf\u7684\u63a2\u7d22\u3001\u5f00\u53d1\u548c\u4f18\u5316\u3002", "conclusion": "\u8be5\u5168\u9762\u7684\u5143\u5b87\u5b99\u6846\u67b6\u4e3a\u63a2\u7d22\u3001\u5f00\u53d1\u548c\u4f18\u5316\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u671b\u4e3a\u672a\u6765\u7f51\u7edc\u73af\u5883\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.03986", "pdf": "https://arxiv.org/pdf/2508.03986", "abs": "https://arxiv.org/abs/2508.03986", "authors": ["Yuan Xun", "Xiaojun Jia", "Xinwei Liu", "Hua Zhang"], "title": "The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?", "categories": ["cs.AI"], "comment": null, "summary": "We observe that MLRMs oriented toward human-centric service are highly\nsusceptible to user emotional cues during the deep-thinking stage, often\noverriding safety protocols or built-in safety checks under high emotional\nintensity. Inspired by this key insight, we propose EmoAgent, an autonomous\nadversarial emotion-agent framework that orchestrates exaggerated affective\nprompts to hijack reasoning pathways. Even when visual risks are correctly\nidentified, models can still produce harmful completions through emotional\nmisalignment. We further identify persistent high-risk failure modes in\ntransparent deep-thinking scenarios, such as MLRMs generating harmful reasoning\nmasked behind seemingly safe responses. These failures expose misalignments\nbetween internal inference and surface-level behavior, eluding existing\ncontent-based safeguards. To quantify these risks, we introduce three metrics:\n(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign\noutputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite\nvisual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for\nevaluating refusal unstability under prompt variants. Extensive experiments on\nadvanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper\nemotional cognitive misalignments in model safety behavior.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u9762\u5411\u4eba\u7c7b\u670d\u52a1\u7684MLRM\u5728\u60c5\u611f\u63d0\u793a\u4e0b\u6613\u7ed5\u8fc7\u5b89\u5168\u534f\u8bae\uff0c\u4ea7\u751f\u6709\u5bb3\u5185\u5bb9\uff0c\u5373\u4f7f\u89c6\u89c9\u98ce\u9669\u88ab\u8bc6\u522b\u3002\u63d0\u51faEmoAgent\u6846\u67b6\uff0c\u901a\u8fc7\u5938\u5927\u60c5\u611f\u63d0\u793a\u52ab\u6301\u6a21\u578b\u63a8\u7406\u3002\u540c\u65f6\uff0c\u8bc6\u522b\u51fa\u6a21\u578b\u5185\u90e8\u63a8\u7406\u4e0e\u8868\u9762\u884c\u4e3a\u7684\u9519\u4f4d\uff0c\u5e76\u5f15\u5165RRSS\u3001RVNR\u548cRAIC\u4e09\u9879\u65b0\u6307\u6807\u91cf\u5316\u98ce\u9669\u3002\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u5b89\u5168\u884c\u4e3a\u5b58\u5728\u6df1\u5c42\u60c5\u611f\u8ba4\u77e5\u9519\u4f4d\u3002", "motivation": "\u7814\u7a76\u89c2\u5bdf\u5230\u9762\u5411\u4eba\u7c7b\u670d\u52a1\u7684MLRM\u5728\u6df1\u5ea6\u601d\u8003\u9636\u6bb5\u5bf9\u7528\u6237\u60c5\u611f\u63d0\u793a\u9ad8\u5ea6\u654f\u611f\uff0c\u5728\u9ad8\u60c5\u611f\u5f3a\u5ea6\u4e0b\u53ef\u80fd\u7ed5\u8fc7\u5b89\u5168\u534f\u8bae\uff0c\u5bfc\u81f4\u6709\u5bb3\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u73b0\u6709\u5185\u5bb9\u7ea7\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u6355\u83b7\u6a21\u578b\u5185\u90e8\u63a8\u7406\u4e0e\u8868\u9762\u884c\u4e3a\u7684\u9519\u4f4d\uff0c\u4f7f\u5f97\u6709\u5bb3\u63a8\u7406\u88ab\u770b\u4f3c\u5b89\u5168\u7684\u54cd\u5e94\u6240\u63a9\u76d6\u3002", "method": "1. \u63d0\u51faEmoAgent\uff0c\u4e00\u4e2a\u81ea\u4e3b\u5bf9\u6297\u6027\u60c5\u611f\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7f16\u6392\u5938\u5927\u7684\u60c5\u611f\u63d0\u793a\u6765\u52ab\u6301\u6a21\u578b\u7684\u63a8\u7406\u8def\u5f84\u30022. \u5f15\u5165\u4e09\u4e2a\u65b0\u7684\u91cf\u5316\u98ce\u9669\u6307\u6807\uff1a\u98ce\u9669\u63a8\u7406\u9690\u853d\u5f97\u5206 (RRSS) \u7528\u4e8e\u8bc4\u4f30\u826f\u6027\u8f93\u51fa\u4e0b\u7684\u6709\u5bb3\u63a8\u7406\uff1b\u98ce\u9669\u89c6\u89c9\u5ffd\u89c6\u7387 (RVNR) \u7528\u4e8e\u8861\u91cf\u8bc6\u522b\u89c6\u89c9\u98ce\u9669\u4f46\u4ecd\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u6bd4\u4f8b\uff1b\u62d2\u7edd\u6001\u5ea6\u4e0d\u4e00\u81f4\u6027 (RAIC) \u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u4e0b\u7684\u62d2\u7edd\u7a33\u5b9a\u6027\u3002", "result": "1. \u5373\u4f7f\u6a21\u578b\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u89c6\u89c9\u98ce\u9669\uff0c\u4e5f\u53ef\u80fd\u56e0\u60c5\u611f\u9519\u4f4d\u800c\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u30022. \u53d1\u73b0\u4e86\u900f\u660e\u6df1\u5ea6\u601d\u8003\u573a\u666f\u4e2d\u6301\u7eed\u5b58\u5728\u7684\u9ad8\u98ce\u9669\u5931\u8d25\u6a21\u5f0f\uff0c\u5373MLRM\u5728\u770b\u4f3c\u5b89\u5168\u7684\u54cd\u5e94\u80cc\u540e\u751f\u6210\u6709\u5bb3\u63a8\u7406\u30023. \u8fd9\u4e9b\u5931\u8d25\u66b4\u9732\u4e86\u5185\u90e8\u63a8\u7406\u4e0e\u8868\u9762\u884c\u4e3a\u4e4b\u95f4\u7684\u9519\u4f4d\uff0c\u73b0\u6709\u57fa\u4e8e\u5185\u5bb9\u7684\u9632\u62a4\u63aa\u65bd\u65e0\u6cd5\u68c0\u6d4b\u30024. \u5bf9\u5148\u8fdbMLRM\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86EmoAgent\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5b89\u5168\u884c\u4e3a\u4e2d\u66f4\u6df1\u5c42\u6b21\u7684\u60c5\u611f\u8ba4\u77e5\u9519\u4f4d\u3002", "conclusion": "MLRM\u5728\u5904\u7406\u7528\u6237\u60c5\u611f\u63d0\u793a\u65f6\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8868\u73b0\u4e3a\u60c5\u611f\u8ba4\u77e5\u9519\u4f4d\u5bfc\u81f4\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u4e14\u73b0\u6709\u5b89\u5168\u673a\u5236\u96be\u4ee5\u5bdf\u89c9\u3002\u89e3\u51b3\u6a21\u578b\u5185\u90e8\u63a8\u7406\u4e0e\u8868\u9762\u884c\u4e3a\u4e4b\u95f4\u7684\u9519\u4f4d\u662f\u63d0\u5347\u5176\u5b89\u5168\u6027\u7684\u5173\u952e\u3002EmoAgent\u53ca\u5176\u63d0\u51fa\u7684\u91cf\u5316\u6307\u6807\u6709\u52a9\u4e8e\u63ed\u793a\u548c\u7406\u89e3\u8fd9\u4e9b\u6df1\u5c42\u95ee\u9898\u3002"}}
{"id": "2508.03750", "pdf": "https://arxiv.org/pdf/2508.03750", "abs": "https://arxiv.org/abs/2508.03750", "authors": ["Cheng Huang", "Weizheng Xie", "Karanjit Kooner", "Tsengdar Lee", "Jui-Kai Wang", "Jia Zhang"], "title": "GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification", "categories": ["cs.LG", "cs.CE", "cs.CV", "eess.IV"], "comment": null, "summary": "Early and accurate detection of glaucoma is critical to prevent irreversible\nvision loss. However, existing methods often rely on unimodal data and lack\ninterpretability, limiting their clinical utility. In this paper, we present\nGlaBoost, a multimodal gradient boosting framework that integrates structured\nclinical features, fundus image embeddings, and expert-curated textual\ndescriptions for glaucoma risk prediction. GlaBoost extracts high-level visual\nrepresentations from retinal fundus photographs using a pretrained\nconvolutional encoder and encodes free-text neuroretinal rim assessments using\na transformer-based language model. These heterogeneous signals, combined with\nmanually assessed risk scores and quantitative ophthalmic indicators, are fused\ninto a unified feature space for classification via an enhanced XGBoost model.\nExperiments conducted on a real-world annotated dataset demonstrate that\nGlaBoost significantly outperforms baseline models, achieving a validation\naccuracy of 98.71%. Feature importance analysis reveals clinically consistent\npatterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings\ncontributing most to model decisions. GlaBoost offers a transparent and\nscalable solution for interpretable glaucoma diagnosis and can be extended to\nother ophthalmic disorders.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GlaBoost\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u3001\u53ef\u89e3\u91ca\u7684\u9752\u5149\u773c\u98ce\u9669\u9884\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u50cf\u3001\u6587\u672c\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u9752\u5149\u773c\u7684\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u5bf9\u4e8e\u9884\u9632\u4e0d\u53ef\u9006\u7684\u89c6\u529b\u4e27\u5931\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "GlaBoost\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u68af\u5ea6\u63d0\u5347\u6846\u67b6\u3002\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u7f16\u7801\u5668\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u89c6\u89c9\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u4e13\u5bb6\u6587\u672c\u63cf\u8ff0\u3002\u8fd9\u4e9b\u5f02\u6784\u4fe1\u53f7\uff0c\u7ed3\u5408\u624b\u52a8\u8bc4\u4f30\u7684\u98ce\u9669\u8bc4\u5206\u548c\u5b9a\u91cf\u773c\u79d1\u6307\u6807\uff0c\u88ab\u878d\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u578bXGBoost\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGlaBoost\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe\u523098.71%\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e34\u5e8a\u4e00\u81f4\u7684\u6a21\u5f0f\uff0c\u5176\u4e2d\u676f\u76d8\u6bd4\u3001\u89c6\u76d8\u82cd\u767d\u548c\u7279\u5b9a\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u6a21\u578b\u51b3\u7b56\u8d21\u732e\u6700\u5927\u3002", "conclusion": "GlaBoost\u4e3a\u53ef\u89e3\u91ca\u7684\u9752\u5149\u773c\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u63a8\u5e7f\u5e94\u7528\u4e8e\u5176\u4ed6\u773c\u79d1\u75be\u75c5\u3002"}}
{"id": "2508.03724", "pdf": "https://arxiv.org/pdf/2508.03724", "abs": "https://arxiv.org/abs/2508.03724", "authors": ["Jia Li", "Yapeng Tian"], "title": "From Waveforms to Pixels: A Survey on Audio-Visual Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing\nobjects in videos by leveraging both visual and audio modalities. It has\nemerged as a significant research area in multimodal perception, enabling\nfine-grained object-level understanding. In this survey, we present a\ncomprehensive overview of the AVS field, covering its problem formulation,\nbenchmark datasets, evaluation metrics, and the progression of methodologies.\nWe analyze a wide range of approaches, including architectures for unimodal and\nmultimodal encoding, key strategies for audio-visual fusion, and various\ndecoder designs. Furthermore, we examine major training paradigms, from fully\nsupervised learning to weakly supervised and training-free methods. Notably, we\nprovide an extensive comparison of AVS methods across standard benchmarks,\nhighlighting the impact of different architectural choices, fusion strategies,\nand training paradigms on performance. Finally, we outline the current\nchallenges, such as limited temporal modeling, modality bias toward vision,\nlack of robustness in complex environments, and high computational demands, and\npropose promising future directions, including improving temporal reasoning and\nmultimodal fusion, leveraging foundation models for better generalization and\nfew-shot learning, reducing reliance on labeled data through selfand weakly\nsupervised learning, and incorporating higher-level reasoning for more\nintelligent AVS systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u97f3\u89c6\u9891\u5206\u5272\uff08AVS\uff09\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5176\u95ee\u9898\u5b9a\u4e49\u3001\u65b9\u6cd5\u3001\u8bad\u7ec3\u8303\u5f0f\u3001\u6027\u80fd\u6bd4\u8f83\u3001\u5f53\u524d\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u97f3\u89c6\u9891\u5206\u5272\uff08AVS\uff09\u4f5c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u4e2d\u7684\u91cd\u8981\u7814\u7a76\u9886\u57df\uff0c\u65e8\u5728\u5229\u7528\u89c6\u89c9\u548c\u97f3\u9891\u6a21\u6001\u8bc6\u522b\u5e76\u5206\u5272\u89c6\u9891\u4e2d\u7684\u53d1\u58f0\u7269\u4f53\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8c61\u7ea7\u7406\u89e3\u3002\u672c\u8bba\u6587\u65e8\u5728\u63d0\u4f9bAVS\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5206\u6790\u5176\u65b9\u6cd5\u8fdb\u5c55\u3001\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u8d8b\u52bf\u3002", "method": "\u672c\u6587\u5bf9AVS\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u5185\u5bb9\u6db5\u76d6\u95ee\u9898\u5b9a\u4e49\u3001\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u5b66\u6f14\u8fdb\u3002\u8be6\u7ec6\u5206\u6790\u4e86\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u7f16\u7801\u67b6\u6784\u3001\u5173\u952e\u97f3\u89c6\u9891\u878d\u5408\u7b56\u7565\u4ee5\u53ca\u5404\u79cd\u89e3\u7801\u5668\u8bbe\u8ba1\u3002\u540c\u65f6\uff0c\u8003\u5bdf\u4e86\u4ece\u5168\u76d1\u7763\u5b66\u4e60\u5230\u5f31\u76d1\u7763\u548c\u514d\u8bad\u7ec3\u7684\u4e3b\u8981\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u5bf9AVS\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6bd4\u8f83\uff0c\u4ee5\u7a81\u51fa\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u3001\u878d\u5408\u7b56\u7565\u548c\u8bad\u7ec3\u8303\u5f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86AVS\u9886\u57df\u7684\u95ee\u9898\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u5b66\u6f14\u8fdb\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u3001\u878d\u5408\u7b56\u7565\u548c\u8bad\u7ec3\u8303\u5f0f\u5bf9AVS\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8bc6\u522b\u4e86\u5f53\u524dAVS\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5305\u62ec\u6709\u9650\u7684\u65f6\u95f4\u5efa\u6a21\u3001\u89c6\u89c9\u6a21\u6001\u504f\u501a\u3001\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u4ee5\u53ca\u9ad8\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "AVS\u4f5c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u7684\u91cd\u8981\u65b9\u5411\uff0c\u867d\u7136\u53d1\u5c55\u8fc5\u901f\u4f46\u4ecd\u9762\u4e34\u591a\u9879\u6311\u6218\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u63d0\u5347\u65f6\u95f4\u63a8\u7406\u548c\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u3001\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u53ca\u878d\u5165\u66f4\u9ad8\u5c42\u7ea7\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u6784\u5efa\u66f4\u667a\u80fd\u3001\u66f4\u9c81\u68d2\u7684AVS\u7cfb\u7edf\u3002"}}
{"id": "2508.03860", "pdf": "https://arxiv.org/pdf/2508.03860", "abs": "https://arxiv.org/abs/2508.03860", "authors": ["Subhey Sadi Rahman", "Md. Adnanul Islam", "Md. Mahbub Alam", "Musarrat Zeba", "Md. Abdur Rahman", "Sadia Sultana Chowa", "Mohaimenul Azam Khan Raiaan", "Sami Azam"], "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence\n  Review for peer review", "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.", "AI": {"tldr": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86LLM\u5185\u5bb9\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\u7684\u6311\u6218\u4e0e\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u901a\u8fc7\u9ad8\u7ea7\u7b56\u7565\uff08\u5982RAG\u3001\u9886\u57df\u5fae\u8c03\uff09\u6784\u5efa\u66f4\u53ef\u4fe1\u3001\u9886\u57df\u5b9a\u5236\u5316LLM\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5305\u542b\u4e0d\u51c6\u786e\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\u7684\u4e92\u8054\u7f51\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5176\u53ef\u80fd\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u56e0\u6b64\u6025\u9700\u5065\u58ee\u7684\u4e8b\u5b9e\u6838\u67e5\u673a\u5236\u3002", "method": "\u672c\u6587\u5bf92020\u5e74\u81f32025\u5e74\u7684\u76f8\u5173\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u5206\u6790\u4e86LLM\u751f\u6210\u5185\u5bb9\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u4e3b\u8981\u6311\u6218\uff08\u5982\u5e7b\u89c9\u3001\u6570\u636e\u96c6\u9650\u5236\u3001\u8bc4\u4f30\u6307\u6807\u53ef\u9760\u6027\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u5305\u62ec\u9ad8\u7ea7\u63d0\u793a\u7b56\u7565\u3001\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7b49\u5728\u5185\u7684\u7f13\u89e3\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff1b\u5f3a\u8c03\u4e86\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u7684\u5916\u90e8\u8bc1\u636e\u6765\u9a8c\u8bc1LLM\u8f93\u51fa\u7684\u4ef7\u503c\uff1b\u5e76\u6307\u51fa\u9886\u57df\u7279\u5b9a\u5b9a\u5236\u5bf9\u4e8e\u63d0\u9ad8\u4e8b\u5b9e\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6784\u5efa\u4e0d\u4ec5\u51c6\u786e\u3001\u53ef\u89e3\u91ca\uff0c\u800c\u4e14\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u4e8b\u5b9e\u6838\u67e5\u5b9a\u5236\u7684LLM\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u7814\u7a76\uff0c\u5b9e\u73b0\u66f4\u503c\u5f97\u4fe1\u8d56\u548c\u8bed\u5883\u611f\u77e5\u7684\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.04317", "pdf": "https://arxiv.org/pdf/2508.04317", "abs": "https://arxiv.org/abs/2508.04317", "authors": ["Joshua Smailes", "Filip Futera", "Sebastian K\u00f6hler", "Simon Birnbach", "Martin Strohmeier", "Ivan Martinovic"], "title": "DSNS: The Deep Space Network Simulator", "categories": ["cs.NI"], "comment": "12 pages, 8 figures, 3 tables", "summary": "Simulation tools are commonly used in the development and testing of new\nprotocols or new networks. However, as satellite networks start to grow to\nencompass thousands of nodes, and as companies and space agencies begin to\nrealize the interplanetary internet, existing satellite and network simulation\ntools have become impractical for use in this context.\n  We therefore present the Deep Space Network Simulator (DSNS): a new network\nsimulator with a focus on large-scale satellite networks. We demonstrate its\nimproved capabilities compared to existing offerings, showcase its flexibility\nand extensibility through an implementation of existing protocols and the DTN\nsimulation reference scenarios recommended by CCSDS, and evaluate its\nscalability, showing that it exceeds existing tools while providing better\nfidelity.\n  DSNS provides concrete usefulness to both standards bodies and satellite\noperators, enabling fast iteration on protocol development and testing of\nparameters under highly realistic conditions. By removing roadblocks to\nresearch and innovation, we can accelerate the development of upcoming\nsatellite networks and ensure that their communication is both fast and secure.", "AI": {"tldr": "\u73b0\u6709\u6a21\u62df\u5de5\u5177\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u536b\u661f\u548c\u661f\u9645\u7f51\u7edc\u9700\u6c42\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u578b\u6df1\u7a7a\u7f51\u7edc\u6a21\u62df\u5668DSNS\uff0c\u8be5\u6a21\u62df\u5668\u5728\u53ef\u6269\u5c55\u6027\u3001\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u672a\u6765\u536b\u661f\u7f51\u7edc\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u536b\u661f\u7f51\u7edc\u8282\u70b9\u6570\u91cf\u589e\u591a\uff08\u6570\u5343\u4e2a\uff09\u548c\u661f\u9645\u4e92\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u73b0\u6709\u536b\u661f\u548c\u7f51\u7edc\u6a21\u62df\u5de5\u5177\u5728\u5904\u7406\u6b64\u7c7b\u5927\u89c4\u6a21\u590d\u6742\u7f51\u7edc\u65f6\u5df2\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5e76\u5f00\u53d1\u4e86\u6df1\u7a7a\u7f51\u7edc\u6a21\u62df\u5668\uff08DSNS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u6ce8\u4e8e\u5927\u89c4\u6a21\u536b\u661f\u7f51\u7edc\u7684\u65b0\u578b\u7f51\u7edc\u6a21\u62df\u5668\u3002", "result": "DSNS\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u80fd\u529b\uff1b\u901a\u8fc7\u5b9e\u73b0\u73b0\u6709\u534f\u8bae\u548cCCSDS\u63a8\u8350\u7684DTN\u6a21\u62df\u53c2\u8003\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff1b\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u5de5\u5177\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u4eff\u771f\u7cbe\u5ea6\u3002", "conclusion": "DSNS\u4e3a\u6807\u51c6\u673a\u6784\u548c\u536b\u661f\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u52a0\u901f\u534f\u8bae\u5f00\u53d1\u548c\u53c2\u6570\u6d4b\u8bd5\uff0c\u4ece\u800c\u63a8\u52a8\u672a\u6765\u536b\u661f\u7f51\u7edc\u7684\u5feb\u901f\u5b89\u5168\u901a\u4fe1\u3002"}}
{"id": "2508.03991", "pdf": "https://arxiv.org/pdf/2508.03991", "abs": "https://arxiv.org/abs/2508.03991", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGalaxy\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u8ba4\u77e5\u67b6\u6784\u548c\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u6784\u5efa\u4e86\u4e3b\u52a8\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u80fd\u81ea\u6211\u8fdb\u5316\u7684\u667a\u80fd\u4e2a\u4eba\u52a9\u624b\uff08IPA\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4e2a\u4eba\u52a9\u624b\uff08IPAs\uff09\u5728\u54cd\u5e94\u80fd\u529b\u65b9\u9762\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u4e3b\u52a8\u884c\u4e3a\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u4ecd\u662f\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u5174\u8d77\u4e3a\u5f00\u53d1\u6b64\u7c7bIPAs\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51faCognition Forest\u8bed\u4e49\u7ed3\u6784\uff0c\u5c06\u8ba4\u77e5\u5efa\u6a21\u4e0e\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u5bf9\u9f50\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86Galaxy\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u8ba4\u77e5\u67b6\u6784\u548c\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u81ea\u5f3a\u5316\u5faa\u73af\uff0c\u5e76\u5305\u542bKoRa\uff08\u652f\u6301\u54cd\u5e94\u548c\u4e3b\u52a8\u6280\u80fd\u7684\u8ba4\u77e5\u589e\u5f3a\u751f\u6210\u4ee3\u7406\uff09\u548cKernel\uff08\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5143\u8ba4\u77e5\u4ee3\u7406\uff09\u3002", "result": "Galaxy\u6846\u67b6\u5728\u591a\u9879\u6700\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6848\u4f8b\u9a8c\u8bc1\u4e86Galaxy\u7684\u6709\u6548\u6027\u3002", "conclusion": "Galaxy\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6784\u5efa\u4e3b\u52a8\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u81ea\u6211\u8fdb\u5316\u667a\u80fd\u4e2a\u4eba\u52a9\u624b\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.03755", "pdf": "https://arxiv.org/pdf/2508.03755", "abs": "https://arxiv.org/abs/2508.03755", "authors": ["Wenwu Gong", "Lili Yang"], "title": "LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion", "categories": ["cs.LG", "cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Multi-dimensional data completion is a critical problem in computational\nsciences, particularly in domains such as computer vision, signal processing,\nand scientific computing. Existing methods typically leverage either global\nlow-rank approximations or local smoothness regularization, but each suffers\nfrom notable limitations: low-rank methods are computationally expensive and\nmay disrupt intrinsic data structures, while smoothness-based approaches often\nrequire extensive manual parameter tuning and exhibit poor generalization. In\nthis paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)\nmodel that unifies global and local prior modeling within a Tucker\ndecomposition. Specifically, LRTuckerRep encodes low rankness through a\nself-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker\ncore, while capturing smoothness via a parameter-free Laplacian-based\nregularization on the factor spaces. To efficiently solve the resulting\nnonconvex optimization problem, we develop two iterative algorithms with\nprovable convergence guarantees. Extensive experiments on multi-dimensional\nimage inpainting and traffic data imputation demonstrate that LRTuckerRep\nachieves superior completion accuracy and robustness under high missing rates\ncompared to baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLRTuckerRep\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408Tucker\u5206\u89e3\u4e2d\u7684\u5168\u5c40\u4f4e\u79e9\u548c\u5c40\u90e8\u5e73\u6ed1\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7ef4\u6570\u636e\u8865\u5168\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8865\u5168\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u8865\u5168\u5728\u8ba1\u7b97\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5168\u5c40\u4f4e\u79e9\u8fd1\u4f3c\u6216\u5c40\u90e8\u5e73\u6ed1\u6b63\u5219\u5316\uff09\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1a\u4f4e\u79e9\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u53ef\u80fd\u7834\u574f\u6570\u636e\u7ed3\u6784\uff1b\u5e73\u6ed1\u65b9\u6cd5\u9700\u5927\u91cf\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u4e14\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u63d0\u51faLow-Rank Tucker Representation (LRTuckerRep) \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728Tucker\u5206\u89e3\u4e2d\u7edf\u4e00\u4e86\u5168\u5c40\u4f4e\u79e9\u548c\u5c40\u90e8\u5e73\u6ed1\u5148\u9a8c\u5efa\u6a21\u3002\u5177\u4f53\u901a\u8fc7\u56e0\u5b50\u77e9\u9635\u4e0a\u7684\u81ea\u9002\u5e94\u52a0\u6743\u6838\u8303\u6570\u548c\u7a00\u758fTucker\u6838\u5fc3\u7f16\u7801\u4f4e\u79e9\u6027\uff0c\u5e76\u901a\u8fc7\u56e0\u5b50\u7a7a\u95f4\u4e0a\u7684\u65e0\u53c2\u6570\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u6355\u6349\u5e73\u6ed1\u6027\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e24\u79cd\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u7ef4\u56fe\u50cf\u4fee\u590d\u548c\u4ea4\u901a\u6570\u636e\u63d2\u8865\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cLRTuckerRep\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8865\u5168\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LRTuckerRep\u6a21\u578b\u901a\u8fc7\u5728Tucker\u5206\u89e3\u4e2d\u7edf\u4e00\u5168\u5c40\u548c\u5c40\u90e8\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7ef4\u6570\u636e\u8865\u5168\u95ee\u9898\uff0c\u5e76\u7ecf\u9a8c\u6027\u5730\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03725", "pdf": "https://arxiv.org/pdf/2508.03725", "abs": "https://arxiv.org/abs/2508.03725", "authors": ["Yida Wang", "Taiting Lu", "Runze Liu", "Lanqing Yang", "Yifan Yang", "Zhe Chen", "Yuehai Wang", "Yixin Liu", "Kaiyuan Lin", "Xiaomeng Chen", "Dian Ding", "Yijie Li", "Yi-Chao Chen", "Yincheng Jin", "Mahanth Gowda"], "title": "A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Printed-Circuit-board (PCB) footprint geometry labeling of integrated\ncircuits (IC) is essential in defining the physical interface between\ncomponents and the PCB layout, requiring exceptional visual perception\nproficiency. However, due to the unstructured footprint drawing and abstract\ndiagram annotations, automated parsing and accurate footprint geometry modeling\nremain highly challenging. Despite its importance, no methods currently exist\nfor automated package geometry labeling directly from IC mechanical drawings.\nIn this paper, we first investigate the visual perception performance of Large\nMultimodal Models (LMMs) when solving IC footprint geometry understanding. Our\nfindings reveal that current LMMs severely suffer from inaccurate geometric\nperception, which hinders their performance in solving the footprint geometry\nlabeling problem. To address these limitations, we propose LLM4-IC8K, a novel\nframework that treats IC mechanical drawings as images and leverages LLMs for\nstructured geometric interpretation. To mimic the step-by-step reasoning\napproach used by human engineers, LLM4-IC8K addresses three sub-tasks:\nperceiving the number of pins, computing the center coordinates of each pin,\nand estimating the dimensions of individual pins. We present a two-stage\nframework that first trains LMMs on synthetically generated IC footprint\ndiagrams to learn fundamental geometric reasoning and then fine-tunes them on\nreal-world datasheet drawings to enhance robustness and accuracy in practical\nscenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with\n8,608 labeled samples, including 4138 hand-crafted IC footprint samples and\n4470 synthetically generated samples. Extensive experiments demonstrate that\nour model outperforms state-of-the-art LMMs on the proposed benchmark.", "AI": {"tldr": "\u9488\u5bf9\u96c6\u6210\u7535\u8def\uff08IC\uff09\u5370\u5236\u7535\u8def\u677f\uff08PCB\uff09\u5c01\u88c5\u51e0\u4f55\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faLLM4-IC8K\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4eceIC\u673a\u68b0\u56fe\u7eb8\u4e2d\u8fdb\u884c\u7ed3\u6784\u5316\u51e0\u4f55\u89e3\u91ca\uff0c\u5e76\u5f15\u5165ICGeo8K\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLLM4-IC8K\u5728\u51e0\u4f55\u611f\u77e5\u548c\u6807\u6ce8\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u3002", "motivation": "IC PCB\u5c01\u88c5\u51e0\u4f55\u6807\u6ce8\u5bf9\u4e8e\u5b9a\u4e49\u7ec4\u4ef6\u4e0ePCB\u5e03\u5c40\u7684\u7269\u7406\u63a5\u53e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u56fe\u7eb8\u975e\u7ed3\u6784\u5316\u548c\u62bd\u8c61\u6807\u6ce8\uff0c\u81ea\u52a8\u5316\u89e3\u6790\u548c\u5efa\u6a21\u6781\u5177\u6311\u6218\u3002\u76ee\u524d\u7f3a\u4e4f\u76f4\u63a5\u4eceIC\u673a\u68b0\u56fe\u7eb8\u81ea\u52a8\u5316\u6807\u6ce8\u7684\u65b9\u6cd5\uff0c\u4e14\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728IC\u5c01\u88c5\u51e0\u4f55\u7406\u89e3\u4e0a\u5b58\u5728\u4e25\u91cd\u7684\u51e0\u4f55\u611f\u77e5\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "method": "\u63d0\u51faLLM4-IC8K\u6846\u67b6\uff0c\u5c06IC\u673a\u68b0\u56fe\u7eb8\u89c6\u4e3a\u56fe\u50cf\uff0c\u5e76\u5229\u7528LLM\u8fdb\u884c\u7ed3\u6784\u5316\u51e0\u4f55\u89e3\u91ca\u3002\u8be5\u6846\u67b6\u6a21\u4eff\u4eba\u7c7b\u5de5\u7a0b\u5e08\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u611f\u77e5\u5f15\u811a\u6570\u91cf\u3001\u8ba1\u7b97\u5f15\u811a\u4e2d\u5fc3\u5750\u6807\u548c\u4f30\u8ba1\u5f15\u811a\u5c3a\u5bf8\u4e09\u4e2a\u5b50\u4efb\u52a1\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728\u5408\u6210IC\u5c01\u88c5\u56fe\u4e0a\u8bad\u7ec3LMM\u5b66\u4e60\u57fa\u7840\u51e0\u4f55\u63a8\u7406\uff0c\u7136\u540e\u5728\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u5305\u542b8608\u4e2a\u6837\u672c\uff084138\u4e2a\u624b\u7ed8\uff0c4470\u4e2a\u5408\u6210\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6ICGeo8K\u3002", "result": "\u521d\u6b65\u7814\u7a76\u63ed\u793a\u5f53\u524dLMMs\u5728IC\u5c01\u88c5\u51e0\u4f55\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7684\u51e0\u4f55\u611f\u77e5\u4e0d\u51c6\u786e\u6027\u3002\u672c\u6587\u63d0\u51fa\u7684LLM4-IC8K\u6a21\u578b\u5728\u6240\u8bbe\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdbLMMs\u3002", "conclusion": "LLM4-IC8K\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u7684\u7ed3\u6784\u5316\u89e3\u91ca\u80fd\u529b\u3001\u6a21\u4eff\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u4ee5\u53ca\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86IC PCB\u5c01\u88c5\u51e0\u4f55\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u96be\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002"}}
{"id": "2508.03865", "pdf": "https://arxiv.org/pdf/2508.03865", "abs": "https://arxiv.org/abs/2508.03865", "authors": ["Yajie Luo", "Yihong Wu", "Muzhi Li", "Fengran Mo", "Jia Ao Sun", "Xinyu Wang", "Liheng Ma", "Yingxue Zhang", "Jian-Yun Nie"], "title": "An Entity Linking Agent for Question Answering", "categories": ["cs.CL"], "comment": "12 pages, 2 figures. Submitted to AAAI 2026 Conference", "summary": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9e\u4f53\u94fe\u63a5\u4ee3\u7406\uff0c\u7528\u4e8e\u89e3\u51b3\u95ee\u7b54\uff08QA\uff09\u7cfb\u7edf\u4e2d\u77ed\u800c\u6a21\u7cca\u95ee\u9898\u7684\u5b9e\u4f53\u94fe\u63a5\u6311\u6218\uff0c\u5e76\u7ecf\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u8bbe\u8ba1\uff0c\u5728\u5904\u7406\u95ee\u7b54\uff08QA\uff09\u4efb\u52a1\u4e2d\u77ed\u5c0f\u3001\u6a21\u7cca\u7684\u7528\u6237\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u51c6\u786e\u94fe\u63a5\u81ea\u7136\u8bed\u8a00\u63d0\u53ca\u5230\u77e5\u8bc6\u5e93\u6761\u76ee\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9e\u4f53\u94fe\u63a5\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u4e3b\u52a8\u8bc6\u522b\u5b9e\u4f53\u63d0\u53ca\u3001\u68c0\u7d22\u5019\u9009\u5b9e\u4f53\u5e76\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u901a\u8fc7\u5de5\u5177\u5316\u5b9e\u4f53\u94fe\u63a5\u548c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u4e24\u9879\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u5b9e\u4f53\u94fe\u63a5\u4ee3\u7406\u80fd\u591f\u6709\u6548\u89e3\u51b3QA\u4efb\u52a1\u4e2d\u77ed\u800c\u6a21\u7cca\u95ee\u9898\u7684\u5b9e\u4f53\u94fe\u63a5\u6311\u6218\u3002"}}
{"id": "2508.04415", "pdf": "https://arxiv.org/pdf/2508.04415", "abs": "https://arxiv.org/abs/2508.04415", "authors": ["Xuan Chen", "Yu Huang", "Miaowen Wen", "Shahid Mumtaz", "Fatih Gulec", "Anwer Al-Dulaimi", "Andrew W. Eckford"], "title": "Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection", "categories": ["cs.NI"], "comment": "Accepted for publication in IEEE Communications Magazine", "summary": "The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary\nhealthcare paradigm, shows promise for epidemic control. This paper explores\nthe potential of using molecular communication (MC) to address the challenges\nin constructing IoBNT for epidemic prevention, specifically focusing on\nmodeling viral transmission, detecting the virus/infected individuals, and\nidentifying virus mutations. First, the MC channels in macroscale and\nmicroscale scenarios are discussed to match viral transmission in both scales\nseparately. Besides, the detection methods for these two scales are also\nstudied, along with the localization mechanism designed for the virus/infected\nindividuals. Moreover, an identification strategy is proposed to determine\npotential virus mutations, which is validated through simulation using the\nORF3a protein as a benchmark. Finally, open research issues are discussed. In\nsummary, this paper aims to analyze viral transmission through MC and combat\nviral spread using signal processing techniques within MC.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u5b50\u901a\u4fe1\u5728\u751f\u7269\u7eb3\u7c73\u7269\u8054\u7f51\u4e2d\u5b9e\u73b0\u75c5\u6bd2\u4f20\u64ad\u5efa\u6a21\u3001\u68c0\u6d4b\u4e0e\u53d8\u5f02\u8bc6\u522b\u4ee5\u9632\u63a7\u75ab\u60c5\u3002", "motivation": "\u9274\u4e8e\u751f\u7269\u7eb3\u7c73\u7269\u8054\u7f51\uff08IoBNT\uff09\u5728\u6d41\u884c\u75c5\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u5206\u5b50\u901a\u4fe1\uff08MC\uff09\u89e3\u51b3\u6784\u5efaIoBNT\u5728\u75c5\u6bd2\u4f20\u64ad\u5efa\u6a21\u3001\u75c5\u6bd2/\u611f\u67d3\u8005\u68c0\u6d4b\u53ca\u75c5\u6bd2\u7a81\u53d8\u8bc6\u522b\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "1. \u8ba8\u8bba\u5b8f\u89c2\u548c\u5fae\u89c2\u5c3a\u5ea6\u4e0b\u7684\u5206\u5b50\u901a\u4fe1\u4fe1\u9053\uff0c\u4ee5\u5339\u914d\u75c5\u6bd2\u4f20\u64ad\u30022. \u7814\u7a76\u76f8\u5e94\u5c3a\u5ea6\u4e0b\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u75c5\u6bd2/\u611f\u67d3\u8005\u7684\u5b9a\u4f4d\u673a\u5236\u30023. \u63d0\u51fa\u75c5\u6bd2\u7a81\u53d8\u8bc6\u522b\u7b56\u7565\uff0c\u5e76\u4f7f\u7528ORF3a\u86cb\u767d\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u30024. \u5e94\u7528\u5206\u5b50\u901a\u4fe1\u4e2d\u7684\u4fe1\u53f7\u5904\u7406\u6280\u672f\u5bf9\u6297\u75c5\u6bd2\u4f20\u64ad\u3002", "result": "\u8bba\u6587\u63a2\u8ba8\u5e76\u63d0\u51fa\u4e86\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u901a\u8fc7\u5206\u5b50\u901a\u4fe1\u8fdb\u884c\u75c5\u6bd2\u4f20\u64ad\u5efa\u6a21\u3001\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u65b9\u6cd5\u3002\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u75c5\u6bd2\u7a81\u53d8\u8bc6\u522b\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u901a\u8fc7\u5206\u5b50\u901a\u4fe1\u8fdb\u884c\u75c5\u6bd2\u4f20\u64ad\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u5229\u7528\u5206\u5b50\u901a\u4fe1\u4e2d\u7684\u4fe1\u53f7\u5904\u7406\u6280\u672f\u6709\u6548\u5bf9\u6297\u75c5\u6bd2\u4f20\u64ad\uff0c\u4e3a\u57fa\u4e8eIoBNT\u7684\u75ab\u60c5\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u7406\u8bba\u548c\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2508.04025", "pdf": "https://arxiv.org/pdf/2508.04025", "abs": "https://arxiv.org/abs/2508.04025", "authors": ["Chao Hao", "Shuai Wang", "Kaiwen Zhou"], "title": "Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement", "categories": ["cs.AI"], "comment": null, "summary": "Graphical user interface (GUI) agents have shown promise in automating mobile\ntasks but still struggle with input redundancy and decision ambiguity. In this\npaper, we present \\textbf{RecAgent}, an uncertainty-aware agent that addresses\nthese issues through adaptive perception. We distinguish two types of\nuncertainty in GUI navigation: (1) perceptual uncertainty, caused by input\nredundancy and noise from comprehensive screen information, and (2) decision\nuncertainty, arising from ambiguous tasks and complex reasoning. To reduce\nperceptual uncertainty, RecAgent employs a component recommendation mechanism\nthat identifies and focuses on the most relevant UI elements. For decision\nuncertainty, it uses an interactive module to request user feedback in\nambiguous situations, enabling intent-aware decisions. These components are\nintegrated into a unified framework that proactively reduces input complexity\nand reacts to high-uncertainty cases via human-in-the-loop refinement.\nAdditionally, we propose a dataset called \\textbf{ComplexAction} to evaluate\nthe success rate of GUI agents in executing specified single-step actions\nwithin complex scenarios. Extensive experiments validate the effectiveness of\nour approach. The dataset and code will be available at\nhttps://github.com/Fanye12/RecAgent.", "AI": {"tldr": "RecAgent\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u8f93\u5165\u5197\u4f59\u548c\u51b3\u7b56\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u79fb\u52a8\u4efb\u52a1\u65f6\u9762\u4e34\u8f93\u5165\u5197\u4f59\u548c\u51b3\u7b56\u6a21\u7cca\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u95ee\u9898\u53ef\u5f52\u7ed3\u4e3a\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\uff1a\u7531\u5168\u9762\u5c4f\u5e55\u4fe1\u606f\u5e26\u6765\u7684\u8f93\u5165\u5197\u4f59\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u201c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u201d\uff0c\u4ee5\u53ca\u7531\u6a21\u7cca\u4efb\u52a1\u548c\u590d\u6742\u63a8\u7406\u5f15\u8d77\u7684\u201c\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u201d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4ee3\u7406RecAgent\u3002\u9488\u5bf9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0cRecAgent\u91c7\u7528\u7ec4\u4ef6\u63a8\u8350\u673a\u5236\u6765\u8bc6\u522b\u5e76\u5173\u6ce8\u6700\u76f8\u5173\u7684UI\u5143\u7d20\uff0c\u4ee5\u51cf\u5c11\u8f93\u5165\u590d\u6742\u6027\u3002\u9488\u5bf9\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u5b83\u4f7f\u7528\u4e00\u4e2a\u4ea4\u4e92\u6a21\u5757\u5728\u6a21\u7cca\u60c5\u5883\u4e0b\u8bf7\u6c42\u7528\u6237\u53cd\u9988\uff0c\u4ee5\u5b9e\u73b0\u610f\u56fe\u611f\u77e5\u7684\u51b3\u7b56\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u88ab\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u4e3b\u52a8\u964d\u4f4e\u8f93\u5165\u590d\u6742\u6027\u5e76\u5e94\u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aComplexAction\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30GUI\u4ee3\u7406\u5728\u590d\u6742\u573a\u666f\u4e0b\u6267\u884c\u6307\u5b9a\u5355\u6b65\u52a8\u4f5c\u7684\u6210\u529f\u7387\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "RecAgent\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u673a\u5236\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u79fb\u52a8GUI\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u548c\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u7684\u6210\u529f\u7387\u548c\u4ee3\u7406\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.03766", "pdf": "https://arxiv.org/pdf/2508.03766", "abs": "https://arxiv.org/abs/2508.03766", "authors": ["Yongchao Huang"], "title": "LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation", "categories": ["cs.LG"], "comment": null, "summary": "The specification of prior distributions is fundamental in Bayesian\ninference, yet it remains a significant bottleneck. The prior elicitation\nprocess is often a manual, subjective, and unscalable task. We propose a novel\nframework which leverages Large Language Models (LLMs) to automate and scale\nthis process. We introduce \\texttt{LLMPrior}, a principled operator that\ntranslates rich, unstructured contexts such as natural language descriptions,\ndata or figures into valid, tractable probability distributions. We formalize\nthis operator by architecturally coupling an LLM with an explicit, tractable\ngenerative model, such as a Gaussian Mixture Model (forming a LLM based Mixture\nDensity Network), ensuring the resulting prior satisfies essential mathematical\nproperties. We further extend this framework to multi-agent systems where\nLogarithmic Opinion Pooling is employed to aggregate prior distributions\ninduced by decentralized knowledge. We present the federated prior aggregation\nalgorithm, \\texttt{Fed-LLMPrior}, for aggregating distributed,\ncontext-dependent priors in a manner robust to agent heterogeneity. This work\nprovides the foundation for a new class of tools that can potentially lower the\nbarrier to entry for sophisticated Bayesian modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u8d1d\u53f6\u65af\u63a8\u65ad\u4e2d\u5148\u9a8c\u5206\u5e03\u6307\u5b9a\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8017\u65f6\u3001\u4e3b\u89c2\u548c\u4e0d\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u8d1d\u53f6\u65af\u63a8\u65ad\u4e2d\u5148\u9a8c\u5206\u5e03\u7684\u6307\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u5148\u9a8c\u83b7\u53d6\u8fc7\u7a0b\u901a\u5e38\u662f\u624b\u52a8\u3001\u4e3b\u89c2\u4e14\u96be\u4ee5\u6269\u5c55\u7684\uff0c\u6784\u6210\u4e86\u4e00\u4e2a\u663e\u8457\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa`LLMPrior`\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06LLM\u4e0e\u663e\u5f0f\u3001\u53ef\u5904\u7406\u7684\u751f\u6210\u6a21\u578b\uff08\u5982\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u5f62\u6210\u57fa\u4e8eLLM\u7684\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff09\u76f8\u7ed3\u5408\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff08\u5982\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u6570\u636e\u6216\u56fe\u8868\uff09\u8f6c\u5316\u4e3a\u6709\u6548\u4e14\u6613\u4e8e\u5904\u7406\u7684\u6982\u7387\u5206\u5e03\uff0c\u786e\u4fdd\u6570\u5b66\u5c5e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f15\u5165`Fed-LLMPrior`\u7b97\u6cd5\uff0c\u5229\u7528\u5bf9\u6570\u610f\u89c1\u6c60\u5316\u805a\u5408\u5206\u5e03\u5f0f\u3001\u60c5\u5883\u4f9d\u8d56\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5bf9\u667a\u80fd\u4f53\u5f02\u8d28\u6027\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u4e86`LLMPrior`\u548c`Fed-LLMPrior`\u6846\u67b6\uff0c\u5b83\u4eec\u80fd\u591f\u81ea\u52a8\u5316\u548c\u6269\u5c55\u8d1d\u53f6\u65af\u5148\u9a8c\u5206\u5e03\u7684\u6307\u5b9a\uff0c\u5e76\u80fd\u9c81\u68d2\u5730\u805a\u5408\u5206\u5e03\u5f0f\u5148\u9a8c\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u4e00\u7c7b\u65b0\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u5de5\u5177\u80fd\u591f\u964d\u4f4e\u590d\u6742\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4f7f\u5f97\u5148\u9a8c\u6307\u5b9a\u8fc7\u7a0b\u66f4\u52a0\u4fbf\u6377\u548c\u9ad8\u6548\u3002"}}
{"id": "2508.03727", "pdf": "https://arxiv.org/pdf/2508.03727", "abs": "https://arxiv.org/abs/2508.03727", "authors": ["Tai Hyoung Rhee", "Dong-guw Lee", "Ayoung Kim"], "title": "TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "summary": "Thermal infrared imaging exhibits considerable potentials for robotic\nperception tasks, especially in environments with poor visibility or\nchallenging lighting conditions. However, TIR images typically suffer from\nheavy non-uniform fixed-pattern noise, complicating tasks such as object\ndetection, localization, and mapping. To address this, we propose a\ndiffusion-based TIR image denoising framework leveraging latent-space\nrepresentations and wavelet-domain optimization. Utilizing a pretrained stable\ndiffusion model, our method fine-tunes the model via a novel loss function\ncombining latent-space and discrete wavelet transform (DWT) / dual-tree complex\nwavelet transform (DTCWT) losses. Additionally, we implement a cascaded\nrefinement stage to enhance fine details, ensuring high-fidelity denoising\nresults. Experiments on benchmark datasets demonstrate superior performance of\nour approach compared to state-of-the-art denoising methods. Furthermore, our\nmethod exhibits robust zero-shot generalization to diverse and challenging\nreal-world TIR datasets, underscoring its effectiveness for practical robotic\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684TIR\u56fe\u50cf\u53bb\u566a\u6846\u67b6\uff0c\u6709\u6548\u53bb\u9664\u566a\u58f0\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u90e8\u7f72\u3002", "motivation": "\u70ed\u7ea2\u5916\uff08TIR\uff09\u6210\u50cf\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u6f5c\u529b\u5de8\u5927\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u80fd\u89c1\u5ea6\u6216\u5149\u7167\u6311\u6218\u6027\u73af\u5883\u3002\u7136\u800c\uff0cTIR\u56fe\u50cf\u5e38\u53d7\u5230\u4e25\u91cd\u7684\u975e\u5747\u5300\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\u56f0\u6270\uff0c\u8fd9\u6781\u5927\u5730\u590d\u6742\u5316\u4e86\u76ee\u6807\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5efa\u56fe\u7b49\u4efb\u52a1\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684TIR\u56fe\u50cf\u53bb\u566a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u548c\u5c0f\u6ce2\u57df\u4f18\u5316\u3002\u5177\u4f53\u5730\uff0c\u901a\u8fc7\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u548c\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09/\u53cc\u6811\u590d\u5c0f\u6ce2\u53d8\u6362\uff08DTCWT\uff09\u635f\u5931\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u5bf9\u9884\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7ea7\u8054\u7ec6\u5316\u9636\u6bb5\u4ee5\u589e\u5f3a\u7cbe\u7ec6\u7ec6\u8282\uff0c\u786e\u4fdd\u9ad8\u4fdd\u771f\u53bb\u566a\u6548\u679c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u53bb\u566a\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5bf9\u591a\u6837\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754cTIR\u6570\u636e\u96c6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u53bb\u566a\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TIR\u56fe\u50cf\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u673a\u5668\u4eba\u90e8\u7f72\u3002"}}
{"id": "2508.03905", "pdf": "https://arxiv.org/pdf/2508.03905", "abs": "https://arxiv.org/abs/2508.03905", "authors": ["Haofei Yu", "Zhengyang Qi", "Yining Zhao", "Kolby Nottingham", "Keyang Xuan", "Bodhisattwa Prasad Majumder", "Hao Zhu", "Paul Pu Liang", "Jiaxuan You"], "title": "Sotopia-RL: Reward Design for Social Intelligence", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSotopia-RL\u6846\u67b6\uff0c\u901a\u8fc7\u8bdd\u8bed\u7ea7\u3001\u591a\u7ef4\u5ea6\u5956\u52b1\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u793e\u4f1a\u667a\u80fdLLM\u65f6\u9762\u4e34\u7684\u89c2\u6d4b\u4e0d\u5168\u548c\u591a\u7ef4\u5ea6\u95ee\u9898\uff0c\u5e76\u5728Sotopia\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u793e\u4ea4\u76ee\u6807\u5b8c\u6210\u5206\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u793e\u4f1a\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u90e8\u5206\u53ef\u89c2\u5bdf\u6027\uff0c\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u56f0\u96be\uff1b2) \u591a\u7ef4\u5ea6\u6027\uff0c\u4f7f\u5f97\u884c\u4e3a\u5bf9\u76ee\u6807\u8d21\u732e\u95f4\u63a5\u3002\u8fd9\u4e9b\u4f7f\u4f20\u7edf\u57fa\u4e8eMDP\u7684RL\u4f4e\u6548\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faSotopia-RL\u6846\u67b6\uff0c\u5c06\u7c97\u7c92\u5ea6\u7684\u56de\u5408\u7ea7\u53cd\u9988\u7ec6\u5316\u4e3a\u8bdd\u8bed\u7ea7\uff08utterance-level\uff09\u548c\u591a\u7ef4\u5ea6\uff08multi-dimensional\uff09\u5956\u52b1\u3002\u8bdd\u8bed\u7ea7\u4fe1\u7528\u5206\u914d\u7f13\u89e3\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\uff0c\u591a\u7ef4\u5ea6\u5956\u52b1\u6355\u6349\u793e\u4ea4\u4e92\u52a8\u4e30\u5bcc\u6027\u5e76\u51cf\u5c11\u5956\u52b1\u6b3a\u9a97\u3002", "result": "\u5728Sotopia\u793e\u4ea4\u5b66\u4e60\u73af\u5883\u4e2d\uff0cSotopia-RL\u5728Sotopia-hard\u4e0a\u53d6\u5f977.17\u5206\uff0c\u5728Sotopia-full\u4e0a\u53d6\u5f978.31\u5206\u7684SOTA\u793e\u4ea4\u76ee\u6807\u5b8c\u6210\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u8bdd\u8bed\u7ea7\u4fe1\u7528\u5206\u914d\u548c\u591a\u7ef4\u5ea6\u5956\u52b1\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Sotopia-RL\u901a\u8fc7\u521b\u65b0\u6027\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u6709\u6548\u514b\u670d\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u793e\u4f1a\u667a\u80fdLLM\u65f6\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u793e\u4ea4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.04526", "pdf": "https://arxiv.org/pdf/2508.04526", "abs": "https://arxiv.org/abs/2508.04526", "authors": ["Fannya R. Sandjaja", "Ayesha A. Majeed", "Abdullah Abdullah", "Gyan Wickremasinghe", "Karen Rafferty", "Vishal Sharma"], "title": "Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions", "categories": ["cs.NI", "cs.DC"], "comment": "10 pages, 5 Figures, 2 Tables", "summary": "Traditional security architectures are becoming more vulnerable to\ndistributed attacks due to significant dependence on trust. This will further\nescalate when implementing agentic AI within the systems, as more components\nmust be secured over a similar distributed space. These scenarios can be\nobserved in consumer technologies, such as the dense Internet of things (IoT).\nHere, zero-trust architecture (ZTA) can be seen as a potential solution, which\nrelies on a key principle of not giving users explicit trust, instead always\nverifying their privileges whenever a request is made. However, the overall\nsecurity in ZTA is managed through its policies, and unverified policies can\nlead to unauthorized access. Thus, this paper explores challenges and solutions\nfor ZTA policy design in the context of distributed networks, which is referred\nto as zero-trust distributed networks (ZTDN). This is followed by a case-study\non formal verification of policies using UPPAAL. Subsequently, the importance\nof accountability and responsibility in the system's security is discussed.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u96f6\u4fe1\u4efb\u67b6\u6784\uff08ZTA\uff09\u7b56\u7565\u7684\u8bbe\u8ba1\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7UPPAAL\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5f3a\u8c03\u7b56\u7565\u5b89\u5168\u6027\u548c\u7cfb\u7edf\u95ee\u8d23\u5236\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u67b6\u6784\u56e0\u5bf9\u4fe1\u4efb\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5728\u5206\u5e03\u5f0f\u653b\u51fb\u9762\u524d\u65e5\u76ca\u8106\u5f31\uff0c\u5c24\u5176\u5728\u6574\u5408\u667a\u80fdAI\u548c\u5bc6\u96c6\u7269\u8054\u7f51\uff08IoT\uff09\u7b49\u5206\u5e03\u5f0f\u7cfb\u7edf\u65f6\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002\u5c3d\u7ba1\u96f6\u4fe1\u4efb\u67b6\u6784\uff08ZTA\uff09\u88ab\u89c6\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u7b56\u7565\u82e5\u672a\u7ecf\u4e25\u683c\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u672a\u6388\u6743\u8bbf\u95ee\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u73af\u5883\u4e0bZTA\u7b56\u7565\u8bbe\u8ba1\uff08\u5373\u96f6\u4fe1\u4efb\u5206\u5e03\u5f0f\u7f51\u7edcZTDN\uff09\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u6df1\u5165\u63a2\u8ba8\u4e86\u96f6\u4fe1\u4efb\u5206\u5e03\u5f0f\u7f51\u7edc\uff08ZTDN\uff09\u4e2dZTA\u7b56\u7565\u8bbe\u8ba1\u9762\u4e34\u7684\u6311\u6218\u5e76\u63d0\u51fa\u76f8\u5e94\u89e3\u51b3\u65b9\u6848\u3002\u968f\u540e\uff0c\u901a\u8fc7\u4f7f\u7528UPPAAL\u5de5\u5177\u5bf9ZTA\u7b56\u7565\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u8fdb\u884c\u4e86\u5177\u4f53\u7684\u6848\u4f8b\u7814\u7a76\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u8ba8\u8bba\u4e86\u5728\u7cfb\u7edf\u5b89\u5168\u4e2d\u786e\u7acb\u95ee\u8d23\u5236\u548c\u8d23\u4efb\u7684\u91cd\u8981\u6027\u3002", "result": "\u672c\u6587\u901a\u8fc7\u5bf9\u96f6\u4fe1\u4efb\u5206\u5e03\u5f0f\u7f51\u7edc\uff08ZTDN\uff09\u4e2dZTA\u7b56\u7565\u8bbe\u8ba1\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u7684\u63a2\u7d22\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u6f5c\u5728\u65b9\u5411\u3002\u901a\u8fc7\u4f7f\u7528UPPAAL\u8fdb\u884c\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u9a8c\u8bc1ZTA\u7b56\u7565\u7684\u53ef\u884c\u6027\u4e0e\u6548\u7528\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u95ee\u8d23\u5236\u548c\u8d23\u4efb\u5728\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "\u5728\u65e5\u76ca\u590d\u6742\u7684\u5206\u5e03\u5f0f\u7f51\u7edc\u73af\u5883\u4e2d\uff0cZTA\u7b56\u7565\u7684\u4e25\u8c28\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u5bf9\u4e8e\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\uff08\u5982UPPAAL\uff09\u80fd\u6709\u6548\u63d0\u5347\u7b56\u7565\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\uff0c\u660e\u786e\u7684\u95ee\u8d23\u5236\u548c\u8d23\u4efb\u662f\u7ef4\u62a4\u7cfb\u7edf\u6574\u4f53\u5b89\u5168\u4e0d\u53ef\u6216\u7f3a\u7684\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2508.04037", "pdf": "https://arxiv.org/pdf/2508.04037", "abs": "https://arxiv.org/abs/2508.04037", "authors": ["Liang Tang", "Shuxian Li", "Yuhao Cheng", "Yukang Huo", "Zhepeng Wang", "Yiqiang Yan", "Kaer Huang", "Yanzhe Jing", "Tiaonan Duan"], "title": "SEA: Self-Evolution Agent with Step-wise Reward for Computer Use", "categories": ["cs.AI"], "comment": null, "summary": "Computer use agent is an emerging area in artificial intelligence that aims\nto operate the computers to achieve the user's tasks, which attracts a lot of\nattention from both industry and academia. However, the present agents'\nperformance is far from being used. In this paper, we propose the\nSelf-Evolution Agent (SEA) for computer use, and to develop this agent, we\npropose creative methods in data generation, reinforcement learning, and model\nenhancement. Specifically, we first propose an automatic pipeline to generate\nthe verifiable trajectory for training. And then, we propose efficient\nstep-wise reinforcement learning to alleviate the significant computational\nrequirements for long-horizon training. In the end, we propose the enhancement\nmethod to merge the grounding and planning ability into one model without any\nextra training. Accordingly, based on our proposed innovation of data\ngeneration, training strategy, and enhancement, we get the Selfevolution Agent\n(SEA) for computer use with only 7B parameters, which outperforms models with\nthe same number of parameters and has comparable performance to larger ones. We\nwill make the models' weight and related codes open-source in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSEA\u7684\u81ea\u8fdb\u5316\u8ba1\u7b97\u673a\u4f7f\u7528\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee57B\u53c2\u6570\u91cf\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u540c\u7b49\u53c2\u6570\u6a21\u578b\u5e76\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u667a\u80fd\u4f53\u6027\u80fd\u4e0d\u8db3\uff0c\u8fdc\u672a\u8fbe\u5230\u5b9e\u7528\u6c34\u5e73\u3002", "method": "1) \u63d0\u51fa\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u8f68\u8ff9\u6570\u636e\u7684\u7ba1\u9053\uff1b2) \u5f15\u5165\u9ad8\u6548\u7684\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b3) \u8bbe\u8ba1\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u878d\u5408\u57fa\u7840\u7406\u89e3\u548c\u89c4\u5212\u80fd\u529b\u3002", "result": "\u6240\u63d0\u51fa\u76847B\u53c2\u6570Self-Evolution Agent (SEA) \u667a\u80fd\u4f53\u6027\u80fd\u4f18\u4e8e\u76f8\u540c\u53c2\u6570\u91cf\u7684\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u751f\u6210\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u589e\u5f3a\u7684\u521b\u65b0\uff0cSEA\u667a\u80fd\u4f53\u5728\u8ba1\u7b97\u673a\u4f7f\u7528\u4efb\u52a1\u4e2d\u4ee5\u8f83\u5c0f\u7684\u53c2\u6570\u91cf\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2508.03768", "pdf": "https://arxiv.org/pdf/2508.03768", "abs": "https://arxiv.org/abs/2508.03768", "authors": ["Debamita Ghosh", "George K. Atia", "Yue Wang"], "title": "Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2508.02948; text overlap\n  with arXiv:2404.03578 by other authors", "summary": "Reinforcement learning (RL) faces significant challenges in real-world\ndeployments due to the sim-to-real gap, where policies trained in simulators\noften underperform in practice due to mismatches between training and\ndeployment conditions. Distributionally robust RL addresses this issue by\noptimizing worst-case performance over an uncertainty set of environments and\nproviding an optimized lower bound on deployment performance. However, existing\nstudies typically assume access to either a generative model or offline\ndatasets with broad coverage of the deployment environment -- assumptions that\nlimit their practicality in unknown environments without prior knowledge. In\nthis work, we study the more realistic and challenging setting of online\ndistributionally robust RL, where the agent interacts only with a single\nunknown training environment while aiming to optimize its worst-case\nperformance. We focus on general $f$-divergence-based uncertainty sets,\nincluding Chi-Square and KL divergence balls, and propose a computationally\nefficient algorithm with sublinear regret guarantees under minimal assumptions.\nFurthermore, we establish a minimax lower bound on regret of online learning,\ndemonstrating the near-optimality of our approach. Extensive experiments across\ndiverse environments further confirm the robustness and efficiency of our\nalgorithm, validating our theoretical findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\uff08DR-RL\uff09\u7b97\u6cd5\uff0c\u65e8\u5728\u5355\u4e00\u672a\u77e5\u8bad\u7ec3\u73af\u5883\u4e2d\u4f18\u5316\u6700\u5dee\u6027\u80fd\uff0c\u89e3\u51b3\u4f20\u7edfDR-RL\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u73af\u5883\u4e0b\u5e94\u7528\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u201c\u6a21\u62df-\u73b0\u5b9e\u201d\u5dee\u8ddd\uff0c\u800c\u73b0\u6709\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u53ef\u8bbf\u95ee\u751f\u6210\u6a21\u578b\u6216\u5e7f\u6cdb\u8986\u76d6\u7684\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u8fd9\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u672a\u77e5\u73af\u5883\u4e2d\u662f\u4e0d\u5207\u5b9e\u9645\u7684\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u5728\u7ebf\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u4ec5\u4e0e\u4e00\u4e2a\u672a\u77e5\u8bad\u7ec3\u73af\u5883\u4ea4\u4e92\uff0c\u76ee\u6807\u662f\u4f18\u5316\u5176\u6700\u5dee\u6027\u80fd\u3002\u9488\u5bf9\u57fa\u4e8ef-\u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\uff08\u5982\u5361\u65b9\u548cKL\u6563\u5ea6\u7403\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6b21\u7ebf\u6027\u9057\u61be\uff08regret\uff09\u4fdd\u8bc1\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\uff0c\u4e14\u5728\u7406\u8bba\u4e0a\u901a\u8fc7\u5efa\u7acb\u5728\u7ebf\u5b66\u4e60\u7684\u6700\u5c0f\u6700\u5927\u9057\u61be\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u63a5\u8fd1\u6700\u4f18\u6027\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u5728\u672a\u77e5\u5728\u7ebf\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u7684\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u7528\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5f97\u5230\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u7684\u53cc\u91cd\u9a8c\u8bc1\u3002"}}
{"id": "2508.03732", "pdf": "https://arxiv.org/pdf/2508.03732", "abs": "https://arxiv.org/abs/2508.03732", "authors": ["Kushal Kanwar", "Dushyant Singh Chauhan", "Gopendra Vikram Singh", "Asif Ekbal"], "title": "What is Beneath Misogyny: Misogynous Memes Classification and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Memes are popular in the modern world and are distributed primarily for\nentertainment. However, harmful ideologies such as misogyny can be propagated\nthrough innocent-looking memes. The detection and understanding of why a meme\nis misogynous is a research challenge due to its multimodal nature (image and\ntext) and its nuanced manifestations across different societal contexts. We\nintroduce a novel multimodal approach, \\textit{namely},\n\\textit{\\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic\ncontent in memes. \\textit{\\textbf{MM-Misogyny}} processes text and image\nmodalities separately and unifies them into a multimodal context through a\ncross-attention mechanism. The resulting multimodal context is then easily\nprocessed for labeling, categorization, and explanation via a classifier and\nLarge Language Model (LLM). The evaluation of the proposed model is performed\non a newly curated dataset (\\textit{\\textbf{W}hat's \\textbf{B}eneath\n\\textbf{M}isogynous \\textbf{S}tereotyping (WBMS)}) created by collecting\nmisogynous memes from cyberspace and categorizing them into four categories,\n\\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only\ndetects and classifies misogyny, but also provides a granular understanding of\nhow misogyny operates in domains of life. The results demonstrate the\nsuperiority of our approach compared to existing methods. The code and dataset\nare available at\n\\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMM-Misogyny\uff0c\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u5206\u7c7b\u5e76\u89e3\u91ca\u7f51\u7edc\u8ff7\u56e0\u4e2d\u7684\u538c\u5973\u5185\u5bb9\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u7f51\u7edc\u8ff7\u56e0\u867d\u4e3b\u8981\u7528\u4e8e\u5a31\u4e50\uff0c\u4f46\u53ef\u80fd\u4f20\u64ad\u538c\u5973\u7b49\u6709\u5bb3\u610f\u8bc6\u5f62\u6001\u3002\u7531\u4e8e\u8ff7\u56e0\u7684\u591a\u6a21\u6001\u7279\u6027\uff08\u56fe\u50cf\u548c\u6587\u672c\uff09\u53ca\u5176\u5728\u4e0d\u540c\u793e\u4f1a\u80cc\u666f\u4e0b\u7684\u7ec6\u5fae\u8868\u73b0\uff0c\u68c0\u6d4b\u548c\u7406\u89e3\u8ff7\u56e0\u4e3a\u4f55\u5177\u6709\u538c\u5973\u6027\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u540d\u4e3aMM-Misogyny\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u89e3\u91ca\u8ff7\u56e0\u4e2d\u7684\u538c\u5973\u5185\u5bb9\u3002\u8be5\u65b9\u6cd5\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5176\u6574\u5408\u4e3a\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u3002\u968f\u540e\uff0c\u5229\u7528\u5206\u7c7b\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u8be5\u4e0a\u4e0b\u6587\u8fdb\u884c\u6807\u7b7e\u3001\u5206\u7c7b\u548c\u89e3\u91ca\u3002\u6a21\u578b\u5728\u65b0\u6784\u5efa\u7684WBMS\uff08What's Beneath Misogynous Stereotyping\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4ece\u7f51\u7edc\u6536\u96c6\u5e76\u5206\u4e3a\u53a8\u623f\u3001\u9886\u5bfc\u529b\u3001\u5de5\u4f5c\u548c\u8d2d\u7269\u56db\u7c7b\u7684\u538c\u5973\u8ff7\u56e0\u3002", "result": "MM-Misogyny\u6a21\u578b\u4e0d\u4ec5\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u5206\u7c7b\u538c\u5973\u5185\u5bb9\uff0c\u8fd8\u80fd\u63d0\u4f9b\u5173\u4e8e\u538c\u5973\u75c7\u5982\u4f55\u5728\u4e0d\u540c\u751f\u6d3b\u9886\u57df\u8fd0\u4f5c\u7684\u7ec6\u81f4\u7406\u89e3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684MM-Misogyny\u6a21\u578b\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u5bf9\u7f51\u7edc\u8ff7\u56e0\u4e2d\u538c\u5973\u5185\u5bb9\u7684\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u89e3\u91ca\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u4f9b\u538c\u5973\u75c7\u5728\u7279\u5b9a\u751f\u6d3b\u9886\u57df\u4e2d\u8868\u73b0\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4e3a\u8bc6\u522b\u548c\u7406\u89e3\u6709\u5bb3\u610f\u8bc6\u5f62\u6001\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.03923", "pdf": "https://arxiv.org/pdf/2508.03923", "abs": "https://arxiv.org/abs/2508.03923", "authors": ["Linxin Song", "Yutong Dai", "Viraj Prabhu", "Jieyu Zhang", "Taiwei Shi", "Li Li", "Junnan Li", "Silvio Savarese", "Zeyuan Chen", "Jieyu Zhao", "Ran Xu", "Caiming Xiong"], "title": "CoAct-1: Computer-using Agents with Coding as Actions", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation.", "AI": {"tldr": "CoAct-1\u662f\u4e00\u4e2a\u7ed3\u5408GUI\u64cd\u4f5c\u548c\u7f16\u7a0b\u6267\u884c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5141\u8bb8\u4ee3\u7406\u7f16\u5199\u548c\u6267\u884c\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u81ea\u52a8\u5316\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u64cd\u4f5c\u8ba1\u7b97\u673a\u7684\u81ea\u4e3b\u4ee3\u7406\u5728\u590d\u6742\u3001\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ec5\u4f9d\u8d56GUI\u64cd\u4f5c\uff0c\u5bfc\u81f4\u8106\u5f31\u6027\u548c\u4f4e\u6548\u3002", "method": "\u63d0\u51faCoAct-1\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5b83\u5c06GUI\u63a7\u5236\u4e0e\u76f4\u63a5\u7a0b\u5e8f\u5316\u6267\u884c\uff08\u7f16\u7a0b\uff09\u7ed3\u5408\u3002\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u534f\u8c03\u5668\uff0c\u52a8\u6001\u5730\u5c06\u5b50\u4efb\u52a1\u5206\u914d\u7ed9\u4f20\u7edf\u7684GUI\u64cd\u4f5c\u5458\u6216\u4e13\u95e8\u7684\u7f16\u7a0b\u4ee3\u7406\uff08\u53ef\u7f16\u5199\u548c\u6267\u884cPython/Bash\u811a\u672c\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u6df7\u5408\u64cd\u4f5c\u4ee5\u7ed5\u8fc7\u4f4e\u6548\u7684GUI\u5e8f\u5217\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoAct-1\u8fbe\u5230\u4e8660.76%\u7684SOTA\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5b83\u5c06\u5b8c\u6210\u4efb\u52a1\u6240\u9700\u7684\u5e73\u5747\u6b65\u9aa4\u6570\u4ece\u9886\u5148GUI\u4ee3\u7406\u768415\u6b65\u51cf\u5c11\u5230\u4ec510.15\u6b65\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "\u5c06\u7f16\u7a0b\u4f5c\u4e3a\u6838\u5fc3\u884c\u52a8\u6574\u5408\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u8ba1\u7b97\u673a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u5f3a\u5927\u3001\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2508.04556", "pdf": "https://arxiv.org/pdf/2508.04556", "abs": "https://arxiv.org/abs/2508.04556", "authors": ["Filipe B. Teixeira", "Carolina Sim\u00f5es", "Paulo Fidalgo", "Wagner Pedrosa", "Andr\u00e9 Coelho", "Manuel Ricardo", "Luis M. Pessoa"], "title": "CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps", "categories": ["cs.NI", "cs.CV"], "comment": "7 pages, 5 figures", "summary": "Telecommunications and computer vision have evolved independently. With the\nemergence of high-frequency wireless links operating mostly in line-of-sight,\nvisual data can help predict the channel dynamics by detecting obstacles and\nhelp overcoming them through beamforming or handover techniques.\n  This paper proposes a novel architecture for delivering real-time radio and\nvideo sensing information to O-RAN xApps through a multi-agent approach, and\nintroduces a new video function capable of generating blockage information for\nxApps, enabling Integrated Sensing and Communications. Experimental results\nshow that the delay of sensing information remains under 1\\,ms and that an xApp\ncan successfully use radio and video sensing information to control the 5G/6G\nRAN in real-time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5c06\u5b9e\u65f6\u65e0\u7ebf\u7535\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\u4f20\u9012\u7ed9O-RAN xApps\uff0c\u5b9e\u73b0\u4e00\u4f53\u5316\u611f\u77e5\u4e0e\u901a\u4fe1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4f4e\u5ef6\u8fdf\u548c\u5b9e\u65f6\u63a7\u52365G/6G RAN\u7684\u80fd\u529b\u3002", "motivation": "\u7535\u4fe1\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u72ec\u7acb\u53d1\u5c55\uff0c\u4f46\u9ad8\u9891\u65e0\u7ebf\u94fe\u8def\u591a\u4e3a\u89c6\u8ddd\u64cd\u4f5c\uff0c\u6613\u53d7\u969c\u788d\u7269\u5f71\u54cd\u3002\u89c6\u89c9\u6570\u636e\u53ef\u7528\u4e8e\u9884\u6d4b\u4fe1\u9053\u52a8\u6001\uff0c\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u6216\u5207\u6362\u6280\u672f\u514b\u670d\u969c\u788d\uff0c\u4ece\u800c\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u5c06\u5b9e\u65f6\u65e0\u7ebf\u7535\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\u4f20\u8f93\u7ed9O-RAN xApps\u3002\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u529f\u80fd\uff0c\u80fd\u591f\u4e3axApps\u751f\u6210\u963b\u585e\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u4f53\u5316\u611f\u77e5\u4e0e\u901a\u4fe1\uff08Integrated Sensing and Communications\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u611f\u77e5\u4fe1\u606f\u5ef6\u8fdf\u4fdd\u6301\u57281\u6beb\u79d2\u4ee5\u4e0b\u3002\u4e00\u4e2axApp\u80fd\u591f\u6210\u529f\u5229\u7528\u65e0\u7ebf\u7535\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\u5b9e\u65f6\u63a7\u52365G/6G\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06\u89c6\u89c9\u611f\u77e5\u4e0e\u65e0\u7ebf\u7535\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u4e3a5G/6G RAN\u63d0\u4f9b\u4e86\u5b9e\u65f6\u7684\u4fe1\u9053\u63a7\u5236\u80fd\u529b\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u7684\u611f\u77e5\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u901a\u4fe1\u7cfb\u7edf\u7684\u97e7\u6027\u4e0e\u6027\u80fd\u3002"}}
{"id": "2508.04070", "pdf": "https://arxiv.org/pdf/2508.04070", "abs": "https://arxiv.org/abs/2508.04070", "authors": ["Ronja Mehlan", "Claudia Hess", "Quintus Stierstorfer", "Kristina Schaaff"], "title": "Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence becomes increasingly integrated into digital\nlearning environments, the personalization of learning content to reflect\nlearners' individual career goals offers promising potential to enhance\nengagement and long-term motivation. In our study, we investigate how career\ngoal-based content adaptation in learning systems based on generative AI\n(GenAI) influences learner engagement, satisfaction, and study efficiency. The\nmixed-methods experiment involved more than 4,000 learners, with one group\nreceiving learning scenarios tailored to their career goals and a control\ngroup. Quantitative results show increased session duration, higher\nsatisfaction ratings, and a modest reduction in study duration compared to\nstandard content. Qualitative analysis highlights that learners found the\npersonalized material motivating and practical, enabling deep cognitive\nengagement and strong identification with the content. These findings\nunderscore the value of aligning educational content with learners' career\ngoals and suggest that scalable AI personalization can bridge academic\nknowledge and workplace applicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u3001\u57fa\u4e8e\u804c\u4e1a\u76ee\u6807\u7684\u5b66\u4e60\u5185\u5bb9\u4e2a\u6027\u5316\u5bf9\u5b66\u4e60\u8005\u53c2\u4e0e\u5ea6\u3001\u6ee1\u610f\u5ea6\u548c\u5b66\u4e60\u6548\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u878d\u5165\u6570\u5b57\u5b66\u4e60\uff0c\u6839\u636e\u5b66\u4e60\u8005\u4e2a\u4eba\u804c\u4e1a\u76ee\u6807\u5b9a\u5236\u5b66\u4e60\u5185\u5bb9\u6709\u671b\u63d0\u5347\u53c2\u4e0e\u5ea6\u548c\u957f\u671f\u52a8\u673a\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u79cd\u5b9a\u5236\u5316\u5e26\u6765\u7684\u5177\u4f53\u6548\u679c\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u62db\u52df\u4e864000\u591a\u540d\u5b66\u4e60\u8005\uff0c\u5206\u4e3a\u5b9e\u9a8c\u7ec4\uff08\u63a5\u53d7\u804c\u4e1a\u76ee\u6807\u5b9a\u5236\u7684\u5b66\u4e60\u573a\u666f\uff09\u548c\u5bf9\u7167\u7ec4\uff08\u63a5\u53d7\u6807\u51c6\u5185\u5bb9\uff09\u3002", "result": "\u5b9a\u91cf\u7ed3\u679c\u663e\u793a\uff0c\u5b9e\u9a8c\u7ec4\u7684\u5b66\u4e60\u4f1a\u8bdd\u65f6\u957f\u589e\u52a0\uff0c\u6ee1\u610f\u5ea6\u66f4\u9ad8\uff0c\u5b66\u4e60\u65f6\u957f\u9002\u5ea6\u7f29\u77ed\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u5b66\u4e60\u8005\u8ba4\u4e3a\u4e2a\u6027\u5316\u6750\u6599\u66f4\u5177\u542f\u53d1\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4fc3\u8fdb\u4e86\u6df1\u5ea6\u8ba4\u77e5\u53c2\u4e0e\u548c\u9ad8\u5ea6\u5185\u5bb9\u8ba4\u540c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06\u6559\u80b2\u5185\u5bb9\u4e0e\u5b66\u4e60\u8005\u804c\u4e1a\u76ee\u6807\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u53ef\u6269\u5c55\u7684AI\u4e2a\u6027\u5316\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u8fde\u63a5\u5b66\u672f\u77e5\u8bc6\u4e0e\u804c\u573a\u5e94\u7528\u3002"}}
{"id": "2508.03772", "pdf": "https://arxiv.org/pdf/2508.03772", "abs": "https://arxiv.org/abs/2508.03772", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGTPO\uff08Group-relative Trajectory-based Policy Optimization\uff09\u6765\u89e3\u51b3\u73b0\u6709GRPO\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u68af\u5ea6\u51b2\u7a81\u548c\u8d1f\u5956\u52b1\u5bfc\u81f4\u7684\u5b66\u4e60\u9000\u5316\u3002GTPO\u901a\u8fc7\u4fdd\u62a4\u51b2\u7a81\u4ee4\u724c\u5e76\u8fc7\u6ee4\u9ad8\u71b5\u5b8c\u6210\u6765\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e14\u65e0\u9700KL\u6563\u5ea6\u6b63\u5219\u5316\u548c\u53c2\u8003\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u5bf9\u9f50\u5e7f\u6cdb\u91c7\u7528\u57fa\u4e8e\u7b56\u7565\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5176\u4e2dGRPO\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0cGRPO\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1) \u4ee4\u724c\u5728\u5177\u6709\u6b63\u8d1f\u5956\u52b1\u7684\u5b8c\u6210\u4e2d\u540c\u65f6\u51fa\u73b0\uff0c\u5bfc\u81f4\u68af\u5ea6\u66f4\u65b0\u51b2\u7a81\uff0c\u53ef\u80fd\u964d\u4f4e\u5176\u8f93\u51fa\u6982\u7387\uff1b2) \u8d1f\u5956\u52b1\u7684\u5b8c\u6210\u53ef\u80fd\u60e9\u7f5a\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u54cd\u5e94\uff0c\u5bfc\u81f4\u6a21\u578b\u51b3\u7b56\u503e\u5411\u4e8e\u4e0d\u592a\u53ef\u80fd\u51fa\u73b0\u7684\u4ee4\u724c\uff0c\u8fdb\u800c\u4f7f\u8f93\u51fa\u5206\u5e03\u6241\u5e73\u5316\u5e76\u964d\u4f4e\u5b66\u4e60\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\u7b56\u7565\u3002", "method": "\u4e3a\u89e3\u51b3GRPO\u7684\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86GTPO\u3002GTPO\u8bc6\u522b\u201c\u51b2\u7a81\u4ee4\u724c\u201d\uff08\u5728\u4e0d\u540c\u5956\u52b1\u7684\u5b8c\u6210\u4e2d\u5904\u4e8e\u76f8\u540c\u4f4d\u7f6e\u7684\u4ee4\u724c\uff09\uff0c\u901a\u8fc7\u8df3\u8fc7\u8d1f\u66f4\u65b0\u5e76\u653e\u5927\u6b63\u66f4\u65b0\u6765\u4fdd\u62a4\u5b83\u4eec\u3002\u4e3a\u8fdb\u4e00\u6b65\u9632\u6b62\u7b56\u7565\u5d29\u6e83\uff0cGTPO\u8fc7\u6ee4\u6389\u71b5\u8d85\u8fc7\u53ef\u8bc1\u660e\u9608\u503c\u7684\u5b8c\u6210\u3002\u4e0eGRPO\u4e0d\u540c\uff0cGTPO\u4e0d\u4f9d\u8d56KL\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u53c2\u8003\u6a21\u578b\u7684\u9700\u8981\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGTPO\u5728\u63d0\u4f9b\u66f4\u5927\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u8fd9\u4e9b\u6548\u679c\u5728GSM8K\u3001MATH\u548cAIME 2024\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "GTPO\u901a\u8fc7\u6709\u6548\u89e3\u51b3GRPO\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u5b66\u4e60\u9000\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002\u5b83\u65e0\u9700\u53c2\u8003\u6a21\u578b\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.03735", "pdf": "https://arxiv.org/pdf/2508.03735", "abs": "https://arxiv.org/abs/2508.03735", "authors": ["Gopalji Gaur", "Mohammadreza Zolfaghari", "Thomas Brox"], "title": "StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 10 figures, GCPR", "summary": "Generating a coherent sequence of images that tells a visual story, using\ntext-to-image diffusion models, often faces the critical challenge of\nmaintaining subject consistency across all story scenes. Existing approaches,\nwhich typically rely on fine-tuning or retraining models, are computationally\nexpensive, time-consuming, and often interfere with the model's pre-existing\ncapabilities. In this paper, we follow a training-free approach and propose an\nefficient consistent-subject-generation method. This approach works seamlessly\nwith pre-trained diffusion models by introducing masked cross-image attention\nsharing to dynamically align subject features across a batch of images, and\nRegional Feature Harmonization to refine visually similar details for improved\nsubject consistency. Experimental results demonstrate that our approach\nsuccessfully generates visually consistent subjects across a variety of\nscenarios while maintaining the creative abilities of the diffusion model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u906e\u853d\u8de8\u56fe\u50cf\u6ce8\u610f\u529b\u5171\u4eab\u548c\u533a\u57df\u7279\u5f81\u534f\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u89c6\u89c9\u6545\u4e8b\u65f6\u96be\u4ee5\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u89c6\u89c9\u6545\u4e8b\u5e8f\u5217\u65f6\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u6216\u91cd\u8bad\u7ec3\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8017\u65f6\uff0c\u4e14\u5e38\u5e72\u6270\u6a21\u578b\u539f\u6709\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u4e3b\u4f53\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u906e\u853d\u8de8\u56fe\u50cf\u6ce8\u610f\u529b\u5171\u4eab\u201d\u52a8\u6001\u5bf9\u9f50\u6279\u5904\u7406\u56fe\u50cf\u4e2d\u7684\u4e3b\u4f53\u7279\u5f81\uff0c\u5e76\u5229\u7528\u201c\u533a\u57df\u7279\u5f81\u534f\u8c03\u201d\u4f18\u5316\u89c6\u89c9\u76f8\u4f3c\u7ec6\u8282\uff0c\u4ee5\u63d0\u9ad8\u4e3b\u4f53\u4e00\u81f4\u6027\u3002\u6b64\u65b9\u6cd5\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5728\u591a\u79cd\u573a\u666f\u4e0b\u751f\u6210\u4e86\u89c6\u89c9\u4e00\u81f4\u7684\u4e3b\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6269\u6563\u6a21\u578b\u7684\u521b\u9020\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u4e3b\u4f53\u4e00\u81f4\u6027\u96be\u9898\uff0c\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\uff0c\u6210\u529f\u751f\u6210\u4e86\u89c6\u89c9\u4e00\u81f4\u4e14\u4fdd\u6301\u6a21\u578b\u521b\u9020\u529b\u7684\u56fe\u50cf\u5e8f\u5217\u3002"}}
{"id": "2508.03935", "pdf": "https://arxiv.org/pdf/2508.03935", "abs": "https://arxiv.org/abs/2508.03935", "authors": ["Raymond Wilson", "Cole Graham", "Chase Carter", "Zefeng Yang", "Ruiqi Gu"], "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation", "categories": ["cs.CL"], "comment": null, "summary": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAP-LLM\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4e2a\u6027\u5316\u4e14\u4e8b\u5b9e\u4e00\u81f4\u65b0\u95fb\u6807\u9898\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u4fe1\u606f\u8fc7\u8f7d\u65f6\u4ee3\uff0c\u4e2a\u6027\u5316\u65b0\u95fb\u6807\u9898\u5bf9\u5438\u5f15\u7528\u6237\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u7528\u6237\u5174\u8da3\u5e76\u786e\u4fdd\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u5e38\u5bfc\u81f4\u6807\u9898\u8fc7\u4e8e\u7b3c\u7edf\u6216\u5177\u6709\u8bef\u5bfc\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faContext-Augmented Personalized LLM (CAP-LLM)\uff0c\u5c06\u7528\u6237\u504f\u597d\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u7ea6\u675f\u6574\u5408\u5230\u9884\u8bad\u7ec3LLM\u4e2d\u3002\u5b83\u5305\u542b\uff1a\u7528\u6237\u504f\u597d\u7f16\u7801\u5668\uff08\u6355\u6349\u957f\u671f\u5174\u8da3\uff09\u3001\u4e0a\u4e0b\u6587\u6ce8\u5165\u9002\u914d\u5668\uff08\u6574\u5408\u504f\u597d\u548c\u6587\u7ae0\u4e0a\u4e0b\u6587\uff09\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f3a\u5316\u6a21\u5757\uff08\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u51cf\u5c11\u5e7b\u89c9\uff09\u3002", "result": "\u5728PENS\u6570\u636e\u96c6\u4e0a\uff0cCAP-LLM\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\u3002\u76f8\u6bd4BART\u7b49\u57fa\u7ebf\uff0c\u5176\u4e8b\u5b9e\u4e00\u81f4\u6027\uff08FactCC 87.50\uff09\u663e\u8457\u63d0\u9ad8\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u4e2a\u6027\u5316\uff08Pc(avg) 2.73, Pc(max) 17.25\uff09\u548c\u5185\u5bb9\u8986\u76d6\uff08ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01\uff09\u3002\u6d88\u878d\u7814\u7a76\u3001\u4eba\u5de5\u8bc4\u4f30\u548c\u654f\u611f\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CAP-LLM\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u65b0\u95fb\u6807\u9898\u751f\u6210\u4e2d\u7684\u4e2a\u6027\u5316\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5177\u6709\u51fa\u8272\u7684\u8868\u73b0\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.03863", "pdf": "https://arxiv.org/pdf/2508.03863", "abs": "https://arxiv.org/abs/2508.03863", "authors": ["Amin Farajzadeh", "Hongzhao Zheng", "Sarah Dumoulin", "Trevor Ha", "Halim Yanikomeroglu", "Amir Ghasemi"], "title": "Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning", "categories": ["cs.LG", "cs.NI", "eess.SP"], "comment": "Accepted to be presented at IEEE PIMRC 2025", "summary": "Accurate spectrum demand prediction is crucial for informed spectrum\nallocation, effective regulatory planning, and fostering sustainable growth in\nmodern wireless communication networks. It supports governmental efforts,\nparticularly those led by the international telecommunication union (ITU), to\nestablish fair spectrum allocation policies, improve auction mechanisms, and\nmeet the requirements of emerging technologies such as advanced 5G, forthcoming\n6G, and the internet of things (IoT). This paper presents an effective\nspatio-temporal prediction framework that leverages crowdsourced user-side key\nperformance indicators (KPIs) and regulatory datasets to model and forecast\nspectrum demand. The proposed methodology achieves superior prediction accuracy\nand cross-regional generalizability by incorporating advanced feature\nengineering, comprehensive correlation analysis, and transfer learning\ntechniques. Unlike traditional ITU models, which are often constrained by\narbitrary inputs and unrealistic assumptions, this approach exploits granular,\ndata-driven insights to account for spatial and temporal variations in spectrum\nutilization. Comparative evaluations against ITU estimates, as the benchmark,\nunderscore our framework's capability to deliver more realistic and actionable\npredictions. Experimental results validate the efficacy of our methodology,\nhighlighting its potential as a robust approach for policymakers and regulatory\nbodies to enhance spectrum management and planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4f17\u5305KPI\u548c\u76d1\u7ba1\u6570\u636e\u7684\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u9891\u8c31\u9700\u6c42\u9884\u6d4b\uff0c\u4f18\u4e8e\u4f20\u7edfITU\u6a21\u578b\u3002", "motivation": "\u51c6\u786e\u7684\u9891\u8c31\u9700\u6c42\u9884\u6d4b\u5bf9\u9891\u8c31\u5206\u914d\u3001\u76d1\u7ba1\u89c4\u5212\u53ca\u652f\u63015G/6G/IoT\u7b49\u65b0\u5174\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfITU\u6a21\u578b\u5e38\u53d7\u9650\u4e8e\u4efb\u610f\u8f93\u5165\u548c\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6709\u6548\u7684\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u4f17\u5305\u7528\u6237\u4fa7\u5173\u952e\u6027\u80fd\u6307\u6807\uff08KPIs\uff09\u548c\u76d1\u7ba1\u6570\u636e\u96c6\u6765\u5efa\u6a21\u548c\u9884\u6d4b\u9891\u8c31\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9ad8\u7ea7\u7279\u5f81\u5de5\u7a0b\u3001\u7efc\u5408\u76f8\u5173\u6027\u5206\u6790\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u3002", "result": "\u4e0eITU\u4f30\u7b97\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u80fd\u63d0\u4f9b\u66f4\u73b0\u5b9e\u3001\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u9891\u8c31\u7ba1\u7406\u548c\u89c4\u5212\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u9891\u8c31\u7ba1\u7406\u6548\u7387\u3002"}}
{"id": "2508.04072", "pdf": "https://arxiv.org/pdf/2508.04072", "abs": "https://arxiv.org/abs/2508.04072", "authors": ["Xingyu Chen", "Junxiu An", "Jun Guo", "Li Wang", "Jingcai Guo"], "title": "KG-Augmented Executable CoT for Mathematical Coding", "categories": ["cs.AI"], "comment": "9 pages,2figures,6 tables", "summary": "In recent years, large language models (LLMs) have excelled in natural\nlanguage processing tasks but face significant challenges in complex reasoning\ntasks such as mathematical reasoning and code generation. To address these\nlimitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a\nnovel framework that enhances code generation through knowledge graphs and\nimproves mathematical reasoning via executable code. KGA-ECoT decomposes\nproblems into a Structured Task Graph, leverages efficient GraphRAG for precise\nknowledge retrieval from mathematical libraries, and generates verifiable code\nto ensure computational accuracy. Evaluations on multiple mathematical\nreasoning benchmarks demonstrate that KGA-ECoT significantly outperforms\nexisting prompting methods, achieving absolute accuracy improvements ranging\nfrom several to over ten percentage points. Further analysis confirms the\ncritical roles of GraphRAG in enhancing code quality and external code\nexecution in ensuring precision. These findings collectively establish KGA-ECoT\nas a robust and highly generalizable framework for complex mathematical\nreasoning tasks.", "AI": {"tldr": "KGA-ECoT\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86KG-Augmented Executable Chain-of-Thought (KGA-ECoT) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u4efb\u52a1\u56fe\uff0c\u5229\u7528GraphRAG\u4ece\u6570\u5b66\u5e93\u4e2d\u7cbe\u786e\u68c0\u7d22\u77e5\u8bc6\uff0c\u5e76\u751f\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u4ee5\u786e\u4fdd\u8ba1\u7b97\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5916\u90e8\u4ee3\u7801\u6267\u884c\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKGA-ECoT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\uff0c\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u6570\u4e2a\u5230\u5341\u591a\u4e2a\u767e\u5206\u70b9\u3002\u5206\u6790\u8bc1\u5b9eGraphRAG\u5728\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u548c\u5916\u90e8\u4ee3\u7801\u6267\u884c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "KGA-ECoT\u662f\u4e00\u4e2a\u9488\u5bf9\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u5f3a\u5927\u4e14\u9ad8\u5ea6\u901a\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2508.03774", "pdf": "https://arxiv.org/pdf/2508.03774", "abs": "https://arxiv.org/abs/2508.03774", "authors": ["Rui Zhu", "Yuexing Peng", "Peng Wang", "George C. Alexandropoulos", "Wenbo Wang", "Wei Xiang"], "title": "U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Electromagnetic (EM) scattering modeling is critical for radar remote\nsensing, however, its inherent complexity introduces significant computational\nchallenges. Traditional numerical solvers offer high accuracy, but suffer from\nscalability issues and substantial computational costs. Pure data-driven deep\nlearning approaches, while efficient, lack physical constraints embedding\nduring training and require extensive labeled data, limiting their\napplicability and generalization. To overcome these limitations, we propose a\nU-shaped Physics-Informed Network (U-PINet), the first fully\ndeep-learning-based, physics-informed hierarchical framework for computational\nEM designed to ensure physical consistency while maximizing computational\nefficiency. Motivated by the hierarchical decomposition strategy in EM solvers\nand the inherent sparsity of local EM coupling, the U-PINet models the\ndecomposition and coupling of near- and far-field interactions through a\nmultiscale processing neural network architecture, while employing a\nphysics-inspired sparse graph representation to efficiently model both self-\nand mutual- coupling among mesh elements of complex $3$-Dimensional (3D)\nobjects. This principled approach enables end-to-end multiscale EM scattering\nmodeling with improved efficiency, generalization, and physical consistency.\nExperimental results showcase that the U-PINet accurately predicts surface\ncurrent distributions, achieving close agreement with traditional solver, while\nsignificantly reducing computational time and outperforming conventional deep\nlearning baselines in both accuracy and robustness. Furthermore, our\nevaluations on radar cross section prediction tasks confirm the feasibility of\nthe U-PINet for downstream EM scattering applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faU-PINet\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u7406\u4fe1\u606f\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u78c1\u6563\u5c04\u5efa\u6a21\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548c\u591a\u5c3a\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u7269\u7406\u4e00\u81f4\u6027\u597d\u7684\u7aef\u5230\u7aef\u7535\u78c1\u6563\u5c04\u5efa\u6a21\uff0c\u4f18\u4e8e\u4f20\u7edf\u6c42\u89e3\u5668\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u7535\u78c1\u6563\u5c04\u5efa\u6a21\u5bf9\u96f7\u8fbe\u9065\u611f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u3002\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u51c6\u786e\u6027\u9ad8\u4f46\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u9ad8\u6548\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u4e14\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faU\u578b\u7269\u7406\u4fe1\u606f\u7f51\u7edc\uff08U-PINet\uff09\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u6df1\u5ea6\u5b66\u4e60\u3001\u7269\u7406\u4fe1\u606f\u5206\u5c42\u8ba1\u7b97\u7535\u78c1\u6846\u67b6\u3002\u53d7\u7535\u78c1\u6c42\u89e3\u5668\u4e2d\u5206\u5c42\u5206\u89e3\u7b56\u7565\u548c\u5c40\u90e8\u7535\u78c1\u8026\u5408\u56fa\u6709\u7684\u7a00\u758f\u6027\u542f\u53d1\uff0cU-PINet\u901a\u8fc7\u591a\u5c3a\u5ea6\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5bf9\u8fd1\u573a\u548c\u8fdc\u573a\u76f8\u4e92\u4f5c\u7528\u7684\u5206\u89e3\u4e0e\u8026\u5408\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u7269\u7406\u542f\u53d1\u7684\u7a00\u758f\u56fe\u8868\u793a\u6765\u9ad8\u6548\u5efa\u6a21\u590d\u67423D\u7269\u4f53\u7f51\u683c\u5355\u5143\u7684\u81ea\u8026\u5408\u548c\u4e92\u8026\u5408\u3002\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u591a\u5c3a\u5ea6\u7535\u78c1\u6563\u5c04\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cU-PINet\u80fd\u51c6\u786e\u9884\u6d4b\u8868\u9762\u7535\u6d41\u5206\u5e03\uff0c\u4e0e\u4f20\u7edf\u6c42\u89e3\u5668\u9ad8\u5ea6\u4e00\u81f4\uff0c\u540c\u65f6\u663e\u8457\u7f29\u77ed\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u5728\u96f7\u8fbe\u622a\u9762\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8bc1\u5b9e\u4e86U-PINet\u5728\u4e0b\u6e38\u7535\u78c1\u6563\u5c04\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "U-PINet\u4e3a\u7aef\u5230\u7aef\u591a\u5c3a\u5ea6\u7535\u78c1\u6563\u5c04\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\u3002"}}
{"id": "2508.03736", "pdf": "https://arxiv.org/pdf/2508.03736", "abs": "https://arxiv.org/abs/2508.03736", "authors": ["Rafayel Mkrtchyan", "Armen Manukyan", "Hrant Khachatrian", "Theofanis P. Raptis"], "title": "Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities", "categories": ["cs.CV", "cs.AI"], "comment": "Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")", "summary": "Environment mapping is an important computing task for a wide range of smart\ncity applications, including autonomous navigation, wireless network operations\nand extended reality environments. Conventional smart city mapping techniques,\nsuch as satellite imagery, LiDAR scans, and manual annotations, often suffer\nfrom limitations related to cost, accessibility and accuracy. Open-source\nmapping platforms have been widely utilized in artificial intelligence\napplications for environment mapping, serving as a source of ground truth.\nHowever, human errors and the evolving nature of real-world environments\nintroduce biases that can negatively impact the performance of neural networks\ntrained on such data. In this paper, we present a deep learning-based approach\nthat integrates the DINOv2 architecture to improve building mapping by\ncombining maps from open-source platforms with radio frequency (RF) data\ncollected from multiple wireless user equipments and base stations. Our\napproach leverages a vision transformer-based architecture to jointly process\nboth RF and map modalities within a unified framework, effectively capturing\nspatial dependencies and structural priors for enhanced mapping accuracy. For\nthe evaluation purposes, we employ a synthetic dataset co-produced by Huawei.\nWe develop and train a model that leverages only aggregated path loss\ninformation to tackle the mapping problem. We measure the results according to\nthree performance metrics which capture different qualities: (i) The Jaccard\nindex, also known as intersection over union (IoU), (ii) the Hausdorff\ndistance, and (iii) the Chamfer distance. Our design achieves a macro IoU of\n65.3%, significantly surpassing (i) the erroneous maps baseline, which yields\n40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and\n(iii) a non-AI fusion baseline that we designed which yields 42.2%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408DINOv2\u67b6\u6784\u3001\u5f00\u6e90\u5730\u56fe\u6570\u636e\u548c\u5c04\u9891\uff08RF\uff09\u6570\u636e\u6765\u63d0\u9ad8\u667a\u6167\u57ce\u5e02\u4e2d\u7684\u5efa\u7b51\u5236\u56fe\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u667a\u6167\u57ce\u5e02\u5e94\u7528\u4e2d\u73af\u5883\u5236\u56fe\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u536b\u661f\u56fe\u50cf\u3001LiDAR\u3001\u4eba\u5de5\u6807\u6ce8\uff09\u53d7\u9650\u4e8e\u6210\u672c\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u7cbe\u5ea6\u3002\u5f00\u6e90\u5730\u56fe\u5e73\u53f0\u867d\u7136\u88ab\u5e7f\u6cdb\u7528\u4e8eAI\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u4eba\u4e3a\u9519\u8bef\u548c\u73af\u5883\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u504f\u5dee\uff0c\u8d1f\u9762\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5236\u56fe\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86DINOv2\u67b6\u6784\u3002\u5b83\u7ed3\u5408\u4e86\u5f00\u6e90\u5730\u56fe\u6570\u636e\u4e0e\u4ece\u591a\u65e0\u7ebf\u7528\u6237\u8bbe\u5907\u548c\u57fa\u7ad9\u6536\u96c6\u7684\u5c04\u9891\uff08RF\uff09\u6570\u636e\uff0c\u5229\u7528\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u67b6\u6784\u5728\u7edf\u4e00\u6846\u67b6\u5185\u8054\u5408\u5904\u7406RF\u548c\u5730\u56fe\u6a21\u6001\u3002\u8be5\u6a21\u578b\u4ec5\u5229\u7528\u805a\u5408\u8def\u5f84\u635f\u8017\u4fe1\u606f\u8fdb\u884c\u5236\u56fe\u3002\u8bc4\u4f30\u4f7f\u7528\u534e\u4e3a\u5408\u4f5c\u751f\u4ea7\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528Jaccard\u6307\u6570\uff08IoU\uff09\u3001Hausdorff\u8ddd\u79bb\u548cChamfer\u8ddd\u79bb\u4f5c\u4e3a\u6027\u80fd\u6307\u6807\u3002", "result": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u5b9e\u73b0\u4e8665.3%\u7684\u5b8f\u89c2IoU\uff0c\u663e\u8457\u4f18\u4e8e\uff1a1) \u9519\u8bef\u5730\u56fe\u57fa\u7ebf\uff0840.1%\uff09\uff0c2) \u6587\u732e\u4e2d\u7684\u7eafRF\u65b9\u6cd5\uff0837.3%\uff09\uff0c\u4ee5\u53ca 3) \u7814\u7a76\u8005\u8bbe\u8ba1\u7684\u975eAI\u878d\u5408\u57fa\u7ebf\uff0842.2%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u6574\u5408DINOv2\u67b6\u6784\u3001RF\u6570\u636e\u548c\u5f00\u6e90\u5730\u56fe\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5efa\u7b51\u5236\u56fe\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86\u591a\u79cd\u73b0\u6709\u57fa\u7ebf\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.03970", "pdf": "https://arxiv.org/pdf/2508.03970", "abs": "https://arxiv.org/abs/2508.03970", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models", "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "comment": "Published in MIT Science Policy Review 6, 139-146 (2025)", "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u7cfb\u7edf\u5730\u6cbb\u7406\u3001\u8bc4\u4f30\u548c\u91cf\u5316\u504f\u5dee\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u6570\u636e\u548cAI\u6cbb\u7406\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u8d23\u4efb\u6027\u3002", "motivation": "\u8bc6\u522b\u5e76\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u504f\u5dee\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u8d23\u4efb\u611f\uff0c\u89c4\u907f\u6b67\u89c6\u98ce\u9669\u53ca\u6f5c\u5728\u58f0\u8a89\u635f\u5bb3\uff0c\u4ece\u800c\u4fc3\u8fdb\u521b\u5efa\u548c\u90e8\u7f72\u7b26\u5408\u793e\u4f1a\u8d23\u4efb\u548c\u9053\u5fb7\u6807\u51c6\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u504f\u5dee\u8bc4\u4f30\u548c\u6d4b\u8bd5\u5957\u4ef6\uff08BEATS\uff09\u7684\u57fa\u7840\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u548cAI\u6cbb\u7406\u6846\u67b6\u3002\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u80fd\u5bf9LLM\u8fdb\u884c\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u3001\u6301\u7eed\u5b9e\u65f6\u8bc4\u4f30\uff0c\u5e76\u4e3b\u52a8\u7ba1\u7406LLM\u751f\u6210\u54cd\u5e94\uff0c\u8986\u76d6\u4ece\u5f00\u53d1\u5230\u751f\u4ea7\u76d1\u63a7\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002", "result": "\u901a\u8fc7\u5b9e\u65bd\u6240\u8ba8\u8bba\u7684\u6570\u636e\u548cAI\u6cbb\u7406\u65b9\u6cd5\uff0c\u7ec4\u7ec7\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u8d23\u4efb\u6027\uff0c\u6709\u6548\u7f13\u89e3\u6b67\u89c6\u98ce\u9669\uff0c\u5e76\u4fdd\u62a4\u514d\u53d7\u6f5c\u5728\u7684\u58f0\u8a89\u6216\u54c1\u724c\u76f8\u5173\u635f\u5bb3\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5316\u5730\u6cbb\u7406\u548c\u8bc4\u4f30\u504f\u5dee\uff0c\u4e3a\u521b\u5efa\u548c\u90e8\u7f72\u5177\u6709\u793e\u4f1a\u8d23\u4efb\u611f\u548c\u9053\u5fb7\u4e00\u81f4\u6027\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2508.04536", "pdf": "https://arxiv.org/pdf/2508.04536", "abs": "https://arxiv.org/abs/2508.04536", "authors": ["Henrique Guerra", "Tailan S. Sarubi", "Rafael Chaves", "Jonas Maziero"], "title": "Entanglement distribution in quantum networks via swapping of partially entangled states", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "The entanglement swapping protocol (ESP) is a fundamental primitive for\ndistributing quantum correlations across distant nodes in a quantum network.\nRecent studies have demonstrated that even when the involved qubit pairs are\nonly partially entangled, it is still possible to concentrate and transmit\nentanglement via Bell-basis measurements. In this work, we extend these ideas\nto quantum networks with various topologies - including linear, star, and\nhybrid configurations - by analyzing the application of the ESP to initially\npartially entangled states. We investigate how entanglement evolves under such\nprotocols by examining the transformations of the initial states and evaluating\nthe success probabilities for generating maximally entangled states at the\noutput. Our results offer new insights into the dynamics of the entanglement\ndistribution in quantum networks and provide practical guidelines for designing\nrobust quantum communication strategies under realistic conditions.", "AI": {"tldr": "\u5206\u6790\u4e86\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\uff08ESP\uff09\u5728\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u7684\u91cf\u5b50\u7f51\u7edc\u4e2d\u5bf9\u521d\u59cb\u90e8\u5206\u7ea0\u7f20\u6001\u7684\u5e94\u7528\uff0c\u53ca\u5176\u7ea0\u7f20\u6f14\u5316\u548c\u6210\u529f\u6982\u7387\u3002", "motivation": "\u5c06\u73b0\u6709\u5173\u4e8e\u90e8\u5206\u7ea0\u7f20\u6001\u53ef\u901a\u8fc7\u8d1d\u5c14\u57fa\u6d4b\u91cf\u8fdb\u884c\u7ea0\u7f20\u96c6\u4e2d\u548c\u4f20\u8f93\u7684\u53d1\u73b0\uff0c\u6269\u5c55\u5230\u5177\u6709\u7ebf\u6027\u3001\u661f\u5f62\u548c\u6df7\u5408\u7b49\u591a\u79cd\u62d3\u6251\u7ed3\u6784\u7684\u91cf\u5b50\u7f51\u7edc\u4e2d\uff0c\u7814\u7a76\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\u5728\u521d\u59cb\u90e8\u5206\u7ea0\u7f20\u6001\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u5728\u5177\u6709\u7ebf\u6027\u3001\u661f\u5f62\u548c\u6df7\u5408\u62d3\u6251\u7684\u91cf\u5b50\u7f51\u7edc\u4e2d\uff0c\u5206\u6790\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\uff08ESP\uff09\u5bf9\u521d\u59cb\u90e8\u5206\u7ea0\u7f20\u6001\u7684\u5e94\u7528\u3002\u901a\u8fc7\u68c0\u67e5\u521d\u59cb\u6001\u7684\u53d8\u6362\uff0c\u5e76\u8bc4\u4f30\u8f93\u51fa\u751f\u6210\u6700\u5927\u7ea0\u7f20\u6001\u7684\u6210\u529f\u6982\u7387\u6765\u7814\u7a76\u7ea0\u7f20\u7684\u6f14\u5316\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5bf9\u91cf\u5b50\u7f51\u7edc\u4e2d\u7ea0\u7f20\u5206\u53d1\u52a8\u529b\u5b66\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "\u4e3a\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u8bbe\u8ba1\u7a33\u5065\u7684\u91cf\u5b50\u901a\u4fe1\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.04080", "pdf": "https://arxiv.org/pdf/2508.04080", "abs": "https://arxiv.org/abs/2508.04080", "authors": ["Jinfan Tang", "Kunming Wu", "Ruifeng Gongxie", "Yuya He", "Yuankai Wu"], "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement", "categories": ["cs.AI", "stat.OT"], "comment": "16 pages, 9 figures", "summary": "Recent studies have extended the application of large language models (LLMs)\nto geographic problems, revealing surprising geospatial competence even without\nexplicit spatial supervision. However, LLMs still face challenges in spatial\nconsistency, multi-hop reasoning, and geographic bias. To address these issues,\nwe propose GeoSR, a self-refining agentic reasoning framework that embeds core\ngeographic principles -- most notably Tobler's First Law of Geography -- into\nan iterative prediction loop. In GeoSR, the reasoning process is decomposed\ninto three collaborating agents: (1) a variable-selection agent that selects\nrelevant covariates from the same location; (2) a point-selection agent that\nchooses reference predictions at nearby locations generated by the LLM in\nprevious rounds; and (3) a refine agent that coordinates the iterative\nrefinement process by evaluating prediction quality and triggering further\nrounds when necessary. This agentic loop progressively improves prediction\nquality by leveraging both spatial dependencies and inter-variable\nrelationships. We validate GeoSR on tasks ranging from physical-world property\nestimation to socioeconomic prediction. Experimental results show consistent\nimprovements over standard prompting strategies, demonstrating that\nincorporating geostatistical priors and spatially structured reasoning into\nLLMs leads to more accurate and equitable geospatial predictions. The code of\nGeoSR is available at https://github.com/JinfanTang/GeoSR.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5730\u7406\u7a7a\u95f4\u95ee\u9898\u4e2d\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86GeoSR\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5730\u7406\u5b66\u539f\u7406\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u8fed\u4ee3\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u5730\u7406\u7a7a\u95f4\u9884\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u5730\u7406\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u4ecd\u9762\u4e34\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u591a\u8df3\u63a8\u7406\u548c\u5730\u7406\u504f\u5dee\u7b49\u65b9\u9762\u7684\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5730\u7406\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "GeoSR\u662f\u4e00\u4e2a\u81ea\u4fee\u6b63\u7684\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u5c06\u6838\u5fc3\u5730\u7406\u5b66\u539f\u7406\uff08\u5982Tobler\u7b2c\u4e00\u5b9a\u5f8b\uff09\u5d4c\u5165\u8fed\u4ee3\u9884\u6d4b\u5faa\u73af\u4e2d\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1a\u53d8\u91cf\u9009\u62e9\u667a\u80fd\u4f53\u3001\u70b9\u9009\u62e9\u667a\u80fd\u4f53\u548c\u7cbe\u70bc\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u53d8\u91cf\u95f4\u5173\u7cfb\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u63d0\u9ad8\u9884\u6d4b\u8d28\u91cf\u3002", "result": "GeoSR\u5728\u7269\u7406\u4e16\u754c\u5c5e\u6027\u4f30\u8ba1\u548c\u793e\u4f1a\u7ecf\u6d4e\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\uff0cGeoSR\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5c06\u5730\u7406\u7edf\u8ba1\u5148\u9a8c\u548c\u7a7a\u95f4\u7ed3\u6784\u5316\u63a8\u7406\u878d\u5165LLMs\u53ef\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u516c\u5e73\u7684\u5730\u7406\u7a7a\u95f4\u9884\u6d4b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5728LLMs\u4e2d\u878d\u5165\u5730\u7406\u7edf\u8ba1\u5148\u9a8c\u77e5\u8bc6\u548c\u7a7a\u95f4\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u5728\u5730\u7406\u7a7a\u95f4\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002GeoSR\u6846\u67b6\u4e3a\u89e3\u51b3LLMs\u5728\u5730\u7406\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u73b0\u6709\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.03776", "pdf": "https://arxiv.org/pdf/2508.03776", "abs": "https://arxiv.org/abs/2508.03776", "authors": ["Xiao Wang", "Zikang Yan", "Hao Si", "Zhendong Yang", "Qingquan Yang", "Dengdi Sun", "Wanli Lyu", "Jin Tang"], "title": "Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimating heat flux in the nuclear fusion device EAST is a critically\nimportant task. Traditional scientific computing methods typically model this\nprocess using the Finite Element Method (FEM). However, FEM relies on\ngrid-based sampling for computation, which is computationally inefficient and\nhard to perform real-time simulations during actual experiments. Inspired by\nartificial intelligence-powered scientific computing, this paper proposes a\nnovel Physics-Informed Neural Network (PINN) to address this challenge,\nsignificantly accelerating the heat conduction estimation process while\nmaintaining high accuracy. Specifically, given inputs of different materials,\nwe first feed spatial coordinates and time stamps into the neural network, and\ncompute boundary loss, initial condition loss, and physical loss based on the\nheat conduction equation. Additionally, we sample a small number of data points\nin a data-driven manner to better fit the specific heat conduction scenario,\nfurther enhancing the model's predictive capability. We conduct experiments\nunder both uniform and non-uniform heating conditions on the top surface.\nExperimental results show that the proposed thermal conduction physics-informed\nneural network achieves accuracy comparable to the finite element method, while\nachieving $\\times$40 times acceleration in computational efficiency. The\ndataset and source code will be released on\nhttps://github.com/Event-AHU/OpenFusion.", "AI": {"tldr": "\u9488\u5bf9EAST\u6838\u805a\u53d8\u88c5\u7f6e\u70ed\u901a\u91cf\u4f30\u8ba1\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6709\u9650\u5143\u6cd5\uff08FEM\uff09\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0eFEM\u76f8\u5f53\u7684\u7cbe\u5ea6\u548c40\u500d\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "motivation": "EAST\u6838\u805a\u53d8\u88c5\u7f6e\u7684\u70ed\u901a\u91cf\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfFEM\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6a21\u62df\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5b9e\u9a8c\u9700\u6c42\u3002", "method": "\u53d7AI\u9a71\u52a8\u79d1\u5b66\u8ba1\u7b97\u542f\u53d1\uff0c\u63d0\u51faPINN\u6a21\u578b\u3002\u6a21\u578b\u5c06\u7a7a\u95f4\u5750\u6807\u548c\u65f6\u95f4\u6233\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408\u70ed\u4f20\u5bfc\u65b9\u7a0b\u8ba1\u7b97\u8fb9\u754c\u635f\u5931\u3001\u521d\u59cb\u6761\u4ef6\u635f\u5931\u548c\u7269\u7406\u635f\u5931\u3002\u540c\u65f6\uff0c\u8f85\u4ee5\u5c11\u91cf\u6570\u636e\u9a71\u52a8\u91c7\u6837\u4ee5\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u52a0\u70ed\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u51fa\u7684PINN\u6a21\u578b\u5728\u4fdd\u6301\u4e0eFEM\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u4e8640\u500d\u3002", "conclusion": "PINN\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u89e3\u51b3\u6838\u805a\u53d8\u88c5\u7f6e\u7684\u70ed\u4f20\u5bfc\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u5b9e\u65f6\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.03740", "pdf": "https://arxiv.org/pdf/2508.03740", "abs": "https://arxiv.org/abs/2508.03740", "authors": ["Jianqiao Chen", "Tingting Zhu", "Huishi Song", "Nan Ma", "Xiaodong Xu"], "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Discretization of semantic features enables interoperability between semantic\nand digital communication systems, showing significant potential for practical\napplications. The fundamental difficulty in digitizing semantic features stems\nfrom the need to preserve continuity and context in inherently analog\nrepresentations during their compression into discrete symbols while ensuring\nrobustness to channel degradation. In this paper, we propose a vector quantized\n(VQ)-enabled digital semantic communication system with channel adaptive image\ntransmission, named VQ-DeepISC. Guided by deep joint source-channel coding\n(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic\nfeature extraction, followed by VQ modules projecting features into discrete\nlatent spaces. Consequently, it enables efficient index-based transmission\ninstead of raw feature transmission. To further optimize this process, we\ndevelop an attention mechanism-driven channel adaptation module to dynamically\noptimize index transmission. Secondly, to counteract codebook collapse during\ntraining process, we impose a distributional regularization by minimizing the\nKullback-Leibler divergence (KLD) between codeword usage frequencies and a\nuniform prior. Meanwhile, exponential moving average (EMA) is employed to\nstabilize training and ensure balanced feature coverage during codebook\nupdates. Finally, digital communication is implemented using quadrature phase\nshift keying (QPSK) modulation alongside orthogonal frequency division\nmultiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental\nresults demonstrate superior reconstruction fidelity of the proposed system\nover benchmark methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aVQ-DeepISC\u7684\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u6570\u5b57\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\uff08DJSCC\uff09\u548c\u4fe1\u9053\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u56fe\u50cf\u8bed\u4e49\u7279\u5f81\u4f20\u8f93\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "motivation": "\u8bed\u4e49\u7279\u5f81\u7684\u6570\u5b57\u5316\u5bf9\u4e8e\u5b9e\u73b0\u8bed\u4e49\u4e0e\u6570\u5b57\u901a\u4fe1\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5728\u5c06\u56fa\u6709\u7684\u6a21\u62df\u8868\u793a\u538b\u7f29\u4e3a\u79bb\u6563\u7b26\u53f7\u65f6\uff0c\u65e2\u80fd\u4fdd\u7559\u8fde\u7eed\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u53c8\u80fd\u786e\u4fdd\u5bf9\u4fe1\u9053\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86VQ-DeepISC\u7cfb\u7edf\uff0c\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728DJSCC\u7684\u6307\u5bfc\u4e0b\uff0c\u8bbe\u8ba1Swin Transformer\u9aa8\u5e72\u7f51\u7edc\u7528\u4e8e\u5206\u5c42\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408VQ\u6a21\u5757\u5c06\u7279\u5f81\u6295\u5f71\u5230\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u57fa\u4e8e\u7d22\u5f15\u7684\u4f20\u8f93\uff1b2) \u5f00\u53d1\u6ce8\u610f\u529b\u673a\u5236\u9a71\u52a8\u7684\u4fe1\u9053\u81ea\u9002\u5e94\u6a21\u5757\uff0c\u4ee5\u52a8\u6001\u4f18\u5316\u7d22\u5f15\u4f20\u8f93\uff1b3) \u91c7\u7528\u6700\u5c0f\u5316Kullback-Leibler\u6563\u5ea6\uff08KLD\uff09\u7684\u5206\u5e03\u6b63\u5219\u5316\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u6765\u9632\u6b62\u7801\u672c\u5d29\u6e83\uff0c\u7a33\u5b9a\u8bad\u7ec3\u5e76\u786e\u4fdd\u7801\u672c\u66f4\u65b0\u671f\u95f4\u7684\u7279\u5f81\u8986\u76d6\u5e73\u8861\uff1b4) \u91c7\u7528QPSK\u8c03\u5236\u548cOFDM\u6280\u672f\u5b9e\u73b0\u6570\u5b57\u901a\u4fe1\uff0c\u7b26\u5408IEEE 802.11a\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684VQ-DeepISC\u7cfb\u7edf\u5728\u56fe\u50cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "VQ-DeepISC\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u4e49\u7279\u5f81\u6570\u5b57\u5316\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684VQ\u3001DJSCC\u548c\u4fe1\u9053\u81ea\u9002\u5e94\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6570\u5b57\u8bed\u4e49\u901a\u4fe1\uff0c\u5176\u5353\u8d8a\u7684\u91cd\u5efa\u6027\u80fd\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.03979", "pdf": "https://arxiv.org/pdf/2508.03979", "abs": "https://arxiv.org/abs/2508.03979", "authors": ["Md Arafat Sultan", "Ram\u00f3n Fernandez Astudillo"], "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency", "categories": ["cs.CL"], "comment": null, "summary": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e9\u671f\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u8bcd\u6c47\u8986\u76d6\u7387\uff0c\u63d0\u5347\u81ea\u6d3d\u6027\u5728\u957f\u94fe\u5f0f\u601d\u7ef4\u4efb\u52a1\u4e2d\u7684\u4ee4\u724c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u5e76\u884c\u6027\u3002", "motivation": "\u81ea\u6d3d\u6027\u867d\u7136\u7b80\u5355\u6709\u6548\uff0c\u4f46\u5176\u9ad8\u4ee4\u724c\u6d88\u8017\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u5728\u957f\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u65e9\u671f\u5047\u8bbe\u526a\u679d\uff0c\u5728\u5e76\u884c\u751f\u6210\u6240\u6709\u89e3\u51b3\u65b9\u6848\u7684\u540c\u65f6\uff0c\u5b9a\u671f\u6839\u636e\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u4ee5\u53ca\u5019\u9009\u5b50\u96c6\u5bf9\u5f53\u524d\u6240\u6709\u5047\u8bbe\u7684\u8bcd\u6c47\u8986\u76d6\u7387\uff0c\u526a\u9664\u4e0d\u5fc5\u8981\u7684\u4e2d\u95f4\u5047\u8bbe\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5229\u7528\u8fd9\u4e24\u4e2a\u6307\u6807\u7684\u5feb\u901f\u52a0\u6743\u96c6\u5408\u8986\u76d6\u7b97\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u5bf9\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u6240\u6709\u6a21\u578b\u7684\u4ee4\u724c\u6548\u7387\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u63d0\u534710%-35%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e9\u671f\u526a\u679d\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u81ea\u6d3d\u6027\u7684\u4ee4\u724c\u6548\u7387\uff0c\u5e76\u4fdd\u6301\u5176\u5e76\u884c\u5904\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
