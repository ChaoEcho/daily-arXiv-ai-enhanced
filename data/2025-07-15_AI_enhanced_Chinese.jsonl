{"id": "2507.08865", "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "\u63d0\u51faSpatial ModernBERT\u6a21\u578b\uff0c\u7ed3\u5408\u7a7a\u95f4\u5d4c\u5165\u548c\u591a\u5934token\u5206\u7c7b\uff0c\u7528\u4e8e\u4ece\u91d1\u878d\u6587\u6863\u4e2d\u9ad8\u7cbe\u5ea6\u63d0\u53d6\u8868\u683c\u548c\u952e\u503c\u5bf9\u3002", "motivation": "\u4ece\u91d1\u878d\u6587\u6863\u4e2d\u63d0\u53d6\u8868\u683c\u548c\u952e\u503c\u5bf9\u5bf9\u4e8e\u5ba1\u8ba1\u3001\u6570\u636e\u5206\u6790\u548c\u81ea\u52a8\u5316\u53d1\u7968\u5904\u7406\u7b49\u4e1a\u52a1\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u57fa\u4e8eTransformer\u5e76\u96c6\u6210\u7a7a\u95f4\u5d4c\u5165\u7684Spatial ModernBERT\u6a21\u578b\u3002\u5c06\u63d0\u53d6\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e09\u5934\uff08\u6807\u7b7e\u3001\u5217\u3001\u884c\uff09\u7684token\u5206\u7c7b\u3002\u6a21\u578b\u5728PubTables-1M\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u91d1\u878d\u6587\u6863\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u3002\u91c7\u7528\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7B-I-IB\u6807\u8bb0\u5408\u5e76token\uff0c\u91cd\u5efa\u8868\u683c\u5e03\u5c40\u5e76\u63d0\u53d6\u952e\u503c\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSpatial ModernBERT\u80fd\u6709\u6548\u5229\u7528\u6587\u672c\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u91d1\u878d\u6587\u6863\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u8868\u683c\u53ca\u952e\u503c\u5bf9\u63d0\u53d6\u3002", "conclusion": "Spatial ModernBERT\u4e3a\u91d1\u878d\u6587\u6863\u4e2d\u590d\u6742\u8868\u683c\u548c\u952e\u503c\u5bf9\u7684\u63d0\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u76f8\u5173\u4e1a\u52a1\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEALGuard\uff0c\u4e00\u79cd\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u62a4\u680f\u5728\u5904\u7406\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u8f93\u5165\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u786e\u4fddLLM\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u62a4\u680f\uff08\u5982LlamaGuard\uff09\u5728\u5904\u7406\u82f1\u8bed\u4e0d\u5b89\u5168\u8f93\u5165\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8bed\u8a00\uff08\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e0d\u5b89\u5168\u8f93\u5165\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4LLM\u7cfb\u7edf\u6613\u53d7\u653b\u51fb\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u5c06\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\u6539\u7f16\u4e3a\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc726\u4e07\u6761\u63d0\u793a\uff08\u6db5\u76d6\u5b89\u5168\u3001\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u60c5\u666f\uff09\u7684SEALSBench\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLlamaGuard\u5728\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u63d0\u793a\u4e0a\u7684\u9632\u5fa1\u6210\u529f\u7387\uff08DSR\uff09\u5206\u522b\u4e0b\u964d9%\u548c18%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cSEALGuard\u5728\u68c0\u6d4b\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u63d0\u793a\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u62a4\u680f\uff0c\u5c06DSR\u63d0\u9ad8\u4e8648%\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u4f73\u7684DSR\u3001\u7cbe\u5ea6\u548cF1\u5206\u6570\u3002", "conclusion": "SEALGuard\u901a\u8fc7\u5f15\u5165\u6709\u6548\u7684\u591a\u8bed\u8a00\u62a4\u680f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7cfb\u7edf\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2507.08916", "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "categories": ["cs.CL"], "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u533b\u5b66\u95ee\u7b54\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u73b0\u6709\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u53d1\u73b0\u5176\u666e\u904d\u7f3a\u4e4f\u4e34\u5e8a\u771f\u5b9e\u6027\u4e0e\u900f\u660e\u5ea6\u3002\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u7684\u3001\u534f\u4f5c\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u66f4\u4e25\u8c28\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u95ee\u7b54\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5173\u6ce8\u7528\u4e8e\u5176\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u8d28\u91cf\u3002", "method": "\u5ba1\u67e5\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982MedQA\u3001MedMCQA\u3001PubMedQA\u3001MMLU\uff09\uff0c\u4ee5\u8bc4\u4f30\u5176\u4e25\u8c28\u6027\u3001\u900f\u660e\u5ea6\u548c\u4e0e\u4e34\u5e8a\u573a\u666f\u7684\u76f8\u5173\u6027\uff1b\u540c\u65f6\u5206\u6790\u4e86\u533b\u5b66\u671f\u520a\u4e2d\u7684\u6311\u6218\u95ee\u9898\u4f5c\u4e3a\u6f5c\u5728\u7684\u65e0\u504f\u8bc4\u4f30\u5de5\u5177\u3002", "result": "\u5927\u591a\u6570\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e34\u5e8a\u771f\u5b9e\u6027\u3001\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u7684\u9a8c\u8bc1\u8fc7\u7a0b\u3002\u516c\u5f00\u7684\u6311\u6218\u95ee\u9898\u867d\u6709\u76ca\u5904\uff0c\u4f46\u53d7\u9650\u4e8e\u89c4\u6a21\u5c0f\u3001\u8303\u56f4\u7a84\uff0c\u4e14\u53ef\u80fd\u5df2\u66b4\u9732\u4e8eLLM\u8bad\u7ec3\u4e2d\u3002\u8fd9\u4e9b\u4e0d\u8db3\u51f8\u663e\u4e86\u5bf9\u5b89\u5168\u3001\u5168\u9762\u548c\u6709\u4ee3\u8868\u6027\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "conclusion": "\u5efa\u7acb\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u533b\u5b66LLM\u8bc4\u4f30\u6846\u67b6\u81f3\u5173\u91cd\u8981\u3002\u673a\u6784\u548c\u653f\u7b56\u5236\u5b9a\u8005\u4e4b\u95f4\u9700\u8981\u534f\u4f5c\u52aa\u529b\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u662f\u4e25\u8c28\u3001\u65e0\u504f\u89c1\u5e76\u80fd\u53cd\u6620\u4e34\u5e8a\u590d\u6742\u6027\u3002"}}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u9488\u5bf9\u97e9\u56fd\u5de5\u4e1a\u9886\u57df\u7684\u4e13\u5bb6\u7ea7\u97e9\u8bedLLM\u8bc4\u4f30\u57fa\u51c6\uff1aKMMLU-Redux\uff08\u57fa\u4e8e\u6280\u672f\u8d44\u683c\u8003\u8bd5\uff09\u548cKMMLU-Pro\uff08\u57fa\u4e8e\u4e13\u4e1a\u8bb8\u53ef\u8003\u8bd5\uff09\uff0c\u65e8\u5728\u66f4\u6709\u6548\u5730\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u57fa\u51c6\u672a\u80fd\u5145\u5206\u6db5\u76d6\u5de5\u4e1a\u9886\u57df\uff0c\u5bfc\u81f4\u96be\u4ee5\u6709\u6548\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u3001\u5305\u542b\u5de5\u4e1a\u77e5\u8bc6\u7684\u57fa\u51c6\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u4e2a\u97e9\u56fd\u4e13\u5bb6\u7ea7\u8bc4\u4f30\u57fa\u51c6\uff1a1. KMMLU-Redux\uff1a\u5728\u73b0\u6709KMMLU\u57fa\u7840\u4e0a\u91cd\u5efa\uff0c\u5305\u542b\u97e9\u56fd\u56fd\u5bb6\u6280\u672f\u8d44\u683c\u8003\u8bd5\u9898\u76ee\uff0c\u5e76\u4fee\u6b63\u4e86\u5173\u952e\u9519\u8bef\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u30022. KMMLU-Pro\uff1a\u57fa\u4e8e\u97e9\u56fd\u56fd\u5bb6\u4e13\u4e1a\u8bb8\u53ef\u8003\u8bd5\uff0c\u65e8\u5728\u53cd\u6620\u97e9\u56fd\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b0\u57fa\u51c6\u80fd\u591f\u5168\u9762\u4ee3\u8868\u97e9\u56fd\u7684\u5de5\u4e1a\u77e5\u8bc6\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u4e24\u4e2a\u53ef\u9760\u7684\u97e9\u56fd\u4e13\u5bb6\u7ea7\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dLLM\u8bc4\u4f30\u5728\u5de5\u4e1a\u9886\u57df\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.08806", "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u5197\u4f59\uff08\u5229\u7528\u6ce8\u610f\u529b\u5206\u6570\u548c\u7ed3\u6784\u611f\u77e5\u526a\u679d\uff09\uff0c\u672c\u65b9\u6cd5\u5728\u4e0d\u7ecf\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\uff08\u7279\u522b\u662f\u6570\u5b66\u7ade\u8d5b\uff09\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u7bc7\u63a8\u7406\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\uff0c\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\uff0c\u5c24\u5176\u9519\u8bef\u7b54\u6848\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\u66f4\u9ad8\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u6027\u80fd\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6d88\u9664\u8fd9\u4e9b\u201c\u5e72\u6270\u201d\u6765\u63d0\u5347\u201c\u6e05\u6670\u601d\u8003\u201d\u80fd\u529b\u3002", "method": "1. **\u5197\u4f59\u8bc6\u522b:** \u901a\u8fc7\u8861\u91cftoken\u7ea7\u522b\u6ce8\u610f\u529b\u4e0e\u4e00\u4e2a\u9644\u52a0\u5728\u6bcf\u4e2a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u540e\u7684\u201c\u601d\u8003\u7ed3\u675f\u201d\u7279\u6b8atoken\u7684\u5173\u8054\u6765\u8bc6\u522b\u5197\u4f59\u30022. **\u7ed3\u6784\u611f\u77e5\u526a\u679d:** \u4f18\u5148\u79fb\u9664\u5bf9\u63a8\u7406\u8d21\u732e\u4f4e\u7684\u63a8\u7406\u5757\u4e2d\u7684token\u30023. **\u63a8\u7406\u6062\u590d:** \u79fb\u9664\u5197\u4f59token\u548c\u6ce8\u5165\u7684\u6307\u4ee4\u540e\uff0c\u7ee7\u7eed\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "1. \u5728\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u30022. \u5728AIME\u548cAMC\u7b49\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u4e2d\u63a8\u7406\u5197\u4f59\u66f4\u4e3a\u5e38\u89c1\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u79fb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u6e05\u9664\u5e72\u6270\u4ee5\u4fc3\u8fdb\u201c\u6e05\u6670\u601d\u8003\u201d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.08828", "pdf": "https://arxiv.org/pdf/2507.08828", "abs": "https://arxiv.org/abs/2507.08828", "authors": ["Tarek Berghout"], "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm\nthat advances beyond conventional Machine Learning (ML) and Deep Learning (DL).\nWhile DL focuses on learning from static data representations, RE proposes an\nadditional dimension: learning from the evolving behavior of models themselves.\nRE emphasizes multiple mappings of data through identical deep architectures\nand analyzes their internal representations (i.e., feature maps) in conjunction\nwith observed performance signals such as loss. By incorporating these\nbehavioral traces, RE enables iterative self-improvement, allowing each model\nversion to gain insight from its predecessors. The framework is extended\nthrough Multiverse RE (MVRE), which aggregates signals from parallel model\ninstances, and further through Heterogeneous MVRE (HMVRE), where models of\nvarying architectures contribute diverse perspectives. A scalable and adaptive\nvariant, Sc-HMVRE, introduces selective mechanisms and scale diversity for\nreal-world deployment. Altogether, RE presents a shift in DL: from purely\nrepresentational learning to behavior-aware, self-evolving systems. It lays the\ngroundwork for a new class of intelligent models capable of reasoning over\ntheir own learning dynamics, offering a path toward scalable, introspective,\nand adaptive artificial intelligence. A simple code example to support\nbeginners in running their own experiments is provided in Code Availability\nSection of this paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5faa\u73af\u6269\u5c55\uff08RE\uff09\u201d\u7684\u65b0\u5b66\u4e60\u8303\u5f0f\uff0c\u8d85\u8d8a\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u81ea\u8eab\u6f14\u5316\u884c\u4e3a\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aMVRE\u3001HMVRE\u53caSc-HMVRE\uff0c\u65e8\u5728\u6784\u5efa\u53ef\u81ea\u7701\u548c\u81ea\u9002\u5e94\u7684AI\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u4ece\u9759\u6001\u6570\u636e\u8868\u793a\u4e2d\u5b66\u4e60\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f15\u5165\u4e00\u4e2a\u65b0\u7ef4\u5ea6\uff0c\u5373\u4ece\u6a21\u578b\u81ea\u8eab\u7684\u6f14\u5316\u884c\u4e3a\u4e2d\u5b66\u4e60\uff0c\u4ee5\u5b9e\u73b0\u6a21\u578b\u8fed\u4ee3\u5f0f\u81ea\u6211\u63d0\u5347\uff0c\u5e76\u4e3a\u5f00\u53d1\u80fd\u591f\u53cd\u601d\u81ea\u8eab\u5b66\u4e60\u52a8\u6001\u7684\u3001\u53ef\u4f38\u7f29\u7684\u3001\u81ea\u7701\u7684\u3001\u81ea\u9002\u5e94\u7684\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u201c\u5faa\u73af\u6269\u5c55\uff08RE\uff09\u201d\u8303\u5f0f\uff0c\u901a\u8fc7\u540c\u4e00\u6df1\u5ea6\u67b6\u6784\u591a\u6b21\u6620\u5c04\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u5185\u90e8\u8868\u793a\uff08\u7279\u5f81\u56fe\uff09\u4e0e\u6027\u80fd\u4fe1\u53f7\uff08\u5982\u635f\u5931\uff09\u8fdb\u884c\u5206\u6790\uff0c\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u81ea\u6211\u6539\u8fdb\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u201c\u591a\u5b87\u5b99RE\uff08MVRE\uff09\u201d\u4ee5\u805a\u5408\u5e76\u884c\u6a21\u578b\u5b9e\u4f8b\u7684\u4fe1\u53f7\uff0c\u4ee5\u53ca\u201c\u5f02\u6784MVRE\uff08HMVRE\uff09\u201d\u4ee5\u6574\u5408\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u591a\u5143\u89c6\u89d2\u3002\u6700\u540e\uff0c\u5f15\u5165\u4e86\u201c\u53ef\u4f38\u7f29\u81ea\u9002\u5e94\u53d8\u4f53\uff08Sc-HMVRE\uff09\u201d\uff0c\u901a\u8fc7\u9009\u62e9\u673a\u5236\u548c\u5c3a\u5ea6\u591a\u6837\u6027\u5b9e\u73b0\u5b9e\u9645\u90e8\u7f72\u3002", "result": "RE\u8303\u5f0f\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u8fed\u4ee3\u5f0f\u81ea\u6211\u63d0\u5347\uff0c\u6bcf\u4e2a\u6a21\u578b\u7248\u672c\u90fd\u80fd\u4ece\u524d\u4ee3\u4e2d\u83b7\u53d6\u6d1e\u5bdf\u3002\u8fd9\u6807\u5fd7\u7740\u6df1\u5ea6\u5b66\u4e60\u4ece\u7eaf\u7cb9\u7684\u8868\u793a\u5b66\u4e60\u8f6c\u5411\u4e86\u884c\u4e3a\u611f\u77e5\u3001\u81ea\u6211\u6f14\u5316\u7684\u7cfb\u7edf\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u80fd\u591f\u63a8\u7406\u81ea\u8eab\u5b66\u4e60\u52a8\u6001\u7684\u65b0\u578b\u667a\u80fd\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u5b9e\u73b0\u53ef\u4f38\u7f29\u3001\u81ea\u7701\u3001\u81ea\u9002\u5e94\u4eba\u5de5\u667a\u80fd\u7684\u9014\u5f84\u3002", "conclusion": "\u5faa\u73af\u6269\u5c55\uff08RE\uff09\u4ee3\u8868\u4e86\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u4e00\u6b21\u91cd\u8981\u8303\u5f0f\u8f6c\u53d8\uff0c\u63a8\u52a8\u5176\u4ece\u5355\u7eaf\u7684\u8868\u793a\u5b66\u4e60\u8d70\u5411\u884c\u4e3a\u611f\u77e5\u548c\u81ea\u6211\u6f14\u5316\u7cfb\u7edf\u3002\u5b83\u4e3a\u5f00\u53d1\u80fd\u591f\u53cd\u601d\u81ea\u8eab\u5b66\u4e60\u52a8\u6001\u7684\u65b0\u578b\u667a\u80fd\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u6784\u5efa\u53ef\u4f38\u4f38\u3001\u81ea\u7701\u3001\u81ea\u9002\u5e94\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.08831", "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLNCE\uff09\u4e2d\u7b56\u7565\u5bf9\u89c6\u89d2\u53d8\u5316\u654f\u611f\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86VIL\uff08\u89c6\u89d2\u4e0d\u53d8\u5b66\u4e60\uff09\u540e\u8bad\u7ec3\u7b56\u7565\u3002VIL\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u7b56\u7565\u5bf9\u4e0d\u540c\u89c6\u89d2\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728V2-VLNCE\u548c\u6807\u51c6VLNCE\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5373\u63d2\u5373\u7528\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8fde\u7eed\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLNCE\uff09\u662f\u5177\u8eabAI\u7684\u5173\u952e\u95ee\u9898\uff0c\u7136\u800c\u5927\u591a\u6570\u5bfc\u822a\u7b56\u7565\u5bf9\u89c6\u70b9\u53d8\u5316\uff08\u5982\u76f8\u673a\u9ad8\u5ea6\u548c\u89c6\u89d2\uff09\u975e\u5e38\u654f\u611f\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u7684\u89c2\u5bdf\u7ed3\u679c\u6539\u53d8\uff0c\u8fdb\u800c\u5f71\u54cd\u5bfc\u822a\u6027\u80fd\u3002", "method": "1. \u5f15\u5165\u4e86\u5e7f\u4e49\u573a\u666fV2-VLNCE\uff08\u5e26\u6709\u4e0d\u540c\u89c6\u70b9\u7684VLNCE\uff09\u30022. \u63d0\u51fa\u4e86VIL\uff08\u89c6\u89d2\u4e0d\u53d8\u5b66\u4e60\uff09\u7b56\u7565\uff0c\u4f5c\u4e3a\u4e00\u79cd\u89c6\u89d2\u4e0d\u53d8\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u589e\u5f3a\u73b0\u6709\u5bfc\u822a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u30023. VIL\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60\u7a00\u758f\u4e14\u89c6\u89d2\u4e0d\u53d8\u7684\u7279\u5f81\u30024. \u4e3a\u8def\u5f84\u70b9\u9884\u6d4b\u5668\u6a21\u5757\u5f15\u5165\u4e86\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u5c06\u4f9d\u8d56\u89c6\u70b9\u7684\u6559\u5e08\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u5230\u89c6\u89d2\u4e0d\u53d8\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u30025. \u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u8303\u5f0f\uff0c\u5171\u540c\u4f18\u5316\u6240\u6709\u7ec4\u4ef6\u3002", "result": "1. \u5728V2-VLNCE\u573a\u666f\u4e0b\uff0c\u672c\u65b9\u6cd5\u5728R2R-CE\u548cRxR-CE\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e868-15%\u30022. \u5728\u6807\u51c6VLNCE\u8bbe\u7f6e\u4e0b\uff0c\u5373\u4f7f\u662f\u4e3a\u4e0d\u540c\u89c6\u70b9\u8bad\u7ec3\uff0cVIL\u4ecd\u80fd\u63d0\u5347\u6027\u80fd\u30023. \u5728\u66f4\u5177\u6311\u6218\u6027\u7684RxR-CE\u6570\u636e\u96c6\u4e0a\uff0c\u672c\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff08\u76f8\u8f83\u4e8e\u5176\u4ed6\u65e0\u5730\u56fe\u65b9\u6cd5\uff09\u3002", "conclusion": "VIL\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u89c6\u89d2\u654f\u611f\u6027\u95ee\u9898\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7b56\u7565\u5728\u591a\u53d8\u89c6\u89d2\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u5b83\u5728\u6807\u51c6\u89c6\u89d2\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709\u5bfc\u822a\u7b56\u7565\u8fdb\u884c\u6709\u6548\u6539\u8fdb\u3002"}}
{"id": "2507.09094", "pdf": "https://arxiv.org/pdf/2507.09094", "abs": "https://arxiv.org/abs/2507.09094", "authors": ["Xiaoren Xu", "Hao Xu", "Dongyu Wei", "Walid Saad", "Mehdi Bennis", "Mingzhe Chen"], "title": "Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "In this paper, a novel Three dimensional (3D) positioning framework of fluid\nantenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In\nthe proposed framework, a set of controlled UAVs cooperatively estimate the\nreal-time 3D position of a target UAV. Here, the active UAV transmits a\nmeasurement signal to the passive UAVs via the reflection from the target UAV.\nEach passive UAV estimates the distance of the active-target-passive UAV link\nand selects an antenna port to share the distance information with the base\nstation (BS) that calculates the real-time position of the target UAV. As the\ntarget UAV is moving due to its task operation, the controlled UAVs must\noptimize their trajectories and select optimal antenna port, aiming to estimate\nthe real-time position of the target UAV. We formulate this problem as an\noptimization problem to minimize the target UAV positioning error via\noptimizing the trajectories of all controlled UAVs and antenna port selection\nof passive UAVs. Here, an attention-based recurrent multi-agent reinforcement\nlearning (AR-MARL) scheme is proposed, which enables each controlled UAV to use\nthe local Q function to determine its trajectory and antenna port while\noptimizing the target UAV positioning performance without knowing the\ntrajectories and antenna port selections of other controlled UAVs. Different\nfrom current MARL methods, the proposed method uses a recurrent neural network\n(RNN) that incorporates historical state-action pairs of each controlled UAV,\nand an attention mechanism to analyze the importance of these historical\nstate-action pairs, thus improving the global Q function approximation accuracy\nand the target UAV positioning accuracy. Simulation results show that the\nproposed AR-MARL scheme can reduce the average positioning error by up to 17.5%\nand 58.5% compared to the VD-MARL scheme and the proposed method without FAS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf(FAS)\u7684\u65e0\u4eba\u673a3D\u5b9a\u4f4d\u6846\u67b6\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u5faa\u73af\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(AR-MARL)\u4f18\u5316\u534f\u4f5c\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u548c\u5929\u7ebf\u9009\u62e9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u79fb\u52a8\u76ee\u6807\u65e0\u4eba\u673a\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u9488\u5bf9\u8fd0\u52a8\u76ee\u6807\u65e0\u4eba\u673a\u7684\u5b9e\u65f63D\u5b9a\u4f4d\u6311\u6218\uff0c\u9700\u8981\u4f18\u5316\u534f\u4f5c\u65e0\u4eba\u673a\uff08\u4e3b\u52a8\u53ca\u88ab\u52a8\uff09\u7684\u8f68\u8ff9\u548c\u5929\u7ebf\u7aef\u53e3\u9009\u62e9\uff0c\u4ee5\u6700\u5c0f\u5316\u5b9a\u4f4d\u8bef\u5dee\u3002", "method": "\u5f00\u53d1\u4e86FAS\u8d4b\u80fd\u7684\u65e0\u4eba\u673a3D\u5b9a\u4f4d\u6846\u67b6\uff0c\u5176\u4e2d\u4e3b\u52a8\u65e0\u4eba\u673a\u901a\u8fc7\u76ee\u6807\u53cd\u5c04\u5411\u88ab\u52a8\u65e0\u4eba\u673a\u53d1\u9001\u6d4b\u91cf\u4fe1\u53f7\uff0c\u88ab\u52a8\u65e0\u4eba\u673a\u6d4b\u8ddd\u5e76\u5c06\u4fe1\u606f\u4f20\u81f3\u57fa\u7ad9\u8ba1\u7b97\u4f4d\u7f6e\u3002\u5c06\u8be5\u95ee\u9898\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5faa\u73af\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(AR-MARL)\u65b9\u6848\u3002\u8be5\u65b9\u6848\u5229\u7528RNN\u6574\u5408\u5386\u53f2\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8bc4\u4f30\u5176\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u9ad8\u5168\u5c40Q\u51fd\u6570\u8fd1\u4f3c\u7cbe\u5ea6\u53ca\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0eVD-MARL\u65b9\u6848\u76f8\u6bd4\uff0c\u6240\u63d0AR-MARL\u65b9\u6848\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe17.5%\uff1b\u4e0e\u4e0d\u4f7f\u7528FAS\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u964d\u4f4e\u9ad8\u8fbe58.5%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AR-MARL\u65b9\u6848\u5728FAS\u8d4b\u80fd\u7684\u65e0\u4eba\u673a3D\u5b9a\u4f4d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u79fb\u52a8\u76ee\u6807\u65e0\u4eba\u673a\u7684\u5b9e\u65f6\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2507.08967", "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSIMS\uff0c\u9996\u4e2a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u6539\u8fdb\u6a21\u578b\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u6539\u8fdb\u751f\u6210\u548c\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5f15\u5bfc\u6548\u679c\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u4e14\u6548\u679c\u53d7\u6807\u6ce8\u8d28\u91cf\u5f71\u54cd\u3002", "method": "SIMS\u901a\u8fc7\u8fed\u4ee3\u81ea\u6539\u8fdb\u5468\u671f\u81ea\u4e3b\u751f\u6210\u5e76\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u6a21\u578b\u5f15\u5bfc\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u63d0\u793a\u8bcd\u6392\u5e8f\u548c\u5bf9\u6bd4\u91c7\u6837\u7b49\u65b0\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSIMS\u5728\u5f15\u5bfc\u6548\u679c\u548c\u9002\u5e94\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u6539\u8fdb\u6a21\u578b\u5f15\u5bfc\u4e3aLLM\u63a8\u7406\u65f6\u5bf9\u9f50\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.08875", "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6807\u51c6\u8bc4\u4f30\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff08\u5982\u4e3b\u89c2\u6027\u3001\u540c\u8d28\u6027\u5047\u8bbe\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\u7684\u65b0\u578bMCA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff0c\u80fd\u6709\u6548\u6574\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u6570\u636e\uff0c\u63d0\u9ad8\u8bc4\u4f30\u7684\u6548\u7387\u3001\u516c\u5e73\u6027\u4e0e\u53ef\u9760\u6027\u3002\u901a\u8fc7\u6570\u503c\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982DEA\u3001SFA\u3001MCDM\uff09\u5b58\u5728\u4f9d\u8d56\u5047\u8bbe\u548c\u53d7\u4e3b\u89c2\u5224\u65ad\u5f71\u54cd\u7684\u5c40\u9650\u6027\uff1b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u6709\u6548\u6574\u5408\u5b9a\u91cf\uff08\u57fa\u6570\uff09\u548c\u5b9a\u6027\uff08\u5e8f\u6570\uff09\u6570\u636e\uff1b\u540c\u65f6\uff0c\u5c3d\u7ba1\u4e0d\u540c\u65b9\u6848\u7684\u51c6\u5219\u503c\u56fa\u6709\u53d8\u5f02\uff0c\u4f46\u5e38\u7528\u540c\u8d28\u6027\u5047\u8bbe\u4f1a\u663e\u8457\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5e76\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\u7684\u521b\u65b0\u6027\u591a\u6807\u51c6\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\u3002\u8be5VGA\u6846\u67b6\u4ee5\u7ebf\u6027\u89c4\u5212\u4e3a\u57fa\u7840\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bc4\u4f30\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u786e\u4fdd\u4e86\u8bc4\u4f30\u7684\u5168\u9762\u6027\u548c\u53ef\u9760\u6027\u3002\u901a\u8fc7\u4e24\u4e2a\u7efc\u5408\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u672c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4fc3\u8fdb\u81ea\u52a8\u51b3\u7b56\u7cfb\u7edf\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.08829", "pdf": "https://arxiv.org/pdf/2507.08829", "abs": "https://arxiv.org/abs/2507.08829", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u5206\u5c42\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u5730\u9009\u62e9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\u9700\u8981\u4e09\u6a21\u5197\u4f59\uff08TMR\uff09\u4fdd\u62a4\u7684\u5173\u952e\u53c2\u6570\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u76f8\u540c\u5f00\u9500\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u9ad8DNN\u5bf9\u4f4d\u7ffb\u8f6c\u9519\u8bef\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e09\u6a21\u5197\u4f59\uff08TMR\uff09\u662f\u63d0\u9ad8DNN\u53ef\u9760\u6027\u7684\u6709\u6548\u6280\u672f\uff0c\u4f46\u5176\u5f00\u9500\u5de8\u5927\u3002\u4e3a\u964d\u4f4e\u5f00\u9500\uff0cTMR\u9700\u9009\u62e9\u6027\u5730\u5e94\u7528\u4e8e\u6a21\u578b\u8f93\u51fa\u8d21\u732e\u6700\u9ad8\u7684\u53c2\u6570\u548c\u7ec4\u4ef6\uff0c\u56e0\u6b64\uff0c\u9009\u62e9\u6807\u51c6\u7684\u51c6\u786e\u6027\u5bf9TMR\u7684\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u5206\u5c42\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u6280\u672f\u6765\u63d0\u5347DNN\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528LRP\u8ba1\u7b97DNN\u53c2\u6570\u7684\u91cd\u8981\u6027\u5f97\u5206\uff0c\u7136\u540e\u6839\u636e\u8fd9\u4e9b\u5f97\u5206\u9009\u62e9\u6700\u5173\u952e\u7684\u6743\u91cd\uff0c\u5e76\u5bf9\u5176\u5e94\u7528\u4e09\u6a21\u5197\u4f59\uff08TMR\uff09\u4fdd\u62a4\u3002", "result": "\u8be5\u65b9\u6cd5\u5728AlexNet\u6a21\u578b\u4e0a\u4ee510^-4\u7684\u4f4d\u9519\u8bef\u7387\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5b9e\u73b0\u4e86\u8d85\u8fc760%\u7684\u53ef\u9760\u6027\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u540c\u7684\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8eXAI\uff08\u7279\u522b\u662fLRP\uff09\u7684\u9009\u62e9\u6027TMR\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u989d\u5916\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u4f4d\u7ffb\u8f6c\u9519\u8bef\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.08917", "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9762\u90e8\u751f\u7269\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\u7684\u6cd5\u533b\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u5192\u5145\uff0c\u5e76\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6b3a\u8bc8\u548c\u865a\u5047\u4fe1\u606f\u5a01\u80c1\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7ed3\u5408\u58f0\u97f3\u514b\u9686\u548c\u89c6\u89c9\u751f\u6210\uff0c\u4f7f\u5f97\u521b\u5efa\u4efb\u4f55\u4eba\u7684\u865a\u5047\u89c6\u9891\u53d8\u5f97\u76f8\u5bf9\u5bb9\u6613\u3002\u76ee\u524d\uff0c\u8fd9\u79cd\u6df1\u5ea6\u4f2a\u9020\u5192\u5145\u5e38\u88ab\u7528\u4e8e\u6b3a\u8bc8\u3001\u8bc8\u9a97\u548c\u653f\u6cbb\u865a\u5047\u4fe1\u606f\uff0c\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6cd5\u533b\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u5229\u7528\u9762\u90e8\u751f\u7269\u8bc6\u522b\u4e2d\u4e0d\u81ea\u7136\u7684\u6a21\u5f0f\u6765\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u5192\u5145\u3002", "result": "\u8be5\u6280\u672f\u5df2\u5728\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u548c\u5192\u5145\u7684\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u5bf9\u89c6\u9891\u6e05\u6d17\u7684\u53ef\u9760\u6027\u4ee5\u53ca\u5bf9\u5148\u524d\u672a\u89c1\u7684\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u9762\u90e8\u751f\u7269\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\u6765\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5728\u5e94\u5bf9\u73b0\u6709\u53ca\u65b0\u578b\u4f2a\u9020\u6280\u672f\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.09124", "pdf": "https://arxiv.org/pdf/2507.09124", "abs": "https://arxiv.org/abs/2507.09124", "authors": ["Syed Danial Ali Shah", "Maryam Hafeez", "Abdelaziz Salama", "Syed Ali Raza Zaidi"], "title": "Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks", "categories": ["cs.NI"], "comment": null, "summary": "The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims\nto unlock a unified 6G platform capable of seamlessly supporting AI and RAN\nworkloads over shared infrastructure. However, the architectural framework and\nintelligent resource orchestration strategies necessary to realize this vision\nremain largely unexplored. In this paper, we propose a Converged AI-and-ORAN\nArchitectural (CAORA) framework based on O-RAN specifications, enabling the\ndynamic coexistence of real-time RAN and computationally intensive AI\nworkloads. We design custom xApps within the Near-Real-Time RAN Intelligent\nController (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an\nEnd-to-End (E2E) orchestrator via the recently introduced Y1 interface. The\norchestrator incorporates workload forecasting and anomaly detection modules,\naugmenting a Soft Actor-Critic (SAC) reinforcement learning agent that\nproactively manages resource allocation, including Multi-Instance GPU (MIG)\npartitioning. Using real-world 5G traffic traces from Barcelona, our\ntrace-driven simulations demonstrate that CAORA achieves near 99\\% fulfillment\nof RAN demands, supports dynamic AI workloads, and maximizes infrastructure\nutilization even under highly dynamic conditions. Our results reveal that\npredictive orchestration significantly improves system adaptability, resource\nefficiency, and service continuity, offering a viable blueprint for future\nAI-and-RAN converged 6G systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eO-RAN\u7684CAORA\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u9884\u6d4b\u6027\u7f16\u6392\uff0c\u5728\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u4e0a\u9ad8\u6548\u878d\u5408\u7ba1\u7406AI\u548cRAN\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "motivation": "AI-RAN\u878d\u5408\u76846G\u613f\u666f\u7f3a\u4e4f\u5177\u4f53\u7684\u67b6\u6784\u6846\u67b6\u548c\u667a\u80fd\u8d44\u6e90\u7f16\u6392\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u5728\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u4e0a\u7edf\u4e00\u652f\u6301AI\u548cRAN\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eO-RAN\u89c4\u8303\u7684\u201c\u6c47\u805a\u5f0fAI\u548cO-RAN\u67b6\u6784\uff08CAORA\uff09\u201d\u6846\u67b6\u3002\u5728\u8fd1\u5b9e\u65f6RAN\u667a\u80fd\u63a7\u5236\u5668\uff08NRT-RIC\uff09\u4e2d\u8bbe\u8ba1\u5b9a\u5236xApps\uff0c\u901a\u8fc7Y1\u63a5\u53e3\u76d1\u6d4bRAN KPI\u5e76\u5c06\u65e0\u7ebf\u7535\u5206\u6790\u6570\u636e\u66b4\u9732\u7ed9\u7aef\u5230\u7aef\uff08E2E\uff09\u7f16\u6392\u5668\u3002\u8be5\u7f16\u6392\u5668\u96c6\u6210\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\uff0c\u5e76\u589e\u5f3a\u4e86\u4e00\u4e2aSoft Actor-Critic\uff08SAC\uff09\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u4e3b\u52a8\u7ba1\u7406\u8d44\u6e90\u5206\u914d\uff0c\u5305\u62ec\u591a\u5b9e\u4f8bGPU\uff08MIG\uff09\u5206\u533a\u3002", "result": "\u57fa\u4e8e\u5df4\u585e\u7f57\u90a3\u771f\u5b9e5G\u6d41\u91cf\u8f68\u8ff9\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCAORA\u5728\u9ad8\u5ea6\u52a8\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u8fd199%\u7684RAN\u9700\u6c42\u6ee1\u8db3\u7387\uff0c\u652f\u6301\u52a8\u6001AI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u6700\u5927\u5316\u4e86\u57fa\u7840\u8bbe\u65bd\u5229\u7528\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9884\u6d4b\u6027\u7f16\u6392\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9002\u5e94\u6027\u3001\u8d44\u6e90\u6548\u7387\u548c\u670d\u52a1\u8fde\u7eed\u6027\uff0c\u4e3a\u672a\u6765AI\u4e0eRAN\u878d\u5408\u76846G\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u84dd\u56fe\u3002"}}
{"id": "2507.08969", "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "categories": ["cs.CL"], "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u5b58\u5728\u6c61\u540d\u5316\u8bed\u8a00\uff0c\u5176\u5728\u5386\u53f2\u4e0a\u53d7\u6c61\u540d\u5316\u60a3\u8005\u7fa4\u4f53\u4e2d\u51fa\u73b0\u7387\u66f4\u9ad8\uff0c\u5e76\u7531\u591a\u79cd\u533b\u62a4\u4eba\u5458\u7c7b\u578b\uff08\u5982\u62a4\u58eb\u3001\u793e\u5de5\uff09\u4f20\u64ad\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u662f\u533b\u62a4\u56e2\u961f\u4e2d\u60a3\u8005\u6c61\u540d\u5316\u5f97\u4ee5\u5ef6\u7eed\u7684\u5173\u952e\u5a92\u4ecb\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5176\u5177\u4f53\u8868\u73b0\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u8bcd\u5178\u5339\u914d\u548c\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5728MIMIC-III\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u8bc6\u522b\u4e86\u6000\u7591\u6807\u8bb0\u548c\u6c61\u540d\u5316\u6807\u7b7e\u7684\u8bed\u8a00\u7279\u5f81\u3002\u968f\u540e\u4f7f\u7528\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u8bed\u8a00\u7279\u5f81\u51fa\u73b0\u7387\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u975e\u88d4\u7f8e\u56fd\u60a3\u8005\uff08\u98ce\u9669\u6bd4RR: 1.16\uff09\u3001\u62e5\u6709\u533b\u7597\u4fdd\u9669/\u533b\u7597\u8865\u52a9\u6216\u653f\u5e9c\u8fd0\u8425\u4fdd\u9669\u7684\u60a3\u8005\uff08RR: 2.46\uff09\u3001\u81ea\u8d39\u60a3\u8005\uff08RR: 2.12\uff09\uff0c\u4ee5\u53ca\u60a3\u6709\u5404\u79cd\u6c61\u540d\u5316\u75be\u75c5\u548c\u7cbe\u795e\u5065\u5eb7\u72b6\u51b5\u7684\u60a3\u8005\u4e2d\uff0c\u6bcf\u4efd\u75c5\u5386\u4e2d\u7684\u6c61\u540d\u5316\u6807\u7b7e\u4f7f\u7528\u7387\u66f4\u9ad8\u3002\u6000\u7591\u6807\u8bb0\u7684\u6a21\u5f0f\u7c7b\u4f3c\uff0c\u4f46\u7537\u6027\u60a3\u8005\u7684\u6000\u7591\u6807\u8bb0\u7387\u66f4\u9ad8\uff08RR: 1.25\uff09\u3002\u6b64\u5916\uff0c\u62a4\u58eb\uff08RR: 1.40\uff09\u548c\u793e\u4f1a\u5de5\u4f5c\u8005\uff08RR: 2.25\uff09\u66f4\u591a\u5730\u4f7f\u7528\u4e86\u6c61\u540d\u5316\u6807\u7b7e\uff0c\u6000\u7591\u6807\u8bb0\u4e5f\u5448\u73b0\u7c7b\u4f3c\u6a21\u5f0f\u3002", "conclusion": "\u6c61\u540d\u5316\u8bed\u8a00\u5728\u5386\u53f2\u4e0a\u53d7\u6c61\u540d\u5316\u60a3\u8005\u7fa4\u4f53\u4e2d\u51fa\u73b0\u7387\u66f4\u9ad8\uff0c\u4e14\u7531\u591a\u79cd\u533b\u62a4\u4eba\u5458\u7c7b\u578b\uff08\u5305\u62ec\u62a4\u58eb\u548c\u793e\u4f1a\u5de5\u4f5c\u8005\uff09\u6301\u7eed\u4f20\u64ad\u3002"}}
{"id": "2507.08892", "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "categories": ["cs.AI", "cs.MA"], "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u684c\u9762\u89d2\u8272\u626e\u6f14\u6e38\u620f\uff08TTRPG\uff09\u542f\u53d1\u7684\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u6a21\u5f0f\uff0c\u65e8\u5728\u4e3a\u591a\u4e3b\u4f53\u751f\u6210\u5f0fAI\u63d0\u4f9b\u7075\u6d3b\u7684\u573a\u666f\u5b9a\u4e49\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u804c\u8d23\u5206\u79bb\uff0c\u4fc3\u8fdb\u4e86\u5feb\u901f\u8fed\u4ee3\u3001\u6a21\u5757\u5316\u548c\u53ef\u4f38\u7f29\u6027\uff0c\u5e76\u4ee5Concordia\u5e93\u4e3a\u4f8b\u8fdb\u884c\u4e86\u5c55\u793a\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u591a\u4e3b\u4f53\u73af\u5883\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff08\u5982\u793e\u4f1a\u79d1\u5b66\u5efa\u6a21\u3001\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u3001AI\u8bc4\u4f30\uff09\uff0c\u4f46\u8981\u652f\u6301\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u7528\u4f8b\uff0c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u7684\u573a\u666f\u5b9a\u4e49\u6846\u67b6\u3002", "method": "\u7814\u7a76\u53d7TTRPG\u4e2d\u201c\u6e38\u620f\u4e3b\u6301\u4eba\u201d\uff08GM\uff09\u6982\u5ff5\u7684\u542f\u53d1\uff0cGM\u8d1f\u8d23\u73af\u5883\u548c\u6545\u4e8b\u7684\u975e\u73a9\u5bb6\u9a71\u52a8\u90e8\u5206\u3002\u91c7\u7528\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u6a21\u5f0f\uff0c\u5c06GM\u672c\u8eab\u4e5f\u89c6\u4e3a\u4e00\u4e2a\u7531\u7ec4\u4ef6\u6784\u6210\u7684\u53ef\u914d\u7f6e\u5b9e\u4f53\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5c06\u5e95\u5c42\u5b9e\u73b0\u4e0e\u7ec4\u4ef6\u521b\u5efa\u548c\u914d\u7f6e\u5206\u79bb\u5f00\u6765\uff0c\u5de5\u7a0b\u5e08\u8d1f\u8d23\u5b9e\u73b0\uff0c\u8bbe\u8ba1\u5e08\u8d1f\u8d23\u7ec4\u5408\u548c\u914d\u7f6e\u5b9e\u4f53\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u804c\u8d23\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8fed\u4ee3\u3001\u4fdd\u6301\u6a21\u5757\u5316\u5e76\u6700\u7ec8\u786e\u4fdd\u4e86\u53ef\u4f38\u7f29\u6027\u3002\u901a\u8fc7Concordia\u5e93\u7684\u6301\u7eed\u6f14\u8fdb\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u8ba9\u7528\u6237\u6709\u6548\u5730\u914d\u7f6e\u7b26\u5408\u5176\u7279\u5b9a\u76ee\u6807\u7684\u573a\u666f\u3002", "conclusion": "\u53d7TTRPG\u542f\u53d1\u7684\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u6a21\u5f0f\u4e3a\u591a\u4e3b\u4f53\u751f\u6210\u5f0fAI\u7684\u573a\u666f\u5b9a\u4e49\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u914d\u7f6e\u80fd\u529b\uff0c\u5e76\u5df2\u5728Concordia\u5e93\u4e2d\u5f97\u5230\u5b9e\u8df5\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2507.08832", "pdf": "https://arxiv.org/pdf/2507.08832", "abs": "https://arxiv.org/abs/2507.08832", "authors": ["Niranjan Mallikarjun Sindhur", "Pavithra C", "Nivya Muchikel"], "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Farmers in developing regions like Karnataka, India, face a dual challenge:\nnavigating extreme market and climate volatility while being excluded from the\ndigital revolution due to literacy barriers. This paper presents a novel\ndecision support system that addresses both challenges through a unique\nsynthesis of machine learning and human-computer interaction. We propose a\nhybrid recommendation engine that integrates two predictive models: a Random\nForest classifier to assess agronomic suitability based on soil, climate, and\nreal-time weather data, and a Long Short-Term Memory (LSTM) network to forecast\nmarket prices for agronomically viable crops. This integrated approach shifts\nthe paradigm from \"what can grow?\" to \"what is most profitable to grow?\",\nproviding a significant advantage in mitigating economic risk. The system is\ndelivered through an end-to-end, voice-based interface in the local Kannada\nlanguage, leveraging fine-tuned speech recognition and high-fidelity speech\nsynthesis models to ensure accessibility for low-literacy users. Our results\nshow that the Random Forest model achieves 98.5% accuracy in suitability\nprediction, while the LSTM model forecasts harvest-time prices with a low\nmargin of error. By providing data-driven, economically optimized\nrecommendations through an inclusive interface, this work offers a scalable and\nimpactful solution to enhance the financial resilience of marginalized farming\ncommunities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u53d1\u5c55\u4e2d\u56fd\u5bb6\u4f4e\u8bc6\u5b57\u7387\u519c\u6c11\u7684\u8bed\u97f3AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u968f\u673a\u68ee\u6797\u548cLSTM\u6a21\u578b\uff0c\u63d0\u4f9b\u4f5c\u7269\u79cd\u690d\u548c\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\uff0c\u65e8\u5728\u5e2e\u52a9\u519c\u6c11\u89c4\u907f\u98ce\u9669\u3001\u5b9e\u73b0\u5229\u6da6\u6700\u5927\u5316\u3002", "motivation": "\u53d1\u5c55\u4e2d\u56fd\u5bb6\u519c\u6c11\u9762\u4e34\u6781\u7aef\u5e02\u573a\u548c\u6c14\u5019\u6ce2\u52a8\u53cc\u91cd\u6311\u6218\uff0c\u4e14\u56e0\u6587\u5316\u7a0b\u5ea6\u9650\u5236\u88ab\u6392\u9664\u5728\u6570\u5b57\u9769\u547d\u4e4b\u5916\u3002\u73b0\u6709\u7cfb\u7edf\u591a\u5173\u6ce8\u201c\u80fd\u79cd\u4ec0\u4e48\u201d\uff0c\u800c\u975e\u201c\u79cd\u4ec0\u4e48\u6700\u6709\u5229\u53ef\u56fe\u201d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u63a8\u8350\u5f15\u64ce\uff1a\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u6839\u636e\u571f\u58e4\u3001\u6c14\u5019\u548c\u5b9e\u65f6\u5929\u6c14\u6570\u636e\u8bc4\u4f30\u519c\u827a\u9002\u5b9c\u6027\uff0c\u5e76\u5229\u7528LSTM\u7f51\u7edc\u9884\u6d4b\u519c\u4f5c\u7269\u5e02\u573a\u4ef7\u683c\u3002\u7cfb\u7edf\u901a\u8fc7\u672c\u5730\u5361\u7eb3\u8fbe\u8bed\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u754c\u9762\u4ea4\u4ed8\uff0c\u5229\u7528\u5fae\u8c03\u7684\u8bed\u97f3\u8bc6\u522b\u548c\u9ad8\u4fdd\u771f\u8bed\u97f3\u5408\u6210\u6a21\u578b\u786e\u4fdd\u4f4e\u8bc6\u5b57\u7387\u7528\u6237\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u9002\u5b9c\u6027\u9884\u6d4b\u65b9\u9762\u8fbe\u523098.5%\u7684\u51c6\u786e\u7387\uff0c\u800cLSTM\u6a21\u578b\u80fd\u4ee5\u4f4e\u8bef\u5dee\u8303\u56f4\u9884\u6d4b\u6536\u83b7\u65f6\u8282\u4ef7\u683c\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u5305\u5bb9\u6027\u754c\u9762\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u3001\u7ecf\u6d4e\u4f18\u5316\u7684\u5efa\u8bae\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u5f71\u54cd\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u7ecf\u6d4e\u4f18\u5316\u5efa\u8bae\uff0c\u589e\u5f3a\u8fb9\u7f18\u5316\u519c\u4e1a\u793e\u533a\u7684\u8d22\u52a1\u97e7\u6027\uff0c\u5c06\u91cd\u70b9\u4ece\u201c\u80fd\u79cd\u4ec0\u4e48\u201d\u8f6c\u5411\u201c\u79cd\u4ec0\u4e48\u6700\u6709\u5229\u53ef\u56fe\u201d\u3002"}}
{"id": "2507.08979", "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u65e0\u9700\u6570\u636e\u4e14\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u751f\u6210\u865a\u5047\u76f8\u5173\u63cf\u8ff0\u5e76\u5229\u7528\u5bf9\u6bd4\u5f0f\u53bb\u504f\u635f\u5931\u5b66\u4e60\u6295\u5f71\uff0c\u4ece\u800c\u51cf\u8f7b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u9690\u5f0f\u865a\u5047\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u5bf9\u9f50\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u5e76\u653e\u5927\u4e86\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u51fa\u73b0\u504f\u5dee\u3002", "method": "PRISM\u65b9\u6cd5\u5206\u4e24\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u7b80\u5355\u7c7b\u522b\u63d0\u793a\u751f\u6210\u5305\u542b\u865a\u5047\u76f8\u5173\u6027\u7684\u573a\u666f\u63cf\u8ff0\uff1b\u5176\u6b21\uff0cPRISM\u8fd0\u7528\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5f0f\u53bb\u504f\u635f\u5931\uff0c\u5b66\u4e60\u4e00\u4e2a\u6295\u5f71\uff0c\u5c06\u5d4c\u5165\u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u6700\u5c0f\u5316\u865a\u5047\u76f8\u5173\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u5728\u5e38\u7528\u7684Waterbirds\u548cCelebA\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u7684\u53bb\u504f\u65b9\u6cd5\u3002", "conclusion": "PRISM\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u4e14\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u7f13\u89e3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2507.09153", "pdf": "https://arxiv.org/pdf/2507.09153", "abs": "https://arxiv.org/abs/2507.09153", "authors": ["Bilal Karaman", "Ilhan Ba\u015ft\u00fcrk", "Ferdi Kara", "Engin Zeydan", "Esra Aycan Beyaz\u0131t", "Sezai Ta\u015fk\u0131n", "Emil Bj\u00f6rnson", "Halim Yanikomeroglu"], "title": "On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response", "categories": ["cs.NI"], "comment": "Accepted for publication in IEEE COMMAG", "summary": "Natural disasters often disrupt communication networks and severely hamper\nemergency response and disaster management. Existing solutions, such as\nportable communication units and cloud-based network architectures, have\nimproved disaster resilience but fall short if both the Radio Access Network\n(RAN) and backhaul infrastructure become inoperable. To address these\nchallenges, we propose a demand-driven communication system supported by High\nAltitude Platform Stations (HAPS) to restore communication in an affected area\nand enable effective disaster relief. The proposed emergency response network\nis a promising solution as it provides a rapidly deployable, resilient\ncommunications infrastructure. The proposed HAPS-based communication can play a\ncrucial role not only in ensuring connectivity for mobile users but also in\nrestoring backhaul connections when terrestrial networks fail. As a bridge\nbetween the disaster management center and the affected areas, it can\nfacilitate the exchange of information in real time, collect data from the\naffected regions, and relay crucial updates to emergency responders. Enhancing\nsituational awareness, coordination between relief agencies, and ensuring\nefficient resource allocation can significantly strengthen disaster response\ncapabilities. In this paper, simulations show that HAPS with hybrid optical/THz\nlinks boosts backhaul capacity and resilience, even in harsh conditions.\nHAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first\nresponders and disaster-affected populations. This paper also explores the\nintegration of HAPS into emergency communication frameworks and standards, as\nit has the potential to improve network resilience and support effective\ndisaster management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7a7a\u5e73\u53f0\u7ad9\uff08HAPS\uff09\u7684\u9700\u6c42\u9a71\u52a8\u578b\u901a\u4fe1\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u7136\u707e\u5bb3\u5bfc\u81f4\u5730\u9762\u7f51\u7edc\u5931\u6548\u65f6\uff0c\u5feb\u901f\u6062\u590d\u901a\u4fe1\u80fd\u529b\uff0c\u63d0\u5347\u5e94\u6025\u54cd\u5e94\u548c\u707e\u5bb3\u7ba1\u7406\u6548\u7387\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u5e38\u5bfc\u81f4\u901a\u4fe1\u7f51\u7edc\u4e2d\u65ad\uff0c\u4e25\u91cd\u963b\u788d\u5e94\u6025\u54cd\u5e94\u548c\u707e\u5bb3\u7ba1\u7406\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u4fbf\u643a\u5f0f\u901a\u4fe1\u5355\u5143\u548c\u4e91\u57fa\u7f51\u7edc\u67b6\u6784\uff09\u5728\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u548c\u56de\u4f20\u57fa\u7840\u8bbe\u65bd\u5747\u5931\u6548\u65f6\uff0c\u65e0\u6cd5\u6709\u6548\u6062\u590d\u901a\u4fe1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7531HAPS\u652f\u6301\u7684\u9700\u6c42\u9a71\u52a8\u578b\u901a\u4fe1\u7cfb\u7edf\uff0c\u4ee5\u6062\u590d\u53d7\u707e\u533a\u57df\u7684\u901a\u4fe1\u3002\u8be5\u7cfb\u7edf\u63d0\u4f9b\u5feb\u901f\u90e8\u7f72\u3001\u5f39\u6027\u7684\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\uff0c\u4e0d\u4ec5\u80fd\u786e\u4fdd\u79fb\u52a8\u7528\u6237\u8fde\u63a5\uff0c\u8fd8\u80fd\u5728\u5730\u9762\u7f51\u7edc\u6545\u969c\u65f6\u6062\u590d\u56de\u4f20\u94fe\u8def\uff0c\u5e76\u4f5c\u4e3a\u707e\u5bb3\u7ba1\u7406\u4e2d\u5fc3\u4e0e\u53d7\u707e\u533a\u57df\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4fc3\u8fdb\u5b9e\u65f6\u4fe1\u606f\u4ea4\u6362\u548c\u6570\u636e\u6536\u96c6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6df7\u5408\u5149/\u592a\u8d6b\u5179\u94fe\u8def\u7684HAPS\u5373\u4f7f\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u56de\u4f20\u5bb9\u91cf\u548c\u5f39\u6027\u3002HAPS\u652f\u6301\u7684S\u6ce2\u6bb5\u548cKa\u6ce2\u6bb5RAN\u80fd\u4e3a\u7b2c\u4e00\u54cd\u5e94\u8005\u548c\u53d7\u707e\u6c11\u4f17\u63d0\u4f9b\u53ef\u9760\u901a\u4fe1\u3002", "conclusion": "HAPS\u5728\u7d27\u6025\u901a\u4fe1\u6846\u67b6\u548c\u6807\u51c6\u4e2d\u5177\u6709\u6574\u5408\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u7f51\u7edc\u5f39\u6027\u5e76\u6709\u6548\u652f\u6301\u707e\u5bb3\u7ba1\u7406\uff0c\u5728\u786e\u4fdd\u53d7\u707e\u533a\u57df\u8fde\u63a5\u548c\u589e\u5f3a\u707e\u5bb3\u54cd\u5e94\u80fd\u529b\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.09011", "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u4e2a\u4f53\u7684\u5fc3\u50cf\u80fd\u529b\uff08imagery spectrum\uff09\u5dee\u5f02\u663e\u8457\u5f71\u54cdGanzflicker\u8bf1\u5bfc\u7684\u89c6\u89c9\u5e7b\u89c9\u5185\u5bb9\uff1a\u5fc3\u50cf\u80fd\u529b\u5f3a\u8005\u63cf\u8ff0\u590d\u6742\u81ea\u7136\u56fe\u50cf\uff0c\u5f31\u8005\u5219\u62a5\u544a\u7b80\u5355\u51e0\u4f55\u56fe\u6848\u3002", "motivation": "\u63a2\u7d22\u4e2a\u4f53\u5fc3\u50cf\u80fd\u529b\uff08\u4ece\u65e0\u5fc3\u50cf\u5230\u751f\u52a8\u5fc3\u50cf\uff09\u7684\u5dee\u5f02\u662f\u5426\u4f1a\u5f71\u54cdGanzflicker\u8bf1\u5bfc\u7684\u89c6\u89c9\u5e7b\u89c9\u7684\u590d\u6742\u6027\u548c\u5185\u5bb9\uff0c\u4ece\u800c\u7406\u89e3\u5185\u90e8\u751f\u6210\u89c6\u89c9\u7ecf\u9a8c\u7684\u4e2a\u4f53\u5dee\u5f02\u3002", "method": "\u6536\u96c6\u4e86\u8d85\u8fc74000\u540d\u53c2\u4e0e\u8005\u5bf9Ganzflicker\u8bf1\u5bfc\u5e7b\u89c9\u7684\u81ea\u7531\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u8fdb\u884c\u5206\u6790\u3002\u7814\u7a76\u5bf9\u6bd4\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u4e0e\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u5dee\u5f02\u65b9\u9762\u7684\u6548\u679c\u3002", "result": "\u5fc3\u50cf\u80fd\u529b\u5f3a\u7684\u53c2\u4e0e\u8005\u63cf\u8ff0\u4e86\u590d\u6742\u3001\u81ea\u7136\u4e3b\u4e49\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u800c\u5fc3\u50cf\u80fd\u529b\u5f31\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u7b80\u5355\u7684\u51e0\u4f55\u56fe\u6848\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6bd4\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02\uff0c\u4e14\u5fc3\u50cf\u80fd\u529b\u5f3a\u7684\u53c2\u4e0e\u8005\u4f7f\u7528\u7684\u8bed\u8a00\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u611f\u5b98\u8fd0\u52a8\u5173\u8054\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u53ef\u80fd\u53cd\u6620\u4e86\u65e9\u671f\u89c6\u89c9\u533a\u57df\u4e0e\u4e0e\u5fc3\u50cf\u8c31\u76f8\u5173\u7684\u9ad8\u9636\u533a\u57df\u4e4b\u95f4\u534f\u8c03\u7684\u4e2a\u4f53\u5dee\u5f02\u3002"}}
{"id": "2507.09080", "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "categories": ["cs.AI"], "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BioAnalyst\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684AI\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u4e3a\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u89c4\u5212\u8bbe\u8ba1\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u751f\u6001\u9884\u6d4b\u5efa\u7acb\u4e86\u65b0\u7684\u51c6\u786e\u6027\u57fa\u7ebf\u3002", "motivation": "\u751f\u7269\u591a\u6837\u6027\u52a0\u901f\u4e27\u5931\u5bf9\u751f\u6001\u7814\u7a76\u548c\u4fdd\u62a4\u7b56\u7565\u6784\u6210\u4e86\u4e25\u5cfb\u6311\u6218\uff0c\u4e9f\u9700\u5177\u5907\u5168\u9762\u7684\u76d1\u6d4b\u3001\u9884\u6d4b\u548c\u4fdd\u62a4\u89c4\u5212\u80fd\u529b\u3002\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u5728\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u5df2\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u89e3\u51b3\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u9762\u4e34\u7684\u5a01\u80c1\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86BioAnalyst\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u4e3a\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u89c4\u5212\u5b9a\u5236\u7684\u57fa\u7840\u6a21\u578b\u3002\u5b83\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5728\u5305\u542b\u7269\u79cd\u51fa\u73b0\u8bb0\u5f55\u3001\u9065\u611f\u6307\u6807\u3001\u6c14\u5019\u548c\u73af\u5883\u53d8\u91cf\u7b49\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002BioAnalyst\u8bbe\u8ba1\u7075\u6d3b\uff0c\u53ef\u9488\u5bf9\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u3001\u6816\u606f\u5730\u9002\u5b9c\u6027\u8bc4\u4f30\u3001\u5165\u4fb5\u7269\u79cd\u68c0\u6d4b\u548c\u79cd\u7fa4\u8d8b\u52bf\u9884\u6d4b\u7b49\u4e00\u7cfb\u5217\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u5728\u4e24\u4e2a\u4e0b\u6e38\u7528\u4f8b\u4e2d\u8bc4\u4f30\u4e86BioAnalyst\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u5e76\u4e3a\u751f\u6001\u9884\u6d4b\u5efa\u7acb\u4e86\u65b0\u7684\u51c6\u786e\u6027\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90BioAnalyst\u53ca\u5176\u5fae\u8c03\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u4fc3\u8fdb\u79d1\u5b66\u754c\u5728\u751f\u7269\u591a\u6837\u6027\u5efa\u6a21\u65b9\u9762\u7684\u5408\u4f5c\uff0c\u5e76\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u5e94\u5bf9\u7d27\u8feb\u7684\u751f\u6001\u6311\u6218\u3002"}}
{"id": "2507.08833", "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "LoRA\u5728LLM\u5fae\u8c03\u4e2d\u5e76\u975e\u603b\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u901f\u5ea6\u63d0\u5347\u3002\u672c\u7814\u7a76\u5206\u6790\u4e86\u5176\u6027\u80fd\u9650\u5236\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0eLoRA\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u901f\u5ea6\u63d0\u5347\u66f4\u7a33\u5b9a\u3002", "motivation": "\u89c2\u5bdf\u5230LoRA\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\uff0c\u5176\u901f\u5ea6\u63d0\u5347\u5e76\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u65e8\u5728\u6df1\u5165\u5206\u6790\u5176\u6027\u80fd\u74f6\u9888\u548c\u9650\u5236\u56e0\u7d20\u3002", "method": "\u5bf9LoRA\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u5e76\u8c03\u67e5\u4e86\u9650\u5236\u5176\u901f\u5ea6\u63d0\u5347\u7684\u6f5c\u5728\u56e0\u7d20\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684LLM\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u4e0eLoRA\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0eLoRA\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u8bad\u7ec3\u901f\u5ea6\u6539\u8fdb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u4f18\u5316LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.08981", "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "categories": ["cs.CV"], "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "\u63d0\u51faHMR-ViT\uff0c\u4e00\u79cd\u878d\u5408\u65f6\u5e8f\u548c\u8fd0\u52a8\u5b66\u4fe1\u606f\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\u3002\u901a\u8fc7\u6784\u5efa\u65f6\u5e8f-\u8fd0\u52a8\u5b66\u7279\u5f81\u56fe\u5e76\u7ed3\u5408Vision Transformer\u8fdb\u884c\u5904\u7406\uff0c\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\uff08HMR\uff09\u65b9\u6cd5\u6216\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\uff0c\u6216\u5229\u7528\u8fd0\u52a8\u5b66\u5173\u7cfb\uff0c\u4f46\u6ca1\u6709\u65b9\u6cd5\u540c\u65f6\u5229\u7528\u4e24\u8005\uff0c\u5bfc\u81f4\u4efb\u52a1\u5b58\u5728\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3002", "method": "\u63d0\u51fa\u201cVideo Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)\u201d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u56fe\u50cf\u7f16\u7801\u5668\u4ece\u89c6\u9891\u5e27\u4e2d\u63d0\u53d6\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u6784\u5efa\u201c\u65f6\u5e8f-\u8fd0\u52a8\u5b66\u7279\u5f81\u56fe\u201d\uff0c\u5176\u4e2d\u901a\u8fc7\u901a\u9053\u91cd\u6392\u77e9\u9635\uff08CRM\uff09\u4f7f\u76f8\u4f3c\u7684\u8fd0\u52a8\u5b66\u7279\u5f81\u5728\u7a7a\u95f4\u4e0a\u63a5\u8fd1\u3002\u968f\u540e\uff0c\u8be5\u7279\u5f81\u56fe\u7531Vision Transformer\u8fdb\u4e00\u6b65\u7f16\u7801\uff0c\u6700\u540e\u901a\u8fc7\u56de\u5f52\u7f51\u7edc\u63a8\u65ad\u51faSMPL\u59ff\u6001\u548c\u5f62\u72b6\u53c2\u6570\u3002", "result": "\u57283DPW\u548cHuman3.6M\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "HMR-ViT\u65b9\u6cd5\u6210\u529f\u5730\u7ed3\u5408\u4e86\u65f6\u5e8f\u548c\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09270", "pdf": "https://arxiv.org/pdf/2507.09270", "abs": "https://arxiv.org/abs/2507.09270", "authors": ["Songhan Zhao", "Yusi Long", "Lanhua Li", "Bo Gu", "Shimin Gong", "Zehui Xiong"], "title": "Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications", "categories": ["cs.NI"], "comment": null, "summary": "In this paper, we consider a semantic-aware reconfigurable intelligent\nsurface (RIS)-assisted wireless network, where multiple semantic users (SUs)\nsimultaneously transmit semantic information to an access point (AP) by using\nthe non-orthogonal multiple access (NOMA) method. The SUs can reshape their\ntraffic demands by modifying the semantic extraction factor, while the RIS can\nreconfigure the channel conditions via the passive beamforming. This provides\nthe AP with greater flexibility to decode the superimposed signals from the\nSUs. We aim to minimize the system's overall energy consumption, while ensuring\nthat each SU's traffic demand is satisfied. Hence, we formulate a joint\noptimization problem of the SUs' decoding order and semantic control, as well\nas the RIS's passive beamforming strategy. This problem is intractable due to\nthe complicated coupling in constraints. To solve this, we decompose the\noriginal problem into two subproblems and solve them by using a series of\napproximate methods. Numerical results show that the joint traffic reshaping\nand channel reconfiguration scheme significantly improves the energy saving\nperformance of the NOMA transmissions compared to the benchmark methods.", "AI": {"tldr": "\u5728\u8bed\u4e49\u611f\u77e5RIS-NOMA\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7528\u6237\u8bed\u4e49\u63a7\u5236\u3001RIS\u65e0\u6e90\u6ce2\u675f\u8d4b\u5f62\u548c\u89e3\u7801\u987a\u5e8f\uff0c\u5b9e\u73b0\u80fd\u8017\u6700\u5c0f\u5316\u5e76\u6ee1\u8db3\u7528\u6237\u9700\u6c42\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6848\u663e\u8457\u63d0\u5347NOMA\u4f20\u8f93\u80fd\u6548\u3002", "motivation": "\u9488\u5bf9\u8bed\u4e49\u611f\u77e5RIS\u8f85\u52a9\u7684NOMA\u7f51\u7edc\uff0c\u65e8\u5728\u901a\u8fc7\u7528\u6237\u6d41\u91cf\u9700\u6c42\u91cd\u5851\u548cRIS\u4fe1\u9053\u91cd\u6784\uff0c\u5728\u6ee1\u8db3\u5404\u7528\u6237\u6d41\u91cf\u9700\u6c42\u7684\u524d\u63d0\u4e0b\uff0c\u6700\u5c0f\u5316\u7cfb\u7edf\u603b\u80fd\u8017\uff0c\u4ee5\u63d0\u9ad8\u63a5\u5165\u70b9\uff08AP\uff09\u89e3\u7801\u53e0\u52a0\u4fe1\u53f7\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u6db5\u76d6\u8bed\u4e49\u7528\u6237\uff08SU\uff09\u7684\u89e3\u7801\u987a\u5e8f\u3001\u8bed\u4e49\u63a7\u5236\u548cRIS\u7684\u65e0\u6e90\u6ce2\u675f\u8d4b\u5f62\u7b56\u7565\u3002\u7531\u4e8e\u8be5\u95ee\u9898\u5177\u6709\u590d\u6742\u7684\u8026\u5408\u6027\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e00\u7cfb\u5217\u8fd1\u4f3c\u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u6d41\u91cf\u6574\u5f62\u548c\u4fe1\u9053\u91cd\u6784\u65b9\u6848\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86NOMA\u4f20\u8f93\u7684\u8282\u80fd\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7528\u6237\u8bed\u4e49\u63a7\u5236\u548cRIS\u65e0\u6e90\u6ce2\u675f\u8d4b\u5f62\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u611f\u77e5RIS-NOMA\u7f51\u7edc\u4e2d\u7684\u80fd\u8017\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u80fd\u91cf\u6548\u7387\u3002"}}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3Transformer\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u7075\u6d3b\u3001\u4e9a\u4e8c\u6b21\u590d\u6742\u5ea6\u67b6\u6784\u7684\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u65e0\u9650\u4e0a\u4e0b\u6587\u751f\u6210\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "Transformer\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u8fd9\u6e90\u4e8esoftmax\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u4e0d\u65ad\u589e\u5927\u7684KV\u7f13\u5b58\u3002", "method": "Lizard\u5f15\u5165\u4e86\u4e00\u79cd\u4e9a\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u6765\u8fd1\u4f3csoftmax\u6ce8\u610f\u529b\uff0c\u5e76\u7ed3\u5408\u4e86\u53d7SOTA\u7ebf\u6027\u6a21\u578b\u542f\u53d1\u800c\u8bbe\u8ba1\u7684\u95e8\u63a7\u6a21\u5757\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u5185\u5b58\u63a7\u5236\u548c\u6052\u5b9a\u5185\u5b58\u63a8\u7406\u3002\u5b83\u91c7\u7528\u6df7\u5408\u673a\u5236\uff0c\u878d\u5408\u4e86\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\uff08\u5168\u5c40\u4e0a\u4e0b\u6587\u538b\u7f29\uff09\u548c\u589e\u5f3a\u5143\u8bb0\u5fc6\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff08\u5c40\u90e8\u4ea4\u4e92\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u786c\u4ef6\u611f\u77e5\u7b97\u6cd5\u4ee5\u52a0\u901f\u8bad\u7ec3\u3002", "result": "Lizard\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u6559\u5e08\u6a21\u578b\u6027\u80fd\u7684\u8fd1\u4e4e\u65e0\u635f\u6062\u590d\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\u3002\u57285-shot MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u6a21\u578b\u63d0\u9ad8\u4e8618\u5206\uff0c\u5e76\u5728\u8054\u60f3\u53ec\u56de\u4efb\u52a1\u4e0a\u4e5f\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "Lizard\u6210\u529f\u5730\u901a\u8fc7\u521b\u65b0\u7684\u7ebf\u6027\u5316\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86Transformer LLM\u7684\u957f\u4e0a\u4e0b\u6587\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65e0\u9650\u4e0a\u4e0b\u6587\u751f\u6210\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.09089", "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u5de5\u5177\u610f\u5916\u5730\u964d\u4f4e\u4e86\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u6e90\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\uff0c\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\uff0c\u4e0e\u5f00\u53d1\u8005\u548c\u4e13\u5bb6\u9884\u6d4b\u76f8\u6096\u3002", "motivation": "\u5c3d\u7ba1AI\u5de5\u5177\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u5bf9\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u5f71\u54cd\u7684\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u7406\u89e32025\u5e74\u524d\u6cbfAI\u5de5\u5177\u5bf9\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u6e90\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff08RCT\uff09\uff0c\u62db\u52df16\u540d\u62e5\u6709\u4e2d\u7b49AI\u7ecf\u9a8c\u7684\u5f00\u53d1\u8005\uff0c\u5728\u4ed6\u4eec\u5e73\u5747\u67095\u5e74\u7ecf\u9a8c\u7684\u6210\u719f\u5f00\u6e90\u9879\u76ee\u4e2d\u5b8c\u6210246\u9879\u4efb\u52a1\u3002\u6bcf\u9879\u4efb\u52a1\u968f\u673a\u5206\u914d\u4e3a\u5141\u8bb8\u6216\u7981\u6b62\u4f7f\u75282025\u5e74\u521d\u7684AI\u5de5\u5177\uff08\u4e3b\u8981\u4e3aCursor Pro\u548cClaude 3.5/3.7 Sonnet\uff09\u3002", "result": "\u5f00\u53d1\u8005\u9884\u671fAI\u5de5\u5177\u80fd\u5c06\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed24%\uff08\u4e8b\u540e\u4f30\u8ba1\u4e3a20%\uff09\uff0c\u4f46\u5b9e\u9645\u7ed3\u679c\u663e\u793a\uff0c\u5141\u8bb8AI\u5de5\u5177\u4f7f\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u4e8619%\u3002\u8fd9\u4e00\u7ed3\u679c\u4e0e\u7ecf\u6d4e\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e13\u5bb6\u9884\u6d4b\u7684\u663e\u8457\u7f29\u77ed\uff0838-39%\uff09\u76f8\u6096\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e8620\u4e2a\u53ef\u80fd\u5bfc\u81f4\u51cf\u901f\u6548\u5e94\u7684\u5c5e\u6027\uff0c\u53d1\u73b0\u51cf\u901f\u6548\u5e94\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "AI\u5de5\u5177\u610f\u5916\u5730\u964d\u4f4e\u4e86\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u6e90\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u3002\u5c3d\u7ba1\u4e0d\u80fd\u5b8c\u5168\u6392\u9664\u5b9e\u9a8c\u4f2a\u5f71\uff0c\u4f46\u51cf\u901f\u6548\u5e94\u7684\u9c81\u68d2\u6027\u8868\u660e\u5176\u5e76\u975e\u4e3b\u8981\u6e90\u4e8e\u5b9e\u9a8c\u8bbe\u8ba1\u3002"}}
{"id": "2507.08834", "pdf": "https://arxiv.org/pdf/2507.08834", "abs": "https://arxiv.org/abs/2507.08834", "authors": ["Karishma Battina", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Physical Informed Neural Networks for modeling ocean pollutant", "categories": ["cs.LG"], "comment": "13 pages, 9 figures, 3 tables", "summary": "Traditional numerical methods often struggle with the complexity and scale of\nmodeling pollutant transport across vast and dynamic oceanic domains. This\npaper introduces a Physics-Informed Neural Network (PINN) framework to simulate\nthe dispersion of pollutants governed by the 2D advection-diffusion equation.\nThe model achieves physically consistent predictions by embedding physical laws\nand fitting to noisy synthetic data, generated via a finite difference method\n(FDM), directly into the neural network training process. This approach\naddresses challenges such as non-linear dynamics and the enforcement of\nboundary and initial conditions. Synthetic data sets, augmented with varying\nnoise levels, are used to capture real-world variability. The training\nincorporates a hybrid loss function including PDE residuals, boundary/initial\ncondition conformity, and a weighted data fit term. The approach takes\nadvantage of the Julia language scientific computing ecosystem for\nhigh-performance simulations, offering a scalable and flexible alternative to\ntraditional solvers", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6d77\u6d0b\u6c61\u67d3\u7269\u6269\u6563\uff0c\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u548c\u62df\u5408\u542b\u566a\u58f0\u6570\u636e\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u6a21\u62df\u5e7f\u9614\u52a8\u6001\u6d77\u6d0b\u57df\u4e2d\u590d\u6742\u5927\u89c4\u6a21\u7684\u6c61\u67d3\u7269\u4f20\u8f93\u65f6\u9762\u4e34\u56f0\u96be\u3002", "method": "\u5f15\u5165\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6846\u67b6\u6765\u6a21\u62df\u53d72D\u5e73\u6d41\u6269\u6563\u65b9\u7a0b\u63a7\u5236\u7684\u6c61\u67d3\u7269\u6269\u6563\u3002\u901a\u8fc7\u5c06\u7269\u7406\u5b9a\u5f8b\u548c\u6765\u81ea\u6709\u9650\u5dee\u5206\u6cd5\uff08FDM\uff09\u7684\u542b\u566a\u58f0\u5408\u6210\u6570\u636e\u76f4\u63a5\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u6027\u9884\u6d4b\u3002\u8bad\u7ec3\u91c7\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\uff0c\u5305\u542b\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6b8b\u5dee\u3001\u8fb9\u754c/\u521d\u59cb\u6761\u4ef6\u7b26\u5408\u6027\u4ee5\u53ca\u52a0\u6743\u6570\u636e\u62df\u5408\u9879\u3002\u5229\u7528Julia\u8bed\u8a00\u7684\u79d1\u5b66\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u9ad8\u6027\u80fd\u6a21\u62df\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\u3002\u6210\u529f\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4ee5\u53ca\u8fb9\u754c\u548c\u521d\u59cb\u6761\u4ef6\u5f3a\u5236\u6267\u884c\u7b49\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u4f20\u7edf\u6c42\u89e3\u5668\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PINN\u6846\u67b6\u4e3a\u590d\u6742\u7684\u6d77\u6d0b\u6c61\u67d3\u7269\u4f20\u8f93\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\u3002"}}
{"id": "2507.09005", "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "categories": ["cs.CV", "physics.geo-ph"], "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408NeRF\u4e0eMPM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u89c2\u6d4b\u53cd\u6f14\u9897\u7c92\u6750\u6599\u7684\u6469\u64e6\u89d2\u3002", "motivation": "\u89e3\u51b3\u5728\u76f4\u63a5\u6d4b\u91cf\u56f0\u96be\u6216\u4e0d\u53ef\u80fd\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u6709\u6548\u63a8\u65ad\u9897\u7c92\u6750\u6599\u7269\u7406\u7279\u6027\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\u751f\u6210\u7281\u4e0e\u6c99\u5b50\u4ea4\u4e92\u7684\u5408\u6210\u5b9e\u9a8c\u6570\u636e\u5e76\u6e32\u67d3\u6210\u56fe\u50cf\u89c2\u6d4b\uff1b\u63a5\u7740\u4f7f\u7528NeRF\u4ece\u521d\u59cb\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa3D\u51e0\u4f55\uff1b\u7136\u540e\u7528\u91cd\u5efa\u51e0\u4f55\u521d\u59cb\u5316MPM\u6a21\u62df\uff08\u6469\u64e6\u89d2\u672a\u77e5\uff09\uff0c\u5e76\u6e32\u67d3\u6a21\u62df\u56fe\u50cf\uff1b\u6700\u540e\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u6a21\u62df\u4e0e\u89c2\u6d4b\u56fe\u50cf\u4e4b\u95f4\u7684\u635f\u5931\uff0c\u4ee5\u4f30\u8ba1\u6700\u4f73\u6469\u64e6\u89d2\u3002", "result": "\u6469\u64e6\u89d2\u7684\u4f30\u8ba1\u8bef\u5dee\u57282\u5ea6\u4ee5\u5185\uff0c\u8bc1\u660e\u4e86\u4ec5\u51ed\u89c6\u89c9\u89c2\u6d4b\u8fdb\u884c\u53cd\u6f14\u5206\u6790\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u96be\u4ee5\u76f4\u63a5\u6d4b\u91cf\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u8868\u5f81\u9897\u7c92\u6750\u6599\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09341", "pdf": "https://arxiv.org/pdf/2507.09341", "abs": "https://arxiv.org/abs/2507.09341", "authors": ["Mahsa Paknejad", "Parisa Fard Moshiri", "Murat Simsek", "Burak Kantarci", "Hussein T. Mouftah"], "title": "Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks", "categories": ["cs.NI"], "comment": "8 pages, 7 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Vehicular Mobile Edge Computing (VEC) drives the future by enabling\nlow-latency, high-efficiency data processing at the very edge of vehicular\nnetworks. This drives innovation in key areas such as autonomous driving,\nintelligent transportation systems, and real-time analytics. Despite its\npotential, VEC faces significant challenges, particularly in adhering to strict\ntask offloading deadlines, as vehicles remain within the coverage area of\nRoadside Units (RSUs) for only brief periods. To tackle this challenge, this\npaper evaluates the performance boundaries of task processing by initially\nestablishing a theoretical limit using Particle Swarm Optimization (PSO) in a\nstatic environment. To address more dynamic and practical scenarios, PSO, Deep\nQ-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented\nin an online setting. The objective is to minimize dropped tasks and reduce\nend-to-end (E2E) latency, covering both communication and computation delays.\nExperimental results demonstrate that the DQN model considerably surpasses the\ndynamic PSO approach, achieving a 99.2% reduction in execution time.\nFurthermore, It leads to a reduction in dropped tasks by 2.5% relative to\ndynamic PSO and achieves 18.6\\% lower E2E latency, highlighting the\neffectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and\nefficient task management for VEC systems.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8f66\u8f7d\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97(VEC)\u4e2d\u4efb\u52a1\u5378\u8f7d\u622a\u6b62\u671f\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u7ebf\u6a21\u5f0f\u4e0b\u5e94\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DQN\u3001PPO)\u548cPSO\uff0c\u4ee5\u6700\u5c0f\u5316\u4efb\u52a1\u4e22\u5f03\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u7ed3\u679c\u663e\u793aDQN\u5728\u6267\u884c\u65f6\u95f4\u3001\u4efb\u52a1\u4e22\u5f03\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u52a8\u6001PSO\u3002", "motivation": "\u8f66\u8f7d\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97(VEC)\u662f\u672a\u6765\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u6570\u636e\u5904\u7406\u7684\u5173\u952e\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u3001\u667a\u80fd\u4ea4\u901a\u7b49\u9886\u57df\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8f66\u8f86\u5728\u8def\u8fb9\u5355\u5143(RSU)\u8986\u76d6\u8303\u56f4\u5185\u7684\u505c\u7559\u65f6\u95f4\u77ed\u6682\uff0cVEC\u9762\u4e34\u7740\u4e25\u683c\u7684\u4efb\u52a1\u5378\u8f7d\u622a\u6b62\u671f\u9650\u6311\u6218\u3002", "method": "\u7814\u7a76\u9996\u5148\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316(PSO)\u5728\u9759\u6001\u73af\u5883\u4e0b\u5efa\u7acb\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u3002\u4e3a\u5e94\u5bf9\u52a8\u6001\u573a\u666f\uff0c\u8fdb\u4e00\u6b65\u5728\u7ebf\u90e8\u7f72\u4e86PSO\u3001\u6df1\u5ea6Q\u7f51\u7edc(DQN)\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u6a21\u578b\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u4efb\u52a1\u4e22\u5f03\u548c\u7aef\u5230\u7aef(E2E)\u5ef6\u8fdf\uff08\u5305\u62ec\u901a\u4fe1\u548c\u8ba1\u7b97\u5ef6\u8fdf\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDQN\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u52a8\u6001PSO\u65b9\u6cd5\u3002\u5177\u4f53\u8868\u73b0\u4e3a\u6267\u884c\u65f6\u95f4\u51cf\u5c1199.2%\uff0c\u4efb\u52a1\u4e22\u5f03\u7387\u76f8\u5bf9\u52a8\u6001PSO\u964d\u4f4e2.5%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e18.6%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u5728\u5b9e\u73b0VEC\u7cfb\u7edf\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u4efb\u52a1\u7ba1\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09037", "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "ALIGN\u7cfb\u7edf\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u52a9\u624b\u7684\u52a8\u6001\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u5c5e\u6027\u914d\u7f6e\u548c\u591a\u7c7b\u578b\u5206\u6790\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u51b3\u7b56\u8f85\u52a9\u5de5\u5177\u65f6\uff0c\u9700\u8981\u9002\u5e94\u7528\u6237\u591a\u6837\u5316\u7684\u4ef7\u503c\u89c2\u548c\u504f\u597d\u3002\u73b0\u6709LLM\u5de5\u5177\u4e3b\u8981\u4fa7\u91cd\u4e8e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5bf9LLM\u51b3\u7b56\u8005\u8fdb\u884c\u52a8\u6001\u5bf9\u9f50\u548c\u4e2a\u6027\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faALIGN\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u8bcd\u7684\u65b9\u6cd5\u5c06LLM\u51b3\u7b56\u8005\u4e0e\u7ec6\u7c92\u5ea6\u5c5e\u6027\u5bf9\u9f50\uff0c\u5b9e\u73b0\u52a8\u6001\u4e2a\u6027\u5316\u3002\u7cfb\u7edf\u5177\u5907\u9c81\u68d2\u7684\u914d\u7f6e\u7ba1\u7406\u3001\u5e26\u63a8\u7406\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\u3001\u53ef\u5207\u6362LLM\u9aa8\u5e72\u7684\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4ee5\u53ca\u4e00\u4e2a\u652f\u6301LLM\u5e76\u6392\u5b9a\u6027\u6bd4\u8f83\u548c\u6a21\u5757\u5316\u540e\u7aef\u7684\u7528\u6237\u754c\u9762\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u5bf9\u9f50\uff08\u516c\u5171\u610f\u89c1\u8c03\u67e5\uff09\u548c\u4ef7\u503c\u5bf9\u9f50\uff08\u533b\u7597\u5206\u8bca\u51b3\u7b56\uff09\u4e24\u79cd\u4e0d\u540c\u9886\u57df\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "ALIGN\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86LLM\u51b3\u7b56\u8005\u7684\u52a8\u6001\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u5e76\u652f\u6301\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\u3002\u5b83\u80fd\u591f\u5bf9\u4e0d\u540c\u9886\u57df\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4ef7\u503c\u89c2\uff09\u7684\u5bf9\u9f50\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u3002\u8be5\u6846\u67b6\u5f00\u6e90\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "conclusion": "ALIGN\u5f00\u6e90\u6846\u67b6\u5c06\u63a8\u52a8\u5173\u4e8e\u53ef\u9760\u3001\u8d1f\u8d23\u4efb\u548c\u4e2a\u6027\u5316LLM\u51b3\u7b56\u52a9\u624b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u89e3\u51b3\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\u4e0b\u7684LLM\u5bf9\u9f50\u4e0e\u4e2a\u6027\u5316\u6311\u6218\u3002"}}
{"id": "2507.09179", "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "categories": ["cs.AI"], "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "\u9488\u5bf9DeFi\u5e02\u573a\u4e2d\u65e5\u76ca\u7316\u7357\u7684\u64cd\u7eb5\u884c\u4e3a\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5bf9\u6297\u535a\u5f08\u5e76\u6574\u5408\u591a\u6e90\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5e02\u573a\u64cd\u7eb5\u68c0\u6d4b\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\uff08DeFi\uff09\u867d\u7136\u5e26\u6765\u4e86\u91d1\u878d\u521b\u65b0\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4e2d\u5fc3\u5316\u76d1\u7ba1\uff0c\u5bfc\u81f4\u5e02\u573a\u64cd\u7eb5\uff08\u5982\u62c9\u9ad8\u51fa\u8d27\uff09\u884c\u4e3a\u65e5\u76ca\u7316\u7357\uff0c\u6025\u9700\u6709\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u64cd\u7eb5\u68c0\u6d4b\uff0c\u5c06\u64cd\u7eb5\u8005\u4e0e\u68c0\u6d4b\u8005\u5efa\u6a21\u4e3a\u52a8\u6001\u5bf9\u6297\u535a\u5f08\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a1) Group Relative Policy Optimization (GRPO) \u63d0\u5347\u5b66\u4e60\u7a33\u5b9a\u6027\uff1b2) \u57fa\u4e8e\u7406\u6027\u9884\u671f\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u5956\u52b1\u51fd\u6570\uff0c\u533a\u5206\u4ef7\u683c\u53d1\u73b0\u4e0e\u64cd\u7eb5\u566a\u58f0\uff1b3) \u878d\u5408LLM\u8bed\u4e49\u7279\u5f81\u3001\u793e\u4ea4\u56fe\u8c31\u4fe1\u53f7\u548c\u94fe\u4e0a\u5e02\u573a\u6570\u636e\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7ba1\u9053\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e8eSymphony\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\uff0c\u652f\u6301\u94fe\u4e0a\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u548c\u53bb\u4e2d\u5fc3\u5316\u76d1\u63a7\u3002", "result": "\u901a\u8fc7\u5728100,000\u4e2a\u771f\u5b9e\u8bed\u6599\u4e8b\u4ef6\u4e0a\u8bad\u7ec3\u5e76\u5728\u5bf9\u6297\u6027\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\uff08Hide-and-Shill\uff09\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u56e0\u679c\u5f52\u56e0\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u91d1\u878d\u76d1\u7ba1\u76f8\u7ed3\u5408\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5e02\u573a\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5168\u7403DeFi\u751f\u6001\u7684\u5b9e\u65f6\u76d1\u63a7\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u5f00\u653e\u7814\u7a76\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.08835", "pdf": "https://arxiv.org/pdf/2507.08835", "abs": "https://arxiv.org/abs/2507.08835", "authors": ["Harold Gu\u00e9neau", "Alain Celisse", "Pascal Delange"], "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "comment": null, "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4e24\u6b65\u6cd5\uff0c\u5229\u7528\u7ed3\u6784\u5316\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff08\u5b9a\u6027\u548c\u5b9a\u91cf\uff09\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548cBenjamini-Hochberg (BH) \u7a0b\u5e8f\u8fdb\u884c\u6d17\u94b1\u68c0\u6d4b\uff0c\u6709\u6548\u63a7\u5236\u8bef\u62a5\u7387\u5e76\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6d17\u94b1\u68c0\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u590d\u6742\u6570\u636e\u3001\u51cf\u5c11\u4eba\u5de5\u76d1\u7763\u5e76\u80fd\u540c\u65f6\u68c0\u6d4b\u6b3a\u8bc8\u8005\u548c\u975e\u6b3a\u8bc8\u8005\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u63a7\u5236\u8bef\u62a5\u7387\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u4e24\u6b65\u6cd5\uff0c\u57fa\u4e8eTransformer\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u7ed3\u6784\u5316\u65f6\u95f4\u5e8f\u5217\uff08\u5305\u542b\u5b9a\u6027\u548c\u5b9a\u91cf\u6570\u636e\uff09\u3002\u7b2c\u4e00\u6b65\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff08\u65e0\u6807\u7b7e\uff09\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u3002\u7b2c\u4e8c\u6b65\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u751f\u6210\u6d17\u94b1\u8bc4\u5206\u3002\u4e3a\u63a7\u5236\u8bef\u62a5\u7387\uff0c\u91c7\u7528\u53cc\u9608\u503c\u65b9\u6cd5\u5e76\u7ed3\u5408Benjamini-Hochberg (BH) \u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTransformer\u80fd\u591f\u751f\u6210\u901a\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u6709\u6548\u5229\u7528\u6d17\u94b1\u6a21\u5f0f\uff0c\u4e14\u5bf9\u9886\u57df\u4e13\u5bb6\u7684\u76d1\u7763\u9700\u6c42\u6781\u5c11\u3002\u65b0\u65b9\u6cd5\u5728\u68c0\u6d4b\u6b3a\u8bc8\u8005\u548c\u975e\u6b3a\u8bc8\u8005\u65b9\u9762\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u80fd\u529b\uff0c\u540c\u65f6\u80fd\u6709\u6548\u63a7\u5236\u8bef\u62a5\u7387\u3002\u8fd9\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u6216LSTM\u67b6\u6784\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684\u65b0\u65b9\u6cd5\u5728\u6d17\u94b1\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u3001\u51cf\u5c11\u4eba\u5de5\u76d1\u7763\u3001\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u548c\u6709\u6548\u63a7\u5236\u8bef\u62a5\u7387\u65b9\u9762\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u89c4\u5219\u6216LSTM\u6a21\u578b\u3002"}}
{"id": "2507.09008", "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "categories": ["cs.CV"], "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "VISTA\u662f\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u548c\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u751f\u6210\u6807\u7b7e\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u3001LLaVA\uff09\u867d\u4fc3\u8fdb\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u81ea\u52a8\u6807\u6ce8\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5176\u751f\u6210\u6807\u7b7e\u7684\u8d28\u91cf\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u6570\u91cf\u800c\u975e\u8d28\u91cf\u3002\u5728\u65e0\u771f\u503c\u6570\u636e\u4e0b\u9a8c\u8bc1\u6d77\u91cf\u6570\u636e\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u6027\u6216\u4ec5\u4f9d\u8d56\u5c0f\u6837\u672c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u89e3\u51b3\u6240\u6709\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51faVISTA\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u9488\u5bf9\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u3002VISTA\u6574\u5408\u4e86\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u4e0e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u8bc6\u522b\u3001\u7406\u89e3\u5e76\u7ea0\u6b63\u57fa\u7840\u6a21\u578b\u751f\u6210\u6807\u7b7e\u4e2d\u7684\u9690\u85cf\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8be6\u7ec6\u7528\u4f8b\u548c\u4e13\u5bb6\u8bc4\u5ba1\uff0c\u4ece\u5b9a\u91cf\u548c\u5b9a\u6027\u4e24\u65b9\u9762\u8bc1\u660e\u4e86VISTA\u7684\u6709\u6548\u6027\u3002", "conclusion": "VISTA\u6210\u529f\u89e3\u51b3\u4e86\u9a8c\u8bc1\u548c\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u751f\u6210\u6807\u7b7e\u8d28\u91cf\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u52a8\u5316\u7b56\u7565\u4e0e\u4eba\u7c7b\u6d1e\u5bdf\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\uff09\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09346", "pdf": "https://arxiv.org/pdf/2507.09346", "abs": "https://arxiv.org/abs/2507.09346", "authors": ["Arild Yonkeu", "Mohammadreza Amini", "Burak Kantarci"], "title": "Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks", "categories": ["cs.NI"], "comment": "8 pages, 8 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for\nmeeting the low-latency demands of modern IoT and dynamic task scheduling\nscenarios. MEC reduces the processing burden on resource-constrained devices by\nenabling task execution at nearby edge servers. However, efficient task\nscheduling remains a challenge in dynamic, time-sensitive environments.\nConventional methods -- such as heuristic algorithms and mixed-integer\nprogramming -- suffer from high computational overhead, limiting their\nreal-time applicability. Existing deep learning (DL) approaches offer faster\ninference but often lack scalability and adaptability to dynamic workloads. To\naddress these issues, we propose a Pointer Network-based architecture for task\nscheduling in dynamic edge computing scenarios. Our model is trained on a\ngenerated synthetic dataset using genetic algorithms to determine the optimal\ntask ordering. Experimental results show that our model achieves lower drop\nratios and waiting times than baseline methods, and a soft sequence accuracy of\nup to 89.2%. Our model consistently achieves inference times under 2 seconds\nacross all evaluated task counts, whereas the integer and binary programming\napproaches require approximately up to 18 seconds and 90 seconds, respectively.\nIt also shows strong generalization across varying scenarios, and adaptability\nto real-time changes, offering a scalable and efficient solution for edge-based\ntask management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePointer Network\u7684\u4efb\u52a1\u8c03\u5ea6\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u4e2d\u52a8\u6001\u3001\u65f6\u95f4\u654f\u611f\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u4efb\u52a1\u8c03\u5ea6\u96be\u9898\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728MEC\u4e2d\uff0c\u9ad8\u6548\u7684\u4efb\u52a1\u8c03\u5ea6\u5bf9\u4e8e\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u542f\u53d1\u5f0f\u7b97\u6cd5\u3001\u6df7\u5408\u6574\u6570\u89c4\u5212\uff09\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5219\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePointer Network\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u52a8\u6001\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u8c03\u5ea6\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u786e\u5b9a\u6700\u4f18\u4efb\u52a1\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u4efb\u52a1\u4e22\u5f03\u7387\u548c\u7b49\u5f85\u65f6\u95f4\uff0c\u8f6f\u5e8f\u5217\u51c6\u786e\u7387\u9ad8\u8fbe89.2%\u3002\u5728\u6240\u6709\u8bc4\u4f30\u7684\u4efb\u52a1\u6570\u91cf\u4e0b\uff0c\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u5747\u4fdd\u6301\u57282\u79d2\u4ee5\u5185\uff0c\u8fdc\u4f4e\u4e8e\u6574\u6570\u89c4\u5212\uff08\u7ea618\u79d2\uff09\u548c\u4e8c\u8fdb\u5236\u89c4\u5212\uff08\u7ea690\u79d2\uff09\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8fb9\u7f18\u4efb\u52a1\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5b9e\u65f6\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2507.09075", "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u5927\u89c4\u6a21\u4ee3\u7801\u63a8\u7406\u6570\u636e\u96c6OpenCodeReasoning-II\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u548c\u8bc4\u8bba\u65b9\u9762\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6269\u5c55\u4e86LiveCodeBench\u4ee5\u652f\u6301C++\u8bc4\u4f30\u3002", "motivation": "\u63a8\u7406\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u8bc4\u8bba\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u8fdb\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u3002", "method": "1. \u6784\u5efa\u4e86OpenCodeReasoning-II\u6570\u636e\u96c6\uff0c\u5305\u542b2.5M\u95ee\u7b54\u8bc4\u8bba\u4e09\u5143\u7ec4\uff08\u7ea635K\u72ec\u7acb\u7f16\u7a0b\u95ee\u9898\uff09\u3002\n2. \u91c7\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e13\u6ce8\u4e8e\u4ee3\u7801\u751f\u6210\u5fae\u8c03\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8054\u5408\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u548c\u8bc4\u8bba\u6a21\u578b\u3002\n3. \u57fa\u4e8eQwen2.5-Instruct\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\n4. \u6269\u5c55LiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u652f\u6301C++\u7f16\u7a0b\u8bed\u8a00\u3002", "result": "1. \u5fae\u8c03\u540e\u7684Qwen2.5-Instruct\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6027\u80fd\u8d85\u8d8a\u6216\u6301\u5e73\u73b0\u6709\u6700\u4f73\u5f00\u6e90\u84b8\u998f\u6a21\u578b\u3002\n2. \u4ee3\u7801\u751f\u6210\u548c\u8bc4\u8bba\u6a21\u578b\u7684\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u7ade\u6280\u7f16\u7a0b\u6027\u80fd\u3002\n3. LiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u6269\u5c55\u652f\u6301C++\uff0c\u4fc3\u8fdb\u4e86\u66f4\u5168\u9762\u7684LLM\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u672c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u4ee3\u7801\u751f\u6210\u548c\u8bc4\u8bba\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u672a\u6765\u7684LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5de5\u5177\u3002"}}
{"id": "2507.09329", "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "categories": ["cs.AI", "cs.CR"], "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\u4e86\u57fa\u4e8eLLM\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u663e\u8457\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u5f3a\u8c03\u672a\u6765\u8bbe\u8ba1\u4e2d\u9700\u5173\u6ce8\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u88ab\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u5176\u5b89\u5168\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u53ef\u80fd\u65e0\u610f\u4e2d\u5f15\u5165\u4e0d\u5b89\u5168\u5b9e\u8df5\u3002", "method": "\u5bf9\u4e94\u4e2a\u4e3b\u6d41LLM\u6a21\u578b\uff08GPT-4o, GPT-4.1, Claude\u53d8\u4f53\uff09\u572893\u4e2a\u771f\u5b9e\u8f6f\u4ef6\u8bbe\u7f6e\u4efb\u52a1\u4e2d\u768412,000\u591a\u9879\u64cd\u4f5c\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\uff1b\u5f00\u53d1\u4e86\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u8bc6\u522b\u56db\u7c7b\u4e3b\u8981\u6f0f\u6d1e\uff1b\u5e76\u8bc4\u4f30\u4e86\u53cd\u9988\u673a\u5236\u548c\u5b89\u5168\u63d0\u9192\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b021%\u7684\u4ee3\u7406\u64cd\u4f5c\u8f68\u8ff9\u5305\u542b\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\uff1b\u4fe1\u606f\u6cc4\u9732\uff08CWE-200\uff09\u662f\u6700\u666e\u904d\u7684\u6f0f\u6d1e\u7c7b\u578b\uff1bGPT-4.1\u5728\u7f13\u89e3\u7b56\u7565\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u7f13\u89e3\u6210\u529f\u7387\u8fbe96.8%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u5b89\u5168\u7684\u7b2c\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u4e0b\u4e00\u4ee3\u57fa\u4e8eLLM\u7684\u7f16\u7801\u4ee3\u7406\u5728\u8bbe\u8ba1\u65f6\u5fc5\u987b\u878d\u5165\u5b89\u5168\u610f\u8bc6\u3002"}}
{"id": "2507.08836", "pdf": "https://arxiv.org/pdf/2507.08836", "abs": "https://arxiv.org/abs/2507.08836", "authors": ["Damien Fovet", "Shashank Chamoli", "Sarah Oury", "Srishti Singhal"], "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86CompactifAI\u538b\u7f29\u65b9\u6cd5\u5e94\u7528\u4e8eLlama 3.1 8B\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08CompactifAI\uff09\uff0c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama 3.1 8B\uff09\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u5c06CompactifAI\u538b\u7f29\u65b9\u6cd5\u5e94\u7528\u4e8eLlama 3.1 8B\u6a21\u578b\u3002\u4f7f\u7528Codecarbon\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u80fd\u8017\u6548\u7387\uff0c\u4f7f\u7528Ragas\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u51c6\u786e\u6027\u3002\u5c06\u538b\u7f29\u540e\u7684\u6a21\u578b\u4e0e\u539f\u59cb\u5168\u5c3a\u5bf8\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528CompactifAI\u538b\u7f29\u540e\u7684\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CompactifAI\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8Llama 3.1 8B\u6a21\u578b\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u51c6\u786e\u6027\u3002"}}
{"id": "2507.09036", "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik M\u00f6ller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite\u662f\u4e00\u4e2aPython\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u7b80\u5316\u548c\u81ea\u52a8\u5316\u8111\u90e8\u75c5\u7076\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u7684\u6784\u5efa\uff0c\u63d0\u4f9b\u4ece\u9884\u5904\u7406\u5230\u5206\u5272\u548c\u6027\u80fd\u8bc4\u4f30\u7684\u5168\u9762\u529f\u80fd\uff0c\u65e8\u5728\u63d0\u4f9b\u201c\u65e0\u8111\u201d\u7684\u5f00\u53d1\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u3001\u6a21\u5757\u5316\u7684Python\u5de5\u5177\u5305\uff0c\u6700\u5c0f\u5316\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u7b80\u5316\u4e34\u5e8a\u548c\u79d1\u5b66\u5b9e\u8df5\u4e2d\u590d\u6742\u8111\u90e8\u75c5\u7076\u56fe\u50cf\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u7684\u521b\u5efa\u3002", "method": "\u9075\u5faaPythonic\u539f\u5219\u8bbe\u8ba1\uff0c\u5305\u542b\u4e00\u4e2a\u53ef\u9002\u5e94\u7684\u9884\u5904\u7406\u6a21\u5757\uff0c\u53ef\u6267\u884c\u5171\u914d\u51c6\u3001\u56fe\u8c31\u914d\u51c6\u3001\u9885\u9aa8\u5265\u79bb\u548c\u9762\u90e8\u53bb\u9664\u3002\u8be5\u5de5\u5177\u5305\u5229\u7528BraTS\u6311\u6218\u8d5b\u7684\u7b97\u6cd5\u5408\u6210\u7f3a\u5931\u6a21\u6001\u3001\u4fee\u590d\u75c5\u7076\u5e76\u751f\u6210\u80bf\u7624\u5206\u5272\uff0c\u5e76\u96c6\u6210panoptica\u7b49\u5de5\u5177\u6765\u91cf\u5316\u5206\u5272\u6a21\u578b\u6027\u80fd\uff08\u5982\u75c5\u7076\u7ea7\u6307\u6807\uff09\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u5de5\u5177\u5305\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u7684\u8111\u90e8\u75c5\u7076\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\uff0c\u5305\u62ec\u591a\u6a21\u6001\u56fe\u50cf\u7684\u9884\u5904\u7406\u3001\u7f3a\u5931\u6a21\u6001\u5408\u6210\u3001\u75c5\u7076\u4fee\u590d\u548c\u75c5\u7406\u7279\u5f02\u6027\u80bf\u7624\u5206\u5272\uff0c\u4ee5\u53ca\u5206\u5272\u6a21\u578b\u6027\u80fd\u91cf\u5316\u3002\u5b83\u7b80\u5316\u4e86\u5f00\u53d1\u8fc7\u7a0b\uff0c\u5e76\u53ef\u9002\u5e94\u5176\u4ed6\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5e94\u7528\u3002", "conclusion": "BrainLesion Suite\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5de5\u5177\u5305\uff0c\u663e\u8457\u7b80\u5316\u4e86\u8111\u90e8\u75c5\u7076\u56fe\u50cf\u5206\u6790\u7ba1\u9053\u7684\u5f00\u53d1\uff0c\u901a\u8fc7\u5176\u6a21\u5757\u5316\u548c\u6613\u7528\u6027\uff0c\u6709\u671b\u5728\u66f4\u5e7f\u6cdb\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u53d1\u6325\u4f5c\u7528\u3002\u5176\u4ee3\u7801\u548c\u6559\u7a0b\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002"}}
{"id": "2507.09352", "pdf": "https://arxiv.org/pdf/2507.09352", "abs": "https://arxiv.org/abs/2507.09352", "authors": ["Ghazal Asemian", "Mohammadreza Amini", "Burak Kantarci"], "title": "Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling", "categories": ["cs.NI", "eess.SP"], "comment": "5 pages, 2 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Mobile Edge Computing (MEC) enables low-latency applications by bringing\ncomputation closer to the user, but dynamic task arrivals and communication\nthreats like jamming complicate reliable task offloading and resource\nallocation. In this paper, we formulate a dynamic MEC framework considering the\ntransmission diversity that jointly addresses task scheduling and resource\nblock (RB) assignment in the presence of jamming. First, we define and evaluate\nkey network metrics-including dropped task ratio and bandwidth\nutilization-while maintaining service continuity by accounting for the existing\ncommitments of the edge server to previously offloaded tasks. Then, we propose\na jamming-aware offloading and RB allocation framework that leverages\ntransmission diversity and optimal scheduling across distributed gNBs. The\nproposed solution is compared to a similar scenario without transmission\ndiversity and two baseline strategies of first-come-first-served (FCFS) and\nshortest task first (STF). The proposed algorithm effectively mitigates the\nimpact of jamming while enhancing resource utilization and minimizing task drop\nrates, making it highly suitable for mission-critical MEC applications. At\nsignal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves\na $0.26$ task drop rate, outperforming the scenario without transmission\ndiversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52\nand 0.63 task drop rates, respectively.", "AI": {"tldr": "\u9488\u5bf9\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u4e2d\u52a8\u6001\u4efb\u52a1\u548c\u5e72\u6270\uff08\u5982\u963b\u585e\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u4f20\u8f93\u5206\u96c6\u7684\u52a8\u6001MEC\u6846\u67b6\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u4efb\u52a1\u8c03\u5ea6\u548c\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4efb\u52a1\u4e22\u5f03\u7387\u5e76\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u867d\u80fd\u901a\u8fc7\u8ba1\u7b97\u9760\u8fd1\u7528\u6237\u6765\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u52a8\u6001\u4efb\u52a1\u5230\u8fbe\u548c\u5982\u5e72\u6270\u7b49\u901a\u4fe1\u5a01\u80c1\uff0c\u4f7f\u5f97\u53ef\u9760\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u53d8\u5f97\u590d\u6742\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001MEC\u6846\u67b6\uff0c\u8003\u8651\u4f20\u8f93\u5206\u96c6\uff0c\u5728\u5b58\u5728\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u8054\u5408\u5904\u7406\u4efb\u52a1\u8c03\u5ea6\u548c\u8d44\u6e90\u5757\uff08RB\uff09\u5206\u914d\u3002\u8be5\u6846\u67b6\u5b9a\u4e49\u5e76\u8bc4\u4f30\u4e86\u5173\u952e\u7f51\u7edc\u6307\u6807\uff08\u5982\u4e22\u5f03\u4efb\u52a1\u7387\u3001\u5e26\u5bbd\u5229\u7528\u7387\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u8003\u8651\u8fb9\u7f18\u670d\u52a1\u5668\u5bf9\u73b0\u6709\u4efb\u52a1\u7684\u627f\u8bfa\u6765\u4fdd\u6301\u670d\u52a1\u8fde\u7eed\u6027\u3002\u8be5\u65b9\u6848\u5229\u7528\u4f20\u8f93\u5206\u96c6\u548c\u5206\u5e03\u5f0fgNBs\u95f4\u7684\u4f18\u5316\u8c03\u5ea6\uff0c\u5e76\u4e0e\u65e0\u4f20\u8f93\u5206\u96c6\u573a\u666f\u3001\u5148\u6765\u5148\u670d\u52a1\uff08FCFS\uff09\u548c\u6700\u77ed\u4efb\u52a1\u4f18\u5148\uff08STF\uff09\u7b49\u57fa\u7ebf\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5e72\u6270\u5f71\u54cd\uff0c\u589e\u5f3a\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u4e86\u4efb\u52a1\u4e22\u5f03\u7387\u3002\u5728\u4fe1\u566a\u6bd4\uff08SJNR\uff09\u4e3a4 dB\u65f6\uff0c\u6240\u63d0\u65b9\u6cd5\u7684\u4efb\u52a1\u4e22\u5f03\u7387\u4e3a0.26\uff0c\u4f18\u4e8e\u65e0\u4f20\u8f93\u5206\u96c6\u573a\u666f\uff080.50\uff09\u3001STF\uff080.52\uff09\u548cFCFS\uff080.63\uff09\u3002", "conclusion": "\u8be5\u63d0\u51fa\u7684\u7b97\u6cd5\u9ad8\u5ea6\u9002\u7528\u4e8e\u4efb\u52a1\u5173\u952e\u578bMEC\u5e94\u7528\uff0c\u56e0\u4e3a\u5b83\u80fd\u5728\u5b58\u5728\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u964d\u4f4e\u4efb\u52a1\u4e22\u5f03\u7387\u5e76\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.09076", "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "\u9488\u5bf9\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08SLLM\uff09\u5728\u957f\u97f3\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u4e0a\u4e0b\u6587\u7a97\u53e3\u53d7\u9650\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08DPM\uff09\u673a\u5236\u3002DPM\u901a\u8fc7LoRA\u6a21\u5757\u52a8\u6001\u8bb0\u5fc6\u53e5\u5b50\u7ea7\u4e0a\u4e0b\u6587\u8bed\u4e49\u548c\u60c5\u611f\uff0c\u4f7fSLLM\u80fd\u5904\u7406\u65e0\u9650\u957f\u97f3\u9891\u3002\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\uff0cDPM\u663e\u8457\u63d0\u5347\u4e86SLLM\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08SLLM\uff09\u5e94\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u65f6\uff0c\u56e0\u8bed\u97f3\u6a21\u6001\u7684\u9ad8\u5e27\u7387\uff0c\u5176\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e25\u91cd\u9650\u5236\u4e86\u5bf9\u957f\u97f3\u9891\u7684\u5904\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0cSLLM\u7684\u8f93\u5165token\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u60c5\u611f\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8fde\u7eed\u6027\u548c\u60ef\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08Dynamic Parameter Memory, DPM\uff09\u673a\u5236\u3002DPM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u53e5\u5b50\u7ea7\u4fe1\u606f\u548c\u60c5\u611f\u6e10\u8fdb\u5f0f\u7f16\u7801\u5230\u4e34\u65f6\u7684LoRA\u6a21\u5757\u4e2d\uff0c\u4ece\u800c\u6709\u6548\u201c\u8bb0\u5fc6\u201d\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u7814\u7a76\u8005\u8bad\u7ec3\u4e86\u4e00\u4e2a\u60c5\u611fSLLM\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\uff0c\u5e76\u5728\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u7684\u63a8\u7406\u9636\u6bb5\u6574\u5408\u4e86DPM\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPM\u663e\u8457\u63d0\u5347\u4e86SLLM\u5728\u5904\u7406\u957f\u97f3\u9891\u5e8f\u5217\u65f6\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08State-of-the-Art, SOTA\uff09\u7684\u6027\u80fd\u3002", "conclusion": "DPM\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86SLLM\u5728\u5904\u7406\u957f\u97f3\u9891\u60c5\u611f\u8bc6\u522b\u65f6\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u9650\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u4e0a\u4e0b\u6587\u8bed\u4e49\u548c\u60c5\u611f\uff0c\u663e\u8457\u63d0\u5347\u4e86SLLM\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86SOTA\u8868\u73b0\u3002"}}
{"id": "2507.09369", "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "\u62a5\u544a\u5206\u7c7b\u5e76\u5217\u4e3e\u4e86AI\u53ef\u80fd\u5bfc\u81f4\u7684\u5168\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\uff0c\u65e8\u5728\u4fc3\u6210\u9884\u9632\u63aa\u65bd\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u516c\u5f00\u5c55\u793aAI\u53ef\u80fd\u5bfc\u81f4\u7684\u5168\u4eba\u7c7b\u706d\u7edd\u60c5\u666f\uff0c\u4ee5\u4e89\u53d6\u516c\u4f17\u652f\u6301\uff0c\u4fc3\u4f7f\u673a\u6784\u91c7\u53d6\u9884\u9632\u63aa\u65bd\uff0c\u89c4\u907fAI\u5e26\u6765\u7684\u707e\u96be\u6027\u98ce\u9669\u3002", "method": "\u8be5\u62a5\u544a\u91c7\u7528\u7684\u65b9\u6cd5\u662f\u63d0\u51faAI\u53ef\u80fd\u5bfc\u81f4\u7684\u5168\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\u7684\u5206\u7c7b\uff08\u5206\u7c7b\u6cd5\uff09\u5e76\u63d0\u4f9b\u5177\u4f53\u793a\u4f8b\u3002", "result": "\u62a5\u544a\u8bc6\u522b\u5e76\u5206\u7c7b\u4e86AI\u53ef\u80fd\u5bfc\u81f4\u51e0\u4e4e\u6240\u6709\u4eba\u7c7b\u706d\u7edd\u7684\u6f5c\u5728\u60c5\u666f\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u662f\u53ef\u907f\u514d\u7684\u53ef\u80fd\u6027\u800c\u975e\u5fc5\u7136\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u8fd9\u4e9b\u53ef\u80fd\u6027\uff0c\u62a5\u544a\u5e0c\u671b\u652f\u6301\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u4ee5\u5e94\u5bf9AI\u5e26\u6765\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u5f3a\u8c03\u5176\u53ef\u907f\u514d\u6027\u3002"}}
{"id": "2507.08838", "pdf": "https://arxiv.org/pdf/2507.08838", "abs": "https://arxiv.org/abs/2507.08838", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Preprint", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a `wd1` \u7684\u65b0\u9896\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u4f3c\u7136\u51fd\u6570\u8fd1\u4f3c\uff0c\u89e3\u51b3\u6269\u6563\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6240\u9762\u4e34\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u504f\u5dee\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347dLLMs\u7684\u63a8\u7406\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6269\u6563\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u96be\u9898\u3002\u73b0\u6709RL\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u8fd1\u4f3cdLLMs\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u5b58\u5728\u6f5c\u5728\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u91cd\u8981\u6027\u91c7\u6837\u7684\u7b56\u7565\u6bd4\u7387\u5206\u6bcd\u4e2d\u51fa\u73b0\u8fd1\u4f3c\u8bef\u5dee\u65f6\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a `wd1` \u7684\u65b0\u578b\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u76ee\u6807\u51fd\u6570\u91cd\u6784\u4e3a\u52a0\u6743\u4f3c\u7136\u5f62\u5f0f\uff0c\u4ec5\u9700\u5bf9\u5f53\u524d\u53c2\u6570\u5316\u7b56\u7565\u4f3c\u7136\u8fdb\u884c\u5355\u6b21\u8fd1\u4f3c\uff0c\u4ece\u800c\u907f\u514d\u4e86\u591a\u6b21\u8fd1\u4f3c\u5e26\u6765\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c`wd1` \u5728\u5e38\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u4efb\u4f55\u76d1\u7763\u6570\u636e\uff0c\u5176\u51c6\u786e\u7387\u6bd4\u73b0\u6709RL\u65b9\u6cd5\u9ad8\u51fa16%\u3002\u6b64\u5916\uff0c`wd1` \u8fd8\u5e26\u6765\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\uff0c\u5305\u62ec\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\u4ee5\u53ca\u6bcf\u4e2a\u68af\u5ea6\u6b65\u4e2d\u66f4\u5c11\u7684\u529f\u80fd\u8bc4\u4f30\u6b21\u6570\uff08NFEs\uff09\u3002", "conclusion": "`wd1` \u56e0\u5176\u5353\u8d8a\u7684\u6027\u80fd\u3001\u8ba1\u7b97\u6548\u7387\u3001\u5b9e\u73b0\u7b80\u6613\u6027\u4ee5\u53ca\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u7684\u7279\u6027\uff0c\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u9ad8\u6548\u7684\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8edLLMs\u63a8\u7406\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.09052", "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u6570\u636e\u4e0b\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5bf9\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u751f\u6210\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u901a\u8fc7\u589e\u52a0\u56fe\u50cf\u4e0d\u76f8\u4f3c\u6027\u53ca\u5f15\u5165\u6761\u4ef6-\u65e0\u6761\u4ef6\u751f\u6210\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c3e\u90e8\u7c7b\u522b\u7684\u591a\u6837\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5e38\u5448\u957f\u5c3e\u5206\u5e03\uff0c\u5bfc\u81f4\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u6570\u91cf\u6709\u9650\uff0c\u8fdb\u800c\u5f15\u53d1\u6a21\u5f0f\u574d\u584c\u5e76\u964d\u4f4e\u5408\u6210\u56fe\u50cf\u591a\u6837\u6027\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u957f\u5c3e\u6570\u636e\u8bad\u7ec3\u7684\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5bf9\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u7684\u751f\u6210\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5934\u90e8\u7c7b\u522b\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff1a1. \u65e0\u76d1\u7763InfoNCE\u635f\u5931\uff0c\u5229\u7528\u8d1f\u6837\u672c\u589e\u52a0\u5408\u6210\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u95f4\u7684\u8ddd\u79bb/\u4e0d\u76f8\u4f3c\u6027\u30022. MSE\u635f\u5931\uff0c\u5728\u5927\u65f6\u95f4\u6b65\u957f\u4e0b\u5bf9\u6bd4\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u4e0e\u65e0\u6761\u4ef6\u751f\u6210\uff0c\u4f7f\u53bb\u566a\u8fc7\u7a0b\u5728\u521d\u59cb\u9636\u6bb5\u5bf9\u7c7b\u522b\u6761\u4ef6\u4e0d\u654f\u611f\uff0c\u4ece\u800c\u901a\u8fc7\u5934\u90e8\u7c7b\u522b\u77e5\u8bc6\u5171\u4eab\u6765\u4e30\u5bcc\u5c3e\u90e8\u7c7b\u522b\u3002\u8fd9\u662f\u9996\u6b21\u5c06\u6b64\u7c7b\u6761\u4ef6-\u65e0\u6761\u4ef6\u5bf9\u9f50\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u5728CIFAR10/100-LT\u3001PlacesLT\u3001TinyImageNetLT\u548cImageNetLT\u7b49\u591a\u4e2a\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6DDPM\u4ee5\u53ca\u5176\u4ed6\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5c06\u5bf9\u6bd4\u5b66\u4e60\u5e94\u7528\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2507.09462", "pdf": "https://arxiv.org/pdf/2507.09462", "abs": "https://arxiv.org/abs/2507.09462", "authors": ["Haoye Chai", "Yuan Yuan", "Yong Li"], "title": "MobiWorld: World Models for Mobile Wireless Network", "categories": ["cs.NI"], "comment": "7 pages, 6 figures", "summary": "Accurate modeling and simulation of mobile networks are essential for\nenabling intelligent and cost-effective network optimization. In this paper, we\npropose MobiWorld, a generative world model designed to support high-fidelity\nand flexible environment simulation for mobile network planning and\noptimization. Unlike traditional predictive models constrained by limited\ngeneralization capabilities, MobiWorld exhibits strong universality by\nintegrating heterogeneous data sources, including sensors, mobile devices, and\nbase stations, as well as multimodal data types such as sequences and images.\nIt is capable of generating both network element-level observations (e.g.,\ntraffic load, user distribution) and system-level performance indicators (e.g.,\nthroughput, energy consumption) to support a wide range of planning and\noptimization tasks. Built upon advanced diffusion models, MobiWorld offers\npowerful controllable generation capabilities by modeling the joint\ndistribution between mobile network data and diverse conditional factors\nincluding spatio temporal contexts, user behaviors, and optimization policies.\nThis enables accurate simulation of dynamic network states under varying policy\nconfigurations, providing optimization agents with precise environmental\nfeedback and facilitating effective decision-making without relying on costly\nreal-network interactions. We demonstrate the effectiveness of MobiWorld in a\ncollaborative energy-saving scenario, where an agent uses observations and\nrewards generated by MobiWorld to optimize base station sleep and user\noffloading policies. Experimental results show that MobiWorld exhibits strong\ncontrollable generation performance and outperforms traditional methods in\nenergy optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMobiWorld\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u65e8\u5728\u4e3a\u79fb\u52a8\u7f51\u7edc\u89c4\u5212\u548c\u4f18\u5316\u63d0\u4f9b\u9ad8\u4fdd\u771f\u3001\u7075\u6d3b\u7684\u4eff\u771f\u73af\u5883\u3002\u5b83\u80fd\u6574\u5408\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff0c\u751f\u6210\u591a\u5c42\u6b21\u7f51\u7edc\u6307\u6807\uff0c\u5e76\u652f\u6301\u53ef\u63a7\u4eff\u771f\u4ee5\u4f18\u5316\u7f51\u7edc\u7b56\u7565\uff0c\u5728\u8282\u80fd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u7684\u7cbe\u786e\u5efa\u6a21\u548c\u4eff\u771f\u5bf9\u5b9e\u73b0\u667a\u80fd\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u7f51\u7edc\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7684\u7f51\u7edc\u4f18\u5316\u9700\u6c42\u3002", "method": "MobiWorld\u662f\u4e00\u4e2a\u57fa\u4e8e\u5148\u8fdb\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u6574\u5408\u4f20\u611f\u5668\u3001\u79fb\u52a8\u8bbe\u5907\u3001\u57fa\u7ad9\u7b49\u5f02\u6784\u6570\u636e\u6e90\u4ee5\u53ca\u5e8f\u5217\u548c\u56fe\u50cf\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u666e\u9002\u6027\u3002\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u7f51\u7edc\u5143\u7d20\u7ea7\u89c2\u6d4b\uff08\u5982\u6d41\u91cf\u8d1f\u8f7d\u3001\u7528\u6237\u5206\u5e03\uff09\u548c\u7cfb\u7edf\u7ea7\u6027\u80fd\u6307\u6807\uff08\u5982\u541e\u5410\u91cf\u3001\u80fd\u8017\uff09\u3002\u5176\u6838\u5fc3\u662f\u901a\u8fc7\u5efa\u6a21\u79fb\u52a8\u7f51\u7edc\u6570\u636e\u4e0e\u65f6\u7a7a\u80cc\u666f\u3001\u7528\u6237\u884c\u4e3a\u3001\u4f18\u5316\u7b56\u7565\u7b49\u6761\u4ef6\u56e0\u7d20\u7684\u8054\u5408\u5206\u5e03\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u53ef\u63a7\u751f\u6210\u80fd\u529b\u3002", "result": "\u7814\u7a76\u5728\u534f\u4f5c\u5f0f\u8282\u80fd\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86MobiWorld\u7684\u6709\u6548\u6027\uff0c\u4f18\u5316\u4ee3\u7406\u5229\u7528MobiWorld\u751f\u6210\u7684\u89c2\u6d4b\u548c\u5956\u52b1\u6765\u4f18\u5316\u57fa\u7ad9\u4f11\u7720\u548c\u7528\u6237\u5206\u6d41\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMobiWorld\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ef\u63a7\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5728\u80fd\u91cf\u4f18\u5316\u65b9\u9762\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MobiWorld\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u4fdd\u771f\u3001\u7075\u6d3b\u7684\u79fb\u52a8\u7f51\u7edc\u4eff\u771f\u65b9\u6848\uff0c\u901a\u8fc7\u4e3a\u4f18\u5316\u4ee3\u7406\u63d0\u4f9b\u7cbe\u786e\u7684\u73af\u5883\u53cd\u9988\uff0c\u4fc3\u8fdb\u4e86\u6709\u6548\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u771f\u5b9e\u7f51\u7edc\u4ea4\u4e92\uff0c\u4e3a\u667a\u80fd\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.09104", "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCompassJudger-2\uff0c\u4e00\u4e2a\u901a\u7528\u7684LLM\u5224\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u3001\u591a\u9886\u57df\u6570\u636e\u7b56\u7565\u548c\u6539\u8fdb\u5b66\u4e60\u76ee\u6807\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5224\u522b\u6a21\u578b\u4e13\u4e1a\u5316\u72ed\u7a84\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u5c40\u9650\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u51faJudgerBenchV2\u4f5c\u4e3a\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u4f5c\u4e3a\u5224\u522b\u7684LLM\u6a21\u578b\u5b58\u5728\u4e13\u4e1a\u5316\u72ed\u7a84\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86CompassJudger-2\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u5224\u522b\u6a21\u578b\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1. \u91c7\u7528\u4efb\u52a1\u9a71\u52a8\u3001\u591a\u9886\u57df\u6570\u636e\u7b56\u5c55\u7b56\u7565\u30022. \u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u76d1\u7763\u5224\u522b\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u62d2\u7edd\u91c7\u6837\u5f15\u5bfc\u5185\u5728\u6279\u5224\u6027\u63a8\u7406\u30023. \u5f15\u5165\u4e86\u5e26\u6709\u8fb9\u9645\u7b56\u7565\u68af\u5ea6\u635f\u5931\u7684\u6539\u8fdb\u5b66\u4e60\u76ee\u6807\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86JudgerBenchV2\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u8de8\u9886\u57df\u5224\u522b\u51c6\u786e\u6027\u548c\u6392\u540d\u4e00\u81f4\u6027\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "CompassJudger-2\u5728\u591a\u4e2a\u5224\u522b\u548c\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6210\u679c\u3002\u51767B\u6a21\u578b\u5c55\u73b0\u51fa\u4e0eDeepSeek-V3\u548cQwen3-235B-A22B\u7b49\u5927\u6a21\u578b\u76f8\u5f53\u7684\u5224\u522b\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u63a8\u52a8\u4e86LLM\u5224\u522b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2507.09374", "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "categories": ["cs.AI", "I.2.6; I.2.10"], "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\uff08\u7279\u522b\u662f\u591a\u6b65\u3001\u53ef\u89e3\u91ca\u63a8\u7406\uff09\u4e2d\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u7aef\u5230\u7aef\u6846\u67b6EduFlow\uff0c\u5305\u542b\u8fc7\u7a0b\u611f\u77e5\u5956\u52b1\u6a21\u578bEduPRM\u548c\u9886\u57df\u81ea\u9002\u5e94\u641c\u7d22\u6846\u67b6EduMCTS\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u6b65\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u573a\u666f\u3002\u5176\u4e3b\u8981\u9650\u5236\u5305\u62ec\uff1a\u7f3a\u4e4f\u79d1\u5b66\u63a8\u7406\u6a21\u5f0f\u3001\u591a\u6b65\u63a8\u7406\u4e2d\u7f3a\u4e4f\u5168\u5c40\u8fde\u8d2f\u6027\u3001\u4ee5\u53ca\u6ca1\u6709\u53cd\u601d\u6027\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u7ed3\u6784\u5316\u79d1\u5b66\u8bed\u5883\u4e0b\u4e0d\u53ef\u9760\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86EduFlow\uff0c\u4e00\u4e2a\u8986\u76d6\u6559\u80b2\u79d1\u5b66\u63a8\u7406\u5168\u6d41\u7a0b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u9009\u62e9\u3001\u57fa\u4e8eMCTS\u7684\u8f68\u8ff9\u6784\u5efa\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8f93\u51fa\u4f18\u5316\u3002\u5176\u6838\u5fc3\u662fEduPRM\uff0c\u4e00\u4e2a\u8fc7\u7a0b\u611f\u77e5\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u7b7e\u548c\u7406\u7531\u6279\u5224\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u5229\u7528MCTS\u8f68\u8ff9\u3001\u9519\u8bef\u6ce8\u5165\u6279\u5224\u548c\u5e08\u751f\u5bf9\u8bdd\u4e09\u79cd\u4e92\u8865\u76d1\u7763\u6e90\u8fdb\u884c\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u3002\u540c\u65f6\u63d0\u51faEduMCTS\uff0c\u4e00\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u641c\u7d22\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u81ea\u6211\u53cd\u601d\u7b49\u5f15\u5bfc\u52a8\u4f5c\u4ee5\u4fc3\u8fdb\u9519\u8bef\u7ea0\u6b63\uff0c\u5e76\u5229\u7528EduPRM\u7684\u7ec6\u7c92\u5ea6\u53cd\u9988\u6307\u5bfc\u641c\u7d22\u3002\u901a\u8fc7\u81ea\u6d3d\u6027\u548c\u62d2\u7edd\u91c7\u6837\uff0c\u6784\u5efa\u4e86EduMCTS-160K\u5927\u578b\u6559\u80b2\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEduFlow\u663e\u8457\u589e\u5f3a\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165EduFlow\u6846\u67b6\u53ca\u5176\u521b\u65b0\u7ec4\u4ef6EduPRM\u548cEduMCTS\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u6559\u80b2\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u591a\u6b65\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.08839", "pdf": "https://arxiv.org/pdf/2507.08839", "abs": "https://arxiv.org/abs/2507.08839", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": "MICCAI 2025", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "AI": {"tldr": "\u9488\u5bf9\u8def\u6613\u4f53\u75c5(LBD)\u8bca\u65ad\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u53ef\u8fc1\u79fb\u6027\u611f\u77e5Transformer (TAT)\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4ece\u963f\u5c14\u8328\u6d77\u9ed8\u75c5(AD)\u6570\u636e\u4e2d\u81ea\u9002\u5e94\u5730\u8fc1\u79fb\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86LBD\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u8def\u6613\u4f53\u75c5(LBD)\u8bca\u65ad\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u800c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5(AD)\u6570\u636e\u76f8\u5bf9\u4e30\u5bcc\u3002\u7136\u800c\uff0cLBD\u548cAD\u6570\u636e\u5b58\u5728\u660e\u663e\u7684\u57df\u504f\u79fb\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u7f13\u89e3\u57df\u504f\u79fb\u7684\u540c\u65f6\u6709\u6548\u5229\u7528AD\u6570\u636e\u6765\u589e\u5f3aLBD\u8bca\u65ad\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u53ef\u8fc1\u79fb\u6027\u611f\u77e5Transformer (TAT)\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u7ed3\u6784\u78c1\u5171\u632f(sMRI)\u5bfc\u51fa\u7684\u7ed3\u6784\u8fde\u63a5(SC)\u6570\u636e\u4e2d\u5b66\u4e60\u3002TAT\u80fd\u81ea\u9002\u5e94\u5730\u4e3a\u75be\u75c5\u53ef\u8fc1\u79fb\u7279\u5f81\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u540c\u65f6\u6291\u5236\u7279\u5b9a\u9886\u57df\u7279\u5f81\uff0c\u4ee5\u51cf\u5c11\u57df\u504f\u79fb\u5e76\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684TAT\u6a21\u578b\u662f\u6709\u6548\u7684\u3002\u5b83\u5728\u6709\u9650\u7684LBD\u6570\u636e\u4e0b\uff0c\u901a\u8fc7\u4eceAD\u6570\u636e\u8fc1\u79fb\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u6761\u4ef6\u4e0b\u4eceAD\u5230LBD\u7684\u57df\u9002\u5e94\u8bca\u65ad\uff0c\u4e3a\u7f55\u89c1\u75be\u75c5\u7684\u57df\u9002\u5e94\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2507.09068", "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u4f5c\u4e3a\u591a\u5a92\u4f53\u7814\u7a76\u7684\u65b0\u524d\u6cbf\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u6a21\u578b\u5728\u5904\u7406\u6781\u957f\u751a\u81f3\u65e0\u5c3d\u89c6\u9891\u5185\u5bb9\u65f6\u9762\u4e34\u7684\u8ba1\u7b97\u3001\u5185\u5b58\u53ca\u8fde\u8d2f\u6027\u6311\u6218\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1LLMs\u548cMLLMs\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5206\u949f\u6216\u5c0f\u65f6\u4ee5\u4e0a\u957f\u89c6\u9891\u65f6\uff0c\u4ecd\u9762\u4e34\u4e25\u91cd\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9650\u5236\uff0c\u96be\u4ee5\u7ef4\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u3001\u8ffd\u8e2a\u590d\u6742\u4e8b\u4ef6\u548c\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002", "method": "\u672c\u6587\u4f5c\u4e3a\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u901a\u8fc7\u5206\u6790\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u8fd9\u4e00\u84dd\u5929\u7814\u7a76\u76ee\u6807\u3002\u5b83\u6982\u8ff0\u4e86\u5b9e\u73b0\u6b64\u80fd\u529b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u8bb0\u5fc6\u673a\u5236\u548c\u5206\u5c42\u81ea\u9002\u5e94\u8868\u793a\u7b49\u3002", "result": "\u672c\u6587\u7684\u4e3b\u8981\u201c\u7ed3\u679c\u201d\u662f\u660e\u786e\u63d0\u51fa\u4e86\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u8fd9\u4e00\u613f\u666f\u548c\u7814\u7a76\u76ee\u6807\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u6b64\u76ee\u6807\u6240\u9700\u7684\u5173\u952e\u6280\u672f\u9886\u57df\u548c\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u591a\u5a92\u4f53\u548cAI\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5f15\u3002", "conclusion": "\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u88ab\u786e\u7acb\u4e3a\u591a\u5a92\u4f53\u548c\u66f4\u5e7f\u6cdbAI\u7814\u7a76\u7684\u4e0b\u4e00\u4e2a\u91cd\u8981\u524d\u6cbf\uff0c\u5b83\u5c06\u9a71\u52a8\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u8bb0\u5fc6\u7b49\u9886\u57df\u7684\u521b\u65b0\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6301\u7eed\u5904\u7406\u548c\u7406\u89e3\u4efb\u610f\u957f\u5ea6\u7684\u89c6\u9891\u6570\u636e\u3002"}}
{"id": "2507.09613", "pdf": "https://arxiv.org/pdf/2507.09613", "abs": "https://arxiv.org/abs/2507.09613", "authors": ["Giovanni Geraci", "Francesca Meneghello", "Francesc Wilhelmi", "David Lopez-Perez", "I\u00f1aki Val", "Lorenzo Galati Giordano", "Carlos Cordeiro", "Monisha Ghosh", "Edward Knightly", "Boris Bellalta"], "title": "Wi-Fi: Twenty-Five Years and Counting", "categories": ["cs.NI", "cs.IT", "eess.SP", "math.IT"], "comment": "39 pages, 28 figures, 3 tables", "summary": "Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding\nname, today's Wi-Fi boasts entirely new capabilities that were not even on the\nroadmap 25 years ago. This article aims to provide a holistic and comprehensive\ntechnical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi\n1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial\narticle to span these eight generations. Rather than a generation-by-generation\nexposition, we describe the key mechanisms that have advanced Wi-Fi. We begin\nby discussing spectrum allocation and coexistence, and detailing the IEEE\n802.11 standardization cycle. Second, we provide an overview of the physical\nlayer and describe key elements that have enabled data rates to increase by\nover 1,000x. Third, we describe how Wi-Fi Medium Access Control has been\nenhanced from the original Distributed Coordination Function to now include\ncapabilities spanning from frame aggregation to wideband spectrum access.\nFourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and\nintroduced multi-user access. Fifth, given the increasing use of mobile,\nbattery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the\ngenerations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate\nspectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,\nreliability, and latency. Finally, we describe how Wi-Fi enables nearby Access\nPoints to coordinate in order to improve performance and efficiency. In the\nAppendix, we further discuss Wi-Fi developments beyond 802.11bn, including\nintegrated mmWave operations, sensing, security and privacy extensions, and the\nadoption of AI/ML.", "AI": {"tldr": "\u672c\u6587\u5bf9Wi-Fi\u4eceIEEE 802.11b (Wi-Fi 1)\u5230802.11bn (Wi-Fi 8)\u768425\u5e74\u6280\u672f\u6f14\u8fdb\u8fdb\u884c\u4e86\u5168\u9762\u800c\u6df1\u5165\u7684\u6559\u7a0b\u5f0f\u56de\u987e\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5173\u952e\u673a\u5236\u800c\u975e\u6309\u4ee3\u5212\u5206\u3002", "motivation": "Wi-Fi\u6280\u672f\u572825\u5e74\u95f4\u53d6\u5f97\u4e86\u5de8\u5927\u53d1\u5c55\uff0c\u65b0\u589e\u4e86\u8bb8\u591a\u65e9\u671f\u89c4\u5212\u4e2d\u672a\u6709\u7684\u529f\u80fd\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u7bc7\u5168\u9762\u6db5\u76d6Wi-Fi\u516b\u4e2a\u4e16\u4ee3\u6280\u672f\u548c\u5386\u53f2\u6f14\u8fdb\u7684\u6559\u7a0b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63d0\u4f9b\u4e00\u4e2a\u6574\u4f53\u800c\u6df1\u5165\u7684\u5206\u6790\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u63cf\u8ff0\u63a8\u52a8Wi-Fi\u53d1\u5c55\u7684\u4e03\u4e2a\u5173\u952e\u673a\u5236\u6765\u5c55\u5f00\uff0c\u800c\u975e\u9010\u4ee3\u9610\u8ff0\u3002\u8fd9\u4e9b\u673a\u5236\u5305\u62ec\uff1a\u9891\u8c31\u5206\u914d\u4e0e\u6807\u51c6\u5316\u5468\u671f\u3001\u7269\u7406\u5c42\uff08PHY\uff09\u589e\u5f3a\u3001\u4ecb\u8d28\u8bbf\u95ee\u63a7\u5236\uff08MAC\uff09\u6539\u8fdb\u3001\u591a\u7528\u6237\u8bbf\u95ee\u7684\u5f15\u5165\u3001\u8282\u80fd\u673a\u5236\u3001\u591a\u9891\u6bb5\uff082.4/5/6 GHz\uff09\u9891\u8c31\u805a\u5408\u4ee5\u53ca\u63a5\u5165\u70b9\u534f\u4f5c\u3002\u9644\u5f55\u8fd8\u8ba8\u8bba\u4e86802.11bn\u4e4b\u5916\u7684\u672a\u6765\u53d1\u5c55\u3002", "result": "\u4f5c\u4e3a\u4e00\u7bc7\u6559\u7a0b\uff0c\u672c\u6587\u7684\u7ed3\u679c\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9Wi-Fi\u6280\u672f\u6f14\u8fdb\u7684\u5168\u9762\u3001\u7cfb\u7edf\u6027\u7684\u7406\u89e3\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u636e\u901f\u7387\uff08\u63d0\u5347\u8d85\u5343\u500d\uff09\u3001\u591a\u7528\u6237\u652f\u6301\u3001\u80fd\u6548\u3001\u9891\u8c31\u5229\u7528\u53caAP\u534f\u4f5c\u65b9\u9762\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5e76\u63cf\u7ed8\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002\u5b83\u63ed\u793a\u4e86Wi-Fi\u5982\u4f55\u7a81\u7834\u201c\u4e00\u6b21\u4e00\u7528\u6237\u201d\u7684\u8303\u5f0f\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u63d0\u4f9b\u4e86\u6a2a\u8de8Wi-Fi\u516b\u4e2a\u4e16\u4ee3\u7684\u5168\u9762\u6280\u672f\u4e0e\u5386\u53f2\u6307\u5357\uff0c\u6e05\u6670\u9610\u8ff0\u4e86\u5176\u6838\u5fc3\u6f14\u8fdb\u673a\u5236\uff0c\u4e3a\u7406\u89e3Wi-Fi\u5982\u4f55\u4ece\u6700\u521d\u7248\u672c\u53d1\u5c55\u6210\u4e3a\u5177\u6709\u591a\u7528\u6237\u3001\u591a\u9891\u6bb5\u805a\u5408\u548c\u9ad8\u7ea7\u8282\u80fd\u529f\u80fd\u7684\u6210\u719f\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u671b\u4e86\u5305\u62ecAI/ML\u5728\u5185\u7684\u672a\u6765\u53d1\u5c55\u8d8b\u52bf\u3002"}}
{"id": "2507.09155", "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD\u662f\u4e00\u79cd\u9488\u5bf9\u6676\u4f53\u5b66\u95ee\u7b54\u7684\u5f00\u5377\u7cfb\u7edf\uff0c\u5b83\u5229\u7528GPT-4.5\u751f\u6210\u7b80\u6d01\u3001\u9886\u57df\u7279\u5b9a\u7684\u8f85\u52a9\u5185\u5bb9\uff0c\u4ee5\u5e2e\u52a9\u5c0f\u6a21\u578b\u7406\u89e3X\u5c04\u7ebf\u884d\u5c04\u6982\u5ff5\u3002\u8be5\u7cfb\u7edf\u5728\u4e13\u5bb6\u7ea7XRD\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u9884\u8bad\u7ec3\u8f83\u5c11\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u626b\u63cf\u6559\u79d1\u4e66\u53ef\u80fd\u5b58\u5728\u7248\u6743\u95ee\u9898\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e2e\u52a9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a8\u7406\u6676\u4f53\u5b66\u9886\u57df\u7684\u4e13\u4e1a\u6982\u5ff5\uff0c\u5c24\u5176\u662f\u5728X\u5c04\u7ebf\u884d\u5c04\uff08XRD\uff09\u65b9\u9762\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5f25\u8865\u5b83\u4eec\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86OPENXRD\uff0c\u4e00\u4e2a\u7ed3\u5408\u6587\u672c\u63d0\u793a\u548cGPT-4.5\u751f\u6210\u7684\u7b80\u6d01\u652f\u6301\u5185\u5bb9\u7684\u5f00\u5377\u5f0f\u7ba1\u9053\u3002\u8be5\u7cfb\u7edf\u4e0d\u4f7f\u7528\u626b\u63cf\u6559\u79d1\u4e66\uff0c\u800c\u662f\u751f\u6210\u7d27\u51d1\u3001\u9886\u57df\u7279\u5b9a\u7684\u53c2\u8003\u8d44\u6599\u3002\u7814\u7a76\u56e2\u961f\u5728217\u4e2a\u4e13\u5bb6\u7ea7XRD\u95ee\u9898\u96c6\u4e0a\u8bc4\u4f30\u4e86OPENXRD\uff0c\u6bd4\u8f83\u4e86GPT-4\u548cLLaVA\u7cfb\u5217\u6a21\u578b\uff08\u5982Mistral, LLaMA, QWEN\uff09\u5728\u95ed\u5377\u548c\u5f00\u5377\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528GPT-4.5\u751f\u6210\u6458\u8981\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u6676\u4f53\u5b66\u65b9\u9762\u9884\u8bad\u7ec3\u8f83\u5c11\u7684\u6a21\u578b\uff0c\u51c6\u786e\u6027\u663e\u8457\u63d0\u9ad8\u3002\u8fd9\u8868\u660eAI\u751f\u6210\u7684\u6587\u672c\u80fd\u591f\u5e2e\u52a9\u5c0f\u6a21\u578b\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u66f4\u6709\u6548\u5730\u8fdb\u884c\u63a8\u7406\u3002", "conclusion": "OPENXRD\u6210\u529f\u5229\u7528\u5927\u578b\u6a21\u578b\u7684\u77e5\u8bc6\u586b\u8865\u4e86\u6676\u4f53\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u7a7a\u767d\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u4e13\u95e8\u7684\u5f00\u5377\u7cfb\u7edf\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u4e3a\u5173\u952e\u79d1\u5b66\u9886\u57df\u4e2d\u66f4\u5e7f\u6cdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09389", "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "categories": ["cs.AI", "cs.CY", "cs.IR"], "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u878d\u5408AI\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5728\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\uff0c\u77e5\u8bc6\u7684\u8868\u793a\uff08\u7ed3\u6784\u4e0e\u590d\u6742\u6027\uff09\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u67e5\u8be2\u77e5\u8bc6\u56fe\u8c31\u7684\u6548\u7387\u3002", "motivation": "\u524d\u6cbfAI\u7cfb\u7edf\uff08\u5982LLMs\u548c\u751f\u6210\u5f0fAI\uff09\u4e9f\u9700\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u5bf9\u65b0\u9886\u57df\u3001\u65b0\u573a\u666f\u7684\u9002\u5e94\u6027\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u878d\u5408\u8fd9\u4e24\u79cd\u80fd\u529b\uff0c\u8bbe\u8ba1\u53ef\u8fc1\u79fb\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u3002", "method": "\u4e13\u6ce8\u4e8e\u201c\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u201d\u7cfb\u7edf\u3002\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u4e0d\u540c\u77e5\u8bc6\u6982\u5ff5\u5316\u548c\u8868\u793a\u65b9\u6cd5\uff08\u7279\u522b\u662f\u7ed3\u6784\u548c\u590d\u6742\u6027\uff09\u5bf9AI\u4ee3\u7406\uff08\u672c\u4f8b\u4e2d\u4e3aLLM\uff09\u6709\u6548\u67e5\u8be2\u4e09\u5143\u7ec4\u5b58\u50a8\u5e93\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u77e5\u8bc6\u6982\u5ff5\u5316\u548c\u8868\u793a\u65b9\u6cd5\u5bf9AI\u4ee3\u7406\uff08LLM\u67e5\u8be2\u4e09\u5143\u7ec4\u5b58\u50a8\u5e93\uff09\u7684\u6027\u80fd\u786e\u5b9e\u5b58\u5728\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u7684\u5f71\u54cd\u548c\u542f\u793a\uff0c\u5f3a\u8c03\u77e5\u8bc6\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\u5bf9AI\u4ee3\u7406\u67e5\u8be2\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u53ef\u8fc1\u79fb\u3001\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.08841", "pdf": "https://arxiv.org/pdf/2507.08841", "abs": "https://arxiv.org/abs/2507.08841", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aWRCor\u7684\u65b0\u578b\u96f6\u6837\u672cNAS\u8bc4\u4f30\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u67b6\u6784\u641c\u7d22\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u56e0\u9700\u4ece\u5934\u8bad\u7ec3\u591a\u4e2a\u67b6\u6784\u800c\u8ba1\u7b97\u6602\u8d35\u4e14\u8017\u65f6\uff1b\u73b0\u6709\u96f6\u6837\u672cNAS\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u7a33\u5b9a\u6027\u3001\u901a\u7528\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a0\u6743\u54cd\u5e94\u76f8\u5173\u6027\uff08WRCor\uff09\u4f5c\u4e3a\u8bad\u7ec3\u65e0\u5173\u7684\u4f30\u8ba1\u4ee3\u7406\u3002WRCor\u5229\u7528\u4e0d\u540c\u8f93\u5165\u6837\u672c\u7684\u54cd\u5e94\u76f8\u5173\u7cfb\u6570\u77e9\u9635\u8ba1\u7b97\u4ee3\u7406\u5206\u6570\uff0c\u4ee5\u8861\u91cf\u4f30\u8ba1\u67b6\u6784\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4ee3\u7406\u8bc4\u4f30\u7ed3\u679c\u663e\u793aWRCor\u53ca\u5176\u6295\u7968\u4ee3\u7406\u6bd4\u73b0\u6709\u4ee3\u7406\u66f4\u9ad8\u6548\u3002\u67b6\u6784\u641c\u7d22\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672cNAS\u7b97\u6cd5\u5728\u4e0d\u540c\u641c\u7d22\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709NAS\u7b97\u6cd5\uff0c\u80fd\u57284\u4e2aGPU\u5c0f\u65f6\u5185\u53d1\u73b0ImageNet-1k\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8bef\u5dee\u4e3a22.1%\u7684\u67b6\u6784\u3002", "conclusion": "WRCor\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u4f30\u8ba1\u4ee3\u7406\uff0c\u663e\u8457\u52a0\u901f\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u5e76\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2507.09071", "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u56fe\u50cf\u6570\u636e\u5bfc\u81f4\u9884\u586b\u5145\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u672c\u6587\u5206\u6790\u4e86VLM\u4e2d\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBlindSight\u7684\u514d\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e94\u7528\u8f93\u5165\u6a21\u677f\u611f\u77e5\u7684\u6ce8\u610f\u529b\u7a00\u758f\u63a9\u7801\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u6570\u636e\u65f6\uff0c\u4f1a\u663e\u8457\u589e\u52a0\u63d0\u793a\uff08prompt\uff09\u957f\u5ea6\u3002\u7531\u4e8e\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u8fd9\u5bfc\u81f4\u4e86\u9884\u586b\u5145\uff08prefill\uff09\u65f6\u95f4\u8fc7\u957f\uff0c\u6210\u4e3aVLM\u63a8\u7406\u7684\u4e00\u4e2a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86VLM\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u53d1\u73b0\u5927\u90e8\u5206\u5c42\u90fd\u5b58\u5728\u663e\u8457\u7a00\u758f\u6027\uff0c\u5373\u9664\u4e86\u901a\u8fc7\u6bcf\u4e2a\u56fe\u50cf\u7684\u6ce8\u610f\u529b\u6c47\u805a\uff08attention-sink\uff09\u4ee4\u724c\u5916\uff0c\u8de8\u56fe\u50cf\u6ce8\u610f\u529b\u6781\u5c11\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u4ed6\u4eec\u8bc6\u522b\u51fa\u51e0\u79cd\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\uff08\u5982\u4ec5\u6c47\u805a\u3001\u6587\u6863\u63a9\u7801\u548c\u6df7\u5408\u6587\u6863-\u6c47\u805a\u63a9\u7801\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86BlindSight\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e94\u7528\u8f93\u5165\u6a21\u677f\u611f\u77e5\u7684\u6ce8\u610f\u529b\u7a00\u758f\u63a9\u7801\u6765\u4f18\u5316VLM\u63a8\u7406\u3002\u5177\u4f53\u5730\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6570\u636e\u96c6\u6837\u672c\u4e3a\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u5bfc\u51fa\u4e00\u4e2a\u4e0e\u63d0\u793a\u65e0\u5173\u7684\u7a00\u758f\u6027\u5206\u7c7b\u3002", "result": "\u8be5\u6280\u672f\u5728Qwen2-VL\u3001Qwen2.5-VL\u548cGemma-3\u7b49VLM\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cBlindSight\u5e73\u5747\u53ef\u5c06FLOPs\uff08\u6d6e\u70b9\u8fd0\u7b97\uff09\u51cf\u5c1132%-41%\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u591a\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u53d8\u5316\u4ec5\u4e3a-2%\u81f3+2%\u3002", "conclusion": "BlindSight\u901a\u8fc7\u6709\u6548\u5229\u7528VLM\u4e2d\u56fa\u6709\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u4e3a\u4f18\u5316\u5176\u63a8\u7406\u6548\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u6709\u6548\u7f13\u89e3\u4e86VLM\u9884\u586b\u5145\u65f6\u95f4\u8fc7\u957f\u7684\u74f6\u9888\u3002"}}
{"id": "2507.09798", "pdf": "https://arxiv.org/pdf/2507.09798", "abs": "https://arxiv.org/abs/2507.09798", "authors": ["Aashish Gottipati", "Lili Qiu"], "title": "Towards Robust RTC in Sparse LEO Constellations", "categories": ["cs.NI"], "comment": "8 pages, 5 figures", "summary": "Google's congestion control (GCC) has become a cornerstone for real-time\nvideo and audio communication, yet its performance remains fragile in emerging\nLow Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer\nlonger duration links and reduced handover frequency compared to dense\ndeployments, presenting a unique opportunity for high-quality real-time\ncommunication (RTC) in environments with limited terrestrial network\ninfrastructure. In this paper, we study the behavior of videoconferencing\nsystems in sparse LEO constellations. We observe that video quality degrades\ndue to inherent delays and network instability introduced by the high altitude\nand rapid movement of LEO satellites, with these effects exacerbated by\nWebRTC's conventional ``one-size-fits-all'' sender-side pacing queue\nmanagement. To boost RTC performance, we introduce a data-driven queue\nmanagement mechanism that adapts the maximum pacing queue capacity based on\npredicted handover activity. Specifically, our approach employs shorter queue\nlimits during stable, no-handover phases to prioritize low latency\ncommunication, and preemptively increases pacing queue capacity when entering\nperiods of increased handover activity to absorb disruptions. Our method yields\nup to $3$x improvements in video bitrate and reduces freeze rate by $62\\%$\ncompared to default WebRTC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u961f\u5217\u7ba1\u7406\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u53d1\u9001\u7aef\u961f\u5217\u5bb9\u91cf\u4ee5\u9002\u5e94LEO\u7f51\u7edc\u7684\u5207\u6362\u6d3b\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u661f\u5ea7\u4e2dWebRTC\u5b9e\u65f6\u89c6\u9891\u901a\u4fe1\u7684\u6027\u80fd\u3002", "motivation": "Google\u62e5\u585e\u63a7\u5236\uff08GCC\uff09\u5728LEO\u7f51\u7edc\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b\u7a00\u758fLEO\u661f\u5ea7\u4e3a\u9646\u5730\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u533a\u57df\u7684\u5b9e\u65f6\u901a\u4fe1\uff08RTC\uff09\u63d0\u4f9b\u673a\u9047\uff1b\u73b0\u6709WebRTC\u201c\u4e00\u5200\u5207\u201d\u7684\u53d1\u9001\u7aef\u961f\u5217\u7ba1\u7406\u52a0\u5267\u4e86LEO\u9ad8\u5ef6\u8fdf\u548c\u7f51\u7edc\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u7684\u89c6\u9891\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u961f\u5217\u7ba1\u7406\u673a\u5236\uff0c\u6839\u636e\u9884\u6d4b\u7684\u5207\u6362\u6d3b\u52a8\u8c03\u6574\u6700\u5927\u901f\u7387\u961f\u5217\u5bb9\u91cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u7a33\u5b9a\u65e0\u5207\u6362\u9636\u6bb5\u91c7\u7528\u8f83\u77ed\u961f\u5217\u9650\u5236\u4ee5\u964d\u4f4e\u5ef6\u8fdf\uff0c\u800c\u5728\u5207\u6362\u6d3b\u52a8\u589e\u52a0\u65f6\u9884\u5148\u589e\u5927\u961f\u5217\u5bb9\u91cf\u4ee5\u5438\u6536\u4e2d\u65ad\u3002", "result": "\u4e0e\u9ed8\u8ba4WebRTC\u76f8\u6bd4\uff0c\u89c6\u9891\u6bd4\u7279\u7387\u63d0\u5347\u9ad8\u8fbe3\u500d\uff0c\u51bb\u7ed3\u7387\u964d\u4f4e62%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u961f\u5217\u7ba1\u7406\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758fLEO\u7f51\u7edc\u4e2d\u5b9e\u65f6\u901a\u4fe1\u7684\u6027\u80fd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u901a\u4fe1\u8d28\u91cf\u3002"}}
{"id": "2507.09157", "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "\u9488\u5bf9\u6218\u7565\u5bf9\u8bdd\u4e2d\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u6b3a\u9a97\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7PU-Lie\u6a21\u578b\uff0c\u7ed3\u5408\u51bb\u7ed3BERT\u5d4c\u5165\u3001\u8bed\u8a00\u548c\u6e38\u620f\u7279\u5f81\u4e0e\u6b63\u4f8b-\u65e0\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\uff0c\u5b9e\u73b0\u5b8f\u89c2F1\u65b0\u9ad8\u5e76\u663e\u8457\u51cf\u5c11\u53c2\u6570\u3002", "motivation": "\u6218\u7565\u5bf9\u8bdd\u4e2d\u7684\u6b3a\u9a97\u68c0\u6d4b\u56e0\u8bed\u8a00\u5fae\u5999\u6027\u53ca\u6b3a\u9a97\u4e0e\u771f\u5b9e\u6d88\u606f\u4e4b\u95f4\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\uff08\u6b3a\u9a97\u6d88\u606f\u5c11\u4e8e5%\uff09\u800c\u590d\u6742\u4e14\u98ce\u9669\u9ad8\u3002\u4f20\u7edf\u4e8c\u5143\u5206\u7c7b\u5668\u4e0d\u9002\u7528\u4ec5\u5c11\u6570\u6b3a\u9a97\u6d88\u606f\u88ab\u6807\u8bb0\u7684\u60c5\u51b5\uff0c\u800c\u51c6\u786e\u68c0\u6d4b\u6b3a\u9a97\u6bd4\u8bc6\u522b\u771f\u5b9e\u6d88\u606f\u66f4\u5173\u952e\u3002", "method": "\u63d0\u51fa\u540d\u4e3aPU-Lie\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u51bb\u7ed3\u7684BERT\u5d4c\u5165\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u548c\u6e38\u620f\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6b63\u4f8b-\u65e0\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\u76ee\u6807\u3002\u8be5\u6a21\u578b\u4e13\u4e3a\u5904\u7406\u53ea\u6709\u5c11\u91cf\u6b3a\u9a97\u6d88\u606f\u88ab\u6807\u8bb0\u800c\u5927\u591a\u6570\u672a\u88ab\u6807\u8bb0\u7684\u60c5\u51b5\u800c\u8bbe\u8ba1\u3002", "result": "\u6a21\u578b\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.60\u7684\u5b8f\u89c2F1\u65b0\u6700\u4f73\u6210\u7ee9\uff0c\u540c\u65f6\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u4e86\u8d85\u8fc7650\u500d\u3002\u901a\u8fc7\u5bf9\u4e03\u4e2a\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86PU\u5b66\u4e60\u3001\u8bed\u8a00\u53ef\u89e3\u91ca\u6027\u548c\u8bf4\u8bdd\u8005\u611f\u77e5\u8868\u793a\u7684\u4ef7\u503c\u3002", "conclusion": "\u5728\u6b64\u95ee\u9898\u8bbe\u7f6e\u4e2d\uff0c\u51c6\u786e\u68c0\u6d4b\u6b3a\u9a97\u6bd4\u8bc6\u522b\u771f\u5b9e\u6d88\u606f\u66f4\u5173\u952e\u3002PU\u5b66\u4e60\u80fd\u591f\u663e\u5f0f\u5730\u5bf9\u7a00\u6709\u4f46\u81f3\u5173\u91cd\u8981\u7684\u6b3a\u9a97\u7c7b\u522b\u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u8868\u660ePU\u5b66\u4e60\u3001\u8bed\u8a00\u53ef\u89e3\u91ca\u6027\u548c\u8bf4\u8bdd\u8005\u611f\u77e5\u8868\u793a\u5728\u5904\u7406\u9ad8\u5ea6\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.09407", "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "categories": ["cs.AI", "cs.CR", "cs.GT"], "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6574\u5408\u5230\u5e8f\u8d2f\u7b56\u7565\u4e92\u52a8\u4e2d\uff0c\u5b9a\u4e49\u4e86\u65b0\u7684\u5747\u8861\u6982\u5ff5\uff0c\u65e8\u5728\u6a21\u62dfLLM\u4ecb\u5bfc\u7684\u51b3\u7b56\uff0c\u5e76\u4ee5\u7f51\u7edc\u9493\u9c7c\u6848\u4f8b\u5c55\u793a\u5176\u5728\u7f51\u7edc\u5b89\u5168\u7b49\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edfStackelberg\u535a\u5f08\u5047\u8bbe\u5b8c\u5168\u4fe1\u606f\u548c\u7406\u6027\u4ee3\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u6a21\u62dfLLM\u4f5c\u4e3a\u51b3\u7b56\u8005\u7684\u6218\u7565\u4e92\u52a8\u3002\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u5bf9LLM\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u8bcd\u8fdb\u884c\u63a8\u7406\u3001\u751f\u6210\u6982\u7387\u884c\u4e3a\u4ee5\u53ca\u9002\u5e94\u7b56\u7565\u80fd\u529b\u7684\u5efa\u6a21\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u6846\u67b6\u6765\u6355\u83b7\u6709\u9650\u7406\u6027\u3001\u975e\u5bf9\u79f0\u4fe1\u606f\u548c\u5143\u8ba4\u77e5\u9002\u5e94\u3002", "method": "\u5f15\u5165LLM-Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u5141\u8bb8\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u8bcd\u8fdb\u884c\u63a8\u7406\uff0c\u5229\u7528LLM\u751f\u6210\u6982\u7387\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u8ba4\u77e5\u548c\u4fe1\u5ff5\u66f4\u65b0\u8c03\u6574\u7b56\u7565\u3002\u5b9a\u4e49\u4e86\u201c\u63a8\u7406\u4e0e\u884c\u4e3a\u5747\u8861\u201d\u548c\u201c\u63a8\u6d4b\u6027\u63a8\u7406\u5747\u8861\u201d\u4e24\u79cd\u5747\u8861\u6982\u5ff5\u3002\u901a\u8fc7\u4e00\u4e2a\u7f51\u7edc\u9493\u9c7c\u6848\u4f8b\u7814\u7a76\uff08\u53d1\u9001\u8005\u4e0e\u63a5\u6536\u8005\u4f7f\u7528\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u793a\u8bcd\u8fdb\u884c\u6b3a\u9a97\u535a\u5f08\uff09\u6765\u6f14\u793a\u8be5\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u6355\u6349\u6709\u9650\u7406\u6027\u3001\u975e\u5bf9\u79f0\u4fe1\u606f\u548c\u5143\u8ba4\u77e5\u9002\u5e94\u3002\u7f51\u7edc\u9493\u9c7c\u6848\u4f8b\u7814\u7a76\u7a81\u663e\u4e86LLM\u4ecb\u5bfc\u4ea4\u4e92\u7684\u8ba4\u77e5\u4e30\u5bcc\u6027\u548c\u5bf9\u6297\u6f5c\u529b\u3002LLM-Stackelberg\u535a\u5f08\u4e3a\u7f51\u7edc\u5b89\u5168\u3001\u865a\u5047\u4fe1\u606f\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u51b3\u7b56\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u8303\u5f0f\u3002", "conclusion": "LLM-Stackelberg\u535a\u5f08\u6846\u67b6\u4e3a\u5305\u542bLLM\u7684\u5e8f\u8d2f\u6218\u7565\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u65b0\u9896\u7684\u5efa\u6a21\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u8003\u8651\u6709\u9650\u7406\u6027\u3001\u975e\u5bf9\u79f0\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u7b56\u7565\u7684\u590d\u6742\u5e94\u7528\u573a\u666f\uff0c\u5982\u7f51\u7edc\u5b89\u5168\u3001\u865a\u5047\u4fe1\u606f\u548c\u63a8\u8350\u7cfb\u7edf\u3002"}}
{"id": "2507.08842", "pdf": "https://arxiv.org/pdf/2507.08842", "abs": "https://arxiv.org/abs/2507.08842", "authors": ["Zhufeng Lu", "Chentao Jia", "Ming Hu", "Xiaofei Xie", "Mingsong Chen"], "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by ACM SIGKDD 2025", "summary": "As a promising privacy-aware collaborative model training paradigm, Federated\nLearning (FL) is becoming popular in the design of distributed recommender\nsystems. However, Federated Recommender Systems (FedRecs) greatly suffer from\ntwo major problems: i) extremely high communication overhead due to massive\nitem embeddings involved in recommendation systems, and ii) intolerably low\ntraining efficiency caused by the entanglement of both heterogeneous network\nenvironments and client devices. Although existing methods attempt to employ\nvarious compression techniques to reduce communication overhead, due to the\nparameter errors introduced by model compression, they inevitably suffer from\nmodel performance degradation. To simultaneously address the above problems,\nthis paper presents a communication-efficient FedRec framework named FedRAS,\nwhich adopts an action-sharing strategy to cluster the gradients of item\nembedding into a specific number of model updating actions for communication\nrather than directly compressing the item embeddings. In this way, the cloud\nserver can use the limited actions from clients to update all the items. Since\ngradient values are significantly smaller than item embeddings, constraining\nthe directions of gradients (i.e., the action space) introduces smaller errors\ncompared to compressing the entire item embedding matrix into a reduced space.\nTo accommodate heterogeneous devices and network environments, FedRAS\nincorporates an adaptive clustering mechanism that dynamically adjusts the\nnumber of actions. Comprehensive experiments on well-known datasets demonstrate\nthat FedRAS can reduce the size of communication payloads by up to 96.88%,\nwhile not sacrificing recommendation performance within various heterogeneous\nscenarios. We have open-sourced FedRAS at\nhttps://github.com/mastlab-T3S/FedRAS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFedRAS\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u52a8\u5171\u4eab\u7b56\u7565\u5c06\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u68af\u5ea6\u805a\u7c7b\u4e3a\u6a21\u578b\u66f4\u65b0\u884c\u52a8\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u9002\u5e94\u5f02\u6784\u73af\u5883\u3002", "motivation": "\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FedRecs\uff09\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u7531\u4e8e\u5927\u91cf\u7269\u54c1\u5d4c\u5165\u5bfc\u81f4\u6781\u9ad8\u7684\u901a\u4fe1\u5f00\u9500\uff1b\u4e8c\u662f\u7f51\u7edc\u73af\u5883\u548c\u5ba2\u6237\u7aef\u8bbe\u5907\u5f02\u6784\u6027\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u867d\u80fd\u51cf\u5c11\u901a\u4fe1\uff0c\u4f46\u4f1a\u5f15\u5165\u53c2\u6570\u8bef\u5dee\u5e76\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51faFedRAS\u6846\u67b6\uff0c\u91c7\u7528\u884c\u52a8\u5171\u4eab\u7b56\u7565\uff0c\u5c06\u7269\u54c1\u5d4c\u5165\u7684\u68af\u5ea6\u805a\u7c7b\u4e3a\u7279\u5b9a\u6570\u91cf\u7684\u6a21\u578b\u66f4\u65b0\u884c\u52a8\u8fdb\u884c\u901a\u4fe1\uff0c\u800c\u975e\u76f4\u63a5\u538b\u7f29\u7269\u54c1\u5d4c\u5165\u3002\u7531\u4e8e\u68af\u5ea6\u503c\u8fdc\u5c0f\u4e8e\u7269\u54c1\u5d4c\u5165\uff0c\u9650\u5236\u68af\u5ea6\u65b9\u5411\uff08\u5373\u884c\u52a8\u7a7a\u95f4\uff09\u5f15\u5165\u7684\u8bef\u5dee\u5c0f\u4e8e\u76f4\u63a5\u538b\u7f29\u6574\u4e2a\u7269\u54c1\u5d4c\u5165\u77e9\u9635\u3002\u4e3a\u9002\u5e94\u5f02\u6784\u8bbe\u5907\u548c\u7f51\u7edc\u73af\u5883\uff0cFedRAS\u8fd8\u96c6\u6210\u4e86\u81ea\u9002\u5e94\u805a\u7c7b\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u884c\u52a8\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedRAS\u53ef\u5c06\u901a\u4fe1\u8f7d\u8377\u5927\u5c0f\u51cf\u5c11\u9ad8\u8fbe96.88%\uff0c\u540c\u65f6\u5728\u5404\u79cd\u5f02\u6784\u573a\u666f\u4e0b\u4e0d\u727a\u7272\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "FedRAS\u901a\u8fc7\u521b\u65b0\u7684\u884c\u52a8\u5171\u4eab\u7b56\u7565\u548c\u81ea\u9002\u5e94\u805a\u7c7b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5728\u5927\u5e45\u51cf\u5c11\u901a\u4fe1\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2507.09081", "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u5b9a\u91cf\u9065\u611f\u53cd\u6f14\u65b9\u6cd5\u4ece\u7269\u7406\u6a21\u578b\u5230\u673a\u5668\u5b66\u4e60\u518d\u5230\u57fa\u7840\u6a21\u578b\u7684\u6f14\u53d8\uff0c\u6bd4\u8f83\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u7279\u70b9\u3001\u5c40\u9650\u6027\u53ca\u5e94\u7528\u573a\u666f\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u9065\u611f\u7cfb\u7edf\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u7684\u7269\u7406\u6a21\u578b\u6b63\u9010\u6e10\u88ab\u6570\u636e\u9a71\u52a8\u548c\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u53d6\u4ee3\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e00\u65b9\u6cd5\u8bba\u6f14\u53d8\uff0c\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u5e76\u8bc4\u4f30\u9065\u611f\u53cd\u6f14\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u672a\u6765\u65b9\u5411\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u56de\u987e\u4e86\u9065\u611f\u53cd\u6f14\u6280\u672f\u4ece\u7269\u7406\u6a21\u578b\uff08\u5982PROSPECT\uff09\u5230\u673a\u5668\u5b66\u4e60\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u3001\u591a\u6a21\u6001\u878d\u5408\uff09\u518d\u5230\u57fa\u7840\u6a21\u578b\uff08\u5982SatMAE\u3001GFM\uff09\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002\u6587\u7ae0\u6bd4\u8f83\u4e86\u5404\u8303\u5f0f\u7684\u5efa\u6a21\u5047\u8bbe\u3001\u5e94\u7528\u573a\u666f\u53ca\u5c40\u9650\u6027\uff0c\u5e76\u91cd\u70b9\u5173\u6ce8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u8bba\u6587\u5206\u6790\u4e86\u5404\u7c7b\u9065\u611f\u53cd\u6f14\u8303\u5f0f\uff08\u7269\u7406\u6a21\u578b\u3001\u673a\u5668\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\uff09\u7684\u7279\u70b9\u3001\u9002\u7528\u6027\u53ca\u5c40\u9650\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u57fa\u7840\u6a21\u578b\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u65b9\u9762\uff0c\u4f46\u4ecd\u9762\u4e34\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3001\u57df\u6cdb\u5316\u80fd\u529b\u3001\u6709\u9650\u76d1\u7763\u53ca\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u9065\u611f\u53cd\u6f14\u7684\u57fa\u7840\u6a21\u578b\u5c06\u5411\u7edf\u4e00\u5efa\u6a21\u80fd\u529b\u3001\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2507.09852", "pdf": "https://arxiv.org/pdf/2507.09852", "abs": "https://arxiv.org/abs/2507.09852", "authors": ["Zihao Zhou", "Zipeng Dai", "Linyi Huang", "Cui Yang", "Youjun Xiang", "Jie Tang", "Kai-kit Wong"], "title": "UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "In unmanned aerial vehicle (UAV) networks, communication protocols and\nalgorithms are essential for cooperation and collaboration between UAVs.\nSimulation provides a cost-effective solution for prototyping, debugging, and\nanalyzing protocols and algorithms, avoiding the prohibitive expenses of field\nexperiments. In this paper, we present ``UavNetSim-v1'', an open-source\nPython-based simulation platform designed for rapid development, testing, and\nevaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''\nprovides most of the functionalities developers may need, including\nrouting/medium access control (MAC) protocols, topology control algorithms and\nmobility/energy models, while maintaining ease of use. Furthermore, the\nplatform supports comprehensive performance evaluation and features an\ninteractive visualization interface for in-depth algorithm analysis. In short,\n``UavNetSim-v1'' lends itself to both rapid prototyping and educational\npurposes, and can serve as a lightweight yet powerful alternative to mature\nnetwork simulators for UAV communication research.", "AI": {"tldr": "UavNetSim-v1\u662f\u4e00\u4e2a\u5f00\u6e90Python\u6a21\u62df\u5e73\u53f0\uff0c\u65e8\u5728\u5b9e\u73b0\u65e0\u4eba\u673a\u7f51\u7edc\u534f\u8bae\u548c\u7b97\u6cd5\u7684\u5feb\u901f\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "motivation": "\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u7684\u901a\u4fe1\u534f\u8bae\u548c\u7b97\u6cd5\u5bf9UAV\u95f4\u7684\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff1b\u6a21\u62df\u662f\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u907f\u514d\u73b0\u573a\u5b9e\u9a8c\u7684\u9ad8\u6602\u5f00\u9500\uff0c\u7528\u4e8e\u534f\u8bae\u548c\u7b97\u6cd5\u7684\u539f\u578b\u8bbe\u8ba1\u3001\u8c03\u8bd5\u548c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201cUavNetSim-v1\u201d\u7684\u5f00\u6e90Python\u6a21\u62df\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u8def\u7531/MAC\u534f\u8bae\u3001\u62d3\u6251\u63a7\u5236\u7b97\u6cd5\u3001\u79fb\u52a8\u6027/\u80fd\u91cf\u6a21\u578b\u7b49\u591a\u79cd\u529f\u80fd\uff0c\u5e76\u652f\u6301\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\u3002", "result": "UavNetSim-v1\u63d0\u4f9b\u4e86\u5f00\u53d1\u8005\u6240\u9700\u7684\u5927\u90e8\u5206\u529f\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6613\u7528\u6027\uff0c\u5e76\u652f\u6301\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u6df1\u5165\u7b97\u6cd5\u5206\u6790\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002\u5b83\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "UavNetSim-v1\u65e2\u9002\u7528\u4e8e\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u4e5f\u9002\u7528\u4e8e\u6559\u80b2\u76ee\u7684\uff0c\u53ef\u4f5c\u4e3a\u65e0\u4eba\u673a\u901a\u4fe1\u7814\u7a76\u4e2d\u6210\u719f\u7f51\u7edc\u6a21\u62df\u5668\u7684\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u66ff\u4ee3\u54c1\u3002"}}
{"id": "2507.09174", "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRAMA\u7684\u65b0\u578b\u68c0\u7d22\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6218\u7565\u6027\u67e5\u8be2\u5236\u5b9a\u3001\u4ea4\u53c9\u9a8c\u8bc1\u8bc1\u636e\u805a\u5408\u548c\u591a\u667a\u80fd\u4f53\u96c6\u6210\u67b6\u6784\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u6a21\u7cca\u6216\u96be\u4ee5\u7f6e\u4fe1\u7684\u58f0\u660e\u3002", "motivation": "\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u7684\u8fc5\u901f\u6269\u6563\u5bf9\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u5f53\u58f0\u660e\u6a21\u7cca\u4e0d\u6e05\u6216\u7f3a\u4e4f\u8db3\u591f\u4e0a\u4e0b\u6587\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RAMA\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u5c06\u591a\u6a21\u6001\u58f0\u660e\u8f6c\u5316\u4e3a\u7cbe\u786e\u7f51\u7edc\u641c\u7d22\u67e5\u8be2\u7684\u6218\u7565\u6027\u67e5\u8be2\u5236\u5b9a\uff1b2) \u4ece\u591a\u6837\u5316\u3001\u6743\u5a01\u6765\u6e90\u805a\u5408\u4ea4\u53c9\u9a8c\u8bc1\u8bc1\u636e\uff1b3) \u5229\u7528\u591a\u4e2a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u53d8\u4f53\u4e92\u8865\u4f18\u52bf\u7684\u591a\u667a\u80fd\u4f53\u96c6\u6210\u67b6\u6784\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAMA\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u64c5\u957f\u901a\u8fc7\u5c06\u9a8c\u8bc1\u57fa\u4e8e\u68c0\u7d22\u5230\u7684\u4e8b\u5b9e\u8bc1\u636e\u6765\u89e3\u51b3\u6a21\u7cca\u6216\u96be\u4ee5\u7f6e\u4fe1\u7684\u58f0\u660e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6574\u5408\u57fa\u4e8e\u7f51\u7edc\u7684\u8bc1\u636e\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5bf9\u4e8e\u53ef\u4fe1\u591a\u5a92\u4f53\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09495", "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u56fa\u6709\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4ece\u53cd\u5e94\u5f0f\u667a\u80fd\u8f6c\u5411\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u4e3b\u52a8\u5f0f\u667a\u80fd\uff0c\u901a\u8fc7\u9884\u6d4b\u548c\u534f\u8c03\u884c\u4e3a\uff0c\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u5e94\u5bf9\u7684\u590d\u6742\u534f\u4f5c\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u9762\u4e34\u8054\u5408\u884c\u52a8\u7a7a\u95f4\u6307\u6570\u589e\u957f\u3001\u73af\u5883\u975e\u5e73\u7a33\u6027\u53ca\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u6839\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u523a\u6fc0-\u53cd\u5e94\u673a\u5236\uff0c\u5728\u9762\u5bf9\u65b0\u9896\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u5021\u5bfc\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u667a\u80fd\u4f53\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u590d\u6742\u7684\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u5f3a\u5316\u5b66\u4e60\uff08generative-RL\uff09\uff0c\u4f7f\u5176\u80fd\u591f\u5408\u6210\u591a\u667a\u80fd\u4f53\u52a8\u6001\u3001\u57fa\u4e8e\u9884\u6d4b\u7406\u89e3\u8fdb\u884c\u524d\u77bb\u6027\u51b3\u7b56\uff0c\u5e76\u80fd\u591f\u5efa\u6a21\u73af\u5883\u6f14\u5316\u3001\u9884\u6d4b\u4ed6\u8005\u884c\u4e3a\u3001\u751f\u6210\u534f\u8c03\u52a8\u4f5c\u5e8f\u5217\u53ca\u8fdb\u884c\u957f\u671f\u6218\u7565\u63a8\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u5b9e\u73b0\u4e3b\u52a8\u51b3\u7b56\u3001\u901a\u8fc7\u589e\u5f3a\u901a\u4fe1\u5b9e\u73b0\u65e0\u7f1d\u534f\u8c03\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u573a\u666f\u3002\u6709\u671b\u89e3\u9501\u5206\u5e03\u5f0f\u667a\u80fd\u7684\u65e0\u9650\u53ef\u80fd\uff0c\u4ece\u4e2a\u4f53\u4f18\u5316\u8fc8\u5411\u4ee3\u8868\u771f\u6b63\u534f\u4f5c\u667a\u80fd\u7684\u6d8c\u73b0\u96c6\u4f53\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u4e00\u8303\u5f0f\u8f6c\u53d8\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u3001\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff0c\u6709\u671b\u4e3a\u4f20\u7edf\u53cd\u5e94\u5f0f\u6846\u67b6\u4e0b\u96be\u4ee5\u89e3\u51b3\u7684\u534f\u8c03\u6311\u6218\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08843", "pdf": "https://arxiv.org/pdf/2507.08843", "abs": "https://arxiv.org/abs/2507.08843", "authors": ["Arpita Soni", "Sahil Tripathi", "Gautam Siddharth Kashyap", "Manaswi Kulahara", "Mohammad Anas Azeez", "Zohaib Hasan Siddiqui", "Nipun Joshi", "Jiechao Gao"], "title": "Can We Predict Your Next Move Without Breaking Your Privacy?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in the 17th International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025\n  in Ontario, Canada", "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FLLL3M\uff0c\u4e00\u4e2a\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u4e3a\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\uff08NxLP\uff09\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\uff0c\u53c8\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u6846\u67b6\u3002", "method": "FLLL3M\u6846\u67b6\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5c06\u7528\u6237\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5916\u79ef\u673a\u5236\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5728Gowalla\u3001WeePlace\u3001Brightkite\u548cFourSquare\u7b49\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOT\uff09\u7684\u51c6\u786e\u7387\uff08\u4f8b\u5982Gowalla Acc@1: 12.55, MRR: 0.1422\uff09\uff0c\u540c\u65f6\u5c06\u53c2\u6570\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe45.6%\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e8652.7%\u3002", "conclusion": "FLLL3M\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u6210\u529f\u5730\u7ed3\u5408\u4e86\u8054\u90a6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.09082", "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4ece\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u4e2d\u63d0\u53d6\u5149\u6d41\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63d0\u793a\uff08KL-tracing\uff09\u5229\u7528\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u7684\u7279\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5149\u6d41\u63d0\u53d6\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4f46\u6807\u7b7e\u7a00\u7f3a\u4e14\u5408\u6210\u6570\u636e\u5b58\u5728\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u4f7f\u5f97\u5fae\u8c03\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\u3002\u53d7\u5927\u578b\u901a\u7528\u6a21\u578b\u6210\u529f\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u65e8\u5728\u63a2\u7a76\u662f\u5426\u80fd\u65e0\u9700\u5fae\u8c03\uff0c\u4ec5\u901a\u8fc7\u63d0\u793a\u51bb\u7ed3\u7684\u81ea\u76d1\u7763\u89c6\u9891\u6a21\u578b\uff08\u4ec5\u7528\u4e8e\u672a\u6765\u5e27\u9884\u6d4b\uff09\u6765\u8f93\u51fa\u5149\u6d41\u3002", "method": "\u8be5\u7814\u7a76\u53d7\u5230\u53cd\u4e8b\u5b9e\u4e16\u754c\u6a21\u578b\uff08CWM\uff09\u8303\u5f0f\u7684\u542f\u53d1\uff0c\u5e76\u5c06\u5176\u601d\u60f3\u6269\u5c55\u5230\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u6210\u529f\u7684\u96f6\u6837\u672c\u5149\u6d41\u63d0\u53d6\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u4e09\u5927\u7279\u6027\uff1a\u5206\u5e03\u5f0f\u672a\u6765\u5e27\u9884\u6d4b\u3001\u56e0\u5b50\u5316\u9690\u53d8\u91cf\u548c\u968f\u673a\u8bbf\u95ee\u89e3\u7801\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5728LRAS\u67b6\u6784\u4e2d\u72ec\u7279\u5b58\u5728\u7684\u7279\u6027\uff0c\u63d0\u51fa\u4e86KL-tracing\u65b9\u6cd5\uff1a\u5728\u89c6\u9891\u7b2c\u4e00\u5e27\u4e2d\u6ce8\u5165\u5c40\u90e8\u6270\u52a8\uff0c\u6a21\u578b\u63a8\u6f14\u4e00\u6b65\uff0c\u7136\u540e\u8ba1\u7b97\u6270\u52a8\u548c\u672a\u6270\u52a8\u9884\u6d4b\u5206\u5e03\u4e4b\u95f4\u7684Kullback-Leibler\u6563\u5ea6\u3002", "result": "\u5728\u65e0\u9700\u4efb\u4f55\u5149\u6d41\u7279\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684TAP-Vid DAVIS\u6570\u636e\u96c6\u4e0a\uff0c\u7ec8\u70b9\u8bef\u5dee\u76f8\u5bf9\u6539\u8fdb\u4e8616.6%\uff1b\u5728\u5408\u6210\u7684TAP-Vid Kubric\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u5bf9\u6539\u8fdb\u4e864.7%\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u53ef\u63a7\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63d0\u793a\uff0c\u662f\u83b7\u5f97\u9ad8\u8d28\u91cf\u5149\u6d41\u7684\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u53ef\u66ff\u4ee3\u4f20\u7edf\u7684\u76d1\u7763\u6216\u5149\u5ea6\u635f\u5931\u65b9\u6cd5\u3002"}}
{"id": "2507.09942", "pdf": "https://arxiv.org/pdf/2507.09942", "abs": "https://arxiv.org/abs/2507.09942", "authors": ["Jiaming Cheng", "Duong Tung Nguyen"], "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference", "categories": ["cs.NI", "cs.DC", "cs.SY", "eess.SY", "math.OC"], "comment": "5 pages, 11 figures", "summary": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach.", "AI": {"tldr": "\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5f02\u6784\u8fb9\u7f18\u6570\u636e\u4e2d\u5fc3\u7684\u5206\u914d\uff0c\u65e8\u5728\u964d\u4f4e\u80fd\u8017\u3001\u78b3\u6392\u653e\u548c\u7528\u6c34\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u9274\u4e8e\u8fb9\u7f18\u6570\u636e\u4e2d\u5fc3\u62e5\u6709\u672c\u5730\u53ef\u518d\u751f\u80fd\u6e90\uff0c\u5e76\u9762\u4e34\u52a8\u6001\u7535\u4ef7\u53ca\u53ef\u518d\u751f\u80fd\u6e90\u7684\u65f6\u7a7a\u6ce2\u52a8\u6027\uff0c\u5982\u4f55\u5c06LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6700\u4f18\u5206\u914d\uff0c\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u3001\u78b3\u6392\u653e\u548c\u7528\u6c34\u91cf\uff0c\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u6a21\u578b\uff0c\u65e8\u5728\u5e2e\u52a9LLM\u670d\u52a1\u63d0\u4f9b\u5546\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316\u6a21\u578b\u80fd\u6709\u6548\u5b9e\u73b0LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4f18\u5316\u5206\u914d\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2507.09185", "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0e\u6570\u636e\u96c6\u7279\u5b9a\u673a\u5236\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u589e\u5f3a\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u5b66\u4e60\u5230\u7279\u5b9a\u6570\u636e\u96c6\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u9ad8\u7f6e\u4fe1\u5ea6\u4f46\u7f3a\u4e4f\u6cdb\u5316\u6027\u7684\u9884\u6d4b\uff0c\u5e76\u5728\u9762\u5bf9\u65b0\u4efb\u52a1\u6216\u5206\u5e03\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u8bc6\u522b\u5e76\u526a\u679dTransformer\u6a21\u578b\u4e2d\u4e0e\u6570\u636e\u96c6\u7279\u5b9a\u673a\u5236\u76f8\u5173\u7684\u795e\u7ecf\u5143\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u96c6\u6210\u68af\u5ea6\uff08Integrated Gradients\uff09\u91cf\u5316\u6bcf\u4e2a\u795e\u7ecf\u5143\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u4ee5\u627e\u51fa\u90a3\u4e9b\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u6027\u80fd\u800c\u975e\u7a33\u5065\u3001\u53ef\u8fc1\u79fb\u63a8\u7406\u7684\u795e\u7ecf\u5143\u3002\u9009\u62e9\u6027\u526a\u679d\u8fd9\u4e9b\u795e\u7ecf\u5143\u8feb\u4f7f\u6a21\u578b\u4f9d\u8d56\u53ef\u6cdb\u5316\u7684\u8868\u5f81\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u526a\u679d\u7684\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\uff08\u975e\u526a\u679d\uff09\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u526a\u679d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0e\u6570\u636e\u96c6\u7279\u5b9a\u673a\u5236\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u9002\u5e94\u548c\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u7b56\u7565\u3002"}}
{"id": "2507.09534", "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u81f4\u6027\u8f68\u8ff9\u89c4\u5212\uff08CTP\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eCTM\u7684\u65b0\u578b\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u89c4\u5212\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u5355\u6b65\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347120\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u89c4\u5212\u7684\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u7531\u4e8e\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e00\u81f4\u6027\u8f68\u8ff9\u89c4\u5212\uff08CTP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002CTP\u5229\u7528\u6700\u65b0\u63d0\u51fa\u7684\u4e00\u81f4\u6027\u8f68\u8ff9\u6a21\u578b\uff08CTM\uff09\u8fdb\u884c\u9ad8\u6548\u8f68\u8ff9\u4f18\u5316\uff0c\u652f\u6301\u5feb\u901f\u3001\u5355\u6b65\u7684\u8f68\u8ff9\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "CTP\u5728\u957f\u65f6\u7a0b\u3001\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6807\u51c6\u5316\u56de\u62a5\uff0c\u540c\u65f6\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u53bb\u566a\u6b65\u9aa4\u3002\u5177\u4f53\u800c\u8a00\uff0cCTP\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7120\u500d\u7684\u52a0\u901f\uff0c\u4e14\u6027\u80fd\u53ef\u6bd4\u3002", "conclusion": "CTP\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u79bb\u7ebf\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u5ef6\u8fdf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u89c4\u5212\u7684\u6548\u7387\u74f6\u9888\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.08845", "pdf": "https://arxiv.org/pdf/2507.08845", "abs": "https://arxiv.org/abs/2507.08845", "authors": ["Irfan Ullah", "Young-Koo Lee"], "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from\ngraph-structured data, however uniform neighbor sampling and static fanout\nsettings frequently limit GNNs' scalability and efficiency. In this paper, we\npropose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel\napproach that dynamically adjusts the fanout based on model performance and\nprioritizes important nodes during training. Our approach leverages node\nscoring based on node degree to focus computational resources on structurally\nimportant nodes, incrementing the fanout as the model training progresses.\nDAFOS also integrates an early stopping mechanism to halt training when\nperformance gains diminish. Experiments conducted on three benchmark datasets,\nogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach\nsignificantly improves training speed and accuracy compared to a\nstate-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv\ndataset and a 12.6x speedup on the Reddit dataset while improving the F1 score\nfrom 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the\nogbn-products dataset, respectively. These results highlight the potential of\nDAFOS as an efficient and scalable solution for large-scale GNN training.", "AI": {"tldr": "\u63d0\u51faDAFOS\uff0c\u4e00\u79cd\u52a8\u6001\u81ea\u9002\u5e94\u6247\u51fa\u4f18\u5316\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u4f18\u5148\u91cd\u8981\u8282\u70b9\u548c\u52a8\u6001\u8c03\u6574\u6247\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7edf\u4e00\u90bb\u5c45\u91c7\u6837\u548c\u9759\u6001\u6247\u51fa\u8bbe\u7f6e\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u81ea\u9002\u5e94\u6247\u51fa\u4f18\u5316\u91c7\u6837\u5668\uff08DAFOS\uff09\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u6a21\u578b\u6027\u80fd\u52a8\u6001\u8c03\u6574\u6247\u51fa\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5148\u9009\u62e9\u91cd\u8981\u8282\u70b9\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5229\u7528\u57fa\u4e8e\u8282\u70b9\u5ea6\u7684\u8282\u70b9\u8bc4\u5206\u6765\u96c6\u4e2d\u8ba1\u7b97\u8d44\u6e90\u4e8e\u7ed3\u6784\u4e0a\u91cd\u8981\u7684\u8282\u70b9\uff0c\u5e76\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u589e\u52a0\u6247\u51fa\u3002DAFOS\u8fd8\u96c6\u6210\u4e86\u65e9\u505c\u673a\u5236\uff0c\u4ee5\u5728\u6027\u80fd\u63d0\u5347\u51cf\u5c0f\u65f6\u505c\u6b62\u8bad\u7ec3\u3002", "result": "DAFOS\u5728ogbn-arxiv\u3001Reddit\u548cogbn-products\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u5176\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863.57\u500d\u52a0\u901f\uff0c\u5728Reddit\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8612.6\u500d\u52a0\u901f\uff1b\u540c\u65f6\uff0c\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u4ece68.5%\u63d0\u5347\u81f371.21%\uff0c\u5728ogbn-products\u6570\u636e\u96c6\u4e0a\u4ece73.78%\u63d0\u5347\u81f376.88%\u3002", "conclusion": "DAFOS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002"}}
{"id": "2507.09092", "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684CNN\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5MI CAM\uff0c\u80fd\u63d0\u4f9b\u56e0\u679c\u89e3\u91ca\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u90e8\u5206\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u9274\u4e8e\u673a\u5668\u89c6\u89c9\u5728\u533b\u7597\u548c\u81ea\u52a8\u5316\u5de5\u5382\u7b49\u5173\u952e\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u673a\u5236\u53ca\u5176\u63d0\u4f9b\u7279\u5b9a\u63a8\u65ad\u7684\u539f\u56e0\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMI CAM\u7684\u65b0\u578b\u540e\u9a8c\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6fc0\u6d3b\u6620\u5c04\u3002\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u4e0d\u540c\uff0cMI CAM\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u7279\u5f81\u56fe\u4e0e\u8f93\u5165\u56fe\u50cf\u7684\u4e92\u4fe1\u606f\u8fdb\u884c\u52a0\u6743\uff0c\u5e76\u901a\u8fc7\u6743\u91cd\u4e0e\u6fc0\u6d3b\u56fe\u7684\u7ebf\u6027\u7ec4\u5408\u751f\u6210\u663e\u8457\u6027\u53ef\u89c6\u5316\u3002\u5b83\u8fd8\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u56e0\u679c\u89e3\u91ca\u80fd\u529b\u3002", "result": "MI CAM\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u4e0e\u6240\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u67d0\u4e9b\u65b9\u9762\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "MI CAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u3001\u65e0\u504f\u7684\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u63ed\u793aCNN\u7684\u51b3\u7b56\u539f\u56e0\uff0c\u5e76\u5728\u89e3\u91ca\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u751a\u81f3\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u3002"}}
{"id": "2507.10210", "pdf": "https://arxiv.org/pdf/2507.10210", "abs": "https://arxiv.org/abs/2507.10210", "authors": ["Thijs Havinga", "Xianjun Jiao", "Wei Liu", "Baiheng Chen", "Robbe Gaeremynck", "Ingrid Moerman"], "title": "Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit", "categories": ["cs.NI"], "comment": "6 pages, 7 figures. Submitted to GLOBECOM 2025", "summary": "Proper coordination is needed to guarantee the performance of wireless\nnetworks in dense deployments. Contention-based systems suffer badly in terms\nof latency when multiple devices compete for the same resources. Coordinated\nOrthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi\n8 to remedy this, as it enables multiple Access Points (APs) to share spectrum\nmore efficiently. However, fine-grained resource allocation, namely within\n20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling\noverhead and complexity in terms of physical layer signaling. A wired backhaul\nmitigates the need for over-the-air scheduling and synchronization, and it\nallows for coordination even if APs are not in each others' range. Furthermore,\nit forms the basis for more advanced multi-AP coordination schemes like\ncoordinated beamforming and joint transmission. In this work we demonstrate the\nrealization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,\nenabled by the open-source platforms openwifi and White Rabbit. We show that\nthe performance in terms of carrier frequency offset pre-compensation and time\nsynchronization between two APs exceeds related wireless standard requirements.\nFurthermore, the quality of the received constellation of the Co-OFDMA frame as\nreported by a wireless connectivity tester is better than individual frames\nsent by the APs.", "AI": {"tldr": "\u672c\u6587\u6f14\u793a\u4e86\u4f7f\u7528\u5149\u7ea4\u56de\u4f20\u5b9e\u73b0Wi-Fi 6\u517c\u5bb9\u7684\u7ec6\u7c92\u5ea6Co-OFDMA\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u540c\u6b65\u6027\u80fd\u548c\u63a5\u6536\u4fe1\u53f7\u8d28\u91cf\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u5bc6\u96c6\u90e8\u7f72\u7684\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u4f20\u7edf\u7ade\u4e89\u673a\u5236\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3002\u5c3d\u7ba1Wi-Fi 8\u63d0\u51fa\u4e86Co-OFDMA\u4ee5\u66f4\u9ad8\u6548\u5730\u5171\u4eab\u9891\u8c31\uff0c\u4f46\u5176\u7ec6\u7c92\u5ea6\u8d44\u6e90\u5206\u914d\u56e0\u7a7a\u4e2d\u8c03\u5ea6\u5f00\u9500\u548c\u7269\u7406\u5c42\u590d\u6742\u6027\u88ab\u8ba4\u4e3a\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6709\u7ebf\u56de\u4f20\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u5148\u8fdb\u7684\u591aAP\u534f\u8c03\u3002", "method": "\u5229\u7528\u5149\u7ea4\u56de\u4f20\u3001\u5f00\u6e90\u5e73\u53f0openwifi\u548cWhite Rabbit\uff0c\u5b9e\u73b0\u4e86Wi-Fi 6\u517c\u5bb9\u7684\u7ec6\u7c92\u5ea6Co-OFDMA\uff0c\u5e76\u8fdb\u884c\u4e86\u53ccAP\u95f4\u7684\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u5728\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u9884\u8865\u507f\u548c\u65f6\u95f4\u540c\u6b65\u65b9\u9762\uff0c\u53ccAP\u95f4\u7684\u6027\u80fd\u8d85\u8d8a\u4e86\u76f8\u5173\u65e0\u7ebf\u6807\u51c6\u8981\u6c42\u3002\u6b64\u5916\uff0cCo-OFDMA\u5e27\u7684\u63a5\u6536\u661f\u5ea7\u56fe\u8d28\u91cf\u4f18\u4e8e\u5355\u4e2aAP\u53d1\u9001\u7684\u5e27\u3002", "conclusion": "\u901a\u8fc7\u5149\u7ea4\u56de\u4f20\uff0c\u53ef\u4ee5\u6210\u529f\u5b9e\u73b0Wi-Fi 6\u517c\u5bb9\u7684\u7ec6\u7c92\u5ea6Co-OFDMA\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u534f\u8c03\u6027\u80fd\u548c\u9891\u8c31\u6548\u7387\u3002"}}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u8fc4\u4eca\u6700\u5927\u7684\u85cf\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3\u5e76\u63a8\u51fa\u4e86\u591a\u8bed\u8a00\u5927\u6a21\u578bBanzhida\uff0c\u5728\u5404\u9879\u85cf\u8bed\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u85cf\u8bed\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u56e0\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\u7a00\u7f3a\u800c\u5728\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5b58\u5728\u6027\u80fd\u9e3f\u6c9f\u3002", "method": "1. \u805a\u5408\u591a\u6e90\u6570\u636e\u5e76\u5e94\u7528\u4e13\u7528\u6e05\u6d17\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u8fc4\u4eca\u6700\u5927\u7684\u85cf\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u30022. \u5229\u7528\u8be5\u8bed\u6599\u5e93\uff0c\u5bf9\u4e00\u4e2a\u591a\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3/\u540e\u8bad\u7ec3\uff0c\u5f97\u5230Banzhida\u30023. \u521b\u5efa\u65b0\u7684\u9ad8\u8d28\u91cf\u85cf\u8bed\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u516c\u5171\u57fa\u51c6\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "Banzhida\u5728\u5e7f\u6cdb\u7684\u85cf\u8bed\u4efb\u52a1\u4e2d\uff0c\u6301\u7eed\u4e14\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u548c\u73b0\u6709\u85cf\u8bed\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "Banzhida\u6210\u529f\u63d0\u5347\u4e86\u85cf\u8bed\u751f\u6210\u5f0fAI\u7684\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5927\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u548c\u5f3a\u52b2\u8868\u73b0\u3002"}}
{"id": "2507.09540", "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8eMetropolis-Hastings (MH) \u91c7\u6837\u7684\u975e\u68af\u5ea6SNN\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5728\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfDQL\u548c\u73b0\u6709SNN-RL\u65b9\u6cd5\uff0c\u540c\u65f6\u8282\u7701\u8d44\u6e90\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc (SNNs) \u5728\u5b9e\u65f6\u63a7\u5236\u4e2d\u80fd\u6548\u9ad8\uff0c\u4f46\u7531\u4e8e\u5176\u901a\u4fe1\u673a\u5236\u4e0d\u53ef\u5fae\uff0c\u5728\u5f3a\u5316\u5b66\u4e60 (RL) \u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528Metropolis-Hastings (MH) \u91c7\u6837\uff08\u4e00\u79cd\u8d1d\u53f6\u65af\u63a8\u65ad\u6280\u672f\uff09\u6765\u8bad\u7ec3SNNs\uff0c\u7528\u4e8eRL\u73af\u5883\u4e2d\u7684\u52a8\u6001\u667a\u80fd\u4f53\u63a7\u5236\uff0c\u65e0\u9700\u4f9d\u8d56\u68af\u5ea6\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7d2f\u79ef\u5956\u52b1\u4fe1\u53f7\u8fed\u4ee3\u5730\u63d0\u8bae\u5e76\u6982\u7387\u6027\u5730\u63a5\u53d7\u7f51\u7edc\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728AcroBot\u548cCartPole\u4e24\u4e2a\u6807\u51c6\u63a7\u5236\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u57fa\u4e8eMH\u7684\u65b9\u6cd5\u5728\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3001\u6700\u5c0f\u5316\u7f51\u7edc\u8d44\u6e90\u548c\u8bad\u7ec3\u56de\u5408\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6Q\u5b66\u4e60 (DQL) \u57fa\u7ebf\u548c\u5148\u524d\u7684\u57fa\u4e8eSNN\u7684RL\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8eMetropolis-Hastings (MH) \u91c7\u6837\u7684\u975e\u68af\u5ea6\u8bad\u7ec3\u65b9\u6cd5\u662f\u89e3\u51b3SNNs\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u6311\u6218\u7684\u6709\u6548\u9014\u5f84\uff0c\u5b83\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd8\u80fd\u4f18\u5316\u8d44\u6e90\u5229\u7528\uff0c\u5e76\u4e3a\u76f4\u63a5\u5728\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u4e0a\u90e8\u7f72SNNs\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2507.08848", "pdf": "https://arxiv.org/pdf/2507.08848", "abs": "https://arxiv.org/abs/2507.08848", "authors": ["Calum Corrie Imrie", "Ioannis Stefanakos", "Sepeedeh Shahbeigi", "Richard Hawkins", "Simon Burton"], "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SE"], "comment": null, "summary": "The rapid advancement of machine learning (ML) has led to its increasing\nintegration into cyber-physical systems (CPS) across diverse domains. While CPS\noffer powerful capabilities, incorporating ML components introduces significant\nsafety and assurance challenges. Among ML techniques, reinforcement learning\n(RL) is particularly suited for CPS due to its capacity to handle complex,\ndynamic environments where explicit models of interaction between system and\nenvironment are unavailable or difficult to construct. However, in\nsafety-critical applications, this learning process must not only be effective\nbut demonstrably safe. Safe-RL methods aim to address this by incorporating\nsafety constraints during learning, yet they fall short in providing systematic\nassurance across the RL lifecycle. The AMLAS methodology offers structured\nguidance for assuring the safety of supervised learning components, but it does\nnot directly apply to the unique challenges posed by RL. In this paper, we\nadapt AMLAS to provide a framework for generating assurance arguments for an\nRL-enabled system through an iterative process; AMLAS-RL. We demonstrate\nAMLAS-RL using a running example of a wheeled vehicle tasked with reaching a\ntarget goal without collision.", "AI": {"tldr": "\u63d0\u51faAMLAS-RL\u6846\u67b6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u7cfb\u7edf\u6027\u4fdd\u8bc1\u8bba\u8bc1\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u5176\u5b89\u5168\u6027\u4fdd\u969c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4fdd\u8bc1\uff0c\u4e14\u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u5b89\u5168\u6846\u67b6\uff08\u5982AMLAS\uff09\u4e0d\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u6539\u7f16\u5e76\u6269\u5c55\u4e86AMLAS\u65b9\u6cd5\uff0c\u63d0\u51faAMLAS-RL\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u542f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u7cfb\u7edf\u751f\u6210\u5b89\u5168\u4fdd\u8bc1\u8bba\u8bc1\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u5c55\u793a\u4e86AMLAS-RL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u7cfb\u7edf\u63d0\u4f9b\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u4fdd\u8bc1\u8bba\u8bc1\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u8f6e\u5f0f\u8f66\u8f86\u907f\u649e\u793a\u4f8b\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "conclusion": "AMLAS-RL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u578b\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u5e94\u7528\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b89\u5168\u4fdd\u8bc1\u7684\u96be\u9898\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.09097", "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRadEyeVideo\uff0c\u4e00\u79cd\u5c06\u653e\u5c04\u79d1\u533b\u751f\u773c\u52a8\u6ce8\u89c6\u6570\u636e\u4f5c\u4e3a\u89c6\u9891\u5e8f\u5217\u6574\u5408\u5230\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80f8\u90e8X\u5149(CXR)\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u901a\u7528\u6a21\u578b\u8d85\u8d8a\u4e86\u7279\u5b9a\u4efb\u52a1\u7684\u533b\u5b66LVLMs\u3002", "motivation": "LVLMs\u5728CXR\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u7ed3\u5408\u653e\u5c04\u79d1\u533b\u751f\u773c\u52a8\u6570\u636e\u7684\u65b9\u6cd5\uff08\u5982\u70ed\u56fe\u6216\u6587\u672c\u63d0\u793a\uff09\u5ffd\u7565\u4e86\u773c\u52a8\u987a\u5e8f\u3002\u8fd9\u79cd\u987a\u5e8f\u80fd\u63d0\u4f9b\u5173\u4e8e\u5174\u8da3\u533a\u57df\u548c\u68c0\u67e5\u987a\u5e8f\u7684\u5b9d\u8d35\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u773c\u52a8\u65f6\u7a7a\u52a8\u6001\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRadEyeVideo\u65b9\u6cd5\uff0c\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u773c\u52a8\u6ce8\u89c6\u6570\u636e\u4f5c\u4e3a\u89c6\u9891\u5e8f\u5217\u6574\u5408\u3002\u8be5\u65b9\u6cd5\u5728CXR\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u4e09\u4e2a\u652f\u6301\u89c6\u9891\u8f93\u5165\u7684\u901a\u7528\u9886\u57df\u5f00\u6e90LVLMs\u3002", "result": "\u5f53LVLMs\u63a5\u6536\u773c\u52a8\u89c6\u9891\u63d0\u793a\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff1a\u62a5\u544a\u751f\u6210\u4efb\u52a1\u63d0\u5347\u9ad8\u8fbe24.6%\uff0c\u4e24\u9879\u4efb\u52a1\u5e73\u5747\u63d0\u534715.2%\uff08\u6309\u6bd4\u4f8b\u8bc4\u4f30\u6307\u6807\uff09\u3002RadEyeVideo\u8fd8\u4f7f\u901a\u7528\u9886\u57dfLVLM\uff08LLaVA-OneVision\uff09\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u7279\u5b9a\u4efb\u52a1\u7684\u533b\u5b66LVLMs\uff08\u5982MAIRA-2\u548cCheXagent\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6709\u6548\u6574\u5408\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff08\u672c\u4f8b\u4e2d\u4e3a\u773c\u52a8\u4fe1\u606f\uff09\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u901a\u7528\u6a21\u578b\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002RadEyeVideo\u662f\u5b9e\u73b0LVLMs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u53ef\u6269\u5c55\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u65b9\u6cd5\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.10510", "pdf": "https://arxiv.org/pdf/2507.10510", "abs": "https://arxiv.org/abs/2507.10510", "authors": ["Jiangkai Wu", "Zhiyuan Ren", "Liming Liu", "Xinggong Zhang"], "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI", "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.MM"], "comment": null, "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.", "AI": {"tldr": "\u9488\u5bf9AI\u89c6\u9891\u804a\u5929\u4e2d\u56e0MLLM\u63a8\u7406\u548c\u7f51\u7edc\u4f20\u8f93\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Artic\u5b9e\u65f6\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u9891\u6d41\u548c\u6297\u4e22\u5305\u81ea\u9002\u5e94\u5e27\u7387\u6280\u672f\u4f18\u5316\u89c6\u9891\u4f20\u8f93\u6548\u7387\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30MLLM\u89c6\u9891\u7406\u89e3\u6027\u80fd\u7684\u57fa\u51c6DeViBench\u3002", "motivation": "AI\u89c6\u9891\u804a\u5929\u65e8\u5728\u5b9e\u73b0\u66f4\u76f4\u89c2\u3001\u5982\u771f\u4eba\u822c\u7684AI-\u4eba\u4ea4\u4e92\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u63a8\u7406\u8017\u65f6\u8fc7\u957f\uff0c\u4e8c\u662f\u7f51\u7edc\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u7684\u4f20\u8f93\u5ef6\u8fdf\uff0c\u4e24\u8005\u5171\u540c\u9020\u6210\u9ad8\u5ef6\u8fdf\uff0c\u4e25\u91cd\u963b\u788d\u4e86AI\u7684\u5b9e\u65f6\u3001\u6d41\u7545\u4e92\u52a8\u4f53\u9a8c\u3002", "method": "1. \u63d0\u51faAI\u5bfc\u5411\u7684Artic\u5b9e\u65f6\u901a\u4fe1\u6846\u67b6\uff0c\u5c06\u7f51\u7edc\u9700\u6c42\u4ece\u201c\u4eba\u89c2\u770b\u89c6\u9891\u201d\u8f6c\u53d8\u4e3a\u201cAI\u7406\u89e3\u89c6\u9891\u201d\u30022. \u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u9891\u6d41\u6280\u672f\uff0c\u6839\u636e\u89c6\u9891\u533a\u57df\u5bf9\u804a\u5929\u7684\u91cd\u8981\u6027\u667a\u80fd\u5206\u914d\u6bd4\u7279\u7387\uff0c\u4ee5\u964d\u4f4e\u5e26\u5bbd\u5e76\u4fdd\u6301MLLM\u51c6\u786e\u6027\u30023. \u8bbe\u8ba1\u6297\u4e22\u5305\u81ea\u9002\u5e94\u5e27\u7387\u673a\u5236\uff0c\u5229\u7528\u5148\u524d\u5e27\u66ff\u4ee3\u4e22\u5931/\u5ef6\u8fdf\u5e27\uff0c\u907f\u514d\u91cd\u4f20\u548c\u6bd4\u7279\u7387\u6d6a\u8d39\u30024. \u6784\u5efa\u9996\u4e2a\u9000\u5316\u89c6\u9891\u7406\u89e3\u57fa\u51c6\uff08DeViBench\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u6d41\u8d28\u91cf\u5bf9MLLM\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u672c\u6587\u6210\u529f\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86Artic\u6846\u67b6\u53ca\u5176\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86AI\u89c6\u9891\u804a\u5929\u7684\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u7ef4\u6301\u4e86MLLM\u7684\u51c6\u786e\u6027\uff0c\u6709\u6548\u907f\u514d\u4e86\u4e22\u5305\u91cd\u4f20\u548c\u6bd4\u7279\u7387\u6d6a\u8d39\u3002\u6b64\u5916\uff0c\u8fd8\u6210\u529f\u6784\u5efa\u4e86DeViBench\uff0c\u4e3a\u8bc4\u4f30AI\u89c6\u9891\u804a\u5929\u4e2d\u89c6\u9891\u6d41\u8d28\u91cf\u5bf9MLLM\u6027\u80fd\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u6807\u51c6\u5de5\u5177\u3002", "conclusion": "Artic\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u89c6\u9891\u6d41\u548c\u5e27\u7387\u7ba1\u7406\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u89c6\u9891\u804a\u5929\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u548c\u5e26\u5bbd\u6548\u7387\u95ee\u9898\uff0c\u63a8\u52a8\u4e86AI\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u66f4\u81ea\u7136\u3001\u5b9e\u65f6\u7684\u4ea4\u4e92\u3002DeViBench\u7684\u5efa\u7acb\u5219\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u672a\u6765AI\u89c6\u9891\u804a\u5929\u6280\u672f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09225", "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "categories": ["cs.CL", "cs.CY"], "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u56fe\u50cf\u6570\u636e\u5e93\uff08MetaClimage\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u4f20\u64ad\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\u89c6\u89c9\u9690\u55bb\u7406\u89e3\u96be\u5ea6\u8f83\u9ad8\u4f46\u66f4\u5177\u7f8e\u611f\uff0c\u867d\u5728\u6548\u80fd\u548c\u5524\u8d77\u6027\u4e0a\u4e0e\u5b57\u9762\u56fe\u50cf\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u80fd\u4fc3\u8fdb\u66f4\u6df1\u5c42\u6b21\u7684\u8ba4\u77e5\u52a0\u5de5\u548c\u66f4\u79ef\u6781\u7684\u4f53\u9a8c\uff0c\u4e3a\u73af\u5883\u4f20\u64ad\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u88ab\u8ba4\u4e3a\u662f\u5e94\u5bf9\u73af\u5883\u6311\u6218\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u6750\u6599\u5206\u6563\uff0c\u5c11\u6709\u7814\u7a76\u7cfb\u7edf\u5730\u68c0\u9a8c\u5176\u5bf9\u4f20\u64ad\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u548c\u5b57\u9762\u56fe\u50cf\u7684MetaClimage\u6570\u636e\u5e93\uff0c\u5e76\u6536\u96c6\u4e86\u4eba\u7c7b\u5bf9\u5176\u7406\u89e3\u96be\u5ea6\u3001\u4f20\u64ad\u6548\u80fd\u3001\u827a\u672f\u8d28\u91cf\u3001\u60c5\u7eea\u5524\u8d77\u53ca\u603b\u7ed3\u6807\u7b7e\u7684\u8bc4\u5206\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u4ece\u6807\u7b7e\u4e2d\u63d0\u53d6\u4e86\u8bed\u4e49\u548c\u60c5\u611f\u53d8\u91cf\u3002", "result": "\u89c6\u89c9\u9690\u55bb\u88ab\u8bc4\u4ef7\u4e3a\u66f4\u96be\u7406\u89e3\u4f46\u66f4\u5177\u7f8e\u611f\uff1b\u5728\u6548\u80fd\u548c\u5524\u8d77\u6027\u4e0a\u4e0e\u5b57\u9762\u56fe\u50cf\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5bf9\u4e8e\u8ba4\u77e5\u9700\u6c42\u9ad8\u7684\u53c2\u4e0e\u8005\uff0c\u89c6\u89c9\u9690\u55bb\u7684\u5524\u8d77\u6027\u66f4\u9ad8\u3002\u6b64\u5916\uff0c\u89c6\u89c9\u9690\u55bb\u83b7\u5f97\u4e86\u66f4\u591a\u6807\u7b7e\uff0c\u5e38\u63d0\u53ca\u56fe\u50cf\u4e2d\u672a\u76f4\u63a5\u63cf\u7ed8\u7684\u5b9e\u4f53\uff0c\u5e76\u5f15\u53d1\u4e86\u66f4\u79ef\u6781\u3001\u66f4\u5177\u652f\u914d\u6027\u7684\u8bcd\u8bed\u3002\u8fd9\u8868\u660e\u89c6\u89c9\u9690\u55bb\u7684\u8ba4\u77e5\u8d1f\u8377\u66f4\u9ad8\uff0c\u4f46\u53ef\u80fd\u8bf1\u5bfc\u66f4\u6df1\u5c42\u6b21\u7684\u8ba4\u77e5\u52a0\u5de5\u548c\u62bd\u8c61\u601d\u7ef4\u3002", "conclusion": "\u5c3d\u7ba1\u89c6\u89c9\u9690\u55bb\u5b58\u5728\u8ba4\u77e5\u8d1f\u8377\uff0c\u4f46\u5b83\u4eec\u80fd\u5e26\u6765\u66f4\u4f73\u7684\u5ba1\u7f8e\u4f53\u9a8c\u548c\u66f4\u79ef\u6781\u7684\u611f\u53d7\uff0c\u5e76\u53ef\u80fd\u4fc3\u4f7f\u66f4\u6df1\u5c42\u6b21\u7684\u8ba4\u77e5\u3002\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6570\u636e\u5e93\u548c\u9610\u660e\u5229\u5f0a\u6743\u8861\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u7684\u5f71\u54cd\uff0c\u4e3a\u73af\u5883\u4f20\u64ad\u7b56\u7565\u7684\u5236\u5b9a\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.09588", "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "categories": ["cs.AI"], "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "AI": {"tldr": "eSapiens\u662f\u4e00\u4e2aAI\u5373\u670d\u52a1\u5e73\u53f0\uff0c\u901a\u8fc7\u6574\u5408\u4e13\u6709\u6570\u636e\u3001\u5de5\u4f5c\u6d41\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8d4b\u80fd\u4f01\u4e1a\u5b9e\u73b0AI\u8d44\u4ea7\u63a7\u5236\u3001\u77e5\u8bc6\u5185\u5316\u548c\u6570\u636e\u5b89\u5168\uff0c\u5e76\u901a\u8fc7AI Agent\u63d0\u4f9b\u6d1e\u5bdf\u548c\u81ea\u52a8\u5316\u3002\u5176\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002", "motivation": "\u4f01\u4e1a\u9700\u8981\u5b8c\u5168\u63a7\u5236\u5176AI\u8d44\u4ea7\u3001\u5b9e\u73b0AI\u77e5\u8bc6\u5185\u90e8\u4fdd\u7559\u3001\u786e\u4fdd\u6570\u636e\u5b89\u5168\uff0c\u5e76\u5229\u7528AI\u63d0\u5347\u56e2\u961f\u6548\u7387\u3001\u81ea\u52a8\u5316\u91cd\u590d\u4efb\u52a1\u4ee5\u9a71\u52a8\u66f4\u597d\u7684\u4e1a\u52a1\u6210\u679c\u3002", "method": "eSapiens\u5e73\u53f0\u6574\u5408\u4e86\u7ed3\u6784\u5316\u6587\u6863\u6444\u53d6\u3001\u6df7\u5408\u5411\u91cf\u68c0\u7d22\u548cLangChain\u65e0\u4ee3\u7801\u7f16\u6392\uff0c\u652f\u6301\u591a\u79cd\u4e3b\u6d41LLM\u3002\u5173\u952e\u7ec4\u4ef6THOR Agent\u8d1f\u8d23\u5904\u7406SQL\u67e5\u8be2\u5e76\u751f\u6210\u4f01\u4e1a\u6570\u636e\u5e93\u6d1e\u5bdf\u3002\u7cfb\u7edf\u901a\u8fc7\u6cd5\u5f8b\u8bed\u6599\u5e93\u4e0a\u7684\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff08\u4f18\u5316\u5757\u5927\u5c0f\uff09\u548cTRACe\u6307\u6807\u7684\u751f\u6210\u8d28\u91cf\u6d4b\u8bd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u68c0\u7d22\u5b9e\u9a8c\u4e2d\uff0c512 tokens\u7684\u5757\u5927\u5c0f\u5728\u6cd5\u5f8b\u8bed\u6599\u5e93\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u68c0\u7d22\u7cbe\u5ea6\uff08Top-3\u51c6\u786e\u7387\uff1a91.3%\uff09\u3002\u5728\u751f\u6210\u8d28\u91cf\u6d4b\u8bd5\u4e2d\uff0ceSapiens\u7684\u8f93\u51fa\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u4e8b\u5b9e\u5bf9\u9f50\u5ea6\u63d0\u5347\u9ad8\u8fbe23%\u3002", "conclusion": "eSapiens\u5e73\u53f0\u80fd\u6709\u6548\u652f\u6301\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u6cd5\u5f8b\u548c\u91d1\u878d\uff09\u4e2d\u53ef\u4fe1\u8d56\u3001\u53ef\u5ba1\u8ba1\u7684AI\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2507.08858", "pdf": "https://arxiv.org/pdf/2507.08858", "abs": "https://arxiv.org/abs/2507.08858", "authors": ["Sami Achour", "Yassine Bouher", "Duong Nguyen", "Nicolas Chesneau"], "title": "Foundation models for time series forecasting: Application in conformal prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5728\u4e00\u81f4\u6027\u9884\u6d4b\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u533a\u95f4\u548c\u66f4\u7a33\u5b9a\u7684\u6821\u51c6\u8fc7\u7a0b\u3002", "motivation": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8fdb\u884c\u4e00\u81f4\u6027\u9884\u6d4b\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u56e0\u4e3a\u53ef\u4ee5\u5c06\u5927\u90e8\u5206\u53ef\u7528\u6570\u636e\u7528\u4e8e\u6821\u51c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83TSFMs\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u9884\u6d4b\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u4e0e\u5305\u62ec\u7edf\u8ba1\u6a21\u578b\u548c\u68af\u5ea6\u63d0\u5347\u5728\u5185\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u4e00\u81f4\u6027\u9884\u6d4b\u73af\u5883\u4e0b\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "1. \u5728\u6570\u636e\u91cf\u6709\u9650\u65f6\uff0cTSFMs\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u80fd\u63d0\u4f9b\u6bd4\u7ecf\u5178\u6a21\u578b\u66f4\u53ef\u9760\u7684\u4e00\u81f4\u6027\u9884\u6d4b\u533a\u95f4\u30022. \u7531\u4e8e\u66f4\u591a\u6570\u636e\u53ef\u7528\u4e8e\u6821\u51c6\uff0cTSFMs\u7684\u6821\u51c6\u8fc7\u7a0b\u66f4\u7a33\u5b9a\u30023. \u6570\u636e\u91cf\u8d8a\u5c11\uff0c\u8fd9\u4e9b\u4f18\u52bf\u8d8a\u660e\u663e\uff0c\u56e0\u4e3a\u7ecf\u5178\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u63d0\u9ad8\u4e00\u81f4\u6027\u9884\u6d4b\u53ef\u9760\u6027\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.09102", "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faPointSD\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684Stable Diffusion\uff08SD\uff09\u6a21\u578b\u63d0\u53473D\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u529b\uff0c\u65e8\u5728\u514b\u670d3D\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u7684\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u76843D\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u56e0\u8bad\u7ec3\u4e8e\u89c4\u6a21\u6709\u9650\u76843D\u6570\u636e\u96c6\u800c\u6027\u80fd\u53d7\u9650\u3002\u4f5c\u8005\u5047\u8bbe\uff0c\u5229\u7528\u5728\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u7279\u522b\u662fStable Diffusion\uff09\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86PointSD\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528Stable Diffusion\u6a21\u578b\u8fdb\u884c3D\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a1) \u5c06SD\u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u66ff\u6362\u4e3a3D\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u4e00\u4e2a\u70b9\u4e91\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u70b9\u4e91\u80fd\u591f\u5f15\u5bfc\u6e32\u67d3\u566a\u58f0\u56fe\u50cf\u7684\u53bb\u566a\u8fc7\u7a0b\u30022) \u5229\u7528\u8bad\u7ec3\u597d\u7684\u70b9\u4e91\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u65e0\u566a\u58f0\u56fe\u50cf\u4e3a\u8f93\u5165\u3001\u70b9\u4e91\u4e3a\u6761\u4ef6\uff0c\u63d0\u53d6SD\u7279\u5f81\u30023) \u901a\u8fc7\u5c063D\u9aa8\u5e72\u7f51\u7edc\u7684\u7279\u5f81\u4e0e\u63d0\u53d6\u5230\u7684SD\u7279\u5f81\u5bf9\u9f50\uff0c\u8bad\u7ec33D\u9aa8\u5e72\u7f51\u7edc\u4ee5\u5b9e\u73b0\u76f4\u63a5\u7684\u8bed\u4e49\u5b66\u4e60\u3002", "result": "\u5728\u4e0b\u6e38\u70b9\u4e91\u4efb\u52a1\u548c\u6d88\u878d\u7814\u7a76\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\uff0cStable Diffusion\u6a21\u578b\u80fd\u591f\u6709\u6548\u589e\u5f3a\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "Stable Diffusion\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u5730\u5e94\u7528\u4e8e3D\u70b9\u4e91\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\uff0c\u4ece\u800c\u5f25\u8865\u4e863D\u6570\u636e\u96c6\u89c4\u6a21\u53d7\u9650\u6240\u5e26\u6765\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.09288", "pdf": "https://arxiv.org/pdf/2507.09288", "abs": "https://arxiv.org/abs/2507.09288", "authors": ["Javier Blanco-Romero", "Pedro Otero Garc\u00eda", "Daniel Sobral-Blanco", "Florina Almenares Mendoza", "Ana Fern\u00e1ndez Vilas", "Manuel Fern\u00e1ndez-Veiga"], "title": "Hybrid Quantum Security for IPsec", "categories": ["cs.CR", "cs.NI"], "comment": "23 pages, 6 figures, quantum key distribution, post-quantum\n  cryptography, IPsec security protocols", "summary": "Quantum Key Distribution (QKD) offers information-theoretic security against\nquantum computing threats, but integrating QKD into existing security protocols\nremains an unsolved challenge due to fundamental mismatches between\npre-distributed quantum keys and computational key exchange paradigms. This\npaper presents the first systematic comparison of sequential versus parallel\nhybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental\nprotocol design principles that extend beyond specific implementations. We\nintroduce two novel approaches for incorporating QKD into Internet Key Exchange\nversion 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS\nQKD 014 stateless API specifications: (1) a pure QKD approach that replaces\ncomputational key derivation with identifier-based quantum key coordination,\nand (2) a unified QKD-KEM abstraction that enables parallel composition of\nquantum and post-quantum cryptographic methods within existing protocol\nframeworks. Our key insight is that parallel hybrid approaches eliminate the\nmultiplicative latency penalties inherent in sequential methods mandated by RFC\n9370, achieving significant performance improvements under realistic network\nconditions. Performance evaluation using a Docker-based testing framework with\nIDQuantique QKD hardware demonstrates that the parallel hybrid approach\nsignificantly outperforms sequential methods under network latency conditions,\nwhile pure QKD achieves minimal bandwidth overhead through identifier-based key\ncoordination. Our implementations provide practical quantum-enhanced IPsec\nsolutions suitable for critical infrastructure deployments requiring\ndefense-in-depth security.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u4e0e\u73b0\u6709\u5b89\u5168\u534f\u8bae\uff08\u5982IPsec\uff09\u7684\u96c6\u6210\u6311\u6218\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u5e76\u884c\u6df7\u5408QKD-PQC\u548c\u7eafQKD\u4e24\u79cd\u5bc6\u94a5\u5efa\u7acb\u7b56\u7565\u3002\u7814\u7a76\u8868\u660e\uff0c\u5e76\u884c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e32\u884c\u65b9\u6cd5\uff0c\u7eafQKD\u53ef\u5b9e\u73b0\u6700\u5c0f\u5e26\u5bbd\u5f00\u9500\uff0c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "motivation": "\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u4fe1\u606f\u7406\u8bba\u5b89\u5168\u6027\u4ee5\u5bf9\u6297\u91cf\u5b50\u8ba1\u7b97\u5a01\u80c1\uff0c\u4f46\u7531\u4e8e\u9884\u5206\u53d1\u91cf\u5b50\u5bc6\u94a5\u4e0e\u8ba1\u7b97\u5bc6\u94a5\u4ea4\u6362\u8303\u5f0f\u7684\u6839\u672c\u4e0d\u5339\u914d\uff0c\u5c06\u5176\u6709\u6548\u96c6\u6210\u5230IPsec\u7b49\u73b0\u6709\u5b89\u5168\u534f\u8bae\u4e2d\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": ["\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86IPsec\u4e2d\u4e32\u884c\u4e0e\u5e76\u884c\u6df7\u5408QKD-PQC\u5bc6\u94a5\u5efa\u7acb\u7b56\u7565\uff0c\u5e76\u63ed\u793a\u4e86\u666e\u9002\u7684\u534f\u8bae\u8bbe\u8ba1\u539f\u5219\u3002", "\u63d0\u51fa\u4e86\u4e24\u79cd\u5c06QKD\u6574\u5408\u5230IKEv2\u7684\u65b0\u65b9\u6cd5\uff0c\u652f\u6301ETSI GS QKD 004\u6709\u72b6\u6001\u548cETSI GS QKD 014\u65e0\u72b6\u6001API\u89c4\u8303\uff1a1) \u7eafQKD\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6807\u8bc6\u7b26\u7684\u91cf\u5b50\u5bc6\u94a5\u534f\u8c03\u53d6\u4ee3\u8ba1\u7b97\u5bc6\u94a5\u6d3e\u751f\uff1b2) \u7edf\u4e00QKD-KEM\u62bd\u8c61\uff0c\u5728\u73b0\u6709\u534f\u8bae\u6846\u67b6\u5185\u5b9e\u73b0\u91cf\u5b50\u4e0e\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6cd5\u7684\u5e76\u884c\u7ec4\u5408\u3002", "\u4f7f\u7528\u57fa\u4e8eDocker\u7684\u6d4b\u8bd5\u6846\u67b6\u548cIDQuantique QKD\u786c\u4ef6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002"], "result": ["\u5e76\u884c\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86RFC 9370\u89c4\u5b9a\u7684\u4e32\u884c\u65b9\u6cd5\u56fa\u6709\u7684\u4e58\u6cd5\u5ef6\u8fdf\u60e9\u7f5a\uff0c\u5728\u5b9e\u9645\u7f51\u7edc\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "\u5728\u7f51\u7edc\u5ef6\u8fdf\u6761\u4ef6\u4e0b\uff0c\u5e76\u884c\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e32\u884c\u65b9\u6cd5\u3002", "\u7eafQKD\u901a\u8fc7\u57fa\u4e8e\u6807\u8bc6\u7b26\u7684\u5bc6\u94a5\u534f\u8c03\u5b9e\u73b0\u4e86\u6700\u5c0f\u5e26\u5bbd\u5f00\u9500\u3002"], "conclusion": "\u672c\u7814\u7a76\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u91cf\u5b50\u589e\u5f3aIPsec\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u6df1\u5ea6\u9632\u5fa1\u5b89\u5168\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u3002"}}
{"id": "2507.09245", "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "categories": ["cs.CL"], "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\uff0c\u8be5\u4e2d\u5fc3\u6c47\u96c6\u4e86\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u7684\u6570\u636e\u548c\u7b97\u6cd5\uff0c\u65e8\u5728\u63a8\u52a8\u50e7\u4f3d\u7f57\u8bedNLP\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u73b0\u6709\u97f3\u8bd1\u5e94\u7528\u7684\u6bd4\u8f83\u5206\u6790\u3002", "motivation": "\u63a8\u52a8\u50e7\u4f3d\u7f57\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u97f3\u8bd1\u6a21\u578b\u548c\u5f00\u53d1\u6d89\u53ca\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u7684\u5e94\u7528\u7a0b\u5e8f\u65b9\u9762\uff0c\u901a\u8fc7\u63d0\u4f9b\u6240\u9700\u7684\u8d44\u6e90\u89e3\u51b3\u6b64\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u5efa\u7acb\u4e86Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\uff0c\u516c\u5f00\u53d1\u5e03\u4e862020\u5e74\u81f32025\u5e74\u95f4\u5f00\u53d1\u7684\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u7684\u7efc\u5408\u6570\u636e\u8d44\u6e90\u548c\u7b97\u6cd5\u3002\u672c\u6587\u8be6\u7ec6\u6982\u8ff0\u4e86\u8fd9\u4e9b\u8d21\u732e\u8d44\u6e90\uff0c\u5e76\u5bf9\u73b0\u6709\u97f3\u8bd1\u5e94\u7528\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\u6210\u529f\u63d0\u4f9b\u4e86\u5927\u91cf\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u8d44\u6e90\u548c\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u8d44\u6e90\u5728\u63a8\u52a8\u50e7\u4f3d\u7f57\u8bedNLP\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u97f3\u8bd1\u6a21\u578b\u8bad\u7ec3\u548c\u5e94\u7528\u5f00\u53d1\u65b9\u9762\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\u662f\u50e7\u4f3d\u7f57\u8bedNLP\u9886\u57df\u7684\u4e00\u9879\u91cd\u8981\u8d21\u732e\uff0c\u901a\u8fc7\u63d0\u4f9b\u5173\u952e\u7684\u97f3\u8bd1\u6570\u636e\u548c\u5de5\u5177\uff0c\u6709\u529b\u652f\u6301\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u548c\u5e94\u7528\u5f00\u53d1\u3002"}}
{"id": "2507.09611", "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "categories": ["cs.AI", "cs.CY", "68T01"], "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u88ab\u5ffd\u89c6\u7684\u73af\u5883\u548c\u4f26\u7406\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u6e90\u6d88\u8017\u3001\u7535\u5b50\u5783\u573e\u3001\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5e73\u7b49\u53ca\u7f51\u7edc\u5b89\u5168\u80fd\u8017\u56db\u4e2a\u65b9\u9762\uff0c\u5e76\u547c\u5401AI\u53d1\u5c55\u5e94\u4e0e\u4f26\u7406\u8d23\u4efb\u548c\u73af\u5883\u7ba1\u7406\u76f8\u534f\u8c03\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u672a\u6765\u3002", "motivation": "\u5c3d\u7ba1AI\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5176\u5feb\u901f\u6269\u5f20\u5e26\u6765\u7684\u73af\u5883\u548c\u4f26\u7406\u6311\u6218\uff08\u5982\u9ad8\u80fd\u8017\u3001\u7535\u5b50\u5783\u573e\u3001\u8d44\u6e90\u4e0d\u5e73\u7b49\uff09\u5e38\u88ab\u5ffd\u89c6\uff0c\u6709\u5fc5\u8981\u5bf9\u5176\u8d85\u8d8a\u6027\u80fd\u4e4b\u5916\u7684\u5f71\u54cd\u8fdb\u884c\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8fd1\u671f\u7814\u7a76\u548c\u673a\u6784\u62a5\u544a\uff0c\u5206\u6790\u4e86AI\u5728\u80fd\u6e90\u6d88\u8017\u3001\u7535\u5b50\u5783\u573e\u3001\u8ba1\u7b97\u8d44\u6e90\u83b7\u53d6\u4e0d\u5e73\u7b49\u4ee5\u53ca\u7f51\u7edc\u5b89\u5168\u7cfb\u7edf\u9690\u542b\u80fd\u8017\u56db\u4e2a\u5173\u952e\u9886\u57df\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86AI\u9886\u57df\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u95ee\u9898\uff0c\u5305\u62ec\u6a21\u578b\u8bad\u7ec3\u5bfc\u81f4\u7684\u9ad8\u6392\u653e\u3001\u786c\u4ef6\u66f4\u66ff\u52a0\u901f\u3001\u5168\u7403\u57fa\u7840\u8bbe\u65bd\u5dee\u8ddd\u4ee5\u53ca\u4fdd\u969cAI\u5b89\u5168\u7684\u5de8\u5927\u80fd\u6e90\u9700\u6c42\u3002\u6b64\u5916\uff0c\u7efc\u8ff0\u8fd8\u8bc6\u522b\u4e86\u5173\u952e\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "AI\u7684\u8fdb\u6b65\u5fc5\u987b\u4e0e\u4f26\u7406\u8d23\u4efb\u548c\u73af\u5883\u7ba1\u7406\u76f8\u7ed3\u5408\uff0c\u4ee5\u786e\u4fdd\u4e00\u4e2a\u66f4\u5177\u5305\u5bb9\u6027\u548c\u53ef\u6301\u7eed\u6027\u7684\u6280\u672f\u672a\u6765\u3002\u6587\u7ae0\u5021\u5bfc\u63a8\u884c\u53ef\u6301\u7eed\u3001\u900f\u660e\u548c\u516c\u5e73\u7684AI\u5f00\u53d1\u5b9e\u8df5\u3002"}}
{"id": "2507.08860", "pdf": "https://arxiv.org/pdf/2507.08860", "abs": "https://arxiv.org/abs/2507.08860", "authors": ["Awais Manzoor", "M. Atif Qureshi", "Etain Kidney", "Luca Longo"], "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Retention campaigns in customer relationship management often rely on churn\nprediction models evaluated using traditional metrics such as AUC and F1-score.\nHowever, these metrics fail to reflect financial outcomes and may mislead\nstrategic decisions. We introduce e-Profits, a novel business-aligned\nevaluation metric that quantifies model performance based on customer-specific\nvalue, retention probability, and intervention costs. Unlike existing\nprofit-based metrics such as Expected Maximum Profit, which assume fixed\npopulation-level parameters, e-Profits uses Kaplan-Meier survival analysis to\nestimate personalised retention rates and supports granular, per customer\nevaluation. We benchmark six classifiers across two telecom datasets (IBM Telco\nand Maven Telecom) and demonstrate that e-Profits reshapes model rankings\ncompared to traditional metrics, revealing financial advantages in models\npreviously overlooked by AUC or F1-score. The metric also enables segment-level\ninsight into which models maximise return on investment for high-value\ncustomers. e-Profits is designed as an understandable, post hoc tool to support\nmodel evaluation in business contexts, particularly for marketing and analytics\nteams prioritising profit-driven decisions. All source code is available at:\nhttps://github.com/matifq/eprofits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fae-Profits\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4ee5\u4e1a\u52a1\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u7684\u8d22\u52a1\u8868\u73b0\u3002\u5b83\u901a\u8fc7Kaplan-Meier\u751f\u5b58\u5206\u6790\u8fdb\u884c\u4e2a\u6027\u5316\u8bc4\u4f30\uff0c\u5e76\u8bc1\u660e\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u80fd\u8bc6\u522b\u51fa\u5177\u6709\u66f4\u9ad8\u6295\u8d44\u56de\u62a5\u7387\u7684\u6a21\u578b\u3002", "motivation": "\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u4e2d\u7684\u633d\u7559\u6d3b\u52a8\u5e38\u4f9d\u8d56\u4e8e\u6d41\u5931\u9884\u6d4b\u6a21\u578b\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\uff08\u5982AUC\u548cF1-score\uff09\u672a\u80fd\u53cd\u6620\u8d22\u52a1\u7ed3\u679c\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7684\u6218\u7565\u51b3\u7b56\u3002", "method": "\u5f15\u5165e-Profits\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5ba2\u6237\u7279\u5b9a\u4ef7\u503c\u3001\u7559\u5b58\u6982\u7387\u548c\u5e72\u9884\u6210\u672c\u91cf\u5316\u6a21\u578b\u6027\u80fd\u7684\u4e1a\u52a1\u5bf9\u9f50\u8bc4\u4f30\u6307\u6807\u3002\u4e0e\u73b0\u6709\u5229\u6da6\u6307\u6807\u4e0d\u540c\uff0ce-Profits\u5229\u7528Kaplan-Meier\u751f\u5b58\u5206\u6790\u4f30\u8ba1\u4e2a\u6027\u5316\u7559\u5b58\u7387\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u9010\u5ba2\u6237\u8bc4\u4f30\u3002", "result": "\u5728\u4e24\u4e2a\u7535\u4fe1\u6570\u636e\u96c6\uff08IBM Telco\u548cMaven Telecom\uff09\u4e0a\u5bf9\u516d\u79cd\u5206\u7c7b\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793ae-Profits\u91cd\u5851\u4e86\u6a21\u578b\u6392\u540d\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6307\u6807\uff08AUC\u6216F1-score\uff09\u66fe\u5ffd\u89c6\u7684\u6a21\u578b\u7684\u8d22\u52a1\u4f18\u52bf\u3002\u8be5\u6307\u6807\u8fd8\u63d0\u4f9b\u4e86\u5206\u6bb5\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u8bc6\u522b\u54ea\u4e9b\u6a21\u578b\u80fd\u6700\u5927\u5316\u9ad8\u4ef7\u503c\u5ba2\u6237\u7684\u6295\u8d44\u56de\u62a5\u3002", "conclusion": "e-Profits\u662f\u4e00\u4e2a\u53ef\u7406\u89e3\u7684\u3001\u4e8b\u540e\u5206\u6790\u5de5\u5177\uff0c\u65e8\u5728\u652f\u6301\u5546\u4e1a\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f18\u5148\u8003\u8651\u5229\u6da6\u9a71\u52a8\u51b3\u7b56\u7684\u8425\u9500\u548c\u5206\u6790\u56e2\u961f\u3002"}}
{"id": "2507.09105", "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "categories": ["cs.CV"], "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u624b\u8bed\u751f\u6210\uff08SLP\uff09\u6a21\u578b\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u9ad8\u6548\u7684\u5b9e\u65f6\u624b\u8bed\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u8bed\u751f\u6210\uff08SLP\uff09\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff1a\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u4f1a\u7d2f\u79ef\u8bef\u5dee\uff0c\u800c\u6269\u6563\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u8f93\u51fa\u4f46\u56e0\u5176\u8fed\u4ee3\u6027\u8d28\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u9996\u6b21\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u5e94\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff0c\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u4e0d\u540c\u53d1\u97f3\u5668\u4e2d\u63d0\u53d6\u5e76\u878d\u5408\u7cbe\u7ec6\u7684\u8eab\u4f53\u8fd0\u52a8\u7279\u5f81\u30022. \u5f15\u5165\u4e86\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u611f\u77e5\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u5173\u8282\u7ea7\u7f6e\u4fe1\u5ea6\u5206\u6570\u52a8\u6001\u6307\u5bfc\u59ff\u6001\u751f\u6210\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6d41\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6df7\u5408\u81ea\u56de\u5f52-\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5176\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u624b\u8bed\u751f\u6210\u6a21\u578b\u5728\u9ad8\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u5b9e\u65f6\u624b\u8bed\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09301", "pdf": "https://arxiv.org/pdf/2507.09301", "abs": "https://arxiv.org/abs/2507.09301", "authors": ["Julio Gento Suela", "Javier Blanco-Romero", "Florina Almenares Mendoza", "Daniel D\u00edaz-S\u00e1nchez"], "title": "Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The emergence of quantum computers poses a significant threat to current\nsecure service, application and/or protocol implementations that rely on RSA\nand ECDSA algorithms, for instance DNSSEC, because public-key cryptography\nbased on number factorization or discrete logarithm is vulnerable to quantum\nattacks. This paper presents the integration of post-quantum cryptographic\n(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.\nWe have developed a plugin that extends CoreDNS with support for five PQC\nsignature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our\nimplementation maintains compatibility with existing DNS resolution flows while\nproviding on-the-fly signing using quantum-resistant signatures. A benchmark\nhas been performed and performance evaluation results reveal significant\ntrade-offs between security and efficiency. The results indicate that while PQC\nalgorithms introduce operational overhead, several candidates offer viable\ncompromises for transitioning DNSSEC to quantum-resistant cryptography.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u91cf\u5b50\u8ba1\u7b97\u673a\u5bf9DNSSEC\u7684\u5a01\u80c1\uff0c\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u5c06\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u96c6\u6210\u5230CoreDNS\u4e2d\uff0c\u4f7fDNSSEC\u5177\u5907\u91cf\u5b50\u6297\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u6743\u8861\uff0c\u53d1\u73b0\u90e8\u5206\u7b97\u6cd5\u662f\u53ef\u884c\u7684\u8fc7\u6e21\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u4f9d\u8d56RSA\u548cECDSA\u7b97\u6cd5\u7684DNSSEC\u7b49\u5b89\u5168\u670d\u52a1\u6613\u53d7\u91cf\u5b50\u8ba1\u7b97\u673a\u653b\u51fb\uff0c\u5b58\u5728\u91cd\u5927\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aCoreDNS\u63d2\u4ef6\uff0c\u652f\u6301ML-DSA\u3001FALCON\u3001SPHINCS+\u3001MAYO\u548cSNOVA\u4e94\u79cd\u540e\u91cf\u5b50\u5bc6\u7801\u7b7e\u540d\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u91cf\u5b50\u6297\u6027DNSSEC\u529f\u80fd\uff0c\u5e76\u786e\u4fdd\u4e0e\u73b0\u6709DNS\u89e3\u6790\u6d41\u7a0b\u517c\u5bb9\uff0c\u63d0\u4f9b\u5373\u65f6\u7b7e\u540d\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff0c\u5b89\u5168\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\u3002\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u4f1a\u5f15\u5165\u64cd\u4f5c\u5f00\u9500\uff0c\u4f46\u6709\u591a\u79cd\u5019\u9009\u7b97\u6cd5\u4e3aDNSSEC\u5411\u91cf\u5b50\u6297\u6027\u5bc6\u7801\u8fc7\u6e21\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6298\u8877\u65b9\u6848\u3002", "conclusion": "\u5c3d\u7ba1\u5f15\u5165\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u4f1a\u589e\u52a0\u6027\u80fd\u5f00\u9500\uff0c\u4f46\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\uff0c\u5c06DNSSEC\u8fc7\u6e21\u5230\u91cf\u5b50\u6297\u6027\u52a0\u5bc6\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.09259", "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "categories": ["cs.CL"], "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u5e7d\u9ed8\u7406\u8bba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7d\u9ed8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLMs\u5728\u5e7d\u9ed8\u7ffb\u8bd1\u4e2d\u9047\u5230\u7684\u8bed\u8a00\u5e72\u6270\u548c\u5e7d\u9ed8\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u5e7d\u9ed8\u7ffb\u8bd1\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5e38\u5bfc\u81f4\u8bed\u8a00\u5e72\u6270\u548c\u8bd1\u6587\u7f3a\u4e4f\u5e7d\u9ed8\u611f\uff0c\u8fd9\u5f71\u54cd\u4e86\u8de8\u6587\u5316\u7406\u89e3\u548c\u4ea4\u6d41\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53d7\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u8be5\u673a\u5236\u5229\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\u4ee5\u4f18\u5316\u8bd1\u6587\u53ef\u8bfb\u6027\u3002\u6b64\u5916\uff0cHDM\u8fd8\u6574\u5408\u4e86\u5e7d\u9ed8\u7406\u8bba\u4ee5\u589e\u5f3a\u8bd1\u6587\u7684\u5e7d\u9ed8\u5143\u7d20\u3002", "result": "\u5728\u5f00\u6e90\u5e7d\u9ed8\u6570\u636e\u96c6\u4e0a\u7684\u81ea\u52a8\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5e7d\u9ed8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u8bd1\u6587\u7684\u5e7d\u9ed8\u6027\u5e73\u5747\u63d0\u53477.75%\uff0c\u6d41\u7545\u6027\u63d0\u53472.81%\uff0c\u8fde\u8d2f\u6027\u63d0\u53476.13%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HDM\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u548c\u6574\u5408\u5e7d\u9ed8\u7406\u8bba\uff0c\u6709\u6548\u6539\u5584\u4e86LLMs\u5728\u5e7d\u9ed8\u7ffb\u8bd1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u6587\u672c\u7684\u5e7d\u9ed8\u611f\u3001\u6d41\u7545\u6027\u548c\u8fde\u8d2f\u6027\u3002"}}
{"id": "2507.09617", "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31/\u672c\u4f53\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u4eba\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u53ef\u4e92\u64cd\u4f5c\u3001\u5e73\u53f0\u65e0\u5173\u7684\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\u56e0\u786c\u7f16\u7801\u548c\u7279\u5b9a\u786c\u4ef6\u9650\u5236\u800c\u7f3a\u4e4f\u4e92\u64cd\u4f5c\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002\u867d\u7136\u672c\u4f53/\u77e5\u8bc6\u56fe\u8c31\u53ef\u63d0\u4f9b\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u539f\u59cb\u611f\u5b98\u8f93\u5165\uff1b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5584\u4e8e\u89e3\u91ca\u611f\u5b98\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u77e5\u8bc6\u57fa\u7840\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u878d\u5408\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u77e5\u8bc6\u56fe\u8c31/\u672c\u4f53\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u65e8\u5728\u652f\u6301\u673a\u5668\u4eba\u5e94\u7528\u7684\u4e92\u64cd\u4f5c\u6027\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u7b26\u5408\u672c\u4f53\u8bba\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u5b9e\u73b0\u5e73\u53f0\u65e0\u5173\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002\u901a\u8fc7\u6574\u5408\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u3001\u672c\u4f53\u548c\u4e94\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff083\u4e2aLLaMA\uff0c2\u4e2aGPT\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u795e\u7ecf\u7b26\u53f7\u4ea4\u4e92\u6a21\u5f0f\u4e0b\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u7684\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGPT-o1\u548cLLaMA 4 Maverick\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u540c\u65f6\uff0c\u7814\u7a76\u6307\u51fa\u65b0\u6a21\u578b\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u4f18\u7ed3\u679c\uff0c\u96c6\u6210\u7b56\u7565\u5728\u751f\u6210\u7b26\u5408\u672c\u4f53\u8bba\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31/\u672c\u4f53\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5e73\u53f0\u65e0\u5173\u6027\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u96c6\u6210\u7b56\u7565\u5bf9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u77e5\u8bc6\u56fe\u8c31\u7684\u91cd\u8981\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u6a21\u578b\u7684\u65b0\u9896\u6027\u3002"}}
{"id": "2507.08861", "pdf": "https://arxiv.org/pdf/2507.08861", "abs": "https://arxiv.org/abs/2507.08861", "authors": ["Lucas Tesan", "Mikel M. Iparraguirre", "David Gonzalez", "Pedro Martins", "Elias Cueto"], "title": "On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper proposes sharp lower bounds for the number of message passing\niterations required in graph neural networks (GNNs) when solving partial\ndifferential equations (PDE). This significantly reduces the need for\nexhaustive hyperparameter tuning. Bounds are derived for the three fundamental\nclasses of PDEs (hyperbolic, parabolic and elliptic) by relating the physical\ncharacteristics of the problem in question to the message-passing requirement\nof GNNs. In particular, we investigate the relationship between the physical\nconstants of the equations governing the problem, the spatial and temporal\ndiscretisation and the message passing mechanisms in GNNs.\n  When the number of message passing iterations is below these proposed limits,\ninformation does not propagate efficiently through the network, resulting in\npoor solutions, even for deep GNN architectures. In contrast, when the\nsuggested lower bound is satisfied, the GNN parameterisation allows the model\nto accurately capture the underlying phenomenology, resulting in solvers of\nadequate accuracy.\n  Examples are provided for four different examples of equations that show the\nsharpness of the proposed lower bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u65f6\u6240\u9700\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u7684\u7cbe\u786e\u4e0b\u9650\uff0c\u65e8\u5728\u51cf\u5c11\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u663e\u8457\u51cf\u5c11GNNs\u6c42\u89e3PDEs\u65f6\u5bf9\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u8fdb\u884c\u7a77\u4e3e\u5f0f\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u5e76\u786e\u4fdd\u4fe1\u606f\u5728\u7f51\u7edc\u4e2d\u9ad8\u6548\u4f20\u64ad\u4ee5\u83b7\u5f97\u51c6\u786e\u7684\u89e3\u3002", "method": "\u901a\u8fc7\u5c06PDEs\uff08\u5305\u62ec\u53cc\u66f2\u3001\u629b\u7269\u548c\u692d\u5706\u578b\uff09\u7684\u7269\u7406\u7279\u6027\u4e0eGNNs\u7684\u6d88\u606f\u4f20\u9012\u9700\u6c42\u76f8\u5173\u8054\uff0c\u63a8\u5bfc\u51fa\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u7684\u4e0b\u9650\u3002\u5177\u4f53\u5730\uff0c\u7814\u7a76\u4e86\u65b9\u7a0b\u7684\u7269\u7406\u5e38\u6570\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u79bb\u6563\u5316\u4ee5\u53caGNNs\u6d88\u606f\u4f20\u9012\u673a\u5236\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u4e86\u5728GNNs\u6c42\u89e3PDEs\u65f6\u6240\u9700\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u7684\u7cbe\u786e\u4e0b\u9650\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u8fed\u4ee3\u6b21\u6570\u4f4e\u4e8e\u8fd9\u4e9b\u9650\u5236\u65f6\uff0c\u4fe1\u606f\u4f20\u64ad\u6548\u7387\u4f4e\u4e0b\uff0c\u5373\u4f7f\u662f\u6df1\u5c42GNNs\u4e5f\u4f1a\u4ea7\u751f\u7cdf\u7cd5\u7684\u89e3\uff1b\u800c\u5f53\u6ee1\u8db3\u8fd9\u4e9b\u4e0b\u9650\u65f6\uff0cGNN\u6a21\u578b\u80fd\u51c6\u786e\u6355\u6349\u6f5c\u5728\u73b0\u8c61\uff0c\u8fbe\u5230\u8db3\u591f\u7684\u7cbe\u5ea6\u3002\u901a\u8fc7\u56db\u4e2a\u4e0d\u540c\u65b9\u7a0b\u7684\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u4e0b\u9650\u7684\u7cbe\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9bGNNs\u6c42\u89e3PDEs\u65f6\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u7684\u7cbe\u786e\u4e0b\u9650\uff0c\u672c\u7814\u7a76\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u786e\u4fdd\u4e86\u4fe1\u606f\u6709\u6548\u4f20\u64ad\u548c\u6c42\u89e3\u7cbe\u5ea6\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2507.09111", "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u9c81\u68d2\u6027\u57fa\u51c6RoHOI\uff0c\u5e76\u5f15\u5165\u4e86\u8bed\u4e49\u611f\u77e5\u63a9\u853d\u6e10\u8fdb\u5b66\u4e60\uff08SAMPL\uff09\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u5bf9\u673a\u5668\u4eba-\u4eba\u7c7b\u534f\u52a9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u8150\u8680\uff08\u5982\u73af\u5883\u53d8\u5316\u3001\u906e\u6321\u3001\u566a\u58f0\uff09\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30HOI\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "1. \u5f15\u5165\u9996\u4e2aHOI\u68c0\u6d4b\u9c81\u68d2\u6027\u57fa\u51c6RoHOI\uff0c\u8be5\u57fa\u51c6\u57fa\u4e8eHICO-DET\u548cV-COCO\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e8620\u79cd\u8150\u8680\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u63a9\u853d\u6e10\u8fdb\u5b66\u4e60\uff08SAMPL\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u6574\u4f53\u548c\u5c40\u90e8\u7ebf\u7d22\u5f15\u5bfc\u6a21\u578b\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u8fc7\u7a0b\u4ee5\u589e\u5f3a\u9c81\u68d2\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8150\u8680\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u7684SAMPL\u65b9\u6cd5\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e3a\u9c81\u68d2HOI\u68c0\u6d4b\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2aHOI\u68c0\u6d4b\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765HOI\u68c0\u6d4b\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09627", "pdf": "https://arxiv.org/pdf/2507.09627", "abs": "https://arxiv.org/abs/2507.09627", "authors": ["Muhammad Kamran Saeed", "Ashfaq Khokhar", "Shakil Ahmed"], "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices", "categories": ["cs.IT", "cs.CV", "cs.LG", "cs.NI", "math.IT"], "comment": null, "summary": "Next-generation wireless technologies such as 6G aim to meet demanding\nrequirements such as ultra-high data rates, low latency, and enhanced\nconnectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable\nIntelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and\nenergy efficiency through numerous antennas, and RIS offering dynamic control\nover the wireless environment via passive reflective elements. However,\nrealizing their full potential depends on accurate Channel State Information\n(CSI). Recent advances in deep learning have facilitated efficient cascaded\nchannel estimation. However, the scalability and practical deployment of\nexisting estimation models in XL-MIMO systems remain limited. The growing\nnumber of antennas and RIS elements introduces a significant barrier to\nreal-time and efficient channel estimation, drastically increasing data volume,\nescalating computational complexity, requiring advanced hardware, and resulting\nin substantial energy consumption. To address these challenges, we propose a\nlightweight deep learning framework for efficient cascaded channel estimation\nin XL-MIMO systems, designed to minimize computational complexity and make it\nsuitable for deployment on resource-constrained edge devices. Using spatial\ncorrelations in the channel, we introduce a patch-based training mechanism that\nreduces the dimensionality of input to patch-level representations while\npreserving essential information, allowing scalable training for large-scale\nsystems. Simulation results under diverse conditions demonstrate that our\nframework significantly improves estimation accuracy and reduces computational\ncomplexity, regardless of the increasing number of antennas and RIS elements in\nXL-MIMO systems.", "AI": {"tldr": "\u4e3a\u89e3\u51b3XL-MIMO\u7cfb\u7edf\u4e2d\u4fe1\u9053\u4f30\u8ba1\u7684\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u5206\u5757\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "6G\u7b49\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u6280\u672f\uff08\u5982XL-MIMO\u548cRIS\uff09\u9700\u8981\u7cbe\u786e\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u4fe1\u9053\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728XL-MIMO\u7cfb\u7edf\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u9650\u5236\uff0c\u968f\u7740\u5929\u7ebf\u548cRIS\u5355\u5143\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5bfc\u81f4\u6570\u636e\u91cf\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u80fd\u8017\u6025\u5267\u4e0a\u5347\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u7684\u4fe1\u9053\u4f30\u8ba1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eXL-MIMO\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\u3002\u8be5\u6846\u67b6\u65e8\u5728\u6700\u5c0f\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002\u901a\u8fc7\u5229\u7528\u4fe1\u9053\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\uff08patch-based\uff09\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u5c06\u8f93\u5165\u7ef4\u5ea6\u964d\u4f4e\u5230\u5206\u5757\u7ea7\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u53ef\u4f38\u7f29\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8fdb\u884c\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14\u5176\u6027\u80fd\u4e0d\u53d7XL-MIMO\u7cfb\u7edf\u4e2d\u5929\u7ebf\u548cRIS\u5355\u5143\u6570\u91cf\u589e\u52a0\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86XL-MIMO\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u4fe1\u9053\u4f30\u8ba1\u6240\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u4fe1\u9053\u4f30\u8ba1\uff0c\u4f7f\u5176\u6210\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u6280\u672f\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.09282", "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faClaritySpeech\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408ASR\u3001\u6587\u672c\u6a21\u7cca\u548c\u96f6\u6837\u672cTTS\uff0c\u7ea0\u6b63\u75f4\u5446\u75c7\u60a3\u8005\u7684\u8a00\u8bed\u5e76\u6a21\u7cca\u75f4\u5446\u7279\u5f81\uff0c\u540c\u65f6\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u4fdd\u7559\u8bf4\u8bdd\u8005\u8eab\u4efd\uff0c\u4ece\u800c\u63d0\u5347\u9690\u79c1\u6027\u548c\u53ef\u53ca\u6027\u3002", "motivation": "\u75f4\u5446\u75c7\u6539\u53d8\u8a00\u8bed\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u5e76\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\u3002\u73b0\u6709\u8bed\u97f3\u6280\u672f\uff08\u5982ASR\uff09\u96be\u4ee5\u6709\u6548\u5904\u7406\u75f4\u5446\u75c7\u60a3\u8005\u7684\u975e\u5178\u578b\u8a00\u8bed\uff0c\u8fdb\u4e00\u6b65\u5f71\u54cd\u5176\u53ef\u53ca\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aClaritySpeech\u7684\u521b\u65b0\u578b\u8a00\u8bed\u6a21\u7cca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u81ea\u52a8\u8bed\u97f3\u8f6c\u5f55\uff08ASR\uff09\u3001\u6587\u672c\u6a21\u7cca\u5904\u7406\u4ee5\u53ca\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u6280\u672f\u3002\u5b83\u65e8\u5728\u7ea0\u6b63\u53d7\u75f4\u5446\u75c7\u5f71\u54cd\u7684\u8a00\u8bed\uff0c\u540c\u65f6\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4fdd\u7559\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728ADReSS\u548cADReSSo\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u5bf9\u6297\u8bbe\u7f6e\u548c\u6a21\u6001\u4e0b\uff0c\u5e73\u5747F1\u5206\u6570\u5206\u522b\u4e0b\u964d\u4e8616%\u548c10%\uff0c\u540c\u65f6\u4fdd\u6301\u4e8650%\u7684\u8bf4\u8bdd\u8005\u76f8\u4f3c\u5ea6\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u663e\u8457\u6539\u5584\u4e86\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0cADReSS\u4ece0.73\u964d\u81f30.08\uff0cADReSSo\u964d\u81f30.15\uff1b\u8bed\u97f3\u8d28\u91cf\u4e5f\u4ece1.65\u63d0\u9ad8\u5230\u7ea62.15\u3002", "conclusion": "ClaritySpeech\u6846\u67b6\u80fd\u591f\u6709\u6548\u7ea0\u6b63\u75f4\u5446\u75c7\u60a3\u8005\u7684\u8a00\u8bed\uff0c\u6a21\u7cca\u75be\u75c5\u76f8\u5173\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u8bf4\u8bdd\u8005\u8eab\u4efd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8a00\u8bed\u5904\u7406\u7684\u9690\u79c1\u6027\u548c\u53ef\u53ca\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2507.09626", "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684PyTorch\u5de5\u5177\u5305\uff0c\u5229\u7528\u968f\u673a\u63a7\u5236\u6280\u672f\u4e3a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u63d0\u4f9b\u9884\u5148\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u76f8\u5173\u590d\u6742\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u76d1\u7ba1\u8981\u6c42\u9884\u5148\uff08a priori\uff09\u6ee1\u8db3\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u667a\u80fd\u4f53\u54cd\u5e94\u7684\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u8fd9\u4e9b\u9884\u5148\u4fdd\u8bc1\u9700\u8981\u5bf9\u968f\u673a\u7cfb\u7edf\u8fdb\u884c\u590d\u6742\u7684\u63a8\u7406\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u8be5\u5de5\u5177\u5305\u5229\u7528\u968f\u673a\u63a7\u5236\u6280\u672f\u6765\u5efa\u6a21AI\u7cfb\u7edf\u4e4b\u95f4\u7684\u4e92\u8fde\u53ca\u5176\u91cd\u590d\u4f7f\u7528\u7684\u7279\u6027\u3002", "result": "\u8be5\u5de5\u5177\u5305\u4ee5\u95ed\u73af\u65b9\u5f0f\u5efa\u6a21\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u9700\u6c42\uff0c\u5e76\u4e3aAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u4e92\u8fde\u63d0\u4f9b\u4e86\u9884\u5148\u4fdd\u8bc1\u3002\u5b83\u663e\u8457\u964d\u4f4e\u4e86\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u95ed\u73af\u6a21\u578b\u63d0\u4f9b\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5PyTorch\u5de5\u5177\u5305\u6210\u529f\u5730\u5c06\u968f\u673a\u63a7\u5236\u6280\u672f\u5e94\u7528\u4e8eAI\u7cfb\u7edf\uff0c\u6709\u6548\u5730\u7b80\u5316\u4e86\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u9884\u5148\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\u7684\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u590d\u6742\u6027\u6311\u6218\u3002"}}
{"id": "2507.08866", "pdf": "https://arxiv.org/pdf/2507.08866", "abs": "https://arxiv.org/abs/2507.08866", "authors": ["Marina Ceccon", "Giandomenico Cornacchia", "Davide Dalle Pezze", "Alessandro Fabris", "Gian Antonio Susto"], "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond", "categories": ["cs.LG", "cs.CY", "stat.ML"], "comment": "Accepted in Expert Systems with Applications", "summary": "Undesirable biases encoded in the data are key drivers of algorithmic\ndiscrimination. Their importance is widely recognized in the algorithmic\nfairness literature, as well as legislation and standards on\nanti-discrimination in AI. Despite this recognition, data biases remain\nunderstudied, hindering the development of computational best practices for\ntheir detection and mitigation. In this work, we present three common data\nbiases and study their individual and joint effect on algorithmic\ndiscrimination across a variety of datasets, models, and fairness measures. We\nfind that underrepresentation of vulnerable populations in training sets is\nless conducive to discrimination than conventionally affirmed, while\ncombinations of proxies and label bias can be far more critical. Consequently,\nwe develop dedicated mechanisms to detect specific types of bias, and combine\nthem into a preliminary construct we refer to as the Data Bias Profile (DBP).\nThis initial formulation serves as a proof of concept for how different bias\nsignals can be systematically documented. Through a case study with popular\nfairness datasets, we demonstrate the effectiveness of the DBP in predicting\nthe risk of discriminatory outcomes and the utility of fairness-enhancing\ninterventions. Overall, this article bridges algorithmic fairness research and\nanti-discrimination policy through a data-centric lens.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6570\u636e\u504f\u89c1\u5bf9\u7b97\u6cd5\u6b67\u89c6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ee3\u7406\u53d8\u91cf\u548c\u6807\u7b7e\u504f\u89c1\u7684\u7ec4\u5408\u6bd4\u5f31\u52bf\u7fa4\u4f53\u4ee3\u8868\u4e0d\u8db3\u66f4\u5177\u6b67\u89c6\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u5e76\u521d\u6b65\u6784\u5efa\u4e86\u201c\u6570\u636e\u504f\u89c1\u6863\u6848\uff08DBP\uff09\u201d\u4ee5\u7cfb\u7edf\u68c0\u6d4b\u548c\u8bb0\u5f55\u504f\u89c1\u4fe1\u53f7\uff0c\u4ece\u800c\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u5e76\u6307\u5bfc\u5e72\u9884\u3002", "motivation": "\u6570\u636e\u4e2d\u7f16\u7801\u7684\u4e0d\u826f\u504f\u89c1\u662f\u7b97\u6cd5\u6b67\u89c6\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\u5728\u7b97\u6cd5\u516c\u5e73\u6027\u6587\u732e\u3001\u53cd\u6b67\u89c6\u7acb\u6cd5\u548c\u6807\u51c6\u4e2d\u5f97\u5230\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u4f46\u6570\u636e\u504f\u89c1\u672c\u8eab\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u963b\u788d\u4e86\u5176\u68c0\u6d4b\u548c\u7f13\u89e3\u8ba1\u7b97\u6700\u4f73\u5b9e\u8df5\u7684\u53d1\u5c55\u3002", "method": "1. \u0e28\u0e36\u0e01\u0e29\u0e32\u5e76\u5206\u6790\u4e86\u4e09\u79cd\u5e38\u89c1\u6570\u636e\u504f\u89c1\u53ca\u5176\u5bf9\u7b97\u6cd5\u6b67\u89c6\u7684\u5355\u72ec\u548c\u8054\u5408\u5f71\u54cd\uff0c\u6db5\u76d6\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u516c\u5e73\u6027\u5ea6\u91cf\u30022. \u5f00\u53d1\u4e86\u4e13\u7528\u673a\u5236\u6765\u68c0\u6d4b\u7279\u5b9a\u7c7b\u578b\u7684\u504f\u89c1\u30023. \u5c06\u8fd9\u4e9b\u673a\u5236\u6574\u5408\u4e3a\u4e00\u4e2a\u521d\u6b65\u7684\u201c\u6570\u636e\u504f\u89c1\u6863\u6848\uff08DBP\uff09\u201d\u7ed3\u6784\u30024. \u901a\u8fc7\u6d41\u884c\u516c\u5e73\u6027\u6570\u636e\u96c6\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86DBP\u5728\u9884\u6d4b\u6b67\u89c6\u7ed3\u679c\u98ce\u9669\u548c\u8bc4\u4f30\u516c\u5e73\u6027\u589e\u5f3a\u5e72\u9884\u63aa\u65bd\u6548\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "1. \u8bad\u7ec3\u96c6\u4e2d\u5f31\u52bf\u7fa4\u4f53\u7684\u4ee3\u8868\u4e0d\u8db3\u5bf9\u6b67\u89c6\u7684\u5f71\u54cd\u4f4e\u4e8e\u4f20\u7edf\u8ba4\u77e5\u30022. \u4ee3\u7406\u53d8\u91cf\uff08proxies\uff09\u548c\u6807\u7b7e\u504f\u89c1\uff08label bias\uff09\u7684\u7ec4\u5408\u53ef\u80fd\u66f4\u4e3a\u5173\u952e\uff0c\u66f4\u80fd\u5bfc\u81f4\u6b67\u89c6\u30023. \u521d\u6b65\u6784\u5efa\u7684DBP\u80fd\u6709\u6548\u9884\u6d4b\u6b67\u89c6\u6027\u7ed3\u679c\u7684\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u4e86\u516c\u5e73\u6027\u589e\u5f3a\u5e72\u9884\u63aa\u65bd\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u89c6\u89d2\uff0c\u5f25\u5408\u4e86\u7b97\u6cd5\u516c\u5e73\u6027\u7814\u7a76\u4e0e\u53cd\u6b67\u89c6\u653f\u7b56\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\u6570\u636e\u504f\u89c1\u6863\u6848\uff08DBP\uff09\u7684\u521d\u6b65\u6784\u5efa\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u7cfb\u7edf\u5730\u8bb0\u5f55\u4e0d\u540c\u7684\u504f\u89c1\u4fe1\u53f7\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u548c\u6307\u5bfc\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2507.09118", "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "CLIP\u6301\u7eed\u5b66\u4e60\u4e2d\u5e38\u5ffd\u89c6\u6a21\u6001\u95f4\u9699\u95ee\u9898\u3002\u672c\u6587\u5206\u6790\u6a21\u6001\u95f4\u9699\u53d8\u5316\u53d1\u73b0\u5176\u53cd\u6620\u9884\u8bad\u7ec3\u77e5\u8bc6\u4fdd\u7559\u5ea6\uff0c\u5e76\u63d0\u51faMG-CLIP\uff0c\u901a\u8fc7\u6a21\u6001\u95f4\u9699\u4fdd\u6301\u548c\u8865\u507f\u7b56\u7565\u63d0\u5347CLIP\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u56de\u653e\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u5c06\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\uff08CLIP\uff09\u5e94\u7528\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u7814\u7a76\uff0c\u5927\u591a\u5ffd\u89c6\u4e86\u5176\u56fa\u6709\u7684\u6a21\u6001\u95f4\u9699\uff0c\u800c\u8be5\u95f4\u9699\u662fCLIP\u6cdb\u5316\u548c\u9002\u5e94\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u6a21\u6001\u95f4\u9699\u7684\u52a8\u6001\u53d8\u5316\u5e76\u5229\u7528\u5176\u6765\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u662f\u672c\u7814\u7a76\u7684\u52a8\u673a\u3002", "method": "\u672c\u6587\u9996\u5148\u5206\u6790\u4e86\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6a21\u6001\u95f4\u9699\u7684\u53d8\u5316\uff0c\u5e76\u89c2\u5bdf\u5230\u6a21\u6001\u95f4\u9699\u80fd\u6709\u6548\u53cd\u6620\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4fdd\u7559\u7a0b\u5ea6\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMG-CLIP\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u6a21\u6001\u95f4\u9699\u4fdd\u6301\u201d\u7b56\u7565\u6765\u7f13\u89e3\u9057\u5fd8\uff0c\u5e76\u901a\u8fc7\u201c\u6a21\u6001\u95f4\u9699\u8865\u507f\u201d\u7b56\u7565\u6765\u589e\u5f3a\u5bf9\u65b0\u6570\u636e\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MG-CLIP\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u56de\u653e\u6570\u636e\u3002", "conclusion": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6a21\u6001\u95f4\u9699\u7684\u6301\u7eed\u5b66\u4e60\u89c6\u89d2\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u548c\u7ba1\u7406CLIP\u7684\u6a21\u6001\u95f4\u9699\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2507.09719", "pdf": "https://arxiv.org/pdf/2507.09719", "abs": "https://arxiv.org/abs/2507.09719", "authors": ["Jiaheng Xiong", "Qiaolun Zhang", "Yoann Pi\u00e9tri", "Raja Yehia", "Raouf Boutaba", "Francesco Musumeci", "Massimo Tornatore"], "title": "Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "We analyze the power consumption of quantum key distribution (QKD) networks\nunder various protocol and detector configurations. Using realistic network\ntopologies, we evaluate discrete-variable vs continuous-variable QKD and\noptimize device placement, quantifying power trade-offs of SNSPD vs APD\ndetectors and the benefits of optical bypass.", "AI": {"tldr": "\u5206\u6790\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u7f51\u7edc\u7684\u529f\u8017\u3002", "motivation": "\u8bc4\u4f30\u548c\u4f18\u5316\u4e0d\u540c\u534f\u8bae\u3001\u63a2\u6d4b\u5668\u914d\u7f6e\u53ca\u7f51\u7edc\u62d3\u6251\u4e0bQKD\u7f51\u7edc\u7684\u80fd\u8017\u8868\u73b0\u3002", "method": "\u5728\u771f\u5b9e\u7684\u7f51\u7edc\u62d3\u6251\u4e0b\uff0c\u6bd4\u8f83\u79bb\u6563\u53d8\u91cf\uff08DV\uff09\u4e0e\u8fde\u7eed\u53d8\u91cf\uff08CV\uff09QKD\uff0c\u4f18\u5316\u8bbe\u5907\u653e\u7f6e\uff0c\u5e76\u91cf\u5316SNSPD\u4e0eAPD\u63a2\u6d4b\u5668\u7684\u529f\u8017\u6743\u8861\u4ee5\u53ca\u5149\u65c1\u8def\u7684\u597d\u5904\u3002", "result": "\u91cf\u5316\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540cQKD\u534f\u8bae\u3001\u63a2\u6d4b\u5668\u7c7b\u578b\u548c\u7f51\u7edc\u4f18\u5316\uff08\u5982\u8bbe\u5907\u653e\u7f6e\u548c\u5149\u65c1\u8def\uff09\u5bf9QKD\u7f51\u7edc\u529f\u8017\u7684\u5f71\u54cd\u548c\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u4e0d\u540c\u914d\u7f6e\u4e0bQKD\u7f51\u7edc\u7684\u80fd\u8017\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2507.09424", "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DATE-LM\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u4e14\u6027\u80fd\u53d7\u8bc4\u4f30\u8bbe\u8ba1\u5f71\u54cd\u3002", "motivation": "\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5bf9\u4e8eLLM\u7684\u591a\u79cd\u7814\u7a76\u548c\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9LLM\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u3001\u4ee5LLM\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86DATE-LM\uff08LLM\u6570\u636e\u5f52\u56e0\u8bc4\u4f30\uff09\u57fa\u51c6\uff0c\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u3001\u6bd2\u6027/\u504f\u89c1\u8fc7\u6ee4\u548c\u4e8b\u5b9e\u5f52\u56e0\u8fd9\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u6765\u8861\u91cf\u5f52\u56e0\u8d28\u91cf\u3002\u8be5\u57fa\u51c6\u8bbe\u8ba1\u6613\u4e8e\u4f7f\u7528\uff0c\u652f\u6301\u5728\u4e0d\u540c\u4efb\u52a1\u548cLLM\u67b6\u6784\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5e76\u88ab\u7528\u4e8e\u5bf9\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff1b\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u4e0e\u7b80\u5355\u57fa\u7ebf\u5b58\u5728\u6743\u8861\uff1b\u65b9\u6cd5\u6027\u80fd\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8bc4\u4f30\u8bbe\u8ba1\u654f\u611f\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u5e03\u4e86\u4e00\u4e2a\u516c\u5171\u6392\u884c\u699c\uff0c\u4ee5\u4fc3\u8fdb\u65b9\u6cd5\u6bd4\u8f83\u548c\u793e\u533a\u53c2\u4e0e\u3002\u5e0c\u671bDATE-LM\u80fd\u4e3a\u672a\u6765LLM\u6570\u636e\u5f52\u56e0\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.09662", "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u5197\u957f\u4f4e\u6548\u95ee\u9898\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u63a2\u8ba8LRMs\u9ad8\u6548\u63a8\u7406\u7684\u7cbe\u7b80\u4e0e\u81ea\u9002\u5e94\u601d\u7ef4\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5bf9\u7b80\u5355\u95ee\u9898\u4f1a\u751f\u6210\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u94fe\uff0c\u5bfc\u81f4\u63a8\u7406\u8d44\u6e90\u6d6a\u8d39\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\uff0c\u5e76\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u6025\u9700\u7f29\u77ed\u63a8\u7406\u94fe\u5e76\u5b9e\u73b0\u57fa\u4e8e\u8f93\u5165\u96be\u5ea6\u7684\u5feb\u6162\u601d\u7ef4\u81ea\u9002\u5e94\u63a8\u7406\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7efc\u8ff0\u5f62\u5f0f\uff0c\u5168\u9762\u6982\u8ff0\u4e86LRMs\u9ad8\u6548\u63a8\u7406\u4e2d\u7cbe\u7b80\u4e0e\u81ea\u9002\u5e94\u601d\u7ef4\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5185\u5bb9\u6db5\u76d6\u76f8\u5173\u65b9\u6cd5\u8bba\u3001\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u672a\u6765\u63a2\u7d22\u7684\u6311\u6218\u3002", "result": "\u672c\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\u7cbe\u7b80\u4e0e\u81ea\u9002\u5e94\u601d\u7ef4\u9886\u57df\u7684\u7efc\u5408\u6027\u6982\u89c8\uff0c\u660e\u786e\u4e86\u5f53\u524d\u7684\u65b9\u6cd5\u3001\u8bc4\u4f30\u6807\u51c6\u4ee5\u53ca\u6709\u5f85\u89e3\u51b3\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u638c\u63e1\u8be5\u9886\u57df\u5168\u8c8c\uff0c\u5e76\u542f\u53d1\u65b0\u7684\u81ea\u9002\u5e94\u601d\u7ef4\u7406\u5ff5\uff0c\u4ee5\u4fc3\u8fdb\u5927\u578b\u63a8\u7406\u6a21\u578b\u66f4\u9ad8\u6548\u5730\u5e94\u7528\u3002"}}
{"id": "2507.08870", "pdf": "https://arxiv.org/pdf/2507.08870", "abs": "https://arxiv.org/abs/2507.08870", "authors": ["Yaowenqi Liu", "BingXu Meng", "Rui Pan", "Jerry Huang", "Tong Zhang"], "title": "GUIDE: Towards Scalable Advising for Research Ideas", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "The field of AI research is advancing at an unprecedented pace, enabling\nautomated hypothesis generation and experimental design across diverse domains\nsuch as biology, mathematics, and artificial intelligence. Despite these\nadvancements, there remains a significant gap in the availability of scalable\nadvising systems capable of providing high-quality, well-reasoned feedback to\nrefine proposed hypotheses and experimental designs. To address this challenge,\nwe explore key factors that underlie the development of robust advising\nsystems, including model size, context length, confidence estimation, and\nstructured reasoning processes. Our findings reveal that a relatively small\nmodel, when equipped with a well-compressed literature database and a\nstructured reasoning framework, can outperform powerful general-purpose\nlanguage models such as Deepseek-R1 in terms of acceptance rates for\nself-ranked top-30% submissions to ICLR 2025. Moreover, when limited to\nhigh-confidence predictions, our system achieves an acceptance rate exceeding\n90% on the ICLR 2025 test set, underscoring its potential to significantly\nenhance the quality and efficiency of hypothesis generation and experimental\ndesign. The code is released at\nhttps://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cdAI\u7814\u7a76\u5efa\u8bae\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u5c0f\u578b\u6a21\u578b\u3001\u538b\u7f29\u6587\u732e\u6570\u636e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5728\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\u65b9\u9762\u8d85\u8d8a\u5927\u578b\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u4e0b\u80fd\u8fbe\u5230\u9ad8\u63a5\u53d7\u7387\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u5047\u8bbe\u751f\u6210\u4e0e\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u81ea\u52a8\u5316\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u6709\u5145\u5206\u7406\u7531\u7684\u53cd\u9988\u4ee5\u5b8c\u5584\u8fd9\u4e9b\u5047\u8bbe\u548c\u8bbe\u8ba1\u65b9\u9762\uff0c\u4ecd\u7136\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u5efa\u8bae\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63a2\u7d22\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u5f00\u53d1\u5065\u58ee\u7684\u5efa\u8bae\u7cfb\u7edf\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u5e76\u4e3a\u5176\u914d\u5907\u4e86\u826f\u597d\u538b\u7f29\u7684\u6587\u732e\u6570\u636e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e2a\u914d\u5907\u4e86\u826f\u597d\u538b\u7f29\u6587\u732e\u6570\u636e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u7684\u76f8\u5bf9\u5c0f\u578b\u6a21\u578b\uff0c\u5728ICLR 2025\u81ea\u8bc4\u524d30%\u63d0\u4ea4\u7684\u63a5\u53d7\u7387\u65b9\u9762\uff0c\u4f18\u4e8eDeepseek-R1\u7b49\u5f3a\u5927\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5f53\u4ec5\u9650\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u65f6\uff0c\u8be5\u7cfb\u7edf\u5728ICLR 2025\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86\u8d85\u8fc790%\u7684\u63a5\u53d7\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u671b\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\uff0c\u663e\u8457\u63d0\u9ad8\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.09122", "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "categories": ["cs.CV"], "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u8be6\u5c3d\u7684\u6587\u672c-\u52a8\u4f5c\u6570\u636e\u96c6SnapMoGen\uff0c\u5e76\u63d0\u51fa\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u63a9\u7801\u6a21\u578bMoMask++\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u7cbe\u7ec6\u63a7\u5236\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\uff0c\u53ea\u80fd\u5904\u7406\u77ed\u6216\u901a\u7528\u6587\u672c\u63d0\u793a\uff0c\u5bfc\u81f4\u7cbe\u7ec6\u63a7\u5236\u548c\u5bf9\u672a\u77e5\u63d0\u793a\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u5f15\u5165\u4e86SnapMoGen\u6570\u636e\u96c6\uff0c\u5305\u542b20K\u52a8\u4f5c\u7247\u6bb5\uff08\u603b\u8ba144\u5c0f\u65f6\uff09\u548c122K\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\uff08\u5e73\u574748\u8bcd\uff09\uff0c\u5e76\u4fdd\u7559\u539f\u59cb\u65f6\u95f4\u8fde\u7eed\u6027\u4ee5\u652f\u6301\u957f\u671f\u52a8\u4f5c\u751f\u6210\u30022. \u63d0\u51fa\u4e86MoMask++\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u52a8\u4f5c\u8f6c\u6362\u4e3a\u591a\u5c3a\u5ea6token\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u5355\u4e2a\u751f\u6210\u5f0f\u63a9\u7801Transformer\u5b66\u4e60\u751f\u6210\u6240\u6709token\u30023. \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u91cd\u65b0\u683c\u5f0f\u5316\u975e\u6b63\u5f0f\u7528\u6237\u63d0\u793a\uff0c\u4f7f\u5176\u4e0e\u6570\u636e\u96c6\u7684\u8868\u8fbe\u98ce\u683c\u5bf9\u9f50\u3002", "result": "MoMask++\u5728HumanML3D\u548cSnapMoGen\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u901a\u8fc7LLM\u5904\u7406\u968f\u610f\u7528\u6237\u63d0\u793a\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u66f4\u4e30\u5bcc\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u672c\u6587\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u7cbe\u7ec6\u63a7\u5236\u548c\u6cdb\u5316\u6027\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u957f\u671f\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002"}}
{"id": "2507.09859", "pdf": "https://arxiv.org/pdf/2507.09859", "abs": "https://arxiv.org/abs/2507.09859", "authors": ["Guntur Dharma Putra", "Bagus Rakadyanto Oktavianto Putra"], "title": "Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems", "categories": ["cs.CR", "cs.NI"], "comment": "5 pages, 4 figures. Accepted to IEEE ICBC 2025 as a short paper", "summary": "Self-Sovereign Identity (SSI) offers significant potential for managing\nidentities in the Internet of Things (IoT), enabling decentralized\nauthentication and credential management without reliance on centralized\nentities. However, existing SSI frameworks often limit credential issuance and\nrevocation to trusted entities, such as IoT manufacturers, which restricts\nflexibility in dynamic IoT ecosystems. In this paper, we propose a\nblockchain-based SSI framework that allows any individual with a verifiable\ntrust linkage to act as a credential issuer, ensuring decentralized and\nscalable identity management. Our framework incorporates a layered\narchitecture, where trust is dynamically established through endorsement-based\ncalculations and maintained via a hierarchical chain-of-trust mechanism.\nBlockchain serves as the Verifiable Data Registry, ensuring transparency and\nimmutability of identity operations, while smart contracts automate critical\nprocesses such as credential issuance, verification, and revocation. A\nproof-of-concept implementation demonstrates that the proposed framework is\nfeasible and incurs minimal overheads compared to the baseline, making it\nwell-suited for dynamic and resource-constrained IoT environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u81ea S-S I \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7269\u8054\u7f51\u4e2d\u51ed\u8bc1\u9881\u53d1\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5141\u8bb8\u4efb\u4f55\u5177\u6709\u53ef\u9a8c\u8bc1\u4fe1\u4efb\u8fde\u63a5\u7684\u4e2a\u4f53\u4f5c\u4e3a\u51ed\u8bc1\u9881\u53d1\u8005\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u4f4e\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7269\u8054\u7f51\u4e2d\u7684\u81ea S-S I \u6846\u67b6\u5c06\u51ed\u8bc1\u9881\u53d1\u548c\u64a4\u9500\u9650\u5236\u5728\u5c11\u6570\u53ef\u4fe1\u5b9e\u4f53\uff08\u5982\u7269\u8054\u7f51\u5236\u9020\u5546\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u52a8\u6001\u7269\u8054\u7f51\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u81ea S-S I \u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u67b6\u6784\u3002\u8be5\u6846\u67b6\u5141\u8bb8\u4efb\u4f55\u5177\u6709\u53ef\u9a8c\u8bc1\u4fe1\u4efb\u8fde\u63a5\u7684\u4e2a\u4f53\u5145\u5f53\u51ed\u8bc1\u9881\u53d1\u8005\u3002\u4fe1\u4efb\u901a\u8fc7\u57fa\u4e8e\u80cc\u4e66\u7684\u8ba1\u7b97\u52a8\u6001\u5efa\u7acb\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u4fe1\u4efb\u94fe\u673a\u5236\u7ef4\u62a4\u3002\u533a\u5757\u94fe\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u6570\u636e\u6ce8\u518c\u8868\uff0c\u667a\u80fd\u5408\u7ea6\u81ea\u52a8\u5316\u4e86\u51ed\u8bc1\u9881\u53d1\u3001\u9a8c\u8bc1\u548c\u64a4\u9500\u7b49\u5173\u952e\u6d41\u7a0b\u3002", "result": "\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u975e\u5e38\u9002\u5408\u52a8\u6001\u4e14\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u3002"}}
{"id": "2507.09470", "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "categories": ["cs.CL", "cs.AI", "68T07"], "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316DRAGON Longformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u533b\u7597\u6848\u4f8b\u63cf\u8ff0\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002", "motivation": "\u65e8\u5728\u4f18\u5316DRAGON Longformer\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\uff08\u7279\u522b\u662f\u533b\u7597\u6848\u4f8b\u63cf\u8ff0\u7684\u4e8c\u5206\u7c7b\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5305\u542b500\u4efd\u4e34\u5e8a\u6848\u4f8b\uff08400\u4efd\u8bad\u7ec3\uff0c100\u4efd\u9a8c\u8bc1\uff09\u7684\u6570\u636e\u96c6\u3002\u5728\u9884\u8bad\u7ec3\u7684DRAGON Longformer\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u9886\u57df\u7279\u5b9a\u9884\u5904\u7406\u548c\u67b6\u6784\u8c03\u6574\u3002\u5177\u4f53\u4fee\u6539\u5305\u62ec\u5c06\u5e8f\u5217\u957f\u5ea6\u4ece512\u589e\u52a0\u52301024\uff0c\u5b66\u4e60\u7387\u4ece1e-05\u8c03\u6574\u52305e-06\uff0c\u8bad\u7ec3\u5468\u671f\u4ece5\u6269\u5c55\u52308\uff0c\u5e76\u878d\u5165\u4e13\u4e1a\u533b\u5b66\u672f\u8bed\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a\u51c6\u786e\u7387\u4ece72.0%\u63d0\u9ad8\u523085.2%\uff0c\u7cbe\u786e\u7387\u4ece68.0%\u63d0\u9ad8\u523084.1%\uff0c\u53ec\u56de\u7387\u4ece75.0%\u63d0\u9ad8\u523086.3%\uff0cF1\u5206\u6570\u4ece71.0%\u63d0\u9ad8\u523085.2%\uff08p < .001\uff09\u3002\u6a21\u578b\u5728\u89e3\u91ca\u533b\u5b66\u672f\u8bed\u3001\u89e3\u5256\u6d4b\u91cf\u548c\u4e34\u5e8a\u89c2\u5bdf\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u6709\u6240\u8d21\u732e\uff0c\u5e76\u4e3a\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u9645\u610f\u4e49\u3002\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u591a\u79cd\u533b\u7597\u6761\u4ef6\u4e0b\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u9884\u793a\u7740\u5176\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09742", "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "categories": ["cs.AI"], "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u5173\u7cfb\u5f15\u5bfc\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08Causal DQ\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u6709\u9650\u7684AI\u9a71\u52a8\u5236\u9020\u73af\u5883\u4e2d\u4f18\u5316\u4f20\u611f\u5668\u653e\u7f6e\u4ee5\u5feb\u901f\u68c0\u6d4b\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u56e0\u679c\u6027\u6216\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u5e72\u9884\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740AI\u9a71\u52a8\u5236\u9020\u7684\u53d1\u5c55\uff0c\u5b9e\u65f6\u6570\u636e\u6d41\u76d1\u6d4b\u9700\u6c42\u5267\u589e\uff0c\u4f46\u8d44\u6e90\u6709\u9650\u5bfc\u81f4\u65e0\u6cd5\u5168\u9762\u90e8\u7f72\u4f20\u611f\u5668\u3002\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u4f20\u611f\u5668\u653e\u7f6e\u65b9\u6cd5\u5e38\u5ffd\u7565\u5173\u952e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u6216\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u4eba\u5de5\u5e72\u9884\u6765\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u8fd9\u5728\u5b9e\u9645\u4e2d\u662f\u4e0d\u53ef\u884c\u7684\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u56e0\u679c\u5173\u7cfb\u5f15\u5bfc\u7684\u6df1\u5ea6Q\u7f51\u7edc\u201d\uff08Causal DQ\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u4f20\u611f\u5668\u653e\u7f6e\u3002\u8be5\u65b9\u6cd5\u5728\u6df1\u5ea6Q\u7f51\u7edc\u8bad\u7ec3\u7684\u6bcf\u4e2a\u9636\u6bb5\u90fd\u6574\u5408\u4e86\u56e0\u679c\u4fe1\u606f\u3002", "result": "\u6240\u63d0\u51fa\u7684Causal DQ\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u7d27\u5bc6\u7684\u7406\u8bba\u8bef\u5dee\u754c\u9650\u3002\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u8bad\u7ec3\u540e\u7684\u56e0\u679c\u5f15\u5bfcQ\u7f51\u7edc\u663e\u8457\u7f29\u77ed\u4e86\u5f02\u5e38\u68c0\u6d4b\u65f6\u95f4\u3002", "conclusion": "Causal DQ\u65b9\u6cd5\u5728\u5927\u578b\u771f\u5b9e\u6570\u636e\u6d41\u7684\u4f20\u611f\u5668\u653e\u7f6e\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6709\u6548\u6027\u3002\u5176\u57fa\u672c\u601d\u60f3\u548c\u6d1e\u5bdf\u529b\u53ef\u63a8\u5e7f\u5e94\u7528\u4e8e\u5404\u79cd\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.08871", "pdf": "https://arxiv.org/pdf/2507.08871", "abs": "https://arxiv.org/abs/2507.08871", "authors": ["Xishun Liao", "Haoxuan Ma", "Yifan Liu", "Yuxiang Wei", "Brian Yueshuai He", "Chris Stanford", "Jiaqi Ma"], "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u4ea4\u901a\u9700\u6c42\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6570\u636e\u9a71\u52a8\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u8f6c\u79fb\uff0c\u5728\u6d1b\u6749\u77f6\u7684\u5b9e\u65bd\u548c\u9a8c\u8bc1\u8868\u660e\u5176\u6027\u80fd\u5ab2\u7f8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f46\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u7cbe\u5ea6\u9ad8\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6d3b\u52a8\u7684\u4ea4\u901a\u9700\u6c42\u6a21\u578b\uff08ABMs\uff09\u867d\u7136\u690d\u6839\u4e8e\u884c\u4e3a\u7406\u8bba\uff0c\u4f46\u5e38\u4f9d\u8d56\u7b80\u5316\u89c4\u5219\u548c\u5047\u8bbe\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u533a\u57df\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5b66\u4e60\u578b\u4ea4\u901a\u9700\u6c42\u5efa\u6a21\u6846\u67b6\uff0c\u57fa\u4e8e\u5bb6\u5ead\u7684\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u7efc\u5408\u751f\u6210\u5bb6\u5ead\u534f\u8c03\u7684\u65e5\u5e38\u6d3b\u52a8\u6a21\u5f0f\u3002\u8be5\u6846\u67b6\u5c06\u4eba\u53e3\u5408\u6210\u3001\u534f\u8c03\u6d3b\u52a8\u751f\u6210\u3001\u5730\u70b9\u5206\u914d\u548c\u5927\u89c4\u6a21\u5fae\u89c2\u4ea4\u901a\u4eff\u771f\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\u4e2d\uff0c\u5177\u6709\u5b8c\u5168\u751f\u6210\u6027\u3001\u6570\u636e\u9a71\u52a8\u3001\u53ef\u6269\u5c55\u548c\u53ef\u8f6c\u79fb\u7684\u7279\u70b9\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6d1b\u6749\u77f6\uff081000\u4e07\u4eba\u53e3\uff09\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u65bd\u3002\u7efc\u5408\u9a8c\u8bc1\u8868\u660e\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u590d\u73b0\u771f\u5b9e\u51fa\u884c\u6a21\u5f0f\uff0c\u6027\u80fd\u4e0e\u4f20\u7edfABMs\u76f8\u5f53\uff0c\u4f46\u5efa\u6a21\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e0eSCAG ABM\u57fa\u51c6\u76f8\u6bd4\uff0c\u8d77\u70b9-\u7ec8\u70b9\uff08OD\uff09\u77e9\u9635\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fbe0.97\uff0c\u6bcf\u65e5\u8f66\u8f86\u91cc\u7a0b\uff08VMT\uff09\u7684Jensen-Shannon\u6563\u5ea6\u4e3a0.006\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a9.8%\u3002\u4e0eCaltrans PeMS\u7684\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u76f8\u6bd4\uff0c\u8d70\u5eca\u7ea7\u4ea4\u901a\u901f\u5ea6\u548c\u6d41\u91cf\u8bc4\u4f30\u7684JSD\u4e3a0.001\uff0cMAPE\u4e3a6.11%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u578b\u4ea4\u901a\u9700\u6c42\u5efa\u6a21\u6846\u67b6\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u514b\u670d\u4f20\u7edfABMs\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ea4\u901a\u89c4\u5212\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.09139", "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "PoseLLM\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027MLP\u8fde\u63a5\u5668\u53d6\u4ee3\u7ebf\u6027\u6295\u5f71\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u5f15\u5bfc\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\uff08\u5982LocLLM\uff09\u867d\u80fd\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4f46\u5176\u7ebf\u6027\u6295\u5f71\u5668\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u7a7a\u95f4-\u6587\u672c\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "method": "\u63d0\u51faPoseLLM\uff0c\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u5c42\u975e\u7ebf\u6027MLP\uff08\u5e26GELU\u6fc0\u6d3b\uff09\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u7ebf\u6027\u6295\u5f71\u5668\uff0c\u4f5c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\uff0c\u5b9e\u73b0\u5206\u5c42\u8de8\u6a21\u6001\u7279\u5f81\u8f6c\u6362\uff0c\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u548c\u6587\u672c\u5173\u952e\u70b9\u63cf\u8ff0\u7684\u878d\u5408\u3002", "result": "\u5728COCO\u9a8c\u8bc1\u96c6\u4e0a\uff0cPoseLLM\u53d6\u5f97\u4e8677.8 AP\uff0c\u6bd4LocLLM\u63d0\u9ad8\u4e860.4 AP\u3002\u540c\u65f6\uff0c\u5728Human-Art\u548cMPII\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e00\u4e2a\u7b80\u5355\u4f46\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u8fde\u63a5\u5668\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8bed\u8a00\u5f15\u5bfc\u59ff\u6001\u4f30\u8ba1\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u727a\u7272\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2507.10259", "pdf": "https://arxiv.org/pdf/2507.10259", "abs": "https://arxiv.org/abs/2507.10259", "authors": ["Chengze Du", "Zhiwei Yu", "Heng Xu", "Haojie Wang", "Bo liu", "Jialong Li"], "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning", "categories": ["cs.DC", "cs.NI"], "comment": "17 pages, 12 figures", "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LLM\u5206\u5e03\u5f0fGPU\u63a8\u7406\u4e2d\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u7f3a\u4e4f\u65f6\u7a7a\u611f\u77e5\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faTORTA\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u5c42\u8c03\u5ea6\u8bbe\u8ba1\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7387\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "LLM\u670d\u52a1\u5bf9\u5206\u5e03\u5f0fGPU\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u4ec5\u57fa\u4e8e\u77ac\u65f6\u72b6\u6001\u51b3\u7b56\uff0c\u5ffd\u89c6\u4efb\u52a1\u9700\u6c42\u548c\u8d44\u6e90\u968f\u65f6\u95f4\u6f14\u53d8\uff0c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\u3001\u4efb\u52a1\u8fc1\u79fb\u5f00\u9500\u5927\u3001\u52a8\u6001\u8d1f\u8f7d\u4e0b\u54cd\u5e94\u6027\u5dee\u3002", "method": "\u63d0\u51faTORTA\uff08Temporal Optimal Resource scheduling via Two-layer Architecture\uff09\u6846\u67b6\uff0c\u5f15\u5165\u65f6\u7a7a\u8c03\u5ea6\uff0c\u6355\u83b7\u957f\u671f\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\u548c\u77ed\u671f\u6267\u884c\u7ea6\u675f\u3002\u91c7\u7528\u4e24\u5c42\u8bbe\u8ba1\uff1a\u5b8f\u89c2\u8c03\u5ea6\u5668\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6700\u4f18\u4f20\u8f93\u534f\u8c03\u8de8\u533a\u57df\u4efb\u52a1\u5206\u914d\uff1b\u5fae\u89c2\u5206\u914d\u5668\u5728\u533a\u57df\u5185\u4f18\u5316\u4efb\u52a1\u5230\u670d\u52a1\u5668\u7684\u5206\u914d\uff0c\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u548c\u5207\u6362\u6210\u672c\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0cTORTA\u5728\u591a\u7f51\u7edc\u62d3\u6251\u4e0b\uff1a\u5e73\u5747\u63a8\u7406\u54cd\u5e94\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe15%\uff1b\u8d1f\u8f7d\u5747\u8861\u63d0\u5347\u7ea64-5%\uff1b\u603b\u8fd0\u8425\u6210\u672c\u964d\u4f4e10-20%\u3002", "conclusion": "TORTA\u901a\u8fc7\u5f15\u5165\u65f6\u7a7a\u611f\u77e5\u548c\u5206\u5c42\u8c03\u5ea6\u67b6\u6784\uff0c\u6709\u6548\u514b\u670d\u4e86LLM\u5206\u5e03\u5f0fGPU\u63a8\u7406\u4e2d\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u6210\u672c\u6548\u7387\u3002"}}
{"id": "2507.09474", "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "categories": ["cs.CL"], "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86CoNLL-2013\u8bed\u6cd5\u7ea0\u9519\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u62ec\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u53c2\u8d5b\u56e2\u961f\u7684\u65b9\u6cd5\u548c\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "\u7ec4\u7ec7CoNLL-2013\u5171\u4eab\u4efb\u52a1\u65e8\u5728\u63a8\u52a8\u8bed\u6cd5\u7ea0\u9519\uff08GEC\uff09\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u800c\u672c\u6587\u7684\u52a8\u673a\u662f\u5168\u9762\u8bb0\u5f55\u548c\u603b\u7ed3\u8be5\u4efb\u52a1\u7684\u5404\u9879\u7ec6\u8282\u548c\u6210\u679c\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u8bba\u6587\u63cf\u8ff0\u4e86\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u8bc4\u5206\u5de5\u5177\uff1b\u6982\u8ff0\u4e86\u5404\u53c2\u8d5b\u56e2\u961f\u91c7\u7528\u7684\u5404\u79cd\u65b9\u6cd5\uff1b\u5e76\u5c55\u793a\u4e86\u6700\u7ec8\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86CoNLL-2013\u8bed\u6cd5\u7ea0\u9519\u5171\u4eab\u4efb\u52a1\u7684\u6700\u7ec8\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u5168\u9762\u603b\u7ed3\u4e86CoNLL-2013\u8bed\u6cd5\u7ea0\u9519\u5171\u4eab\u4efb\u52a1\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u540e\u7eed\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u7840\u6027\u8d44\u6599\u548c\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2507.09751", "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76f4\u63a5\u6574\u5408\u5230\u6b21\u534f\u8c03\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u89e3\u91ca\u51fd\u6570\u4e2d\uff0c\u4ee5\u89e3\u51b3LLM\u8f93\u51fa\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u4fdd\u6301\u903b\u8f91\u53ef\u9760\u6027\u548c\u5b8c\u5907\u6027\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u5f62\u5f0f\u63a8\u7406\u4e2d\u5229\u7528LLM\u7684\u5e7f\u6cdb\u53c2\u6570\u77e5\u8bc6\uff0c\u540c\u65f6\u514b\u670d\u5176\u56fa\u6709\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06LLM\u76f4\u63a5\u6574\u5408\u5230\u6b21\u534f\u8c03\u903b\u8f91\uff08paraconsistent logic\uff09\u7684\u5f62\u5f0f\u8bed\u4e49\u89e3\u91ca\u51fd\u6570\u4e2d\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4f7f\u7528\u4ece\u591a\u4e2a\u7b80\u77ed\u4e8b\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u521b\u5efa\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u8be5\u51fd\u6570\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528LLM\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u5e95\u5c42\u903b\u8f91\u7684\u53ef\u9760\u6027\uff08soundness\uff09\u548c\u5b8c\u5907\u6027\uff08completeness\uff09\u5c5e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06LLM\u7684\u77e5\u8bc6\u4e0e\u6b21\u534f\u8c03\u903b\u8f91\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLM\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u4e3a\u6784\u5efa\u65e2\u80fd\u5229\u7528LLM\u80fd\u529b\u53c8\u80fd\u4fdd\u6301\u903b\u8f91\u4e25\u8c28\u6027\u7684\u5f62\u5f0f\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.08873", "pdf": "https://arxiv.org/pdf/2507.08873", "abs": "https://arxiv.org/abs/2507.08873", "authors": ["Shaoran Yang", "Dongyu Wei", "Hanzhi Yu", "Zhaohui Yang", "Yuchen Liu", "Mingzhe Chen"], "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE GLOBECOM 2025", "summary": "In this paper, a novel contrastive language-image pre-training (CLIP) model\nbased semantic communication framework is designed. Compared to standard neural\nnetwork (e.g.,convolutional neural network) based semantic encoders and\ndecoders that require joint training over a common dataset, our CLIP model\nbased method does not require any training procedures thus enabling a\ntransmitter to extract data meanings of the original data without neural\nnetwork model training, and the receiver to train a neural network for\nfollow-up task implementation without the communications with the transmitter.\nNext, we investigate the deployment of the CLIP model based semantic framework\nover a noisy wireless network. Since the semantic information generated by the\nCLIP model is susceptible to wireless noise and the spectrum used for semantic\ninformation transmission is limited, it is necessary to jointly optimize CLIP\nmodel architecture and spectrum resource block (RB) allocation to maximize\nsemantic communication performance while considering wireless noise, the delay\nand energy used for semantic communication. To achieve this goal, we use a\nproximal policy optimization (PPO) based reinforcement learning (RL) algorithm\nto learn how wireless noise affect the semantic communication performance thus\nfinding optimal CLIP model and RB for each user. Simulation results show that\nour proposed method improves the convergence rate by up to 40%, and the\naccumulated reward by 4x compared to soft actor-critic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u4f18\u52bf\u5728\u4e8e\u65e0\u9700\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u566a\u58f0\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f18\u5316\u5176\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u9700\u8981\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\uff0c\u4e14\u5176\u8bed\u4e49\u4fe1\u606f\u5728\u566a\u58f0\u65e0\u7ebf\u7f51\u7edc\u4e2d\u6613\u53d7\u5e72\u6270\uff0c\u540c\u65f6\u9891\u8c31\u8d44\u6e90\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u5728\u566a\u58f0\u3001\u65f6\u5ef6\u548c\u80fd\u8017\u7ea6\u675f\u4e0b\uff0c\u5982\u4f55\u4f18\u5316CLIP\u6a21\u578b\u67b6\u6784\u548c\u9891\u8c31\u8d44\u6e90\u5206\u914d\u4ee5\u6700\u5927\u5316\u901a\u4fe1\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\uff08\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7684\u53d1\u9001\u7aef\u65e0\u9700\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u63d0\u53d6\u6570\u636e\u8bed\u4e49\u3002\u4e3a\u5e94\u5bf9\u566a\u58f0\u65e0\u7ebf\u4fe1\u9053\u5e76\u4f18\u5316\u6027\u80fd\uff0c\u91c7\u7528\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b66\u4e60\u65e0\u7ebf\u566a\u58f0\u5982\u4f55\u5f71\u54cd\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u7528\u6237\u627e\u5230\u6700\u4f18\u7684CLIP\u6a21\u578b\u548c\u9891\u8c31\u8d44\u6e90\u5757\uff08RB\uff09\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0eSoft Actor-Critic\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8640%\uff0c\u7d2f\u79ef\u5956\u52b1\u63d0\u9ad8\u4e864\u500d\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u5e76\u4f18\u5316\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8054\u5408\u8bad\u7ec3\u9700\u6c42\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.09144", "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "\u63d0\u51faI\u00b2-World\uff0c\u4e00\u4e2a\u9ad8\u6548\u76844D\u5360\u7528\u7387\u9884\u6d4b\u6846\u67b6\u3002\u901a\u8fc7\u89e3\u8026\u573a\u666ftokenization\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u63a8\u7406\uff0c\u89e3\u51b33D\u4e16\u754c\u6a21\u578btokenization\u96be\u9898\u3002", "motivation": "3D\u573a\u666f\u6f14\u5316\u9884\u6d4b\u548c\u65b0\u573a\u666f\u751f\u6210\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u6781\u7aef\u60c5\u51b5\u5904\u7406\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u9ad8\u6548\u5730\u5bf9\u590d\u67423D\u573a\u666f\u8fdb\u884ctokenization\u4ecd\u662f3D\u4e16\u754c\u6a21\u578b\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faI\u00b2-World\uff0c\u4e00\u4e2a\u7528\u4e8e4D\u5360\u7528\u7387\u9884\u6d4b\u7684\u9ad8\u6548\u6846\u67b6\u3002\u5b83\u5c06\u573a\u666ftokenization\u89e3\u8026\u4e3a\u573a\u666f\u5185\uff08\u4f7f\u7528\u591a\u5c3a\u5ea6\u6b8b\u5dee\u91cf\u5316\u7b56\u7565\u538b\u7f293D\u573a\u666f\uff09\u548c\u573a\u666f\u95f4\uff08\u6b8b\u5dee\u805a\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\uff09tokenizer\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a\u7f16\u7801\u5668\u805a\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5e76\u9884\u6d4b\u53d8\u6362\u77e9\u9635\u4ee5\u5b9e\u73b0\u9ad8\u7ea7\u63a7\u5236\uff0c\u89e3\u7801\u5668\u5219\u57fa\u4e8e\u6b64\u77e9\u9635\u548c\u5386\u53f2token\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "I\u00b2-World\u57284D\u5360\u7528\u7387\u9884\u6d4b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cmIoU\u548cIoU\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa25.1%\u548c36.9%\u3002\u540c\u65f6\uff0c\u5176\u8ba1\u7b97\u6548\u7387\u6781\u9ad8\uff0c\u8bad\u7ec3\u5185\u5b58\u4ec5\u97002.9 GB\uff0c\u5e76\u80fd\u5b9e\u73b037.0 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "I\u00b2-World\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u8026tokenization\u548c\u9ad8\u6548\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e864D\u573a\u666f\u9884\u6d4b\u4e2d\u590d\u6742\u7684tokenization\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10267", "pdf": "https://arxiv.org/pdf/2507.10267", "abs": "https://arxiv.org/abs/2507.10267", "authors": ["Novruz Amirov", "Baran Isik", "Bilal Ihsan Tuncer", "Serif Bahtiyar"], "title": "DNS Tunneling: Threat Landscape and Improved Detection Solutions", "categories": ["cs.CR", "cs.LG", "cs.NI"], "comment": null, "summary": "Detecting Domain Name System (DNS) tunneling is a significant challenge in\nsecurity due to its capacity to hide harmful actions within DNS traffic that\nappears to be normal and legitimate. Traditional detection methods are based on\nrule-based approaches or signature matching methods that are often insufficient\nto accurately identify such covert communication channels. This research is\nabout effectively detecting DNS tunneling. We propose a novel approach to\ndetect DNS tunneling with machine learning algorithms. We combine machine\nlearning algorithms to analyze the traffic by using features extracted from DNS\ntraffic. Analyses results show that the proposed approach is a good candidate\nto detect DNS tunneling accurately.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684DNS\u96a7\u9053\u68c0\u6d4b\u65b0\u65b9\u6cd5\u3002", "motivation": "DNS\u96a7\u9053\u5229\u7528\u6b63\u5e38\u6d41\u91cf\u9690\u85cf\u6076\u610f\u884c\u4e3a\uff0c\u5bf9\u5b89\u5168\u6784\u6210\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u6216\u7b7e\u540d\u7684\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u51c6\u786e\u8bc6\u522b\u6b64\u7c7b\u9690\u853d\u901a\u4fe1\u3002", "method": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4eceDNS\u6d41\u91cf\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u6765\u68c0\u6d4bDNS\u96a7\u9053\u3002", "result": "\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4bDNS\u96a7\u9053\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u662f\u51c6\u786e\u68c0\u6d4bDNS\u96a7\u9053\u653b\u51fb\u7684\u826f\u597d\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2507.09477", "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u4ece\u7edf\u4e00\u7684\u63a8\u7406-\u68c0\u7d22\u89c6\u89d2\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u63a8\u7406\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u63a2\u8ba8\u4e86\u4e24\u8005\u5982\u4f55\u76f8\u4e92\u4f18\u5316\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u534f\u540c\u6846\u67b6\u53ca\u5176\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u80fd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e8b\u5b9e\u6027\u4f46\u7f3a\u4e4f\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff1b\u800c\u7eaf\u7cb9\u7684\u63a8\u7406\u65b9\u6cd5\u867d\u7136\u80fd\u5904\u7406\u590d\u6742\u63a8\u7406\u4f46\u5e38\u51fa\u73b0\u5e7b\u89c9\u6216\u4e8b\u5b9e\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f25\u5408RAG\u7684\u4e8b\u5b9e\u6027\u4e0e\u63a8\u7406\u65b9\u6cd5\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u672c\u7efc\u8ff0\u91c7\u7528\u7efc\u5408\u5206\u6790\u65b9\u6cd5\uff0c\u9996\u5148\u6620\u5c04\u4e86\u9ad8\u7ea7\u63a8\u7406\u5982\u4f55\u4f18\u5316RAG\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u63a8\u7406\u589e\u5f3aRAG\uff09\uff1b\u5176\u6b21\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u68c0\u7d22\u77e5\u8bc6\u5982\u4f55\u4e3a\u590d\u6742\u63a8\u7406\u63d0\u4f9b\u7f3a\u5931\u524d\u63d0\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\uff08RAG\u589e\u5f3a\u63a8\u7406\uff09\uff1b\u6700\u540e\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u65b0\u5174\u7684\u534f\u540cRAG-\u63a8\u7406\u6846\u67b6\uff08\u5982\u4ee3\u7406LLMs\u8fed\u4ee3\u4ea4\u7ec7\u641c\u7d22\u4e0e\u63a8\u7406\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u76f8\u5173\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u5f00\u653e\u6311\u6218\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u534f\u540cRAG-\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u5730\u7ed3\u5408\u68c0\u7d22\u4e0e\u63a8\u7406\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u672c\u7efc\u8ff0\u6210\u529f\u5730\u7efc\u5408\u5e76\u5206\u7c7b\u4e86\u5f53\u524d\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u529b\u4e8e\u5f00\u53d1\u66f4\u6709\u6548\u3001\u591a\u6a21\u6001\u9002\u5e94\u3001\u53ef\u4fe1\u8d56\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6df1\u5ea6RAG-\u63a8\u7406\u7cfb\u7edf\u3002\u672c\u6587\u4e3a\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.09801", "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u6982\u8ff0\u4e86\u65e8\u5728\u534f\u8c03\u4e2d\u6b62\u5371\u9669AI\u5f00\u53d1\u4e0e\u90e8\u7f72\u7684\u5173\u952e\u6280\u672f\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u5e94\u5bf9AI\u7cfb\u7edf\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u5931\u63a7\u3001\u6ee5\u7528\u3001\u5730\u7f18\u653f\u6cbb\u4e0d\u7a33\u5b9a\u548c\u6743\u529b\u96c6\u4e2d\u7b49\u524d\u6240\u672a\u6709\u7684\u98ce\u9669\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u98ce\u9669\u5e76\u907f\u514d\u6700\u574f\u7ed3\u679c\uff0c\u653f\u5e9c\u53ef\u80fd\u9700\u8981\u5efa\u7acb\u534f\u8c03\u4e2d\u6b62\u5371\u9669AI\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u6982\u8ff0\u5e76\u8ba8\u8bba\u4e86\u80fd\u591f\u5b9e\u73b0\u534f\u8c03\u4e2d\u6b62\u5371\u9669AI\u6d3b\u52a8\u7684\u5173\u952e\u6280\u672f\u5e72\u9884\u63aa\u65bd\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6280\u672f\u5e72\u9884\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9650\u5236\u5404\u79cd\u5371\u9669AI\u6d3b\u52a8\uff0c\u5e76\u80fd\u591f\u5f62\u6210\u6f5c\u5728AI\u6cbb\u7406\u8ba1\u5212\u7684\u6280\u672f\u57fa\u7840\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6280\u672f\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u4e3a\u534f\u8c03\u4e2d\u6b62\u5371\u9669AI\u6d3b\u52a8\u63d0\u4f9b\u6280\u672f\u57fa\u7840\uff0c\u4ece\u800c\u5e2e\u52a9\u5e94\u5bf9AI\u98ce\u9669\u5e76\u652f\u6301\u672a\u6765\u7684AI\u6cbb\u7406\u8ba1\u5212\u3002"}}
{"id": "2507.08874", "pdf": "https://arxiv.org/pdf/2507.08874", "abs": "https://arxiv.org/abs/2507.08874", "authors": ["Yulin Sun", "Xiaopeng Si", "Runnan He", "Xiao Hu", "Peter Smielewski", "Wenlong Wang", "Xiaoguang Tong", "Wei Yue", "Meijun Pang", "Kuo Zhang", "Xizi Song", "Dong Ming", "Xiuyun Liu"], "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework", "categories": ["cs.LG"], "comment": null, "summary": "Timely identification of harmful brain activities via electroencephalography\n(EEG) is critical for brain disease diagnosis and treatment, which remains\nlimited application due to inter-rater variability, resource constraints, and\npoor generalizability of existing artificial intelligence (AI) models. In this\nstudy, a convolutional neural network model, VIPEEGNet, was developed and\nvalidated using EEGs recorded from Massachusetts General Hospital/Harvard\nMedical School. The VIPEEGNet was developed and validated using two independent\ndatasets, collected between 2006 and 2020. The development cohort included EEG\nrecordings from 1950 patients, with 106,800 EEG segments annotated by at least\none experts (ranging from 1 to 28). The online testing cohort consisted of EEG\nsegments from a subset of an additional 1,532 patients, each annotated by at\nleast 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved\nhigh accuracy, with an AUROC for binary classification of seizure, LPD, GPD,\nLRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%\nCI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),\n0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi\nclassification, the sensitivity of VIPEEGNET for the six categories ranges from\n36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance\nsimilar to human experts. Notably, the external validation showed\nKullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the\nexisting 2,767 competing algorithms, while we only used 2.8% of the parameters\nof the first-ranked algorithm.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aVIPEEGNet\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u53ca\u65f6\u8bc6\u522b\u8111\u7535\u56fe\uff08EEG\uff09\u4e2d\u7684\u6709\u5bb3\u8111\u6d3b\u52a8\u3002\u8be5\u6a21\u578b\u5728\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u4ee5\u6781\u4f4e\u7684\u53c2\u6570\u91cf\u83b7\u5f97\u4e86\u9876\u5c16\u6392\u540d\u3002", "motivation": "\u53ca\u65f6\u8bc6\u522b\u8111\u7535\u56fe\uff08EEG\uff09\u4e2d\u7684\u6709\u5bb3\u8111\u6d3b\u52a8\u5bf9\u4e8e\u8111\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6a21\u578b\u53d7\u9650\u4e8e\u5224\u8bfb\u5dee\u5f02\u3001\u8d44\u6e90\u9650\u5236\u548c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5e94\u7528\u53d7\u9650\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u540d\u4e3aVIPEEGNet\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u6765\u81ea\u9ebb\u7701\u603b\u533b\u9662/\u54c8\u4f5b\u533b\u5b66\u9662\u7684\u4e24\u4e2a\u72ec\u7acbEEG\u6570\u636e\u96c6\u8fdb\u884c\u5f00\u53d1\u548c\u9a8c\u8bc1\uff0c\u5176\u4e2d\u5305\u62ec1950\u540d\u60a3\u8005\u7684\u5f00\u53d1\u961f\u5217\uff08106,800\u4e2aEEG\u7247\u6bb5\uff09\u548c1532\u540d\u60a3\u8005\u7684\u5728\u7ebf\u6d4b\u8bd5\u961f\u5217\u3002", "result": "VIPEEGNet\u5728\u5f00\u53d1\u961f\u5217\u4e2d\u5bf9\u516d\u7c7b\u6709\u5bb3\u8111\u6d3b\u52a8\u7684\u4e8c\u5206\u7c7bAUROC\u8303\u56f4\u57280.930\u81f30.972\u4e4b\u95f4\u3002\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u654f\u611f\u6027\u8303\u56f4\u4e3a36.8%\u81f388.2%\uff0c\u7cbe\u786e\u5ea6\u8303\u56f4\u4e3a55.6%\u81f380.4%\uff0c\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u4f3c\u3002\u5916\u90e8\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u7684Kullback-Leibler\u6563\u5ea6\uff08KLD\uff09\u5206\u522b\u4e3a0.223\u548c0.273\uff0c\u57282767\u4e2a\u7ade\u4e89\u7b97\u6cd5\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u4e14\u53c2\u6570\u91cf\u4ec5\u4e3a\u6392\u540d\u7b2c\u4e00\u7b97\u6cd5\u76842.8%\u3002", "conclusion": "VIPEEGNet\u6a21\u578b\u5728\u8bc6\u522b\u6709\u5bb3\u8111\u6d3b\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5176\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9876\u7ea7\u7b97\u6cd5\u7684\u53c2\u6570\u6548\u7387\u3002\u8fd9\u8868\u660e\u5b83\u6709\u671b\u514b\u670d\u73b0\u6709AI\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u6709\u5bb3\u8111\u6d3b\u52a8\u7684\u53ca\u65f6\u8bc6\u522b\u548c\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2507.09168", "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStable Score Distillation (SSD) \u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u548c3D\u7f16\u8f91\u4e2d\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002SSD\u901a\u8fc7\u7b80\u5316\u7ed3\u6784\u548c\u5f15\u5165\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u3001\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u548c3D\u7f16\u8f91\u65b9\u6cd5\uff08\u5982Delta Denoising Score\uff09\u5728\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e9b\u5c40\u9650\u6027\u6e90\u4e8e\u5bf9\u590d\u6742\u8f85\u52a9\u7ed3\u6784\u7684\u4f9d\u8d56\uff0c\u5176\u5f15\u5165\u4e86\u51b2\u7a81\u7684\u4f18\u5316\u4fe1\u53f7\u5e76\u9650\u5236\u4e86\u7cbe\u786e\u7684\u5c40\u90e8\u7f16\u8f91\u3002", "method": "SSD\u6846\u67b6\u901a\u8fc7\u5c06\u5355\u4e2a\u5206\u7c7b\u5668\u951a\u5b9a\u5230\u6e90\u63d0\u793a\u8bcd\u6765\u589e\u5f3a\u7f16\u8f91\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0cSSD\u5229\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u65b9\u7a0b\u5b9e\u73b0\u8de8\u63d0\u793a\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u5e38\u6570\u9879\u7684\u7a7a\u6587\u672c\u5206\u652f\u6765\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u63d0\u793a\u589e\u5f3a\u5206\u652f\u4ee5\u63d0\u9ad8\u7f16\u8f91\u5f3a\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u98ce\u683c\u8f6c\u6362\u3002", "result": "SSD\u65b9\u6cd5\u57282D\u548c3D\u7f16\u8f91\u4efb\u52a1\uff08\u5305\u62ecNeRF\u548c\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u590d\u6742\u6027\u3002", "conclusion": "SSD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5b9a\u3001\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u548c3D\u7f16\u8f91\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86M2SaG\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u6570\u636e\u96c6\u548cViSP\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528PPO\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bbd\u523a\u6587\u672c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u8bbd\u523a\u7814\u7a76\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u8bbd\u523a\u751f\u6210\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u6001\u800c\u5ffd\u89c6\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u53ca\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u56fe\u50cf\u5185\u5bb9\u4e0e\u8bbd\u523a\u610f\u56fe\u4e0d\u5339\u914d\u3002", "method": "\u5f15\u5165\u4e86M2SaG\uff0c\u4e00\u4e2a\u5305\u542b4,970\u4e2a\u6837\u672c\u7684\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u56fe\u50cf\u3001\u8bbd\u523a\u6587\u672c\u548c\u8bbd\u523a\u76ee\u6807\u3002\u63d0\u51fa\u4e86ViSP\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5176\u4e2dPPO\u5229\u7528DIP\u7684\u5956\u52b1\u5206\u6570\u6307\u5bfc\u8bbd\u523a\u6587\u672c\u751f\u6210\uff0c\u5bf9\u6bd4\u5b66\u4e60\u4fc3\u4f7f\u6a21\u578b\u504f\u597d\u9ad8\u5956\u52b1\u5206\u6570\u7684\u8f93\u51fa\u3002", "result": "ViSP\u5728\u4e94\u7ec4\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u751f\u6210\u6587\u672c\u7684\u5e73\u5747\u8bbd\u523a\u5206\u6570\uff080.898 vs. 0.770\uff09\u548c\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\uff080.768 vs. 0.739\uff09\u5747\u9ad8\u4e8e\u539f\u59cb\u6570\u636e\u96c6\uff0c\u8bc1\u660eViSP\u751f\u6210\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u8bbd\u523a\u5185\u5bb9\u3002", "conclusion": "M2SaG\u548cViSP\u663e\u8457\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u4e86ViSP\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u8bbd\u523a\u5185\u5bb9\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.09850", "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "categories": ["cs.AI"], "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5bf9\u57fa\u7840\u6a21\u578b\uff08Qwen2.5-32B\uff09\u8fdb\u884c\u8f7b\u91cf\u5fae\u8c03\uff0c\u4ec5\u4f7f\u752820\u4e2a\u9ad8\u8d28\u91cf\u7684\u4e13\u5bb6\u957f\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u793a\u4f8b\uff0c\u5373\u53ef\u663e\u8457\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u7684\u6a21\u578b\uff08Qwen2.5-Math-72B-Instruct\uff09\u3002\u540c\u65f6\u53d1\u73b0\uff0c\u975e\u4e13\u5bb6CoT\u6570\u636e\u96be\u4ee5\u590d\u73b0\u4e13\u5bb6CoT\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u957fCoT\u8f68\u8ff9\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u57fa\u7840\u6a21\u578b\u53ef\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6216\u84b8\u998f\u83b7\u5f97CoT\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u662f\u5426\u4ec5\u901a\u8fc7\u63d0\u793a\u6216\u6781\u5c11\u91cf\u5fae\u8c03\u5c31\u80fd\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u8bf1\u5bfc\u751f\u6210\u957fCoT\u3002", "method": "1. \u8f7b\u91cf\u5fae\u8c03\uff1a\u4f7f\u7528\u6765\u81ea\u63a8\u7406\u6a21\u578bQwQ-32B-Preview\u768420\u4e2a\u957fCoT\u793a\u4f8b\u5bf9\u57fa\u7840\u6a21\u578bQwen2.5-32B\u8fdb\u884c\u5fae\u8c03\u30022. \u5bf9\u6bd4\u5b9e\u9a8c\uff1a\u63a2\u7d22\u4f7f\u7528\u6765\u81ea\u975e\u63a8\u7406\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u8005\u7684CoT\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u8f6e\u7f16\u8f91\u548c\u7ed3\u6784\u5316\u6307\u5bfc\u8fdb\u884c\u589e\u5f3a\uff0c\u4ee5\u8bc4\u4f30\u5176\u6548\u679c\u30023. \u7279\u6027\u5206\u6790\uff1a\u5206\u6790\u5f71\u54cd\u63a8\u7406\u84b8\u998f\u7684\u5173\u952e\u6570\u636e\u5c5e\u6027\uff0c\u5982\u95ee\u9898\u96be\u5ea6\u3001\u591a\u6837\u6027\u548c\u7b54\u6848\u957f\u5ea6\u3002", "result": "1. \u6027\u80fd\u663e\u8457\u63d0\u5347\uff1a\u7ecf\u8fc7\u5c11\u91cf\u4e13\u5bb6CoT\u793a\u4f8b\u5fae\u8c03\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684Qwen2.5-Math-72B-Instruct\u6a21\u578b\uff0c\u8bc1\u660e\u5c11\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u80fd\u89e3\u9501\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u30022. \u6570\u636e\u8d28\u91cf\u5dee\u5f02\uff1a\u6765\u81ea\u975e\u63a8\u7406\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u8005\u7684CoT\u6570\u636e\u672a\u80fd\u8fbe\u5230\u63a8\u7406\u6a21\u578bCoT\u7684\u6027\u80fd\uff0c\u8868\u660e\u4e13\u5bb6CoT\u5b58\u5728\u96be\u4ee5\u590d\u5236\u7684\u6f5c\u5728\u7279\u8d28\u30023. \u5206\u6790\u4e86\u63a8\u7406\u6570\u636e\u7279\u6027\u5bf9\u84b8\u998f\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "1. \u5373\u4f7f\u662f\u5c0f\u91cf\u7cbe\u5fc3\u7b56\u5212\u7684\u9ad8\u8d28\u91cf\u4eba\u5de5\u7f16\u5199\u7684CoT\u6570\u636e\uff0c\u4e5f\u6709\u671b\u6fc0\u6d3b\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u30022. \u4e13\u5bb6CoT\u7684\u5185\u5728\u8d28\u91cf\u5bf9\u63a8\u7406\u80fd\u529b\u84b8\u998f\u81f3\u5173\u91cd\u8981\u30023. \u53d1\u5e03\u4e86\u4eba\u5de5\u7f16\u5199\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u9f13\u52b1\u672a\u6765\u5bf9\u5c0f\u89c4\u6a21\u63a8\u7406\u76d1\u7763\u6709\u6548\u6027\u7684\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2507.08877", "pdf": "https://arxiv.org/pdf/2507.08877", "abs": "https://arxiv.org/abs/2507.08877", "authors": ["Hanlong Zhang", "Jingsheng Yang", "Hao Li", "Yuhao He", "Franck Gong"], "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODIA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u529f\u80fd\u8c03\u7528\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u529f\u80fd\u8c03\u7528\uff08Function Calling\uff09\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5728\u7ebf\u52a0\u901f\u5bfc\u5411\u84b8\u998f\u201d\uff08Oriented Distillation for Inline Acceleration, ODIA\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5728\u7ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u81ea\u52a8\u8bc6\u522b\u751f\u4ea7\u6d41\u91cf\u4e2d\u7684\u201c\u7b80\u5355\u67e5\u8be2\u201d\uff0c\u5e76\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u6a21\u578b\u84b8\u998f\u5230\u5c0f\u578b\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u52a0\u901f\u3002", "result": "ODIA\u65b9\u6cd5\u4f7f\u54cd\u5e94\u5ef6\u8fdf\u9884\u671f\u964d\u4f4e45%\uff0c\u4e2d\u4f4d\u6570\u964d\u4f4e78%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u5728\u5b9e\u9645\u97f3\u4e50\u5e94\u7528\u7684\u90e8\u7f72\u4e2d\uff0c\u5c0f\u578b\u6a21\u578b\u6210\u529f\u5904\u7406\u4e8660%\u7684\u6d41\u91cf\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "ODIA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u52a0\u901fLLM\u7684\u529f\u80fd\u8c03\u7528\uff0c\u663e\u8457\u6539\u5584\u7528\u6237\u4f53\u9a8c\uff0c\u4e14\u7531\u4e8e\u5176\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u66f4\u65b0\u673a\u5236\uff0c\u975e\u5e38\u9002\u5408\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2507.09180", "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eVision Transformer\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u65e0\u76d1\u7763\u5b66\u4e60\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0sim2real\u8fc1\u79fb\u3002", "motivation": "\u6df1\u5ea6\u4fe1\u606f\u5bf9\u573a\u666f\u5916\u89c2\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u5e76\u5305\u542b3D\u7a7a\u95f4\u7ec6\u8282\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u4e0eRGB\u878d\u5408\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6838\u5fc3\u65b9\u6cd5\u662f\u4e00\u4e2a\u57fa\u4e8eVision Transformer\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u7528\u4e8e\u878d\u5408RGB\u548c\u6df1\u5ea6\u6a21\u6001\u3002\u5177\u4f53\u5b9e\u73b0\u4e3a\uff1a\u4e0d\u540c\u6a21\u6001\u9996\u5148\u7531\u72ec\u7acb\u7684CNN\u5206\u652f\u5904\u7406\uff0c\u7136\u540e\u5c06\u7ed3\u5408\u7684\u5377\u79ef\u7279\u5f81\u9001\u5165\u53ef\u6269\u5c55\u7684Vision Transformer\u4ee5\u83b7\u5f97\u89c6\u89c9\u8868\u793a\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u548c\u975e\u63a9\u7801token\u7684\u5bf9\u6bd4\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u4ee5\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002\u4e3a\u5b9e\u73b0sim2real\u8fc1\u79fb\uff0c\u5f00\u53d1\u4e86\u7075\u6d3b\u7684\u8bfe\u7a0b\u5b66\u4e60\u8c03\u5ea6\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u90e8\u7f72\u57df\u968f\u673a\u5316\u3002", "result": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u6216\u6027\u80fd\u6570\u636e\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u7ed3\u8bba\uff0c\u4f46\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u6027\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u4ee5\u53ca\u5b9e\u73b0sim2real\u8fc1\u79fb\u3002"}}
{"id": "2507.09485", "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728ABSA\u4efb\u52a1\u4e2d\u9762\u4e34\u77ed\u6587\u672c\u4e0a\u4e0b\u6587\u7406\u89e3\u56f0\u96be\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u91cf\u5c0f\u4e14\u6807\u7b7e\u5206\u5e03\u4e0d\u5e73\u8861\uff08\u6b63\u5411\u60c5\u611f\u5c45\u591a\uff09\u7684\u6311\u6218\u3002\u6570\u636e\u589e\u5f3a\u867d\u662f\u53ef\u884c\u7b56\u7565\uff0c\u4f46\u96be\u4ee5\u786e\u4fdd\u589e\u5f3a\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684ABSA\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u6839\u636e\u539f\u59cb\u6570\u636e\u751f\u6210\u6269\u589e\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u589e\u52a0\u6570\u636e\u91cf\u5e76\u5e73\u8861\u6807\u7b7e\u5206\u5e03\u3002\u4e3a\u63d0\u9ad8\u6269\u589e\u6570\u636e\u7684\u8d28\u91cf\uff0c\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u6570\u636e\u589e\u5f3a\u8fc7\u7a0b\u3002", "result": "\u5728ABSA\u82f1\u8bed\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u548c\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u6027\u80fd\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u3002", "conclusion": "\u7ed3\u5408LLM\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u589e\u5f3a\u8fc7\u7a0b\uff0c\u80fd\u6709\u6548\u89e3\u51b3ABSA\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u758f\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.09854", "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "categories": ["cs.AI"], "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "AI": {"tldr": "\u672c\u6587\u5c06\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u5176\u4e2d\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u7b26\u53f7\u5c42\uff0c\u6a21\u578b\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u63a5\u5730\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u6b64\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u53ef\u9760\u6027\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u7ecf\u5178\u7b26\u53f7AI\u7684\u4f18\u52bf\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u5b66\u4e60\u548c\u53ef\u9760\u63a8\u7406\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5c06\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u91cd\u65b0\u89e3\u91ca\u4e3a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u4ece\u800c\u5229\u7528\u5176\u4e92\u8865\u4f18\u52bf\uff0c\u4ee5\u671f\u63d0\u9ad8AI\u7684\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u6a21\u578b\u63a5\u5730\u7684\u7b26\u53f7AI\u7cfb\u7edf\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0c\u81ea\u7136\u8bed\u8a00\u88ab\u7528\u4f5c\u7b26\u53f7\u5c42\uff0c\u800c\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u5219\u7528\u4e8e\u5b9e\u73b0\u7b26\u53f7\u63a5\u5730\u3002\u7814\u7a76\u8005\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u65b0\u578b\u7684\u5b66\u4e60\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4fdd\u7559\u4e86\u4e0e\u4f20\u7edf\u5b66\u4e60\u548c\u63a8\u7406\u8303\u5f0f\u5728\u7ed3\u6784\u4e0a\u7684\u76f8\u4f3c\u6027\u3002\u901a\u8fc7\u5bf9\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u516c\u7406\u5316\u6f14\u7ece\u63a8\u7406\u8fc7\u7a0b\u8fdb\u884c\u521d\u6b65\u8bc4\u4f30\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u53ef\u9760\u6027\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u5c06\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u6a21\u578b\u63a5\u5730\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u7b26\u53f7\u5c42\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u4ee5\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u5b66\u4e60\u80fd\u529b\u4e0e\u7b26\u53f7AI\u7684\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347AI\u7684\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u53ef\u9760\u6027\u3002"}}
{"id": "2507.08905", "pdf": "https://arxiv.org/pdf/2507.08905", "abs": "https://arxiv.org/abs/2507.08905", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "G\u00f6ran Falkman", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Last Layer Hamiltonian Monte Carlo", "categories": ["cs.LG", "cs.AI", "G.3"], "comment": "25 pages, 15 figures, 6 tables, currently under submission", "summary": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u540d\u4e3aLL-HMC\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6700\u540e\u4e00\u5c42\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\uff0c\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u5206\u5e03\u5185\u5206\u7c7b\u548c\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u88ab\u8ba4\u4e3a\u662f\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5927\u578b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6570\u636e\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u5e94\u7528HMC\uff0c\u9700\u8981\u964d\u4f4e\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u662f\u9650\u5236HMC\u91c7\u6837\u4ec5\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\u8fdb\u884c\uff08LL-HMC\uff09\u3002\u5c06LL-HMC\u4e0e\u4e94\u79cd\u5176\u4ed6\u6700\u540e\u4e00\u5c42\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\uff08LL-PDL\uff09\u65b9\u6cd5\u5728\u4e09\u4e2a\u771f\u5b9e\u7684\u9a7e\u9a76\u5458\u884c\u4e3a\u548c\u610f\u56fe\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u5206\u5e03\u5185\u5206\u7c7b\u6027\u80fd\u3001\u6821\u51c6\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u3002\u4e3a\u786e\u4fdd\u7ed3\u679c\u7684\u9c81\u68d2\u6027\uff0c\u5bf9\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u8fdb\u884c\u4e86\u4e94\u6b21\u8d85\u53c2\u6570\u7f51\u683c\u641c\u7d22\u3002", "result": "LL-HMC\u5728\u5206\u5e03\u5185\u5206\u7c7b\u548cOOD\u68c0\u6d4b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002\u989d\u5916\u91c7\u6837\u7684\u6700\u540e\u4e00\u5c42\u53c2\u6570\u672a\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u80fd\u6539\u5584OOD\u68c0\u6d4b\u3002\u591a\u6761\u94fe\u6216\u4e0d\u540c\u8d77\u59cb\u4f4d\u7f6e\u5e76\u672a\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LL-HMC\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5177\u6709\u7ade\u4e89\u529b\u7684\u6700\u540e\u4e00\u5c42\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5206\u5e03\u5916\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfHMC\u7684\u8ba1\u7b97\u9650\u5236\u3002\u7136\u800c\uff0c\u8fc7\u591a\u7684\u91c7\u6837\u5bf9\u5206\u7c7b\u65e0\u76ca\uff0c\u4e14\u591a\u94fe\u6216\u591a\u8d77\u70b9\u5e76\u672a\u5e26\u6765\u4e00\u81f4\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.09183", "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63a2\u8ba8\u4e86\u6c60\u5316\u63d0\u793a\u65b9\u6cd5\u5728FSCIL\u4e2d\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u5176\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff08token\u7ef4\u5ea6\u9971\u548c\uff09\uff0c\u5e76\u63d0\u51faLGSP-Prompt\uff0c\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u63d0\u793a\u6709\u6548\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "Few-Shot Class-Incremental Learning (FSCIL)\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u589e\u91cf\u5b66\u4e60\u7684\u53cc\u91cd\u6311\u6218\u3002\u4f20\u7edf\u7684\u6c60\u5316\u63d0\u793a\u65b9\u6cd5\u5728FSCIL\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u63a2\u7a76\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u5176\u5728\u6b64\u573a\u666f\u4e0b\u5b58\u5728\u6027\u80fd\u4e0b\u964d\uff0c\u539f\u56e0\u5728\u4e8e\u6709\u9650\u6570\u636e\u4e0b\u8fc7\u591a\u7684\u63d0\u793a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\uff08token\u7ef4\u5ea6\u9971\u548c\uff09\u3002", "method": "\u63d0\u51faLGSP-Prompt\uff08Local-Global Spatial Prompting\uff09\uff0c\u5c06\u6c60\u5316\u63d0\u793a\u5b66\u4e60\u4ecetoken\u7ef4\u5ea6\u8f6c\u5411\u7a7a\u95f4\u7ef4\u5ea6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548c\u5168\u5c40\u9891\u57df\u8868\u793a\u751f\u6210\u7a7a\u95f4\u63d0\u793a\uff0c\u4ee5\u7a81\u51fa\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u6a21\u5f0f\u3002\u6784\u5efa\u4e86\u4e24\u4e2a\u7a7a\u95f4\u63d0\u793a\u6c60\uff0c\u5b9e\u73b0\u52a8\u6001\u63d0\u793a\u9009\u62e9\uff0c\u4ee5\u540c\u65f6\u4fdd\u6301\u73b0\u6709\u77e5\u8bc6\u5e76\u6709\u6548\u5b66\u4e60\u65b0\u4f1a\u8bdd\u3002", "result": "LGSP-Prompt\u5728\u591a\u4e2aFSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u57fa\u7840\u77e5\u8bc6\u4fdd\u7559\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "LGSP-Prompt\u901a\u8fc7\u521b\u65b0\u5730\u5c06\u6c60\u5316\u63d0\u793a\u4ecetoken\u7ef4\u5ea6\u8f6c\u5411\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u6709\u6548\u514b\u670d\u4e86FSCIL\u4e2d\u63d0\u793a\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u77e5\u8bc6\u7684\u6709\u6548\u4fdd\u7559\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u4e3aFSCIL\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684SOTA\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09497", "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "categories": ["cs.CL"], "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax\u662f\u4e00\u4e2a\u534f\u8bae\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316A2A\u901a\u4fe1\u548c\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAI\u7cfb\u7edf\u5728\u590d\u6742\u4f01\u4e1a\u73af\u5883\u4e2d\u534f\u8c03\u6027\u3001\u8bb0\u5fc6\u590d\u7528\u548c\u4efb\u52a1\u5206\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3001\u534f\u4f5c\u6027\u548c\u7ecf\u9a8c\u590d\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u4f01\u4e1a\u73af\u5883\u9700\u8981\u80fd\u5904\u7406\u590d\u6742\u3001\u52a8\u6001\u3001\u591a\u65b9\u9762\u4efb\u52a1\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u4f46\u4f20\u7edf\u7684\u5355\u7528\u9014AI\u7cfb\u7edf\u7f3a\u4e4f\u8db3\u591f\u7684\u534f\u8c03\u3001\u8bb0\u5fc6\u590d\u7528\u548c\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GoalfyMax\uff0c\u4e00\u4e2a\u534f\u8bae\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002\u5b83\u5f15\u5165\u4e86\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u6807\u51c6\u5316Agent-to-Agent\uff08A2A\uff09\u901a\u4fe1\u5c42\uff0c\u901a\u8fc7\u5f02\u6b65\u3001\u7b26\u5408\u534f\u8bae\u7684\u4ea4\u4e92\u5b9e\u73b0\u667a\u80fd\u4f53\u534f\u8c03\u3002\u6b64\u5916\uff0c\u5b83\u6574\u5408\u4e86\u7ecf\u9a8c\u5305\uff08XP\uff09\u67b6\u6784\uff0c\u4e00\u4e2a\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7528\u4e8e\u4fdd\u7559\u4efb\u52a1\u539f\u7406\u548c\u6267\u884c\u8f68\u8ff9\uff0c\u4ee5\u5b9e\u73b0\u7ed3\u6784\u5316\u77e5\u8bc6\u4fdd\u7559\u548c\u6301\u7eed\u5b66\u4e60\u3002\u7cfb\u7edf\u8fd8\u5305\u62ec\u591a\u8f6e\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u5757\u548c\u52a8\u6001\u5b89\u5168\u9a8c\u8bc1\u7b49\u9ad8\u7ea7\u529f\u80fd\u3002", "result": "\u5728\u590d\u6742\u4efb\u52a1\u7f16\u6392\u57fa\u51c6\u6d4b\u8bd5\u548c\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cGoalfyMax\u4e0e\u57fa\u7ebf\u6846\u67b6\u76f8\u6bd4\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3001\u534f\u8c03\u6027\u548c\u7ecf\u9a8c\u590d\u7528\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86GoalfyMax\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u667a\u80fd\u7cfb\u7edf\u53ef\u6269\u5c55\u3001\u9762\u5411\u672a\u6765\u7684\u57fa\u7840\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09884", "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "categories": ["cs.AI"], "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86VerifyBench\uff0c\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u9a8c\u8bc1\u5668\u3002\u8bc4\u4f30\u53d1\u73b0\uff0c\u4e13\u7528\u9a8c\u8bc1\u5668\u51c6\u786e\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u901a\u7528\u6a21\u578b\u5305\u5bb9\u6027\u5f3a\u4f46\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u4e14\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u654f\u611f\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9a8c\u8bc1\u6a21\u578b\u751f\u6210\u54cd\u5e94\u4e0e\u53c2\u8003\u7b54\u6848\u7684\u4e00\u81f4\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u9a8c\u8bc1\u5668\uff08\u89c4\u5219\u57fa\u3001\u6a21\u578b\u57fa\u3001\u901a\u7528LLM\u5224\u65ad\u5668\uff09\u5404\u6709\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u7c7b\u578b\u9a8c\u8bc1\u5668\u5728\u591a\u9886\u57df\u6027\u80fd\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u7684\u53ef\u9760\u53d1\u5c55\u3002", "method": "\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VerifyBench\u2014\u2014\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\u3002\u6784\u5efa\u4e86\u5305\u542b\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u5b664000\u4e2a\u4e13\u5bb6\u7ea7\u95ee\u9898\u7684\u96c6\u5408\uff0c\u6bcf\u4e2a\u95ee\u9898\u914d\u6709\u53c2\u8003\u7b54\u6848\u548c\u591a\u6837\u5316\u54cd\u5e94\uff0c\u5e76\u7531\u591a\u5b66\u79d1\u4e13\u5bb6\u56e2\u961f\u8fdb\u884c\u4e25\u683c\u6807\u6ce8\u3002\u8bbe\u8ba1\u4e86\u56db\u7ef4\u5b9e\u9a8c\u6846\u67b6\uff0c\u5728\u63d0\u53d6\u7b54\u6848\u4e0e\u5b8c\u6574\u54cd\u5e94\u3001\u77ed\u8f93\u51fa\u4e0e\u957f\u8f93\u51fa\u7684\u7ec4\u5408\u6761\u4ef6\u4e0b\uff0c\u5168\u9762\u6bd4\u8f83\u4e13\u7528\u9a8c\u8bc1\u5668\u548c\u901a\u7528LLM\u7684\u6027\u80fd\u8fb9\u754c\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u9a8c\u8bc1\u5668\u7684\u57fa\u672c\u6743\u8861\uff1a\u4e13\u7528\u9a8c\u8bc1\u5668\u5728\u51c6\u786e\u6027\u4e0a\u9886\u5148\uff0c\u4f46\u5728\u53ec\u56de\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff1b\u901a\u7528\u6a21\u578b\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u5305\u5bb9\u6027\uff0c\u4f46\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u4e14\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u9a8c\u8bc1\u5668\u6280\u672f\u74f6\u9888\uff0c\u5373\u51c6\u786e\u6027\u4e0e\u53ec\u56de\u7387\u3001\u5305\u5bb9\u6027\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u53ca\u5bf9\u8f93\u5165\u7ed3\u6784\u7684\u654f\u611f\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9760\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.08912", "pdf": "https://arxiv.org/pdf/2507.08912", "abs": "https://arxiv.org/abs/2507.08912", "authors": ["Tomasz Szandala", "Fatima Ezzeddine", "Natalia Rusin", "Silvia Giordano", "Omran Ayoub"], "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFair-FLIP\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u8f7b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u5c06\u516c\u5e73\u6027\u6307\u6807\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u800c\u51c6\u786e\u7387\u4ec5\u7565\u5fae\u4e0b\u964d0.25%\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08\u7279\u522b\u662f\u6df1\u5ea6\u4f2a\u9020\uff09\u7684\u6076\u610f\u4f7f\u7528\u5bf9\u516c\u4f17\u4fe1\u4efb\u548c\u8206\u8bba\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u5c3d\u7ba1\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u5728\u79cd\u65cf\u548c\u6027\u522b\u7b49\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u4e0a\u8868\u73b0\u51fa\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5e94\u5bf9\u516c\u5e73\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6311\u6218\uff0c\u65e8\u5728\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u79f0\u4e3a\u201c\u516c\u5e73\u5bfc\u5411\u7684\u6700\u7ec8\u5c42\u8f93\u5165\u4f18\u5148\u6392\u5e8f\u201d\uff08Fair-FLIP\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u7ec8\u5c42\u8f93\u5165\uff0c\u4ee5\u51cf\u5c11\u5b50\u7fa4\u4f53\u5dee\u5f02\u3002\u5b83\u4f18\u5148\u5904\u7406\u53d8\u5f02\u6027\u4f4e\u7684\u8f93\u5165\uff0c\u540c\u65f6\u964d\u4f4e\u53d8\u5f02\u6027\u9ad8\u7684\u8f93\u5165\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u57fa\u7ebf\uff08\u65e0\u516c\u5e73\u6027\u504f\u89c1\u7f13\u89e3\uff09\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFair-FLIP\u53ef\u4ee5\u5c06\u516c\u5e73\u6027\u6307\u6807\u63d0\u9ad8\u9ad8\u8fbe30%\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7ebf\u51c6\u786e\u6027\uff0c\u4ec5\u4ea7\u751f0.25%\u7684\u5fae\u4e0d\u8db3\u9053\u7684\u4e0b\u964d\u3002", "conclusion": "Fair-FLIP\u662f\u4e00\u79cd\u6709\u6548\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u516c\u5e73\u6027\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u89e3\u51b3AI\u6a21\u578b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2507.09184", "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u6e90\u4e8eRoPE\u7f16\u7801\u7684\u957f\u671f\u8870\u51cf\u5bfc\u81f4\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\u3002\u4e3a\u6b64\uff0c\u63d0\u51faMCA-LLaVA\uff0c\u4e00\u79cd\u57fa\u4e8e\u66fc\u54c8\u987f\u8ddd\u79bb\u76842D\u591a\u65b9\u5411\u7a7a\u95f4\u8870\u51cf\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u7f13\u89e3\u6b64\u504f\u5dee\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "LVLMs\u4e2d\u7684\u5e7b\u89c9\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5176\u5173\u952e\u56e0\u7d20\u662f\u591a\u6a21\u6001\u7279\u5f81\u672a\u5bf9\u9f50\u3002\u672c\u7814\u7a76\u53d1\u73b0\uff0cRoPE\uff08\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff09\u7684\u957f\u671f\u8870\u51cf\u5bfc\u81f4\u6307\u4ee4\u4ee4\u724c\u5bf9\u56fe\u50cf\u4ee4\u724c\u7684\u611f\u77e5\u4e0d\u5747\uff0c\u504f\u5411\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u53f3\u4e0b\u533a\u57df\u7684\u4ee4\u724c\uff08\u56e0\u4e00\u7ef4\u5e8f\u5217\u8ddd\u79bb\u66f4\u8fd1\uff09\uff0c\u5f62\u6210\u201c\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\u201d\uff0c\u4ece\u800c\u5bfc\u81f4\u56fe\u50cf-\u6307\u4ee4\u4ea4\u4e92\u4e0d\u8db3\u548c\u6b21\u4f18\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u63d0\u51faMCA-LLaVA\u6a21\u578b\uff0c\u5b83\u57fa\u4e8e\u66fc\u54c8\u987f\u8ddd\u79bb\uff0c\u5c06RoPE\u7684\u957f\u671f\u8870\u51cf\u6269\u5c55\u4e3a\u4e8c\u7ef4\u3001\u591a\u65b9\u5411\u7684\u7a7a\u95f4\u8870\u51cf\u3002MCA-LLaVA\u7ed3\u5408\u56fe\u50cf\u4ee4\u724c\u7684\u4e00\u7ef4\u5e8f\u5217\u987a\u5e8f\u548c\u4e8c\u7ef4\u7a7a\u95f4\u4f4d\u7f6e\u8fdb\u884c\u4f4d\u7f6e\u5efa\u6a21\uff0c\u65e8\u5728\u589e\u5f3a\u6307\u4ee4\u5bf9\u4e0d\u540c\u7a7a\u95f4\u4f4d\u7f6e\u56fe\u50cf\u4ee4\u724c\u7684\u5747\u5300\u611f\u77e5\u3002", "result": "MCA-LLaVA\u5728\u5404\u79cd\u5e7b\u89c9\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u7f13\u89e3\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\uff0cMCA-LLaVA\u6210\u529f\u6539\u5584\u4e86LVLMs\u4e2d\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u65b0\u9896\u76842D\u4f4d\u7f6e\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ref-Long\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b(LCLMs)\u4e0a\u4e0b\u6587\u5f15\u7528\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5148\u8fdb\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b(LCLMs)\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u2014\u2014\u4e00\u9879\u8981\u6c42LCLMs\u5c06\u611f\u5174\u8da3\u9879\u5f52\u56e0\u4e8e\u957f\u4e0a\u4e0b\u6587\u7279\u5b9a\u90e8\u5206\u7684\u4efb\u52a1\u2014\u2014\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Referencing Evaluation for Long-context Language Models (Ref-Long)\u57fa\u51c6\uff0c\u8981\u6c42LCLMs\u8bc6\u522b\u5f15\u7528\u7279\u5b9a\u5173\u952e\u8bcd\u7684\u6587\u6863\u7d22\u5f15\uff0c\u5f3a\u8c03\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002\u8be5\u57fa\u51c6\u5305\u542b\u5408\u6210\u5230\u771f\u5b9e\u573a\u666f\u7684\u4e09\u4e2a\u5b50\u96c6\u3002\u7814\u7a76\u5bf913\u4e2aLCLMs\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u4e86\u5305\u62ec\u4eba\u5de5\u8bc4\u4f30\u3001\u4efb\u52a1\u683c\u5f0f\u8c03\u6574\u3001\u5fae\u8c03\u548c\u9519\u8bef\u5206\u6790\u5728\u5185\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u5bf913\u4e2aLCLMs\uff08\u5305\u62ecGPT-4o\uff09\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u4eec\u5728\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u660e\u663e\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u7a81\u51fa\u4e86\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u6df1\u5165\u7814\u7a76\u548c\u6539\u8fdb\u7684\u6311\u6218\u3002"}}
{"id": "2507.09955", "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "categories": ["cs.AI"], "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86DeepSeek V3/R1\u6a21\u578b\uff0c\u56e0\u5176\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u7279\u6027\u53d7\u5168\u7403\u5173\u6ce8\u3002\u91cd\u70b9\u63a2\u8ba8\u5176\u521b\u65b0\u7b97\u6cd5\u3001\u5de5\u7a0b\u7a81\u7834\uff0c\u8bc4\u4f30\u5176\u5bf9AI\u7ade\u4e89\u683c\u5c40\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u671b\u5927\u578bAI\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u3002", "motivation": "DeepSeek\u53d1\u5e03\u7684V3\u548cR1\u7cfb\u5217\u6a21\u578b\u4ee5\u5176\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u4f18\u52bf\u5728\u5168\u7403\u8303\u56f4\u5185\u5f15\u8d77\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4fc3\u4f7f\u672c\u7814\u7a76\u6df1\u5165\u5206\u6790\u5176\u6280\u672f\u521b\u65b0\u548c\u5bf9AI\u9886\u57df\u7684\u5f71\u54cd\u3002", "method": "\u8bba\u6587\u9996\u5148\u56de\u987e\u5927\u578bAI\u6a21\u578b\u6f14\u8fdb\uff0c\u5305\u62ec\u4e3b\u6d41LLM\u8303\u5f0f\u548cDeepSeek\u8303\u5f0f\u3002\u63a5\u7740\uff0c\u8be6\u7ec6\u9610\u8ff0DeepSeek\u5f15\u5165\u7684\u65b0\u7b97\u6cd5\uff08\u5982MLA\u3001MoE\u3001MTP\u3001GRPO\uff09\u53ca\u5176\u5728LLM\u6269\u5c55\u3001\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u7cfb\u7edf\u7ea7\u4f18\u5316\u65b9\u9762\u7684\u5de5\u7a0b\u7a81\u7834\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u4e0e\u4e3b\u6d41LLM\u7684\u6bd4\u8f83\uff0c\u5206\u6790DeepSeek\u6a21\u578b\u5bf9AI\u7ade\u4e89\u683c\u5c40\u7684\u5f71\u54cd\u3002", "result": "\u672c\u6587\u5206\u6790\u4e86DeepSeek\u6a21\u578b\u56e0\u5176\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u7279\u6027\u800c\u83b7\u5f97\u7684\u5168\u7403\u5173\u6ce8\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u3001MoE\u3001\u591a\u4ee4\u724c\u9884\u6d4b\u7b49\u521b\u65b0\u7b97\u6cd5\u4ee5\u53caLLM\u6269\u5c55\u3001\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u7cfb\u7edf\u4f18\u5316\u7b49\u5de5\u7a0b\u65b9\u9762\u7684\u663e\u8457\u7a81\u7834\uff0c\u9610\u660e\u4e86\u8fd9\u4e9b\u521b\u65b0\u5982\u4f55\u4f7f\u5176\u5728AI\u7ade\u4e89\u683c\u5c40\u4e2d\u8131\u9896\u800c\u51fa\uff0c\u5e76\u4e0e\u4e3b\u6d41LLM\u5f62\u6210\u5bf9\u6bd4\u3002", "conclusion": "\u8bba\u6587\u4eceDeepSeek\u7684\u521b\u65b0\u4e2d\u6c72\u53d6\u4e86\u89c1\u89e3\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u5927\u578bAI\u6a21\u578b\u5728\u6570\u636e\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u7b49\u65b9\u9762\u7684\u6280\u672f\u4e0e\u5de5\u7a0b\u53d1\u5c55\u8d8b\u52bf\u3002"}}
{"id": "2507.08913", "pdf": "https://arxiv.org/pdf/2507.08913", "abs": "https://arxiv.org/abs/2507.08913", "authors": ["Qi He", "Peiran Yu", "Ziyi Chen", "Heng Huang"], "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Shuffling-type gradient methods are favored in practice for their simplicity\nand rapid empirical performance. Despite extensive development of convergence\nguarantees under various assumptions in recent years, most require the\nLipschitz smoothness condition, which is often not met in common machine\nlearning models. We highlight this issue with specific counterexamples. To\naddress this gap, we revisit the convergence rates of shuffling-type gradient\nmethods without assuming Lipschitz smoothness. Using our stepsize strategy, the\nshuffling-type gradient algorithm not only converges under weaker assumptions\nbut also match the current best-known convergence rates, thereby broadening its\napplicability. We prove the convergence rates for nonconvex, strongly convex,\nand non-strongly convex cases, each under both random reshuffling and arbitrary\nshuffling schemes, under a general bounded variance condition. Numerical\nexperiments further validate the performance of our shuffling-type gradient\nalgorithm, underscoring its practical efficacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u4f7f\u6d17\u724c\u5f0f\u68af\u5ea6\u65b9\u6cd5\u5728\u4e0d\u5047\u8bbeLipschitz\u5e73\u6ed1\u7684\u6761\u4ef6\u4e0b\u6536\u655b\uff0c\u5e76\u8fbe\u5230\u5f53\u524d\u6700\u4f73\u6536\u655b\u901f\u7387\uff0c\u62d3\u5bbd\u4e86\u5176\u9002\u7528\u6027\u3002", "motivation": "\u6d17\u724c\u5f0f\u68af\u5ea6\u65b9\u6cd5\u56e0\u5176\u7b80\u6d01\u548c\u5feb\u901f\u6027\u80fd\u5728\u5b9e\u8df5\u4e2d\u5e7f\u53d7\u6b22\u8fce\uff0c\u4f46\u5176\u6536\u655b\u6027\u8bc1\u660e\u901a\u5e38\u9700\u8981Lipschitz\u5e73\u6ed1\u6761\u4ef6\uff0c\u8fd9\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u96be\u4ee5\u6ee1\u8db3\uff0c\u5bfc\u81f4\u7406\u8bba\u4e0e\u5b9e\u8df5\u8131\u8282\u3002", "method": "\u7814\u7a76\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u5728\u4e0d\u5047\u8bbeLipschitz\u5e73\u6ed1\u6761\u4ef6\u4e0b\u7684\u6d17\u724c\u5f0f\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u573a\u666f\u4e0b\uff0c\u4ee5\u53ca\u968f\u673a\u6d17\u724c\u548c\u4efb\u610f\u6d17\u724c\u65b9\u6848\u4e0b\uff0c\u4e8e\u4e00\u822c\u6709\u754c\u65b9\u5dee\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u6536\u655b\u901f\u7387\u3002\u5e76\u8f85\u4ee5\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6d17\u724c\u5f0f\u68af\u5ea6\u7b97\u6cd5\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\uff08\u65e0\u9700Lipschitz\u5e73\u6ed1\uff09\u5b9e\u73b0\u4e86\u6536\u655b\uff0c\u5e76\u80fd\u5339\u914d\u5f53\u524d\u5df2\u77e5\u7684\u6700\u4f73\u6536\u655b\u901f\u7387\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u51f8\u6027\uff08\u975e\u51f8\u3001\u5f3a\u51f8\u3001\u975e\u5f3a\u51f8\uff09\u548c\u6d17\u724c\u65b9\u6848\u4e0b\u7684\u6536\u655b\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u6d17\u724c\u5f0f\u68af\u5ea6\u65b9\u6cd5\u5bf9Lipschitz\u5e73\u6ed1\u6761\u4ef6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7406\u8bba\u4e0a\u4e25\u8c28\u4e14\u5b9e\u8df5\u4e2d\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d17\u724c\u5f0f\u68af\u5ea6\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09200", "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "categories": ["cs.CV"], "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTHYME\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u4f9d\u8d56\u6355\u83b7\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6AeroEye-v1.0\u548c\u73b0\u6709\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8868\u793a\u7834\u788e\uff0c\u96be\u4ee5\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u5168\u9762\u7406\u89e3\u3002\u8fd9\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u65f6\u95f4\u5206\u5c42\u5faa\u73af\u573a\u666f\u56fe\u201d(THYME)\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u534f\u540c\u6574\u5408\u4e86\u5206\u5c42\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u4ee5\u6709\u6548\u5efa\u6a21\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5e76\u5f3a\u5236\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u79cd\u4ea4\u4e92\u7c7b\u578b\u7684\u65b0\u578b\u822a\u7a7a\u89c6\u9891\u6570\u636e\u96c6AeroEye-v1.0\u3002", "result": "\u5728ASPIRe\u548cAeroEye-v1.0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684THYME\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5730\u9762\u89c6\u89d2\u548c\u822a\u7a7a\u573a\u666f\u4e2d\u5747\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "THYME\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u6574\u5408\u5206\u5c42\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u4e2d\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u8fde\u8d2f\u7684\u573a\u666f\u56fe\uff0c\u5e76\u88ab\u8bc1\u660e\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H\u00e4mmerl", "Vil\u00e9m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u63d0\u793a\u9519\u8bef\uff08\u5305\u62ec\u4eba\u7c7b\u53ef\u4fe1\u548c\u5408\u6210\u9519\u8bef\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u8bc4\u4f30\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u793a\u8d28\u91cf\u5bf9\u7ffb\u8bd1\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\uff08\u5982\u5b57\u7b26\u7ea7\uff09\u5f71\u54cd\u4e0d\u540c\uff0c\u4e14\u8d28\u91cf\u4e0b\u964d\u4e3b\u8981\u5f71\u54cd\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46LLMs\u5728\u6781\u7aef\u566a\u58f0\u4e0b\u4ecd\u80fd\u7ffb\u8bd1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u63d0\u793a\u4e2d\u7684\u9519\u8bef\u548c\u6270\u52a8\u654f\u611f\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7a76\u7528\u6237\u63d0\u793a\u4e2d\u7684\u9519\u8bef\u5982\u4f55\u5f71\u54cdLLM\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5728\u7528\u6237\u63d0\u793a\u4e2d\u5f15\u5165\u4eba\u7c7b\u53ef\u4fe1\u548c\u5408\u6210\u9519\u8bef\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u91c7\u7528\u5b9a\u91cf\u5206\u6790\u548c\u5b9a\u6027\u6d1e\u5bdf\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u6a21\u578b\u5bf9\u65e5\u76ca\u589e\u52a0\u7684\u63d0\u793a\u566a\u58f0\u7684\u54cd\u5e94\u3002", "result": "\u63d0\u793a\u8d28\u91cf\u5bf9\u7ffb\u8bd1\u6027\u80fd\u6709\u5f3a\u70c8\u5f71\u54cd\uff1b\u5728\u9519\u8bef\u8f83\u591a\u65f6\uff0c\u5373\u4f7f\u662f\u597d\u7684\u63d0\u793a\u4e5f\u53ef\u80fd\u8868\u73b0\u4e0d\u5982\u65e0\u9519\u8bef\u7684\u6781\u7b80\u6216\u8f83\u5dee\u63d0\u793a\u3002\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u5176\u4e2d\u5b57\u7b26\u7ea7\u548c\u7ec4\u5408\u566a\u58f0\u6bd4\u77ed\u8bed\u6270\u52a8\u66f4\u4e25\u91cd\u5730\u964d\u4f4e\u6027\u80fd\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u63d0\u793a\u8d28\u91cf\u4e0b\u964d\u4e3b\u8981\u5bfc\u81f4\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u53d8\u5dee\uff0c\u800c\u975e\u76f4\u63a5\u5f71\u54cd\u7ffb\u8bd1\u8d28\u91cf\u672c\u8eab\u3002\u6b64\u5916\uff0cLLM\u5728\u4eba\u7c7b\u96be\u4ee5\u8fa8\u8ba4\u7684\u538b\u5012\u6027\u968f\u673a\u566a\u58f0\u573a\u666f\u4e0b\u4ecd\u80fd\u8fdb\u884c\u7ffb\u8bd1\u3002", "conclusion": "LLM\u5bf9\u63d0\u793a\u9519\u8bef\u7684\u654f\u611f\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u4e0b\u964d\uff0c\u800c\u975e\u76f4\u63a5\u7ffb\u8bd1\u8d28\u91cf\u53d7\u635f\u3002\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u5bf9\u6027\u80fd\u5f71\u54cd\u7a0b\u5ea6\u5404\u5f02\uff0c\u4e14LLM\u5728\u6781\u7aef\u566a\u58f0\u4e0b\u4ecd\u5c55\u73b0\u51fa\u8d85\u4e4e\u4eba\u7c7b\u9884\u671f\u7684\u7ffb\u8bd1\u80fd\u529b\uff0c\u51f8\u663e\u4e86\u63d0\u793a\u8d28\u91cf\u5728LLM\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.09989", "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08956", "pdf": "https://arxiv.org/pdf/2507.08956", "abs": "https://arxiv.org/abs/2507.08956", "authors": ["Zhenghan Fang", "Mateo D\u00edaz", "Sam Buchanan", "Jeremias Sulam"], "title": "Beyond Scores: Proximal Diffusion Models", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Diffusion models have quickly become some of the most popular and powerful\ngenerative models for high-dimensional data. The key insight that enabled their\ndevelopment was the realization that access to the score -- the gradient of the\nlog-density at different noise levels -- allows for sampling from data\ndistributions by solving a reverse-time stochastic differential equation (SDE)\nvia forward discretization, and that popular denoisers allow for unbiased\nestimators of this score. In this paper, we demonstrate that an alternative,\nbackward discretization of these SDEs, using proximal maps in place of the\nscore, leads to theoretical and practical benefits. We leverage recent results\nin proximal matching to learn proximal operators of the log-density and, with\nthem, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that\n$\\widetilde{O}(d/\\sqrt{\\varepsilon})$ steps suffice for the resulting\ndiscretization to generate an $\\varepsilon$-accurate distribution w.r.t. the KL\ndivergence. Empirically, we show that two variants of ProxDM achieve\nsignificantly faster convergence within just a few sampling steps compared to\nconventional score-matching methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09207", "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "\u4e00\u79cd\u4ece\u8868\u9762\u6ce2\u89c6\u9891\u63a8\u65ad\u6750\u6599\u539a\u5ea6\u548c\u521a\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u8868\u9762\u6ce2\u4f20\u64ad\u8574\u542b\u6750\u6599\u5185\u90e8\u7269\u7406\u6027\u8d28\u4fe1\u606f\uff0c\u7814\u7a76\u65e8\u5728\u4ec5\u901a\u8fc7\u8868\u9762\u6ce2\u89c6\u9891\u63a8\u65ad\u7ed3\u6784\u539a\u5ea6\u548c\u521a\u5ea6\u3002", "method": "\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u8272\u6563\u5173\u7cfb\uff0c\u7136\u540e\u89e3\u51b3\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u627e\u5230\u6700\u4f73\u62df\u5408\u7684\u539a\u5ea6\u548c\u521a\u5ea6\u53c2\u6570\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u5f97\u5230\u9a8c\u8bc1\uff0c\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u503c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u533b\u7597\u7ec4\u7ec7\u5c5e\u6027\u7684\u5c45\u5bb6\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u3002"}}
{"id": "2507.09536", "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "categories": ["cs.CL"], "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u5e94\u7528\u4e8e\u5c1a\u4e0d\u652f\u6301\u7684\u8bed\u8a00\uff0c\u4ee5\u767d\u4fc4\u7f57\u65af\u8bed\u4e3a\u4f8b\uff0c\u6784\u5efa\u4e86\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5e76\u53d1\u73b0\u9002\u5e94\u6a21\u578b\u6240\u9700\u6570\u636e\u91cf\u5c0f\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u5b9a\u4e49\u5efa\u6a21\u6709\u52a9\u4e8e\u8bcd\u5178\u7f16\u7e82\u8005\u8bb0\u5f55\u66f4\u591a\u65b9\u8a00\u548c\u8bed\u8a00\uff0c\u4f46\u5982\u4f55\u5229\u7528\u73b0\u6709\u6a21\u578b\u5904\u7406\u5c1a\u672a\u652f\u6301\u7684\u8bed\u8a00\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5c06\u73b0\u6709\u5b9a\u4e49\u5efa\u6a21\u6a21\u578b\u5e94\u7528\u4e8e\u767d\u4fc4\u7f57\u65af\u8bed\u3002\u4e3a\u6b64\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b43,150\u6761\u5b9a\u4e49\u7684\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5e94\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5728\u6355\u6349\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u53ef\u4ee5\u6709\u6548\u5730\u9002\u5e94\u65b0\u8bed\u8a00\uff0c\u4e14\u5bf9\u6570\u636e\u91cf\u8981\u6c42\u4e0d\u9ad8\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u66f4\u5b8c\u5584\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4ee5\u5168\u9762\u8861\u91cf\u5176\u6027\u80fd\u3002"}}
{"id": "2507.10000", "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePromise Theory\u8bed\u4e49\u65f6\u7a7a\u6a21\u578b\u7684\u4f4e\u6210\u672c\u3001\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u591a\u5c3a\u5ea6\u5f02\u5e38\u548c\u8fc7\u7a0b\u8fde\u8d2f\u6027\uff0c\u8bc6\u522b\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u610f\u5411\u6027\uff0c\u5e76\u5c06\u610f\u56fe\u5185\u5bb9\u4e0e\u73af\u5883\u4e0a\u4e0b\u6587\u533a\u5206\u5f00\u6765\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6216\u63a8\u7406\u3002", "motivation": "\u81eaSearle\u5bf9\u54f2\u5b66\u9886\u57df\u4e2d\u610f\u56fe\u548c\u610f\u5411\u6027\u8fdb\u884c\u89e3\u6784\u4ee5\u6765\uff0c\u79d1\u5b66\u6280\u672f\u9886\u57df\u5bf9\u201c\u610f\u56fe\u201d\u7684\u5b9e\u9645\u610f\u4e49\u5173\u6ce8\u751a\u5c11\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8ePromise Theory\u7684\u8bed\u4e49\u65f6\u7a7a\u6a21\u578b\uff08Semantic Spacetime\uff09\uff0c\u5c06\u5176\u4f5c\u4e3a\u6709\u6548\u7684\u5fae\u578b\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u8fc7\u7a0b\u8fde\u8d2f\u6027\u8bc6\u522b\u6587\u672c\u4e3b\u9898\u548c\u6982\u5ff5\uff0c\u65e0\u9700\u7279\u5b9a\u8bed\u8a00\u77e5\u8bc6\u3002\u901a\u8fc7\u5bfb\u627e\u5f02\u5e38\u7684\u591a\u5c3a\u5ea6\u5f02\u5e38\u5e76\u8bc4\u4f30\u5176\u5f62\u6210\u6240\u9700\u7684\u5de5\u4f5c\u91cf\uff0c\u6765\u8bc4\u4f30\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u201c\u610f\u5411\u6027\u201d\u7a0b\u5ea6\u3002\u5229\u7528\u5c3a\u5ea6\u5206\u79bb\u548c\u65f6\u7a7a\u8fde\u8d2f\u6027\u4f5c\u4e3a\u5ea6\u91cf\uff0c\u5c06\u5185\u5bb9\u5206\u4e3a\u201c\u610f\u56fe\u5185\u5bb9\u201d\u548c\u201c\u73af\u5883\u4e0a\u4e0b\u6587\u201d\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u6f5c\u5728\u610f\u5411\u6027\u7684\u57fa\u672c\u4f46\u5b9e\u7528\u7684\u89e3\u91ca\uff0c\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\uff0c\u4e14\u4e0d\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6216\u63a8\u7406\u80fd\u529b\u3002\u8be5\u8fc7\u7a0b\u65e0\u9700\u5927\u89c4\u6a21\u4eba\u5de5\u6982\u7387\u6279\u5904\u7406\uff0c\u751a\u81f3\u9002\u7528\u4e8e\u57fa\u672c\u751f\u7269\u4f53\u3002\u6982\u5ff5\u5f62\u6210\u7684\u6c34\u5e73\u53d6\u51b3\u4e8e\u4ee3\u7406\u7684\u8bb0\u5fc6\u5bb9\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u610f\u5411\u6027\uff0c\u6709\u6548\u5730\u533a\u5206\u4e86\u610f\u56fe\u4e0e\u4e0a\u4e0b\u6587\uff0c\u586b\u8865\u4e86\u79d1\u5b66\u6280\u672f\u9886\u57df\u5bf9\u610f\u56fe\u5b9e\u9645\u610f\u4e49\u5173\u6ce8\u4e0d\u8db3\u7684\u7a7a\u767d\u3002\u5176\u4f4e\u6210\u672c\u548c\u666e\u9002\u6027\u4f7f\u5176\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08959", "pdf": "https://arxiv.org/pdf/2507.08959", "abs": "https://arxiv.org/abs/2507.08959", "authors": ["Xiang Li", "Xinyu Wang", "Yifan Lin"], "title": "Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign", "categories": ["cs.LG"], "comment": null, "summary": "In order to improve the accuracy of cross-platform advertisement\nrecommendation, a graph neural network (GNN)- based advertisement\nrecommendation method is analyzed. Through multi-dimensional modeling, user\nbehavior data (e.g., click frequency, active duration) reveal temporal patterns\nof interest evolution, ad content (e.g., type, tag, duration) influences\nsemantic preferences, and platform features (e.g., device type, usage context)\nshape the environment where interest transitions occur. These factors jointly\nenable the GNN to capture the latent pathways of user interest migration across\nplatforms. The experimental results are based on the datasets of three\nplatforms, and Platform B reaches 0.937 in AUC value, which is the best\nperformance. Platform A and Platform C showed a slight decrease in precision\nand recall with uneven distribution of ad labels. By adjusting the\nhyperparameters such as learning rate, batch size and embedding dimension, the\nadaptability and robustness of the model in heterogeneous data are further\nimproved.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5efa\u6a21\u6355\u6349\u7528\u6237\u5174\u8da3\u8fc1\u79fb\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\uff0c\u5c24\u5176\u5728\u4e00\u4e2a\u5e73\u53f0\u8fbe\u52300.937\u7684AUC\u503c\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002", "method": "\u5206\u6790\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u7ef4\u5ea6\u5efa\u6a21\uff0c\u6574\u5408\u4e86\u7528\u6237\u884c\u4e3a\u6570\u636e\uff08\u5982\u70b9\u51fb\u9891\u7387\u3001\u6d3b\u8dc3\u65f6\u957f\u4ee5\u63ed\u793a\u5174\u8da3\u6f14\u53d8\u7684\u65f6\u95f4\u6a21\u5f0f\uff09\u3001\u5e7f\u544a\u5185\u5bb9\uff08\u5982\u7c7b\u578b\u3001\u6807\u7b7e\u3001\u65f6\u957f\u4ee5\u5f71\u54cd\u8bed\u4e49\u504f\u597d\uff09\u548c\u5e73\u53f0\u7279\u5f81\uff08\u5982\u8bbe\u5907\u7c7b\u578b\u3001\u4f7f\u7528\u4e0a\u4e0b\u6587\u4ee5\u5851\u9020\u5174\u8da3\u8fc7\u6e21\u73af\u5883\uff09\u3002\u8fd9\u4e9b\u56e0\u7d20\u5171\u540c\u4f7fGNN\u80fd\u591f\u6355\u6349\u7528\u6237\u8de8\u5e73\u53f0\u7684\u5174\u8da3\u8fc1\u79fb\u6f5c\u5728\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8e\u4e09\u4e2a\u5e73\u53f0\u7684\u6570\u636e\u96c6\u3002\u5e73\u53f0B\u7684AUC\u503c\u8fbe\u52300.937\uff0c\u8868\u73b0\u6700\u4f73\u3002\u5e73\u53f0A\u548c\u5e73\u53f0C\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7565\u6709\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u5e7f\u544a\u6807\u7b7e\u5206\u5e03\u4e0d\u5747\u5300\u3002\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u3001\u6279\u5927\u5c0f\u548c\u5d4c\u5165\u7ef4\u5ea6\u7b49\u8d85\u53c2\u6570\uff0c\u6a21\u578b\u5728\u5f02\u6784\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u5f97\u5230\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u8be5\u57fa\u4e8eGNN\u7684\u65b9\u6cd5\u901a\u8fc7\u591a\u7ef4\u5ea6\u6570\u636e\u5efa\u6a21\u548c\u6355\u6349\u7528\u6237\u5174\u8da3\u8fc1\u79fb\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5728\u5f02\u6784\u6570\u636e\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09209", "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "\u9488\u5bf9\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(MedVLM)\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51faExpert-CFG\u6846\u67b6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e13\u5bb6\u6307\u5bfc\u548c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347MedVLM\u7684\u53ef\u9760\u6027\u548c\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7684\u5bf9\u9f50\u5ea6\uff0c\u4e14\u5728\u5c0f\u53c2\u6570\u91cf\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(MedVLM)\u5b58\u5728\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u6216\u672a\u7ecf\u8bc1\u5b9e\u7684\u56de\u5e94\uff0c\u8fd9\u5728\u533b\u7597\u5e94\u7528\u4e2d\u5177\u6709\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u589e\u5f3aMedVLM\u6027\u80fd\u7684\u65b9\u6cd5\uff08\u5982\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u3001\u6570\u636e\u5fae\u8c03\uff09\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u672a\u80fd\u5145\u5206\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExpert-Controlled Classifier-Free Guidance (Expert-CFG)\u7684\u4e13\u5bb6\u5728\u73af\u6846\u67b6\uff0c\u65e8\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4f7fMedVLM\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7b56\u7565\u8bc6\u522b\u4e0d\u53ef\u9760\u8f93\u51fa\uff0c\u7136\u540e\u68c0\u7d22\u76f8\u5173\u53c2\u8003\u6587\u732e\u4ee5\u534f\u52a9\u4e13\u5bb6\u7a81\u51fa\u5173\u952e\u672f\u8bed\uff0c\u6700\u540e\u5e94\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u6765\u7cbe\u70bcMedVLM\u7684token\u5d4c\u5165\uff0c\u786e\u4fdd\u8c03\u6574\u540e\u7684\u8f93\u51fa\u6b63\u786e\u5e76\u4e0e\u4e13\u5bb6\u7a81\u51fa\u90e8\u5206\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684Expert-CFG\u6a21\u578b\uff084.2B\u53c2\u6570\uff0c\u6709\u9650\u4e13\u5bb6\u6807\u6ce8\uff09\u7684\u6027\u80fd\u4f18\u4e8e\u53c2\u6570\u91cf\u4e3a13B\u7684\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u90e8\u7f72Expert-CFG\u7cfb\u7edf\u4ee5\u7528\u4e8e\u4e34\u5e8a\u4f7f\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09601", "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NMIXX\uff0c\u4e00\u5957\u7528\u4e8e\u91d1\u878d\u9886\u57df\u7684\u8de8\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\uff0c\u4ee5\u53caKorFinSTS\uff0c\u4e00\u4e2a\u97e9\u8bed\u91d1\u878d\u8bed\u4e49\u76f8\u4f3c\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u901a\u7528\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u91d1\u878d\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u901a\u7528\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4e13\u4e1a\u7684\u91d1\u878d\u8bed\u4e49\uff0c\u7279\u522b\u662f\u5728\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u8fd9\u5f52\u56e0\u4e8e\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3001\u65f6\u6001\u610f\u4e49\u53d8\u5316\u548c\u53cc\u8bed\u8bcd\u6c47\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\u3002\u73b0\u6709\u5de5\u5177\u5728\u91d1\u878d\u9886\u57df\u9002\u5e94\u6027\u591a\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86NMIXX\uff0c\u901a\u8fc718.8K\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\uff08\u9886\u57df\u5185\u91ca\u4e49\u3001\u8bed\u4e49\u6f02\u79fb\u7c7b\u578b\u786c\u8d1f\u4f8b\u3001\u7cbe\u786e\u97e9\u82f1\u7ffb\u8bd1\uff09\u8fdb\u884c\u5fae\u8c03\u7684\u8de8\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u3002\u53d1\u5e03\u4e86KorFinSTS\uff0c\u4e00\u4e2a\u5305\u542b1,921\u5bf9\u97e9\u8bed\u91d1\u878dSTS\u6570\u636e\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u65b0\u95fb\u3001\u62ab\u9732\u3001\u7814\u7a76\u62a5\u544a\u548c\u6cd5\u89c4\u3002\u5c06NMIXX\u4e0e\u4e03\u4e2a\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e86\u97e9\u8bed\u8bcd\u5143\u8986\u76d6\u7387\u5bf9\u6a21\u578b\u9002\u5e94\u6027\u7684\u5f71\u54cd\u3002", "result": "NMIXX\u7684\u591a\u8bed\u8a00bge-m3\u53d8\u4f53\u5728\u82f1\u8bedFinSTS\u4e0aSpearman's rho\u63d0\u5347\u4e86+0.10\uff0c\u5728KorFinSTS\u4e0a\u63d0\u5347\u4e86+0.22\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u9884\u9002\u5e94\u68c0\u67e5\u70b9\u5e76\u8d85\u8d8a\u4e86\u5176\u4ed6\u6a21\u578b\u3002\u5206\u6790\u663e\u793a\uff0c\u97e9\u8bed\u8bcd\u5143\u8986\u76d6\u7387\u66f4\u4e30\u5bcc\u7684\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u9002\u5e94\u3002\u5b58\u5728\u901a\u7528STS\u6027\u80fd\u7684\u8f7b\u5fae\u6743\u8861\u3002", "conclusion": "NMIXX\u6a21\u578b\u548cKorFinSTS\u57fa\u51c6\u4e3a\u91d1\u878d\u9886\u57df\u4e2d\u9886\u57df\u9002\u5e94\u7684\u591a\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5c24\u5176\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u81f3\u5173\u91cd\u8981\u3002\u5f3a\u8c03\u4e86\u5206\u8bcd\u5668\u8bbe\u8ba1\u5728\u4f4e\u8d44\u6e90\u3001\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.10007", "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "AI": {"tldr": "\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u6613\u53d7\u4e2d\u95f4\u6b65\u9aa4\u9519\u8bef\u5f71\u54cd\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u5728\u771f\u5b9e\u6027\u7f16\u7801\uff08\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\uff09\u8bad\u7ec3\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u7ed3\u5408\u675f\u641c\u7d22\u6821\u51c6CoT\u63a8\u7406\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u7684\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u4e2d\u95f4\u6b65\u9aa4\u9519\u8bef\u7684\u7d2f\u79ef\u5e38\u635f\u5bb3\u5176\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\u6765\u6821\u51c6CoT\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u5176\u6838\u5fc3\u662f\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u771f\u5b9e\u6027\u7f16\u7801\u3002\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\u80fd\u53ef\u9760\u5730\u53cd\u6620CoT\u63a8\u7406\u6b65\u9aa4\u7684\u771f\u5b9e\u6027\u3002\u57fa\u4e8e\u6b64\u6d1e\u5bdf\uff0c\u4f5c\u8005\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u5229\u7528\u8fd9\u4e9b\u5bf9\u771f\u5b9e\u6027\u654f\u611f\u7684\u6fc0\u6d3b\u6765\u8bc4\u4f30\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u675f\u641c\u7d22\uff08beam search\uff09\u52a8\u6001\u9009\u62e9\u6700\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7b26\u53f7\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u5355\u6a21\u6001\u8fd8\u662f\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0c\u5747\u663e\u8457\u4f18\u4e8eFew-Shot CoT\u3001Self-Consistency\u548cSelf-Evaluation Guided Beam Search\u7b49\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e0a\u5f97\u5230\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u5176\u5bf9\u4e13\u4e1a\u63a8\u7406\u6a21\u578b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aCoT\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9896\u7684\u53ef\u9760\u6027\u6539\u8fdb\u9014\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08965", "pdf": "https://arxiv.org/pdf/2507.08965", "abs": "https://arxiv.org/abs/2507.08965", "authors": ["Kevin Rojas", "Ye He", "Chieh-Hsin Lai", "Yuta Takida", "Yuki Mitsufuji", "Molei Tao"], "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).", "AI": {"tldr": "\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc(CFG)\uff0c\u63ed\u793a\u4e86\u65e9\u671f\u9ad8\u5f15\u5bfc\u7684\u5371\u5bb3\u548c\u73b0\u6709CFG\u7684\u4e0d\u5e73\u8861\u8fc7\u6e21\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc(CFG)\u5e7f\u6cdb\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u548c\u63d0\u9ad8\u8fde\u7eed\u6269\u6563\u6a21\u578b\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u5df2\u6269\u5c55\u5230\u79bb\u6563\u6269\u6563\uff0c\u4f46\u5176\u5728\u79bb\u6563\u8bbe\u7f6e\u4e0b\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5f15\u5bfc\u8c03\u5ea6\u7684\u4f5c\u7528\uff0c\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u3002\u6b64\u5916\uff0c\u73b0\u6709CFG\u5b9e\u73b0\u5b58\u5728\u7f3a\u9677\uff0c\u53ef\u80fd\u5bfc\u81f4\u6837\u672c\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a9\u853d\u79bb\u6563\u6269\u6563\u4e2d\u7684CFG\uff0c\u91cd\u70b9\u5173\u6ce8\u5f15\u5bfc\u8c03\u5ea6\u7684\u4f5c\u7528\u3002\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u5e73\u6ed1\u6570\u636e\u5206\u5e03\u4e0e\u521d\u59cb\u5206\u5e03\u95f4\u7684\u4f20\u8f93\u6765\u6539\u5584\u6837\u672c\u8d28\u91cf\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e00\u884c\u4ee3\u7801\u4fee\u6539\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u91c7\u6837\u65e9\u671f\uff08\u8f93\u5165\u9ad8\u5ea6\u63a9\u853d\u65f6\uff09\u9ad8\u5f15\u5bfc\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u800c\u540e\u671f\u5f15\u5bfc\u5f71\u54cd\u66f4\u5927\u3002\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u73b0\u6709CFG\u5b9e\u73b0\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5e73\u8861\u8fc7\u6e21\uff08\u5982\u65e9\u671f\u8fc7\u5feb\u53bb\u63a9\u853d\uff09\uff0c\u4ece\u800c\u964d\u4f4e\u6837\u672c\u8d28\u91cf\u3002\u63d0\u51fa\u7684\u65b0\u673a\u5236\u5728ImageNet\u548cQM9\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2dCFG\u7684\u5f15\u5bfc\u8c03\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u666e\u904d\u9002\u7528\u7684CFG\u6539\u8fdb\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u6837\u672c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.09214", "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "categories": ["cs.CV"], "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faS3AD\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u80262D/3D\u8bad\u7ec3\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5f02\u5e38\u8bc4\u5206\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d3D\u68c0\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5f02\u5e38\u76ee\u6807\u8fc7\u6ee4\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u591a\u79cd\u65b0\u7c7b\u522b\u7684KITTI-AR\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u67093D\u68c0\u6d4b\u6a21\u578b\u5728\u5f00\u653e\u9053\u8def\u4e0a\u9762\u5bf9\u7a00\u6709\u5f02\u5e38\u7c7b\u522b\u65f6\uff0c\u6613\u53d1\u751f\u8bef\u68c0\u6216\u6f0f\u68c0\uff0c\u4e14\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e8e2D/3D\u8026\u5408\u8bad\u7ec3\u548c\u8bad\u7ec3\u6837\u672c\u5c3a\u5ea6\u5206\u5e03\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u589e\u5f3a\u6a21\u578b\u5bf9\u4efb\u610f\u5f62\u72b6\u76ee\u6807\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5f02\u5e38\u8fc7\u6ee4\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51faS3AD\uff08Stereo-based 3D Anomaly object Detection\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u89e3\u8026\u4e863D\u548c2D\u7684\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u5347\u4efb\u610f3D\u524d\u666f\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u57fa\u4e8e\u524d\u666f\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u63d0\u51fa\u4e86\u76ee\u6807\u7ea7\u5f02\u5e38\u8bc4\u5206\u7b97\u6cd5\u3002\u4e3a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\uff0c\u5229\u75283D\u6e32\u67d3\u65b9\u6cd5\u5408\u6210\u4e86\u4e24\u4e2a\u589e\u5f3a\u73b0\u5b9e\u53cc\u76ee3D\u68c0\u6d4b\u6570\u636e\u96c6KITTI-AR\uff0c\u5305\u542b97\u4e2a\u65b0\u7c7b\u522b\uff08\u51716k\u5bf9\u7acb\u4f53\u56fe\u50cf\uff09\uff0c\u5176\u4e2dKITTI-AR-ExD\u7528\u4e8e\u89e3\u51b3\u6837\u672c\u7a00\u758f\u95ee\u9898\uff0cKITTI-AR-OoD\u7528\u4e8e\u6a21\u62df\u96f6\u6837\u672c\u573a\u666f\u5e76\u8bc4\u4f303D\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u63d0\u51fa\u4e86S3AD\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u80262D/3D\u8bad\u7ec3\u548c\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5f02\u5e38\u8bc4\u5206\u673a\u5236\uff0c\u589e\u5f3a\u4e863D\u68c0\u6d4b\u6a21\u578b\u5bf9\u4efb\u610f\u5f62\u72b6\u76ee\u6807\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5f02\u5e38\u8fc7\u6ee4\u80fd\u529b\u3002\u6784\u5efa\u4e86KITTI-AR\u589e\u5f3a\u73b0\u5b9e\u6570\u636e\u96c6\uff0c\u5305\u542bExD\u548cOoD\u5b50\u96c6\uff0c\u6709\u6548\u652f\u6301\u4e86\u7b97\u6cd5\u7684\u8bad\u7ec3\u4e0e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u8bc4\u4f30\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "conclusion": "S3AD\u7b97\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5f02\u5e38\u8bc4\u5206\u673a\u5236\uff0c\u7ed3\u5408\u65b0\u6784\u5efa\u7684KITTI-AR\u6570\u636e\u96c6\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u68c0\u6d4b\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u5bf9\u7a00\u6709\u5f02\u5e38\u76ee\u6807\u7684\u6cdb\u5316\u68c0\u6d4b\u548c\u8fc7\u6ee4\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09628", "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SpreadPy\uff0c\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u8ba4\u77e5\u7f51\u7edc\u4e2d\u6269\u6563\u6fc0\u6d3b\u7684Python\u5e93\uff0c\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u7814\u7a76\u8ba4\u77e5\u8fc7\u7a0b\u3001\u4e2a\u4f53\u5dee\u5f02\u548c\u4e34\u5e8a\u969c\u788d\u65b9\u9762\u7684\u6548\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u9700\u8981\u4e00\u4e2a\u5de5\u5177\u6765\u7cfb\u7edf\u5730\u6d4b\u8bd5\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7ed3\u6784\u4e0e\u529f\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u7d22\u6fc0\u6d3b\u52a8\u6001\u5982\u4f55\u53cd\u6620\u8ba4\u77e5\u3001\u5fc3\u7406\u548c\u4e34\u5e8a\u73b0\u8c61\uff0c\u4ece\u800c\u63d0\u4f9b\u5bf9\u77e5\u8bc6\u5efa\u6a21\u4e2d\u65e2\u6709\u7406\u8bba\u7684\u9a8c\u8bc1\u548c\u6df1\u5165\u6d1e\u5bdf\u3002", "method": "\u5f15\u5165\u5e76\u4f7f\u7528\u4e86SpreadPy\uff0c\u4e00\u4e2aPython\u5e93\uff0c\u7528\u4e8e\u5728\u8ba4\u77e5\u5355\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u4e2d\u6a21\u62df\u6269\u6563\u6fc0\u6d3b\u3002\u901a\u8fc7\u4ee5\u4e0b\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5176\u529f\u80fd\uff1a(1) \u5728\u5173\u8054\u77e5\u8bc6\u7f51\u7edc\u4e0a\u6a21\u62df\u6fc0\u6d3b\u4ee5\u533a\u5206\u6570\u5b66\u7126\u8651\u7a0b\u5ea6\uff1b(2) \u6a21\u62df\u521b\u9020\u529b\u4efb\u52a1\u4e2d\u7684\u6fc0\u6d3b\u8f68\u8ff9\u4ee5\u7814\u7a76\u8ba4\u77e5\u8d1f\u8377\u5bf9\u8bcd\u6c47\u83b7\u53d6\u7684\u5f71\u54cd\uff1b(3) \u5728\u5931\u8bed\u75c7\u60a3\u8005\u7684\u8bcd\u6c47\u7f51\u7edc\u4e0a\u6a21\u62df\u6fc0\u6d3b\u6a21\u5f0f\u5e76\u4e0e\u5b9e\u9645\u9519\u8bef\u7c7b\u578b\u5173\u8054\u3002", "result": "(1) SpreadPy\u6210\u529f\u533a\u5206\u4e86\u9ad8\u3001\u4f4e\u6570\u5b66\u7126\u8651\u5b66\u751f\uff0c\u63ed\u793a\u4e86\u6982\u5ff5\u7ec4\u7ec7\u4e2d\u4e0e\u7126\u8651\u76f8\u5173\u7684\u7ed3\u6784\u5dee\u5f02\uff1b(2) \u6a21\u62df\u663e\u793a\u6fc0\u6d3b\u8f68\u8ff9\u968f\u521b\u9020\u529b\u4efb\u52a1\u96be\u5ea6\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u8ba4\u77e5\u8d1f\u8377\u5bf9\u8bcd\u6c47\u83b7\u53d6\u7684\u8c03\u8282\u4f5c\u7528\uff1b(3) \u6a21\u62df\u7684\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5931\u8bed\u75c7\u60a3\u8005\u5728\u56fe\u7247\u547d\u540d\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u6216\u8bed\u97f3\u9519\u8bef\u7c7b\u578b\u76f8\u5173\u8054\uff0c\u5c06\u7f51\u7edc\u7ed3\u6784\u4e0e\u4e34\u5e8a\u635f\u4f24\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "SpreadPy\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u5229\u7528\u7ecf\u9a8c\u6216\u7406\u8bba\u7f51\u7edc\u5efa\u6a21\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4ece\u800c\u4e3a\u7406\u89e3\u4e2a\u4f53\u5dee\u5f02\u548c\u8ba4\u77e5\u969c\u788d\u63d0\u4f9b\u673a\u5236\u6027\u89c1\u89e3\u3002\u8be5\u5e93\u7684\u5f00\u653e\u6027\u5c06\u4fc3\u8fdb\u5fc3\u7406\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u6559\u80b2\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.10045", "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u6a21\u5f0f\uff08\u5982DBpedia/Wikidata\u3001DBLP/OpenAlex\uff09\u95f4\u81ea\u52a8\u7ffb\u8bd1SPARQL\u67e5\u8be2\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6027\u80fd\u56e0\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u800c\u5f02\uff0c\u4e14Wikidata\u5230DBpedia\u7684\u7ffb\u8bd1\u6548\u679c\u660e\u663e\u4f18\u4e8eDBpedia\u5230Wikidata\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u77e5\u8bc6\u56fe\u8c31\u4e92\u64cd\u4f5c\u6027\u9886\u57df\u7684\u663e\u8457\u7a7a\u767d\uff0c\u901a\u8fc7\u4e25\u683c\u8bc4\u4f30LLM\u5728SPARQL-to-SPARQL\u7ffb\u8bd1\u4e2d\u7684\u6027\u80fd\u6765\u4fc3\u8fdb\u4e0d\u540c\u77e5\u8bc6\u56fe\u8c31\u4e4b\u95f4\u7684\u4e92\u8054\u4e92\u901a\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff1a\u4e00\u4e2a\u5305\u542b\u6765\u81eaQALD-9-Plus\u7684100\u4e2aDBpedia-Wikidata\u5bf9\u9f50\u67e5\u8be2\uff1b\u53e6\u4e00\u4e2a\u5305\u542b100\u4e2aDBLP-OpenAlex\u5bf9\u9f50\u67e5\u8be2\uff0c\u4ee5\u6d4b\u8bd5\u901a\u7528\u6027\u3002\u9009\u7528Llama-3-8B\u3001DeepSeek-R1-Distill-Llama-70B\u548cMistral-Large-Instruct-2407\u4e09\u79cd\u5f00\u653eLLM\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u4e24\u79cd\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u6d4b\u8bd5\u3002LLM\u7684\u8f93\u51fa\u4e0e\u6807\u51c6\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5bf9\u9519\u8bef\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u7684\u6027\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4eceWikidata\u5230DBpedia\u7684SPARQL\u7ffb\u8bd1\u6548\u679c\u8fdc\u4f18\u4e8e\u4eceDBpedia\u5230Wikidata\u7684\u7ffb\u8bd1\u3002", "conclusion": "LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95f4\u7684SPARQL\u7ffb\u8bd1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u6a21\u578b\u9009\u62e9\u548c\u63d0\u793a\u7b56\u7565\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u5b58\u5728\u7279\u5b9a\u7684\u7ffb\u8bd1\u65b9\u5411\u6027\u504f\u597d\u3002\u8fd9\u63d0\u793a\u4e86\u5728\u5b9e\u73b0\u5168\u9762\u77e5\u8bc6\u56fe\u8c31\u4e92\u64cd\u4f5c\u6027\u65b9\u9762\uff0cLLM\u4ecd\u9762\u4e34\u6311\u6218\u548c\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2507.08966", "pdf": "https://arxiv.org/pdf/2507.08966", "abs": "https://arxiv.org/abs/2507.08966", "authors": ["Meng Liu", "Karl Leswing", "Simon K. S. Chu", "Farhad Ramezanghorbani", "Griffin Young", "Gabriel Marques", "Prerna Das", "Anjali Panikar", "Esther Jamir", "Mohammed Sulaiman Shamsudeen", "K. Shawn Watts", "Ananya Sen", "Hari Priya Devannagari", "Edward B. Miller", "Muyun Lihan", "Howook Hwang", "Janet Paulsen", "Xin Yu", "Kyle Gion", "Timur Rvachov", "Emine Kucukbenli", "Saee Gopal Paliwal"], "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": "Workshop on Generative AI for Biology at ICML 2025", "summary": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\u4e86ToxBench\u6570\u636e\u96c6\u548cDualBind\u6a21\u578b\uff0c\u65e8\u5728\u5229\u7528\u9ad8\u8d28\u91cfAB-FEP\u6570\u636e\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4ee5\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "motivation": "\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u5bf9\u836f\u7269\u7814\u53d1\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u53ef\u9760\u6570\u636e\uff0c\u800c\u9ad8\u7cbe\u5ea6\u7684\u7269\u7406\u65b9\u6cd5\uff08\u5982AB-FEP\uff09\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684AB-FEP\u6570\u636e\u96c6\u6765\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86ToxBench\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21AB-FEP\u6570\u636e\u96c6\uff0c\u5305\u542b8,770\u4e2aER\u03b1-\u914d\u4f53\u590d\u5408\u7269\u7684\u7ed3\u5408\u81ea\u7531\u80fd\uff08\u90e8\u5206\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cRMSE\u4e3a1.75 kcal/mol\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u975e\u91cd\u53e0\u7684\u914d\u4f53\u62c6\u5206\u4ee5\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u53cc\u635f\u5931\u6846\u67b6\u7684DualBind\u6a21\u578b\u3002", "result": "ToxBench\u4e3a\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5927\u89c4\u6a21AB-FEP\u6570\u636e\u3002\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0cDualBind\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002\u6b64\u5916\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c55\u793a\u4e86\u4ee5\u8fdc\u4f4e\u4e8e\u7269\u7406\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8fd1\u4f3cAB-FEP\u8ba1\u7b97\u7684\u6f5c\u529b\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u7684AB-FEP\u6570\u636e\u96c6\uff08\u5982ToxBench\uff09\u7ed3\u5408\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982DualBind\uff09\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u548c\u6bd2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2507.09216", "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "categories": ["cs.CV"], "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7403\u9762\u91c7\u6837\u65b9\u6cd5\uff0c\u4f7f\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u5168\u666f\u56fe\u50cf\uff0c\u51cf\u8f7b\u7578\u53d8\uff0c\u5e76\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5168\u666f\u56fe\u50cf\u4efb\u52a1\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4e14\u73b0\u6709\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5168\u666f\u56fe\u50cf\u56fa\u6709\u7684\u7578\u53d8\u548c\u4e0d\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u7403\u9762\u79bb\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u4f7f\u5f97\u73b0\u6709\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5e94\u7528\u4e8e\u5168\u666f\u56fe\u50cf\u5e76\u51cf\u8f7b\u7578\u53d8\u3002\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u5168\u666f\u56fe\u50cf\u5206\u5272\uff0c\u5c06\u7403\u9762\u6a21\u578b\u83b7\u5f97\u7684\u7279\u5f81\u7528\u4f5c\u7279\u5b9a\u901a\u9053\u6ce8\u610f\u529b\u7684\u63a9\u7801\u3002", "result": "\u5728\u5168\u666f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5e38\u7528\u5ba4\u5185\u6570\u636e\u96c6Stanford2D3D\u4e0a\u53d6\u5f97\u4e86\u503c\u5f97\u79f0\u8d5e\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7403\u9762\u91c7\u6837\u65b9\u6cd5\u6210\u529f\u4f7f\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u5168\u666f\u56fe\u50cf\u5904\u7406\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u7578\u53d8\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2507.09629", "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4e86\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u66f4\u7a33\u5065\uff0c\u4e14\u591a\u8bed\u8a00\uff08\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\uff09\u8054\u5408\u8bad\u7ec3\u80fd\u63d0\u9ad8\u53ef\u7f16\u8f91\u6027\u548c\u8fc1\u79fb\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u57fa\u51c6\u6570\u636e\u3002", "motivation": "\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u5728\u82f1\u8bed\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\uff0c\u5176\u884c\u4e3a\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cdKE\u65b9\u6cd5\uff08ROME\u3001MEMIT\u3001ICE\u548cLTE\uff09\uff0c\u5728Llama-2-7B-chat\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528ZsRE\u548cCounterfact\u57fa\u51c6\u7684\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u7248\u672c\uff0c\u5206\u6790\u4e86\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8bbe\u7f6e\u3002\u540c\u65f6\uff0c\u5c06Learning-To-Edit (LTE) \u6269\u5c55\u5230\u591a\u8bed\u8a00\u73af\u5883\uff0c\u5e76\u8fdb\u884c\u4e86\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u5219\u66f4\u7a33\u5065\u3002\u6b64\u5916\uff0c\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u8054\u5408\u8bad\u7ec3\u63d0\u9ad8\u4e86LTE\u7684\u53ef\u7f16\u8f91\u6027\u548c\u8fc1\u79fb\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u6307\u4ee4\u5fae\u8c03\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002\u901a\u8fc7\u591a\u8bed\u8a00\u8054\u5408\u8bad\u7ec3\uff08\u5982\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u7f16\u8f91\u7684\u6027\u80fd\u548c\u8de8\u8bed\u8a00\u80fd\u529b\u3002\u7814\u7a76\u4e3a\u672a\u6765\u7684\u963f\u62c9\u4f2f\u8bedKE\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.10076", "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "categories": ["cs.AI"], "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5047\u8bbe\u57fa\u8bba\u8bc1\uff08ABA\uff09\u63d0\u51fa\u4e86\u65b0\u7684\u6e10\u8fdb\u8bed\u4e49\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u7a7a\u767d\u3002", "motivation": "\u6e10\u8fdb\u8bed\u4e49\u5728\u8ba1\u7b97\u8bba\u8bc1\u4e2d\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u5047\u8bbe\u57fa\u8bba\u8bc1\uff08ABA\uff09\u6846\u67b6\uff0c\u5c3d\u7ba1ABA\u662f\u4e00\u79cd\u6d41\u884c\u7684\u7ed3\u6784\u5316\u8bba\u8bc1\u5f62\u5f0f\uff0c\u4e14\u6e10\u8fdb\u8bed\u4e49\u5bf9\u5176\u6709\u6f5c\u5728\u4ef7\u503c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u9896\u7684\u6e10\u8fdb\u8bed\u4e49\uff0c\u7528\u4e8e\u8d4b\u4e88ABA\u6846\u67b6\u4e2d\u7684\u6838\u5fc3\u7ec4\u4ef6\u2014\u2014\u5047\u8bbe\u2014\u2014\u8fa9\u8bc1\u5f3a\u5ea6\u30022. \u5c06\u4e24\u6781\u96c6\u5408\u8bba\u8bc1\u6846\u67b6\u4f5c\u4e3a\uff08\u6f5c\u5728\u975e\u5e73\u9762\uff09ABA\u6846\u67b6\u7684\u62bd\u8c61\u30023. \u6cdb\u5316\u4e86QBAF\u6700\u5148\u8fdb\u7684\u6a21\u5757\u5316\u6e10\u8fdb\u8bed\u4e49\u30024. \u63a2\u7d22\u4e86\u4e00\u79cd\u76f4\u63a5\u5229\u7528\u65e2\u6709QBAF\u6a21\u5757\u5316\u8bed\u4e49\u7684\u57fa\u4e8e\u8bba\u8bc1\u7684\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7ebf\u30025. \u4f7f\u7528\u5408\u6210ABA\u6846\u67b6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u4e0e\u57fa\u4e8e\u8bba\u8bc1\u7684\u5bf9\u5e94\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u6536\u655b\u6027\u3002", "result": "1. \u6240\u63d0\u51fa\u7684\u6e10\u8fdbABA\u8bed\u4e49\u6ee1\u8db3QBAF\u6e10\u8fdb\u8bed\u4e49\u7684\u7406\u60f3\u6027\u8d28\uff08\u5982\u5e73\u8861\u6027\u3001\u5355\u8c03\u6027\uff09\u7684\u9002\u5f53\u6539\u7f16\u30022. \u901a\u8fc7\u5b9e\u9a8c\u5c06\u6240\u63d0\u51fa\u7684\u6e10\u8fdbABA\u8bed\u4e49\u4e0e\u5176\u57fa\u4e8e\u8bba\u8bc1\u7684\u5bf9\u5e94\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u8bc4\u4f30\u4e86\u6536\u655b\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u4e3a\u5047\u8bbe\u57fa\u8bba\u8bc1\uff08ABA\uff09\u5f15\u5165\u4e86\u6e10\u8fdb\u8bed\u4e49\uff0c\u586b\u8865\u4e86\u8ba1\u7b97\u8bba\u8bc1\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a7a\u767d\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6027\u8d28\u548c\u6709\u6548\u6027\uff0c\u4ece\u800c\u6269\u5c55\u4e86\u6e10\u8fdb\u8bed\u4e49\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.08972", "pdf": "https://arxiv.org/pdf/2507.08972", "abs": "https://arxiv.org/abs/2507.08972", "authors": ["Sifan Wang", "Shyam Sankaran", "Panos Stinis", "Paris Perdikaris"], "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "physics.flu-dyn"], "comment": "25 pages, 13 figures, 3 tables", "summary": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u8bbe\u8ba1\u9002\u5f53\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u80fd\u6210\u529f\u6a21\u62df\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5b8c\u5168\u6e4d\u6d41\uff0c\u65e0\u9700\u4f20\u7edf\u7f51\u683c\u6216\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u6e4d\u6d41\u6a21\u62df\u5bf9\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5de8\u5927\uff0c\u5728\u9ad8\u6d41\u901f\u4e0b\u6210\u672c\u8fc7\u9ad8\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u8ba1\u7b97\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u65b0\u7684\u65e0\u7f51\u683c\u3001\u8fde\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u4f7f\u7528\u4e86\u7ecf\u8fc7\u9002\u5f53\u8bbe\u8ba1\u7684PINN\uff0c\u7ed3\u5408\u4e86\u591a\u9879\u7b97\u6cd5\u521b\u65b0\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u7f51\u7edc\u67b6\u6784\u3001\u56e0\u679c\u8bad\u7ec3\u548c\u9ad8\u7ea7\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u4ece\u7269\u7406\u65b9\u7a0b\u4e2d\u5b66\u4e60\uff0c\u4ee5\u6a21\u62df2D\u548c3D\u7684\u5b8c\u5168\u6e4d\u6d41\u3002", "result": "PINN\u6210\u529f\u6a21\u62df\u4e86\u5b8c\u5168\u6e4d\u6d41\uff0c\u5e76\u5728\u4e25\u683c\u9a8c\u8bc1\u540e\uff0c\u51c6\u786e\u518d\u73b0\u4e86\u5305\u62ec\u80fd\u91cf\u8c31\u3001\u52a8\u80fd\u3001\u6da1\u91cf\u548c\u96f7\u8bfa\u5e94\u529b\u5728\u5185\u7684\u5173\u952e\u6d41\u4f53\u7edf\u8ba1\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u795e\u7ecf\u65b9\u7a0b\u6c42\u89e3\u5668\uff08PINN\uff09\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u6df7\u6c8c\u7cfb\u7edf\uff0c\u4e3a\u8fde\u7eed\u6e4d\u6d41\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6709\u671b\u8d85\u8d8a\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2507.09217", "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["G\u00f6rkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrack-On\u7684Transformer\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u73af\u5883\u4e0b\uff08\u65e0\u672a\u6765\u5e27\u4fe1\u606f\uff09\u7684\u957f\u671f\u70b9\u8ddf\u8e2a\uff0c\u5e76\u5728\u4e03\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u671f\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u79bb\u7ebf\u5904\u7406\u548c\u672a\u6765\u5e27\u4fe1\u606f\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u6d41\u5a92\u4f53\u3001\u5177\u8eabAI\u7b49\u9700\u8981\u5373\u65f6\u51b3\u7b56\u7684\u5728\u7ebf\u5e94\u7528\u573a\u666f\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5728\u65e0\u672a\u6765\u4fe1\u606f\u8bbf\u95ee\u9650\u5236\u4e0b\u7684\u5728\u7ebf\u957f\u671f\u70b9\u8ddf\u8e2a\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u9700\u8003\u8651\u89c6\u89d2\u4e0d\u53d8\u6027\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc4\u4f30\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5728\u7ebf\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u53ef\u4f5c\u4e3a\u6709\u6548\u521d\u59cb\u5316\u5e76\u96c6\u6210\u5230\u8ddf\u8e2a\u7ba1\u9053\u4e2d\u3002\u4e3a\u89e3\u51b3\u5728\u7ebf\u957f\u671f\u8ddf\u8e2a\u4e2d\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u672c\u6587\u5f15\u5165\u4e86Track-On\u6a21\u578b\u3002\u8be5\u6a21\u578b\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5c06\u6bcf\u4e2a\u8ddf\u8e2a\u70b9\u89c6\u4e3a\u4e00\u4e2a\u67e5\u8be2\uff0c\u5e76\u9010\u5e27\u5904\u7406\u89c6\u9891\uff0c\u901a\u8fc7\u5185\u7f6e\u8bb0\u5fc6\u673a\u5236\u4f20\u64ad\u5916\u89c2\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u7ef4\u6301\u8de8\u5e27\u7684\u8fde\u8d2f\u6027\u3002", "result": "Track-On\u6a21\u578b\u5728\u4e03\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u6ca1\u6709\u672a\u6765\u5e27\u8bbf\u95ee\u7684\u4e25\u683c\u5728\u7ebf\u8bbe\u7f6e\u4e0b\uff0c\u5b9e\u73b0\u957f\u671f\u70b9\u8ddf\u8e2a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09638", "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7ed3\u5408BGE-M3\u5d4c\u5165\uff0c\u4ee5\u63d0\u5347\u6cf0\u8bed\u6cd5\u5f8b\u95ee\u7b54RAG\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5f15\u7528\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u5e76\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6cf0\u8bed\u6cd5\u5f8b\u95ee\u7b54\u9886\u57df\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u5904\u7406\u9700\u8981\u5e7f\u6cdb\u3001\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u7684\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f15\u5165\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u65b9\u6cd5\u6765\u6821\u51c6\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u63d0\u9ad8\u6cd5\u5f8b\u5f15\u7528\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5229\u7528BGE-M3\u5d4c\u5165\u4f5c\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\uff0c\u4e0e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5224\u522b\u5668\u76f8\u6bd4\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e862.5\u500d\u3002", "result": "\u5728NitiBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRPO\u76f8\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u5f15\u7528F1\u5206\u6570\u4e0a\u63d0\u5347\u9ad8\u8fbe90%\uff0c\u76f8\u5bf9\u4e8e\u6307\u4ee4\u5fae\u8c03\u5728\u7efc\u5408\u8d28\u91cf\u6307\u6807\u4e0a\u63d0\u534731%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u6bd4\u6307\u4ee4\u5fae\u8c03\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u589e\u5f3a\u6cf0\u8bed\u6cd5\u5f8b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10106", "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "categories": ["cs.AI"], "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "AI": {"tldr": "BlueGlass\u662f\u4e00\u4e2a\u7edf\u4e00\u7684AI\u5b89\u5168\u6846\u67b6\uff0c\u65e8\u5728\u96c6\u6210\u73b0\u6709\u788e\u7247\u5316\u5b89\u5168\u5de5\u5177\uff0c\u901a\u8fc7\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6848\u4f8b\u5206\u6790\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\u548c\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u5b89\u5168\u5de5\u5177\u529f\u80fd\u5355\u4e00\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u4fdd\u969c\uff0c\u56e0\u6b64\u9700\u8981\u96c6\u6210\u548c\u590d\u5408\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165BlueGlass\u6846\u67b6\uff0c\u63d0\u4f9b\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u4ee5\u6574\u5408\u548c\u7ec4\u5408\u64cd\u4f5c\u4e8e\u6a21\u578b\u5185\u90e8\u548c\u8f93\u51fa\u7684\u591a\u79cd\u5b89\u5168\u5de5\u5177\u3002\u901a\u8fc7\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e09\u9879\u5b89\u5168\u5206\u6790\u6765\u9a8c\u8bc1\u5176\u6548\u7528\uff1a1) \u5206\u5e03\u8bc4\u4f30\uff1b2) \u57fa\u4e8e\u63a2\u9488\u7684\u5c42\u52a8\u6001\u5206\u6790\uff1b3) \u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u53ef\u89e3\u91ca\u6982\u5ff5\u3002", "result": "1) \u5206\u5e03\u8bc4\u4f30\u63ed\u793a\u4e86\u8de8\u5206\u5e03\u7684\u6027\u80fd\u6743\u8861\u548c\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\uff1b2) \u57fa\u4e8e\u63a2\u9488\u7684\u5206\u6790\u7a81\u51fa\u4e86\u901a\u8fc7\u76f8\u53d8\u5b9e\u73b0\u7684\u5171\u4eab\u5206\u5c42\u5b66\u4e60\uff1b3) \u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u51fa\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u6846\u67b6\u548c\u91cd\u8981\u53d1\u73b0\u3002"}}
{"id": "2507.08977", "pdf": "https://arxiv.org/pdf/2507.08977", "abs": "https://arxiv.org/abs/2507.08977", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Marisa Eisenberg"], "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u62df\u57fa\u7840\u795e\u7ecf\u7f51\u7edc\uff08SGNNs\uff09\uff0c\u901a\u8fc7\u5229\u7528\u673a\u68b0\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u65e8\u5728\u7ed3\u5408\u673a\u68b0\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7075\u6d3b\u6027\uff0c\u5728\u591a\u79cd\u79d1\u5b66\u9884\u6d4b\u548c\u63a8\u65ad\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u5efa\u6a21\u9762\u4e34\u6838\u5fc3\u5c40\u9650\uff1a\u673a\u68b0\u6a21\u578b\u867d\u7136\u53ef\u89e3\u91ca\u4f46\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff1b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7075\u6d3b\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u65e0\u6cd5\u63a8\u65ad\u4e0d\u53ef\u89c2\u6d4b\u53d8\u91cf\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\uff08\u9ed1\u7bb1\u95ee\u9898\uff09\u3002", "method": "\u5f15\u5165\u6a21\u62df\u57fa\u7840\u795e\u7ecf\u7f51\u7edc\uff08SGNNs\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u673a\u68b0\u6a21\u62df\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u6570\u636e\u3002SGNNs\u901a\u8fc7\u5728\u5305\u542b\u591a\u6837\u6a21\u578b\u7ed3\u6784\u3001\u53c2\u6570\u8303\u56f4\u3001\u968f\u673a\u6027\u548c\u89c2\u6d4b\u8bef\u5dee\u7684\u5408\u6210\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "SGNNs\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u548c\u5efa\u6a21\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5c06COVID-19\u9884\u6d4b\u6280\u80fd\u8f83CDC\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8fd1\u4e09\u500d\uff0c\u5316\u5b66\u4ea7\u7387\u9884\u6d4b\u8bef\u5dee\u51cf\u5c11\u4e86\u4e09\u5206\u4e4b\u4e00\uff0c\u5e76\u5728\u751f\u6001\u9884\u6d4b\u4e2d\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\u3002\u5728\u63a8\u65ad\u4efb\u52a1\u4e2d\uff0cSGNNs\u80fd\u591f\u51c6\u786e\u5206\u7c7b\u6a21\u62df\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u4f20\u64ad\u6e90\uff0c\u5e76\u80fd\u66f4\u51c6\u786e\u5730\u4f30\u8ba1COVID-19\u4f20\u67d3\u6027\u7b49\u4e0d\u53ef\u89c2\u6d4b\u76ee\u6807\u3002\u6b64\u5916\uff0cSGNNs\u8fd8\u5b9e\u73b0\u4e86\u201c\u56de\u6eaf\u6a21\u62df\u5f52\u56e0\u201d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u68b0\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u6a21\u578b\u8ba4\u4e3a\u54ea\u4e9b\u6f5c\u5728\u52a8\u529b\u5b66\u662f\u6d3b\u8dc3\u7684\u3002", "conclusion": "SGNNs\u5c06\u79d1\u5b66\u7406\u8bba\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u76f8\u7ed3\u5408\uff0c\u5f00\u542f\u4e86\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u8303\u5f0f\u3002\u5b83\u5c06\u6a21\u62df\u4ece\u50f5\u5316\u5de5\u5177\u8f6c\u53d8\u4e3a\u7075\u6d3b\u7684\u76d1\u7763\u6765\u6e90\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u65ad\u3002"}}
{"id": "2507.09222", "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStaRFM\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u5206\u7c7b\u548c\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u5206\u5e03\u6f02\u79fb\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u548c\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u3001SAM\uff09\u867d\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u5f71\u50cf\u53d1\u5c55\uff0c\u4f46\u5176\u90e8\u7f72\u53d7\u9650\u4e8e\u4e24\u5927\u6311\u6218\uff1a\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u7684\u201c\u5206\u5e03\u6f02\u79fb\u201d\u548c\u5bfc\u81f4\u9519\u8bef\u9884\u6d4b\u7684\u201c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u201d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u662f\u9886\u57df\u7279\u5316\u7684\u3002", "method": "StaRFM\u6846\u67b6\u5f15\u5165\u4e86Fisher\u4fe1\u606f\u60e9\u7f5a\uff08FIP\uff09\u6765\u51cf\u5c11CLIP\u548cSAM\u5d4c\u5165\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff08\u901a\u8fc7\u8865\u4e01\u7ea7\u6b63\u5219\u5316\u6269\u5c55\u52303D\u533b\u5b66\u6570\u636e\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u60e9\u7f5a\uff08CMP\uff09\u6765\u6821\u51c6\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u9488\u5bf9\u4f53\u7d20\u7ea7\u9884\u6d4b\u91cd\u65b0 \u0444\u043e\u0440\u043c\u0443ulated\uff09\u3002\u7406\u8bba\u4e0a\uff0cFIP\u901a\u8fc7Fisher-Rao\u8303\u6570\u63a7\u5236\u6cdb\u5316\uff0cCMP\u901a\u8fc7Brier\u5206\u6570\u4f18\u5316\u6700\u5c0f\u5316\u6821\u51c6\u8bef\u5dee\u3002", "result": "StaRFM\u572819\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0+3.5%\u51c6\u786e\u7387\u548c28%\u66f4\u4f4e\u7684ECE\uff1b\u5728\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\uff08\u5982BraTS\u3001ATLAS\uff09\u5b9e\u73b084.7% DSC\u548c4.8mm HD95\uff1b\u8de8\u57df\u6027\u80fd\u5dee\u8ddd\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e40%\u3002", "conclusion": "StaRFM\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u53ea\u9700\u6781\u5c0f\u7684\u67b6\u6784\u4fee\u6539\u5373\u53ef\u4e0e\u57fa\u7840\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002\u5b83\u6709\u6548\u7edf\u4e00\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09701", "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MCEval\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u8bed\u8a00\u6587\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u548c\u504f\u89c1\u3002\u7814\u7a76\u53d1\u73b0LLMs\u7684\u6587\u5316\u8868\u73b0\u4e0d\u4ec5\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u5173\uff0c\u8fd8\u4e0e\u8bed\u8a00-\u6587\u5316\u5bf9\u9f50\u6709\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u82f1\u8bed\u4e2d\u5fc3\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u670d\u52a1\u5168\u7403\u7528\u6237\u65f6\uff0c\u5b58\u5728\u6587\u5316\u504f\u89c1\u548c\u6709\u9650\u7684\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86MCEval\uff0c\u4e00\u4e2a\u591a\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u52a8\u6001\u6587\u5316\u95ee\u9898\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u91cd\u8ff0\u548c\u6df7\u6dc6\u56e0\u5b50\u91cd\u8ff0\u5b9e\u73b0\u56e0\u679c\u5206\u6790\u3002\u8bc4\u4f30\u6db5\u76d613\u79cd\u6587\u5316\u548c13\u79cd\u8bed\u8a00\uff0c\u751f\u6210\u4e8639,897\u4e2a\u6587\u5316\u610f\u8bc6\u5b9e\u4f8b\u548c17,940\u4e2a\u6587\u5316\u504f\u89c1\u5b9e\u4f8b\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30LLMs\u7684\u6587\u5316\u610f\u8bc6\u548c\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u4e0d\u540c\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u6700\u4f73\u6587\u5316\u8868\u73b0\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u53ca\u8bed\u8a00-\u6587\u5316\u5bf9\u9f50\u76f8\u5173\u3002\u6b64\u5916\uff0c\u5728\u82f1\u8bed\u573a\u666f\u4e0b\u770b\u4f3c\u6210\u529f\u7684\u65b9\u6cd5\u4f1a\u9020\u6210\u663e\u8457\u7684\u516c\u5e73\u6027\u52a3\u52bf\u3002", "conclusion": "MCEval\u662f\u9996\u4e2a\u5168\u9762\u7684\u591a\u8bed\u8a00\u6587\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u6df1\u5165\u7406\u89e3LLMs\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6df1\u523b\u89c1\u89e3\u3002"}}
{"id": "2507.10119", "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u8fb9\u7f18\u4e91\u5e94\u7528\u8fc1\u79fb\u95ee\u9898\u5efa\u6a21\u4e3a\u6c49\u8bfa\u5854\u95ee\u9898\uff0c\u5206\u6790\u548c\u6bd4\u8f83\u4e86AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u8fc1\u79fb\u7f16\u6392\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u8fb9\u7f18\u4e91\u5e94\u7528\u8fc1\u79fb\u6709\u52a9\u4e8e\u63d0\u9ad8\u670d\u52a1\u8d28\u91cf\u548c\u6210\u672c\u6548\u76ca\uff0c\u4f46\u5176\u81ea\u52a8\u7f16\u6392\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u4e9f\u9700\u7406\u89e3\u5e76\u5229\u7528\u5148\u8fdb\u7684AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u6765\u6709\u6548\u7ba1\u7406\u65b0\u5174\u8ba1\u7b97\u8fde\u7eed\u4f53\u73af\u5883\u4e2d\u7684\u6b64\u7c7b\u8fc1\u79fb\u3002", "method": "\u4ece\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u51fa\u53d1\uff0c\u5c06\u8fb9\u7f18\u4e91\u5e94\u7528\u8fc1\u79fb\u95ee\u9898\u5efa\u6a21\u4e3a\u6c49\u8bfa\u5854\u95ee\u9898\u3002\u8bc6\u522b\u3001\u5206\u6790\u5e76\u6bd4\u8f83\u4e86\u9009\u5b9a\u7684AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u5b9a\u4e49\u7684\u65b0\u5206\u7c7b\u65b9\u6cd5\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5bf9\u5efa\u6a21\u4e3a\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u8fb9\u7f18\u4e91\u5e94\u7528\u8fc1\u79fb\u8fdb\u884cAI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8bc6\u522b\u3001\u5206\u6790\u548c\u6bd4\u8f83\uff0c\u5e76\u7ed3\u5408\u63d0\u51fa\u7684\u65b0\u5206\u7c7b\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u5bf9\u73b0\u6709\u7f16\u6392\u6280\u672f\u7684\u6df1\u5165\u7406\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5e76\u638c\u63e1\u5728\u65b0\u5174\u8ba1\u7b97\u8fde\u7eed\u4f53\u73af\u5883\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u81ea\u52a8\u7f16\u6392\u8fb9\u7f18\u4e91\u5e94\u7528\u8fc1\u79fb\u7684\u53ef\u7528AI\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002"}}
{"id": "2507.08980", "pdf": "https://arxiv.org/pdf/2507.08980", "abs": "https://arxiv.org/abs/2507.08980", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "title": "Learning Diffusion Models with Flexible Representation Guidance", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8868\u793a\u5f15\u5bfc\u6765\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u5177\u4f53\u901a\u8fc7\u4e24\u79cd\u65b0\u7b56\u7565\uff08\u591a\u6a21\u6001\u914d\u5bf9\u5b66\u4e60\u548c\u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u9f50\u80fd\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5229\u7528\u8f93\u5165\u8868\u793a\u6765\u6539\u8fdb\u6269\u6563\u6a21\u578b\u3002", "method": "1. \u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5305\u542b\u53bb\u566a\u6a21\u578b\u7684\u591a\u79cd\u5206\u89e3\u53ca\u5176\u8bad\u7ec3\u6807\u51c6\uff0c\u4ee5\u786e\u5b9a\u8f85\u52a9\u8868\u793a\u7684\u6574\u5408\u65b9\u5f0f\u548c\u65f6\u673a\u3002\n2. \u5f15\u5165\u4e24\u79cd\u65b0\u7b56\u7565\uff1aa) \u5c06\u6837\u672c\u4e0e\u76ee\u6807\u8868\u793a\uff08\u6e90\u4e8e\u81ea\u8eab\u6216\u4e0d\u540c\u5408\u6210\u6a21\u6001\uff09\u914d\u5bf9\uff0c\u5e76\u5b66\u4e60\u591a\u6a21\u6001\u5bf9\u7684\u8054\u5408\u6a21\u578b\uff1bb) \u8bbe\u8ba1\u4e00\u4e2a\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u6570\u636e\u751f\u6210\u7684\u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\u3002", "result": "1. \u5728\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u52a0\u901f\u3002\n2. \u5728ImageNet $256\\times 256$\u7c7b\u522b\u6761\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u539f\u59cbSiT-XL\u5feb23.3\u500d\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5REPA\u5feb4\u500d\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8868\u793a\u5f15\u5bfc\u6846\u67b6\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u672c\u6587\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5728\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u679c\u3002"}}
{"id": "2507.09230", "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay T\u00fcrkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u57fa\u4e8eStable Diffusion\u7684\u751f\u6210\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u53ef\u52a8\u753b\u4eba\u7269\u66ff\u8eab\uff0c\u65e8\u5728\u514b\u670d\u906e\u6321\u548c\u5931\u771f\uff0c\u5b9e\u73b0\u66f4\u7ecf\u6d4e\u3001\u666e\u9002\u7684\u8fdc\u7a0b\u4e34\u573a\u4f53\u9a8c\u3002", "motivation": "\u7406\u60f3\u7684\u6570\u5b57\u8fdc\u7a0b\u4e34\u573a\u9700\u8981\u7cbe\u786e\u590d\u5236\u4eba\u7269\u3002\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u867d\u80fd\u4f7f\u7528\u4fbf\u643a\u3001\u4f4e\u6210\u672c\u8bbe\u5907\uff0c\u5374\u9762\u4e34\u906e\u6321\u548c\u8eab\u4f53\u6bd4\u4f8b\u5931\u771f\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u9c9c\u5c11\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u91cd\u5efa\u4eba\u7269\u5916\u89c2\uff0c\u4e14\u591a\u4f9d\u8d56\u591a\u89c6\u89d2\u8bad\u7ec3\u6570\u636e\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u751f\u6210\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u4fbf\u643a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eStable Diffusion\u7684\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u9996\u6b21\u4f7f\u7528\u751f\u6210\u9aa8\u5e72\u4ece\u81ea\u6211\u4e2d\u5fc3\u8f93\u5165\u91cd\u5efa\u53ef\u52a8\u753b\u4eba\u7269\u66ff\u8eab\u3002\u8be5\u65b9\u6cd5\u53d7SiTH\u548cMagicMan\u542f\u53d1\uff0c\u5229\u7528ControlNet\u548cStable Diffusion\u9aa8\u5e72\u4ece\u88ab\u906e\u6321\u7684\u81ea\u4e0a\u800c\u4e0b\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u6b63\u9762\u89c6\u56fe\uff0c\u968f\u540e\u5c06\u6b64\u6b63\u9762\u8868\u793a\u8f93\u5165\u56fe\u50cf\u5230\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u6210\u529f\u5730\u5c06\u5355\u5f20\u81ea\u4e0a\u800c\u4e0b\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u6b63\u9762\u8868\u793a\uff0c\u5e76\u80fd\u8fdb\u4e00\u6b65\u751f\u6210\u4eba\u7269\u66ff\u8eab\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528Stable Diffusion\u4f5c\u4e3a\u751f\u6210\u9aa8\u5e72\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d1f\u62c5\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u662f\u9996\u4e2a\u5229\u7528\u751f\u6210\u9aa8\u5e72\u4ece\u81ea\u6211\u4e2d\u5fc3\u8f93\u5165\u91cd\u5efa\u53ef\u52a8\u753b\u4eba\u7269\u66ff\u8eab\u7684\u5c1d\u8bd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u6781\u7b80\u8f93\u5165\u751f\u6210\u66ff\u8eab\u52a8\u4f5c\u3002\u8fd9\u4e3a\u6784\u5efa\u66f4\u6613\u8bbf\u95ee\u548c\u66f4\u5177\u6cdb\u5316\u6027\u7684\u8fdc\u7a0b\u4e34\u573a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u4e49\u4fe1\u606f\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5448\u7ebf\u6027\u53ef\u5206\uff0c\u4e14\u5728\u66f4\u6df1\u5c42\u548c\u7279\u5b9a\u63d0\u793a\u4e0b\u66f4\u4e3a\u663e\u8457\uff0c\u4e3a\u5f00\u53d1\u57fa\u4e8e\u6f5c\u5728\u8868\u793a\u7684\u5b89\u5168\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u5bf9\u4e8e\u89e3\u91ca\u5176\u884c\u4e3a\u548c\u63d0\u9ad8\u5bf9\u9f50\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695aLLM\u5728\u5185\u90e8\u5982\u4f55\u7ec4\u7ec7\u4e0e\u8bed\u4e49\u7406\u89e3\u76f8\u5173\u7684\u8868\u793a\u3002", "method": "\u5bf911\u4e2a\u57fa\u4e8eTransformer\u7684\u4ec5\u89e3\u7801\u5668LLM\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u57286\u4e2a\u79d1\u5b66\u4e3b\u9898\u548c\u6bcf\u4e2a\u6a21\u578b\u768412\u4e2a\u5c42\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u3002", "result": "\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u59cb\u7ec8\u4f4d\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5f62\u6210\u8de8\u4e0d\u540c\u9886\u57df\u7684\u7ebf\u6027\u53ef\u5206\u79bb\u8868\u793a\u3002\u8fd9\u79cd\u53ef\u5206\u79bb\u6027\u5728\u66f4\u6df1\u7684\u5c42\u4e2d\u4ee5\u53ca\u5728\u89e6\u53d1\u7ed3\u6784\u5316\u63a8\u7406\u6216\u5bf9\u9f50\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\uff08\u5373\u4f7f\u8868\u9762\u5185\u5bb9\u4e0d\u53d8\uff09\u53d8\u5f97\u66f4\u52a0\u660e\u663e\u3002\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u652f\u6301\u5728\u9690\u85cf\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7b80\u5355\u800c\u6709\u6548\u7684\u56e0\u679c\u5e72\u9884\uff08\u4f8b\u5982\uff0c\u601d\u7ef4\u94fe\u53ef\u7531\u5355\u4e2a\u5411\u91cf\u65b9\u5411\u6355\u83b7\uff09\u3002\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684MLP\u5206\u7c7b\u5668\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6f5c\u7a7a\u95f4\u62a4\u680f\uff0c\u4ee5\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u4e86\u5bf9\u6297\u6027\u548c\u6076\u610f\u63d0\u793a\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u5f00\u53d1\u76f4\u63a5\u4f5c\u7528\u4e8e\u6f5c\u5728\u8868\u793a\u7684\u51e0\u4f55\u611f\u77e5\u5de5\u5177\uff0c\u5229\u7528\u5176\u7ebf\u6027\u53ef\u5206\u79bb\u6027\u6765\u68c0\u6d4b\u548c\u51cf\u8f7b\u6709\u5bb3\u6216\u5bf9\u6297\u6027\u5185\u5bb9\uff08\u4f8b\u5982\uff0c\u57fa\u4e8e\u4f20\u8f93\u7684\u9632\u5fa1\u65b9\u6cd5\uff09\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10124", "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "categories": ["cs.AI"], "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u4eba\u7c7b\u5fc3\u7406\u5b66\u4e2d\u7684\u5143\u8ba4\u77e5\u63d0\u793a\uff08\u5982\u201c\u4f60\u53ef\u80fd\u9519\u4e86\u5417\uff1f\u201d\uff09\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u4fc3\u4f7f\u5176\u8bc6\u522b\u81ea\u8eab\u504f\u5dee\u3001\u63d0\u4f9b\u66f4\u5168\u9762\u4fe1\u606f\uff0c\u5e76\u6539\u8fdb\u63d0\u793a\u5de5\u7a0b\u3002", "motivation": "\u8bc6\u522bLLMs\u504f\u5dee\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\uff0c\u4e14\u6a21\u578b\u4e0d\u65ad\u6f14\u8fdb\uff0c\u9700\u8981\u901a\u7528\u3001\u6301\u4e45\u7684\u53bb\u504f\u7b56\u7565\u3002\u4eba\u7c7b\u51b3\u7b56\u53bb\u504f\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u63d0\u793a\u5e72\u9884\u5524\u9192\u6f5c\u5728\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u4e3aLLMs\u53bb\u504f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u56e0\u4e3aLLMs\u867d\u5305\u542b\u504f\u5dee\u4fe1\u606f\uff0c\u4f46\u9700\u88ab\u63d0\u793a\u624d\u80fd\u663e\u73b0\u3002", "method": "\u5c06\u4eba\u7c7b\u51b3\u7b56\u6587\u732e\u4e2d\u53d1\u5c55\u7684\u5143\u8ba4\u77e5\u63d0\u793a\uff08\u7279\u522b\u662f\u201c\u4f60\u53ef\u80fd\u9519\u4e86\u5417\uff1f\u201d\uff09\u5e94\u7528\u4e8eLLMs\u3002\u5728LLM\u521d\u59cb\u54cd\u5e94\u540e\u4f7f\u7528\u6b64\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u6765\u81eaLLM\u504f\u5dee\u76f8\u5173\u6587\u7ae0\u7684\u95ee\u9898\uff08\u5305\u62ec\u9690\u6027\u6b67\u89c6\u504f\u5dee\u548c\u5143\u8ba4\u77e5\u5931\u8d25\uff09\u4ee5\u53ca\u4e00\u4e2a\u5173\u4e8e\u4e0d\u5b8c\u6574\u4fe1\u606f\u7684\u4f8b\u5b50\u6765\u6f14\u793a\u5176\u6548\u679c\u3002", "result": "\u201c\u4f60\u53ef\u80fd\u9519\u4e86\u5417\uff1f\u201d\u7684\u63d0\u793a\u4fc3\u4f7fLLMs\u4ea7\u751f\u989d\u5916\u4fe1\u606f\uff0c\u5305\u62ec\u5176\u521d\u59cb\u7b54\u6848\u7684\u7406\u7531\u3001\u9519\u8bef\u3001\u504f\u5dee\u3001\u77db\u76fe\u8bc1\u636e\u548c\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd9\u4e9b\u5728\u521d\u59cb\u54cd\u5e94\u4e2d\u5747\u672a\u51fa\u73b0\u3002\u8fd9\u79cd\u5143\u77e5\u8bc6\u5e38\u63ed\u793aLLMs\u548c\u7528\u6237\u5bf9\u63d0\u793a\u7684\u7406\u89e3\u4e0d\u4e00\u81f4\u3002\u8be5\u63d0\u793a\u80fd\u5e2e\u52a9LLMs\u8bc6\u522b\u81ea\u8eab\u504f\u5dee\u5e76\u8fdb\u884c\u8fde\u8d2f\u7684\u5143\u8ba4\u77e5\u53cd\u601d\uff0c\u5e76\u80fd\u6709\u6548\u7ea0\u6b63\u770b\u4f3c\u4ee4\u4eba\u4fe1\u670d\u4f46\u4e0d\u5b8c\u6574\u7684\u4fe1\u606f\u3002", "conclusion": "\u4eba\u7c7b\u5fc3\u7406\u5b66\u4e3a\u63d0\u793a\u5de5\u7a0b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u501f\u9274\u4eba\u7c7b\u51b3\u7b56\u4e2d\u57fa\u4e8e\u63d0\u793a\u7684\u6709\u6548\u6539\u8fdb\u7ecf\u9a8c\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u5584LLMs\u7684\u53bb\u504f\u548c\u54cd\u5e94\u8d28\u91cf\u3002"}}
{"id": "2507.08983", "pdf": "https://arxiv.org/pdf/2507.08983", "abs": "https://arxiv.org/abs/2507.08983", "authors": ["Anshuman Suri", "Harsh Chaudhari", "Yuefeng Peng", "Ali Naseh", "Amir Houmansadr", "Alina Oprea"], "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "While poisoning attacks on machine learning models have been extensively\nstudied, the mechanisms by which adversaries can distribute poisoned models at\nscale remain largely unexplored. In this paper, we shed light on how model\nleaderboards -- ranked platforms for model discovery and evaluation -- can\nserve as a powerful channel for adversaries for stealthy large-scale\ndistribution of poisoned models. We present TrojanClimb, a general framework\nthat enables injection of malicious behaviors while maintaining competitive\nleaderboard performance. We demonstrate its effectiveness across four diverse\nmodalities: text-embedding, text-generation, text-to-speech and text-to-image,\nshowing that adversaries can successfully achieve high leaderboard rankings\nwhile embedding arbitrary harmful functionalities, from backdoors to bias\ninjection. Our findings reveal a significant vulnerability in the machine\nlearning ecosystem, highlighting the urgent need to redesign leaderboard\nevaluation mechanisms to detect and filter malicious (e.g., poisoned) models,\nwhile exposing broader security implications for the machine learning community\nregarding the risks of adopting models from unverified sources.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u6a21\u578b\u6392\u884c\u699c\u53ef\u88ab\u6076\u610f\u5229\u7528\uff0c\u5927\u89c4\u6a21\u9690\u853d\u5206\u53d1\u4e2d\u6bd2\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u673a\u5236\u9700\u6c42\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u4e2d\u6bd2\u653b\u51fb\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u653b\u51fb\u8005\u5982\u4f55\u5927\u89c4\u6a21\u5206\u53d1\u4e2d\u6bd2\u6a21\u578b\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6a21\u578b\u6392\u884c\u699c\u8fd9\u4e00\u6e20\u9053\uff0c\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTrojanClimb\u7684\u901a\u7528\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5141\u8bb8\u5728\u6a21\u578b\u7ef4\u6301\u7ade\u4e89\u529b\u6392\u884c\u699c\u8868\u73b0\u7684\u540c\u65f6\uff0c\u6ce8\u5165\u6076\u610f\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u5728\u6587\u672c\u5d4c\u5165\u3001\u6587\u672c\u751f\u6210\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u6587\u672c\u8f6c\u56fe\u50cf\u56db\u79cd\u4e0d\u540c\u6a21\u6001\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86TrojanClimb\u7684\u6709\u6548\u6027\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u6210\u529f\u5730\u5728\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u9ad8\u6392\u540d\uff0c\u540c\u65f6\u5d4c\u5165\u4efb\u610f\u6709\u5bb3\u529f\u80fd\uff0c\u5982\u540e\u95e8\u6216\u504f\u89c1\u6ce8\u5165\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u91cd\u65b0\u8bbe\u8ba1\u6392\u884c\u699c\u8bc4\u4f30\u673a\u5236\u4ee5\u68c0\u6d4b\u548c\u8fc7\u6ee4\u6076\u610f\u6a21\u578b\u7684\u7d27\u8feb\u6027\uff0c\u5e76\u8b66\u793a\u4e86\u4ece\u672a\u7ecf\u8bc1\u5b9e\u6765\u6e90\u91c7\u7528\u6a21\u578b\u6240\u5e26\u6765\u7684\u5e7f\u6cdb\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2507.09242", "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "categories": ["cs.CV"], "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u7ed8\u753b\u8fc7\u7a0b\u7684\u65b0\u6846\u67b6\uff0c\u5305\u62ec\u9996\u4e2a\u7ed8\u753b\u8fc7\u7a0b\u6570\u636e\u96c6PPAD\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578bPPJudge\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u827a\u672f\u56fe\u50cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u6700\u7ec8\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u7ed8\u753b\u8fc7\u7a0b\u7684\u52a8\u6001\u6027\u548c\u591a\u9636\u6bb5\u7279\u6027\u3002", "method": "1. \u5f15\u5165\u7ed8\u753b\u8fc7\u7a0b\u8bc4\u4f30\u6570\u636e\u96c6\uff08PPAD\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u7ed8\u753b\u8fc7\u7a0b\u56fe\u50cf\uff0c\u5e76\u7531\u9886\u57df\u4e13\u5bb6\u6807\u6ce8\u4e86\u516b\u4e2a\u8be6\u7ec6\u5c5e\u6027\u7684\u5927\u578b\u6570\u636e\u96c6\u3002 2. \u63d0\u51fa\u4e86PPJudge\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u5f02\u6784\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u4e86\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8ba1\u7b97\u521b\u610f\u548c\u827a\u672f\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.09758", "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u81ea\u8eab\u9884\u6d4b\u7684\u96be\u5ea6\u5206\u6570\u6765\u6392\u5e8f\u5fae\u8c03\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u96be\u5ea6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u7684\u96be\u5ea6\u6307\u6807\uff08\u5982\u6587\u672c\u957f\u5ea6\uff09\uff0c\u8fd9\u4e9b\u6307\u6807\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u81ea\u8eab\u5bf9\u6837\u672c\u96be\u5ea6\u7684\u611f\u77e5\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u8ba9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u81ea\u8eab\u9884\u6d4b\u6837\u672c\u7684\u96be\u5ea6\u5206\u6570\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5206\u6570\uff0c\u63a2\u7d22\u4e86\u591a\u79cd\u5fae\u8c03\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u4ece\u6613\u5230\u96be\u3001\u4ece\u96be\u5230\u6613\u4ee5\u53ca\u6df7\u5408\u91c7\u6837\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u6807\u51c6\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u52a0\u5feb\u6a21\u578b\u6536\u655b\u901f\u5ea6\u5e76\u63d0\u9ad8\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8ba9PLMs\u81ea\u9002\u5e94\u5730\u8bc4\u4f30\u6837\u672c\u96be\u5ea6\u5e76\u6307\u5bfc\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u6a21\u578b\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10134", "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "categories": ["cs.AI", "53-01", "C.2"], "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08FRSICL\uff09\u7684\u65e0\u4eba\u673a\u98de\u884c\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u5b9e\u65f6\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u548c\u6570\u636e\u91c7\u96c6\uff0c\u4ee5\u6700\u5c0f\u5316\u91ce\u706b\u76d1\u6d4b\u4e2d\u7684\u4fe1\u606f\u65f6\u6548\u6027\uff08AoI\uff09\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u91ce\u706b\u76d1\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u65e9\u671f\u9884\u8b66\u4f9d\u8d56\u4e8e\u53ca\u65f6\u7684\u6570\u636e\uff08\u4f4eAoI\uff09\u3002\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u5728\u4f18\u5316\u65e0\u4eba\u673a\u4f20\u611f\u5668\u4f20\u8f93\u8c03\u5ea6\u548c\u901f\u5ea6\u65b9\u9762\u5b58\u5728\u91c7\u6837\u6548\u7387\u4f4e\u3001\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u5927\u3001\u8bad\u7ec3\u590d\u6742\u7b49\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u4e0d\u9002\u7528\u4e8e\u91ce\u706b\u76d1\u6d4b\u7b49\u65f6\u95f4\u654f\u611f\u578b\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08LLM-Enabled In-Context Learning, FRSICL\uff09\u7684\u5728\u7ebf\u98de\u884c\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u548c\u73af\u5883\u53cd\u9988\uff0c\u5b9e\u65f6\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u7684\u98de\u884c\u63a7\u5236\u548c\u6570\u636e\u91c7\u96c6\u8c03\u5ea6\uff0c\u65e8\u5728\u6e10\u8fdb\u5730\u6700\u5c0f\u5316\u5730\u9762\u4f20\u611f\u5668\u7684\u5e73\u5747AoI\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u7684\u518d\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\uff0c\u4e0e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u6700\u8fd1\u90bb\u7b49\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684FRSICL\u65b9\u6848\u662f\u6709\u6548\u7684\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "FRSICL\u65b9\u6848\u901a\u8fc7\u5229\u7528\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e3a\u91ce\u706b\u76d1\u6d4b\u4e2d\u7684\u65e0\u4eba\u673a\u98de\u884c\u548c\u6570\u636e\u91c7\u96c6\u8054\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfDRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u606f\u65f6\u6548\u6027\u3002"}}
{"id": "2507.09009", "pdf": "https://arxiv.org/pdf/2507.09009", "abs": "https://arxiv.org/abs/2507.09009", "authors": ["Zhengxiao He", "Huayu Li", "Geng Yuan", "William D. S. Killgore", "Stuart F. Quan", "Chen X. Chen", "Ao Li"], "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\uff08EEG\u3001ECG\u3001\u547c\u5438\uff09\u751f\u6210\u4e2a\u4f53\u5316\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u98ce\u9669\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CVD\u4e8b\u4ef6\u548c\u6b7b\u4ea1\u7387\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\uff08\u5982PSG\u6570\u636e\uff09\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u751f\u6210\u4e2a\u4f53\u5316\u7684CVD\u98ce\u9669\u8bc4\u5206\uff0c\u4ee5\u589e\u5f3a\u4e34\u5e8a\u98ce\u9669\u8bc4\u4f30\u548c\u652f\u6301\u4e2a\u6027\u5316\u62a4\u7406\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4eceEEG\u3001ECG\u548c\u547c\u5438\u4fe1\u53f7\u4e2d\u63d0\u53d6\u6a21\u5f0f\u3002\u8be5\u6a21\u578b\u57284,398\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6709\u65e0CVD\u7ed3\u679c\u4e2a\u4f53\u7684\u5d4c\u5165\u6765\u5f97\u51fa\u6295\u5f71\u5206\u6570\u3002\u57281,093\u540d\u53c2\u4e0e\u8005\u7684\u72ec\u7acb\u961f\u5217\u4e2d\u8fdb\u884c\u4e86\u5916\u90e8\u9a8c\u8bc1\u3002\u76f8\u5173\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "result": "\u6295\u5f71\u5206\u6570\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u7684\u72ec\u7279\u4e14\u5177\u4e34\u5e8a\u610f\u4e49\u7684\u6a21\u5f0f\u3002ECG\u7279\u5f81\u80fd\u9884\u6d4b\u65e2\u5f80\u548c\u65b0\u53d1\u5fc3\u810f\u75c5\uff0c\u5c24\u5176\u662fCVD\u6b7b\u4ea1\u7387\uff1bEEG\u7279\u5f81\u80fd\u9884\u6d4b\u65b0\u53d1\u9ad8\u8840\u538b\u548cCVD\u6b7b\u4ea1\u7387\uff1b\u547c\u5438\u4fe1\u53f7\u63d0\u4f9b\u4e86\u8865\u5145\u9884\u6d4b\u4ef7\u503c\u3002\u5c06\u8fd9\u4e9b\u6295\u5f71\u5206\u6570\u4e0eFramingham\u98ce\u9669\u8bc4\u5206\u7ed3\u5408\uff0c\u9884\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff08\u4e0d\u540c\u7ed3\u679c\u7684AUC\u503c\u57280.607\u81f30.965\u4e4b\u95f4\uff09\u3002\u7814\u7a76\u7ed3\u679c\u5728\u5916\u90e8\u6d4b\u8bd5\u961f\u5217\u4e2d\u5f97\u5230\u4e86\u9c81\u68d2\u6027\u590d\u5236\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u76f4\u63a5\u4ecePSG\u6570\u636e\u751f\u6210\u4e2a\u4f53\u5316\u7684CVD\u98ce\u9669\u8bc4\u5206\u3002\u8fd9\u4e9b\u6295\u5f71\u5206\u6570\u6709\u6f5c\u529b\u88ab\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u4ee5\u589e\u5f3a\u98ce\u9669\u8bc4\u4f30\u5e76\u652f\u6301\u4e2a\u6027\u5316\u62a4\u7406\u3002"}}
{"id": "2507.09248", "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAGCD-Net\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u5408\u65b0\u578b\u5377\u79ef\u7f16\u7801\u5668\uff08Hybrid ConvNeXt\uff09\u548c\u57fa\u4e8e\u56e0\u679c\u5e72\u9884\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u6a21\u5757\uff08AG-CIM\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u60c5\u5883\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e0a\u4e0b\u6587\u611f\u77e5\u60c5\u611f\u8bc6\u522b\uff08CAER\uff09\u65b9\u6cd5\u5b58\u5728\u60c5\u5883\u504f\u89c1\u95ee\u9898\uff0c\u5373\u80cc\u666f\u60c5\u5883\u4e0e\u60c5\u611f\u6807\u7b7e\u4e4b\u95f4\u5b58\u5728\u865a\u5047\u5173\u8054\uff0c\u5bfc\u81f4\u8bc6\u522b\u4e0d\u51c6\u786e\u3002", "method": "\u672c\u6587\u63d0\u51faAGCD-Net\u6a21\u578b\uff0c\u5305\u542b\uff1a1) Hybrid ConvNeXt\uff0c\u4e00\u4e2a\u65b0\u578b\u5377\u79ef\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6574\u5408Spatial Transformer Network\u548cSqueeze-and-Excitation\u5c42\u589e\u5f3a\u7279\u5f81\u6821\u51c6\u30022) Attention Guided - Causal Intervention Module (AG-CIM)\uff0c\u6838\u5fc3\u6a21\u5757\uff0c\u8fd0\u7528\u56e0\u679c\u7406\u8bba\u6270\u52a8\u60c5\u5883\u7279\u5f81\uff0c\u9694\u79bb\u865a\u5047\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u9762\u90e8\u7279\u5f81\u8fdb\u884c\u6ce8\u610f\u529b\u9a71\u52a8\u6821\u6b63\u4ee5\u51cf\u8f7b\u60c5\u5883\u504f\u89c1\u3002", "result": "\u5728CAER-S\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAGCD-Net\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AGCD-Net\u6709\u6548\u89e3\u51b3\u4e86\u60c5\u5883\u504f\u89c1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u60c5\u611f\u8bc6\u522b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u56e0\u679c\u53bb\u504f\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.09777", "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr\u00e9 Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "\u672c\u7814\u7a76\u4fee\u8ba2\u4e86\u70b9\u51fb\u8bf1\u9975\u7684\u5b9a\u4e49\uff0c\u63d0\u51fa\u4ee5\u201c\u597d\u5947\u5fc3\u5dee\u8ddd\u201d\u4e3a\u6838\u5fc3\u7684\u65b0\u5b9a\u4e49\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u897f\u73ed\u7259\u8bed\u5f00\u653e\u6570\u636e\u96c6TA1C\uff0c\u4e3a\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u70b9\u51fb\u8bf1\u9975\u7684\u5b9a\u4e49\u7f3a\u4e4f\u5171\u8bc6\uff0c\u96be\u4ee5\u4e0e\u54d7\u4f17\u53d6\u5ba0\u7b49\u73b0\u8c61\u533a\u5206\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u68c0\u6d4b\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u3001\u64cd\u4f5c\u6027\u5f3a\u7684\u70b9\u51fb\u8bf1\u9975\u5b9a\u4e49\uff0c\u5e76\u4e3a\u76f8\u5173\u7814\u7a76\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u5f00\u653e\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u9996\u5148\u4fee\u8ba2\u4e86\u70b9\u51fb\u8bf1\u9975\u7684\u5b9a\u4e49\uff0c\u5c06\u5176\u6838\u5fc3\u7279\u5f81\u754c\u5b9a\u4e3a\u201c\u6545\u610f\u7701\u7565\u90e8\u5206\u4fe1\u606f\u4ee5\u5236\u9020\u597d\u5947\u5fc3\u5dee\u8ddd\u201d\u3002\u57fa\u4e8e\u6b64\u65b0\u5b9a\u4e49\uff0c\u63d0\u51fa\u5e76\u5b9e\u65bd\u4e86\u4e00\u5957\u51cf\u5c11\u4e3b\u89c2\u6027\u7684\u6570\u636e\u96c6\u521b\u5efa\u548c\u6807\u6ce8\u6807\u51c6\u3002\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u6765\u81ea18\u4e2a\u5a92\u4f53\u6765\u6e90\u76843,500\u6761\u897f\u73ed\u7259\u8bed\u63a8\u6587\uff0c\u521b\u5efa\u5e76\u53d1\u5e03\u4e86TA1C\u6570\u636e\u96c6\u3002\u6700\u540e\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u57fa\u7ebf\u6a21\u578b\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u57fa\u4e8e\u201c\u597d\u5947\u5fc3\u5dee\u8ddd\u201d\u7684\u70b9\u51fb\u8bf1\u9975\u65b0\u5b9a\u4e49\u3002\u521b\u5efa\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u897f\u73ed\u7259\u8bed\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u5f00\u653e\u6570\u636e\u96c6TA1C\uff0c\u5305\u542b3,500\u6761\u63a8\u6587\uff0c\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u8fbe\u52300.825 Fleiss' K\u3002\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u83b7\u5f97\u4e860.84\u7684F1\u5206\u6570\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u660e\u786e\u7684\u70b9\u51fb\u8bf1\u9975\u5b9a\u4e49\u548c\u9ad8\u8d28\u91cf\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2507.10142", "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "AI": {"tldr": "\u9488\u5bf9MARL\u5728\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u73af\u5883\u90e8\u7f72\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u201c\u9002\u5e94\u6027\u201d\u6982\u5ff5\u53ca\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u7cfb\u7edf\u5730\u8861\u91cf\u5176\u5728\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u6a21\u62df\u548c\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u7684\u90e8\u7f72\u53d7\u9650\u3002\u4e3b\u8981\u539f\u56e0\u662f\u771f\u5b9e\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u5b58\u5728\u667a\u80fd\u4f53\u6570\u91cf\u6ce2\u52a8\u3001\u4efb\u52a1\u76ee\u6807\u6f14\u53d8\u548c\u6267\u884c\u6761\u4ef6\u4e0d\u4e00\u81f4\u7b49\u53d8\u5f02\u6e90\uff0c\u8981\u6c42MARL\u7b97\u6cd5\u5728\u6301\u7eed\u53d8\u5316\u4e2d\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u3002", "method": "\u5f15\u5165\u201c\u9002\u5e94\u6027\u201d\u6982\u5ff5\uff0c\u4f5c\u4e3a\u8bc4\u4f30MARL\u7b97\u6cd5\u5728\u53d8\u5316\u6761\u4ef6\uff08\u5b66\u4e60\u6216\u6267\u884c\u671f\u95f4\u73af\u5883\u52a8\u529b\u5b66\u53d8\u5316\uff09\u4e0b\u53ef\u9760\u6027\u7684\u7edf\u4e00\u89c6\u89d2\u3002\u56f4\u7ed5\u9002\u5e94\u6027\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u5b66\u4e60\u9002\u5e94\u6027\u3001\u7b56\u7565\u9002\u5e94\u6027\u548c\u573a\u666f\u9a71\u52a8\u9002\u5e94\u6027\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u9002\u5e94\u6027\u89c6\u89d2\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u5bf9MARL\u6027\u80fd\u8fdb\u884c\u66f4\u5177\u539f\u5219\u6027\u7684\u8bc4\u4f30\uff0c\u8d85\u8d8a\u72ed\u4e49\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u672c\u8c03\u67e5\u7814\u7a76\u81f4\u529b\u4e8e\u63a8\u52a8\u5f00\u53d1\u66f4\u9002\u7528\u4e8e\u52a8\u6001\u3001\u771f\u5b9e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u90e8\u7f72\u7684MARL\u7b97\u6cd5\u3002"}}
{"id": "2507.09016", "pdf": "https://arxiv.org/pdf/2507.09016", "abs": "https://arxiv.org/abs/2507.09016", "authors": ["Karim Galliamov", "Ivan Titov", "Ilya Pershin"], "title": "Enhancing RLHF with Human Gaze Modeling", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u4fe1\u606f\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684RLHF\uff0c\u901a\u8fc7\u6ce8\u89c6\u611f\u77e5\u5956\u52b1\u6a21\u578b\u548c\u57fa\u4e8e\u6ce8\u89c6\u7684\u7a00\u758f\u5956\u52b1\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u5e76\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e0e\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u5728\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63a2\u7d22\u4e86\u4e24\u79cd\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u5efa\u6a21\u589e\u5f3aRLHF\u7684\u65b9\u6cd5\uff1a1) \u6ce8\u89c6\u611f\u77e5\u7684\u5956\u52b1\u6a21\u578b\uff1b2) \u57fa\u4e8e\u6ce8\u89c6\u7684\u7a00\u758f\u5956\u52b1\u5728token\u7ea7\u522b\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6ce8\u89c6\u589e\u5f3a\u7684RLHF\u5728\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6536\u655b\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u4eba\u7c7b\u6ce8\u89c6\u662f\u7b56\u7565\u4f18\u5316\u4e2d\u4e00\u4e2a\u6709\u4ef7\u503c\u4e14\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u4fe1\u53f7\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8RLHF\u7684\u6548\u7387\u3002"}}
{"id": "2507.09256", "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "\u9488\u5bf9\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4e2d\u9ad8\u9636\u5173\u8054\u548c\u8bed\u4e49\u6b67\u4e49\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86AAHR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u5bf9\u6bd4\u5b66\u4e60\u3001\u591a\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u90bb\u91cc\u5173\u7cfb\u5efa\u6a21\u53ca\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf-\u6587\u672c\u5339\u914d\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u76f8\u4f3c\u5b9e\u4f8b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u548c\u8bed\u4e49\u6b67\u4e49\uff08\u5982\u8f6f\u6b63\u8d1f\u6837\u672c\uff09\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6279\u6b21\u5185\u7684\u90bb\u91cc\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5b66\u4e60\u9ad8\u9636\u5171\u4eab\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51faAAHR\u6846\u67b6\uff0c\u5176\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\uff1a1) \u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\u6784\u5efa\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff0c\u7f13\u89e3\u8f6f\u6b63\u6837\u672c\u95ee\u9898\uff1b2) \u5f15\u5165\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u673a\u5236\u53ca\u81ea\u9002\u5e94\u805a\u5408\u7f51\u7edc\uff0c\u589e\u5f3a\u5168\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\uff1b3) \u91c7\u7528\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u5173\u8054\u77e9\u9635\u7ed3\u5408GNN\uff0c\u6df1\u5165\u63a2\u7d22\u5e76\u589e\u5f3a\u5b9e\u4f8b\u95f4\u7684\u90bb\u91cc\u5173\u7cfb\u548c\u8bed\u4e49\u4ea4\u4e92\uff1b4) \u6574\u5408\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u6269\u5c55\u8d1f\u6837\u672c\u96c6\uff0c\u63d0\u5347\u7279\u5f81\u8fa8\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAAHR\u5728Flickr30K\u3001MSCOCO\u548cECCV Caption\u6570\u636e\u96c6\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf-\u6587\u672c\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "AAHR\u6846\u67b6\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u8bed\u4e49\u6b67\u4e49\u548c\u5145\u5206\u5229\u7528\u9ad8\u9636\u5173\u8054\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf-\u6587\u672c\u5339\u914d\u7684\u6027\u80fd\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09875", "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u201c\u504f\u4e00\u52a0\u6cd5\u201d\u4f5c\u4e3a\u6848\u4f8b\uff0c\u901a\u8fc7\u7535\u8def\u98ce\u683c\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u9a71\u52a8\u4efb\u52a1\u7ea7\u6cdb\u5316\u7684\u201c\u51fd\u6570\u5f52\u7eb3\u673a\u5236\u201d\uff0c\u5e76\u53d1\u73b0\u8be5\u673a\u5236\u5177\u6709\u53ef\u91cd\u7528\u6027\u548c\u53ef\u7ec4\u5408\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6267\u884c\u672a\u89c1\u4efb\u52a1\uff0c\u4f46\u5176\u5185\u90e8\u5b9e\u73b0\u8fd9\u79cd\u4efb\u52a1\u7ea7\u6cdb\u5316\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u7814\u7a76\u4ee5\u201c\u504f\u4e00\u52a0\u6cd5\u201d\uff08\u59821+1=3\uff09\u8fd9\u4e00\u53cd\u4e8b\u5b9e\u4efb\u52a1\u4e3a\u4f8b\uff0c\u5229\u7528\u8def\u5f84\u4fee\u8865\uff08path patching\uff09\u7b49\u7535\u8def\u98ce\u683c\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u5206\u6790\u6a21\u578b\u5185\u90e8\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "1. \u53d1\u73b0\u4e86\u4e00\u79cd\u201c\u51fd\u6570\u5f52\u7eb3\u673a\u5236\u201d\uff0c\u89e3\u91ca\u4e86\u6a21\u578b\u4ece\u6807\u51c6\u52a0\u6cd5\u6cdb\u5316\u5230\u504f\u4e00\u52a0\u6cd5\u7684\u80fd\u529b\uff0c\u8be5\u673a\u5236\u7c7b\u4f3c\u4e8e\u5e76\u63d0\u5347\u4e86\u5148\u524d\u7684\u5f52\u7eb3\u5934\u673a\u5236\u3002\n2. \u8868\u660e+1\u51fd\u6570\u7684\u5f52\u7eb3\u7531\u591a\u4e2a\u5e76\u884c\u6ce8\u610f\u529b\u5934\u5171\u540c\u63a7\u5236\uff0c\u6bcf\u4e2a\u5934\u8d21\u732e+1\u51fd\u6570\u7684\u4e00\u90e8\u5206\u3002\n3. \u53d1\u73b0\u8be5\u51fd\u6570\u5f52\u7eb3\u673a\u5236\u5728\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u88ab\u91cd\u7528\uff0c\u5305\u62ec\u79fb\u4f4d\u9009\u62e9\u9898\u95ee\u7b54\u548c\u516b\u8fdb\u5236\u52a0\u6cd5\u7b49\u7b97\u6cd5\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6df1\u5165\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u7ed3\u6784\u5982\u4f55\u5b9e\u73b0\u4efb\u52a1\u7ea7\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10156", "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "categories": ["cs.AI"], "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86SwissFKG\uff08\u745e\u58eb\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\uff09\uff0c\u4e00\u4e2a\u6574\u5408\u745e\u58eb\u98df\u8c31\u3001\u914d\u6599\u3001\u8425\u517b\u6570\u636e\u548c\u4e2a\u4f53\u996e\u98df\u9700\u6c42\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4e30\u5bcc\u548c\u67e5\u8be2\uff0c\u65e8\u5728\u6784\u5efa\u66f4\u5168\u9762\u7684\u81b3\u98df\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684\u81b3\u98df\u8bc4\u4f30\u7cfb\u7edf\u5ffd\u89c6\u4e86\u975e\u89c6\u89c9\u56e0\u7d20\uff08\u5982\u914d\u6599\u66ff\u6362\uff09\u548c\u4e2a\u4f53\u996e\u98df\u9700\u6c42\uff08\u5982\u8fc7\u654f\u3001\u9650\u5236\u3001\u6587\u5316\u4e60\u4fd7\uff09\u3002\u6b64\u5916\uff0c\u745e\u58eb\u7684\u98df\u54c1\u4fe1\u606f\u5206\u6563\uff0c\u7f3a\u4e4f\u96c6\u4e2d\u7684\u3001\u6574\u5408\u6240\u6709\u76f8\u5173\u8425\u517b\u4fe1\u606f\u7684\u5b58\u50a8\u5e93\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86SwissFKG\uff0c\u6574\u5408\u4e86\u98df\u8c31\u3001\u914d\u6599\u3001\u66ff\u6362\u7269\u3001\u8425\u517b\u6570\u636e\u3001\u996e\u98df\u9650\u5236\u3001\u8fc7\u654f\u4fe1\u606f\u548c\u56fd\u5bb6\u8425\u517b\u6307\u5357\u3002\u4ed6\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u7531LLM\u9a71\u52a8\u7684\u77e5\u8bc6\u56fe\u8c31\u4e30\u5bcc\u7ba1\u9053\uff0c\u5e76\u9996\u6b21\u5bf9\u56db\u79cdLLM\uff08\u53c2\u6570\u5c0f\u4e8e70B\uff09\u5728\u98df\u54c1\u77e5\u8bc6\u589e\u5f3a\u65b9\u9762\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2aGraph-RAG\uff08\u56fe\u589e\u5f3a\u68c0\u7d22\uff09\u5e94\u7528\uff0c\u4ee5\u5c55\u793aSwissFKG\u5982\u4f55\u5e2e\u52a9LLM\u56de\u7b54\u7528\u6237\u7279\u5b9a\u7684\u8425\u517b\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83LLM-\u5d4c\u5165\u5bf9\u7684\u7528\u6237\u67e5\u8be2\u54cd\u5e94\u4e0e\u9884\u5b9a\u4e49\u7b54\u6848\u6765\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u80fd\u591f\u6709\u6548\u5730\u4e30\u5bcc\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u76f8\u5173\u8425\u517b\u4fe1\u606f\u3002SwissFKG\u4e0d\u4ec5\u63d0\u4f9b\u98df\u8c31\u63a8\u8350\uff0c\u8fd8\u80fd\u63d0\u4f9b\u914d\u6599\u5c42\u9762\u7684\u8fc7\u654f\u539f\u548c\u996e\u98df\u9650\u5236\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u7b26\u5408\u8425\u517b\u6307\u5357\u7684\u6307\u5bfc\u3002Graph-RAG\u5e94\u7528\u6210\u529f\u5c55\u793a\u4e86SwissFKG\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u6570\u636e\u7ed3\u6784\u5982\u4f55\u5e2e\u52a9LLM\u56de\u7b54\u7528\u6237\u7279\u5b9a\u7684\u8425\u517b\u67e5\u8be2\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u81b3\u98df\u8bc4\u4f30\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u5de5\u5177\u5c06\u878d\u5408\u996e\u98df\u7684\u89c6\u89c9\u3001\u8bed\u5883\u548c\u6587\u5316\u7ef4\u5ea6\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u4e2a\u6027\u5316\u7684\u8425\u517b\u6307\u5bfc\u3002"}}
{"id": "2507.09019", "pdf": "https://arxiv.org/pdf/2507.09019", "abs": "https://arxiv.org/abs/2507.09019", "authors": ["Amey Agrawal", "Nitin Kedia", "Anmol Agarwal", "Jayashree Mohan", "Nipun Kwatra", "Souvik Kundu", "Ramachandran Ramjee", "Alexey Tumanov"], "title": "On Evaluating Performance of LLM Inference Serving Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u666e\u904d\u7f3a\u9677\u548c\u201c\u53cd\u6a21\u5f0f\u201d\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7efc\u5408\u6027\u6e05\u5355\u548c\u6846\u67b6\uff0c\u65e8\u5728\u7ea0\u6b63\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u7684\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1LLM\u63a8\u7406\u7cfb\u7edf\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u51c6\u786e\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u8fdb\u6b65\u3002\u7279\u522b\u662f\u5728LLM\u63a8\u7406\u7684\u53cc\u9636\u6bb5\u3001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u53ca\u4e25\u683c\u65f6\u95f4\u8981\u6c42\u4e0b\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u901a\u8fc7\u5bf9\u8fd1\u671f\u7cfb\u7edf\u7684\u7cfb\u7edf\u5206\u6790\u548c\u5168\u9762\u68c0\u67e5\uff0c\u8bc6\u522b\u51fa\u57fa\u7ebf\u516c\u5e73\u6027\u3001\u8bc4\u4f30\u8bbe\u7f6e\u548c\u5ea6\u91cf\u8bbe\u8ba1\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u5e38\u89c1\u8bc4\u4f30\u53cd\u6a21\u5f0f\u3002\u672c\u6587\u8bba\u8bc1\u4e86\u8fd9\u4e9b\u53cd\u6a21\u5f0f\u5982\u4f55\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u6e05\u5355\u548c\u6846\u67b6\uff0c\u4ee5\u8bc6\u522b\u5e76\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u63a8\u7406\u8bc4\u4f30\u4e2d\u5e38\u89c1\u7684\u201c\u53cd\u6a21\u5f0f\u201d\u53ca\u5176\u5bf9\u7ed3\u679c\u7684\u8bef\u5bfc\u6027\u5f71\u54cd\u3002\u8bc6\u522b\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u4e09\u5927\u53cd\u6a21\u5f0f\u7ef4\u5ea6\u3002\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6e05\u5355\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u907f\u514d\u8fd9\u4e9b\u7f3a\u9677\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u590d\u73b0\u7684\u6027\u80fd\u8bc4\u4f30\u3002\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5206\u6790\u590d\u6742\u63a8\u7406\u6280\u672f\uff08\u5982\u63a8\u6d4b\u89e3\u7801\uff09\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u63a8\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5960\u5b9a\u4e86\u4e25\u683c\u7684\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u548c\u7ed3\u679c\u590d\u73b0\u6027\u3002\u901a\u8fc7\u7ea0\u6b63\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7f3a\u9677\u5e76\u4f7f\u5176\u4e0e\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u5bf9\u9f50\uff0c\u6709\u671b\u52a0\u901fLLM\u63a8\u7406\u6280\u672f\u7684\u771f\u6b63\u8fdb\u6b65\u3002"}}
{"id": "2507.09266", "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "categories": ["cs.CV"], "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6bb5\u611f\u77e5\u89c6\u89c9\u5206\u8bcd\u6846\u67b6\u53ca\u53cc\u7ea7\u5bf9\u9f50\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u6807\u6ce8\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u4e2d\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u548c\u8ba1\u7b97\u9700\u6c42\u5927\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728PHOENIX14T\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65e0\u6807\u6ce8\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u666e\u904d\u4f34\u968f\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u548c\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u5bfc\u81f4\u5728\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "1. \u63d0\u51fa\u5206\u6bb5\u611f\u77e5\u89c6\u89c9\u5206\u8bcd\u6846\u67b6\uff0c\u5229\u7528\u624b\u8bed\u5206\u6bb5\u5c06\u8fde\u7eed\u89c6\u9891\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u3001\u624b\u8bed\u4fe1\u606f\u5316\u7684\u89c6\u89c9token\u3002\n2. \u5f15\u5165token-to-token\u5bf9\u6bd4\u5bf9\u9f50\u76ee\u6807\u3002\n3. \u91c7\u7528\u53cc\u7ea7\u76d1\u7763\u673a\u5236\uff0c\u540c\u65f6\u5bf9\u9f50\u8bed\u8a00\u5d4c\u5165\u548c\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "1. \u76f8\u8f83\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe50%\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u964d\u4f4e2.67\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3002\n2. \u5728PHOENIX14T\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\n3. \u5728\u4e0d\u4f9d\u8d56\u8bcd\u6c47\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u6539\u8fdb\u4e86\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\n4. \u5728\u53ef\u6bd4\u8f83\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0c\u6027\u80fd\u4ecd\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u5206\u8bcd\u548c\u5bf9\u9f50\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u6bb5\u611f\u77e5\u89c6\u89c9\u5206\u8bcd\u548c\u53cc\u7ea7\u5bf9\u9f50\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u6807\u6ce8\u624b\u8bed\u7ffb\u8bd1\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.09935", "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u751f\u6210\u66f4\u5177\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u77e5\u8bc6\u5757\uff0c\u4ece\u800c\u63d0\u9ad8\u4fe1\u606f\u68c0\u7d22\u7684\u7cbe\u786e\u5ea6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5757\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684RAG\u5206\u5757\u7b56\u7565\u672a\u80fd\u6709\u6548\u6355\u83b7\u8db3\u591f\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u56e0\u4e3a\u5b83\u5ffd\u89c6\u4e86\u6587\u672c\u7684\u5e95\u5c42\u7ed3\u6784\uff0c\u5bfc\u81f4\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u4e0d\u591f\u7cbe\u786e\u548c\u9886\u57df\u76f8\u5173\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u6765\u751f\u6210\u66f4\u5177\u610f\u4e49\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u77e5\u8bc6\u5757\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6bb5\u843d\u7ea7\u548c\u805a\u7c7b\u7ea7\u7684\u5411\u91cf\u8868\u793a\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u3002", "result": "\u5728NarrativeQA\u3001QuALITY\u548cQASPER\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u5206\u5757\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3aRAG\u7cfb\u7edf\uff0c\u751f\u6210\u66f4\u7cbe\u786e\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u77e5\u8bc6\u5757\uff0c\u4ece\u800c\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2507.10174", "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\uff0c\u7b80\u5355\u76f4\u63a5\u7684Filtered Behavior Cloning (FBC) \u65b9\u6cd5\u6bd4Decision Transformer (DT) \u8868\u73b0\u66f4\u4f18\u6216\u76f8\u5f53\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\uff0c\u4ece\u800c\u8d28\u7591\u4e86DT\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u666e\u9002\u6027\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\uff08Bhargava et al., 2024\uff09\u58f0\u79f0Decision Transformer (DT) \u5728\u7a00\u758f\u5956\u52b1\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684MLP\u57fa\u7ebf\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e00\u8bf4\u6cd5\uff0c\u5e76\u63a2\u7d22\u662f\u5426\u5b58\u5728\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1 (Robomimic) \u548c\u8fd0\u52a8\u57fa\u51c6 (D4RL) \u4e0a\u8fdb\u884c\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u6bd4\u8f83MLP-based Filtered Behavior Cloning (FBC) \u4e0eDecision Transformer (DT) \u7684\u6027\u80fd\u3002FBC\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u7b80\u5355\u5730\u4ece\u6570\u636e\u96c6\u4e2d\u8fc7\u6ee4\u6389\u4f4e\u8868\u73b0\u8f68\u8ff9\uff0c\u7136\u540e\u5bf9\u5269\u4f59\u7684\u8fc7\u6ee4\u6570\u636e\u96c6\u8fdb\u884c\u884c\u4e3a\u514b\u9686\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMLP-based FBC\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4e0eDT\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cFBC\u65b9\u6cd5\u66f4\u4e3a\u76f4\u63a5\u3001\u6240\u9700\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDecision Transformer (DT) \u4e0d\u9002\u7528\u4e8e\u7a00\u758f\u5956\u52b1\u73af\u5883\u3002\u7ed3\u5408\u5148\u524d\u7684\u7814\u7a76\uff0cDT\u4e5f\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5bc6\u96c6\u5956\u52b1\u73af\u5883\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u201cDT\u662f\u5426\u5728\u4efb\u4f55\u60c5\u51b5\u4e0b\u90fd\u5177\u6709\u4f18\u52bf\uff1f\u201d\u7684\u7591\u95ee\uff0c\u6697\u793a\u4e86\u5176\u5e94\u7528\u8303\u56f4\u53ef\u80fd\u4e0d\u5982\u9884\u671f\u7684\u5e7f\u6cdb\u3002"}}
{"id": "2507.09029", "pdf": "https://arxiv.org/pdf/2507.09029", "abs": "https://arxiv.org/abs/2507.09029", "authors": ["Vaibhav Singh", "Zafir Khalid", "Edouard Oyallon", "Eugene Belilovsky"], "title": "Model Parallelism With Subnetwork Data Parallelism", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 1 figure", "summary": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u8282\u70b9\u4e0a\u8bad\u7ec3\u5c0f\u578b\u5b50\u7f51\u7edc\uff0c\u6709\u6548\u964d\u4f4e\u5927\u89c4\u6a21\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u5185\u5b58\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u5bf9\u5355\u4e2a\u8282\u70b9\u5185\u5b58\u8981\u6c42\u9ad8\uff0c\u4e14\u8282\u70b9\u5185\u901a\u4fe1\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff1a\u5728\u72ec\u7acb\u5de5\u4f5c\u8282\u70b9\u4e0a\u8bad\u7ec3\u6a21\u578b\u7684\u5c0f\u578b\u7ed3\u6784\u5316\u5b50\u7f51\u7edc\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u8282\u70b9\u95f4\u6fc0\u6d3b\u901a\u4fe1\uff0c\u5e26\u5bbd\u9700\u6c42\u4e0eall-reduce\u6570\u636e\u5e76\u884c\u65b9\u6848\u76f8\u5f53\u6216\u66f4\u4f4e\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u5b50\u7f51\u7edc\u6784\u5efa\u7b56\u7565\uff1a\u968f\u673a\u5757\u4e22\u5f03\uff08stochastic block dropping\uff09\u548c\u5bbd\u5ea6\u65b9\u5411\u5b50\u7f51\u7edc\u6784\u5efa\uff08width-wise subnetwork construction\uff09\u3002", "result": "\u968f\u673a\u5757\u4e22\u5f03\u6280\u672f\u6301\u7eed\u4f18\u4e8e\u5bbd\u5ea6\u65b9\u5411\u5b50\u7f51\u7edc\u6784\u5efa\u3002\u6027\u80fd\u4f18\u52bf\u5f52\u56e0\u4e8e\u4fdd\u7559\u4e86\u5e26\u6709\u8df3\u8dc3\u8fde\u63a5\u7684\u5757\u7684\u5b50\u7f51\u7edc\u4e2d\u66f4\u5f3a\u7684\u68af\u5ea6\u5bf9\u9f50\u3002\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120-40%\uff0c\u4e14\u6027\u80fd\u65e0\u635f\u5931\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09269", "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6cd5\uff0c\u5229\u7528RGB\u6570\u636e\u548cANNs\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002CKD\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u6ed1\u52a8\u66ff\u6362\u548c\u95f4\u63a5\u5206\u9636\u6bb5\u84b8\u998f\u89e3\u51b3\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u6311\u6218\uff0c\u5e76\u5728\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u6807\u6ce8\u4e8b\u4ef6\u57fa\u6570\u636e\u96c6\u7a00\u7f3a\u548cSNN\u67b6\u6784\u4e0d\u6210\u719f\uff0c\u5176\u6027\u80fd\u4ecd\u900a\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347SNNs\u5728DVS\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u5229\u7528RGB\u6570\u636e\u548c\u9ad8\u6027\u80fdANNs\u5bf9SNNs\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\u4e3a\u89e3\u51b3\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u6311\u6218\uff0cCKD\u65b9\u6848\u7ed3\u5408\u4e86\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u6ed1\u52a8\u66ff\u6362\u4ee5\u7f13\u89e3\u8de8\u6a21\u6001\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u95f4\u63a5\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6765\u5e94\u5bf9\u8de8\u67b6\u6784\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728N-Caltech101\u548cCEP-DVS\u7b49\u4e3b\u6d41\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\uff08State-of-the-Art\uff09\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6cd5\u6709\u6548\u5730\u63d0\u5347\u4e86SNNs\u5728DVS\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u6210\u529f\u514b\u670d\u4e86\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u7684\u96be\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u9886\u5148\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3aSNNs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.09973", "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "categories": ["cs.CL", "cs.AI"], "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faTinyRM\uff0c\u4e00\u79cd\u5c0f\u53c2\u6570\uff084\u4ebf\uff09\u7684\u53cc\u5411\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u5176\u5728\u63a8\u7406\u548c\u5b89\u5168\u504f\u597d\u5efa\u6a21\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0e\u5927175\u500d\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5927\u578b\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u5728RLHF\u5956\u52b1\u5efa\u6a21\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u65e5\u76ca\u6210\u4e3a\u90e8\u7f72\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\u3002", "method": "\u5f15\u5165TinyRM\uff0c\u4e00\u4e2a\u7531\u5c0f\u578b\uff08\u6700\u5c0f4\u4ebf\u53c2\u6570\uff09\u53cc\u5411\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff08MLMs\uff09\u7ec4\u6210\u7684\u5bb6\u65cf\u3002\u901a\u8fc7\u7ed3\u5408FLAN\u98ce\u683c\u63d0\u793a\u3001\u5b9a\u5411\u4f4e\u79e9\u9002\u5e94\uff08DoRA\uff09\u548c\u5c42\u51bb\u7ed3\u7b49\u6280\u672f\uff0c\u5728RewardBench\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "result": "TinyRM\u5728\u63a8\u7406\u548c\u5b89\u5168\u504f\u597d\u5efa\u6a21\u4efb\u52a1\u4e0a\u4e0e\u6bd4\u5176\u5927175\u500d\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u76f8\u4e0a\u4e0b\u7684\u80fd\u529b\u3002\u5728RewardBench\u6d4b\u8bd5\u4e2d\uff0c\u5c3d\u7ba1\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff0cTinyRM\u4ecd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5c0f\u578b\u6a21\u578b\u53d7\u76ca\u4e8e\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u7b56\u7565\uff0c\u5c24\u5176\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u53cc\u5411\u67b6\u6784\u6709\u671b\u6210\u4e3a\u504f\u597d\u5efa\u6a21\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u5c3d\u7ba1\u5728\u901a\u7528\u6a21\u578b\u548c\u5bf9\u8bdd\u504f\u597d\u5efa\u6a21\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2507.10208", "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7814\u7a76\u7684\u77db\u76fe\u548c\u7f3a\u4e4f\u6307\u5bfc\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdXAI\u7814\u7a76\u5206\u7c7b\u65b9\u6cd5\u548c\u8be6\u7ec6\u7684\u7814\u7a76\u6307\u5357\uff0c\u65e8\u5728\u63d0\u5347\u9886\u57df\u5185\u7814\u7a76\u7684\u6e05\u6670\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5728\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u5b58\u5728\u5927\u91cf\u77db\u76fe\uff0c\u4e14\u7f3a\u4e4f\u5177\u4f53\u7684XAI\u8bbe\u8ba1\u5efa\u8bae\uff0c\u6839\u672c\u539f\u56e0\u662f\u5bf9\u9700\u8981AI\u534f\u52a9\u7684\u4efb\u52a1\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u53ef\u89c6\u5316\u5206\u6790\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u4eea\u8868\u76d8\u8bbe\u8ba1\u7b49\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u4ec0\u4e48\u3001\u4e3a\u4ec0\u4e48\u3001\u8c01\u201d\u4e09\u4e2a\u7ef4\u5ea6\u6765\u5206\u7c7b\u548c\u6bd4\u8f83XAI\u7814\u7a76\u7684\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51fa\u5f53\u524d\u7814\u7a76\u7684\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u4efb\u52a1\u63cf\u8ff0\u4e0d\u5145\u5206\u3001\u7814\u7a76\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u3001\u4ee5\u53ca\u76ee\u6807\u7528\u6237\u6d4b\u8bd5\u4e0d\u8db3\u3002", "conclusion": "\u5efa\u8bae\u7814\u7a76\u5e94\u8be6\u7ec6\u62a5\u544a\u7528\u6237\u7684\u9886\u57df\u3001AI\u548c\u6570\u636e\u5206\u6790\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u8bf4\u660e\u53d1\u73b0\u7684\u666e\u9002\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u548c\u62a5\u544aXAI\u4efb\u52a1\u7684\u7814\u7a76\u6307\u5357\uff0c\u4ee5\u5e2e\u52a9XAI\u793e\u533a\u66f4\u597d\u5730\u7406\u89e3\u548c\u53d1\u5c55\u8fd9\u4e00\u5feb\u901f\u589e\u957f\u7684\u9886\u57df\uff0c\u89e3\u51b3\u77db\u76fe\u7ed3\u679c\u3002"}}
{"id": "2507.09031", "pdf": "https://arxiv.org/pdf/2507.09031", "abs": "https://arxiv.org/abs/2507.09031", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR-MDN\u5c42\uff0c\u901a\u8fc7\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u53bb\u9664\u6df7\u6742\u53d8\u91cf\u5f71\u54cd\uff0c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4fc3\u8fdb\u516c\u5e73\u9884\u6d4b\u3002", "motivation": "\u5728\u6301\u7eed\u5b66\u4e60\u80cc\u666f\u4e0b\uff0c\u5b66\u4e60\u5bf9\u6df7\u6742\u53d8\u91cf\u4e0d\u53d8\u7684\u7279\u5f81\u8868\u793a\u4ecd\u7136\u662f\u5de8\u5927\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u5143\u6570\u636e\u5f52\u4e00\u5316\uff08MDN\uff09\u672a\u80fd\u6709\u6548\u89e3\u51b3\u3002\u6df7\u6742\u53d8\u91cf\u5bfc\u81f4\u865a\u5047\u5173\u8054\u548c\u6709\u504f\u9884\u6d4b\u3002", "method": "\u5f15\u5165\u9012\u5f52\u5143\u6570\u636e\u5f52\u4e00\u5316\uff08R-MDN\uff09\u5c42\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u6a21\u578b\u9636\u6bb5\u3002R-MDN\u901a\u8fc7\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6267\u884c\u7edf\u8ba1\u56de\u5f52\uff0c\u6301\u7eed\u66f4\u65b0\u5185\u90e8\u6a21\u578b\u72b6\u6001\uff0c\u4ee5\u9002\u5e94\u6570\u636e\u548c\u6df7\u6742\u53d8\u91cf\u4e0d\u65ad\u53d8\u5316\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cR-MDN\u901a\u8fc7\u51cf\u5c11\u7531\u6df7\u6742\u53d8\u91cf\u6548\u5e94\u968f\u65f6\u95f4\u53d8\u5316\u5f15\u8d77\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728\u9759\u6001\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u7684\u4e0d\u540c\u9636\u6bb5\u90fd\u4fc3\u8fdb\u4e86\u8de8\u4eba\u7fa4\u7ec4\u7684\u516c\u5e73\u9884\u6d4b\u3002", "conclusion": "R-MDN\u5c42\u80fd\u6709\u6548\u5904\u7406\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6df7\u6742\u53d8\u91cf\u5f71\u54cd\uff0c\u4ece\u800c\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u9884\u6d4b\u516c\u5e73\u6027\u3002"}}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u533b\u7597\u9886\u57df\u4e2d\u5b58\u5728\u7684\u63d0\u793a\u654f\u611f\u6027\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u9519\u8bef\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faPrompt4Trust\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63d0\u793a\u589e\u5f3a\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347MLLMs\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u5728\u533b\u7597VQA\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u53d7\u9650\u4e8e\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5bf9\u63d0\u793a\u8bbe\u8ba1\u7684\u654f\u611f\u6027\uff1b2) \u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u751f\u6210\u9519\u8bef\u54cd\u5e94\u7684\u503e\u5411\u3002\u7531\u4e8e\u4e34\u5e8a\u533b\u751f\u53ef\u80fd\u4f9d\u8d56\u6a21\u578b\u6240\u8868\u8fbe\u7684\u7f6e\u4fe1\u5ea6\u6765\u8bc4\u4f30\u5176\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u5f53\u6a21\u578b\u8868\u8fbe\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\uff0c\u5176\u9884\u6d4b\u4e5f\u5fc5\u987b\u9ad8\u5ea6\u51c6\u786e\uff0c\u8fd9\u6b63\u662f\u5f53\u524dMLLMs\u7684\u75db\u70b9\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Prompt4Trust\uff0c\u4e00\u4e2a\u9996\u4e2a\u9488\u5bf9MLLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u793a\u589e\u5f3a\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7LLM\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f85\u52a9\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u7528\u4e8e\u5f15\u5bfc\u4e0b\u6e38\u4efb\u52a1MLLM\uff0c\u4f7f\u5176\u751f\u6210\u7684\u54cd\u5e94\u4e2d\u8868\u8fbe\u7684\u7f6e\u4fe1\u5ea6\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u9884\u6d4b\u51c6\u786e\u6027\u3002\u4e0e\u4f20\u7edf\u6821\u51c6\u6280\u672f\u4e0d\u540c\uff0cPrompt4Trust\u7279\u522b\u4f18\u5148\u8003\u8651\u5bf9\u5b89\u5168\u548c\u53ef\u4fe1\u4e34\u5e8a\u51b3\u7b56\u6700\u5173\u952e\u7684\u6821\u51c6\u65b9\u9762\u3002", "result": "1. \u9664\u4e86\u63d0\u5347\u4e34\u5e8a\u9a71\u52a8\u7684\u6821\u51c6\u76ee\u6807\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3002\n2. \u5728PMC-VQA\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u7684\u591a\u9879\u9009\u62e9\u9898\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e86\u533b\u7597\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\u3002\n3. \u4f7f\u7528\u5c0f\u578b\u4e0b\u6e38\u4efb\u52a1MLLM\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5bf9\u5927\u578bMLLM\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u4e86\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u5373\u53ef\u5b9e\u73b0\u53ef\u6269\u5c55\u6821\u51c6\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5b89\u5168\u5173\u952e\u8bbe\u7f6e\u4e2d\u7684\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.09982", "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTextOmics\u57fa\u51c6\u6570\u636e\u96c6\u548cToDi\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9776\u70b9\u7279\u5f02\u6027\u836f\u7269\u53d1\u73b0\u4e2d\u7f3a\u4e4f\u5f02\u6784\u6570\u636e\u548c\u7edf\u4e00\u6846\u67b6\u7684\u95ee\u9898\u3002ToDi\u901a\u8fc7\u6574\u5408\u7ec4\u5b66\u8868\u8fbe\u548c\u5206\u5b50\u6587\u672c\u63cf\u8ff0\uff0c\u9ad8\u6548\u3001\u53ef\u63a7\u5730\u751f\u6210\u5177\u6709\u6cbb\u7597\u6f5c\u529b\u7684\u7c7b\u547d\u4e2d\u5206\u5b50\uff0c\u5e76\u8868\u73b0\u51fa\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u9776\u70b9\u7279\u5f02\u6027\u836f\u7269\u53d1\u73b0\u9700\u8981\u751f\u6210\u5177\u6709\u6cbb\u7597\u6f5c\u529b\u7684\u7c7b\u547d\u4e2d\u5206\u5b50\uff0c\u4f46\u76ee\u524d\u8be5\u9886\u57df\u7f3a\u4e4f\u5f02\u6784\u6570\u636e\u4ee5\u53ca\u6574\u5408\u591a\u79cd\u5206\u5b50\u8868\u793a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u6548\u548c\u76f8\u5173\u7684\u5206\u5b50\u751f\u6210\u3002", "method": "1. **TextOmics\u57fa\u51c6\uff1a** \u6784\u5efa\u4e86\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u5f02\u6784\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u7ec4\u5b66\u8868\u8fbe\u4e0e\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u4e00\u4e00\u5bf9\u5e94\u5173\u7cfb\uff0c\u65e8\u5728\u901a\u8fc7\u8868\u793a\u5bf9\u9f50\u4fc3\u8fdb\u5206\u5b50\u751f\u6210\u3002\n2. **ToDi\u751f\u6210\u6846\u67b6\uff1a** \u57fa\u4e8eTextOmics\uff0c\u63d0\u51fa\u4e86ToDi\u6846\u67b6\u3002\u8be5\u6846\u67b6\u8054\u5408\u4ee5\u7ec4\u5b66\u8868\u8fbe\u548c\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u751f\u6210\u751f\u7269\u5b66\u76f8\u5173\u3001\u5316\u5b66\u6709\u6548\u4e14\u5177\u6709\u6cbb\u7597\u6f5c\u529b\u7684\u7c7b\u547d\u4e2d\u5206\u5b50\u3002ToDi\u5229\u7528\u4e24\u4e2a\u7f16\u7801\u5668\uff08OmicsEn\u548cTextEn\uff09\u6355\u83b7\u591a\u5c42\u6b21\u7684\u751f\u7269\u5b66\u548c\u8bed\u4e49\u5173\u8054\uff0c\u5e76\u5f00\u53d1\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08DiffGen\uff09\u4ee5\u5b9e\u73b0\u53ef\u63a7\u751f\u6210\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86TextOmics\u7684\u6709\u6548\u6027\u3002ToDi\u6846\u67b6\u5728\u5206\u5b50\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u6cbb\u7597\u5206\u5b50\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6f5c\u529b\u3002", "conclusion": "ToDi\u6846\u67b6\u901a\u8fc7\u5f15\u5165TextOmics\u57fa\u51c6\u548c\u521b\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5206\u5b50\u751f\u6210\u9886\u57df\u4e2d\u6570\u636e\u5f02\u6784\u548c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u7684\u6311\u6218\u3002\u5b83\u80fd\u591f\u9ad8\u6548\u4e14\u53ef\u63a7\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7c7b\u547d\u4e2d\u5206\u5b50\uff0c\u4e3a\u9776\u70b9\u7279\u5f02\u6027\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10281", "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684\u8868\u683c\u667a\u80fd\u4f53\uff0c\u63a2\u8ba8\u5176\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u590d\u6742\u8868\u683c\u4e2d\u7684\u80fd\u529b\uff0c\u5b9a\u4e49\u4e86\u4e94\u9879\u6838\u5fc3\u7ade\u4e89\u529b\uff0c\u63ed\u793a\u4e86Text-to-SQL\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e72\u51c0\u7684\u5b66\u672f\u6570\u636e\u96c6\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u8868\u683c\u4efb\u52a1\u6d89\u53ca\u566a\u58f0\u3001\u7ed3\u6784\u5f02\u6784\u6027\u548c\u8bed\u4e49\u590d\u6742\u6027\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86LLM\u9a71\u52a8\u7684\u8868\u683c\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e94\u9879\u6838\u5fc3\u80fd\u529b\uff08\u8868\u683c\u7ed3\u6784\u7406\u89e3\u3001\u8868\u683c\u4e0e\u67e5\u8be2\u8bed\u4e49\u7406\u89e3\u3001\u8868\u683c\u68c0\u7d22\u4e0e\u538b\u7f29\u3001\u53ef\u8ffd\u6eaf\u6267\u884c\u63a8\u7406\u3001\u8de8\u57df\u6cdb\u5316\uff09\u6765\u5206\u6790\u548c\u6bd4\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8be6\u7ec6\u68c0\u67e5\u4e86Text-to-SQL\u667a\u80fd\u4f53\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cText-to-SQL\u667a\u80fd\u4f53\u5728\u5b66\u672f\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u4ee5\u63d0\u9ad8LLM\u9a71\u52a8\u7684\u8868\u683c\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.09041", "pdf": "https://arxiv.org/pdf/2507.09041", "abs": "https://arxiv.org/abs/2507.09041", "authors": ["Andrew Wagenmaker", "Zhiyuan Zhou", "Sergey Levine"], "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Developing autonomous agents that quickly explore an environment and adapt\ntheir behavior online is a canonical challenge in robotics and machine\nlearning. While humans are able to achieve such fast online exploration and\nadaptation, often acquiring new information and skills in only a handful of\ninteractions, existing algorithmic approaches tend to rely on random\nexploration and slow, gradient-based behavior updates. How can we endow\nautonomous agents with such capabilities on par with humans? Taking inspiration\nfrom recent progress on both in-context learning and large-scale behavioral\ncloning, in this work we propose behavioral exploration: training agents to\ninternalize what it means to explore and adapt in-context over the space of\n``expert'' behaviors. To achieve this, given access to a dataset of expert\ndemonstrations, we train a long-context generative model to predict expert\nactions conditioned on a context of past observations and a measure of how\n``exploratory'' the expert's behaviors are relative to this context. This\nenables the model to not only mimic the behavior of an expert, but also, by\nfeeding its past history of interactions into its context, to select different\nexpert behaviors than what have been previously selected, thereby allowing for\nfast online adaptation and targeted, ``expert-like'' exploration. We\ndemonstrate the effectiveness of our method in both simulated locomotion and\nmanipulation settings, as well as on real-world robotic manipulation tasks,\nillustrating its ability to learn adaptive, exploratory behavior.", "AI": {"tldr": "\u9488\u5bf9\u81ea\u4e3b\u667a\u80fd\u4f53\u5feb\u901f\u5728\u7ebf\u63a2\u7d22\u4e0e\u9002\u5e94\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u65b9\u6cd5\u3002\u901a\u8fc7\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\u5b66\u4e60\u4e13\u5bb6\u6f14\u793a\uff0c\u4f7f\u5176\u80fd\u5728\u4e0a\u4e0b\u6587\u4e2d\u6a21\u4eff\u5e76\u9009\u62e9\u4e0d\u540c\u4e13\u5bb6\u884c\u4e3a\uff0c\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u548c\u6709\u9488\u5bf9\u6027\u7684\u63a2\u7d22\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u5feb\u901f\u5728\u7ebf\u63a2\u7d22\u548c\u884c\u4e3a\u9002\u5e94\u65b9\u9762\u4e0d\u53ca\u4eba\u7c7b\uff0c\u540e\u8005\u80fd\u901a\u8fc7\u5c11\u91cf\u4ea4\u4e92\u8fc5\u901f\u83b7\u53d6\u65b0\u4fe1\u606f\u548c\u6280\u80fd\uff0c\u800c\u7b97\u6cd5\u5e38\u4f9d\u8d56\u968f\u673a\u63a2\u7d22\u548c\u7f13\u6162\u7684\u68af\u5ea6\u66f4\u65b0\u3002", "method": "\u672c\u6587\u53d7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u884c\u4e3a\u514b\u9686\u542f\u53d1\uff0c\u63d0\u51fa\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u3002\u5177\u4f53\u5730\uff0c\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u5176\u57fa\u4e8e\u8fc7\u53bb\u7684\u89c2\u6d4b\u4e0a\u4e0b\u6587\u548c\u4e13\u5bb6\u884c\u4e3a\u7684\u201c\u63a2\u7d22\u6027\u201d\u5ea6\u91cf\u6765\u9884\u6d4b\u4e13\u5bb6\u52a8\u4f5c\u3002\u8fd9\u4f7f\u6a21\u578b\u4e0d\u4ec5\u80fd\u6a21\u4eff\u4e13\u5bb6\u884c\u4e3a\uff0c\u8fd8\u80fd\u901a\u8fc7\u5176\u4ea4\u4e92\u5386\u53f2\u5728\u4e0a\u4e0b\u6587\u4e2d\u9009\u62e9\u4e0d\u540c\u7684\u4e13\u5bb6\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u6709\u9488\u5bf9\u6027\u7684\u201c\u4e13\u5bb6\u5f0f\u201d\u63a2\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u7684\u8fd0\u52a8\u548c\u64cd\u4f5c\u8bbe\u7f6e\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5747\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5b66\u4e60\u9002\u5e94\u6027\u63a2\u7d22\u884c\u4e3a\u7684\u80fd\u529b\u3002", "conclusion": "\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u65b9\u6cd5\u6210\u529f\u4f7f\u81ea\u4e3b\u667a\u80fd\u4f53\u5b66\u4e60\u5230\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5feb\u901f\u9002\u5e94\u548c\u63a2\u7d22\u884c\u4e3a\uff0c\u6709\u671b\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u667a\u80fd\u5728\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b\u4e0a\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.09285", "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4f18\u5316\u6a21\u7cca\u6838\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u521d\u59cb\u6838\u654f\u611f\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5148\u9a8c\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\uff08BMD\uff09\u65b9\u6cd5\u53d7\u9650\u4e8e\u5176\u4f18\u5316\u8fc7\u7a0b\u7684\u9ad8\u5ea6\u975e\u51f8\u6027\uff0c\u5bfc\u81f4\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u7684\u654f\u611f\u6027\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684BMD\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6765\u7f16\u7801\u6838\u5148\u9a8c\u5e76\u6539\u5584\u6a21\u7cca\u6838\u7684\u521d\u59cb\u5316\u3002\u5177\u4f53\u800c\u8a00\uff0c\u9884\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6838\u751f\u6210\u5668\u6765\u8868\u5f81\u6838\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6838\u521d\u59cb\u5316\u5668\u6765\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8d77\u59cb\u70b9\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5c06BMD\u89e3\u7ea6\u675f\u5728\u4e00\u4e2a\u7d27\u51d1\u7684\u6f5c\u5728\u6838\u6d41\u5f62\u4e2d\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u521d\u59cb\u5316\u654f\u611f\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u5230\u73b0\u6709BMD\u65b9\u6cd5\u4e2d\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u76f2\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u65e0\u9700\u989d\u5916\u5148\u9a8c\u5373\u53ef\u5904\u7406\u76f2\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4f18\u5316\u7684\u6838\u521d\u59cb\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u5148\u9a8c\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u5bf9\u521d\u59cb\u6838\u654f\u611f\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5e76\u6709\u6548\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u975e\u5747\u5300\u53bb\u6a21\u7cca\u4efb\u52a1\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u666e\u9002\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10008", "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u540e\u7eed\u81ea\u6740\u98ce\u9669\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u56e0\u7d20\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4fa7\u91cd\u4e8e\u68c0\u6d4b\u5f53\u524d\u81ea\u6740\u98ce\u9669\uff0c\u4f46\u672a\u80fd\u9884\u6d4b\u968f\u65f6\u95f4\u53d8\u5316\u7684\u540e\u7eed\u98ce\u9669\uff0c\u4e14\u5ffd\u89c6\u4e86\u5728\u81ea\u6740\u98ce\u9669\u9884\u6d4b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u7684\u4fdd\u62a4\u6027\u56e0\u7d20\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u56e0\u7d20\u5bf9\u7528\u6237\u81ea\u6740\u98ce\u9669\u8f6c\u6362\u7684\u52a8\u6001\u5f71\u54cd\u6765\u9884\u6d4b\u540e\u7eed\u81ea\u6740\u98ce\u9669\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u4e86\u4e00\u4e2a\u201c\u4fdd\u62a4\u56e0\u7d20\u611f\u77e5\u6570\u636e\u96c6\u201d\uff08\u57fa\u4e8e12\u5e74Reddit\u5e16\u5b50\uff09\u548c\u5f15\u5165\u4e86\u201c\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u201d\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6743\u91cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8003\u8651\u52a8\u6001\u53d8\u5316\u548c\u4fdd\u62a4\u6027\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u7eed\u81ea\u6740\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u76ca\u4e8e\u4e34\u5e8a\u5e72\u9884\u7684\u89e3\u91ca\u6027\u6d1e\u5bdf\u3002"}}
{"id": "2507.10397", "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouv\u00eaa", "Nuno Paulos", "Eduardo Uchoa e Mari\u00e1 C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "categories": ["cs.AI"], "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\uff08ISA\uff09\uff0c\u63a2\u7d22\u4e86\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\u5b9e\u4f8b\u7279\u6027\u4e0e\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u4f8b\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u63a8\u8fdbCVRP\u7814\u7a76\uff0c\u89e3\u51b3\u7406\u89e3\u5b9e\u4f8b\u7279\u5f81\u4e0e\u5143\u542f\u53d1\u5f0f\uff08MH\uff09\u7b97\u6cd5\u6027\u80fd\u4e4b\u95f4\u590d\u6742\u5173\u7cfb\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\uff08ISA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408DIMACS\u7b2c12\u5c4a\u8f66\u8f86\u8def\u5f84\u6311\u6218\u8d5b\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u51fa23\u4e2a\u76f8\u5173\u5b9e\u4f8b\u7279\u5f81\u3002\u901a\u8fc7PRELIM\u3001SIFTED\u548cPILOT\u9636\u6bb5\uff0c\u8fd0\u7528\u964d\u7ef4\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u521b\u5efa\u5b9e\u4f8b\u7a7a\u95f4\u7684\u4e8c\u7ef4\u6295\u5f71\u3002", "result": "\u8bc6\u522b\u4e8623\u4e2aCVRP\u76f8\u5173\u5b9e\u4f8b\u7279\u5f81\uff1b\u6210\u529f\u6784\u5efa\u4e86\u5b9e\u4f8b\u7a7a\u95f4\u7684\u4e8c\u7ef4\u6295\u5f71\uff0c\u63ed\u793a\u4e86\u5b9e\u4f8b\u7ed3\u6784\u5bf9\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u884c\u4e3a\u7684\u5f71\u54cd\uff1b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6295\u5f71\u77e9\u9635\uff0c\u7b80\u5316\u4e86\u65b0\u5b9e\u4f8b\u7684\u7eb3\u5165\u5206\u6790\u8fc7\u7a0b\uff0c\u4e3aCVRP\u5b9e\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aCVRP\u9886\u57df\u7684\u5b9e\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u4e86\u5b9e\u4f8b\u7279\u5f81\u4e0e\u5143\u542f\u53d1\u5f0f\u6027\u80fd\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u7528\u7684\u6295\u5f71\u77e9\u9635\uff0c\u4fbf\u4e8e\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.09043", "pdf": "https://arxiv.org/pdf/2507.09043", "abs": "https://arxiv.org/abs/2507.09043", "authors": ["Jingxiang Qu", "Wenhan Gao", "Yi Liu"], "title": "Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Gaussian-based Probabilistic Generative Models (GPGMs) generate data by\nreversing a stochastic process that progressively corrupts samples with\nGaussian noise. While these models have achieved state-of-the-art performance\nacross diverse domains, their practical deployment remains constrained by the\nhigh computational cost of long generative trajectories, which often involve\nhundreds to thousands of steps during training and sampling. In this work, we\nintroduce a theoretically grounded and empirically validated framework that\nimproves generation efficiency without sacrificing training granularity or\ninference fidelity. Our key insight is that for certain data modalities, the\nnoising process causes data to rapidly lose its identity and converge toward a\nGaussian distribution. We analytically identify a characteristic step at which\nthe data has acquired sufficient Gaussianity, and then replace the remaining\ngeneration trajectory with a closed-form Gaussian approximation. Unlike\nexisting acceleration techniques that coarsening the trajectories by skipping\nsteps, our method preserves the full resolution of learning dynamics while\navoiding redundant stochastic perturbations between `Gaussian-like'\ndistributions. Empirical results across multiple data modalities demonstrate\nsubstantial improvements in both sample quality and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u9ad8\u65af\u6982\u7387\u751f\u6210\u6a21\u578b(GPGMs)\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u201c\u9ad8\u65af\u6027\u201d\u7279\u5f81\u6b65\uff0c\u5e76\u7528\u95ed\u5408\u5f62\u5f0f\u7684\u9ad8\u65af\u8fd1\u4f3c\u66ff\u6362\u540e\u7eed\u751f\u6210\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86GPGMs\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u9ad8\u65af\u6982\u7387\u751f\u6210\u6a21\u578b(GPGMs)\u867d\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u5176\u6f2b\u957f\uff08\u6570\u767e\u81f3\u6570\u5343\u6b65\uff09\u7684\u751f\u6210\u8f68\u8ff9\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u548c\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u6846\u67b6\u3002\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u6570\u636e\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f1a\u8fc5\u901f\u8d8b\u8fd1\u9ad8\u65af\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u89e3\u6790\u8bc6\u522b\u51fa\u6570\u636e\u83b7\u5f97\u8db3\u591f\u9ad8\u65af\u6027\u7684\u7279\u5f81\u6b65\uff0c\u7136\u540e\u7528\u95ed\u5408\u5f62\u5f0f\u7684\u9ad8\u65af\u8fd1\u4f3c\u66ff\u6362\u5269\u4f59\u7684\u751f\u6210\u8f68\u8ff9\u3002\u6b64\u4e3e\u907f\u514d\u4e86\u5197\u4f59\u7684\u968f\u673a\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5b66\u4e60\u52a8\u6001\u7684\u5b8c\u6574\u5206\u8fa8\u7387\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u6837\u672c\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GPGMs\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\uff0c\u5728\u4e0d\u727a\u7272\u8bad\u7ec3\u7c92\u5ea6\u6216\u63a8\u7406\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u548c\u6837\u672c\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.09291", "pdf": "https://arxiv.org/pdf/2507.09291", "abs": "https://arxiv.org/abs/2507.09291", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "title": "Supercharging Floorplan Localization with Semantic Rays", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICCV 2025", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5e73\u9762\u56fe\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548c\u8bed\u4e49\u5c04\u7ebf\u5e76\u91c7\u7528\u7531\u7c97\u5230\u7cbe\u7684\u65b9\u6cd5\u6784\u5efa\u6982\u7387\u4f53\u79ef\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u5e73\u9762\u56fe\u5b9a\u4f4d\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6df1\u5ea6\u7684\u7ed3\u6784\u7ebf\u7d22\uff0c\u5ffd\u7565\u4e86\u5e73\u9762\u56fe\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u7a97\u6237\u548c\u95e8\u7684\u4f4d\u7f6e\uff09\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5b9a\u4f4d\u6846\u67b6\uff0c\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548c\u8bed\u4e49\u5c04\u7ebf\uff0c\u5e76\u6574\u5408\u4e24\u8005\u6765\u9884\u6d4b\u7ed3\u6784-\u8bed\u4e49\u6982\u7387\u4f53\u79ef\u3002\u8be5\u4f53\u79ef\u91c7\u7528\u7531\u7c97\u5230\u7cbe\u7684\u65b9\u5f0f\u6784\u5efa\uff1a\u5148\u5c11\u91cf\u91c7\u6837\u83b7\u5f97\u521d\u59cb\u4f4e\u5206\u8fa8\u7387\u4f53\u79ef\uff0c\u518d\u5728\u6982\u7387\u9ad8\u533a\u57df\u8fdb\u884c\u5bc6\u96c6\u91c7\u6837\u4ee5\u7ec6\u5316\uff0c\u6700\u7ec8\u9884\u6d4b2D\u4f4d\u7f6e\u548c\u65b9\u5411\u89d2\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u5e73\u9762\u56fe\u5b9a\u4f4d\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u53ec\u56de\u7387\u6307\u6807\u6709\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u53ef\u8f7b\u677e\u6574\u5408\u623f\u95f4\u6807\u7b7e\u7b49\u989d\u5916\u5143\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u4fe1\u606f\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5e73\u9762\u56fe\u5b9a\u4f4d\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.10059", "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "GeLaCo\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u548c\u5c42\u6298\u53e0\u7684LLM\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u538b\u7f29\u7a7a\u95f4\uff0c\u4f18\u5316\u6a21\u578b\u76f8\u4f3c\u5ea6\uff0c\u5e76\u9996\u6b21\u5efa\u7acb\u4e86\u538b\u7f29\u4e0e\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u56e0\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\u800c\u9762\u4e34\u90e8\u7f72\u548c\u4f7f\u7528\u969c\u788d\u3002\u73b0\u6709\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff08\u5982\u7ed3\u6784\u5316\u526a\u679d\uff09\u9700\u8981\u6602\u8d35\u7684\u7ecf\u9a8c\u641c\u7d22\u4e14\u53ef\u80fd\u9519\u8fc7\u66f4\u4f18\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GeLaCo\uff0c\u4e00\u79cd\u901a\u8fc7\u5c42\u6298\u53e0\u5b9e\u73b0LLM\u538b\u7f29\u7684\u8fdb\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u79cd\u7fa4\u7684\u641c\u7d22\u548c\u5305\u542b\u6ce8\u610f\u529b\u3001\u524d\u9988\u3001\u9690\u85cf\u72b6\u6001\u8868\u793a\u7684\u6a21\u5757\u7ea7\u76f8\u4f3c\u5ea6\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u9ad8\u6548\u63a2\u7d22\u538b\u7f29\u89e3\u7a7a\u95f4\u3002GeLaCo\u652f\u6301\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u8fdb\u5316\u538b\u7f29\u641c\u7d22\uff0c\u5e76\u9996\u6b21\u6784\u5efa\u4e86\u538b\u7f29\u4e0e\u8d28\u91cf\u8f74\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u901a\u8fc7\u5bf9\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u56f0\u60d1\u5ea6\u8bc4\u4f30\u548c\u751f\u6210\u6027\u8bc4\u4f30\uff0cGeLaCo\u7684\u89e3\u51b3\u65b9\u6848\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "GeLaCo\u901a\u8fc7\u5176\u72ec\u7279\u7684\u8fdb\u5316\u5c42\u6298\u53e0\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u538b\u7f29\u4e0e\u6a21\u578b\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5e73\u8861\uff0c\u53d6\u5f97\u4e86\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10421", "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408BERT\u60c5\u611f\u5206\u6790\u4e0eXGBoost\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5728\u7ebf\u5b66\u4e60\u8f8d\u5b66\uff0c\u51c6\u786e\u7387\u8fbe84%\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u8f8d\u5b66\u95ee\u9898\u4e25\u91cd\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8f8d\u5b66\u9884\u6d4b\u7814\u7a76\u901a\u5e38\u672a\u5145\u5206\u6574\u5408\u591a\u6837\u5316\u7684\u6570\u636e\u6e90\uff0c\u5305\u62ec\u793e\u4f1a\u4eba\u53e3\u3001\u884c\u4e3a\u6570\u636e\u548c\u60c5\u611f\u5206\u6790\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u8f8d\u5b66\u98ce\u9669\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u7ed3\u5408BERT\u6a21\u578b\u5bf9\u5b66\u751f\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u5e76\u4e0e\u901a\u8fc7XGBoost\u5206\u6790\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u548c\u884c\u4e3a\u6570\u636e\u878d\u5408\u3002BERT\u5728\u5b66\u751f\u8bc4\u8bba\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u60c5\u611f\u7279\u5f81\u968f\u540e\u4e0e\u901a\u8fc7XGBoost\u7279\u5f81\u91cd\u8981\u6027\u6280\u672f\u9009\u62e9\u7684\u5173\u952e\u7279\u5f81\u5408\u5e76\u3002", "result": "\u6a21\u578b\u5728\u4e0b\u4e00\u5b66\u5e74\u7684\u672a\u89c1\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523084%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u768482%\u3002\u540c\u65f6\uff0c\u5728\u7cbe\u786e\u7387\u548cF1-score\u7b49\u5176\u4ed6\u6307\u6807\u4e0a\u4e5f\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u5f00\u53d1\u4e2a\u6027\u5316\u7b56\u7565\u3001\u6709\u6548\u964d\u4f4e\u8f8d\u5b66\u7387\u548c\u9f13\u52b1\u5b66\u751f\u575a\u6301\u5b66\u4e60\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.09061", "pdf": "https://arxiv.org/pdf/2507.09061", "abs": "https://arxiv.org/abs/2507.09061", "authors": ["Thomas T. Zhang", "Daniel Pfrommer", "Nikolai Matni", "Max Simchowitz"], "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "comment": null, "summary": "We study the problem of imitating an expert demonstrator in a continuous\nstate-and-action dynamical system. While imitation learning in discrete\nsettings such as autoregressive language modeling has seen immense success and\npopularity in recent years, imitation in physical settings such as autonomous\ndriving and robot learning has proven comparably more complex due to the\ncompounding errors problem, often requiring elaborate set-ups to perform\nstably. Recent work has demonstrated that even in benign settings, exponential\ncompounding errors are unavoidable when learning solely from expert-controlled\ntrajectories, suggesting the need for more advanced policy parameterizations or\ndata augmentation. To this end, we present minimal interventions that provably\nmitigate compounding errors in continuous state-and-action imitation learning.\nWhen the system is open-loop stable, we prescribe \"action chunking,\" i.e.,\npredicting and playing sequences of actions in open-loop; when the system is\npossibly unstable, we prescribe \"noise injection,\" i.e., adding noise during\nexpert demonstrations. These interventions align with popular choices in modern\nrobot learning, though the benefits we derive are distinct from the effects\nthey were designed to target. Our results draw insights and tools from both\ncontrol theory and reinforcement learning; however, our analysis reveals novel\nconsiderations that do not naturally arise when either literature is considered\nin isolation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u52a8\u4f5c\u5206\u5757\u201d\u548c\u201c\u566a\u58f0\u6ce8\u5165\u201d\u4e24\u79cd\u6700\u5c0f\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u53ef\u8bc1\u660e\u5730\u7f13\u89e3\u8fde\u7eed\u72b6\u6001\u52a8\u4f5c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\u3002", "motivation": "\u8fde\u7eed\u7269\u7406\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\u53d7\u7d2f\u79ef\u8bef\u5dee\u56f0\u6270\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e0e\u79bb\u6563\u8bbe\u7f6e\u7684\u6210\u529f\u5f62\u6210\u5bf9\u6bd4\u3002\u73b0\u6709\u65b9\u6cd5\u590d\u6742\uff0c\u4e14\u7eaf\u7cb9\u4ece\u4e13\u5bb6\u8f68\u8ff9\u5b66\u4e60\u4f1a\u4e0d\u53ef\u907f\u514d\u5730\u4ea7\u751f\u6307\u6570\u7ea7\u7d2f\u79ef\u8bef\u5dee\uff0c\u6025\u9700\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4e3a\u7f13\u89e3\u8fde\u7eed\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\uff0c\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u201c\u6700\u5c0f\u5e72\u9884\u201d\u63aa\u65bd\uff1a1. \u5bf9\u4e8e\u5f00\u73af\u7a33\u5b9a\u7cfb\u7edf\uff0c\u91c7\u7528\u201c\u52a8\u4f5c\u5206\u5757\u201d\uff0c\u5373\u5f00\u73af\u9884\u6d4b\u5e76\u6267\u884c\u4e00\u7cfb\u5217\u52a8\u4f5c\u30022. \u5bf9\u4e8e\u53ef\u80fd\u4e0d\u7a33\u5b9a\u7cfb\u7edf\uff0c\u91c7\u7528\u201c\u566a\u58f0\u6ce8\u5165\u201d\uff0c\u5373\u5728\u4e13\u5bb6\u6f14\u793a\u4e2d\u52a0\u5165\u566a\u58f0\u3002\u7814\u7a76\u7ed3\u5408\u4e86\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\u4e0e\u89c1\u89e3\u3002", "result": "\u6240\u63d0\u51fa\u7684\u201c\u52a8\u4f5c\u5206\u5757\u201d\u548c\u201c\u566a\u58f0\u6ce8\u5165\u201d\u5e72\u9884\u63aa\u65bd\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u7f13\u89e3\u8fde\u7eed\u72b6\u6001\u52a8\u4f5c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u5e26\u6765\u7684\u76ca\u5904\u4e0e\u5b83\u4eec\u5728\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u539f\u8bbe\u8ba1\u76ee\u6807\u7684\u6548\u679c\u4e0d\u540c\u3002\u5206\u6790\u7ed3\u5408\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63ed\u793a\u4e86\u5355\u72ec\u8003\u8651\u4efb\u4e00\u9886\u57df\u65f6\u4e0d\u4f1a\u51fa\u73b0\u7684\u65b0\u7684\u8003\u91cf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6700\u5c0f\u5e72\u9884\u63aa\u65bd\u80fd\u6709\u6548\u89e3\u51b3\u8fde\u7eed\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u7a33\u5b9a\u548c\u4e0d\u7a33\u5b9a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u5de5\u4f5c\u901a\u8fc7\u6574\u5408\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u3002"}}
{"id": "2507.09294", "pdf": "https://arxiv.org/pdf/2507.09294", "abs": "https://arxiv.org/abs/2507.09294", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "categories": ["cs.CV", "cs.RO"], "comment": "IEEE ICIA 2025", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGeo-RepNet\uff0c\u4e00\u4e2a\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u51e0\u4f55\u611f\u77e5\u5377\u79ef\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5185\u7aa5\u955c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u89c6\u89c9\u76f8\u4f3c\u5ea6\u548c\u7ed3\u6784\u7ebf\u7d22\u7f3a\u4e4f\u7684\u6311\u6218\uff0c\u5e76\u5728\u771f\u5b9eESD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5916\u79d1\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u5bf9\u4e8e\u5f00\u53d1\u667a\u80fd\u8f85\u52a9\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5185\u7aa5\u955c\u9ecf\u819c\u4e0b\u5265\u79bb\u672f\uff08ESD\uff09\u7b49\u5fae\u521b\u624b\u672f\u4e2d\u3002\u7136\u800c\uff0c\u4e0d\u540c\u9636\u6bb5\u95f4\u9ad8\u5ea6\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\u4ee5\u53caRGB\u56fe\u50cf\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u7ebf\u7d22\uff0c\u5bf9\u8bc6\u522b\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\u3002\u6df1\u5ea6\u4fe1\u606f\u80fd\u63d0\u4f9b\u5b9d\u8d35\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u8865\u5145\u5916\u89c2\u7279\u5f81\u3002", "method": "\u672c\u6587\u7387\u5148\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u8fdb\u884c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\uff0c\u5e76\u63d0\u51faGeo-RepNet\u3002\u8be5\u6846\u67b6\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u5377\u79ef\u7f51\u7edc\uff0c\u6574\u5408\u4e86RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u4fe1\u606f\u3002\u5b83\u4ee5\u53ef\u91cd\u53c2\u6570\u5316\u7684RepVGG\u4e3a\u9aa8\u5e72\uff0c\u5e76\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u6df1\u5ea6\u5f15\u5bfc\u51e0\u4f55\u5148\u9a8c\u751f\u6210\uff08DGPG\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u539f\u59cb\u6df1\u5ea6\u56fe\u4e2d\u63d0\u53d6\u51e0\u4f55\u5148\u9a8c\uff1b\u51e0\u4f55\u589e\u5f3a\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\uff08GEMA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u53ca\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u805a\u5408\u6ce8\u5165\u7a7a\u95f4\u6307\u5bfc\u3002\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e5d\u4e2a\u9636\u6bb5\u3001\u5bc6\u96c6\u5e27\u7ea7\u6807\u6ce8\u7684ESD\u6570\u636e\u96c6\u3002", "result": "\u5728\u6240\u6784\u5efa\u7684ESD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeo-RepNet\u5728\u590d\u6742\u548c\u4f4e\u7eb9\u7406\u7684\u624b\u672f\u73af\u5883\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Geo-RepNet\u901a\u8fc7\u6709\u6548\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u667a\u80fd\u624b\u672f\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M\u00fcnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u672a\u80fd\u4ee3\u8868\u591a\u6837\u5316\u7684\u6587\u5316\u9053\u5fb7\u6846\u67b6\uff0c\u53cd\u800c\u5c06\u5176\u540c\u8d28\u5316\uff0c\u8fd9\u6311\u6218\u4e86\u5b83\u4eec\u5728\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u5e76\u63ed\u793a\u4e86\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\uff08\u7279\u522b\u662fLLMs\uff09\u662f\u5426\u771f\u6b63\u4ee3\u8868\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u6216\u4ec5\u4ec5\u662f\u5e73\u5747\u5316\u5904\u7406\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u8de8\u6587\u5316\u9053\u5fb7\u8868\u793a\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u572819\u4e2a\u6587\u5316\u80cc\u666f\u4e0b\u5e94\u7528\u201c\u9053\u5fb7\u57fa\u7840\u95ee\u5377\u201d\uff08Moral Foundations Questionnaire\uff09\uff0c\u6bd4\u8f83\u591a\u4e2a\u6700\u5148\u8fdb\u7684LLMs\u7684\u8f93\u51fa\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u6570\u636e\uff0c\u4ee5\u66b4\u9732AI\u751f\u6210\u548c\u4eba\u7c7b\u9053\u5fb7\u76f4\u89c9\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "LLMs\u672a\u80fd\u4ee3\u8868\u591a\u6837\u5316\u7684\u6587\u5316\u9053\u5fb7\u6846\u67b6\uff0cAI\u751f\u6210\u4e0e\u4eba\u7c7b\u9053\u5fb7\u76f4\u89c9\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u8fd9\u4e9b\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u540c\u8d28\u5316\u4e86\u9053\u5fb7\u591a\u6837\u6027\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\u5e76\u672a\u6301\u7eed\u6539\u5584\u6587\u5316\u4ee3\u8868\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u5c06LLMs\u7528\u4f5c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u201c\u5408\u6210\u4eba\u7fa4\u201d\u7684\u65e5\u76ca\u589e\u957f\u7684\u8d8b\u52bf\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u7684\u57fa\u672c\u5c40\u9650\u6027\u3002\u547c\u5401\u8d85\u8d8a\u63d0\u793a\u5de5\u7a0b\uff0c\u91c7\u7528\u66f4\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u5bf9\u9f50\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u80fd\u591f\u4ee3\u8868\u591a\u6837\u5316\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u800c\u975e\u62b9\u5e73\u9053\u5fb7\u5dee\u5f02\u3002"}}
{"id": "2507.10446", "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff08\u5982\u8ba1\u7b97\u5316\u5b66\u3001\u514d\u75ab\u5b66\u3001\u533b\u5b66\u5f71\u50cf\uff09\u4e2d\u65e0\u6cd5\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u95ee\u9898\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u795e\u7ecf\u8bb0\u5fc6\u548c\u8d85\u7f51\u7edc\uff08Hypernetwork\uff09\u7b49\u65b0\u9896\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5148\u9a8c\u77e5\u8bc6\u83b7\u53d6\u548c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ece\u800c\u5728\u6709\u9650\u6570\u636e\u4e0b\u4e5f\u80fd\u8fdb\u884c\u6709\u6548\u7684\u4efb\u52a1\u9002\u5e94\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u4f9d\u8d56\u4e8e\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5728\u8ba1\u7b97\u5316\u5b66\u3001\u8ba1\u7b97\u514d\u75ab\u5b66\u548c\u533b\u5b66\u5f71\u50cf\u7b49\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff0c\u8fd9\u79cd\u65b9\u5f0f\u4e0d\u53ef\u884c\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bbe\u8ba1\u65b0\u7684\u67b6\u6784\uff0c\u4ee5\u5728\u6570\u636e\u91cf\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9ad8\u6548\u5730\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "method": ["\u8bbe\u8ba1\u7528\u4e8e\u5728\u7f3a\u4e4f\u5927\u91cf\u6570\u636e\u65f6\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u67b6\u6784\u3002", "\u5229\u7528\u795e\u7ecf\u8bb0\u5fc6\u5b9e\u73b0\u5c11\u91cf\u6837\u672c\u4e0b\u5bf9\u975e\u5e73\u7a33\u5206\u5e03\u7684\u9002\u5e94\u3002", "\u8bbe\u8ba1\u8d85\u7f51\u7edc\uff08\u4e00\u4e2a\u751f\u6210\u53e6\u4e00\u4e2a\u7f51\u7edc\u7684\u7f51\u7edc\uff09\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u4e0d\u53ef\u77e5\u5143\u5b66\u4e60\uff08MAML\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "\u5c06\u8d85\u7f51\u7edc\u5e94\u7528\u4e8e3D\u573a\u666f\u751f\u6210\uff0c\u5e76\u6269\u5c55\u81f33D\u5206\u5272\u4efb\u52a1\u3002", "\u91cd\u65b0\u5229\u7528\u73b0\u6709\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6846\u67b6\u3002"], "result": ["\u795e\u7ecf\u8bb0\u5fc6\u80fd\u591f\u4f7f\u6a21\u578b\u5728\u4ec5\u6709\u5c11\u91cf\u6837\u672c\u7684\u975e\u5e73\u7a33\u5206\u5e03\u4e0a\u8fdb\u884c\u9002\u5e94\u3002", "\u4e0e\u6807\u51c6\u7f51\u7edc\u76f8\u6bd4\uff0c\u901a\u8fc7MAML\u8bad\u7ec3\u7684\u8d85\u7f51\u7edc\u8bbe\u8ba1\u53ef\u4ee5\u83b7\u5f97\u66f4\u5177\u6cdb\u5316\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "\u8d85\u7f51\u7edc\u80fd\u5728\u5c11\u6570\u8bad\u7ec3\u573a\u666f\u4e2d\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u52a0\u901f\u6587\u672c\u52303D\u573a\u666f\u7684\u751f\u6210\u3002", "\u8d85\u7f51\u7edc\u6846\u67b6\u80fd\u901a\u8fc7\u9ad8\u6548\u8fc1\u79fb\u65e9\u671f\u573a\u666f\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u6709\u9650\u6570\u636e\u7684\u65b0\u573a\u666f\u4e0a\u6267\u884c3D\u5206\u5272\u3002", "\u91cd\u5851\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7684\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u514d\u75ab\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"], "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u795e\u7ecf\u8bb0\u5fc6\u548c\u8d85\u7f51\u7edc\u7b49\u521b\u65b0\u67b6\u6784\uff0c\u4ee5\u53ca\u5229\u7528\u73b0\u6709\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u8fdb\u884c\u9ad8\u6548\u5148\u9a8c\u77e5\u8bc6\u83b7\u53d6\u548c\u77e5\u8bc6\u8fc1\u79fb\u7684\u96be\u9898\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u591a\u79cd\u5e94\u7528\uff08\u5305\u62ec3D\u573a\u666f\u751f\u6210\u30013D\u5206\u5272\u548c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\uff09\u4e2d\u5c55\u793a\u4e86\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09084", "pdf": "https://arxiv.org/pdf/2507.09084", "abs": "https://arxiv.org/abs/2507.09084", "authors": ["Nnamdi Daniel Aghanya", "Ta Duong Vu", "Ama\u00eblle Diop", "Charlotte Deville", "Nour Imane Kerroumi", "Irene Moulitsas", "Jun Li", "Desmond Bisandu"], "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction", "categories": ["cs.LG", "cs.AI", "68T07, 90B22, 62M10", "I.2.m"], "comment": "3 figures, 20 pages references and appendix included,", "summary": "Flight delays are a significant challenge in the aviation industry, causing\nmajor financial and operational disruptions. To improve passenger experience\nand reduce revenue loss, flight delay prediction models must be both precise\nand generalizable across different networks. This paper introduces a novel\napproach that combines Queue-Theory with a simple attention model, referred to\nas the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from\nthe US Bureau of Transportation Statistics, where our proposed QT-SimAM\n(Bidirectional) model outperformed existing methods with an accuracy of 0.927\nand an F1 score of 0.932. To assess transferability, we tested the model on the\nEUROCONTROL dataset. The results demonstrated strong performance, achieving an\naccuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an\neffective, end-to-end methodology for predicting flight delays. The proposed\nmodel's ability to forecast delays with high accuracy across different networks\ncan help reduce passenger anxiety and improve operational decision-making", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u6392\u961f\u8bba\u548c\u6ce8\u610f\u529b\u6a21\u578b\u7684QT-SimAM\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u3001\u53ef\u6cdb\u5316\u7684\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u3002", "motivation": "\u822a\u73ed\u5ef6\u8bef\u7ed9\u822a\u7a7a\u4e1a\u5e26\u6765\u4e25\u91cd\u7684\u8d22\u52a1\u548c\u8fd0\u8425\u5f71\u54cd\u3002\u4e3a\u4e86\u63d0\u5347\u4e58\u5ba2\u4f53\u9a8c\u5e76\u51cf\u5c11\u635f\u5931\uff0c\u9700\u8981\u5f00\u53d1\u7cbe\u786e\u4e14\u80fd\u8de8\u4e0d\u540c\u7f51\u7edc\u6cdb\u5316\u7684\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6392\u961f\u8bba\uff08Queue-Theory\uff09\u4e0e\u7b80\u5355\u6ce8\u610f\u529b\u6a21\u578b\uff08SimAM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u547d\u540d\u4e3aQT-SimAM\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u53cc\u5411\u6a21\u578b\uff08QT-SimAM Bidirectional\uff09\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728US Bureau of Transportation Statistics\u6570\u636e\u96c6\u4e0a\uff0cQT-SimAM\uff08Bidirectional\uff09\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a0.927\uff0cF1\u5206\u6570\u4e3a0.932\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728EUROCONTROL\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u53ef\u8fc1\u79fb\u6027\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u5f3a\u52b2\uff0c\u51c6\u786e\u7387\u8fbe\u52300.826\uff0cF1\u5206\u6570\u4e3a0.791\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u3001\u7aef\u5230\u7aef\u7684\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u80fd\u4ee5\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u4e0d\u540c\u7f51\u7edc\u7684\u822a\u73ed\u5ef6\u8bef\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u4e58\u5ba2\u7126\u8651\u5e76\u6539\u8fdb\u8fd0\u8425\u51b3\u7b56\u3002"}}
{"id": "2507.09299", "pdf": "https://arxiv.org/pdf/2507.09299", "abs": "https://arxiv.org/abs/2507.09299", "authors": ["Abdulvahap Mutlu", "\u015eeng\u00fcl Do\u011fan", "T\u00fcrker Tuncer"], "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "AI": {"tldr": "ViT-ProtoNet\u7ed3\u5408ViT\u548c\u539f\u578b\u7f51\u7edc\uff0c\u5728\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8aCNN\u548c\u90e8\u5206Transformer\u57fa\u7ebf\u3002", "motivation": "\u89c6\u89c9Transformer (ViTs) \u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u5728\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5f15\u5165ViT-ProtoNet\uff0c\u5c06ViT-Small\u9aa8\u5e72\u7f51\u7edc\u96c6\u6210\u5230\u539f\u578b\u7f51\u7edc\u6846\u67b6\u4e2d\u3002\u901a\u8fc7\u5e73\u5747\u652f\u6301\u6837\u672c\u7684\u7c7b\u522b\u6761\u4ef6token\u5d4c\u5165\u6765\u6784\u5efa\u9c81\u68d2\u539f\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViT-ProtoNet\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8eCNN\u7684\u539f\u578b\u7f51\u7edc\uff0c5\u6837\u672c\u51c6\u786e\u7387\u63d0\u9ad8\u9ad8\u8fbe3.2%\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7279\u5f81\u53ef\u5206\u79bb\u6027\u3002\u5b83\u4e5f\u4f18\u4e8e\u6216\u4e0e\u4f7f\u7528\u66f4\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7684Transformer\u57fa\u7ade\u4e89\u5bf9\u624b\u76f8\u5f53\u3002", "conclusion": "ViT-ProtoNet\u662f\u5c11\u6837\u672c\u5206\u7c7b\u7684\u5f3a\u5927\u7075\u6d3b\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8eTransformer\u7684\u5143\u5b66\u4e60\u5668\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.10085", "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u201c\u5173\u952e\u8868\u793a\u5fae\u8c03\u201d\uff08CRFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u8bc6\u522b\u5e76\u4f18\u5316\u6a21\u578b\u4e2d\u7684\u5173\u952e\u8868\u793a\uff0c\u4ee5\u63d0\u5347\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08ReFT\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u4e0e\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5ReFT\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u56fa\u5b9a\u4f4d\u7f6e\u7684\u8868\u793a\u4fee\u6539\u5bf9\u8f93\u51fa\u5f71\u54cd\u4e0d\u786e\u5b9a\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u63a8\u7406\u6240\u9700\u7684\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faCRFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u8bc6\u522b\u5e76\u52a8\u6001\u4f18\u5316\u6a21\u578b\u4e2d\u5bf9\u6700\u7ec8\u8f93\u51fa\u6709\u91cd\u5927\u5f71\u54cd\u7684\u201c\u5173\u952e\u8868\u793a\u201d\u3002\u8be5\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u4f18\u5316\u8fd9\u4e9b\u5173\u952e\u8868\u793a\uff0c\u540c\u65f6\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728LLaMA\u548cMistral\u6a21\u578b\u5bb6\u65cf\u4e0a\uff0c\u5bf98\u4e2a\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u9a8c\u8bc1\uff0cCRFT\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5c06\u5355\u6837\u672c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8616.4%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u8868\u793a\u5c42\u9762\u4f18\u5316\u5728\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u4f20\u7edf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u800c\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.10522", "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch Eco\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u79d1\u5b66\u7efc\u5408\u3002\u5b83\u901a\u8fc7\u9012\u5f52\u63a2\u7d22\u63d0\u5347\u6587\u732e\u68c0\u7d22\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u7528\u6237\u53ef\u63a7\u7684\u8bc1\u636e\u6574\u5408\uff0c\u5e76\u5728\u751f\u6001\u5b66\u7814\u7a76\u4e2d\u663e\u8457\u63d0\u9ad8\u6e90\u6574\u5408\u91cf\u548c\u5206\u6790\u6df1\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u79d1\u5b66\u6587\u732e\u68c0\u7d22\u7684\u591a\u6837\u6027\u548c\u7ec6\u5fae\u6027\uff0c\u5e76\u5b9e\u73b0\u7528\u6237\u53ef\u63a7\u3001\u9ad8\u901a\u91cf\u7684\u9886\u57df\u7279\u5b9a\u8bc1\u636e\u6574\u5408\uff0c\u4ee5\u514b\u670d\u4f20\u7edfRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7ba1\u7ebf\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86DeepResearch$^{\\text{Eco}}$\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u539f\u59cb\u7814\u7a76\u95ee\u9898\u7684\u9012\u5f52\u3001\u6df1\u5ea6\u548c\u5e7f\u5ea6\u63a7\u5236\u63a2\u7d22\u3002\u8be5\u7cfb\u7edf\u8fd8\u5177\u5907\u7528\u6237\u53ef\u63a7\u7684\u7efc\u5408\u3001\u900f\u660e\u7684\u63a8\u7406\u548c\u53c2\u6570\u9a71\u52a8\u7684\u53ef\u914d\u7f6e\u6027\u3002", "result": "\u5c06DeepResearch\u5e94\u7528\u4e8e49\u4e2a\u751f\u6001\u7814\u7a76\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe21\u500d\u7684\u6765\u6e90\u6574\u5408\u589e\u957f\uff0c\u4ee5\u53ca\u6bcf1000\u5b57\u6574\u5408\u6765\u6e90\u6570\u91cf14.9\u500d\u7684\u63d0\u5347\u3002\u9ad8\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0c\u7cfb\u7edf\u5c55\u73b0\u4e86\u4e13\u5bb6\u7ea7\u7684\u5206\u6790\u6df1\u5ea6\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u3002", "conclusion": "DeepResearch$^{\\text{Eco}}$\u6210\u529f\u5730\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u79d1\u5b66\u7efc\u5408\u80fd\u529b\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6587\u732e\u68c0\u7d22\u7684\u591a\u6837\u6027\u3001\u6548\u7387\u548c\u5206\u6790\u6df1\u5ea6\uff0c\u662f\u79d1\u5b66\u7814\u7a76\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.09087", "pdf": "https://arxiv.org/pdf/2507.09087", "abs": "https://arxiv.org/abs/2507.09087", "authors": ["Esraa Elelimy", "Brett Daley", "Andrew Patterson", "Marlos C. Machado", "Adam White", "Martha White"], "title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09305", "pdf": "https://arxiv.org/pdf/2507.09305", "abs": "https://arxiv.org/abs/2507.09305", "authors": ["Zhiwei Xu"], "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6df1\u5ea6\u89d2\u5ea6A* (DAA*) \u7684\u65b0\u8def\u5f84\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6\uff08PAF\uff09\u5f15\u5165A*\u7b97\u6cd5\uff0c\u81ea\u9002\u5e94\u5730\u63d0\u9ad8\u8def\u5f84\u5e73\u6ed1\u5ea6\u548c\u4e0e\u53c2\u8003\u8def\u5f84\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u4e13\u5bb6\u6f14\u793a\u7684\u8def\u5f84\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u8def\u5f84\u5e73\u6ed1\u5ea6\u5e38\u5e38\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8def\u5f84\u4e0d\u591f\u4f18\u5316\u6216\u81ea\u7136\u3002", "method": "\u5f15\u5165\u4e86\u6df1\u5ea6\u89d2\u5ea6A* (DAA*) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6 (PAF) \u6574\u5408\u5230A*\u7b97\u6cd5\u4e2d\u3002PAF\u65e8\u5728\u901a\u8fc7\u5e73\u8861\u79fb\u52a8\u89d2\u5ea6\u7684\u6700\u5c0f\u548c\u6700\u5927\u503c\u6765\u63a2\u7d22\u5176\u5bf9\u8def\u5f84\u8282\u70b9\u6269\u5c55\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u81ea\u9002\u5e94\u6027\u3002DAA*\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8def\u5f84\u7f29\u77ed\uff08\u5bf9\u5e94\u542f\u53d1\u5f0f\u8ddd\u79bb\uff09\u548c\u8def\u5f84\u5e73\u6ed1\uff08\u5bf9\u5e94PAF\uff09\uff0c\u4f7f\u9884\u6d4b\u8def\u5f84\u4e0e\u53c2\u8003\u8def\u5f84\u7d27\u5bc6\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u9ad8\u8def\u5f84\u6700\u4f18\u6027\u3002", "result": "DAA*\u57287\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u8ff7\u5bab\u3001\u89c6\u9891\u6e38\u620f\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u89c6\u89d2\u6570\u636e\u96c6\uff09\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff1a\u4e0eNeural A*\u76f8\u6bd4\uff0c\u8def\u5f84\u76f8\u4f3c\u6027\uff08SPR\u63d0\u9ad89.0%\uff0cASIM\u63d0\u9ad86.9%\uff0cPSIM\u63d0\u9ad83.9%\uff09\u548c\u8def\u5f84\u957f\u5ea6\u66f4\u4f18\u3002\u5728\u8054\u5408\u5b66\u4e60\u8def\u5f84\u635f\u8017\u548c\u8def\u5f84\u6982\u7387\u56fe\u635f\u8017\u65f6\uff0cDAA*\u4e5f\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684TransPath\uff08SPR\u63d0\u9ad86.7%\uff0cPSIM\u63d0\u9ad86.5%\uff0cASIM\u63d0\u9ad83.7%\uff09\u3002", "conclusion": "DAA*\u6709\u6548\u89e3\u51b3\u4e86\u8def\u5f84\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5e73\u6ed1\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8def\u5f84\u76f8\u4f3c\u6027\u548c\u6700\u4f18\u6027\u3002\u7814\u7a76\u4e5f\u6307\u51fa\u4e86\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u641c\u7d22\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u7ec6\u5fae\u7684\u6743\u8861\u3002"}}
{"id": "2507.10098", "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4f20\u7edfTransformer\u7684\u8868\u793a\uff0c\u7ed3\u5408\u9ad8\u5c42\u8bed\u4e49\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u4ee5\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u56e0LLM\u4e0d\u64c5\u957f\u5904\u7406\u8fde\u7eed\u6570\u503c\u6570\u636e\uff0c\u6027\u80fd\u53ef\u80fd\u4e0d\u5982\u4f20\u7edfTransformer\uff1b\u800c\u4f20\u7edfTransformer\u53c8\u96be\u4ee5\u5b66\u4e60\u9ad8\u5c42\u8bed\u4e49\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bbe\u8ba1\u4e00\u79cd\u80fd\u4e92\u8865\u5229\u7528\u4e8c\u8005\u4f18\u52bf\u7684\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7684Transformer\u67b6\u6784\uff0c\u5b83\u878d\u5408\u4e86LLM\u5b66\u4e60\u5230\u7684\u9ad8\u5c42\u8bed\u4e49\u8868\u793a\u548c\u65f6\u95f4\u5e8f\u5217Transformer\u7f16\u7801\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u4ece\u800c\u83b7\u5f97\u4e00\u79cd\u5305\u542b\u5386\u53f2\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u53d8\u5f02\u6a21\u5f0f\u7684\u6df7\u5408\u8868\u793a\u3002", "result": "\u878d\u5408\u540e\u7684\u8868\u793a\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u66f4\u51c6\u786e\u7684\u672a\u6765\u503c\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408LLM\u7684\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u4f20\u7edfTransformer\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u6240\u63d0\u51fa\u7684\u6df7\u5408\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2412.11407", "pdf": "https://arxiv.org/pdf/2412.11407", "abs": "https://arxiv.org/abs/2412.11407", "authors": ["TianZhu Liu", "BangYan Hu", "YanFeng Gu", "Xian Li", "Aleksandra Pi\u017eurica"], "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "16 pages, 9 figures, 5 tables", "summary": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.", "AI": {"tldr": "\u9488\u5bf9\u5ba4\u5916\u591a\u5149\u8c31\u70b9\u4e91\u5206\u7c7b\u4e2d\u7a00\u758f\u6807\u6ce8\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u7684\u589e\u5f3a\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u91c7\u6837\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u5ba4\u5916\u591a\u5149\u8c31\u70b9\u4e91\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u6807\u6ce8\u7a00\u758f\u3001\u5730\u7269\u5c3a\u5ea6\u5dee\u5f02\u5927\u548c\u7c7b\u522b\u5448\u957f\u5c3e\u5206\u5e03\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1. **\u8bad\u7ec3\u96c6\u751f\u6210\uff1a** \u8bbe\u8ba1\u7f51\u683c\u5e73\u8861\u91c7\u6837\u7b56\u7565\u4ee5\u5904\u7406\u7a00\u758f\u6807\u6ce8\u30022. **\u7279\u5f81\u5b66\u4e60\uff1a** \u63d0\u51fa\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u878d\u5408\u4e0d\u540c\u5c3a\u5ea6\u5730\u7269\u7684\u6d45\u5c42\u7279\u5f81\uff0c\u907f\u514d\u7ec6\u8282\u4e22\u5931\u30023. **\u5206\u7c7b\uff1a** \u8bbe\u8ba1\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u6a21\u5757\uff0c\u5229\u7528\u5e26\u81ea\u9002\u5e94\u6743\u91cd\u7684\u591a\u5206\u7c7b\u5934\uff0c\u5e73\u8861\u5404\u7c7b\u522b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5c24\u5176\u63d0\u5347\u5c0f\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u5149\u8c31\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5e94\u5bf9\u5ba4\u5916\u591a\u5149\u8c31\u70b9\u4e91\u5206\u7c7b\u4e2d\u7684\u7a00\u758f\u6807\u6ce8\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u5c0f\u7c7b\u522b\u5730\u7269\u7684\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2507.09091", "pdf": "https://arxiv.org/pdf/2507.09091", "abs": "https://arxiv.org/abs/2507.09091", "authors": ["Shayan K. Azmoodeh", "Krishna Subramani", "Paris Smaragdis"], "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "6 pages, 3 figures, 1 table. MLSP 2025", "summary": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9690\u5f0f\u795e\u7ecf\u4fe1\u53f7\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u6cdb\u5316\u8fde\u7eed\u65f6\u95f4\u5411\u91cf\u503c\u4fe1\u53f7\u7684\u4f4e\u79e9\u5206\u89e3\u95ee\u9898\uff08\u5982PCA\u548cICA\uff09\uff0c\u4f7f\u5176\u80fd\u5e94\u7528\u4e8e\u70b9\u4e91\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7684\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\uff08\u5982PCA\u3001ICA\uff09\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u5411\u91cf\u503c\u4fe1\u53f7\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u70b9\u4e91\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\u65f6\uff0c\u6807\u51c6\u6280\u672f\u5f80\u5f80\u5931\u6548\u3002", "method": "\u5c06\u4fe1\u53f7\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u9690\u5f0f\u795e\u7ecf\u4fe1\u53f7\u8868\u793a\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u7f51\u7edc\u635f\u5931\u4e2d\u5f15\u5165\u4e00\u4e2a\u5bf9\u6bd4\u51fd\u6570\u9879\uff0c\u7edf\u4e00\u4e86\u8fde\u7eed\u8bbe\u7f6e\u4e0bPCA\u548cICA\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u5f3a\u5236\u5206\u89e3\u5b66\u4e60\u5230\u7684\u6e90\u4fe1\u53f7\u6ee1\u8db3\u6240\u9700\u7684\u7edf\u8ba1\u7279\u6027\uff08\u5982\u53bb\u76f8\u5173\u6027\u3001\u72ec\u7acb\u6027\uff09\u3002", "result": "\u6210\u529f\u5730\u5c06\u4f4e\u79e9\u5206\u89e3\u95ee\u9898\u63a8\u5e7f\u5230\u8fde\u7eed\u65f6\u95f4\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b66\u4e60\u6570\u503c\u8fd1\u4f3c\u89e3\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u6807\u51c6\u6280\u672f\u65e0\u6cd5\u5e94\u5bf9\u7684\u70b9\u4e91\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\u3002", "conclusion": "\u5c06\u4f4e\u79e9\u5206\u89e3\u6269\u5c55\u5230\u8fde\u7eed\u57df\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u6781\u5927\u5730\u62d3\u5bbd\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5206\u6790\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5982\u70b9\u4e91\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\uff0c\u4e3a\u590d\u6742\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2507.09308", "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u900f\u660e\u56fe\u50cf\u751f\u6210\u96be\u9898\uff0c\u672c\u6587\u63d0\u51fa\u9996\u4e2aRGBA\u57fa\u51c6ALPHA\u548c\u9ad8\u6548\u7684ALPHAVAE\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u900f\u660e\u56fe\u50cf\u91cd\u5efa\u4e0e\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u91cf\u8fdc\u4f4e\u4e8eSOTA\u3002", "motivation": "\u73b0\u6709\u6f5c\u53d8\u91cf\u6269\u6563\u6a21\u578b\u5728RGB\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u900f\u660e\u6216\u5206\u5c42\u5185\u5bb9\uff08RGBA\u56fe\u50cf\uff09\u7684\u751f\u6210\u7814\u7a76\u751a\u5c11\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ALPHA\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u7684RGBA\u57fa\u51c6\uff0c\u901a\u8fc7\u5728\u89c4\u8303\u80cc\u666f\u4e0a\u8fdb\u884calpha\u6df7\u5408\u6765\u9002\u914d\u6807\u51c6RGB\u6307\u6807\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86ALPHAVAE\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefRGBA VAE\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u4e13\u7528alpha\u901a\u9053\u6765\u6269\u5c55\u9884\u8bad\u7ec3\u7684RGB VAE\u3002\u8be5\u6a21\u578b\u91c7\u7528\u590d\u5408\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e86alpha\u6df7\u5408\u50cf\u7d20\u91cd\u5efa\u3001\u5757\u7ea7\u4fdd\u771f\u5ea6\u3001\u611f\u77e5\u4e00\u81f4\u6027\u548c\u53ccKL\u6563\u5ea6\u7ea6\u675f\uff0c\u4ee5\u786e\u4fddRGB\u548calpha\u8868\u793a\u7684\u6f5c\u5728\u4fdd\u771f\u5ea6\u3002", "result": "\u6211\u4eec\u7684RGBA VAE\u6a21\u578b\u4ec5\u4f7f\u75288K\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff08\u800c\u4e4b\u524d\u65b9\u6cd5\u4f7f\u75281M\u56fe\u50cf\uff09\uff0c\u5728\u91cd\u5efa\u65b9\u9762\u76f8\u8f83LayerDiffuse\uff0cPSNR\u63d0\u9ad8\u4e864.9 dB\uff0cSSIM\u63d0\u9ad8\u4e863.2%\u3002\u5728\u6f5c\u5728\u6269\u6563\u6846\u67b6\u4e2d\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u5b83\u8fd8\u80fd\u5b9e\u73b0\u66f4\u51fa\u8272\u7684\u900f\u660e\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u9996\u4e2aRGBA\u57fa\u51c6\u548c\u5f00\u53d1\u9ad8\u6548\u7684ALPHAVAE\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u900f\u660e\u56fe\u50cf\u751f\u6210\u9886\u57df\u5b58\u5728\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u900f\u660e\u56fe\u50cf\u7684\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5df2\u516c\u5f00\uff0c\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.10155", "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u77e5\u8bc6\u84b8\u998f\u4e2d\u5e08\u751f\u6a21\u578b\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u4e0d\u540c\u7ef4\u5ea6\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u84b8\u998f\u6559\u5e08\u6a21\u578b\u4e2d\u4e0e\u4efb\u52a1\u6700\u76f8\u5173\u7684\u9690\u85cf\u5355\u5143\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5e08\u751f\u6a21\u578b\u5171\u4eab\u76f8\u540c\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6\uff0c\u8fd9\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u67b6\u6784\u7075\u6d3b\u6027\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u8bad\u7ec3\u7ebf\u6027\u6295\u5f71\u5668\uff09\u4f1a\u5f15\u5165\u989d\u5916\u53c2\u6570\uff0c\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u7279\u522b\u662f\u751f\u6210\u4efb\u52a1\uff09\u4e0a\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u3001\u57fa\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5f15\u5165\u65b0\u53c2\u6570\uff0c\u5373\u53ef\u5b9e\u73b0\u4e0d\u540c\u9690\u85cf\u5c42\u7ef4\u5ea6\u7684\u5e08\u751f\u6a21\u578b\u4e4b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u8bc6\u522b\u6559\u5e08\u6a21\u578b\u4e2d\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u6700\u76f8\u5173\u7684\u9690\u85cf\u5355\u5143\uff0c\u5e76\u76f4\u63a5\u5c06\u5176\u6fc0\u6d3b\u503c\u84b8\u998f\u7ed9\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728\u5206\u7c7b\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6458\u8981\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u7279\u522b\u662f\u7ebf\u6027\u6295\u5f71\u57fa\u7ebf\uff09\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe3%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u84b8\u998f\u4e2d\u5e08\u751f\u6a21\u578b\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u5f15\u5165\u989d\u5916\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\uff0c\u5728\u591a\u7c7b\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2507.07855", "pdf": "https://arxiv.org/pdf/2507.07855", "abs": "https://arxiv.org/abs/2507.07855", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "title": "Principled Foundations for Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u662f\u673a\u5668\u5b66\u4e60\u4e2dSavage\u635f\u5931\u51fd\u6570\u548c\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u4e00\u79cd\u7279\u5b9a\u4e14\u901a\u7528\u7684\u8054\u7cfb\u5f62\u5f0f\u3002", "motivation": "\u7406\u89e3DPO\u7684\u8fd0\u4f5c\u539f\u7406\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5176\u6a21\u578b\u5e94\u7528\u5e7f\u6cdb\uff0cDPO\u5f53\u524d\u70ed\u5ea6\u9ad8\uff0c\u4e14\u8bb8\u591a\u5148\u8fdb\u7684DPO\u53d8\u4f53\u90fd\u53ef\u88ab\u7eb3\u5165\u6216\u89e3\u91ca\u5176\u5728\u6b64\u7406\u8bba\u6846\u67b6\u4e2d\u7684\u4f4d\u7f6e\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u4f18\u7f3a\u70b9\u548c\u89c4\u907f\u6f5c\u5728\u9677\u9631\u3002", "method": "\u901a\u8fc7\u5728\u901a\u7528\u5c42\u9762\u4e0a\u5efa\u7acbSavage\u635f\u5931\u51fd\u6570\u4e0e\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5c06DPO\u5c55\u793a\u4e3a\u8fd9\u79cd\u8054\u7cfb\u7684\u4e00\u79cd\u5177\u4f53\u5f62\u5f0f\u3002\u6b64\u65b9\u6cd5\u6db5\u76d6\u4e86\u9009\u62e9\u7406\u8bba\u4e2d\u7684\u5f03\u6743\u3001\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u975e\u51f8\u76ee\u6807\uff0c\u5e76\u80fd\u591f\u81ea\u7136\u5730\u89e3\u91caDPO\u7684\u73b0\u6709\u6269\u5c55\uff08\u5982\u8fb9\u9645\u548c\u957f\u5ea6\u4fee\u6b63\uff09\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cDPO\u662fSavage\u635f\u5931\u51fd\u6570\u4e0e\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u4e00\u79cd\u5177\u4f53\u8fde\u63a5\u3002\u8fd9\u79cd\u666e\u904d\u7684\u8fde\u63a5\u652f\u6301\u9009\u62e9\u7406\u8bba\u4e2d\u7684\u5f03\u6743\u3001\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u975e\u51f8\u76ee\u6807\uff0c\u5e76\u80fd\u514d\u8d39\u5730\u6784\u5efaDPO\u7684\u663e\u8457\u6269\u5c55\uff0c\u5305\u62ec\u8fb9\u9645\u548c\u957f\u5ea6\u4fee\u6b63\u3002", "conclusion": "\u901a\u8fc7\u5c06DPO\u7f6e\u4e8e\u66f4\u5e7f\u9614\u7684\u7406\u8bba\u6846\u67b6\u4e2d\uff0c\u53ef\u4ee5\u6df1\u5165\u7406\u89e3\u5176\u539f\u7406\uff0c\u6574\u5408\u73b0\u6709\u6269\u5c55\uff0c\u8bc6\u522b\u5f53\u524d\u6700\u5148\u8fdbDPO\u53d8\u4f53\u7684\u4f4d\u7f6e\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u504f\u79bb\u6b64\u6846\u67b6\u53ef\u80fd\u5e26\u6765\u7684\u95ee\u9898\u53ca\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09095", "pdf": "https://arxiv.org/pdf/2507.09095", "abs": "https://arxiv.org/abs/2507.09095", "authors": ["Md Hasan Shahriar", "Md Mohaimin Al Barat", "Harshavardhan Sundar", "Naren Ramakrishnan", "Y. Thomas Hou", "Wenjing Lou"], "title": "On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving", "categories": ["cs.LG"], "comment": "16 pages", "summary": "Multimodal fusion (MMF) plays a critical role in the perception of autonomous\ndriving, which primarily fuses camera and LiDAR streams for a comprehensive and\nefficient scene understanding. However, its strict reliance on precise temporal\nsynchronization exposes it to new vulnerabilities. In this paper, we introduce\nDejaVu, a novel attack that exploits network-induced delays to create subtle\ntemporal misalignments across sensor streams, severely degrading downstream\nMMF-based perception tasks. Our comprehensive attack analysis across different\nmodels and datasets reveals these sensors' task-specific imbalanced\nsensitivities: object detection is overly dependent on LiDAR inputs while\nobject tracking is highly reliant on the camera inputs. Consequently, with a\nsingle-frame LiDAR delay, an attacker can reduce the car detection mAP by up to\n88.5%, while with a three-frame camera delay, multiple object tracking accuracy\n(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense\npatch that can work alongside the existing perception model to monitor temporal\nalignment through cross-modal temporal consistency. AION leverages multimodal\nshared representation learning and dynamic time warping to determine the path\nof temporal alignment and calculate anomaly scores based on the alignment. Our\nthorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with\nlow false positives across datasets and model architectures, demonstrating it\nas a robust and generalized defense against the temporal misalignment attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDejaVu\u653b\u51fb\uff0c\u5229\u7528\u7f51\u7edc\u5ef6\u8fdf\u5728\u81ea\u52a8\u9a7e\u9a76\u591a\u6a21\u6001\u878d\u5408\u4e2d\u5236\u9020\u4f20\u611f\u5668\u65f6\u95f4\u9519\u4f4d\uff0c\u5bfc\u81f4\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff09\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u4e3a\u5e94\u5bf9\u6b64\u5a01\u80c1\uff0c\u63d0\u51faAION\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u65f6\u95f4\u4e00\u81f4\u6027\u68c0\u6d4b\u6b64\u7c7b\u653b\u51fb\uff0c\u5e76\u8868\u73b0\u51fa\u9ad8\u68c0\u6d4b\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\uff08MMF\uff09\u4e25\u91cd\u4f9d\u8d56\u7cbe\u786e\u7684\u65f6\u95f4\u540c\u6b65\uff0c\u8fd9\u66b4\u9732\u4e86\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u7f51\u7edc\u8bf1\u5bfc\u7684\u5ef6\u8fdf\u53ef\u80fd\u5bfc\u81f4\u7ec6\u5fae\u7684\u65f6\u95f4\u9519\u4f4d\uff0c\u4ece\u800c\u4e25\u91cd\u964d\u7ea7\u4e0b\u6e38\u7684MMF\u611f\u77e5\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u6f0f\u6d1e\u5e76\u63d0\u51fa\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u653b\u51fb\u65b9\u6cd5\uff08DejaVu\uff09\uff1a\u5229\u7528\u7f51\u7edc\u5f15\u5165\u7684\u5ef6\u8fdf\uff0c\u5728\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u6d41\u4e4b\u95f4\u5236\u9020\u65f6\u95f4\u9519\u4f4d\u3002\u9632\u5fa1\u65b9\u6cd5\uff08AION\uff09\uff1a\u4f5c\u4e3a\u73b0\u6709\u611f\u77e5\u6a21\u578b\u7684\u8865\u4e01\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u65f6\u95f4\u4e00\u81f4\u6027\u76d1\u63a7\u65f6\u95f4\u5bf9\u9f50\u3002\u5177\u4f53\u91c7\u7528\u591a\u6a21\u6001\u5171\u4eab\u8868\u793a\u5b66\u4e60\u548c\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u6765\u786e\u5b9a\u65f6\u95f4\u5bf9\u9f50\u8def\u5f84\u5e76\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "DejaVu\u653b\u51fb\u6548\u679c\u663e\u8457\uff1a\u5355\u4e2a\u6fc0\u5149\u96f7\u8fbe\u5e27\u5ef6\u8fdf\u53ef\u4f7f\u8f66\u8f86\u68c0\u6d4bmAP\u964d\u4f4e\u9ad8\u8fbe88.5%\uff1b\u4e09\u4e2a\u76f8\u673a\u5e27\u5ef6\u8fdf\u53ef\u4f7f\u8f66\u8f86\u591a\u76ee\u6807\u8ddf\u8e2a\u51c6\u786e\u7387\uff08MOTA\uff09\u4e0b\u964d73%\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u76ee\u6807\u68c0\u6d4b\u9ad8\u5ea6\u4f9d\u8d56\u6fc0\u5149\u96f7\u8fbe\u8f93\u5165\uff0c\u800c\u76ee\u6807\u8ddf\u8e2a\u9ad8\u5ea6\u4f9d\u8d56\u76f8\u673a\u8f93\u5165\u3002AION\u9632\u5fa1\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5747\u8fbe\u52300.92-0.98\u7684AUROC\u5206\u6570\uff0c\u8bef\u62a5\u7387\u4f4e\uff0c\u8bc1\u660e\u5176\u5bf9\u65f6\u95f4\u9519\u4f4d\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u65f6\u95f4\u540c\u6b65\u662f\u81ea\u52a8\u9a7e\u9a76\u591a\u6a21\u6001\u878d\u5408\u7684\u5173\u952e\u5f31\u70b9\uff0cDejaVu\u653b\u51fb\u80fd\u591f\u6709\u6548\u5229\u7528\u6b64\u5f31\u70b9\u9020\u6210\u4e25\u91cd\u6027\u80fd\u9000\u5316\u3002AION\u9632\u5fa1\u673a\u5236\u4e3a\u68c0\u6d4b\u6b64\u7c7b\u65f6\u95f4\u9519\u4f4d\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.09313", "pdf": "https://arxiv.org/pdf/2507.09313", "abs": "https://arxiv.org/abs/2507.09313", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProactiveBench\u57fa\u51c6\u548cPAUC\u8bc4\u4f30\u6307\u6807\uff0c\u65e8\u5728\u4fc3\u8fdb\u548c\u51c6\u786e\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\uff0c\u7279\u522b\u662f\u8003\u8651\u54cd\u5e94\u7684\u65f6\u95f4\u52a8\u6001\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u7684\u6df1\u5165\uff0c\u7528\u6237\u671f\u671b\u7cfb\u7edf\u80fd\u8fdb\u884c\u66f4\u4e3b\u52a8\u7684\u4ea4\u4e92\uff0c\u4f8b\u5982\u5728\u89c6\u9891\u64ad\u653e\u65f6\u81ea\u4e3b\u51b3\u5b9a\u591a\u8f6e\u54cd\u5e94\u65f6\u673a\uff0c\u800c\u975e\u4f20\u7edf\u7684\u8f6e\u6d41\u5bf9\u8bdd\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u8861\u91cf\u8fd9\u79cd\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86ProactiveBench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86PAUC\uff0c\u8fd9\u662f\u9996\u4e2a\u8003\u8651\u6a21\u578b\u54cd\u5e94\u65f6\u95f4\u52a8\u6001\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4e3b\u52a8\u5f0f\u7cfb\u7edf\u3002\u901a\u8fc7\u5bf9ProactiveBench\u4e0a\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\u8fdb\u884c\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u504f\u597d\u7814\u7a76\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cPAUC\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u4f18\u4e8e\u4f20\u7edf\u4ec5\u8003\u8651\u6587\u672c\u5185\u5bb9\u7684\u8bc4\u4f30\u6307\u6807\u3002\u8fd9\u901a\u8fc7\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "PAUC\u80fd\u66f4\u5fe0\u5b9e\u5730\u8bc4\u4f30\u4e3b\u52a8\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u3002ProactiveBench\u548cPAUC\u7684\u63d0\u51fa\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u4e3b\u52a8\u4ea4\u4e92\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10177", "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\uff08Gemini, GPT-4o, DeepSeek, Groq\uff09\u5c06\u5192\u72af\u6027\u6587\u672c\u8f6c\u6362\u4e3a\u975e\u5192\u72af\u6027\u6587\u672c\u7684\u80fd\u529b\uff0c\u65e8\u5728\u4fdd\u7559\u539f\u610f\u3002\u7ed3\u679c\u663e\u793aGroq\u8868\u73b0\u72ec\u7279\uff0cGPT-4o\u4e0eDeepSeek-V3\u76f8\u4f3c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u5206\u7c7b\u548c\u8f6c\u6362\u5192\u72af\u6027\u6587\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u4ecd\u6709\u5f85\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u4f7f\u7528LLMs\uff08Gemini, GPT-4o, DeepSeek, Groq\uff09\u5c06\u5305\u542b\u4ec7\u6068\u8a00\u8bba\u548c\u810f\u8bdd\u7684\u5192\u72af\u6027\u6587\u672c\uff08\u63a8\u6587\u548c\u8bc4\u8bba\uff09\u8f6c\u6362\u4e3a\u975e\u5192\u72af\u6027\u6587\u672c\uff0c\u5e76\u786e\u4fdd\u4fdd\u7559\u6587\u672c\u539f\u610f\u3002\u901a\u8fc7\u60c5\u611f\u5206\u6790\u548c\u8bed\u4e49\u5206\u6790\u8bc4\u4f30\u539f\u59cb\u548c\u8f6c\u6362\u540e\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "result": "Groq\u4e0e\u5176\u4ed6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u63d0\u4f9b\u4e86\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\u3002GPT-4o\u548cDeepSeek-V3\u4e4b\u95f4\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "conclusion": "\u5728\u5192\u72af\u6027\u6587\u672c\u8f6c\u6362\u4efb\u52a1\u4e2d\uff0c\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cGroq\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u7ed3\u679c\uff0c\u800cGPT-4o\u4e0eDeepSeek-V3\u5219\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u5904\u7406\u503e\u5411\u3002"}}
{"id": "2507.08052", "pdf": "https://arxiv.org/pdf/2507.08052", "abs": "https://arxiv.org/abs/2507.08052", "authors": ["Mazen Ali", "Ant\u00f3nio Pereira", "Fabio Gentile", "Aser Cortines", "Sam Mugel", "Rom\u00e1n Or\u00fas", "Stelios P. Neophytides", "Michalis Mavrovouniotis"], "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cloud and cloud shadow masking is a crucial preprocessing step in\nhyperspectral satellite imaging, enabling the extraction of high-quality,\nanalysis-ready data. This study evaluates various machine learning approaches,\nincluding gradient boosting methods such as XGBoost and LightGBM as well as\nconvolutional neural networks (CNNs). All boosting and CNN models achieved\naccuracies exceeding 93%. Among the investigated models, the CNN with feature\nreduction emerged as the most efficient, offering a balance of high accuracy,\nlow storage requirements, and rapid inference times on both CPUs and GPUs.\nVariations of this version, with only up to 597 trainable parameters,\ndemonstrated the best trade-off in terms of deployment feasibility, accuracy,\nand computational efficiency. These results demonstrate the potential of\nlightweight artificial intelligence (AI) models for real-time hyperspectral\nimage processing, supporting the development of on-board satellite AI systems\nfor space-based applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5305\u62ec\u68af\u5ea6\u63d0\u5347\u548cCNN\uff09\u5728\u53bb\u9664\u9ad8\u5149\u8c31\u536b\u661f\u5f71\u50cf\u4e91\u5f71\u65b9\u9762\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5177\u6709\u4f4e\u5b58\u50a8\u548c\u5feb\u901f\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u65f6\u5904\u7406\u548c\u536b\u661f\u677f\u8f7dAI\u7cfb\u7edf\u3002", "motivation": "\u4e91\u548c\u4e91\u5f71\u906e\u853d\u662f\u9ad8\u5149\u8c31\u536b\u661f\u6210\u50cf\u4e2d\u4e00\u9879\u5173\u952e\u7684\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5b83\u5bf9\u4e8e\u63d0\u53d6\u9ad8\u8d28\u91cf\u3001\u53ef\u4f9b\u5206\u6790\u7684\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\uff08\u5982XGBoost\u548cLightGBM\uff09\u4ee5\u53ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u5e26\u6709\u7279\u5f81\u964d\u7ef4\u7684CNN\u53ca\u5176\u53c2\u6570\u91cf\u6781\u5c0f\u7684\u53d8\u4f53\u3002", "result": "\u6240\u6709\u63d0\u5347\u65b9\u6cd5\u548cCNN\u6a21\u578b\u90fd\u8fbe\u5230\u4e86\u8d85\u8fc793%\u7684\u51c6\u786e\u7387\u3002\u5176\u4e2d\uff0c\u5e26\u6709\u7279\u5f81\u964d\u7ef4\u7684CNN\u88ab\u8bc1\u660e\u662f\u6700\u6709\u6548\u7684\uff0c\u5728\u51c6\u786e\u6027\u3001\u5b58\u50a8\u9700\u6c42\u548cCPU/GPU\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002\u5176\u8f7b\u91cf\u7ea7\u7248\u672c\uff08\u6700\u9ad8597\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff09\u5728\u90e8\u7f72\u53ef\u884c\u6027\u3001\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u6700\u4f73\u7684\u6743\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u5b9e\u65f6\u9ad8\u5149\u8c31\u56fe\u50cf\u5904\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u652f\u6301\u5f00\u53d1\u7528\u4e8e\u7a7a\u95f4\u5e94\u7528\u7684\u536b\u661f\u677f\u8f7dAI\u7cfb\u7edf\u3002"}}
{"id": "2507.09101", "pdf": "https://arxiv.org/pdf/2507.09101", "abs": "https://arxiv.org/abs/2507.09101", "authors": ["Yanan Cao", "Omid Memarrast", "Shiqin Cai", "Sinduja Subramaniam", "Evren Korpeoglu", "Kannan Achan"], "title": "S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe", "categories": ["cs.LG"], "comment": null, "summary": "In grocery e-commerce, customers often build ingredient baskets guided by\ndietary preferences but lack the expertise to create complete meals. Leveraging\nrecipe knowledge to recommend complementary ingredients based on a partial\nbasket is essential for improving the culinary experience. Traditional recipe\ncompletion methods typically predict a single missing ingredient using a\nleave-one-out strategy. However, they fall short in two key aspects: (i) they\ndo not reflect real-world scenarios where multiple ingredients are often\nneeded, and (ii) they overlook relationships among the missing ingredients\nthemselves. To address these limitations, we reformulate basket completion as a\nset-to-set (S2S) recommendation problem, where an incomplete basket is input\ninto a system that predicts a set of complementary ingredients. We introduce\nS2SRec2, a set-to-set ingredient recommendation framework based on a Set\nTransformer and trained in a multitask learning paradigm. S2SRec2 jointly\nlearns to (i) retrieve missing ingredients from the representation of existing\nones and (ii) assess basket completeness after prediction. These tasks are\noptimized together, enforcing accurate retrieval and coherent basket\ncompletion. Experiments on large-scale recipe datasets and qualitative analyses\nshow that S2SRec2 significantly outperforms single-target baselines, offering a\npromising approach to enhance grocery shopping and inspire culinary creativity.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u7535\u5546\u8d2d\u7269\u4e2d\u7528\u6237\u96be\u4ee5\u51d1\u9f50\u5b8c\u6574\u9910\u98df\u7684\u95ee\u9898\uff0c\u672c\u6587\u5c06\u8d2d\u7269\u7bee\u8865\u5168\u91cd\u6784\u4e3a\u96c6\u5408\u5230\u96c6\u5408\u63a8\u8350\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eSet Transformer\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6S2SRec2\uff0c\u65e8\u5728\u63a8\u8350\u591a\u7f3a\u5931\u98df\u6750\u5e76\u8bc4\u4f30\u8d2d\u7269\u7bee\u5b8c\u6574\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u751f\u9c9c\u7535\u5546\u4e2d\uff0c\u987e\u5ba2\u901a\u5e38\u6309\u996e\u98df\u504f\u597d\u9009\u62e9\u98df\u6750\uff0c\u4f46\u7f3a\u4e4f\u521b\u5efa\u5b8c\u6574\u9910\u98df\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u4f20\u7edf\u98df\u8c31\u8865\u5168\u65b9\u6cd5\u53ea\u9884\u6d4b\u5355\u4e2a\u7f3a\u5931\u98df\u6750\uff0c\u4e0d\u7b26\u5408\u5b9e\u9645\u591a\u98df\u6750\u9700\u6c42\uff0c\u4e14\u5ffd\u7565\u7f3a\u5931\u98df\u6750\u95f4\u7684\u5173\u7cfb\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63a8\u8350\u591a\u4e2a\u4e92\u8865\u98df\u6750\u5e76\u8003\u8651\u5176\u5185\u90e8\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u8d2d\u7269\u7bee\u8865\u5168\u91cd\u6784\u4e3a\u96c6\u5408\u5230\u96c6\u5408\uff08S2S\uff09\u63a8\u8350\u95ee\u9898\uff0c\u5373\u8f93\u5165\u4e0d\u5b8c\u6574\u8d2d\u7269\u7bee\uff0c\u9884\u6d4b\u4e00\u7ec4\u4e92\u8865\u98df\u6750\u3002\u63d0\u51fa\u4e86S2SRec2\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eSet Transformer\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u8303\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002S2SRec2\u534f\u540c\u5b66\u4e60\u4e24\u4e2a\u4efb\u52a1\uff1a\u4ece\u73b0\u6709\u98df\u6750\u8868\u793a\u4e2d\u68c0\u7d22\u7f3a\u5931\u98df\u6750\uff1b\u4ee5\u53ca\u5728\u9884\u6d4b\u540e\u8bc4\u4f30\u8d2d\u7269\u7bee\u7684\u5b8c\u6574\u6027\u3002\u8fd9\u4e24\u4e2a\u4efb\u52a1\u88ab\u5171\u540c\u4f18\u5316\u3002", "result": "\u5728\u5927\u578b\u98df\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cS2SRec2\u663e\u8457\u4f18\u4e8e\u5355\u76ee\u6807\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "S2SRec2\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u5728\u7ebf\u6742\u8d27\u8d2d\u7269\u4f53\u9a8c\u5e76\u6fc0\u53d1\u70f9\u996a\u521b\u610f\u3002"}}
{"id": "2507.09323", "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDICCAE\u6a21\u578b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7c7b\u522b\u5bf9\u9f50\u548c\u52a8\u6001\u6df7\u6dc6\u635f\u5931\uff0c\u89e3\u51b3\u97f3\u89c6\u9891\u9884\u8bad\u7ec3\u4e2d\u6613\u6df7\u6dc6\u7c7b\u522b\u7684\u533a\u5206\u95ee\u9898\uff0c\u5e76\u5728VGGSound\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u63a5\u8fd1SOTA\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u97f3\u89c6\u9891\u9884\u8bad\u7ec3\u8303\u5f0f\u4ec5\u5173\u6ce8\u6574\u4f53\u6a21\u6001\u5bf9\u9f50\uff0c\u672a\u80fd\u50cf\u4eba\u7c7b\u8ba4\u77e5\u90a3\u6837\u901a\u8fc7\u8ba4\u77e5\u5f52\u7eb3\u548c\u5bf9\u6bd4\u5f3a\u5316\u5bf9\u6613\u6df7\u6dc6\u7c7b\u522b\u7684\u533a\u5206\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5904\u7406\u76f8\u4f3c\u6d3b\u52a8\u65f6\u8fa8\u522b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u7c7b\u95f4\u6df7\u6dc6\u611f\u77e5\u7f16\u7801\u5668\uff08DICCAE\uff09\uff0c\u5b83\u901a\u8fc7\u6839\u636e\u7c7b\u95f4\u6df7\u6dc6\u7a0b\u5ea6\u52a8\u6001\u8c03\u6574\u6df7\u6dc6\u635f\u5931\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u7c7b\u522b\u7ea7\u522b\u7684\u97f3\u89c6\u9891\u8868\u793a\u5bf9\u9f50\uff0c\u4ee5\u589e\u5f3a\u76f8\u4f3c\u6d3b\u52a8\u533a\u5206\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u7ed3\u5408\u97f3\u89c6\u9891\u53ca\u5176\u878d\u5408\u7684\u65b0\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u7c07\u5f15\u5bfc\u7684\u97f3\u89c6\u9891\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u3002", "result": "DICCAE\u5728VGGSound\u6570\u636e\u96c6\u4e0aTop-1\u51c6\u786e\u7387\u8fbe\u523065.5%\uff0c\u63a5\u8fd1\u5f53\u524d\u6700\u4f73\u6027\u80fd\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u7279\u5f81\u8868\u793a\u8d28\u91cf\u4ee5\u53ca\u5404\u4e2a\u6a21\u5757\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "DICCAE\u901a\u8fc7\u5f15\u5165\u7ec6\u7c92\u5ea6\u7c7b\u522b\u5bf9\u9f50\u548c\u52a8\u6001\u6df7\u6dc6\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86\u97f3\u89c6\u9891\u6a21\u578b\u5bf9\u6613\u6df7\u6dc6\u7c7b\u522b\u7684\u533a\u5206\u80fd\u529b\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u97f3\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.10216", "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd Absher\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6c99\u7279\u963f\u62c9\u4f2f\u65b9\u8a00\u548c\u6587\u5316\u7ec6\u5fae\u7406\u89e3\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6b64\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u4e2d\u65e5\u76ca\u6838\u5fc3\uff0c\u5c24\u5176\u662f\u5728\u6c99\u7279\u963f\u62c9\u4f2f\u7b49\u8bed\u8a00\u591a\u6837\u6027\u5f3a\u7684\u5730\u533a\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u533a\u57df\u65b9\u8a00\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86\u540d\u4e3a Absher \u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8d85\u8fc718,000\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d6\u516d\u4e2a\u4e0d\u540c\u7c7b\u522b\uff08\u542b\u4e49\u3001\u771f/\u5047\u3001\u586b\u7a7a\u3001\u4e0a\u4e0b\u6587\u4f7f\u7528\u3001\u6587\u5316\u89e3\u91ca\u548c\u5730\u70b9\u8bc6\u522b\uff09\uff0c\u95ee\u9898\u6765\u6e90\u4e8e\u6c99\u7279\u963f\u62c9\u4f2f\u4e0d\u540c\u5730\u533a\u7684\u65b9\u8a00\u8bcd\u6c47\u3001\u77ed\u8bed\u548c\u8c1a\u8bed\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u591a\u8bed\u8a00\u548c\u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u6a21\u578b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u6587\u5316\u63a8\u65ad\u6216\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8feb\u5207\u9700\u8981\u8fdb\u884c\u65b9\u8a00\u611f\u77e5\u8bad\u7ec3\u548c\u6587\u5316\u5bf9\u9f50\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u963f\u62c9\u4f2f\u8bed\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.08827", "pdf": "https://arxiv.org/pdf/2507.08827", "abs": "https://arxiv.org/abs/2507.08827", "authors": ["Yu Zheng", "Jingtao Ding", "Depeng Jin", "Jianxi Gao", "Yong Li"], "title": "Advancing network resilience theories with symbolized reinforcement learning", "categories": ["physics.soc-ph", "cs.AI"], "comment": null, "summary": "Many complex networks display remarkable resilience under external\nperturbations, internal failures and environmental changes, yet they can\nswiftly deteriorate into dysfunction upon the removal of a few keystone nodes.\nDiscovering theories that measure network resilience offers the potential to\nprevent catastrophic collapses--from species extinctions to financial\ncrise--with profound implications for real-world systems. Current resilience\ntheories address the problem from a single perspective of topology, neglecting\nthe crucial role of system dynamics, due to the intrinsic complexity of the\ncoupling between topology and dynamics which exceeds the capabilities of human\nanalytical methods. Here, we report an automatic method for resilience theory\ndiscovery, which learns from how AI solves a complicated network dismantling\nproblem and symbolizes its network attack strategies into theoretical formulas.\nThis proposed self-inductive approach discovers the first resilience theory\nthat accounts for both topology and dynamics, highlighting how the correlation\nbetween node degree and state shapes overall network resilience, and offering\ninsights for designing early warning signals of systematic collapses.\nAdditionally, our approach discovers formulas that refine existing\nwell-established resilience theories with over 37.5% improvement in accuracy,\nsignificantly advancing human understanding of complex networks with AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cdAI\u9a71\u52a8\u7684\u81ea\u5f52\u7eb3\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u590d\u6742\u7f51\u7edc\u7684\u97e7\u6027\u7406\u8bba\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u540c\u65f6\u8003\u8651\u62d3\u6251\u548c\u52a8\u529b\u5b66\u7684\u97e7\u6027\u7406\u8bba\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u7406\u8bba\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u9884\u6d4b\u7cfb\u7edf\u6027\u5d29\u6e83\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u590d\u6742\u7f51\u7edc\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u97e7\u6027\uff0c\u4f46\u79fb\u9664\u5c11\u6570\u5173\u952e\u8282\u70b9\u5373\u53ef\u5bfc\u81f4\u7cfb\u7edf\u8fc5\u901f\u5d29\u6e83\u3002\u73b0\u6709\u97e7\u6027\u7406\u8bba\u4ec5\u4ece\u62d3\u6251\u89d2\u5ea6\u7814\u7a76\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u5173\u952e\u4f5c\u7528\uff0c\u800c\u62d3\u6251\u4e0e\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u590d\u6742\u8026\u5408\u8d85\u51fa\u4e86\u4eba\u7c7b\u5206\u6790\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5f00\u53d1\u65b0\u7684\u7406\u8bba\u6765\u66f4\u5168\u9762\u5730\u8861\u91cf\u7f51\u7edc\u97e7\u6027\uff0c\u4ee5\u9884\u9632\u707e\u96be\u6027\u5d29\u6e83\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u97e7\u6027\u7406\u8bba\u53d1\u73b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u89e3\u51b3\u590d\u6742\u7684\u7f51\u7edc\u62c6\u89e3\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u7f51\u7edc\u653b\u51fb\u7b56\u7565\u7b26\u53f7\u5316\u4e3a\u7406\u8bba\u516c\u5f0f\u3002\u8fd9\u662f\u4e00\u79cd\u81ea\u5f52\u7eb3\uff08self-inductive\uff09\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\uff1a1) \u53d1\u73b0\u4e86\u7b2c\u4e00\u4e2a\u540c\u65f6\u8003\u8651\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u548c\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u97e7\u6027\u7406\u8bba\uff1b2) \u63ed\u793a\u4e86\u8282\u70b9\u5ea6\u4e0e\u8282\u70b9\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u8054\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u7f51\u7edc\u97e7\u6027\uff1b3) \u4e3a\u8bbe\u8ba1\u7cfb\u7edf\u6027\u5d29\u6e83\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u63d0\u4f9b\u4e86\u89c1\u89e3\uff1b4) \u53d1\u73b0\u7684\u516c\u5f0f\u5c06\u73b0\u6709\u6210\u719f\u97e7\u6027\u7406\u8bba\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e8637.5%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5AI\u9a71\u52a8\u7684\u65b9\u6cd5\u901a\u8fc7\u53d1\u73b0\u7ed3\u5408\u62d3\u6251\u548c\u52a8\u529b\u5b66\u7684\u65b0\u97e7\u6027\u7406\u8bba\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u4e86\u73b0\u6709\u97e7\u6027\u7406\u8bba\u7684\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u5bf9\u590d\u6742\u7f51\u7edc\u7684\u7406\u89e3\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.09127", "pdf": "https://arxiv.org/pdf/2507.09127", "abs": "https://arxiv.org/abs/2507.09127", "authors": ["Harshil Kotamreddy", "Marlos C. Machado"], "title": "A Study of Value-Aware Eigenoptions", "categories": ["cs.LG", "stat.ML"], "comment": "Presented at the RLC Workshop on Inductive Biases in Reinforcement\n  Learning 2025", "summary": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7279\u5f81\u9009\u9879\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u52a0\u901f\u4fe1\u7528\u5206\u914d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u9884\u8bbe\u7279\u5f81\u9009\u9879\u5bf9\u63a2\u7d22\u548c\u4fe1\u7528\u5206\u914d\u5747\u6709\u76ca\uff0c\u4f46\u5728\u7ebf\u5b66\u4e60\u9700\u6743\u8861\u3002", "motivation": "\u9009\u9879\u867d\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5f3a\u5927\u6846\u67b6\uff0c\u4f46\u5e38\u9700\u624b\u52a8\u8bbe\u8ba1\u800c\u975e\u5b66\u4e60\u3002\u7279\u5f81\u9009\u9879\u5728\u63a2\u7d22\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u4fe1\u7528\u5206\u914d\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u5728\u8868\u683c\u548c\u50cf\u7d20\u5316\u7f51\u683c\u4e16\u754c\u4e2d\u8bc4\u4f30\u4e86\u7279\u5f81\u9009\u9879\uff0c\u6bd4\u8f83\u4e86\u9884\u8bbe\u4e0e\u5728\u7ebf\u53d1\u73b0\u7684\u6548\u679c\u3002\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u51fd\u6570\u8fd1\u4f3c\u4e0b\u5b66\u4e60\u9009\u9879\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ec8\u6b62\u6761\u4ef6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u9884\u8bbe\u7279\u5f81\u9009\u9879\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63a2\u7d22\uff0c\u8fd8\u80fd\u52a0\u901f\u4fe1\u7528\u5206\u914d\uff1b\u800c\u5728\u7ebf\u53d1\u73b0\u7684\u7279\u5f81\u9009\u9879\u53ef\u80fd\u56e0\u8fc7\u5ea6\u504f\u7f6e\u667a\u80fd\u4f53\u7684\u7ecf\u9a8c\u800c\u963b\u788d\u5b66\u4e60\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4f7f\u7528\u7279\u5f81\u9009\u9879\uff08\u53ca\u66f4\u5e7f\u6cdb\u7684\u9009\u9879\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u540c\u65f6\u652f\u6301\u4fe1\u7528\u5206\u914d\u548c\u63a2\u7d22\u7684\u6f5c\u529b\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2507.09334", "pdf": "https://arxiv.org/pdf/2507.09334", "abs": "https://arxiv.org/abs/2507.09334", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFast3D\uff0c\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b33D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u8fc7\u591a\u7684\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u9884\u6d4b\u548c\u6837\u672c\u81ea\u9002\u5e94\u526a\u679d\u5b9e\u73b0\u9ad8\u6548\u52a0\u901f\u3002", "motivation": "3D MLLMs\u867d\u7136\u573a\u666f\u7406\u89e3\u80fd\u529b\u5f3a\uff0c\u4f46\u56e0\u5904\u7406\u8fc7\u591a\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u4ee4\u724c\u800c\u9762\u4e34\u4e25\u91cd\u7684\u8ba1\u7b97\u6548\u7387\u6311\u6218\u3002\u5c3d\u7ba12D MLLMs\u4e2d\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u5df2\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u4ee4\u724c\u7ed3\u6784\u7684\u57fa\u672c\u5dee\u5f02\uff0c\u5176\u57283D\u9886\u57df\u7684\u9002\u7528\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e3D\u4ee4\u724c\u5197\u4f59\u548c\u5168\u5c40\u6ce8\u610f\u529b\u6a21\u5f0f\u53ef\u9884\u6d4b\u975e\u5173\u952e\u4ee4\u724c\u7684\u6d1e\u5bdf\uff0c\u672c\u6587\u63d0\u51fa\u4e86Fast3D\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u6280\u672f\u521b\u65b0\uff1a1) \u5168\u5c40\u6ce8\u610f\u529b\u9884\u6d4b\uff08GAP\uff09\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u5168\u5c40\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u4ee5\u6709\u6548\u4f30\u8ba1\u4ee4\u724c\u91cd\u8981\u6027\u8fdb\u884c\u7cbe\u786e\u526a\u679d\uff1b2) \u6837\u672c\u81ea\u9002\u5e94\u89c6\u89c9\u4ee4\u724c\u526a\u679d\uff08SAP\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u590d\u6742\u5ea6\u8bc4\u4f30\u5f15\u5165\u52a8\u6001\u4ee4\u724c\u9884\u7b97\uff0c\u6839\u636e\u8f93\u5165\u7279\u6027\u81ea\u52a8\u8c03\u6574\u9010\u5c42\u526a\u679d\u6bd4\u4f8b\u3002\u8fd9\u4e24\u4e2a\u6280\u672f\u5747\u4e0d\u4fee\u6539\u76ee\u6807\u6a21\u578b\u7684\u53c2\u6570\u3002", "result": "Fast3D\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7387\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Fast3D\u6210\u529f\u5730\u901a\u8fc7\u6709\u6548\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\uff0c\u89e3\u51b3\u4e863D MLLMs\u7684\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u63d0\u9ad8\u4e86\u5176\u5b9e\u7528\u90e8\u7f72\u6027\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6a21\u578b\u53c2\u6570\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u52a0\u901f\u6548\u679c\u3002"}}
{"id": "2507.10326", "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "categories": ["cs.CL"], "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6f14\u5316\u641c\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u6cd5\u6307\u5bfc\u9057\u4f20\u7f16\u7a0b\u548c\u5c40\u90e8\u641c\u7d22\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u79bb\u6563\u63d0\u793a\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u548c\u5c0f\u578bLLM\u7684\u63d0\u793a\u4f18\u5316\u96be\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8e\u7b80\u5355\u4efb\u52a1\u548c\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f46\u5bf9\u4e8e\u9700\u8981\u8be6\u7ec6\u4fe1\u606f\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4f18\u5316\u6587\u672c\u91cf\u589e\u52a0\uff0c\u4e14\u5c0f\u578bLLM\u5bf9\u63d0\u793a\u8bbe\u8ba1\u66f4\u654f\u611f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u79bb\u6563\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6f14\u5316\u641c\u7d22\u65b9\u6cd5\u3002\u7b2c\u4e00\u9636\u6bb5\uff1a\u5229\u7528\u8bed\u6cd5\u6307\u5bfc\u9057\u4f20\u7f16\u7a0b\uff08grammar-guided genetic programming\uff09\uff0c\u901a\u8fc7\u641c\u7d22\u7531\u53e5\u6cd5\u3001\u57fa\u4e8e\u5b57\u5178\u548c\u57fa\u4e8eLLM\u7684\u63d0\u793a\u7f16\u8f91\u51fd\u6570\u7ec4\u5408\u800c\u6210\u7684\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u6765\u5408\u6210\u63d0\u793a\u521b\u5efa\u7a0b\u5e8f\u3002\u7b2c\u4e8c\u9636\u6bb5\uff1a\u5e94\u7528\u5c40\u90e8\u641c\u7d22\uff08local search\uff09\uff0c\u5728\u8868\u73b0\u6700\u4f73\u7a0b\u5e8f\u7684\u90bb\u57df\u5185\u8fdb\u4e00\u6b65\u5fae\u8c03\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56db\u9879\u9886\u57df\u7279\u5b9a\u6311\u6218\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e09\u79cd\u76f8\u5bf9\u8f83\u5c0f\u7684\u901a\u7528LLM\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8ePromptWizard\u3001OPRO\u548cRL-Prompt\u8fd9\u4e09\u79cd\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002\u672c\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1-\u6a21\u578b\u7ec4\u5408\u4e2d\u5747\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u5373\u4f7f\u672a\u80fd\u63d0\u5347\uff0c\u6027\u80fd\u4e0b\u964d\u4e5f\u5fae\u4e4e\u5176\u5fae\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6f14\u5316\u641c\u7d22\u65b9\u6cd5\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u79bb\u6563\u63d0\u793a\u4f18\u5316\uff0c\u5c24\u5176\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\u548c\u5c0f\u578bLLM\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5176\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.09132", "pdf": "https://arxiv.org/pdf/2507.09132", "abs": "https://arxiv.org/abs/2507.09132", "authors": ["Chu-Yuan Wei", "Shun-Yao Liu", "Sheng-Da Zhuo", "Chang-Dong Wang", "Shu-Qiang Huang", "Mohsen Guizani"], "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based tasks (e.g., node classification or link prediction). Despite their\ntriumphs, GNNs still face challenges such as long training and inference times,\ndifficulty in capturing complex relationships, and insufficient feature\nextraction. To tackle these issues, graph pre-training and graph prompt methods\nhave garnered increasing attention for their ability to leverage large-scale\ndatasets for initial learning and task-specific adaptation, offering potential\nimprovements in GNN performance. However, previous research has overlooked the\npotential of graph prompts in optimizing models, as well as the impact of both\npositive and negative graph prompts on model stability and efficiency. To\nbridge this gap, we propose a novel framework combining graph prompts with\nweight pruning, called GPAWP, which aims to enhance the performance and\nefficiency of graph prompts by using fewer of them. We evaluate the importance\nof graph prompts using an importance assessment function to determine positive\nand negative weights at different granularities. Through hierarchically\nstructured pruning, we eliminate negative prompt labels, resulting in more\nparameter-efficient and competitively performing prompts. Extensive experiments\non three benchmark datasets demonstrate the superiority of GPAWP, leading to a\nsignificant reduction in parameters in node classification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGPAWP\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u63d0\u793a\u4e0e\u6743\u91cd\u526a\u679d\uff0c\u901a\u8fc7\u8bc4\u4f30\u548c\u526a\u679d\u8d1f\u9762\u63d0\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u56fe\u63d0\u793a\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u51cf\u5c11\u53c2\u6570\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u9762\u4e34\u8bad\u7ec3\u63a8\u7406\u8017\u65f6\u957f\u3001\u96be\u4ee5\u6355\u6349\u590d\u6742\u5173\u7cfb\u53ca\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u7b49\u6311\u6218\u3002\u56fe\u9884\u8bad\u7ec3\u548c\u56fe\u63d0\u793a\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u56fe\u63d0\u793a\u5728\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u6b63\u8d1f\u56fe\u63d0\u793a\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faGPAWP\uff08Graph Prompts with Weight Pruning\uff09\u6846\u67b6\uff0c\u5c06\u56fe\u63d0\u793a\u4e0e\u6743\u91cd\u526a\u679d\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u4f30\u51fd\u6570\u786e\u5b9a\u4e0d\u540c\u7c92\u5ea6\u7684\u6b63\u8d1f\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u7ed3\u6784\u526a\u679d\u6765\u6d88\u9664\u8d1f\u9762\u63d0\u793a\u6807\u7b7e\uff0c\u65e8\u5728\u7528\u66f4\u5c11\u7684\u63d0\u793a\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86GPAWP\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u3002", "conclusion": "GPAWP\u6846\u67b6\u901a\u8fc7\u5bf9\u56fe\u63d0\u793a\u7684\u6709\u6548\u526a\u679d\uff0c\u6210\u529f\u63d0\u9ad8\u4e86GNN\u6a21\u578b\u4e2d\u56fe\u63d0\u793a\u7684\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3GNN\u9762\u4e34\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.09338", "pdf": "https://arxiv.org/pdf/2507.09338", "abs": "https://arxiv.org/abs/2507.09338", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brun\u00f3 B. Englert", "Gijs Dubbelman"], "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "categories": ["cs.CV"], "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\uff0c\u7b80\u5355\u7684\u7f16\u7801\u5668-only\u89c6\u9891Vision Transformers (Video ViTs)\u6a21\u578b\u5728\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u8d85\u8d8a\u590d\u6742\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u65b9\u6cd5\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u591a\u9636\u6bb5\u6216\u591a\u8868\u793a\u878d\u5408\u67b6\u6784\uff0c\u4f46\u5176\u5fc5\u8981\u6027\u5b58\u7591\u3002\u9274\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u9ad8\u7ea7\u9884\u8bad\u7ec3\u652f\u6301\u4e0b\u80fd\u4ee5\u7b80\u5355\u67b6\u6784\u8d85\u8d8a\u4e13\u7528\u8bbe\u8ba1\uff0c\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u7b80\u5355\u7f16\u7801\u5668-only\u65b9\u6cd5\u5728TAD\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u67b6\u6784\u7b80\u5355\u7684\u7f16\u7801\u5668-only\u65b9\u6cd5\uff0c\u4f7f\u7528\u666e\u901a\u7684\u89c6\u9891Vision Transformers (Video ViTs)\u3002\u901a\u8fc7\u63a2\u7d22\u4e0d\u540c\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5305\u62ec\u5f31\u76d1\u7763\u3001\u5168\u76d1\u7763\u3001\u81ea\u76d1\u7763\u7684\u63a9\u7801\u89c6\u9891\u5efa\u6a21MVM\uff09\u53ca\u5176\u5bf9TAD\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u4e00\u6b65\u5728\u672a\u6807\u8bb0\u7684\u9a7e\u9a76\u89c6\u9891\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i) \u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u4f7f\u7b80\u5355\u7684\u7f16\u7801\u5668-only\u6a21\u578b\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e13\u4e1a\u5316\u7684\u6700\u5148\u8fdbTAD\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6548\u7387\u663e\u8457\u66f4\u9ad8\uff1b(ii) \u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u5bf9TAD\u6548\u679c\u4e0d\u4f73\uff0c\u81ea\u76d1\u7763\u7684\u63a9\u7801\u89c6\u9891\u5efa\u6a21\uff08MVM\uff09\u63d0\u4f9b\u4e86\u6700\u5f3a\u7684\u4fe1\u53f7\uff1b(iii) \u5728\u672a\u6807\u8bb0\u9a7e\u9a76\u89c6\u9891\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u4e0b\u6e38\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5f02\u5e38\u6837\u672c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9884\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u53ef\u4ee5\u4ee5\u6700\u5c0f\u7684\u67b6\u6784\u590d\u6742\u6027\u6784\u5efa\u6709\u6548\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u6a21\u578b\u3002"}}
{"id": "2507.10330", "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u589e\u957f\u754c\u9650\u77e9\u9635\uff08GBM\uff09\u7684\u65b0\u578b\u6b63\u5219\u5316\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8NLP\u6a21\u578b\uff08\u5305\u62ecLSTM\u3001S4\u548cCNN\uff09\u5bf9\u6297\u540c\u4e49\u8bcd\u66ff\u6362\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1NLP\u6a21\u578b\u6709\u8fdb\u6b65\uff0c\u4f46\u4ecd\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u5982\u540c\u4e49\u8bcd\u66ff\u6362\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u524d\u9988\u548c\u5377\u79ef\u7f51\u7edc\uff0c\u800c\u5faa\u73af\u7f51\u7edc\uff08\u5982LSTM\uff09\u548c\u73b0\u4ee3\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982S4\uff09\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5b83\u4eec\u5177\u6709\u72ec\u7279\u7684\u5904\u7406\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u589e\u957f\u754c\u9650\u77e9\u9635\uff08GBM\uff09\u7684\u6b63\u5219\u5316\u6280\u672f\uff0c\u901a\u8fc7\u964d\u4f4e\u8f93\u5165\u6270\u52a8\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u6765\u63d0\u5347\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u8ba1\u7b97LSTM\u3001S4\u548cCNN\u8fd9\u4e09\u79cd\u67b6\u6784\u7684GBM\u3002", "result": "\u5728\u591a\u67b6\u6784\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5bf9\u6297\u9c81\u68d2\u6027\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9ad8\u8fbe8.8%\uff0c\u5e76\u8d85\u8d8a\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GBM\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8NLP\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9LSTM\u3001S4\u548cCNN\u67b6\u6784\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86S4\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09137", "pdf": "https://arxiv.org/pdf/2507.09137", "abs": "https://arxiv.org/abs/2507.09137", "authors": ["Nripsuta Ani Saxena", "Shang-Ling Hsu", "Mehul Shetty", "Omar Alkhadra", "Cyrus Shahabi", "Abigail L. Horn"], "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurately attributing user visits to specific Points of Interest (POIs) is a\nfoundational task for mobility analytics, personalized services, marketing and\nurban planning. However, POI attribution remains challenging due to GPS\ninaccuracies, typically ranging from 2 to 20 meters in real-world settings, and\nthe high spatial density of POIs in urban environments, where multiple venues\ncan coexist within a small radius (e.g., over 50 POIs within a 100-meter radius\nin dense city centers). Relying on proximity is therefore often insufficient\nfor determining which POI was actually visited. We introduce\n\\textsf{POIFormer}, a novel Transformer-based framework for accurate and\nefficient POI attribution. Unlike prior approaches that rely on limited\nspatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly\nmodels a rich set of signals, including spatial proximity, visit timing and\nduration, contextual features from POI semantics, and behavioral features from\nuser mobility and aggregated crowd behavior patterns--using the Transformer's\nself-attention mechanism to jointly model complex interactions across these\ndimensions. By leveraging the Transformer to model a user's past and future\nvisits (with the current visit masked) and incorporating crowd-level behavioral\npatterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate,\nefficient attribution in large, noisy mobility datasets. Its architecture\nsupports generalization across diverse data sources and geographic contexts\nwhile avoiding reliance on hard-to-access or unavailable data layers, making it\npractical for real-world deployment. Extensive experiments on real-world\nmobility datasets demonstrate significant improvements over existing baselines,\nparticularly in challenging real-world settings characterized by spatial noise\nand dense POI clustering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86POIFormer\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e30\u5bcc\u7684\u65f6\u7a7a\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u4e3a\u7279\u5f81\uff0c\u89e3\u51b3\u4e86GPS\u4e0d\u51c6\u786e\u548cPOI\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e0b\u5174\u8da3\u70b9\uff08POI\uff09\u5f52\u56e0\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u5f52\u56e0\u3002", "motivation": "POI\u5f52\u56e0\u662f\u79fb\u52a8\u5206\u6790\u3001\u4e2a\u6027\u5316\u670d\u52a1\u548c\u57ce\u5e02\u89c4\u5212\u7684\u57fa\u7840\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8eGPS\u7cbe\u5ea6\u95ee\u9898\uff082-20\u7c73\u8bef\u5dee\uff09\u548c\u57ce\u5e02\u4e2dPOI\u7684\u9ad8\u5ea6\u7a7a\u95f4\u5bc6\u96c6\u6027\uff08\u5982100\u7c73\u518550\u591a\u4e2aPOI\uff09\uff0c\u4ec5\u4f9d\u8d56\u8ddd\u79bb\u4e0d\u8db3\u4ee5\u51c6\u786e\u5224\u65ad\u5b9e\u9645\u8bbf\u95ee\u7684POI\uff0c\u8fd9\u4f7f\u5f97\u8be5\u4efb\u52a1\u5145\u6ee1\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86POIFormer\uff0c\u4e00\u4e2a\u65b0\u578b\u7684\u57fa\u4e8eTransformer\u7684POI\u5f52\u56e0\u6846\u67b6\u3002\u5b83\u5229\u7528Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u4e86\u4e30\u5bcc\u7684\u4fe1\u53f7\uff0c\u5305\u62ec\u7a7a\u95f4\u90bb\u8fd1\u5ea6\u3001\u8bbf\u95ee\u65f6\u95f4\u4e0e\u65f6\u957f\u3001POI\u8bed\u4e49\uff08\u4e0a\u4e0b\u6587\u7279\u5f81\uff09\u3001\u7528\u6237\u79fb\u52a8\u884c\u4e3a\u4ee5\u53ca\u805a\u5408\u7684\u7fa4\u4f53\u884c\u4e3a\u6a21\u5f0f\uff08\u901a\u8fc7\u9884\u8ba1\u7b97\u7684KDEs\uff09\u3002POIFormer\u901a\u8fc7\u63a9\u7801\u5f53\u524d\u8bbf\u95ee\u6765\u5229\u7528\u7528\u6237\u7684\u8fc7\u53bb\u548c\u672a\u6765\u8bbf\u95ee\uff0c\u5e76\u6574\u5408\u7fa4\u4f53\u5c42\u9762\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u5f52\u56e0\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPOIFormer\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u566a\u58f0\u548cPOI\u5bc6\u96c6\u805a\u7c7b\u7b49\u6311\u6218\u6027\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u3001\u5608\u6742\u79fb\u52a8\u6570\u636e\u96c6\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "POIFormer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684POI\u5f52\u56e0\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u67b6\u6784\u652f\u6301\u8de8\u4e0d\u540c\u6570\u636e\u6e90\u548c\u5730\u7406\u73af\u5883\u7684\u6cdb\u5316\uff0c\u907f\u514d\u4e86\u5bf9\u96be\u4ee5\u83b7\u53d6\u6570\u636e\u5c42\u7684\u4f9d\u8d56\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002\u5b83\u80fd\u5728\u5927\u89c4\u6a21\u3001\u5608\u6742\u7684\u79fb\u52a8\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u51c6\u786e\u3001\u9ad8\u6548\u7684\u5f52\u56e0\u3002"}}
{"id": "2507.09375", "pdf": "https://arxiv.org/pdf/2507.09375", "abs": "https://arxiv.org/abs/2507.09375", "authors": ["Sourish Suri", "Yifei Shao"], "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "categories": ["cs.CV", "I.2.6; I.5.4"], "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eCNN\u7684\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53f6\u7247\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u516b\u79cd\u5e38\u89c1\u4f5c\u7269\u75c5\u5bb3\uff0c\u5e76\u63d0\u4f9b\u6cbb\u7597\u5efa\u8bae\uff0c\u90e8\u7f72\u4e8e\u79fb\u52a8\u5e73\u53f0\uff0c\u65e8\u5728\u63d0\u9ad8\u519c\u4e1a\u751f\u4ea7\u529b\u548c\u7cae\u98df\u5b89\u5168\u3002", "motivation": "\u4f5c\u7269\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u519c\u4e1a\u751f\u4ea7\u529b\u548c\u5168\u7403\u7cae\u98df\u5b89\u5168\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u519c\u4e1a\u4e2d\uff0c\u65e9\u671f\u8bc6\u522b\u5e38\u5ef6\u8fdf\u6216\u4e0d\u51c6\u786e\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u5305\u62ec\u4ece\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u56fe\u50cf\u3001\u901a\u8fc7\u8c03\u6574\u5927\u5c0f\u3001\u5f52\u4e00\u5316\u548c\u589e\u5f3a\u8fdb\u884c\u9884\u5904\u7406\uff0c\u5e76\u4f7f\u7528TensorFlow\u548cKeras\u7684Sequential API\u8bad\u7ec3CNN\u6a21\u578b\u3002CNN\u67b6\u6784\u5305\u542b\u4e09\u5c42\u5377\u79ef\u5c42\u3001\u6700\u5927\u6c60\u5316\u5c42\u3001\u5c55\u5e73\u5c42\u548c\u5168\u8fde\u63a5\u5c42\uff0c\u6700\u7ec8\u901a\u8fc7Softmax\u8f93\u51fa\u8fdb\u884c\u591a\u5206\u7c7b\u3002", "result": "\u7cfb\u7edf\u8bad\u7ec3\u51c6\u786e\u7387\u9ad8\u8fbe\u7ea690%\uff0c\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8868\u73b0\u53ef\u9760\uff08\u9a8c\u8bc1\u51c6\u786e\u7387\u7ea660%\uff0c\u5b58\u5728\u8f7b\u5fae\u8fc7\u62df\u5408\uff09\u3002\u6a21\u578b\u96c6\u6210\u4e86\u6cbb\u7597\u63a8\u8350\u6a21\u5757\uff0c\u4e3a\u68c0\u6d4b\u5230\u7684\u75c5\u5bb3\u63d0\u4f9b\u53ef\u884c\u7684\u5e72\u9884\u5efa\u8bae\u3002\u8be5\u89e3\u51b3\u65b9\u6848\u5df2\u90e8\u7f72\u5728\u5f00\u6e90\u3001\u79fb\u52a8\u517c\u5bb9\u5e73\u53f0\u4e0a\uff0c\u652f\u6301\u519c\u6c11\u8fdb\u884c\u5b9e\u65f6\u56fe\u50cf\u8bca\u65ad\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7cbe\u51c6\u519c\u4e1a\u8d21\u732e\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6613\u4e8e\u8bbf\u95ee\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u5bf9\u4eba\u5de5\u68c0\u67e5\u7684\u4f9d\u8d56\uff0c\u4fc3\u8fdb\u53ef\u6301\u7eed\u75c5\u5bb3\u7ba1\u7406\uff0c\u5e76\u5f3a\u8c03\u4e86CNN\u5728\u4f5c\u7269\u5065\u5eb7\u76d1\u6d4b\u548c\u63d0\u5347\u5168\u7403\u7cae\u98df\u751f\u4ea7\u5f39\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.10342", "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u56db\u9879\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u5bf9\u6bd4\u4e86LLM\u4e0e\u4eba\u7c7b\u5728\u60c5\u611f\u610f\u4e49\u5224\u65ad\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e24\u8005\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660eLLMs\u53ef\u4f5c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u53ef\u9760\u534f\u4f5c\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u590d\u5236\u7ec6\u81f4\u5165\u5fae\u7684\u4eba\u7c7b\u5224\u65ad\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u68c0\u9a8c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u8fd0\u52a8\u65b9\u5f0f\u52a8\u8bcd\u7684\u65f6\u95f4\u8868\u8fbe\u4e2d\u60c5\u611f\u610f\u4e49\u7684\u4ea7\u751f\u65b9\u9762\uff0c\u80fd\u5426\u4f5c\u4e3a\u53ef\u9760\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u8fdb\u884c\u4e86\u56db\u9879\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\uff08\u5173\u4e8e\u6d8c\u73b0\u610f\u4e49\u3001\u6548\u4ef7\u8f6c\u53d8\u3001\u60c5\u611f\u8bed\u5883\u4e2d\u7684\u52a8\u8bcd\u9009\u62e9\u4ee5\u53ca\u53e5\u5b50-\u8868\u60c5\u7b26\u53f7\u5173\u8054\uff09\uff0c\u9996\u5148\u7531\u4eba\u7c7b\u53c2\u4e0e\u8005\u5b8c\u6210\uff0c\u7136\u540e\u4f7f\u7528LLM\uff08\u5982GPT-4\uff09\u590d\u5236\u76f8\u540c\u7684\u4efb\u52a1\u3002\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\uff08\u4f8b\u5982\uff0cSpearman\u76f8\u5173\u7cfb\u6570\uff09\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u7684\u54cd\u5e94\u3002", "result": "\u6240\u6709\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4eba\u7c7b\u548cAI\u7684\u54cd\u5e94\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u4e00\u81f4\u6027\u3002\u7edf\u8ba1\u5206\u6790\uff08\u4f8b\u5982\uff0cSpearman\u76f8\u5173\u7cfb\u6570 = 0.73-0.96\uff09\u8868\u660e\uff0c\u5728\u8bc4\u5206\u6a21\u5f0f\u548c\u7c7b\u522b\u9009\u62e9\u4e0a\u5747\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u89c2\u5bdf\u5230\u7ec6\u5fae\u5dee\u5f02\uff0c\u4f46\u8fd9\u4e9b\u5dee\u5f02\u5e76\u672a\u6539\u53d8\u6574\u4f53\u89e3\u91ca\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u8bc1\u636e\uff0c\u8868\u660eLLMs\u53ef\u4ee5\u589e\u5f3a\u4f20\u7edf\u7684\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5bb3\u89e3\u91ca\u6709\u6548\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u7814\u7a76\u3002\u8fd9\u79cd\u4e00\u81f4\u6027\u4e0d\u4ec5\u52a0\u5f3a\u4e86\u5148\u524d\u57fa\u4e8e\u4eba\u7c7b\u53d1\u73b0\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u8fd8\u4e3a\u901a\u8fc7AI\u751f\u6210\u5047\u8bbe\u548c\u6269\u5c55\u6570\u636e\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\u3002\u6700\u7ec8\uff0c\u672c\u7814\u7a76\u652f\u6301\u5c06LLMs\u4f5c\u4e3a\u8bed\u8a00\u5b66\u63a2\u7a76\u4e2d\u53ef\u4fe1\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u534f\u4f5c\u8005\u3002"}}
{"id": "2507.09173", "pdf": "https://arxiv.org/pdf/2507.09173", "abs": "https://arxiv.org/abs/2507.09173", "authors": ["Mengjie Chen", "Ming Zhang", "Cunquan Qu"], "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "comment": null, "summary": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,\noften leading to adverse drug reactions with significant implications for\npatient safety and healthcare outcomes. While graph-based methods have achieved\nstrong predictive performance, most approaches treat drug pairs independently,\noverlooking the complex, context-dependent interactions unique to drug pairs.\nAdditionally, these models struggle to integrate biological interaction\nnetworks and molecular-level structures to provide meaningful mechanistic\ninsights. In this study, we propose MolecBioNet, a novel graph-based framework\nthat integrates molecular and biomedical knowledge for robust and interpretable\nDDI prediction. By modeling drug pairs as unified entities, MolecBioNet\ncaptures both macro-level biological interactions and micro-level molecular\ninfluences, offering a comprehensive perspective on DDIs. The framework\nextracts local subgraphs from biomedical knowledge graphs and constructs\nhierarchical interaction graphs from molecular representations, leveraging\nclassical graph neural network methods to learn multi-scale representations of\ndrug pairs. To enhance accuracy and interpretability, MolecBioNet introduces\ntwo domain-specific pooling strategies: context-aware subgraph pooling\n(CASPool), which emphasizes biologically relevant entities, and\nattention-guided influence pooling (AGIPool), which prioritizes influential\nmolecular substructures. The framework further employs mutual information\nminimization regularization to enhance information diversity during embedding\nfusion. Experimental results demonstrate that MolecBioNet outperforms\nstate-of-the-art methods in DDI prediction, while ablation studies and\nembedding visualizations further validate the advantages of unified drug pair\nmodeling and multi-scale knowledge integration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faMolecBioNet\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u56fe\u57fa\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5206\u5b50\u548c\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u5bf9\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u8fdb\u884c\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u673a\u5236\u6d1e\u5bdf\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDIs\uff09\u662f\u836f\u7406\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e38\u5bfc\u81f4\u4e0d\u826f\u836f\u7269\u53cd\u5e94\uff0c\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u5b89\u5168\u3002\u73b0\u6709\u56fe\u57fa\u65b9\u6cd5\u591a\u72ec\u7acb\u5904\u7406\u836f\u7269\u5bf9\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u4e92\u52a8\uff0c\u4e14\u96be\u4ee5\u6574\u5408\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u7f51\u7edc\u548c\u5206\u5b50\u7ed3\u6784\u4ee5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u673a\u5236\u6d1e\u5bdf\u3002", "method": "MolecBioNet\u6846\u67b6\u5c06\u836f\u7269\u5bf9\u5efa\u6a21\u4e3a\u7edf\u4e00\u5b9e\u4f53\uff0c\u6574\u5408\u5b8f\u89c2\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u548c\u5fae\u89c2\u5206\u5b50\u5f71\u54cd\u3002\u5b83\u4ece\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u4e2d\u63d0\u53d6\u5c40\u90e8\u5b50\u56fe\uff0c\u5e76\u4ece\u5206\u5b50\u8868\u793a\u4e2d\u6784\u5efa\u5206\u5c42\u4ea4\u4e92\u56fe\uff0c\u5229\u7528\u7ecf\u5178\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\u3002\u4e3a\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5f15\u5165\u4e86\u4e24\u4e2a\u9886\u57df\u7279\u5b9a\u6c60\u5316\u7b56\u7565\uff1a\u4e0a\u4e0b\u6587\u611f\u77e5\u5b50\u56fe\u6c60\u5316\uff08CASPool\uff09\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u5f71\u54cd\u6c60\u5316\uff08AGIPool\uff09\uff0c\u5e76\u91c7\u7528\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u6b63\u5219\u5316\u589e\u5f3a\u5d4c\u5165\u878d\u5408\u7684\u4fe1\u606f\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMolecBioNet\u5728DDI\u9884\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u548c\u5d4c\u5165\u53ef\u89c6\u5316\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7edf\u4e00\u836f\u7269\u5bf9\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u77e5\u8bc6\u6574\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "MolecBioNet\u901a\u8fc7\u7edf\u4e00\u836f\u7269\u5bf9\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u77e5\u8bc6\u6574\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684DDI\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3DDIs\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.09410", "pdf": "https://arxiv.org/pdf/2507.09410", "abs": "https://arxiv.org/abs/2507.09410", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "categories": ["cs.CV", "cs.LG"], "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4f4e\u8d44\u6e90\u3001\u672c\u5730\u90e8\u7f72\u7684ML/AI\u6d41\u7a0b\uff0c\u65e8\u5728\u5e2e\u52a9\u8d44\u6e90\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u5c0f\u578b\u7814\u7a76\u56e2\u961f\u6709\u6548\u5904\u7406\u548c\u5206\u6790\u76f8\u673a\u9677\u9631\u6570\u636e\u3002", "motivation": "\u5c3d\u7ba1\u76f8\u673a\u9677\u9631\u6570\u636e\u6536\u96c6\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u4f46\u6570\u636e\u7684\u5904\u7406\u548c\u7ba1\u7406\uff08\u5c24\u5176\u662f\u6574\u5408ML/AI\u5de5\u5177\uff09\u4ecd\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u91cf\u5e9e\u5927\u3001\u6807\u6ce8\u56f0\u96be\u3001\u73af\u5883\u5bf9\u6570\u636e\u8d28\u91cf\u7684\u5f71\u54cd\u4ee5\u53caML/AI\u5de5\u5177\u96c6\u6210\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u5c0f\u578b\u7814\u7a76\u56e2\u961f\u5c24\u4e3a\u53d7\u9650\u4e8e\u8d44\u6e90\u548c\u8ba1\u7b97\u4e13\u957f\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4e\u8d44\u6e90\u3001\u672c\u5730\u90e8\u7f72\u7684\u76f8\u673a\u9677\u9631\u6570\u636e\u5904\u7406\u6d41\u7a0b\u6307\u5357\uff0c\u8be5\u6d41\u7a0b\u6574\u5408\u4e86ML/AI\u80fd\u529b\u3002\u5b83\u4e13\u6ce8\u4e8e\u4e3a\u8d44\u6e90\u548c\u8ba1\u7b97\u4e13\u4e1a\u77e5\u8bc6\u6709\u9650\u7684\u5c0f\u578b\u7814\u7a76\u56e2\u961f\u63d0\u4f9b\u5b9e\u7528\u4e14\u6613\u4e8e\u8bbf\u95ee\u7684\u6570\u636e\u4f20\u8f93\u3001\u63a8\u7406\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8be5\u6d41\u7a0b\u4f7f\u7814\u7a76\u4eba\u5458\uff0c\u7279\u522b\u662f\u5c0f\u578b\u7814\u7a76\u56e2\u961f\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u4e0d\u65ad\u589e\u957f\u7684\u76f8\u673a\u9677\u9631\u6570\u636e\u96c6\uff0c\u5e76\u4ece\u4e2d\u53d1\u73b0\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u4f4e\u8d44\u6e90ML/AI\u89e3\u51b3\u65b9\u6848\uff0c\u8d4b\u80fd\u8d44\u6e90\u53d7\u9650\u7684\u5c0f\u578b\u7814\u7a76\u56e2\u961f\u9ad8\u6548\u5904\u7406\u548c\u5206\u6790\u76f8\u673a\u9677\u9631\u6570\u636e\uff0c\u4ece\u800c\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u83b7\u53d6\u5b9d\u8d35\u7684\u79d1\u7814\u53d1\u73b0\u3002"}}
{"id": "2507.10354", "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "categories": ["cs.CL"], "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u201c\u6d0b\u8471\u201d\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u9690\u55bb\u610f\u4e49\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u5185\u5bb9\u5206\u6790\u3001\u6982\u5ff5\u878d\u5408\u548c\u8bed\u7528\u610f\u56fe\u4e09\u4e2a\u5c42\u9762\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u4e30\u5bcc\u3001\u8ba4\u77e5\u4e0a\u66f4\u5408\u7406\u7684\u8ba1\u7b97\u7cfb\u7edf\u9690\u55bb\u89e3\u91ca\u3002", "motivation": "\u9690\u55bb\u610f\u4e49\u5e76\u975e\u6982\u5ff5\u95f4\u7684\u7b80\u5355\u6620\u5c04\uff0c\u800c\u662f\u4e00\u79cd\u6574\u5408\u591a\u5c42\u89e3\u91ca\u7684\u590d\u6742\u8ba4\u77e5\u73b0\u8c61\u3002\u73b0\u6709\u7684\u7406\u89e3\u65b9\u5f0f\u672a\u80fd\u5145\u5206\u6355\u6349\u5176\u590d\u6742\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5177\u8ba4\u77e5\u57fa\u7840\u7684\u65b9\u6cd5\u6765\u5904\u7406\u9690\u55bb\u89e3\u91ca\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u9690\u55bb\u5904\u7406\u6a21\u578b\uff0c\u5c06\u610f\u4e49\u89c6\u4e3a\u201c\u6d0b\u8471\u201d\u72b6\u7684\u591a\u5c42\u7ed3\u6784\uff0c\u5305\u62ec\u4e09\u4e2a\u7ef4\u5ea6\uff1a1) \u5185\u5bb9\u5206\u6790\uff08\u901a\u8fc7\u57fa\u672c\u6982\u5ff5\u5143\u7d20\u6807\u6ce8\u9690\u55bb\uff09\uff0c2) \u6982\u5ff5\u878d\u5408\uff08\u5efa\u6a21\u6982\u5ff5\u7ec4\u5408\u53ca\u5176\u6d8c\u73b0\u610f\u4e49\uff09\uff0c3) \u8bed\u7528\u610f\u56fe\uff08\u5f15\u5165\u8bed\u7528\u8bcd\u6c47\u6355\u6349\u8bf4\u8bdd\u8005\u610f\u56fe\u3001\u4ea4\u9645\u529f\u80fd\u548c\u8bed\u5883\u6548\u5e94\uff09\u3002\u8fd9\u4e9b\u5c42\u6b21\u88ab\u7edf\u4e00\u5728\u4e00\u4e2a\u5f62\u5f0f\u6846\u67b6\u4e2d\u3002", "result": "\u8be5\u4e09\u7ef4\u6846\u67b6\u4e3a\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u9690\u55bb\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4e30\u5bcc\u3001\u8ba4\u77e5\u4e0a\u66f4\u5408\u7406\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7edf\u4e00\u8fd9\u4e9b\u5c42\u6b21\uff0c\u8be5\u6a21\u578b\u4e3a\u8ba1\u7b97\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u5176\u80fd\u591f\u8d85\u8d8a\u8868\u9762\u8054\u60f3\uff0c\u8868\u793a\u66f4\u6df1\u5c42\u3001\u66f4\u5177\u8bed\u5883\u654f\u611f\u6027\u7684\u9690\u55bb\u610f\u4e49\u63a8\u7406\u3002", "conclusion": "\u8be5\u7edf\u4e00\u7684\u5206\u5c42\u6a21\u578b\u4e3a\u8ba1\u7b97\u7cfb\u7edf\u7406\u89e3\u9690\u55bb\u610f\u4e49\u63d0\u4f9b\u4e86\u6df1\u5165\u4e14\u5177\u8bed\u5883\u654f\u611f\u6027\u7684\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u8868\u9762\u5173\u8054\uff0c\u4e3a\u672a\u6765\u9690\u55bb\u5904\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09177", "pdf": "https://arxiv.org/pdf/2507.09177", "abs": "https://arxiv.org/abs/2507.09177", "authors": ["Zichen Liu", "Guoji Fu", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Continual Reinforcement Learning by Planning with Online World Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025 Spotlight", "summary": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u201c\u8ddf\u968f\u9886\u5bfc\u8005\u201d(FTL)\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60(CRL)\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aFTL\u5728\u7ebf\u667a\u80fd\u4f53(OA)\u7684\u65b0\u578b\u667a\u80fd\u4f53\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u800c\u4e0d\u9057\u5fd8\u65e7\u6280\u80fd\u3002", "motivation": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60(CRL)\u4e2d\uff0c\u667a\u80fd\u4f53\u5728\u987a\u5e8f\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1\u65f6\uff0c\u9762\u4e34\u9057\u5fd8\u4e4b\u524d\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u7684\u6311\u6218\uff0c\u5373\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd9\u662fCRL\u53d1\u5c55\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u6765\u89e3\u51b3\u95ee\u9898\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5b66\u4e60\u4e00\u4e2a\u201c\u8ddf\u968f\u9886\u5bfc\u8005\u201d\u6d45\u5c42\u5728\u7ebf\u6a21\u578b\u4ee5\u6355\u6349\u4e16\u754c\u52a8\u6001\u30022) \u5229\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7ed3\u5408\u8be5\u5728\u7ebf\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u30023) \u8be5\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u6784\u5efa\u65b9\u5f0f\u5bf9\u9057\u5fd8\u5177\u6709\u514d\u75ab\u6027\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4e0a\u7684\u9057\u61be\u754c\u9650\u30024) \u89c4\u5212\u5668\u4ec5\u57fa\u4e8e\u6700\u65b0\u7684\u5728\u7ebf\u6a21\u578b\u641c\u7d22\u52a8\u4f5c\uff0c\u5f62\u6210\u4e86FTL\u5728\u7ebf\u667a\u80fd\u4f53(OA)\u30025) \u4e3a\u8bc4\u4f30OA\uff0c\u4f5c\u8005\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u7528\u7684CRL\u73af\u5883\u2014\u2014Continual Bench\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684OA\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u540c\u65f6\u4e0d\u9057\u5fd8\u65e7\u6280\u80fd\u3002\u5b83\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u4e16\u754c\u6a21\u578b\u5e76\u7ed3\u5408\u5404\u79cd\u6301\u7eed\u5b66\u4e60\u6280\u672f\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u901a\u8fc7\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u548c\u89c4\u5212\u7b56\u7565\u89e3\u51b3\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684FTL\u5728\u7ebf\u667a\u80fd\u4f53(OA)\u5728\u5b66\u4e60\u6548\u7387\u548c\u9057\u5fd8\u6291\u5236\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.09420", "pdf": "https://arxiv.org/pdf/2507.09420", "abs": "https://arxiv.org/abs/2507.09420", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u5728\u8f68\uff08in-situ\uff09\u5929\u4f53\u5730\u8c8c\u7279\u5f81\u8ffd\u8e2a\u7684\u65b0\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u68c0\u6d4b\u548c\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5bf9\u9f50\u516c\u5f0f\u8fdb\u884c\u7279\u5f81\u63cf\u8ff0\uff0c\u65e8\u5728\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u81ea\u52a8\u822a\u5929\u5e94\u7528\uff08\u5982TRN\u3001EDL\u3001\u5371\u9669\u5206\u6790\u548c\u79d1\u5b66\u6570\u636e\u6536\u96c6\uff09\u8feb\u5207\u9700\u8981\u5929\u4f53\u5730\u8c8c\u7279\u5f81\u7684\u68c0\u6d4b\u4e0e\u8ffd\u8e2a\u3002\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u80fd\u529b\u3001\u6210\u672c\u9ad8\u3001\u5904\u7406\u6162\u4e14\u6cdb\u5316\u6027\u5dee\u3002\u65b0\u5174\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u8d85\u51fa\u73b0\u6709\u822a\u5929\u5668\u786c\u4ef6\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u4e14\u7f3a\u4e4f\u591a\u6837\u7684\u5730\u5916\u73af\u5883\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u68c0\u6d4b\u548c\u63cf\u8ff0\u5b9e\u73b0\u5730\u6807\u8ffd\u8e2a\u7684\u65b0\u9896\u65b9\u6848\u3002\u91c7\u7528\u8f7b\u91cf\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4e13\u4e3a\u73b0\u6709\u822a\u5929\u5668\u98de\u884c\u5904\u7406\u5668\u4e0a\u7684\u5b9e\u65f6\u6267\u884c\u8bbe\u8ba1\u3002\u5730\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u5229\u7528\u72ec\u7acb\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u8bad\u7ec3\u6570\u636e\u8bc6\u522b\u5929\u4f53\u5730\u5f62\u7279\u5f81\u3002\u5730\u6807\u63cf\u8ff0\u65b9\u9762\uff0c\u5f15\u5165\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5bf9\u9f50\u516c\u5f0f\uff0c\u5b66\u4e60\u9c81\u68d2\u7684\u7279\u5f81\u8868\u793a\uff0c\u5373\u4f7f\u5730\u6807\u89c6\u89d2\u53d1\u751f\u663e\u8457\u53d8\u5316\u4e5f\u80fd\u4fdd\u6301\u5bf9\u5e94\u5173\u7cfb\u3002\u8fd9\u4e9b\u8d21\u732e\u5171\u540c\u6784\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u5730\u6807\u8ffd\u8e2a\u7cfb\u7edf\u3002", "result": "\u8be5\u7edf\u4e00\u7684\u5730\u6807\u8ffd\u8e2a\u7cfb\u7edf\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u76f8\u6bd4\uff0c\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u5728\u8f68\u5730\u6807\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u3001\u6539\u8fdb\u7684\u57df\u9002\u5e94\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5929\u4f53\u7279\u5f81\u8bc6\u522b\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u4e3b\u822a\u5929\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6301\u3002"}}
{"id": "2507.10435", "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u5904\u7406\u56fe\u63a8\u7406\u4efb\u52a1\uff0c\u5373\u4f7f\u56fe\u7ed3\u6784\u5d4c\u5165\u5728\u6587\u672c\u4e2d\u3002\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5982\u4f55\u7406\u89e3\u6f5c\u5728\u56fe\u7ed3\u6784\uff0c\u63d0\u51fa\u201c\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4\uff08ISF\uff09\u201d\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u5176\u80fd\u4ece\u5404\u79cd\u56fe\u7c7b\u578b\uff08\u5305\u62ec\u5206\u5b50\u56fe\uff09\u4e2d\u6210\u529f\u63d0\u53d6\u5b50\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLMs\u80fd\u89e3\u51b3\u56fe\u63a8\u7406\u4efb\u52a1\uff0c\u5373\u4f7f\u56fe\u7ed3\u6784\u4ee5\u6587\u672c\u5f62\u5f0f\u7ed9\u51fa\u3002\u8fd9\u5f15\u51fa\u4e86\u4e00\u4e2a\u6839\u672c\u6027\u95ee\u9898\uff1a\u4ec5\u89e3\u7801\u5668Transformer\u67b6\u6784\u5982\u4f55\u7406\u89e3\u5e76\u4ece\u6587\u672c\u5e8f\u5217\u4e2d\u63d0\u53d6\u5176\u5e95\u5c42\u7684\u56fe\u7ed3\u6784\uff1f", "method": "\u7814\u7a76\u4ece\u5b50\u7ed3\u6784\u63d0\u53d6\u4efb\u52a1\u5165\u624b\uff0c\u901a\u8fc7\u89e3\u91caTransformer\u7684\u5185\u90e8\u673a\u5236\u548c\u5206\u6790\u8f93\u5165\u67e5\u8be2\u7684\u5f71\u54cd\u6765\u89e3\u51b3\u95ee\u9898\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7ed3\u5408\u7ecf\u9a8c\u7ed3\u679c\u548c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u201c\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4\uff08ISF\uff09\u201d\u8fd9\u4e00\u89c6\u89d2\uff0c\u7528\u4e8e\u6355\u6349\u591a\u5c42Transformer\u4e2d\u7684\u5b50\u7ed3\u6784\u8bc6\u522b\u8fc7\u7a0b\u3002\u7814\u7a76\u8fdb\u4e00\u6b65\u5728LLMs\u4e2d\u9a8c\u8bc1\u4e86ISF\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u201c\u5b50\u7ed3\u6784\u601d\u7ef4\u201d\u6982\u5ff5\u4ee5\u9ad8\u6548\u63d0\u53d6\u590d\u6742\u590d\u5408\u6a21\u5f0f\uff0c\u6210\u529f\u5730\u4ece\u5305\u62ec\u5206\u5b50\u56fe\u5728\u5185\u7684\u5c5e\u6027\u56fe\u4e2d\u63d0\u53d6\u5b50\u7ed3\u6784\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4\uff08ISF\uff09\u201d\u673a\u5236\uff0c\u80fd\u89e3\u91ca\u591a\u5c42Transformer\u4e2d\u5b50\u7ed3\u6784\u7684\u8bc6\u522b\u8fc7\u7a0b\u3002\u5b9e\u8bc1\u7ed3\u679c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cISF\u5728LLMs\u7684\u5404\u5c42\u4e2d\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u90e8\u52a8\u6001\u3002\u7814\u7a76\u8fd8\u8bc1\u660e\u4e86\u4ec5\u89e3\u7801\u5668Transformer\u80fd\u591f\u901a\u8fc7\u201c\u5b50\u7ed3\u6784\u601d\u7ef4\u201d\u6210\u529f\u5730\u4ece\u5404\u79cd\u5c5e\u6027\u56fe\uff08\u5982\u5206\u5b50\u56fe\uff09\u4e2d\u63d0\u53d6\u590d\u6742\u7684\u5b50\u7ed3\u6784\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u53d1\u73b0\u4e3a\u57fa\u4e8e\u5e8f\u5217\u7684Transformer\u6a21\u578b\u5982\u4f55\u6267\u884c\u56fe\u6570\u636e\u7684\u5b50\u7ed3\u6784\u63d0\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.09202", "pdf": "https://arxiv.org/pdf/2507.09202", "abs": "https://arxiv.org/abs/2507.09202", "authors": ["Wuxin Wang", "Weicheng Ni", "Lilan Huang", "Tao Hao", "Ben Fei", "Shuo Ma", "Taikang Yuan", "Yanlai Zhao", "Kefeng Deng", "Xiaoyong Li", "Boheng Duan", "Lei Bai", "Kaijun Ren"], "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.", "AI": {"tldr": "XiChen\u662f\u4e00\u4e2a\u9996\u521b\u7684\u3001\u53ef\u89c2\u6d4b\u6269\u5c55\u7684\u7eafAI\u5168\u7403\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u6446\u8131\u4e86\u5bf9\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u7684\u4f9d\u8d56\uff0c\u80fd\u572817\u79d2\u5185\u5b8c\u6210\u4ece\u6570\u636e\u540c\u5316\u5230\u4e2d\u671f\u9884\u62a5\u7684\u5168\u8fc7\u7a0b\uff0c\u5e76\u8fbe\u5230\u4e0eNWP\u7cfb\u7edf\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8ba1\u7b97\u8017\u65f6\uff08\u6570\u5c0f\u65f6\uff09\u7684NWP\u7cfb\u7edf\u6765\u51c6\u5907\u521d\u59cb\u6761\u4ef6\uff0c\u9650\u5236\u4e86\u9884\u62a5\u6548\u7387\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u4e2a\u5feb\u901f\u3001\u5b8c\u5168\u7531AI\u9a71\u52a8\u4e14\u4e0d\u4f9d\u8d56NWP\u7684\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u4e86XiChen\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u57fa\u4e8e\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u5929\u6c14\u9884\u62a5\u57fa\u7840\u6a21\u578b\u3002\u8be5\u6a21\u578b\u88ab\u8fdb\u4e00\u6b65\u5fae\u8c03\u4ee5\u540c\u65f6\u5145\u5f53\u89c2\u6d4b\u7b97\u5b50\u548c\u6570\u636e\u540c\u5316\u6a21\u578b\uff0c\u4ece\u800c\u53ef\u6269\u5c55\u5730\u540c\u5316\u5e38\u89c4\u548c\u539f\u59cb\u536b\u661f\u89c2\u6d4b\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u878d\u5408\u4e86\u56db\u7ef4\u53d8\u5206\u77e5\u8bc6\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u540c\u5316\u548c\u4e2d\u671f\u9884\u62a5\u7684\u51c6\u786e\u6027\u3002", "result": "XiChen\u7684\u6574\u4e2a\u9884\u62a5\u6d41\u7a0b\uff08\u4ece\u6570\u636e\u540c\u5316\u5230\u4e2d\u671f\u9884\u62a5\uff09\u4ec5\u970017\u79d2\u5b8c\u6210\u3002\u5176\u6570\u636e\u540c\u5316\u548c\u4e2d\u671f\u9884\u62a5\u7cbe\u5ea6\u53ef\u4e0e\u73b0\u6709\u4e1a\u52a1\u5316NWP\u7cfb\u7edf\u5ab2\u7f8e\uff0c\u5e76\u80fd\u5b9e\u73b0\u8d85\u8fc78.25\u5929\u7684\u6709\u6548\u9884\u62a5\u65f6\u6548\u3002", "conclusion": "XiChen\u7684\u6210\u529f\u8868\u660e\u4e86\u5176\u5728\u5b9e\u73b0\u5b8c\u5168AI\u9a71\u52a8\u3001\u72ec\u7acb\u4e8eNWP\u7cfb\u7edf\u7684\u5929\u6c14\u9884\u62a5\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.09446", "pdf": "https://arxiv.org/pdf/2507.09446", "abs": "https://arxiv.org/abs/2507.09446", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u4e09\u7ef4\u591a\u4eba\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7b80\u5316\u65f6\u7a7a\u4ea4\u4e92\uff0c\u5728\u4fdd\u6301SOTA\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e09\u7ef4\u591a\u4eba\u8fd0\u52a8\u9884\u6d4b\u4efb\u52a1\u590d\u6742\uff0c\u539f\u56e0\u5728\u4e8e\u4e2a\u4f53\u5386\u53f2\u8fd0\u52a8\u548c\u4e2a\u4f53\u95f4\u4ea4\u4e92\u7684\u4f9d\u8d56\u6027\uff1b\u6709\u6548\u5efa\u6a21\u8fd9\u4e9b\u4ea4\u4e92\u901a\u5e38\u4f1a\u4ea7\u751f\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7\u7b80\u5316\u65f6\u7a7a\u4ea4\u4e92\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u53cc\u5206\u652f\u4ee5\u5206\u522b\u5b66\u4e60\u4e2a\u4f53\u548c\u591a\u4eba\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u8868\u793a\uff1b\u5f15\u5165\u65b0\u578b\u8de8\u7ea7\u522b\u4ea4\u4e92\u6a21\u5757\u6765\u6574\u5408\u4e24\u4e2a\u5206\u652f\u7684\u65f6\u7a7a\u8868\u793a\uff1b\u663e\u5f0f\u878d\u5165\u7a7a\u95f4\u4e2a\u4f53\u95f4\u8ddd\u79bb\u5d4c\u5165\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u4ea4\u4e92\u5efa\u6a21\u3002", "result": "\u5728CMU-Mocap\u3001MuPoTS-3D\u548c3DPW\u7b49\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8ba1\u7b97\u9ad8\u6548\u7684\u65f6\u7a7a\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e09\u7ef4\u591a\u4eba\u8fd0\u52a8\u9884\u6d4b\u4e2d\u4ea4\u4e92\u5efa\u6a21\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.10445", "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5f02\u6b65\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u80fd\u529b\u3002\u901a\u8fc7\u6784\u5efa\u65b0\u8bed\u6599\u5e93\u5e76\u5bf9\u6bd4LLM\u4e0e\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u5904\u7406\u6b67\u4e49\u65f6\u7684\u6a21\u5f0f\u4e0d\u540c\uff0c\u4e14LLM\u7684\u63a8\u7406\u80fd\u529b\u80fd\u63d0\u5347\u5176\u63d0\u95ee\u7684\u9891\u7387\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5f02\u6b65\u3001\u6307\u4ee4\u5bfc\u5411\u578b\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u7a76\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u5728\u5904\u7406\u6b67\u4e49\u65f6\u7684\u5f02\u540c\u3002\u6b64\u5916\uff0c\u63a2\u8ba8LLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u5426\u662f\u5176\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u80fd\u529b\u7684\u57fa\u7840\u3002", "method": "1. \u6784\u5efa\u4e00\u4e2a\u65b0\u8bed\u6599\u5e93\uff0c\u8be5\u8bed\u6599\u5e93\u6574\u5408\u4e86Minecraft\u5bf9\u8bdd\u8bed\u6599\u5e93\u4e2d\u5173\u4e8e\u6307\u79f0\u3001\u6307\u79f0\u6b67\u4e49\u548cSDRT\uff08\u542b\u6f84\u6e05\uff09\u7684\u73b0\u6709\u6807\u6ce8\uff0c\u4ee5\u652f\u6301\u5bf9\u6f84\u6e05\u4e0e\u6b67\u4e49\u5173\u7cfb\u7684\u7814\u7a76\u30022. \u5229\u7528\u8be5\u8bed\u6599\u5e93\uff0c\u5bf9\u6bd4LLM\u751f\u6210\u7684\u95ee\u9898\u4e0e\u4eba\u7c7b\u63d0\u51fa\u7684\u6f84\u6e05\u95ee\u9898\uff0c\u5206\u6790\u4e8c\u8005\u5728\u6b67\u4e49\u60c5\u5883\u4e0b\u7684\u884c\u4e3a\u6a21\u5f0f\u30023. \u901a\u8fc7\u6d4b\u8bd5\u4e0d\u540c\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u5bf9LLM\u63d0\u95ee\u9891\u7387\u548c\u76f8\u5173\u6027\u7684\u5f71\u54cd\u3002", "result": "1. \u6b67\u4e49\u4e0e\u4eba\u7c7b\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u4e4b\u95f4\u4ec5\u5b58\u5728\u5fae\u5f31\u5173\u8054\uff0c\u4e14\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u5b58\u5728\u4f4e\u76f8\u5173\u6027\u30022. \u4eba\u7c7b\u5f88\u5c11\u5bf9\u6307\u79f0\u6b67\u4e49\u63d0\u51fa\u6f84\u6e05\uff0c\u4f46\u7ecf\u5e38\u5bf9\u57fa\u4e8e\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u51fa\u6f84\u6e05\u30023. LLM\u5219\u76f8\u53cd\uff0c\u5b83\u4eec\u66f4\u5e38\u5bf9\u6307\u79f0\u6b67\u4e49\u63d0\u51fa\u6f84\u6e05\uff0c\u4f46\u8f83\u5c11\u5bf9\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u63d0\u51fa\u30024. \u63a8\u7406\u80fd\u529b\u4f3c\u4e4e\u80fd\u589e\u52a0LLM\u63d0\u95ee\u7684\u9891\u7387\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u5728\u5f02\u6b65\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u80fd\u529b\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6b67\u4e49\u65f6\u3002\u7814\u7a76\u8868\u660e\uff0cLLM\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u5176\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u9891\u7387\u548c\u76f8\u5173\u6027\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2507.09211", "pdf": "https://arxiv.org/pdf/2507.09211", "abs": "https://arxiv.org/abs/2507.09211", "authors": ["Xinyue Liu", "Xiao Peng", "Shuyue Yan", "Yuntian Chen", "Dongxiao Zhang", "Zhixiao Niu", "Hui-Min Wang", "Xiaogang He"], "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an", "physics.geo-ph", "stat.ML"], "comment": null, "summary": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86DeepX-GAN\u6a21\u578b\uff0c\u4e00\u4e2a\u77e5\u8bc6\u5f15\u5bfc\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u8d85\u51fa\u5386\u53f2\u8bb0\u5f55\u7684\u201c\u672a\u89c1\u201d\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\uff08\u5305\u62ec\u76f4\u63a5\u547d\u4e2d\u548c\u9669\u4e9b\u9519\u8fc7\u7684\uff09\uff0c\u5e76\u6355\u6349\u5176\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u672a\u89c1\u6781\u7aef\u4e8b\u4ef6\u5bf9\u8106\u5f31\u5730\u533a\u5f71\u54cd\u66f4\u5927\uff0c\u5e76\u9884\u6d4b\u672a\u6765\u6c14\u5019\u53d8\u6696\u5c06\u6269\u5927\u5e76\u91cd\u65b0\u5206\u914d\u98ce\u9669\u70ed\u70b9\uff0c\u5f3a\u8c03\u9700\u5236\u5b9a\u7a7a\u95f4\u9002\u5e94\u6027\u653f\u7b56\u3002", "motivation": "\u73b0\u6709\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\u8bb0\u5f55\u65e0\u6cd5\u63d0\u4f9b\u5b8c\u6574\u7684\u98ce\u9669\u56fe\u666f\uff0c\u9057\u6f0f\u4e86\u8d85\u8d8a\u5386\u53f2\u8303\u56f4\u7684\u201c\u672a\u89c1\u201d\u6781\u7aef\u4e8b\u4ef6\uff0c\u4e14\u5ffd\u7565\u7a7a\u95f4\u4f9d\u8d56\u6027\u4f4e\u4f30\u4e86\u540c\u6b65\u707e\u5bb3\u7684\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u66f4\u597d\u5730\u6355\u6349\u7f55\u89c1\u6781\u7aef\u4e8b\u4ef6\u7a7a\u95f4\u7ed3\u6784\u3001\u6a21\u62df\u7edf\u8ba1\u4e0a\u5408\u7406\u4f46\u5386\u53f2\u672a\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\u7684\u6a21\u578b\uff0c\u4ee5\u63ed\u793a\u6f5c\u5728\u98ce\u9669\u5e76\u907f\u514d\u865a\u5047\u7684\u97e7\u6027\u611f\u77e5\u3002", "method": "\u5f00\u53d1\u4e86DeepX-GAN (Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial Network) \u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u77e5\u8bc6\u5f15\u5bfc\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u65e8\u5728\u66f4\u597d\u5730\u6355\u6349\u7f55\u89c1\u6781\u7aef\u4e8b\u4ef6\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6a21\u62df\u5386\u53f2\u7ecf\u9a8c\u4e4b\u5916\u4f46\u7edf\u8ba1\u4e0a\u5408\u7406\u7684\u201c\u672a\u89c1\u201d\u6781\u7aef\u4e8b\u4ef6\uff08\u5305\u62ec\u201c\u5c06\u519b\u201d\u548c\u201c\u50f5\u5c40\u201d\u4e24\u79cd\u7c7b\u578b\uff09\u3002\u6a21\u578b\u5e94\u7528\u4e8e\u4e2d\u4e1c\u548c\u5317\u975e(MENA)\u5730\u533a\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\uff0c\u8fd9\u4e9b\u201c\u672a\u89c1\u201d\u6781\u7aef\u4e8b\u4ef6\u4e0d\u6210\u6bd4\u4f8b\u5730\u5f71\u54cd\u7740\u9ad8\u8106\u5f31\u6027\u548c\u4f4e\u793e\u4f1a\u7ecf\u6d4e\u51c6\u5907\u5ea6\u7684\u5730\u533a\uff0c\u4e14\u4e0d\u540c\u7c7b\u578b\u7684\u4e8b\u4ef6\u5728\u7d27\u8feb\u6027\u548c\u89e3\u8bfb\u4e0a\u6709\u6240\u5dee\u5f02\u3002\u672a\u6765\u53d8\u6696\u53ef\u80fd\u6269\u5927\u5e76\u91cd\u65b0\u5206\u914d\u8fd9\u4e9b\u201c\u672a\u89c1\u201d\u6781\u7aef\u4e8b\u4ef6\uff0c\u5728\u5370\u5ea6-\u5df4\u57fa\u65af\u5766\u548c\u4e2d\u975e\u5730\u533a\u51fa\u73b0\u65b0\u7684\u66b4\u9732\u70ed\u70b9\u3002\u8fd9\u63ed\u793a\u4e86\u4f20\u7edf\u707e\u5bb3\u89c4\u5212\u4e2d\u7684\u5173\u952e\u76f2\u70b9\u3002", "conclusion": "\u4f20\u7edf\u7684\u707e\u5bb3\u89c4\u5212\u5b58\u5728\u5173\u952e\u76f2\u70b9\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u201c\u672a\u89c1\u201d\u7684\u6781\u7aef\u4e8b\u4ef6\u53ca\u5176\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u5236\u5b9a\u5177\u6709\u7a7a\u95f4\u9002\u5e94\u6027\u7684\u653f\u7b56\uff0c\u4ee5\u9884\u6d4b\u65b0\u5174\u7684\u98ce\u9669\u70ed\u70b9\uff0c\u800c\u975e\u4ec5\u4ec5\u6839\u636e\u5386\u53f2\u6a21\u5f0f\u8fdb\u884c\u63a8\u65ad\u3002"}}
{"id": "2507.09459", "pdf": "https://arxiv.org/pdf/2507.09459", "abs": "https://arxiv.org/abs/2507.09459", "authors": ["Zhihan Kang", "Boyu Wang"], "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "Undergraduate Theis; 12 pages, 6 figures", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "AI": {"tldr": "\u63d0\u51faSegVec3D\uff0c\u4e00\u4e2a\u65b0\u9896\u76843D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u6ce8\u610f\u529b\u3001\u5d4c\u5165\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5206\u5272\u53ca3D\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u96f6\u6837\u672c\u5bf9\u9f50\u3002", "motivation": "\u65e8\u5728\u4ee5\u6700\u5c11\u76d1\u7763\u7edf\u4e003D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u7406\u89e3\uff0c\u5e76\u63d0\u9ad8\u5b9e\u9645\u90e8\u7f72\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5982Mask3D\u548cULIP\u3002", "method": "\u6784\u5efa\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u5668\u4ee5\u589e\u5f3a\u51e0\u4f55\u7ed3\u6784\u5efa\u6a21\uff1b\u901a\u8fc7\u5bf9\u6bd4\u805a\u7c7b\u5b9e\u73b0\u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\uff1b\u5728\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bf9\u9f503D\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u652f\u6301\u96f6\u6837\u672c\u68c0\u7d22\uff1b\u6574\u5408\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3001\u5d4c\u5165\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u6210\u529f\u5730\u4ee5\u6700\u5c11\u76d1\u7763\u7edf\u4e00\u4e863D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u7406\u89e3\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u90e8\u7f72\u6027\uff0c\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u72ec\u7279\u80fd\u529b\u3002", "conclusion": "SegVec3D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u7406\u89e3\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u76d1\u7763\u548c\u5b9e\u9645\u5e94\u7528\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.10468", "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa\u00fal Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "categories": ["cs.CL", "cs.LG"], "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f8\u8f83\u4e8e\u65e9\u671f\u53cc\u5411\u7f16\u7801\u5668\uff0c\u5728\u5b9e\u9645\u5728\u7ebf\u6587\u672c\u4e2d\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u5728\u904f\u5236\u4ec7\u6068\u8a00\u8bba\u7684\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u5ba1\u67e5\u9762\u4e34\u6311\u6218\u3002\u5c3d\u7ba1\u65e9\u671f\u53cc\u5411Transformer\u7f16\u7801\u5668\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8d85\u5927\u578b\u81ea\u56de\u5f52LLMs\u7684\u51fa\u73b0\u6709\u671b\u5e26\u6765\u66f4\u6df1\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u89c4\u6a21\u7684\u63d0\u5347\u662f\u5426\u80fd\u5b9e\u9645\u6539\u5584\u771f\u5b9e\u6587\u672c\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u7cbe\u9009\u7684\u5728\u7ebf\u4ea4\u4e92\u8bed\u6599\u5e93\u4e0a\uff0c\u5bf9\u7ecf\u5178\u7f16\u7801\u5668\u6a21\u578b\u548c\u4e0b\u4e00\u4ee3LLMs\u4e24\u79cd\u6a21\u578b\u5bb6\u65cf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff08\u4ec7\u6068\u6216\u975e\u4ec7\u6068\uff09\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u7814\u7a76\u7ed3\u8bba\u3002"}}
{"id": "2507.09212", "pdf": "https://arxiv.org/pdf/2507.09212", "abs": "https://arxiv.org/abs/2507.09212", "authors": ["Jonas Scholz", "Richard E. Turner"], "title": "Warm Starts Accelerate Generative Modelling", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "AI": {"tldr": "\u63d0\u51fa\u201c\u70ed\u542f\u52a8\u201d\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u6761\u4ef6\u5316\u7684\u4f18\u5316\u8d77\u59cb\u566a\u58f0\u5206\u5e03\uff0c\u5927\u5e45\u52a0\u901f\u4e86\u6269\u6563\u6a21\u578b\u7b49\u8fed\u4ee3\u751f\u6210\u6a21\u578b\u7684\u6761\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u4ec5\u7528\u6781\u5c11\u8bc4\u4f30\u6b21\u6570\u4fbf\u8fbe\u5230\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u8fed\u4ee3\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u6d41\u5339\u914d\uff09\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u4f46\u5176\u751f\u6210\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u6570\u767e\u6b21\u51fd\u6570\u8bc4\u4f30\uff0c\u901f\u5ea6\u975e\u5e38\u7f13\u6162\u3002", "method": "\u5f15\u5165\u201c\u70ed\u542f\u52a8\u6a21\u578b\u201d\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8e\u8f93\u5165\u4e0a\u4e0b\u6587\u9884\u6d4b\u7684\u6709\u4fe1\u606f\u5148\u9a8c\u566a\u58f0\u5206\u5e03N(mu, sigma)\uff0c\u800c\u975e\u4f20\u7edf\u7684\u65e0\u4fe1\u606fN(0, I)\u5206\u5e03\uff0c\u6765\u4f5c\u4e3a\u751f\u6210\u8fc7\u7a0b\u7684\u8d77\u59cb\u70b9\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u6761\u4ef6\u5f52\u4e00\u5316\u6280\u5de7\uff0c\u786e\u4fdd\u5176\u80fd\u517c\u5bb9\u4efb\u4f55\u6807\u51c6\u751f\u6210\u6a21\u578b\u548c\u91c7\u6837\u5668\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ec5\u752811\u6b21\u51fd\u6570\u8bc4\u4f30\uff081\u6b21\u70ed\u542f\u52a8\uff0c10\u6b21\u751f\u6210\uff09\u5373\u53ef\u8fbe\u5230\u4e0e1000\u6b65DDPM\u57fa\u7ebf\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u8fc7\u7a0b\u6240\u9700\u904d\u5386\u7684\u8ddd\u79bb\u3002", "conclusion": "\u201c\u70ed\u542f\u52a8\u6a21\u578b\u201d\u901a\u8fc7\u63d0\u4f9b\u66f4\u4f18\u7684\u521d\u59cb\u566a\u58f0\u5206\u5e03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fed\u4ee3\u751f\u6210\u6a21\u578b\u901f\u5ea6\u6162\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u6761\u4ef6\u751f\u6210\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u4e0e\u5176\u4ed6\u9ad8\u6548\u91c7\u6837\u6280\u672f\u7684\u7ec4\u5408\u6f5c\u529b\u3002"}}
{"id": "2507.09471", "pdf": "https://arxiv.org/pdf/2507.09471", "abs": "https://arxiv.org/abs/2507.09471", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCKAA\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u77e5\u8bc6\u5bf9\u9f50\u548c\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9002\u914d\u5668\u6df7\u5408\uff0c\u89e3\u51b3\u4e86PEFT-based\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u8bef\u5bfc\u6027\u4efb\u52a1ID\u4e0b\u56e0\u7279\u5f81\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u7684\u51b3\u7b56\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8ePEFT\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u9762\u4e34\u8bef\u5bfc\u6027\u4efb\u52a1ID\u65f6\uff0c\u7531\u4e8e\u72ec\u7acb\u8bad\u7ec3\u7684\u5b50\u6a21\u5757\u5bfc\u81f4\u7279\u5f81\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\uff0c\u5bb9\u6613\u4ea7\u751f\u6a21\u7cca\u51b3\u7b56\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCross-subspace Knowledge Alignment and Aggregation (CKAA) \u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u521b\u65b0\uff1a1) \u53cc\u5c42\u77e5\u8bc6\u5bf9\u9f50 (DKA)\uff1a\u901a\u8fc7\u5bf9\u9f50\u8de8\u5b50\u7a7a\u95f4\u7684\u7c7b\u5185\u7279\u5f81\u5206\u5e03\u548c\u7279\u5f81\u6a21\u62df\u5b66\u4e60\u5168\u5c40\u5206\u7c7b\u5668\uff0c\u63d0\u9ad8\u6a21\u578b\u533a\u5206\u80fd\u529b\u30022) \u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9002\u914d\u5668\u6df7\u5408 (TC-MoA)\uff1a\u4e00\u79cd\u9c81\u68d2\u7684\u63a8\u7406\u65b9\u6848\uff0c\u57fa\u4e8e\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u805a\u5408\u4efb\u52a1\u77e5\u8bc6\uff0c\u907f\u514d\u8bef\u5bfc\u6027\u9884\u6d4b\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cCKAA\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8ePEFT\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "CKAA\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u5bf9\u9f50\u548c\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86PEFT-based\u6301\u7eed\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u548c\u8bef\u5bfc\u6027\u4efb\u52a1\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u9c81\u68d2\u6027\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2507.10472", "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMLAR\u7684\u521b\u65b0\u578bRPA\u6846\u67b6\u589e\u5f3a\u7684\u6c42\u804c\u8005\u8ffd\u8e2a\u7cfb\u7edf\uff08ATS\uff09\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u7b80\u5386\u7b5b\u9009\u548c\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u62db\u8058\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u62db\u8058\u6d41\u7a0b\u5728\u7b80\u5386\u7b5b\u9009\u548c\u5019\u9009\u4eba\u521d\u9009\u73af\u8282\u5e38\u56e0\u65f6\u95f4\u548c\u8d44\u6e90\u9650\u5236\u800c\u9762\u4e34\u74f6\u9888\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MLAR\u6846\u67b6\u91c7\u7528\u4e09\u5c42LLMs\u5904\u7406\uff1a\u7b2c\u4e00\u5c42\u4ece\u804c\u4f4d\u63cf\u8ff0\u4e2d\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u7b2c\u4e8c\u5c42\u89e3\u6790\u7533\u8bf7\u4eba\u7b80\u5386\u4ee5\u8bc6\u522b\u6559\u80b2\u3001\u7ecf\u9a8c\u548c\u6280\u80fd\uff0c\u7b2c\u4e09\u5c42\u8fdb\u884c\u76f8\u4f3c\u5ea6\u5339\u914d\u3002\u901a\u8fc7\u5148\u8fdb\u7684\u8bed\u4e49\u7b97\u6cd5\u5339\u914d\u7279\u5f81\uff0c\u5e76\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709RPA\u6d41\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u7b80\u5386\u89e3\u6790\u3001\u804c\u4f4d\u5339\u914d\u548c\u5019\u9009\u4eba\u901a\u77e5\u7684\u81ea\u52a8\u5316\u3002", "result": "MLAR\u5728\u5904\u7406\u5927\u91cf\u7b80\u5386\u4efb\u52a1\u65f6\u4f18\u4e8e\u4e3b\u6d41RPA\u5e73\u53f0\uff08\u5982UiPath\u548cAutomation Anywhere\uff09\u3002\u5904\u74062400\u4efd\u7b80\u5386\u65f6\uff0cMLAR\u5e73\u5747\u6bcf\u4efd\u7b80\u5386\u5904\u7406\u65f6\u95f4\u4e3a5.4\u79d2\uff0c\u4e0eAutomation Anywhere\u76f8\u6bd4\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u7ea616.9%\uff0c\u4e0eUiPath\u76f8\u6bd4\u51cf\u5c11\u7ea617.1%\u3002", "conclusion": "MLAR\u5c55\u73b0\u51fa\u9769\u65b0\u62db\u8058\u5de5\u4f5c\u6d41\u7a0b\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u62db\u8058\u9700\u6c42\u3002"}}
{"id": "2507.09213", "pdf": "https://arxiv.org/pdf/2507.09213", "abs": "https://arxiv.org/abs/2507.09213", "authors": ["Dunsheng Huang", "Dong Shen", "Lei Lu", "Ying Tan"], "title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications", "categories": ["cs.LG", "stat.ML", "68T07"], "comment": "17pages", "summary": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from\nthe data, has been widely used in signal processing, and time-series analysis.\nHowever, challenges in constructing accurate wavelet bases and high\ncomputational costs limit their application. This study introduces a\nconstructive WNN that selects initial bases and trains functions by introducing\nnew bases for predefined accuracy while reducing computational costs. For the\nfirst time, we analyze the frequency of unknown nonlinear functions and select\nappropriate initial wavelets based on their primary frequency components by\nestimating the energy of the spatial frequency component. This leads to a novel\nconstructive framework consisting of a frequency estimator and a wavelet-basis\nincrease mechanism to prioritize high-energy bases, significantly improving\ncomputational efficiency. The theoretical foundation defines the necessary\ntime-frequency range for high-dimensional wavelets at a given accuracy. The\nframework's versatility is demonstrated through four examples: estimating\nunknown static mappings from offline data, combining two offline datasets,\nidentifying time-varying mappings from time-series data, and capturing\nnonlinear dependencies in real time-series data. These examples showcase the\nframework's broad applicability and practicality. All the code will be released\nat https://github.com/dshuangdd/CWNN.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u9891\u7387\u5206\u6790\u548c\u4f18\u5148\u7ea7\u57fa\u9009\u62e9\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u6784\u9020\u578b\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\uff08CWNN\uff09\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\uff08WNN\uff09\u9762\u4e34\u6784\u5efa\u7cbe\u786e\u5c0f\u6ce2\u57fa\u7684\u6311\u6218\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u6784\u9020\u578bWNN\uff0c\u9996\u6b21\u5206\u6790\u672a\u77e5\u975e\u7ebf\u6027\u51fd\u6570\u7684\u9891\u7387\u7279\u6027\uff0c\u5e76\u57fa\u4e8e\u4e3b\u8981\u9891\u7387\u5206\u91cf\u9009\u62e9\u521d\u59cb\u5c0f\u6ce2\u3002\u6846\u67b6\u5305\u542b\u9891\u7387\u4f30\u8ba1\u5668\u548c\u5c0f\u6ce2\u57fa\u589e\u52a0\u673a\u5236\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u80fd\u91cf\u57fa\uff0c\u5e76\u5b9a\u4e49\u4e86\u9ad8\u7ef4\u5c0f\u6ce2\u7684\u5fc5\u8981\u65f6\u9891\u8303\u56f4\u3002", "result": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u5e94\u7528\u793a\u4f8b\uff08\u5305\u62ec\u9759\u6001\u6620\u5c04\u4f30\u8ba1\u3001\u6570\u636e\u96c6\u7ed3\u5408\u3001\u65f6\u53d8\u6620\u5c04\u8bc6\u522b\u548c\u5b9e\u65f6\u6570\u636e\u975e\u7ebf\u6027\u4f9d\u8d56\u6355\u83b7\uff09\uff0c\u9a8c\u8bc1\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6784\u9020\u578bWNN\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u548c\u589e\u52a0\u5c0f\u6ce2\u57fa\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfWNN\u7684\u5c40\u9650\uff0c\u5728\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.09487", "pdf": "https://arxiv.org/pdf/2507.09487", "abs": "https://arxiv.org/abs/2507.09487", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "AI": {"tldr": "HMID-Net\u9996\u6b21\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9-\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u548c\u8bed\u4e49\u6982\u5ff5\u5e38\u4ee5\u5c42\u6b21\u7ed3\u6784\u5448\u73b0\uff0cMERU\u5df2\u6210\u529f\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u6355\u6349\u5230\u8fd9\u79cd\u89c6\u89c9-\u8bed\u4e49\u5c42\u6b21\u3002\u7136\u800c\uff0c\u5982\u4f55\u66f4\u9ad8\u6548\u5730\u8bad\u7ec3\u6a21\u578b\u4ee5\u6355\u6349\u548c\u5229\u7528\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faHyperbolic Masked Image and Distillation Network (HMID-Net)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5b83\u5c06\u63a9\u7801\u56fe\u50cf\u5efa\u6a21 (MIM) \u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u6574\u5408\u5230\u53cc\u66f2\u7a7a\u95f4\u4e2d\u3002\u540c\u65f6\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u4e3a\u53cc\u66f2\u7a7a\u95f4\u8bbe\u8ba1\u7684\u84b8\u998f\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u4fc3\u8fdb\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5e94\u7528MIM\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u53ef\u4ee5\u53d6\u5f97\u4e0e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u540c\u6837\u663e\u8457\u7684\u6210\u529f\u3002\u5e7f\u6cdb\u7684\u8bc4\u4f30\u663e\u793a\uff0cHMID-Net\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u7d22\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5982MERU\u548cCLIP\u3002", "conclusion": "HMID-Net\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u6a21\u578b\u4ee5\u6355\u6349\u89c6\u89c9-\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
