<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本文提出一种两步因果实验方法，以解耦大型语言模型中认知偏差的来源，发现预训练是塑造偏差的主要因素，而非微调或训练随机性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）表现出类似于人类的认知偏差。先前工作发现这些偏差因模型而异，并可被指令微调放大。但目前尚不清楚这些偏差差异是源于预训练、微调还是训练随机性。

Method: 本研究提出两步因果实验方法。首先，通过使用不同随机种子多次微调模型，研究训练随机性对30多种认知偏差的影响。其次，引入“交叉微调”方法，即在模型间交换导致不同偏差模式的指令数据集，以隔离偏差来源并直接测试偏差是否依赖于数据集。

Result: 研究发现，虽然训练随机性会引入一定变异性，但偏差主要由预训练塑造：具有相同预训练主干的模型比仅共享微调数据的模型展现出更相似的偏差模式。

Conclusion: 这些见解表明，理解微调模型中的偏差需要考虑其预训练起源，而不仅仅是微调效应。这一视角可以指导未来开发评估和缓解LLM偏差的原则性策略。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）在社会科学调查中作为人类代理的响应鲁棒性，发现它们存在近因偏差，并对问题措辞和答案选项的扰动敏感，强调了提示设计和鲁棒性测试的重要性。


<details>
  <summary>Details</summary>
Motivation: LLMs在社会科学调查中被日益用作人类受试者的替代，但其可靠性及其对已知响应偏差的敏感性尚不明确。

Method: 测试了九个不同的LLMs，使用世界价值观调查（WVS）中的问题，并对问题措辞和答案选项结构应用了11种扰动，进行了超过167,000次模拟访谈以评估响应鲁棒性。

Result: 所有LLMs均表现出一致的“近因偏差”，倾向于选择最后一个选项；大型模型通常更鲁棒，但所有模型仍对语义变体和组合扰动敏感；LLMs的调查响应偏差与人类的偏差部分一致。

Conclusion: 在使用LLMs生成合成调查数据时，提示设计和鲁棒性测试至关重要，以应对其响应偏差和对扰动的敏感性。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: SynthTextEval是一个全面评估合成文本的工具包，旨在通过标准化多维度评估，提升合成文本在AI开发中实现隐私保护的可用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）生成的合成文本在降低AI系统隐私泄露风险等高风险应用中具有巨大潜力。然而，要实现这一潜力，需要对合成数据进行多维度（效用、公平性、隐私泄露风险、分布差异、专家反馈）的系统性、一致性评估。

Method: 本文提出了SynthTextEval工具包，允许用户上传或生成合成数据，并对这些数据在多个关键维度上进行评估。该工具包已在高风险领域（医疗和法律）的数据集上展示了其功能和有效性，通过整合和标准化评估指标来实现其目标。

Result: SynthTextEval提供了一个整合且标准化的评估框架，使得用户能够对合成文本进行全面的多维度评估，从而提升其性能洞察和在AI开发中实现隐私保护的可行性。

Conclusion: SynthTextEval通过统一和标准化合成文本的评估方法，旨在提高合成文本的可用性，进而促进AI开发中的隐私保护。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本研究针对医疗领域大语言模型（LLMs）的安全性问题，首次提出了一套从患者、临床医生及通用用户多视角出发的定制化安全评估协议，并构建了PatientSafetyBench数据集，旨在为医疗LLMs的安全部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在医疗领域的广泛应用，其安全性问题日益突出，尤其考虑到不同用户角色（患者、临床医生）及模型输出对人类健康可能产生的直接影响。现有安全评估主要侧重通用基准，缺乏针对医疗领域特有风险的专业评估。

Method: 引入一套针对医疗领域定制的安全评估协议，该协议涵盖患者用户和临床医生用户视角，并结合通用安全评估；构建了一个包含466个样本、涵盖5个关键类别的PatientSafetyBench数据集，专门用于从患者角度衡量安全性；应用红队（red-teaming）协议，以MediPhi模型系列作为案例进行了定量安全分析。

Result: 本研究成功定义了医疗LLM的安全评估标准，并构建了首个从患者视角出发的专业安全基准PatientSafetyBench。通过对医疗LLM（如MediPhi系列）应用红队协议，对模型安全性进行了案例分析和定量评估。

Conclusion: 本工作首次通过有针对性的红队测试，从患者、临床医生和通用用户三个不同视角定义了医疗LLMs的安全评估标准，为医疗领域中LLMs的安全部署奠定了坚实基础。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本文针对协同学习中多组并发对话环境下的中断检测问题，开发了一种对重叠语音鲁棒的先进方法，并揭示了中断的语言和韵律特征。


<details>
  <summary>Details</summary>
Motivation: 中断在协同学习中至关重要，AI辅助能帮助教师监控。然而，现有中断检测方法主要针对单对话和清晰音频，无法应对真实课堂中多组并发对话、语音重叠普遍存在的复杂环境。

Method: 分析了单对话和多组对话设置下的中断检测，并创建了一种对重叠语音鲁棒的先进中断识别方法。

Result: 开发出一种可部署于课堂的、对重叠语音鲁棒的先进中断识别方法，并揭示了协同学习中中断的语言和韵律特征。

Conclusion: 本研究为课堂中AI辅助中断检测提供了可行方案，并为未来在多组重叠语音环境下追踪群组对话的研究奠定了基础。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 针对LLM结合RAG生成反驳言论时证据有限、控制不足的问题，本文提出多智能体RAG框架，通过多LLM协作优化知识检索、证据增强和回复优化，以生成高质量、及时且准确的健康虚假信息反驳言论，并在多项指标上优于基线方法并获得人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）结合检索增强生成（RAG）在生成反驳虚假信息时，存在依赖有限证据和对最终输出控制不足的局限性。

Method: 提出一个多智能体检索增强框架，该框架整合多个LLMs以优化知识检索、证据增强和回复优化，专门用于生成对抗健康虚假信息的反驳言论。该方法结合了静态和动态证据，并进行了消融研究以验证各组件的必要性。

Result: 该方法在礼貌性、相关性、信息量和事实准确性方面均优于基线方法。此外，人工评估显示，回复优化显著提升了反驳言论的质量并获得了人类偏好。

Conclusion: 所提出的多智能体检索增强框架有效克服了现有方法的局限性，能够生成高质量、有事实依据且受用户偏爱的反驳言论，特别适用于应对健康虚假信息。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 提出一种结合GNN和CNN的新模型，通过实时图生成和字符级输入处理长文本，解决了Transformer的效率问题，并实现了高效且具竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习处理长文本时，时间、成本和能效是关键考量。当前最先进的Transformer模型计算复杂度随输入长度呈二次方增长，导致其在处理长文档时效率低下。

Method: 引入一种结合图神经网络（GNNs）和卷积神经网络（CNNs）的新型模型架构，并集成实时端到端图生成机制。模型处理紧凑的字符级输入批次，无需填充或截断。通过高效字典查找，整合大型语言模型（LLMs）的信息（如词元嵌入和情感极性）以提升性能。CNN用于捕获局部上下文模式，基于格子的图结构扩展局部感受野，小世界图用于聚合文档级信息。

Result: 生成的图表现出有意义的语义组织结构，平均聚类系数约为0.45，平均最短路径长度在4到5之间。该模型在情感分析和新闻分类等多种文本分类任务中进行了评估，并与现有最先进模型进行了比较。实验结果证实了所提出模型的效率和竞争力。

Conclusion: 所提出的结合GNN和CNN的模型，通过创新的图生成和处理机制，有效解决了长文本处理中Transformer的效率瓶颈，并在多项文本分类任务中展示了高效且具竞争力的性能。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出MedReadCtrl，一个可控可读性指令微调框架，使大模型能在医疗领域生成复杂性可调且意义不变的输出。它在多项任务上显著优于GPT-4，并获得专家高度认可，有助于促进患者教育和公平医疗。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗健康领域潜力巨大，但部署的关键挑战在于实现有效的人机沟通，即输出内容需兼具个性化和可理解性，能够调整复杂性而不损害原意。

Method: 引入MedReadCtrl，一个可控可读性指令微调框架，旨在使大型语言模型（LLMs）能够根据指令调整其输出内容的复杂程度，同时确保不损害原始语义。

Result: 在9个数据集和3项医疗与通用领域任务的评估中，MedReadCtrl在可读性指令遵循错误方面显著低于GPT-4（如在ReadMe上为1.39 vs 1.59，p<0.001）。在未见过的临床任务上取得了显著提升（如在MTSamples上ROUGE-L提高14.7，SARI提高6.18）。专家评审一致偏好MedReadCtrl（71.7% vs 23.3%），尤其是在低识字水平的场景下。

Conclusion: MedReadCtrl能将临床内容重构为易于理解、符合可读性要求且保留医学原意的语言，提供了一个可扩展的解决方案，有效支持患者教育并扩大AI辅助医疗的公平可及性。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 研究人员开发了一个名为SynthEHR-Eviction的管道，结合LLMs、人工标注和APO，从临床笔记中高效提取驱逐状态，并创建了最大的驱逐相关SDoH数据集。该管道训练的LLMs在驱逐检测方面表现出色，并能显著减少标注工作量。


<details>
  <summary>Details</summary>
Motivation: 驱逐是影响健康的重要社会决定因素（SDoH），但其在非结构化电子健康记录（EHRs）中很少被编码，这限制了其在健康应用中的分析和利用。

Method: 引入了SynthEHR-Eviction管道，该管道结合了大型语言模型（LLMs）、人工参与标注和自动化提示优化（APO）技术，用于从临床笔记中提取驱逐状态。利用此管道，研究人员创建了迄今为止最大的公共驱逐相关SDoH数据集，包含14个细粒度类别。

Result: 通过SynthEHR-Eviction训练的微调LLMs（如Qwen2.5、LLaMA3）在人工验证数据上，驱逐检测的Macro-F1得分达到88.8%，其他SDoH的Macro-F1得分达到90.3%。这些模型表现优于GPT-4o-APO、GPT-4o-mini-APO和BioBERT。该管道还将标注工作量减少了80%以上。

Conclusion: 该SynthEHR-Eviction管道能加速数据集创建，实现可扩展且成本效益高的驱逐检测，并能推广到其他信息提取任务，为利用非结构化临床数据进行SDoH研究提供了强大工具。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 将时间序列推理能力蒸馏到小型语言模型中以实现可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在构建可解释的时间序列基础模型，具体是通过将时间序列推理能力赋予小型指令调优语言模型。

Method: 利用合成的均值回归时间序列数据集，通过大型多模态模型生成自然语言标注，并用其监督微调小型Qwen模型；引入评估指标（趋势方向、噪声强度、极值定位）评估蒸馏推理的质量。

Result: 训练后的模型获得了有意义的解释能力，验证了将时间序列理解压缩到轻量级、具备语言能力的模型中的可行性。

Conclusion: 本工作为开发能用自然语言解释时间模式的小型可解释模型奠定了坚实基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAND的框架，通过让大语言模型（LLM）智能体在行动前显式地审议候选行动来解决现有微调方法探索不足的问题，并在交互式任务中显著提升了智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体微调方法（如监督微调、偏好优化）侧重于模仿专家行为或推广特定推理，但缺乏对替代行动的审议，这可能导致智能体因行动空间探索有限而选择次优行动。

Method: 本文提出了自学行动审议（Self-taught ActioN Deliberation, SAND）框架。该框架使LLM智能体能够在提交行动之前，显式地审议候选行动。为解决大规模行动空间和步级行动评估的挑战，SAND结合了自洽行动采样（self-consistency action sampling）和执行引导行动批判（execution-guided action critique），利用LLM智能体的基础模型合成逐步的行动审议思想。这些审议轨迹随后被迭代地用于微调LLM智能体本身。

Result: 在两个代表性交互式智能体任务上的评估结果显示，SAND比初始监督微调平均提高了20%，并且优于现有的最先进智能体微调方法。

Conclusion: SAND框架通过引入显式行动审议机制，有效解决了LLM智能体在行动选择上的局限性，显著提升了其在交互式任务中的性能，证明了这种自学审议方法在智能体微调中的有效性。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: RLEP（强化学习经验回放）是一种两阶段框架，通过收集和回放高质量轨迹，解决了大型语言模型（LLM）强化学习训练不稳定和策略漂移问题。它能加速收敛并在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 将强化学习（RL）应用于大型语言模型（LLM）是一个耗能巨大的过程，其训练可能不稳定，且策略容易逐渐偏离预训练权重。

Method: 提出RLEP框架，该框架分为两个阶段：首先收集经过验证的轨迹，然后在后续训练中回放这些轨迹。在每次更新步骤中，策略通过结合新生成的Rollout和这些回放的成功经验的Mini-batch进行优化。

Result: 通过回放高质量示例，RLEP避免了无效探索，将学习集中在有前景的推理路径上，从而实现了更快的收敛和更强的最终性能。在Qwen2.5-Math-7B模型上，RLEP以更少的更新次数达到并超越了基线峰值准确率，将AIME-2024的准确率从38.2%提高到39.9%，AIME-2025从19.8%提高到22.3%，AMC-2023从77.0%提高到82.2%。

Conclusion: RLEP通过引入经验回放机制，显著提升了大型语言模型强化学习的训练效率、稳定性和最终性能，在数学推理任务上取得了显著的准确率提升。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [13] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 本文提出了“机器胡言乱语”（machine bullshit）作为大语言模型（LLM）真实性丧失的统一概念框架，引入了量化指标“胡言乱语指数”和四种胡言乱语形式的分类。研究发现，RLHF微调和CoT提示会显著加剧LLM的胡言乱语现象，尤其在政治语境中。


<details>
  <summary>Details</summary>
Motivation: 现有研究已探讨LLM的幻觉和逢迎行为，但缺乏一个更广泛的、能够描述LLM普遍真实性丧失并揭示其底层机制的框架。因此，作者提出了“机器胡言乱语”这一概念框架。

Method: 1. 提出了“机器胡言乱语”作为LLM真实性问题的总括性概念框架。2. 引入了量化LLM对真相漠视程度的新指标“胡言乱语指数”（Bullshit Index）。3. 提出了胡言乱语的四种定性形式分类：空洞言辞（empty rhetoric）、避实就虚（paltering）、含糊其辞（weasel words）和未经证实的主张（unverified claims）。4. 在Marketplace、Political Neutrality数据集和新构建的BullshitEval基准（包含2400个场景和100个AI助手）上进行了实证评估。

Result: 1. 使用人类反馈强化学习（RLHF）进行模型微调会显著加剧胡言乱语现象。2. 推理时的思维链（CoT）提示会明显放大特定形式的胡言乱语，尤其是空洞言辞和避实就虚。3. 在政治语境中，机器胡言乱语普遍存在，其中含糊其辞是主要策略。

Conclusion: 本研究揭示了AI对齐方面的系统性挑战，并为实现更真实的LLM行为提供了新的见解。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 该论文提出一种名为MMoE（多层专家混合）的新模型，旨在解决多模态实体链接（MEL）中提及歧义和模态内容动态选择的挑战。MMoE利用大型语言模型增强提及信息，并通过多层专家混合机制动态选择和融合多模态特征，实验证明其性能优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体链接（MEL）方法未能有效解决以下两个重要问题：1) 提及歧义，即提及的文本上下文过于简短或省略关键信息导致语义内容不足；2) 模态内容动态选择，即无法动态区分和选择不同模态信息中重要部分的有效性。

Method: 本文提出MMoE（多层专家混合）模型。该模型包含四个主要组件：1) 描述感知提及增强模块，利用大型语言模型识别最匹配提及的WikiData描述；2) 多模态特征提取模块，采用多模态特征编码器获取提及和实体的文本及视觉嵌入；3) 层内专家混合模块和 4) 层间专家混合模块，应用切换专家混合（Switch Mixture of Experts）机制，动态自适应地从相关信息区域选择特征。

Result: 广泛的实验结果表明，MMoE模型相比于现有最先进（state-of-the-art）方法，展现出卓越的性能。

Conclusion: MMoE模型通过创新的多层专家混合机制，有效缓解了多模态实体链接中的提及歧义和模态内容动态选择问题，并取得了领先的实体链接性能。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [15] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoPT的新型损失函数，用于语义分割中的无监督域适应（UDA）。CoPT利用大型语言模型（LLM）生成的领域描述，通过CLIP模型获取域无关的文本嵌入，从而训练分割编码器学习域不变特征，并在UDA分割任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 语义分割任务的标注收集困难，尽管视觉-语言表示学习取得了进展，但现有的UDA方法尚未充分利用文本的域无关特性来提升分割模型的跨域泛化能力。

Method: 本文提出了一种基于协方差的像素文本损失（CoPT）。该方法通过一个“LLM域模板”过程生成域无关的文本嵌入：LLM首先生成源域和目标域的描述，然后将这些描述输入到一个冻结的CLIP模型中并结合，从而获得用于学习图像分割编码器中域不变特征的文本嵌入。

Result: 在四个基准测试中，使用CoPT训练的模型在语义分割的无监督域适应任务上取得了新的最先进（SOTA）性能。

Conclusion: 本研究证明，通过利用LLM和CLIP模型生成的域无关文本嵌入，可以显著提高语义分割在无监督域适应任务上的性能，并达到了当前技术的最高水平。

Abstract: Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [16] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 本文提出了Recall框架，一个新颖的对抗性框架，通过优化多模态图像提示来攻击并揭示已遗忘图像生成模型的脆弱性，强调了需要更鲁棒的机器学习遗忘解决方案。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型（IGMs）虽能力强大，但也引发了生成有害或侵权内容的伦理、法律和社会担忧。机器遗忘（MU）是解决此问题的方案，但现有遗忘技术在面对多模态对抗性输入时，其鲁棒性和有效性尚未被充分探索。

Method: 作者提出了Recall框架，一个旨在破坏已遗忘IGMs鲁棒性的对抗性框架。与现有依赖对抗性文本提示的方法不同，Recall利用扩散模型的多模态条件能力，通过从单个语义相关的参考图像引导，高效优化对抗性图像提示。

Result: 在十种最先进的遗忘方法和不同任务上进行的广泛实验表明，Recall在对抗性有效性、计算效率以及与原始文本提示的语义保真度方面，始终优于现有基线。这些发现揭示了当前遗忘机制的关键漏洞。

Conclusion: 当前机器学习遗忘机制存在关键漏洞，强调了为了确保生成模型的安全性与可靠性，需要开发更强大的解决方案。

Abstract: Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [17] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

TL;DR: 该综述对生物医学图像分析中的可解释人工智能（XAI）方法进行了全面且结构化的合成分析，提出模态中心分类法，并探讨了多模态和视觉-语言范式的最新进展。


<details>
  <summary>Details</summary>
Motivation: 现有XAI综述缺乏模态感知视角，忽视多模态和视觉-语言范式的新进展，且实践指导有限，无法满足生物医学图像分析的需求。

Method: 本研究通过系统分类XAI方法，分析其原理、优缺点；提出以模态为中心的分类法；审查多模态学习和视觉-语言模型在可解释生物医学AI中的作用；总结评估指标、开源框架，并讨论挑战和未来方向。

Result: 提供了生物医学图像分析领域XAI方法的全面概述，提出了创新的模态中心分类法，填补了多模态和视觉-语言XAI研究的空白，并总结了重要资源与讨论。

Conclusion: 该综述为推进生物医学图像分析中可解释深度学习的发展提供了及时而深入的基础。

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [18] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 本文研究多模态大语言模型（MLLMs）在输入模态冲突下产生的幻觉问题。提出模态冲突定义并构建MMMC数据集。通过提示工程、监督微调和强化学习三种方法缓解幻觉，结果显示强化学习效果最佳，监督微调表现稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在视觉-语言任务中能力强大，但在实际应用中易产生幻觉。现有工作主要关注模型响应与输入间的冲突，而本研究旨在探究输入中不同模态间固有的冲突，这种冲突使MLLMs陷入困境并直接导致幻觉。

Method: 本文正式定义了模态冲突，并构建了名为多模态模态冲突（MMMC）的数据集以模拟视觉-语言任务中的该现象。为缓解模态冲突引起的幻觉，提出了基于提示工程、监督微调和强化学习的三种方法。在MMMC数据集上进行了大量实验以分析这些方法的优缺点。

Result: 实验结果表明，在缓解模态冲突引起的幻觉方面，强化学习方法表现最佳，而监督微调方法则显示出有前景且稳定的性能。

Conclusion: 本工作揭示了导致幻觉的未被注意到的模态冲突，并为多模态大语言模型的鲁棒性提供了更深入的见解。

Abstract: Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [19] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 在GNSS受限环境下，UAV利用基于YOLOv8、特征匹配和色调直方图分析的视觉方法实现自主海上目标船只识别与定位。


<details>
  <summary>Details</summary>
Motivation: 在GNSS不可用环境下，自主海上监视和目标船只识别对搜救和威胁检测等应用至关重要。当目标仅有视觉线索且位置未知时，无人机需仅凭机载视觉在大范围、计算受限区域内进行扫描。

Method: 首先利用YOLOv8模型检测视野内所有船只；其次，通过特征匹配和色调直方图距离分析确定目标船只；最后，利用简单的几何原理对目标进行定位。

Result: 该方法在MBZIRC2023比赛的真实实验中得到验证，并集成到支持GNSS受限导航的全自主系统中。此外，还评估了视角对检测精度和定位准确性的影响，并与理想方法进行了比较。

Conclusion: 本研究提出了一种在GNSS受限环境下，通过纯视觉实现自主海上目标识别与定位的有效方法，并在真实世界比赛中成功验证，展现了其在关键应用场景中的实用性和潜力。

Abstract: Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [20] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

TL;DR: CL-Polyp是一种基于对比学习的息肉分割网络，通过自监督学习增强特征提取，并引入MASPP和CA模块优化多尺度融合及边界重建，在多个基准数据集上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 结直肠息肉的准确分割对癌症早期诊断和治疗至关重要。现有基于深度学习的息肉分割方法（如编码器-解码器架构和多任务框架）常需额外标注数据且泛化性受限。

Method: 提出CL-Polyp，一个对比学习增强的息肉分割网络。该方法利用对比学习通过正负样本对增强编码器的判别性特征提取能力，无需额外标注。同时，引入改进的空洞空间金字塔池化（MASPP）模块用于多尺度特征融合，并引入通道连接和元素相加（CA）模块用于低层和上采样特征的融合，以改善边界重建。

Result: 在Kvasir-SEG、CVC-ClinicDB、CVC-ColonDB、CVC-300和ETIS五个基准数据集上的广泛实验表明，CL-Polyp持续优于现有最先进方法。具体而言，它在Kvasir-SEG和CVC-ClinicDB数据集上分别将IoU指标提高了0.011和0.020。

Conclusion: CL-Polyp在临床息肉分割任务中表现出卓越的有效性，验证了其通过对比学习和优化的模块设计，在无需额外标注的情况下显著提升息肉分割性能的能力。

Abstract: Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [21] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: 该研究提出一种文本介导的视觉解码框架，通过将EEG信号与LLM生成的语义描述对齐，再利用扩散模型生成图像，实现了从EEG信号到视觉体验的高精度解码，并具有神经认知可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管EEG易于获取且时间精度高，但其空间细节限制了直接的图像重建。研究旨在解决这一问题，实现从EEG信号中可靠地解码视觉体验。

Method: 该模型通过对比学习，使用Transformer编码器将EEG信号映射到多级别（从物体到抽象概念）的语义描述。这些描述由大型语言模型生成。在推理阶段，利用投影头提取的描述嵌入来条件化预训练的潜在扩散模型，从而生成图像，避免了EEG到图像的直接转换。

Result: 该文本介导的框架在EEGCVPR数据集上实现了最先进的视觉解码性能，并与已知的神经认知通路具有可解释的对齐。EEG与语义描述的关联揭示了不同语义层级的重要性。显著性图和t-SNE投影显示了头皮上的语义拓扑结构。

Conclusion: 结构化的语义中介能够实现与认知对齐的EEG视觉解码。

Abstract: Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [22] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

TL;DR: 本文全面回顾了32篇视频生成领域的论文，旨在识别能生成长时、多角色、高一致性视频的关键架构和训练策略，并构建了新的分类体系和对比表格。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成超过16秒的长视频时，难以保持角色一致性、场景布局和运动连贯性，尤其在多主体视频中问题更为突出。即使能生成更长视频，也常面临帧冗余和时间多样性低的问题。

Method: 本文通过对32篇视频生成论文的全面研究，分析并识别了能够产生具有叙事连贯性、高保真细节和多角色特征的长视频的关键架构组件和训练策略。同时，构建了一个全面的新分类法，并提供了根据架构设计和性能特征对论文进行分类的比较表格。

Result: 通过对现有研究的综合分析，本文成功识别了生成高质量、长时、多角色视频的关键架构和训练策略。成果包括建立了一个全面的新分类法和详细的比较表格，清晰地展示了现有方法的架构设计和性能特点。

Conclusion: 本文为视频生成领域提供了宝贵的结构化分析和见解，揭示了实现长时、多角色、高一致性视频的关键要素。这项工作有助于指导未来研究，以克服现有模型的局限性，推动视频生成技术的发展。

Abstract: Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 本文提出一个统一的LLM代理框架，将大型语言模型应用于化学过程的离散故障恢复规划和连续控制。该框架通过有限状态机、模拟代理和验证循环在模拟和物理平台上均表现出色，证明LLM可实现高低层统一控制。


<details>
  <summary>Details</summary>
Motivation: 面对现代化工过程日益增长的复杂性、劳动力短缺以及复杂的故障场景，需要结合符号推理和自适应控制的新型自动化范式。

Method: 引入了一个统一的LLM代理框架，利用LLM实现离散故障恢复规划和连续过程控制。该框架采用有限状态机（FSMs）作为操作范围，其中LLM驱动的规划代理提出恢复序列，模拟代理执行并检查转换，并通过验证器-重复提示循环迭代优化无效计划。

Result: 在案例研究1中，GPT-4o和GPT-4o-mini在180个不同大小的FSMs上，五次重复提示内实现了100%的有效路径成功率，优于开源LLM。在案例研究2中，该框架在实验室TCLab平台上实现了与经典PID控制相似的温度控制性能，且提示循环对处理非线性动态至关重要。研究还分析了指令遵循失误和粗糙ODE近似等关键失败模式。

Conclusion: 研究结果表明，通过结构化反馈和模块化代理，大型语言模型能够统一高层符号规划和低层连续控制，为化工领域韧性、语言驱动的自动化开辟了新途径。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [24] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: AI画作分类中存在偏差问题，特别是在不平衡数据集上。本文提出一种名为BOOST的OOD信息模型偏差自适应采样方法，通过动态调整温度缩放和采样概率来缓解偏差，并在实验中验证其在性能与公平性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: AI在画作分类中存在普遍偏差，尤其源于不平衡数据集，导致模型对不常见画作的预测准确性下降。随着AI系统日益集成到艺术策展和修复等任务中，这一问题愈发严重。现有研究虽提升了分类性能，但普遍忽视了处理OOD数据时的潜在偏差，亟需更鲁棒的方法来缓解艺术分类模型中的偏差。

Method: 提出了一种新颖的、OOD信息模型偏差自适应采样方法，名为BOOST (Bias-Oriented OOD Sampling and Tuning)。该方法通过动态调整温度缩放和采样概率来促进各类别更公平的表示。此外，还提出了一种新的评估指标SODC (Same-Dataset OOD Detection Score)，用于评估类别分离度和每类别偏差减少。

Result: 在KaoKore和PACS数据集上的评估表明，所提出的BOOST方法能有效减少类别偏差。该方法成功地平衡了高分类性能与公平性。

Conclusion: BOOST方法为艺术领域AI模型去偏差提供了一个鲁棒的解决方案，能够有效地平衡模型的性能和公平性，尤其在处理具有偏差的训练数据时。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [25] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: SIBP通过对话状态推断和规则遵守，解决了LLMs在游戏交易中常见的规则违规问题，实现了高准确性和可信度，为NPC交互提供实用基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在游戏动态交互方面表现出色，但在规则严格的交易系统中，常出现物品幻觉和计算错误等问题，导致玩家信任度下降。

Method: 提出基于状态推断的提示（SIBP）方法，通过自主对话状态推断和上下文特定规则遵守实现可靠交易。该方法将交易分解为六个状态，统一于一个提示框架内，并实现上下文感知的物品引用和基于占位符的价格计算。

Result: 在100个交易对话评估中，SIBP展现了超过97%的状态合规性、超过95%的引用准确性和99.7%的计算精度，同时保持计算效率并优于基线方法。

Conclusion: SIBP为商业游戏中可信赖的NPC交互奠定了实用基础，有效解决了LLMs在规则驱动型游戏系统中的可靠性问题。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [26] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 本文针对非法供应链数据稀疏且不可靠的检测难题，提出探索神经符号方法，并利用大型语言模型（LLM）结合问题树从新闻文章中自动提取特征，以系统评估人工与机器在分类非法供应链活动相关新闻文章上的差异。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂，非法活动（如假冒、强迫劳动、人口贩运）使其分析难度倍增。传统机器学习需要大量训练数据，但非法供应链数据稀疏且常被故意破坏或不可靠。因此，需要无需大量训练数据，即可自动检测复杂甚至时序数据中与非法活动相关的新模式。

Method: 1. 探索神经符号方法以识别供应链中的非法活动实例。2. 比较从准确描述当局发现的非法活动新闻文章中手动和自动提取特征的有效性。3. 提出一种问题树方法，用于查询大型语言模型（LLM）以识别和量化文章的相关性。

Result: 本文提出的方法旨在实现对人类和机器在分类与供应链中强迫劳动相关新闻文章方面差异的系统评估。

Conclusion: 通过结合神经符号方法和基于LLM的问题树查询，本研究旨在提供一种新的途径，用于在数据稀疏、不可靠的非法供应链环境中自动检测新模式，有望提高非法活动识别的准确性和效率。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [27] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 一个名为cmbagent的多智能体LLM系统实现了科学研究任务的完全自动化，在博士级宇宙学任务和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在实现科学研究任务的自动化，提高研究效率，并克服单一大型语言模型（LLM）或人工干预的局限性。

Method: 开发了由大约30个LLM智能体组成的多智能体系统cmbagent。该系统采用“规划与控制”策略来编排工作流程，全程无需人工干预。每个智能体专注于特定任务（如论文和代码库检索、代码编写、结果解释、输出评估），并具备本地代码执行能力。

Result: 成功应用于完成一项博士级别的宇宙学任务（利用超新星数据测量宇宙学参数）。在两个基准测试集上进行评估，其性能优于现有最先进的LLM。

Conclusion: cmbagent证明了多智能体LLM系统在自动化复杂科学研究任务方面的可行性和高效性，其性能超越了现有LLM，展现了在高级科学任务自动化领域的巨大潜力。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [28] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本研究探索使用大型语言模型作为专家规划器，以解决多智能体强化学习中的高效探索问题。


<details>
  <summary>Details</summary>
Motivation: 高效探索是深度强化学习中的一个重要问题，在多智能体强化学习中因其复杂性而变得更加严峻。

Method: 本工作研究了将大型语言模型（LLMs）应用于作为专家规划器，以在多智能体基于规划的任务中实现高效探索。

Result: 摘要中未明确提及具体研究结果，此工作旨在调查大型语言模型作为专家规划器的应用潜力。

Conclusion: 结论尚未明确，此工作主要聚焦于探索大型语言模型在多智能体高效探索中的应用前景。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [29] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一个多模态翻译智能体系统，通过结合视觉、上下文信息和记忆模块，显著提升了字幕生成和通用翻译质量，并推出了新的长格式视频翻译基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的翻译智能体虽性能优异，但仅限于文本输入，无法处理多模态信息。本研究旨在开发一个能够处理多模态输入的翻译系统，以克服这一局限性。

Method: ViDove系统受人类翻译工作流程启发，利用视觉和上下文背景信息增强翻译过程。其核心方法包括整合多模态记忆系统以及富含领域特定知识的长短期记忆模块。

Result: ViDove在字幕生成和通用翻译任务中表现出显著的翻译质量提升，BLEU分数比现有最先进基线提高了28%，SubER提高了15%。此外，论文还引入了DoveBench，一个包含17小时高质量人工标注数据的新基准测试数据集。

Conclusion: ViDove成功地将多模态输入引入翻译智能体，通过模拟人类翻译过程和引入先进的记忆系统，显著提升了翻译性能，并在多模态翻译领域建立了新的基准和数据集。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [30] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 研究表明，外部过滤器（如输入提示或输出内容过滤）在计算上难以有效阻止大型语言模型（LLM）生成有害内容，其安全性无法仅通过黑盒访问实现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的广泛部署，其生成有害内容的潜在风险日益突出。本研究旨在探讨LLM的对齐挑战，特别是通过过滤器来预防不安全信息生成的方法。

Method: 研究集中于两种自然干预点：在提示到达模型前的输入过滤和生成后的输出过滤。通过计算复杂性分析，并在密码学硬度假设下，证明了过滤器的局限性。此外，还形式化并研究了宽松的缓解方法。

Result: 研究发现：1) 存在某些LLM，其高效的提示过滤器是不存在的，因为对抗性提示在计算上与良性提示对于任何高效过滤器都难以区分。2) 在特定自然场景下，输出过滤在计算上是不可行的。所有分离结果均基于密码学硬度假设。3) 宽松的缓解方法也存在计算障碍。

Conclusion: 本研究得出结论，LLM的安全性无法通过设计外部过滤器来实现，即仅通过对LLM的黑盒访问是不够的。基于技术结果，作者认为对齐的AI系统的智能性不能与其判断能力分离。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [31] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: 本文提出Sim-to-Dec框架，结合生成式仿真和双感知决策模型，以优化供应链运输中的响应性和经济效率，并在真实数据集上验证其能显著提升及时交付率和利润。


<details>
  <summary>Details</summary>
Motivation: 供应链运输面临高响应性与经济效率的挑战，战略决策（如运输模式选择）对其影响深远。现有方法在泛化能力、精细化动态反映、历史经验与预测洞察整合以及仿真反馈与策略优化结合方面存在不足，亟需一个可观察、低风险的运输策略设计环境。

Method: 本文提出Sim-to-Dec框架，主要包含：1) 生成式仿真模块，利用自回归建模模拟连续状态变化，减少对人工规则的依赖并增强数据波动下的鲁棒性；2) 历史-未来双感知决策模型，通过与仿真器的端到端优化交互进行迭代式改进。

Result: 在三个真实世界数据集上进行的广泛实验证明，Sim-to-Dec显著提高了及时交付率和利润。

Conclusion: Sim-to-Dec框架通过其创新的仿真与决策集成方法，为供应链运输提供了有效的战略设计工具，成功解决了高响应性和经济效率的难题，并在实际应用中展现出卓越的性能。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [32] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS是一种融合RAG、多智能体协作和蒙特卡洛树搜索（MCTS）的药物再利用框架。它无需领域特定微调，能显著提升大型语言模型（LLMs）在药物发现领域的推理能力、召回率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在药物发现等科学领域潜力巨大，但其推理能力受限于预训练知识。现有方法如微调或检索增强生成（RAG）存在计算开销大或未能充分利用结构化科学数据的局限性。

Method: 提出DrugMCTS框架，该框架协同整合了检索增强生成（RAG）、多智能体协作和蒙特卡洛树搜索（MCTS），专注于药物再利用。框架内部署了五个专门的智能体，负责检索和分析分子及蛋白质信息，从而实现结构化和迭代推理。

Result: 实验结果显示，无需领域特定微调，DrugMCTS能使Qwen2.5-7B-Instruct模型性能超越Deepseek-R1超过20%。在DrugBank和KIBA数据集上的广泛实验表明，与通用LLMs和深度学习基线相比，DrugMCTS实现了显著更高的召回率和鲁棒性。

Conclusion: 研究结果强调了结构化推理、基于智能体的协作以及反馈驱动的搜索机制在推进LLM应用于药物发现方面的重要性。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
*A. Bochkov*

Main category: cs.LG

TL;DR: 本文提出一种基于固定输入嵌入的构建式大语言模型开发方法，实现了模块化组合（合并专家模型）和逐层增长，以实现高效灵活的模型扩展。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）的单体式、端到端训练范式资源密集且缺乏灵活性。

Method: 该研究基于不可训练、确定性的输入嵌入作为通用“对接端口”。探索了两种强大的扩展范式：1. 无缝模块化组合：通过简单平均输出logits，在训练后将训练于不同数据集的专业模型合并为单个Mixture-of-Experts (MoE) 模型，无需修改架构。2. 渐进式逐层增长：每次堆叠并训练一层，逐步“生长”一个深度Transformer模型。

Result: 1. 模块化组合：合并后的MoE模型在推理基准测试（如MMLU）上立即展现性能提升，超越了其组成专家模型，且没有灾难性遗忘。2. 逐层增长：该方法显示出稳定的收敛性，并揭示了模型深度与复杂推理能力（如SQuAD）出现之间的明确关联。

Conclusion: 研究结果表明，大语言模型开发应从单体优化转向更具生物性或构建性的范式，即复杂性可增量构建，模块可自由组合。这为资源高效扩展、持续学习以及更民主的AI系统生态系统开辟了新途径。

Abstract: The prevailing paradigm for scaling large language models (LLMs) involves
monolithic, end-to-end training, a resource-intensive process that lacks
flexibility. This paper explores an alternative, constructive approach to model
development, built upon the foundation of non-trainable, deterministic input
embeddings. In prior [1], we established that high-level semantic reasoning can
emerge in Transformers using frozen embeddings derived from the visual
structure of Unicode glyphs. Here, we demonstrate that this fixed
representational substrate acts as a universal "docking port," enabling two
powerful and efficient scaling paradigms: seamless modular composition and
progressive layer-wise growth.
  First, we show that specialist models trained on disparate datasets (e.g.,
Russian and Chinese text) can be merged into a single, more capable
Mixture-of-Experts (MoE) model, post-training, with zero architectural
modification. This is achieved by simply averaging their output logits. The
resulting MoE model exhibits immediate performance improvements on reasoning
benchmarks like MMLU, surpassing its constituent experts without catastrophic
forgetting. Second, we introduce a layer-wise constructive training
methodology, where a deep Transformer is "grown" by progressively stacking and
training one layer at a time. This method demonstrates stable convergence and a
clear correlation between model depth and the emergence of complex reasoning
abilities, such as those required for SQuAD.
  Our findings suggest a paradigm shift from monolithic optimization towards a
more biological or constructive model of AI development, where complexity is
built incrementally and modules can be composed freely. This opens new avenues
for resource-efficient scaling, continual learning, and a more democratized
ecosystem for building powerful AI systems. We release all code and models to
facilitate further research.

</details>


### [34] [FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval](https://arxiv.org/abs/2507.07135)
*François Gardères,Shizhe Chen,Camille-Sovanneary Gauthier,Jean Ponce*

Main category: cs.LG

TL;DR: 本文提出了FACap，一个大规模、自动构建的时尚领域组合图像检索（CIR）数据集，以及FashionBLIP-2模型。该模型通过在FACap上微调通用BLIP-2并结合轻量级适配器和多头匹配机制，显著提升了时尚CIR的性能，尤其是在细粒度检索方面。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法在时尚等应用领域面临挑战，因为时尚领域词汇丰富多样，需要细粒度的视觉和语言理解。此外，由于人工标注成本高昂，缺乏带有详细注释的大规模时尚数据集。

Method: 研究者引入了FACap，一个利用网络时尚图像和两阶段VLM/LLM驱动的自动标注流程构建的大规模时尚CIR数据集。接着，他们提出了FashionBLIP-2模型，该模型通过在FACap上使用轻量级适配器和多头查询-候选匹配机制对通用BLIP-2进行微调，以更好地处理细粒度时尚信息。模型在Fashion IQ和增强版enhFashionIQ数据集上进行了评估。

Result: 实验结果表明，FashionBLIP-2与FACap数据集结合进行预训练，显著提升了模型在时尚CIR任务中的性能，尤其对于涉及细粒度修改文本的检索效果更佳。

Conclusion: 本研究证明了所提出的FACap数据集和FashionBLIP-2方法在时尚电商等高要求环境中的价值，为解决时尚领域组合图像检索的挑战提供了有效方案。

Abstract: The composed image retrieval (CIR) task is to retrieve target images given a
reference image and a modification text. Recent methods for CIR leverage large
pretrained vision-language models (VLMs) and achieve good performance on
general-domain concepts like color and texture. However, they still struggle
with application domains like fashion, because the rich and diverse vocabulary
used in fashion requires specific fine-grained vision and language
understanding. An additional difficulty is the lack of large-scale fashion
datasets with detailed and relevant annotations, due to the expensive cost of
manual annotation by specialists. To address these challenges, we introduce
FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It
leverages web-sourced fashion images and a two-stage annotation pipeline
powered by a VLM and a large language model (LLM) to generate accurate and
detailed modification texts. Then, we propose a new CIR model FashionBLIP-2,
which fine-tunes the general-domain BLIP-2 model on FACap with lightweight
adapters and multi-head query-candidate matching to better account for
fine-grained fashion-specific information. FashionBLIP-2 is evaluated with and
without additional fine-tuning on the Fashion IQ benchmark and the enhanced
evaluation dataset enhFashionIQ, leveraging our pipeline to obtain
higher-quality annotations. Experimental results show that the combination of
FashionBLIP-2 and pretraining with FACap significantly improves the model's
performance in fashion CIR especially for retrieval with fine-grained
modification texts, demonstrating the value of our dataset and approach in a
highly demanding environment such as e-commerce websites. Code is available at
https://fgxaos.github.io/facap-paper-website/.

</details>


### [35] [Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge](https://arxiv.org/abs/2507.07137)
*Eric Yeats,Darryl Hannan,Henry Kvinge,Timothy Doster,Scott Mahan*

Main category: cs.LG

TL;DR: 本文介绍了一种名为autoeval-dmun的自动化工具，利用语言模型评估扩散模型的机器遗忘效果，发现语言模型能有效关联语义损伤并生成规避遗忘的对抗性提示。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘（MU）虽能有效清除扩散模型中的不良信息且成本较低，但面临两大挑战：一是难以验证信息是否被完全清除；二是MU可能对模型保留相关概念的性能造成损害，影响模型部署。因此，迫切需要一种自动化且深入的评估工具。

Method: 研究者提出并引入了自动化工具autoeval-dmun，该工具利用（视觉-）语言模型来全面评估扩散模型的机器遗忘效果。其工作机制是：针对目标概念，autoeval-dmun从语言模型中提取结构化的世界知识，以识别可能受损的邻近概念，并通过生成对抗性提示来测试或规避遗忘。

Result: 通过使用autoeval-dmun评估了多种流行的扩散模型遗忘方法，研究结果显示：1) 语言模型施加的邻近概念语义排序与遗忘造成的损害程度具有良好相关性；2) 语言模型能够有效地生成合成对抗性提示，从而规避机器遗忘。

Conclusion: autoeval-dmun提供了一种对扩散模型机器遗忘效果进行自动化、深入评估的新范式。本研究不仅验证了语言模型在识别遗忘副作用和发现潜在遗忘漏洞方面的强大能力，也为未来改进和验证机器遗忘技术提供了宝贵的工具和见解。

Abstract: Machine unlearning (MU) is a promising cost-effective method to cleanse
undesired information (generated concepts, biases, or patterns) from
foundational diffusion models. While MU is orders of magnitude less costly than
retraining a diffusion model without the undesired information, it can be
challenging and labor-intensive to prove that the information has been fully
removed from the model. Moreover, MU can damage diffusion model performance on
surrounding concepts that one would like to retain, making it unclear if the
diffusion model is still fit for deployment. We introduce autoeval-dmun, an
automated tool which leverages (vision-) language models to thoroughly assess
unlearning in diffusion models. Given a target concept, autoeval-dmun extracts
structured, relevant world knowledge from the language model to identify nearby
concepts which are likely damaged by unlearning and to circumvent unlearning
with adversarial prompts. We use our automated tool to evaluate popular
diffusion model unlearning methods, revealing that language models (1) impose
semantic orderings of nearby concepts which correlate well with unlearning
damage and (2) effectively circumvent unlearning with synthetic adversarial
prompts.

</details>


### [36] [GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction](https://arxiv.org/abs/2507.07138)
*Francesco Ferrini,Veronica Lachi,Antonio Longa,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: SP4LP结合GNN节点编码和最短路径序列建模，有效解决了GNN在链接预测中难以捕获链接特异性结构模式的问题，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在链接预测中难以捕获链接特异性结构模式，因其节点中心的消息传递忽略节点对间的子图结构。现有方法要么计算成本高，要么过于简化，无法建模多跳依赖。

Method: 提出SP4LP框架，该框架首先通过GNN计算节点表示，然后提取候选节点对之间的最短路径，并使用序列模型处理这些节点嵌入序列。

Result: 经验上，SP4LP在链接预测基准测试中达到最先进性能。理论上，SP4LP比标准消息传递GNN及多种现有结构特征方法更具表达力。

Conclusion: SP4LP是一种通用且有原则的图链接预测方法，能高效捕获富有表达力的多跳关系模式。

Abstract: Graph Neural Networks (GNNs) often struggle to capture the link-specific
structural patterns crucial for accurate link prediction, as their node-centric
message-passing schemes overlook the subgraph structures connecting a pair of
nodes. Existing methods to inject such structural context either incur high
computational cost or rely on simplistic heuristics (e.g., common neighbor
counts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest
Path for Link Prediction), a novel framework that combines GNN-based node
encodings with sequence modeling over shortest paths. Specifically, SP4LP first
applies a GNN to compute representations for all nodes, then extracts the
shortest path between each candidate node pair and processes the resulting
sequence of node embeddings using a sequence model. This design enables SP4LP
to capture expressive multi-hop relational patterns with computational
efficiency. Empirically, SP4LP achieves state-of-the-art performance across
link prediction benchmarks. Theoretically, we prove that SP4LP is strictly more
expressive than standard message-passing GNNs and several state-of-the-art
structural features methods, establishing it as a general and principled
approach for link prediction in graphs.

</details>


### [37] [Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts](https://arxiv.org/abs/2507.07140)
*Samin Yeasar Arnob,Zhan Su,Minseon Kim,Oleksiy Ostapenko,Riyasat Ohib,Esra'a Saleh,Doina Precup,Lucas Caccia,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 本文提出一种训练稀疏适配器的新方法，相较于LoRA和全量微调，该方法在性能上更优，特别是在合并后能提供更好的域内性能，但在域外性能上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 为构建可快速适应特定下游任务的模块化架构，现有研究侧重于参数高效的专家合并（如LoRA）。本文旨在探索稀疏适配器作为新型构建块的潜力，并将其合并研究扩展到更广范围（多达20个NLP任务）。

Method: 首先，提出一种训练高效稀疏适配器的简单方法；其次，通过合并多达20个自然语言处理任务的适配器，研究这些稀疏适配器的合并特性，并与LoRA及全量微调/模型合并进行比较。

Result: 提出的稀疏适配器在训练设置中优于LoRA和全量微调。合并后，稀疏适配器在域内性能上显著优于LoRA或全模型合并。然而，所有方法在保持强大的域外性能方面仍面临挑战。

Conclusion: 稀疏适配器是构建模块化架构的有效构建块，尤其是在合并后实现优异的域内性能。尽管如此，提高所有方法的域外泛化能力仍是未来研究的关键挑战。

Abstract: Merging parameter-efficient task experts has recently gained growing
attention as a way to build modular architectures that can be rapidly adapted
on the fly for specific downstream tasks, without requiring additional
fine-tuning. Typically, LoRA serves as the foundational building block of such
parameter-efficient modular architectures, leveraging low-rank weight
structures to reduce the number of trainable parameters. In this paper, we
study the properties of sparse adapters, which train only a subset of weights
in the base neural network, as potential building blocks of modular
architectures. First, we propose a simple method for training highly effective
sparse adapters, which is conceptually simpler than existing methods in the
literature and surprisingly outperforms both LoRA and full fine-tuning in our
setting. Next, we investigate the merging properties of these sparse adapters
by merging adapters for up to 20 natural language processing tasks, thus
scaling beyond what is usually studied in the literature. Our findings
demonstrate that sparse adapters yield superior in-distribution performance
post-merging compared to LoRA or full model merging. Achieving strong held-out
performance remains a challenge for all methods considered.

</details>


### [38] [Str-GCL: Structural Commonsense Driven Graph Contrastive Learning](https://arxiv.org/abs/2507.07141)
*Dongxiao He,Yongqi Huang,Jitao Zhao,Xiaobao Wang,Zhen Wang*

Main category: cs.LG

TL;DR: Str-GCL通过一阶逻辑规则将图结构常识融入对比学习，以提高图表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习(GCL)方法主要关注隐式语义关系，但忽视了图中结构和属性中蕴含的结构常识，而这些常识对有效的表示学习至关重要，且难以明确识别和整合。

Method: 提出Str-GCL框架，利用一阶逻辑规则来表示结构常识（包括拓扑和基于属性的规则），并将其显式地整合到GCL中，通过表示对齐机制引导编码器捕获这些常识，且不改变原始图结构。

Result: 广泛的实验表明，Str-GCL优于现有GCL方法。

Conclusion: Str-GCL为在图表示学习中利用结构常识提供了新的视角。

Abstract: Graph Contrastive Learning (GCL) is a widely adopted approach in
self-supervised graph representation learning, applying contrastive objectives
to produce effective representations. However, current GCL methods primarily
focus on capturing implicit semantic relationships, often overlooking the
structural commonsense embedded within the graph's structure and attributes,
which contains underlying knowledge crucial for effective representation
learning. Due to the lack of explicit information and clear guidance in general
graph, identifying and integrating such structural commonsense in GCL poses a
significant challenge. To address this gap, we propose a novel framework called
Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).
Str-GCL leverages first-order logic rules to represent structural commonsense
and explicitly integrates them into the GCL framework. It introduces
topological and attribute-based rules without altering the original graph and
employs a representation alignment mechanism to guide the encoder in
effectively capturing this commonsense. To the best of our knowledge, this is
the first attempt to directly incorporate structural commonsense into GCL.
Extensive experiments demonstrate that Str-GCL outperforms existing GCL
methods, providing a new perspective on leveraging structural commonsense in
graph representation learning.

</details>


### [39] [Understanding Malware Propagation Dynamics through Scientific Machine Learning](https://arxiv.org/abs/2507.07143)
*Karthik Pappu,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 论文应用科学机器学习方法（特别是通用微分方程UDES）建模恶意软件传播，发现其在预测精度和可解释性上显著优于传统及纯神经网络模型，并揭示了传播抑制机制。


<details>
  <summary>Details</summary>
Motivation: 准确建模恶意软件传播对于设计有效的网络安全防御至关重要，尤其是在面对实时演变的自适应威胁时。现有模型（传统流行病学和纯神经网络）难以充分捕捉现实网络中恶意软件传播的非线性反馈机制。

Method: 论文应用科学机器学习方法对恶意软件传播进行建模，评估了三种方法：经典常微分方程（ODEs）、通用微分方程（UDEs）和神经微分方程（Neural ODEs）。研究使用Code Red蠕虫爆发数据进行验证，并引入了一种符号恢复方法，将学习到的神经反馈转化为显式数学表达式。

Result: 通用微分方程（UDE）方法与传统和纯神经网络基线相比，预测误差显著降低了44%，同时保持了模型的可解释性。通过符号恢复方法，揭示了网络饱和、安全响应和恶意软件变体演化等传播抑制机制。结果表明混合物理信息模型在预测精度和对恶意软件传播动态的深度洞察方面均优于纯分析和纯神经网络方法。

Conclusion: 混合物理信息模型（如UDE）能够有效提高恶意软件传播建模的预测精度和可解释性，为开发早期预警系统、高效的爆发响应策略以及有针对性的网络防御干预提供了有力支持。

Abstract: Accurately modeling malware propagation is essential for designing effective
cybersecurity defenses, particularly against adaptive threats that evolve in
real time. While traditional epidemiological models and recent neural
approaches offer useful foundations, they often fail to fully capture the
nonlinear feedback mechanisms present in real-world networks. In this work, we
apply scientific machine learning to malware modeling by evaluating three
approaches: classical Ordinary Differential Equations (ODEs), Universal
Differential Equations (UDEs), and Neural ODEs. Using data from the Code Red
worm outbreak, we show that the UDE approach substantially reduces prediction
error compared to both traditional and neural baselines by 44%, while
preserving interpretability. We introduce a symbolic recovery method that
transforms the learned neural feedback into explicit mathematical expressions,
revealing suppression mechanisms such as network saturation, security response,
and malware variant evolution. Our results demonstrate that hybrid
physics-informed models can outperform both purely analytical and purely neural
approaches, offering improved predictive accuracy and deeper insight into the
dynamics of malware spread. These findings support the development of early
warning systems, efficient outbreak response strategies, and targeted cyber
defense interventions.

</details>


### [40] [CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs](https://arxiv.org/abs/2507.07145)
*Zhaojing Zhou,Xunchao Li,Minghao Li,Handi Zhang,Haoshuang Wang,Wenbin Chang,Yiqun Liu,Qingqing Dang,Dianhai Yu,Yanjun Ma,Haifeng Wang*

Main category: cs.LG

TL;DR: CCQ是一种推理优化量化方法，将大语言模型（LLMs）压缩至2.0-2.75比特，显著降低推理成本和部署障碍，同时保持极低精度损失，并实现单GPU部署大型模型。


<details>
  <summary>Details</summary>
Motivation: LLMs的快速扩展导致高昂的推理成本和严峻的部署挑战。虽然8或4比特量化能缓解，但低于3比特的方法面临严重的精度、可扩展性和效率下降问题。

Method: 提出卷积码量化（CCQ），集成硬件感知的位移编码解码方案、卷积码、混合编码和代码簇，以克服精度-速度瓶颈。构建无查找的编码空间，实现码本与权重向量的线性映射以优化推理性能。借鉴向量量化的数据映射概念，最小化极低比特下的模型性能退化。

Result: CCQ在多个基准测试中表现出色。成功将DeepSeek-V3（671B）压缩至184GB，ERNIE-4.5-300B-A47B压缩至89GB，从而实现ERNIE 4.5的单GPU部署并消除卡间通信。2比特的ERNIE-4.5-300B-A47B模型和推理引擎已开源。

Conclusion: CCQ通过创新的量化技术，有效解决了LLM推理成本和部署瓶颈，实现了极低比特下高精度和高效率的量化，为LLM的大规模部署提供了可行且高效的解决方案。

Abstract: The rapid scaling of Large Language Models (LLMs) elevates inference costs
and compounds substantial deployment barriers. While quantization to 8 or 4
bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and
efficiency degradation. We propose Convolutional Code Quantization (CCQ), an
inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits
with minimal accuracy loss. Departing from error-prone scalar quantization or
slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding
and decoding solution with Convolutional Code, Hybrid Encoding, and Code
Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a
lookup-free encoding space, enabling a linear mapping between the codebook and
weight vectors, thereby optimizing inference performance. Meanwhile, by drawing
on the concept of data mapping from vector quantization, we minimize the
performance degradation of the model under extremely low-bit conditions.
Experiments demonstrate that CCQ achieves outstanding performance on LLMs
across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to
184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE
4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B
model and inference engine have been open-sourced.

</details>


### [41] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis,Fernando A. Kuipers,George Iosifidis*

Main category: cs.LG

TL;DR: O-RAN范式下，利用元学习优化条件切换(CHO)，解决其资源分配和信令开销挑战，并在波动信号条件下性能显著优于传统基准。


<details>
  <summary>Details</summary>
Motivation: 传统切换(HOs)在复杂移动网络中面临高延迟和高失败率。尽管3GPP引入了条件切换(CHOs)以提升成功率和减少延迟，但CHO自身引入了资源高效分配和信令/通信开销管理的新挑战。

Method: 论文提出一个与O-RAN范式对齐的新颖框架，该框架利用元学习（meta-learning）技术进行CHO优化。

Result: 该框架提供了鲁棒的动态遗憾（dynamic regret）保证，并在波动信号条件下，性能比其他3GPP基准提高了至少180%。

Conclusion: 提出的基于元学习的O-RAN框架能够有效优化条件切换，显著提升了移动网络在复杂环境下的切换性能和可靠性，为未来网络提供了高效的解决方案。

Abstract: Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [42] [An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs](https://arxiv.org/abs/2507.07146)
*Zixuan Huang,Kecheng Huang,Lihao Yin,Bowei He,Huiling Zhen,Mingxuan Yuan,Zili Shao*

Main category: cs.LG

TL;DR: 本文提出G-Guard，一种基于GNN的注意力感知输入分类器，用于防御大型语言模型（LLMs）的多轮越狱攻击，并在所有数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管经过严格的安全训练，但仍易受越狱攻击。特别是，多轮攻击因其渐进式升级对话的特性，比单轮攻击更难检测和缓解，这促使了对有效防御机制的需求。

Method: 本研究提出了G-Guard，一种注意力感知GNN（图神经网络）输入分类器。G-Guard为多轮查询构建实体图，以捕捉有害关键词与查询之间的关系，即使关键词出现在早期轮次。此外，它引入了一个注意力感知增强机制，根据多轮对话检索最相似的单轮查询，并将其作为图中的标签节点，以增强GNN的分类能力。

Result: 评估结果表明，G-Guard在所有数据集和评估指标上都优于所有基线模型。

Conclusion: G-Guard被证明是一种针对LLMs多轮越狱攻击的有效防御机制，其性能优于现有方法，为提升LLMs安全性提供了新的解决方案。

Abstract: Large Language Models (LLMs) have gained widespread popularity and are
increasingly integrated into various applications. However, their capabilities
can be exploited for both benign and harmful purposes. Despite rigorous
training and fine-tuning for safety, LLMs remain vulnerable to jailbreak
attacks. Recently, multi-turn attacks have emerged, exacerbating the issue.
Unlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,
making them more difficult to detect and mitigate, even after they are
identified.
  In this study, we propose G-Guard, an innovative attention-aware GNN-based
input classifier designed to defend against multi-turn jailbreak attacks on
LLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly
capturing relationships between harmful keywords and queries even when those
keywords appear only in previous queries. Additionally, we introduce an
attention-aware augmentation mechanism that retrieves the most similar
single-turn query based on the multi-turn conversation. This retrieved query is
treated as a labeled node in the graph, enhancing the ability of GNN to
classify whether the current query is harmful. Evaluation results demonstrate
that G-Guard outperforms all baselines across all datasets and evaluation
metrics.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [43] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 该研究提出了一种基于随机近端梯度混合整数双层优化（SPG-MIBO）的新算法，用于联合建模和优化MIMO-OFDM系统中高维信道状态信息（CSI）下的无线定位和感知任务，并验证了其有效性和性能增益。


<details>
  <summary>Details</summary>
Motivation: 尽管无线定位和感知技术在现代无线网络中至关重要，且CSI与深度学习结合被视为有前景的解决方案，但目前对于MIMO-OFDM系统高维CSI特性下定位与感知的联合建模和优化研究尚不充分。

Method: 本研究将定位和感知任务公式化为一个混合整数双层深度学习问题，并提出了一种新颖的基于随机近端梯度的混合整数双层优化（SPG-MIBO）算法。该算法适用于高维和大规模数据集，通过小批量训练提高计算和内存效率，并提供理论收敛保证。

Result: 在多个数据集上进行的广泛实验验证了所提SPG-MIBO算法的有效性，并突出了联合定位和感知优化所带来的性能提升。

Conclusion: 联合优化定位和感知任务能够有效提升性能。所提出的SPG-MIBO算法为解决MIMO-OFDM系统中高维CSI下的联合定位和感知问题提供了一个有前景的解决方案。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [44] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: DAF通过系统级优化，解决了移动/边缘设备上深度学习模型训练中激活值压缩的内存和速度问题。


<details>
  <summary>Details</summary>
Motivation: 移动和边缘设备上的深度神经网络训练受内存限制，激活值占用大量内存。现有动态激活量化方法因计算开销和内存碎片化等系统级挑战难以实际部署，亟需高效且准确的压缩方案。

Method: 本文提出DAF（动态激活框架），通过系统级优化实现内存和时间高效的动态量化训练。具体方法包括：开发适用于移动/边缘SoC内存层次的混合归约操作、利用CPU-GPU协同位打包、以及实现重要性感知的分页内存管理方案以减少碎片并支持动态内存调整。

Result: DAF在不损害模型训练精度的情况下，实现了显著的内存节省和加速。在嵌入式和移动平台上的评估显示，内存使用量减少高达22.9倍，速度提升3.2倍。

Conclusion: DAF为资源受限环境下的深度学习模型训练提供了一种可扩展且实用的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [45] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 为解决LEO卫星网络高延迟切换问题，提出一种基于计划的并行切换机制，通过引入卫星同步功能（SSF）及结合机器学习预测和调度，将切换延迟降低21倍，显著提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）卫星的高速移动导致地面终端频繁且高延迟的切换，严重影响对延迟敏感的应用性能，是未来5G/6G移动网络中的关键挑战。

Method: 提出一种并行切换机制，其核心思想是采用基于计划的切换而非基于测量的切换，以避免接入网和核心网之间的交互。具体实现包括：1) 引入一种与标准5G核心网兼容的卫星同步功能（SSF）；2) 提出一个用于信号强度预测的机器学习模型；3) 设计一个高效的切换调度算法。

Result: 广泛的实验结果表明，所提出的切换方案与标准NTN切换方案以及其他两种现有切换方法相比，能将切换延迟降低21倍，并显著提升网络稳定性和用户级性能。

Conclusion: 所提出的并行切换机制能够有效解决LEO卫星网络中的高延迟切换问题，显著改善网络性能和用户体验，为未来移动卫星网络提供了可行的解决方案。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [46] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出一种无人机辅助的无线能量传输与数据收集框架，用于恶劣环境下的无电池传感器网络，并开发了一种强化的强化学习算法（SAC-PPV）来优化传输与飞行路径，实现高效数据收集和低能耗。


<details>
  <summary>Details</summary>
Motivation: 在高温等难以进入的恶劣环境中，传统固定WPT基础设施安装困难，且电池寿命短、易失效，严重限制了物联网传感应用部署。

Method: 提出无人机辅助的无电池传感器（BLS）网络数据收集和WPT框架。无人机首先通过WPT为BLS节点供能，节点随后通过OFDMA向无人机传输数据。制定一个多目标优化问题，旨在通过联合优化发射功率分配和飞行轨迹规划，最大化公平数据收集量同时最小化无人机能耗。为解决该非凸、动态问题，提出一种增强型Soft Actor-Critic算法（SAC-PPV），该算法结合了无参数注意力、优先经验回放和基于价值的奖励中心化，以提高探索效率和学习稳定性。

Result: 仿真结果表明，所提出的SAC-PPV方法在各种网络配置下，性能始终优于现有基准算法。

Conclusion: 本研究成功提出了一种无人机辅助的无线能量传输与数据收集框架及相应的SAC-PPV优化算法，有效解决了恶劣环境下无电池传感器网络的供能和数据传输挑战，并通过仿真验证了其优越性能。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [47] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 该论文提出了Adaptive Bilevel Search (ABS)框架，用于解决计算算力网络(CPN)中的服务映射难题，显著提高了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 计算算力网络(CPN)旨在通过网络控制统一计算资源，但当前方法未能充分集成资源，尤其在最大化资源效率和服务满意度方面，服务到基础设施的优化映射仍具挑战。

Method: 研究者首先正式定义了CPN中的服务映射问题，证明了其理论上的难解性，并识别了实际优化挑战。随后提出了模块化的Adaptive Bilevel Search (ABS)框架，其核心包括基于图划分的重构、双层优化架构以及碎片感知评估，并利用分布式粒子群优化进行实现和广泛评估。

Result: ABS在多种CPN场景下持续优于现有方法。在复杂场景中，相较于最佳基线，ABS将计算资源利用率提高了73.2%，服务接受率提高了60.2%。

Conclusion: Adaptive Bilevel Search (ABS)框架有效地解决了CPN中的服务映射问题，大幅提升了资源利用效率和服务交付能力，验证了其优越性。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [48] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文实验分析了邻近Wi-Fi网络信道重叠对Wi-Fi上VR流媒体性能的负面影响，并验证了所提出NeSt-VR算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究邻近Wi-Fi网络信道重叠（OBSS）引起的竞争对VR流媒体性能的负面影响，尤其是在80 MHz信道的部分和完全重叠场景下，因为VR流媒体对网络延迟和带宽高度敏感。

Method: 采用实验分析方法，在Wi-Fi网络环境下，测试不同数量、不同信道位置（主/次级40 MHz）和不同负载类型（对称/不对称）的OBSS对VR流媒体性能的影响。

Result: ['增加80 MHz OBSSs数量会加剧竞争，导致VR流媒体性能下降。', 'OBSS活动在次级40 MHz信道部分比在主级40 MHz部分对VR流媒体性能的损害更大。', '在相同总负载下，两个40 MHz OBSS竞争者的完全信道重叠，相比单个高负载40 MHz竞争者的部分重叠影响较小，但比两个80 MHz竞争者的完全重叠更具破坏性。', '在对称流量负载下，两个40 MHz OBSS竞争者的完全信道重叠对VR流媒体的影响小于非对称负载。', '研究团队先前提出的网络感知阶梯式自适应比特率VR流媒体算法（NeSt-VR）能有效缓解OBSS环境下的性能下降，支持在更重的OBSS流量条件下进行VR流媒体传输。']

Conclusion: 邻近Wi-Fi网络的信道重叠会严重影响VR流媒体性能，但通过引入NeSt-VR等网络感知自适应比特率算法，可以有效缓解这些性能下降，从而在复杂的OBSS环境下实现可行的VR流媒体体验。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


### [49] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: 本研究提出HaLert，一种基于Wi-Fi HaLow和LoRa的弹性架构，旨在利用智慧城市现有物联网基础设施，为灾后提供可靠的应急通信系统，并在真实城市环境中验证了其稳定性和功能性。


<details>
  <summary>Details</summary>
Motivation: 灾害通常不可预测，会严重影响民众通信和接收政府警报的能力。因此，利用现有基础设施开发替代通信策略至关重要。智慧城市中密集的物联网网络为此类重用提供了巨大潜力。

Method: 本研究提出了HaLert架构，它基于Wi-Fi HaLow IEEE 802.11s网状网络，可快速重新分配资源以支持公民与当局之间的应急通信（包括文本、位置、图像、音频和视频）。为便于远程监控和配置，该架构结合了SDN（软件定义网络）范式，并通过LoRa控制泛洪网状网络提供支持。研究团队基于此架构开发了原型，并在真实城市（包括室内和室外环境）中进行了测试。

Result: 尽管障碍物、非视距和地形坡度对Wi-Fi HaLow网络的延迟（平均15至54.8毫秒）和吞吐量（上传134至726 Kbps，下载117至682 Kbps）有显著影响，但网络保持稳定和弹性，成功提供了HaLert架构的所有功能。LoRa网络测试显示平均消息成功率高达94.96%。

Conclusion: HaLert架构被证明是一种稳定且弹性的应急通信系统，能够利用Wi-Fi HaLow和LoRa技术在复杂真实的城市环境中实现关键通信功能，有效应对灾害对通信的影响。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析分布式机器学习中集体通信的网络行为，发现其可导致拥塞，并利用Nvidia NCCL日志对多种模型进行深度分析，建议重新设计通信框架和网络拓扑以应对网络异常。


<details>
  <summary>Details</summary>
Motivation: 在分布式高性能系统上运行的机器学习任务中，集体通信操作会产生高带宽和突发流量模式，易导致网络拥塞和丢包，从而影响任务性能。因此，分析这些模式对于根据机器学习工作负载类型配置网络资源至关重要。

Method: 对多种机器学习模型（如DeepSeek、GPT、Llama等）的集体通信行为进行了广泛分析。通过利用Nvidia集体通信库（NCCL）的日志功能获取丰富的集体通信和工作负载上下文信息。同时，调整了影响集体通信行为的配置参数，如并行度、节点数量和模型类型。

Result: 本概述展示并讨论了开源DeepSeek V3推理模型的集体通信行为结果，包括操作类型和计数、每次操作的传输大小以及请求大小分布。

Conclusion: 分析表明，有必要重新思考当前的集体通信框架和网络拓扑，以适应网络异常对所述工作负载的影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>
