<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出指令引导内容选择（IGCS）作为统一框架，并引入首个统一基准IGCSBench和大规模合成数据集，旨在解决NLP内容选择任务的孤立研究问题。


<details>
  <summary>Details</summary>
Motivation: 尽管许多NLP任务都涉及从文本中选择相关内容，但这些“内容选择”任务传统上是孤立研究的，各有独立的建模方法、数据集和评估指标，缺乏统一性。

Method: 1. 提出“指令引导内容选择（IGCS）”作为统一框架，将任务定义和请求封装为语言模型的指令。2. 创建首个统一的基准测试集IGCSBench，涵盖多种内容选择任务。3. 构建一个大型通用合成数据集。4. 利用这些数据集进行迁移学习。5. 解决基于LLM的内容选择中出现的通用推理时间问题。6. 评估一个通用评估指标。

Result: 1. 通过统一框架IGCS，有效整合了内容选择任务。2. 使用所创建的数据集进行迁移学习，通常能提升性能，无论目标任务是否有专门的训练数据。3. 解决了LLM内容选择的推理时间问题，并评估了通用评估指标。

Conclusion: 本文提出的资源（IGCSBench、合成数据集）和方法（IGCS框架、推理优化、通用评估）对未来内容选择模型具有实用价值。

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: 在真实诊疗环境中评估了基于大型语言模型的临床决策支持工具（AI Consult）的影响，发现其能显著减少诊断和治疗错误，并提高医生满意度。


<details>
  <summary>Details</summary>
Motivation: 评估基于大型语言模型的临床决策支持工具在真实临床环境中减少医疗错误和提高护理质量的潜力。

Method: 开展了一项质量改进研究，比较了肯尼亚15家诊所中39,849次患者就诊的结果。研究通过独立医生评估有或没有AI Consult工具辅助的医生所犯的临床错误。AI Consult旨在识别潜在的文档和临床决策错误。同时进行了医生调查。

Result: 获得AI Consult辅助的医生诊断错误减少了16%，治疗错误减少了13%。这相当于Penda诊所每年可避免22,000次诊断错误和29,000次治疗错误。所有受访医生都表示AI Consult提高了他们提供的护理质量，其中75%认为效果“显著”。

Conclusion: 基于大型语言模型的临床决策支持工具（如AI Consult）在有效融入临床工作流程时，能够显著减少真实环境中的医疗错误。本研究为负责任地推广此类工具以提升护理质量提供了实用框架。

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: 本文提出SALU模型，将无法回答问题检测直接集成到大型语言模型（LLM）的生成过程中，通过多任务学习和RLHF有效减少了对话式信息检索（CIR）系统中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 对话式信息检索系统难以可靠处理无法回答的问题，导致生成误导性或幻觉内容；传统外部分类器与核心LLM存在不一致性。

Method: 引入自感知LLM处理无法回答问题（SALU）方法。使用多任务学习框架训练模型，同时进行标准问答和对无法回答问题的明确拒绝生成。通过置信度分数引导的强化学习与人类反馈（RLHF）阶段，惩罚幻觉响应并奖励适当拒绝，以培养LLM的内在知识边界自感知能力。在自定义的C-IR_Answerability数据集上进行实验。

Result: SALU在正确回答或拒绝回答问题方面的整体准确性持续优于包括混合LLM-分类器系统在内的强基线模型。人工评估进一步证实SALU在事实性、适当拒绝和大幅减少幻觉方面具有卓越的可靠性。

Conclusion: SALU模型能够可靠地“知道何时说‘我不知道’”，显著增强了CIR系统的可靠性，有效防止了误导性内容的生成，展示了其对知识边界的内在自感知能力。

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: mKGQAgent是一个模仿人类思维的LLM代理框架，通过模块化子任务将多语言自然语言问题转换为SPARQL查询，并在Text2SPARQL挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 通过多语言自然语言接口访问知识图谱中的结构化知识是一项新兴挑战。由于知识图谱使用特定查询语言（如SPARQL），需要将自然语言输入转换为查询以满足信息需求。

Method: 本文提出了mKGQAgent框架，它受人类思维启发，将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务。通过协调的LLM代理工作流（包括规划、实体链接和查询优化），并结合经验池进行上下文学习，mKGQAgent能高效处理多语言知识图谱问答。

Result: 在Text2SPARQL挑战赛2025的DBpedia和企业知识图谱问答基准测试中，mKGQAgent的表现优于其他所有参与者，取得了第一名。

Conclusion: 这项工作为在多语言语义解析领域开发类人推理系统开辟了新的研究方向。

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: 本研究通过生成多语言合成农业数据集并对特定语言的LLM进行微调，显著提升了其在农业问答中的准确性和本地化能力，尤其适用于多语言和低资源环境。


<details>
  <summary>Details</summary>
Motivation: 农民及时获取准确的本地化农业信息至关重要。现有通用大型语言模型（LLMs）在农业领域缺乏领域特定训练和高质量数据集，导致其提供的农业建议过于笼统，无法满足多语言和本地化语境的需求。

Method: 从农业特定文档中生成多语言（英语、印地语、旁遮普语）合成农业数据集，并利用这些数据集对特定语言的LLMs进行微调。

Result: 与基线模型相比，经过微调的模型在事实准确性、相关性和农业共识方面均表现出显著改进。

Conclusion: 合成数据驱动的、特定语言的微调是提高LLMs在农业领域性能的有效策略，尤其适用于多语言和低资源环境，有助于为不同语言社区提供更准确和本地化的农业咨询服务，弥合知识鸿沟。

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）即使在没有明确人口学标记的情况下也存在对特定国籍的潜在偏见。本研究引入一种基于名称的基准测试方法，发现小型模型比大型模型准确性更低且偏见更大，凸显了偏见的顽固性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在更贴近真实应用的场景中（即使用文化指示性名称而非明确国籍标签时），如何展现和受影响于对特定国籍的潜在偏见。

Method: 基于Bias Benchmark for QA (BBQ) 数据集，提出一种新颖的、基于名称的基准测试方法，用文化指示性名称替代显式国籍标签，以评估OpenAI、Google和Anthropic等主流LLM的偏见程度和准确性。

Result: 研究发现小型模型比大型模型准确性更低，偏见更严重。例如，在模糊语境下，Claude Haiku的刻板偏见得分高达9%，而Claude Sonnet仅为3.5%，后者准确性高出117.7%。此外，小型模型在模糊语境中保留了更高比例的既有错误（如GPT-4o-mini保留76%错误率，GPT-4o保留68%）。

Conclusion: LLMs中的偏见表现出顽固的弹性，这对AI系统在全球化背景下的开发和部署具有深刻影响。

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: 本研究利用生成式大语言模型（LLMs）对精神病学电子健康记录（EHRs）中的自杀相关因素（SrFs）进行多标签分类，实现了高精度识别，并揭示了模型行为模式，为临床数据结构化提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球性的健康危机，早期识别相关风险因素至关重要。现有AI研究多将自杀性视为二元分类任务，忽视了多种风险因素并存的复杂性，无法提供全面识别。

Method: 探索使用生成式大语言模型（GPT-3.5和GPT-4.5）进行多标签分类（MLC），从精神病学EHR中识别SrFs。提出了一个端到端的生成式MLC流程，并引入了包括标签集级别指标和多标签混淆矩阵在内的先进评估方法。

Result: 微调后的GPT-3.5表现最佳，部分匹配准确率为0.94，F1分数为0.91。GPT-4.5在引导式提示下在各标签集（包括稀有标签集）上表现更优，显示出更平衡和稳健的性能。研究还揭示了系统性错误模式（如自杀意念与自杀企图的混淆）以及模型谨慎的过度标注倾向。

Conclusion: 本工作不仅证明了生成式AI在复杂临床分类任务中的可行性，还为构建非结构化EHR数据以支持大规模临床研究和循证医学提供了蓝图。

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 本文提出一种利用工具（如网页搜索和代码执行）增强的AI代理系统，以提高在长篇事实、数学和代码等挑战性领域的大语言模型（LLM）响应的评估质量。研究表明工具增强在多数情况下能提升性能，但也存在局限性和对参数的敏感性，并强调了开发更优质基准测试的重要性。


<details>
  <summary>Details</summary>
Motivation: LLM响应的成对偏好评估对于模型评价和反馈至关重要，尤其是在难以获得硬性指标的领域（如聊天响应质量）。然而，在某些特定领域（如长篇事实、数学和代码），无论是人类还是AI标注者，都难以提供高质量的成对比较，因为他们可能过分侧重于写作质量而非底层事实准确性。因此，需要改进AI标注系统以提升在这些复杂领域的表现。

Method: 作者提出了一种工具增强的代理系统来改进标准AI标注器。该系统利用外部工具，如网页搜索和代码执行，进行基于外部验证的判断，从而避免依赖LLM的内部知识和偏见。研究在三个目标响应领域（长篇事实、数学和代码）以及通用标注任务上，通过RewardBench（包括AlpacaEval和LLMBar）、RewardMath以及为饱和数据集创建的三个新数据集进行了广泛的实验评估。

Result: 实验结果表明，在许多情况下，外部工具确实可以提高性能，但并非所有情况都如此。更普遍的是，实验突出显示了性能对简单参数（例如，提示词）的敏感性。

Conclusion: 外部工具可以有效提升AI标注器在复杂领域的性能，但其效果并非普适，且易受参数设置影响。研究强调了对改进（非饱和）标注器基准测试的迫切需求。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: 该研究提出一种基于坐标搜索的优化框架，为NLP嵌入的每个特征寻找最佳二值化阈值，以生成高效且准确的二进制表示（条形码），性能优于传统二值化方法。


<details>
  <summary>Details</summary>
Motivation: 大规模自然语言处理（NLP）应用对文本嵌入的存储和计算效率有很高要求。将连续嵌入转换为二进制表示是一种解决方案，但现有固定阈值二值化方法效果不佳。

Method: 提出了一种基于坐标搜索（Coordinate Search）的优化框架。该框架能够为每个特征识别最佳的二值化阈值，从而将连续的NLP嵌入转换为二进制表示（条形码），以替代传统的固定阈值二值化方法。

Result: 通过该方法生成的二进制嵌入在精度方面优于传统的二值化方法。在不同的NLP任务和数据集上进行了广泛的实验和统计测试，证明了其优越性。

Conclusion: 该技术能够生成准确且高效的二进制表示，在NLP应用中具有显著潜力。同时，该二值化技术具有通用性，可应用于机器学习领域中任何类型的特征，不仅仅局限于NLP嵌入。

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: 提出CogDual，一种基于“认知-响应”范式的角色扮演语言智能体（RPLA），通过建模内外部感知并结合强化学习，提升了角色一致性和上下文对齐，超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演语言智能体（RPLA）多依赖提示工程或监督微调，但忽视了驱动角色行为的底层认知机制，导致其在特定情境下角色行为的模仿能力存在局限。

Method: 受认知心理学启发，本文引入CogDual，一种采用“认知-响应”推理范式的新型RPLA。通过联合建模外部情境感知和内部自我感知来生成回应。此外，采用强化学习并设计了两种通用奖励方案来优化模型在开放域文本生成中的性能。

Result: 在CoSER基准测试以及Cross-MR和LifeChoice数据集上的大量实验表明，CogDual持续优于现有基线模型，并能在各种角色扮演任务中有效泛化。

Conclusion: CogDual通过整合认知机制和强化学习，显著提升了RPLA的角色一致性和上下文对齐能力，为多样的角色扮演任务提供了一个更优且更具泛化性的解决方案。

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 研究提出了SKA-Bench，一个综合且严格的结构化知识理解基准，用于评估大型语言模型（LLMs）对KG、Table、KG+Text和Table+Text的理解能力，并发现现有LLMs在此方面仍面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型结构化知识理解的评估不够严格（缺乏特定能力评估）且仅关注单一类型，因此需要一个更全面、更严格的基准来诊断LLMs的不足。

Method: 引入SKA-Bench，一个结构化知识增强问答基准，涵盖KG、Table、KG+Text和Table+Text四种形式。通过三阶段流程构建实例，包含问题、答案、正向和噪声知识单元。将实例扩展为噪声鲁棒性、顺序不敏感性、信息整合和负面拒绝四种基本能力测试平台，以细粒度评估LLMs。

Result: 对8个代表性LLMs（包括DeepSeek-R1）的实证评估显示，现有LLMs在理解结构化知识方面仍面临显著挑战，其性能受噪声量、知识单元顺序和幻觉现象等因素影响。

Conclusion: 现有大型语言模型在结构化知识理解方面存在显著不足，亟需在噪声鲁棒性、顺序不敏感性、信息整合和负面信息拒绝等核心能力上进行改进。

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: 引入金融领域AI智能体基准FinGAIA，评估主流智能体发现其性能远低于金融专家，并揭示了核心失败模式。


<details>
  <summary>Details</summary>
Motivation: AI智能体在自动化复杂任务方面潜力巨大，但其在金融领域的多步骤、多工具协作能力尚未得到充分探索。

Method: 提出FinGAIA，一个包含407个任务、覆盖七大金融子领域和三层场景深度的端到端基准，用于评估AI智能体的金融实践能力。在零样本设置下评估了10个主流AI智能体。

Result: 表现最佳的ChatGPT准确率为48.9%，虽优于非专业人士，但仍比金融专家低35个百分点以上。错误分析揭示了五种常见失败模式，如跨模态对齐缺陷和金融术语偏差。

Conclusion: FinGAIA是首个金融领域AI智能体基准，旨在客观评估并促进该领域AI智能体的发展，并为未来研究提供了关键方向。

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在提供道德建议时与人类存在“多元化道德鸿沟”，即判断分布和价值观多样性上的不匹配，并提出一种名为DMP的方法来改善对齐度。


<details>
  <summary>Details</summary>
Motivation: 人们日益依赖大型语言模型（LLMs）获取道德建议，这可能影响人类决策。然而，目前对LLMs与人类道德判断的对齐程度知之甚少。

Method: 引入“道德困境数据集”（包含1,618个真实世界道德困境及其人类判断分布和自由文本理由）。将问题视为“多元化分布对齐任务”，比较LLM与人类判断的分布。利用从理由中提取的3,783个价值观表达式构建60个价值观分类体系，分析LLM所依赖的道德价值观集合。提出“动态道德画像（DMP）”，这是一种基于Dirichlet的采样方法，用于根据人类衍生的价值观画像调整模型输出。

Result: 模型仅在人类共识度高时才能复现人类判断，而当人类分歧增加时对齐度急剧下降。LLMs依赖的道德价值观集合比人类更窄。这些发现揭示了“多元化道德鸿沟”：在判断分布和表达价值观的多样性上均存在不匹配。DMP方法将对齐度提高了64.3%，并增强了价值观多样性。

Conclusion: LLMs在提供道德建议时与人类判断存在多元化道德鸿沟（即判断分布和价值观多样性上的不匹配）。提出的DMP方法为LLMs提供更具多元化且更符合人类的道德指导迈出了一步。

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID是一种新型框架，通过模仿专家工作流并融合多视图信息，优化诊断准确性以自动生成临床可靠的放射报告，并在MIMIC-CXR数据集上取得了显著优于现有基线的效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成放射报告的方法难以提供临床可靠的结论，主要问题在于：1) 侧重文本流畅性而非事实正确性；2) 依赖单视图图像限制诊断全面性。因此需要一种能确保诊断准确性的新方法来减轻放射科医生的工作量。

Method: 提出CLARIFID框架，模仿专家“发现”到“印象”的两步工作流以优化诊断正确性。具体方法包括：1) 通过章节感知预训练学习“发现”到“印象”的逻辑流；2) 使用近端策略优化(PPO)进行微调，奖励函数为“印象”部分的CheXbert F1分数；3) 强制执行推理感知解码，确保先完成“发现”再合成“印象”；4) 通过基于Vision Transformer的多视图编码器融合多张胸部X射线视图。推理时，采用推理感知下一词强制策略和报告级重排序，以保证临床推理的连贯性。

Result: 在MIMIC-CXR数据集上的实验结果表明，CLARIFID方法在临床有效性方面表现优越，并且在标准自然语言生成(NLG)指标和临床感知分数上均优于现有基线方法。

Conclusion: CLARIFID框架能有效生成具有更高诊断正确性和临床可靠性的放射报告，成功解决了现有方法在事实正确性和诊断全面性方面的不足，从而有望减轻放射科医生的工作负担。

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Triple X的多语言对话语音识别系统，该系统采用创新的编码器-适配器-LLM架构，并在挑战赛中取得了第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 优化多语言对话场景下的语音识别准确率。

Method: 提出了一种创新的编码器-适配器-LLM架构，该架构利用文本大型语言模型的推理能力并融入领域特定适配。此外，还采用了精心设计的多阶段训练策略，利用了大量的多语言音频数据集。

Result: 在开发集和测试集上均取得了有竞争力的词错误率（WER）表现，并在MLC-SLM挑战赛中获得了第二名。

Conclusion: Triple X系统有效提升了多语言对话语音识别的准确性，并在竞争性环境中展现出卓越的性能。

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [16] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文旨在通过在SIGIR 2025 LiveRAG挑战赛中应用先进的图增强检索生成（RAG）方案GeAR，探究其在更广泛数据集上的通用性与局限性，以弥补现有方法任务特异性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法通常针对特定任务（如多跳问答）设计，缺乏在更广泛数据集上通用适用性的证据。

Method: 调整并应用一种先进的图增强RAG解决方案：GeAR。

Result: 探索GeAR在SIGIR 2025 LiveRAG挑战赛中的性能和局限性。

Conclusion: 通过实验，评估GeAR的通用适用性，并识别其在应对更广泛数据集挑战时的优势和不足。

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 本研究通过回归分析，量化了情感、叙事和对冲等主观因素对论证强度（包括客观论证质量和主观说服力）的影响，并评估了其自动化标注方法。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域日益重视主观性，但当前缺乏对主观特征（如个人故事、情感）与论证强度之间关系的大规模分析，存在研究空白。

Method: 对两个标注有客观论证质量和主观说服力的标准数据集进行了回归分析，以量化情感、叙事和对冲这三类主观因素的影响。同时，比较并评估了这些主观特征的自动化标注方法，以解决现有数据集中缺乏统一标注的问题。

Result: 研究发现，叙事和对冲对客观和主观论证质量具有截然不同的影响；而情感的影响则取决于其修辞运用，而非特定领域。

Conclusion: 主观特征对论证强度的不同维度（客观质量和主观说服力）具有不同的影响模式，揭示了主观因素在论证评估中的复杂且重要的作用。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: 本文提出并评估了两种结合多种嵌入模型以增强RAG性能的方法：Mixture-Embedding RAG和Confident RAG。其中，Confident RAG通过选择置信度最高的响应，显著优于传统的RAG和LLM。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，如何有效整合最新信息或外部知识成为关键。RAG作为一种低成本方法受到关注，但不同嵌入模型由于数据和架构异构性，导致相似度计算和LLM响应质量不一致。研究动机在于解决RAG中单一嵌入模型带来的性能波动和次优表现。

Method: 本文提出了两种增强RAG的方法：1. Mixture-Embedding RAG：基于标准化相似度对来自多个嵌入模型的检索结果进行排序和选择。2. Confident RAG：利用不同的嵌入模型多次生成响应，然后选择置信度最高的响应。研究在不同LLM和嵌入模型上进行了实验验证。

Result: Mixture-Embedding RAG并未超越传统的RAG。而Confident RAG则平均比传统LLMs和RAG分别提高了约10%和5%。这些结果在不同LLMs和嵌入模型间保持一致。

Conclusion: Confident RAG是一种高效、即插即用的方法，适用于各种领域，能够通过结合多个嵌入模型的优势来显著提升RAG的性能和响应质量。

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: 现有大型语言模型（LLMs）在原生多语言推理方面表现不佳，本研究引入了新的多语言基准MultiNRC，并发现LLMs在非英语文化相关推理上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言推理基准主要通过翻译英语问题构建，偏向英语语境，限制了对LLMs在多样语言和文化背景下真实推理能力的评估。

Method: 引入MultiNRC基准，包含1000多个由母语者在法语、西班牙语和中文中撰写的原生、语言和文化相关推理问题。该基准涵盖语言特有语言推理、文字游戏与谜语、文化/传统推理、文化相关数学推理四类。为部分问题提供英文翻译版以进行直接比较。系统评估了14个主流LLMs。

Result: 1) 当前LLMs在原生多语言推理方面仍不佳，MultiNRC得分均未超过50%；2) LLMs在处理语言、文化和逻辑推理任务时表现出不同的优劣势；3) 大多数模型在英语数学推理中表现显著优于原始语言（+10%），表明在文化相关知识方面持续存在挑战。

Conclusion: 当前LLMs在原生多语言推理，特别是涉及文化背景知识时，仍面临显著挑战，尤其是在非英语环境中表现有待提高。

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [20] [Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery](https://arxiv.org/abs/2507.16849)
*Yi-Shan Chu,Hsuan-Cheng Wei*

Main category: cs.CV

TL;DR: 提出一个基于Vision Transformer（ViT）的深度学习框架，通过结合弱监督训练与遥感影像，提高灾区分割的平滑度和可靠性，旨在支持并增强台湾太空机构（TASA）的紧急增值产品（EVAP）。


<details>
  <summary>Details</summary>
Motivation: 支持和增强台湾太空机构（TASA）开发的紧急增值产品（EVAP），通过遥感影像提升灾害影响区域的分割精度。

Method: 首先，利用少量人工标注区域，通过主成分分析（PCA）和置信度指数（CI）扩展标签，构建弱监督训练集。随后，使用此训练集训练基于ViT的编解码器模型，输入来自Sentinel-2和Formosat-5的多波段影像，并采用多解码器变体和多阶段损失策略。最后，通过与更高分辨率的EVAP输出进行比较来评估模型性能。

Result: 在2022年鄱阳湖干旱和2023年罗德岛野火的案例研究中，所提出的框架显著改善了分割结果的平滑度和可靠性。

Conclusion: 该框架提供了一种在缺乏准确地面真实数据时，可用于灾害测绘的可扩展方法，有效提高了灾区分割的质量。

Abstract: We propose a vision transformer (ViT)-based deep learning framework to refine
disaster-affected area segmentation from remote sensing imagery, aiming to
support and enhance the Emergent Value Added Product (EVAP) developed by the
Taiwan Space Agency (TASA). The process starts with a small set of manually
annotated regions. We then apply principal component analysis (PCA)-based
feature space analysis and construct a confidence index (CI) to expand these
labels, producing a weakly supervised training set. These expanded labels are
then used to train ViT-based encoder-decoder models with multi-band inputs from
Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder
variants and multi-stage loss strategies to improve performance under limited
supervision. During the evaluation, model predictions are compared with
higher-resolution EVAP output to assess spatial coherence and segmentation
consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes
wildfire demonstrate that our framework improves the smoothness and reliability
of segmentation results, offering a scalable approach for disaster mapping when
accurate ground truth is unavailable.

</details>


### [21] [Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors](https://arxiv.org/abs/2507.16850)
*Mohamed Adjel*

Main category: cs.CV

TL;DR: 该论文提出一种单目3D人体姿态估计框架，结合实时2D关键点检测和几何感知的2D到3D提升技术，利用相机内参和解剖学先验知识，旨在实现快速、个性化且精确的姿态估计，无需专用硬件。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿态估计，尤其是在实时和非受限环境下，仍是一个具有挑战性的不适定问题。直接的图像到3D方法需要大量标注数据和重型模型，而结合先验知识的2D到3D提升方法则提供了一种更轻量级和灵活的替代方案。

Method: 该研究提出一个框架，将实时2D关键点检测与几何感知的2D到3D提升相结合，明确利用已知的相机内参和主体特定的解剖学先验知识。其方法基于自校准和生物力学约束的逆运动学，从MoCap和合成数据集中生成大规模、合理的2D-3D训练对。

Result: 该方法能够实现从单目图像中进行快速、个性化和准确的3D姿态估计，且无需专用硬件。它通过结合数据驱动学习和模型化先验知识来提升效果。

Conclusion: 该提案旨在促进关于数据驱动学习与基于模型的先验知识融合的讨论，以提高3D人体运动捕捉的准确性、可解释性，并增强其在野外边缘设备上的部署能力。

Abstract: Monocular 3D human pose estimation remains a challenging and ill-posed
problem, particularly in real-time settings and unconstrained environments.
While direct imageto-3D approaches require large annotated datasets and heavy
models, 2D-to-3D lifting offers a more lightweight and flexible
alternative-especially when enhanced with prior knowledge. In this work, we
propose a framework that combines real-time 2D keypoint detection with
geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics
and subject-specific anatomical priors. Our approach builds on recent advances
in self-calibration and biomechanically-constrained inverse kinematics to
generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic
datasets. We discuss how these ingredients can enable fast, personalized, and
accurate 3D pose estimation from monocular images without requiring specialized
hardware. This proposal aims to foster discussion on bridging data-driven
learning and model-based priors to improve accuracy, interpretability, and
deployability of 3D human motion capture on edge devices in the wild.

</details>


### [22] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

TL;DR: CrackCue是一种利用裂缝细结构特性生成鲁棒裂缝线索的方法，显著提升了深度学习裂缝检测模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习裂缝检测方法泛化能力不足，难以适应未见领域，且常忽视裂缝的细结构特性。

Method: 提出CrackCue，通过粗到细的线索生成方法。首先通过最大池化和上采样操作获取粗糙无裂缝背景，再通过重建网络得到精细无裂缝背景。原始图像与精细无裂缝背景的差异即为精细裂缝线索，该线索对复杂背景、阴影和光照变化具有鲁棒性。CrackCue可作为即插即用模块集成到现有裂缝检测网络中。

Result: 广泛的实验结果表明，CrackCue显著提高了基线方法的泛化能力和鲁棒性。

Conclusion: CrackCue通过生成不受复杂环境影响的鲁棒裂缝线索，有效解决了深度学习裂缝检测在泛化性和鲁棒性方面的挑战。

Abstract: Crack detection is an important task in computer vision. Despite impressive
in-dataset performance, deep learning-based methods still struggle in
generalizing to unseen domains. The thin structure property of cracks is
usually overlooked by previous methods. In this work, we introduce CrackCue, a
novel method for robust crack detection based on coarse-to-fine crack cue
generation. The core concept lies on leveraging the thin structure property to
generate a robust crack cue, guiding the crack detection. Specifically, we
first employ a simple max-pooling and upsampling operation on the crack image.
This results in a coarse crack-free background, based on which a fine
crack-free background can be obtained via a reconstruction network. The
difference between the original image and fine crack-free background provides a
fine crack cue. This fine cue embeds robust crack prior information which is
unaffected by complex backgrounds, shadow, and varied lighting. As a
plug-and-play method, we incorporate the proposed CrackCue into three advanced
crack detection networks. Extensive experimental results demonstrate that the
proposed CrackCue significantly improves the generalization ability and
robustness of the baseline methods. The source code will be publicly available.

</details>


### [23] [CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.16854)
*Xiaoqiang He*

Main category: cs.CV

TL;DR: 本文提出一个名为CLAMP的端到端对比学习框架，用于多模态方面情感分析（MABSA）。该框架通过渐进注意力融合、多任务对比学习和自适应多损失聚合模块，解决了现有方法在跨模态对齐噪声和细粒度表示一致性方面的挑战，并在标准基准测试中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态方面情感分析（MABSA）是产品评论系统和舆情监控等应用的基础任务，但现有方法面临跨模态对齐噪声、细粒度表示一致性不足以及全局模态对齐忽略方面词与局部视觉区域关联等挑战，这些限制了其有效性。

Method: 本文提出了一个端到端的对比学习框架CLAMP，包含三个新颖模块：
1.  **渐进注意力融合网络**：通过分层、多阶段的跨模态交互，增强文本特征与图像区域的细粒度对齐，并有效抑制无关视觉噪声。
2.  **多任务对比学习**：结合全局模态对比和局部粒度对齐，以增强跨模态表示的一致性。
3.  **自适应多损失聚合**：采用基于动态不确定性的加权机制，根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。

Result: 在标准公共基准测试中进行评估，CLAMP持续优于绝大多数现有最先进方法。

Conclusion: CLAMP框架通过其创新的模块设计，有效解决了多模态方面情感分析中的关键挑战（如跨模态对齐噪声和细粒度表示不一致性），显著提升了任务性能，为相关应用提供了更强大的支持。

Abstract: Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect
terms within paired image-text data and determine their fine grained sentiment
polarities, representing a fundamental task for improving the effectiveness of
applications such as product review systems and public opinion monitoring.
Existing methods face challenges such as cross modal alignment noise and
insufficient consistency in fine-grained representations. While global modality
alignment methods often overlook the connection between aspect terms and their
corresponding local visual regions, bridging the representation gap between
text and images remains a challenge. To address these limitations, this paper
introduces an end to end Contrastive Learning framework with Adaptive
Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed
of three novel modules: Progressive Attention Fusion network, Multi-task
Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive
Attention Fusion network enhances fine-grained alignment between textual
features and image regions via hierarchical, multi-stage cross modal
interactions, effectively suppressing irrelevant visual noise. Secondly,
multi-task contrastive learning combines global modal contrast and local
granularity alignment to enhance cross modal representation consistency.
Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting
mechanism to calibrate loss contributions according to each task's uncertainty,
thereby mitigating gradient interference. Evaluation on standard public
benchmarks demonstrates that CLAMP consistently outperforms the vast majority
of existing state of the art methods.

</details>


### [24] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

TL;DR: 本文提出SIA框架，通过三阶段提示工程，主动检测并缓解视觉语言模型（VLMs）中多模态输入的潜在有害意图，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在现实世界中广泛应用，由图像和文本微妙交互引发的新安全风险日益突出。尤其是不显眼的输入组合可能揭示有害意图，导致不安全响应。现有方法（如事后过滤或静态拒绝提示）难以检测这些潜在风险，特别是当危害仅从输入组合中浮现时。

Method: 提出SIA（Safety via Intent Awareness），一个免训练的提示工程框架，用于主动检测和缓解多模态输入中的有害意图。SIA采用三阶段推理过程：1) 通过图像描述进行视觉抽象，2) 通过少量示例思维链提示进行意图推断，3) 进行意图条件响应优化。SIA能够动态适应从图文对中推断出的隐含意图，而非依赖预定义规则或分类器。

Result: 在SIUO、MM-SafetyBench和HoliSafe等安全关键基准测试中，SIA实现了显著的安全改进，表现优于现有方法。尽管在MMStar上通用推理准确性略有下降，但其在安全性上的提升突显了意图感知推理的价值。

Conclusion: SIA通过意图感知推理，有效提升了视觉语言模型的安全性，使其更好地与以人为本的价值观对齐。尽管可能对通用推理准确性有轻微影响，但其在安全方面的显著收益证明了该方法的价值。

Abstract: As vision-language models (VLMs) are increasingly deployed in real-world
applications, new safety risks arise from the subtle interplay between images
and text. In particular, seemingly innocuous inputs can combine to reveal
harmful intent, leading to unsafe model responses. Despite increasing attention
to multimodal safety, previous approaches based on post hoc filtering or static
refusal prompts struggle to detect such latent risks, especially when
harmfulness emerges only from the combination of inputs. We propose SIA (Safety
via Intent Awareness), a training-free prompt engineering framework that
proactively detects and mitigates harmful intent in multimodal inputs. SIA
employs a three-stage reasoning process: (1) visual abstraction via captioning,
(2) intent inference through few-shot chain-of-thought prompting, and (3)
intent-conditioned response refinement. Rather than relying on predefined rules
or classifiers, SIA dynamically adapts to the implicit intent inferred from the
image-text pair. Through extensive experiments on safety-critical benchmarks
including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves
substantial safety improvements, outperforming prior methods. Although SIA
shows a minor reduction in general reasoning accuracy on MMStar, the
corresponding safety gains highlight the value of intent-aware reasoning in
aligning VLMs with human-centric values.

</details>


### [25] [Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection](https://arxiv.org/abs/2507.16861)
*Xiang Li*

Main category: cs.CV

TL;DR: 本文提出一种利用2D目标先验解决LiDAR和相机特征错位的方法，通过PGDC、DAGF和SGDM组件实现跨模态特征的预对齐与有效融合，在nuScenes数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR和相机融合方法常受特征错位影响，导致深度监督不准和融合错误。错位源于投影误差（标定不准、滚动快门），且主要集中在物体-背景边界。因此，研究动机是利用2D目标先验在融合前预对齐跨模态特征。

Method: ['Prior Guided Depth Calibration (PGDC)：利用2D先验校正局部错位，保留正确的跨模态特征对。', 'Discontinuity Aware Geometric Fusion (DAGF)：处理PGDC结果，抑制噪声并增强物体-背景边界的尖锐过渡，解决全局错位。', 'Structural Guidance Depth Modulator (SGDM)：使用门控注意力机制，有效融合对齐后的深度和图像特征。']

Result: 在nuScenes验证数据集上取得了最先进的性能，mAP达到71.5%，NDS达到73.6%。

Conclusion: 本研究提出的方法有效解决了LiDAR和相机特征错位问题，显著提升了自主驾驶的3D感知能力，并在基准数据集上展现出卓越性能。

Abstract: Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)
representation is crucial for enhancing 3D perception capabilities of
autonomous vehicles. However, current methods are often affected by
misalignment between camera and LiDAR features. This misalignment leads to
inaccurate depth supervision in camera branch and erroneous fusion during
cross-modal feature aggregation. The root cause of this misalignment lies in
projection errors, stemming from minor extrinsic calibration inaccuracies and
rolling shutter effect of LiDAR during vehicle motion. In this work, our key
insight is that these projection errors are predominantly concentrated at
object-background boundaries, which are readily identified by 2D detectors.
Based on this, our main motivation is to utilize 2D object priors to pre-align
cross-modal features before fusion. To address local misalignment, we propose
Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct
local misalignment and preserve correct cross-modal feature pairs. To resolve
global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)
to process calibrated results from PGDC, suppressing noise and explicitly
enhancing sharp transitions at object-background boundaries. To effectively
utilize these transition-aware depth representations, we incorporate Structural
Guidance Depth Modulator (SGDM), using a gated attention mechanism to
efficiently fuse aligned depth and image features. Our proposed method achieves
state-of-the-art performance on nuScenes validation dataset, with its mAP and
NDS reaching 71.5% and 73.6% respectively.

</details>


### [26] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 本研究引入图灵眼测试 (TET) 衡量多模态大语言模型 (MLLMs) 的感知能力，发现当前最先进的 MLLMs 在人类直观感知的任务上表现灾难性失败，并指出这主要是由于视觉塔泛化能力的不足。


<details>
  <summary>Details</summary>
Motivation: 实现多模态大语言模型 (MLLMs) 的类人感知和推理是一个核心挑战。虽然现有研究主要关注推理能力，但一个基本问题是 MLLMs 是否能像人类一样真正感知世界。本研究旨在将焦点从推理转向感知，填补衡量 MLLMs 感知能力基准的空白。

Method: 引入了图灵眼测试 (TET) 这一面向感知的基准。TET 包含四个诊断任务，使用人类直观处理的合成图像来评估 MLLMs 的表现。研究者测试了上下文学习、语言主干训练，并发现微调视觉塔能实现快速适应。

Result: 最先进的 MLLMs 在对人类来说微不足道的感知任务上表现出灾难性的失败。上下文学习和针对语言主干的训练（在以往基准中有效）都未能提升性能。然而，微调视觉塔能实现快速适应，这表明该基准对视觉塔的泛化能力提出了挑战，而非语言主干的知识和推理能力。

Conclusion: 当前 MLLMs 与人类感知之间存在关键差距，主要体现在视觉塔的泛化能力上。未来的工作将引入更多任务和方法来增强视觉泛化能力。

Abstract: Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [27] [HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting](https://arxiv.org/abs/2507.16873)
*Jeongeun Lee,Youngjae Yu,Dongha Lee*

Main category: cs.CV

TL;DR: 针对视频内容增长和用户偏好复杂性问题，本文提出了一个利用LLM用户模拟器生成个性化观看历史的新数据集HIPPO-Video，并开发了HiPHer方法利用该数据集进行个性化视频高光，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集缺乏个性化，无法捕捉用户复杂行为。然而，随着视频内容呈指数级增长，个性化视频高光成为一项核心任务，以满足用户高度可变和复杂的偏好。

Method: 1. 引入了HIPPO-Video数据集，该数据集通过基于LLM的用户模拟器生成真实的观看历史，包含2040对(观看历史, 显著性得分)数据，覆盖20400个视频。2. 提出了HiPHer方法，该方法利用个性化观看历史来预测受偏好影响的视频分段显著性得分，旨在验证数据集的有效性。

Result: 通过广泛实验，所提出的HiPHer方法在个性化视频高光方面表现优异，超越了现有的通用和基于查询的方法。

Conclusion: HIPPO-Video数据集和HiPHer方法展现了在真实场景中实现高度以用户为中心的视频高光的巨大潜力。

Abstract: The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.

</details>


### [28] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: 针对多实体指代表达理解（REC）中现有方法忽略实体间关系及数据集不足的问题，本文构建了ReMeX数据集，并提出了ReMeREC框架，该框架包含TMP和EIR模块，能有效建模实体关系，并结合LLMs生成辅助数据，显著提升了多实体定位和关系预测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达理解（REC）方法主要处理单实体定位，忽视了多实体场景中复杂的实体间关系，导致准确性和可靠性受限。同时，缺乏高质量、细粒度的图像-文本-关系配对数据集阻碍了该领域的进一步发展。此外，语言中隐式实体边界造成的语义歧义也是一个挑战。

Method: 1. 构建了一个关系感知、多实体的REC数据集ReMeX，包含详细的关系和文本标注。 2. 提出了ReMeREC框架，联合利用视觉和文本线索，在定位多个实体的同时建模其相互关系。 3. 引入了文本自适应多实体感知器（TMP），动态推断实体数量和跨度，解决语义歧义。 4. 引入了实体间关系推理器（EIR），增强关系推理和全局场景理解。 5. 构建了一个小规模辅助数据集EntityText，通过大型语言模型生成，以提高对细粒度提示的语言理解能力。

Result: 在四个基准数据集上的实验表明，ReMeREC在多实体定位和关系预测方面达到了最先进的性能，大幅优于现有方法。

Conclusion: 通过构建专用数据集（ReMeX）和提出创新的ReMeREC框架（包含TMP和EIR模块，并辅以LLM生成的数据），该研究成功解决了多实体指代表达理解中的关系建模和语义歧义问题，显著提升了该任务的性能和可靠性。

Abstract: Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [29] [CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos](https://arxiv.org/abs/2507.16878)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了CausalStep，一个新颖的视频基准，专门用于评估大型语言模型在视频中的逐步因果推理能力。实验显示，现有模型与人类在这一复杂推理任务上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本和图像推理方面取得了进展，但鲁棒的视频推理仍是巨大挑战。现有视频基准主要评估浅层理解和推理，并允许模型利用全局上下文，未能严格评估真正的因果和逐步推理。

Method: 引入了CausalStep基准，旨在进行视频中明确的逐步因果推理。CausalStep将视频分割成因果关联单元，并强制实行严格的逐步问答（QA）协议，要求顺序回答以防止捷径解。每个问题都包含基于错误类型精心设计的干扰项。该基准包含100个视频（六个类别）和1,852对多项选择QA。还提出了七种诊断指标，用于全面评估模型的因果推理能力。

Result: 通过对领先的专有和开源模型以及人类基线进行实验，结果揭示了当前模型与人类水平的逐步推理之间存在显著差距。

Conclusion: CausalStep提供了一个严格的基准，有助于推动鲁棒和可解释视频推理领域的进展。

Abstract: Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.

</details>


### [30] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 现有文生图扩散模型的数据记忆缓解策略（如剪枝）脆弱且不足，微调文本嵌入即可重新触发复制。本文挑战了记忆局部性假设，并提出了一种对抗性微调方法以增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 文生图扩散模型虽成功，但存在数据隐私和知识产权风险，因其可能无意中记忆和复制训练数据。现有缓解措施（基于剪枝）的有效性和鲁棒性需被评估。

Method: 评估了基于剪枝的防御方法的鲁棒性；通过微调输入提示的文本嵌入来演示数据复制的重新触发；挑战了记忆局部性假设，展示了从不同文本嵌入位置触发复制的可能性；提出了一种新颖的对抗性微调方法，该方法迭代搜索复制触发器并更新模型以增强鲁棒性。

Result: 即使在剪枝后，对文本嵌入进行微小调整也足以重新触发数据复制，表明防御的脆弱性。记忆并非局部化，复制可以从文本嵌入空间的不同位置触发，并遵循不同的模型路径。现有缓解策略不足以解决记忆问题。

Conclusion: 现有缓解策略不足以真正去除记忆内容，需要开发真正移除记忆内容而非仅抑制其检索的方法。本研究为理解文生图扩散模型中的记忆性质提供了新见解，并为构建更可信赖和合规的生成式AI奠定了基础。

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [31] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: S2S-ST是一个新颖的框架，仅通过单次、低成本的稀疏采样空间转录组（ST）数据和自然图像，即可高精度地补全ST数据，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率空间转录组（ST）数据成本高昂且稀缺，限制了其在生物医学研究中的广泛应用。

Method: 提出了Single-shot Sparser-to-Sparse (S2S-ST)框架，通过以下创新实现ST数据补全：1) 利用ST数据内在空间模式的稀疏到稀疏自监督学习策略；2) 结合自然图像进行跨域协同学习以增强特征表示；3) 使用级联数据一致性补全网络（CDCIN）迭代细化预测并保持采样数据保真度。

Result: 在乳腺癌、肝脏和淋巴组织等多种组织类型上的广泛实验表明，该方法在补全精度上优于最先进的方法。

Conclusion: S2S-ST框架能从稀疏输入中鲁棒重建ST数据，显著降低了对昂贵高分辨率数据的依赖，有助于ST技术在生物医学研究和临床应用中的更广泛普及。

Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.

</details>


### [32] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

TL;DR: 本文介绍AURA，首个基于LLM的视觉语言可解释性AI代理，专为医学图像的全面分析、解释和评估设计。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在智能代理领域取得了显著进展，但在医学影像应用中仍处于早期阶段，缺乏能提供透明、适应性强且符合临床需求的AI系统。

Method: 研究引入了AURA，一个基于Qwen-32B LLM架构的视觉语言可解释性代理。AURA集成了模块化工具箱，包括：1) 用于病理和解剖区域定位的分割套件；2) 支持图像级解释的反事实图像生成模块；3) 包含像素级差异分析和分类等评估工具，以评估诊断相关性和视觉可解释性。

Result: AURA通过实现动态交互、上下文解释和假设检验，显著提升了医学图像分析的透明度、适应性和临床一致性。它将医学图像分析从静态预测转变为交互式决策支持。

Conclusion: AURA展现了智能代理AI在医疗图像分析领域的巨大潜力，能够实现更透明、适应性更强并与临床需求高度结合的AI系统。

Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [33] [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/abs/2507.16946)
*Chiao-An Yang,Kuan-Chuan Peng,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本文提出并解决了长尾在线异常检测（LTOAD）这一新颖任务，通过引入类别无关框架，在离线和在线设置中均超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测（AD）研究虽在无异常图像、长尾数据及统一模型等方面取得进展，并探索在线学习，但离线长尾AD（LTAD）方法因依赖在线环境中不可用的类别标签而无法直接应用于在线设置。为应对此挑战并贴近现实，本文引入了长尾在线异常检测（LTOAD）这一新任务。

Method: 首先识别了离线LTAD方法在在线场景中因需要类别标签而存在的局限性。为解决此问题，提出了一种类别无关（class-agnostic）的LTAD框架，并将其进一步调整以适应在线学习设置。

Result: 在大多数离线LTAD设置中，包括工业制造和医疗领域，本方法均优于现有最先进的基线。尤其在MVTec数据集上，图像AUROC性能提升了+4.63%，甚至超过了可访问类别标签的方法。在最具挑战性的长尾在线设置中，图像AUROC比基线提高了+0.53%。同时，发布了LTOAD基准数据集。

Conclusion: 本文成功提出了长尾在线异常检测（LTOAD）这一新颖且更贴近现实的任务，并通过创新的类别无关框架在离线和在线环境下均显著提升了异常检测性能，为该领域未来的研究奠定了基础。

Abstract: Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .

</details>


### [34] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

TL;DR: 通过整合真实类和错误类的类别激活图（CAM），作者提出了新的显著性指导训练方法，以提高深度学习模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的显著性指导训练方法仅考虑真实类别的类别激活图（CAM），而忽略了错误类别的CAM。作者假设在二元任务中，真实类和错误类的CAM在人类识别的重要分类特征上应该存在差异，因此提出利用错误类CAM来改进模型的泛化能力。

Method: 提出三种新的显著性指导训练方法，这些方法将模型的真实类和错误类CAM都纳入训练策略中。此外，还提出了一个新颖的后验工具用于识别重要特征。

Result: 在多种二元分类任务（包括合成人脸检测、生物识别攻击检测和胸部X光异常分类）上进行评估，结果表明，所提出的方法比传统的（仅使用真实类CAM的）显著性指导训练方法更能提高深度学习模型的泛化能力。

Conclusion: 将模型的真实类和错误类类别激活图（CAM）都纳入显著性指导训练策略中，可以有效提升深度学习模型在二元分类任务中的泛化性能。

Abstract: Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [35] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

TL;DR: 通过GAN生成合成数据来增强手语手形分类器训练，解决了小规模不平衡数据集问题，并将RWTH数据集的准确率提升5%，同时展现出跨数据集的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大多数手语手形数据集规模有限且不平衡，严重阻碍了模型的有效训练。

Method: 通过生成合成数据来增强手形分类器的训练数据。使用EfficientNet分类器在小而高度不平衡的RWTH德语手语手形数据集上进行训练，并尝试不同策略结合生成图像和真实图像。比较了两种GAN架构：ReACGAN（通过辅助分类器利用标签信息生成）和SPADE（利用空间自适应归一化基于姿态信息生成）。

Result: 所提出的技术将RWTH数据集上的最新准确率提高了5%，有效解决了小规模和不平衡数据集的限制。此外，该方法通过利用在HaGRID数据集上训练的基于姿态的生成，展示了在不同手语数据集上的泛化能力，且无需重新训练生成器即可达到与单一来源训练分类器相当的性能。

Conclusion: 通过GAN生成合成数据是克服手语手形数据集规模有限和不平衡挑战的有效策略，能够显著提高分类准确性并增强模型的泛化能力。

Abstract: Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 该研究提出了一种基于多模态AI智能体的新型生命周期评估（LCA）方法，通过利用在线公开数据克服传统LCA的数据稀缺问题，实现了电子产品碳排放的快速且相对准确的估算。


<details>
  <summary>Details</summary>
Motivation: 近年来对可持续性信息的需求激增，但执行生命周期评估（LCA）所需的详细数据（如产品材料、生产流程至处置的环境影响）通常难以获取，阻碍了环境影响评估的进行。

Method: 该研究引入多模态AI智能体，模拟LCA专家与利益相关者（如产品经理、工程师）的交互，以计算电子设备的碳排放。AI智能体利用自定义数据抽象和软件工具，从在线文本和图像（来自维修社区、政府认证）中提取信息，迭代生成详细的生命周期清单。此外，还开发了两种方法：1) 通过将输入产品与具有相似描述和已知碳足迹的产品集群进行比较，直接估算环境影响；2) 基于数据驱动，将未知材料表示为相似材料排放因子的加权和来生成排放因子。

Result: 该方法将LCA专家分析时间从数周/数月缩短至一分钟以内，同时弥补了数据可用性空白。在无专有数据的情况下，碳足迹估算与专家LCA结果的误差在19%以内。直接环境影响估算仅需3毫秒，对电子产品的平均绝对百分比误差（MAPE）为12.28%。排放因子生成方法在MAPE上比人工选择提高120.26%。

Conclusion: 该研究提出的AI驱动LCA方法为未来的LCA工作流程提供了高效、准确且可扩展的解决方案，有效克服了传统LCA的数据获取难题，显著提升了可持续性评估的效率和精度。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [37] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 提出并验证了改进的弹性分配机制，以提高EECBS算法在多智能体路径寻找（MAPF）中的效率，同时保持次优界限。


<details>
  <summary>Details</summary>
Motivation: 现有的EECBS算法通过“弹性分配”（flex distribution）来提高效率，但这种贪婪的分配方式可能导致总路径成本（SOC）超出用户指定的次优因子，从而降低算法效率，因为它会迫使算法在不同路径集合之间切换而非专注于解决特定冲突。

Method: 提出了三种新的弹性分配机制：1) 基于冲突的弹性分配（Conflict-Based Flex Distribution），根据冲突数量按比例分配弹性；2) 基于延迟的弹性分配（Delay-Based Flex Distribution），估计满足约束所需的延迟；3) 混合策略弹性分配（Mixed-Strategy Flex Distribution），以分层框架结合前两种机制。这些新机制被证明是完整且次优界限的。

Result: 实验结果表明，与原始（贪婪）弹性分配相比，我们提出的方法表现更优异。

Conclusion: 通过引入基于冲突、基于延迟以及混合策略的弹性分配机制，我们成功解决了EECBS中传统弹性分配可能导致的效率问题。这些新机制不仅在理论上保证了完整性和次优界限，而且在实践中显著提升了算法性能，使得EECBS在解决多智能体路径寻找问题时更加高效。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [38] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 本研究表明，使用LoRA进行安全对齐微调可以有效提升大型语言模型的安全性，同时不损害其推理能力，从而解决了“安全税”问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）安全对齐方法通常会导致推理能力显著下降，即所谓的“安全税”现象，这限制了LLMs在解决复杂问题方面的应用。

Method: 本研究提出使用LoRA（Low-Rank Adaptation）技术在拒绝数据集上进行安全对齐微调（SFT）。通过将安全权重更新限制在低秩空间，最小化其对推理权重的干扰。作者在涵盖数学、科学和编码的四个基准上进行了广泛实验。

Result: 实验结果显示，该方法在不损害LLMs推理能力的前提下，实现了与全模型微调相当的高水平安全性。研究还观察到LoRA诱导的权重更新与初始权重的重叠更小，并且通过正则化或权重合并进一步减少这种重叠的方法在某些任务上显示出改进。

Conclusion: LoRA是一种有效且高效的LLMs安全对齐方法，能够成功避免“安全税”问题，同时保持强大的推理能力。这一成果有望激励未来设计在推理-安全权衡方面取得更一致改进的方法。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [39] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本文分析了AI在安全关键领域中端到端（E2E）架构对传统安全分析的挑战，评估了FMEA和FTA的局限性，并提出了一个名为HySAFE-AI的混合框架，旨在改进AI系统的安全评估，最后对未来AI安全标准提出了建议。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）在自动驾驶等安全关键领域日益普及，其系统架构正向端到端（E2E）的整体式模型（如大型语言模型和视觉语言模型）发展。这种复杂性使得传统的安全分析方法（如故障模式及影响分析FMEA和故障树分析FTA）难以有效评估AI系统的安全性。

Method: 1. 审查了不同的AI系统架构解决方案。2. 评估了FMEA和FTA等常用安全分析方法对AI系统的有效性。3. 演示了如何改进这些技术以适应基础模型（特别是其潜在表示）的复杂性。4. 引入并提出了HySAFE-AI（AI系统混合安全架构分析框架），该框架旨在将传统方法与AI系统安全评估相结合。

Result: 1. 揭示了传统安全分析技术（FMEA、FTA）在分析复杂基础AI模型（特别是其潜在表示）时的改进空间。2. 成功开发并提出了HySAFE-AI框架，该框架能够适应并改进传统方法，以有效评估AI系统的安全性。

Conclusion: 本研究通过引入HySAFE-AI框架，提供了一种将传统方法与AI系统安全评估相结合的混合方法。此外，论文还为指导未来AI安全标准的演进提供了建议和方向。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [40] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 本文开创性地利用图问题推理（GPR）和GraphPile数据集对大型语言模型（LLMs）进行持续预训练，显著提升了LLMs在数学和非数学任务上的通用推理能力，弥补了领域专用预训练与通用推理之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在处理新颖和复杂推理问题时表现不佳。现有的领域特定持续预训练（CPT）方法（如数学推理）虽然有前景，但缺乏向更广泛推理任务的迁移性。

Method: 本研究开创性地使用图问题推理（GPR）来增强LLMs的通用推理能力。为此，作者构建了GraphPile，这是首个专为GPR数据设计的持续预训练大规模语料库，包含109亿tokens，涵盖23个图任务，并包含思维链、程序思维、执行轨迹和真实世界图数据。研究团队使用GraphPile在Llama 3、3.1和Gemma 2等基础模型上训练了GraphMind。

Result: 通过使用GraphPile训练GraphMind，模型在数学推理方面准确率提高了高达4.9%，在逻辑和常识推理等非数学推理任务中性能提升高达21.2%。

Conclusion: 本工作首次利用GPR增强LLMs的推理模式，并引入了首个此类数据集GraphPile，成功弥合了领域特定预训练与通用推理能力之间的差距，显著提高了LLMs的适应性和鲁棒性。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [41] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 该文探讨将AI引入车辆作为传感平台，并通过能理解机器和驾驶员语言的AI副驾驶，实现从被动到主动维护的转变，旨在指导未来智能车辆系统研究。


<details>
  <summary>Details</summary>
Motivation: 将车辆维护从被动式转变为主动式，需要将AI集成到车辆中并使其成为有效的传感平台。

Method: 本文提供了一个概念性和技术性视角，旨在促使整合能够理解机器和驾驶员语言的AI副驾驶。

Result: 该文旨在激发跨学科对话，并为智能车辆系统、预测性维护和AI驱动用户交互领域的未来研究与开发提供指导。

Conclusion: 集成AI副驾驶对于未来智能车辆系统和预测性维护至关重要，它将促进新型AI驱动的用户交互，并指引未来的研发方向。

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [42] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 为解决语言模型智能体（LMA）因大语言模型（LLM）固有缺陷导致的身份流失问题，本文提出了一种名为AIE（agent identity evals）的评估框架，用于衡量和维护LMA的智能体身份，从而提升其可靠性和信任度。


<details>
  <summary>Details</summary>
Motivation: 语言模型智能体（LMA）的智能能力和可信赖性依赖于其稳定、可靠的身份。然而，LMA继承了大型语言模型（LLM）的无状态性、随机性等缺陷，这些缺陷会破坏其身份，进而削弱其可靠性、可信赖性及效用，并干扰其推理、规划和行动等智能能力。

Method: 本文引入了“智能体身份评估”（agent identity evals, AIE）框架。这是一个严谨、统计驱动的实证框架，用于衡量LMA系统随时间表现和维持其智能体身份的程度，包括其能力、属性以及从状态扰动中恢复的能力。AIE包含一套新颖的度量标准。

Result: AIE框架提供了一套新颖的度量标准，可与性能、能力及智能体鲁棒性等其他评估指标相结合。这些度量标准有助于设计最优的LMA基础设施和支架（如内存和工具），从而提升LMA的整体性能和可靠性。

Conclusion: 通过AIE框架，可以有效地衡量和维护语言模型智能体的身份稳定性，克服其从大型语言模型继承的固有缺陷，最终提升LMA的可靠性、可信赖性与实用性，并为优化LMA的基础设施设计提供支持。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [43] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究开发并评估了基于ChatGPT-4o-mini的编程教育聊天机器人SCRIPT，发现学生反馈请求有特定序列，且机器人响应与请求反馈类型高度匹配，为未来GenAI学习系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI用于编程教育的背景下，旨在开发一个支持编程初学者的工具，以提供开放式互动和结构化指导。

Method: 开发了名为SCRIPT的聊天机器人，基于ChatGPT-4o-mini。通过一项涉及136名德国大学编程入门课程学生的实验进行评估，分析学生在使用SCRIPT解决编程任务时的互动方式，重点关注他们的反馈偏好。

Result: 研究结果显示，学生的反馈请求似乎遵循一个特定序列；聊天机器人75%的响应与学生请求的反馈类型高度匹配，并遵守了系统提示的约束。

Conclusion: 这些洞察为基于生成式AI的学习支持系统设计提供了信息，并强调了在AI辅助工具中平衡指导与灵活性的挑战。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [44] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: 本文提出合规大脑助手（CBA），一个AI助手，通过智能路由机制平衡响应质量与延迟，显著提升企业合规任务效率，其性能优于传统大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 提高企业环境中合规人员日常任务的效率，并解决AI助手在响应质量与延迟之间难以平衡的问题。

Method: 开发了对话式、代理型AI助手CBA。设计了一个用户查询路由器，根据请求复杂性智能选择：1) FastTrack模式处理简单请求；2) FullAgentic模式处理复杂请求，涉及复合操作、工具调用及API集成。通过实验将CBA与开箱即用的大型语言模型进行对比，并评估了路由设计与单一模式（仅FastTrack/仅FullAgentic）的性能。

Result: CBA在平均关键词匹配率（83.7% vs. 41.7%）和LLM判断通过率（82.0% vs. 20.0%）上显著优于普通LLM。完整的路由设计在保持近似运行时间的情况下，比单一模式具有更好的平均匹配率和通过率。

Conclusion: 路由机制在响应质量和运行时间之间取得了良好的平衡，验证了其设计假设。CBA能够有效提高企业合规任务的效率。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [45] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型（LLMs）与实时优化算法结合的“共生代理”范式，旨在为6G网络构建值得信赖的通用人工智能（AGI）驱动系统，并在5G测试平台上验证了其在降低决策错误、提升资源效率方面的显著优势。


<details>
  <summary>Details</summary>
Motivation: LLM代理有望在6G网络中实现实时决策，但需要从处理孤立任务的专业AI向具备更广泛推理能力的AGI网络转型。现有LLM在数值精度任务中可能存在不确定性，且单独使用时资源消耗大，亟需一种结合优化算法以提高效率、准确性和可信赖性的方案。

Method: 研究引入了一种将LLMs与实时优化算法结合的“共生代理”范式，具体实现为：优化器在LLM的输入端提供有界不确定性引导以处理数值精确任务；LLM监督下的输出端优化器实现自适应实时控制。设计并实现了两种代理类型：无线接入网络（RAN）优化器和多代理服务水平协议（SLA）协商器。提案了一个AGI网络的端到端架构，并在捕获车辆移动引起信道波动的5G测试平台上进行了评估。

Result: 实验结果表明，“共生代理”相比独立的LLM代理能将决策错误减少五倍。小型语言模型（SLM）在达到相似精度的同时，GPU资源开销减少99.9%，并实现了82毫秒的近实时循环。多代理RAN协作演示在真实测试台上显著提高了服务水平协议和资源分配的灵活性，RAN过载利用率降低了约44%。

Conclusion: 本研究提出的“共生范式”是构建下一代AGI驱动网络系统的基础，这些系统即使在LLM技术不断发展的情况下，仍能保持适应性、高效性和可信赖性。此范式通过结合LLM的推理能力和优化算法的精确控制，有效解决了LLM在网络管理中面临的挑战，并提供了开源实现以促进未来研究。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [46] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出了Ctx2TrajGen，一个基于GAIL的上下文感知轨迹生成框架，利用PPO和WGAN-GP生成逼真的城市车辆轨迹，通过考虑周围环境和交互，在真实性、多样性和上下文保真度方面表现优异，解决了数据稀缺和领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对于交通行为分析和自动驾驶系统至关重要。

Method: 提出了Ctx2TrajGen框架，使用生成对抗模仿学习（GAIL）合成逼真的城市驾驶行为。该模型利用PPO和WGAN-GP解决非线性相互依赖和训练不稳定性。它通过明确地以周围车辆和道路几何形状为条件，生成交互感知的轨迹。

Result: 在DRIFT数据集上的实验表明，Ctx2TrajGen在真实性、行为多样性和上下文保真度方面优于现有方法。

Conclusion: Ctx2TrajGen为数据稀缺和领域转移问题提供了一个无需仿真的稳健解决方案。

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [47] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: 提出UDASA框架，通过量化不确定性实现LLM的完全自动化自对齐，并在无人工标注的情况下，在多个对齐任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在指令遵循和通用推理方面取得显著进展，但在缺乏人工标注的情况下，实现与人类意图和安全规范的高质量对齐仍然是一个根本性挑战。

Method: 本文提出不确定性驱动的自适应自对齐（UDASA）框架，旨在实现LLM的完全自动化对齐。UDASA首先为每个输入生成多个响应，并从语义、事实性和价值对齐三个维度量化输出不确定性。基于这些不确定性分数构建偏好对，并根据不确定性差异将训练样本分为保守、中等和探索性三个阶段，模型随后在这些阶段中逐步优化。

Result: 实验结果表明，UDASA在无害性、有益性、真实性和受控情感生成等多种任务上均优于现有对齐方法，显著提升了模型性能。

Conclusion: UDASA框架通过不确定性驱动的自动化方法，有效解决了在缺乏人工标注情况下LLM对齐的挑战，并在多个关键对齐维度上取得了优异的性能表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [48] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 本文引入LTLZinc，一个用于生成时间推理和持续学习任务的基准框架，以评估神经符号和持续学习方法，并揭示了现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI方法主要应用于静态场景，对时间维度上的推理和持续学习研究不足。

Method: 提出LTLZinc框架，通过结合线性时序逻辑（LTL）、MiniZinc约束和任意图像分类数据集，生成多样化的时间推理和持续学习任务，用于评估相关方法。

Result: LTLZinc生成的任务（包括序列分类和持续学习）在实验中展现了时间学习和推理的挑战性，并揭示了当前最先进方法的局限。

Conclusion: 发布LTLZinc生成器及现成任务，旨在促进神经符号和持续学习社区对统一时间学习与推理框架的研究。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [49] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 本文研究了本体上的受控查询评估（CQE），利用认知依赖（EDs）和最优GA审查器的交集来规范信息披露，旨在回答布尔合取查询并集（BUCQs）。研究确定了该方法的安全性，证明了特定条件下的数据复杂度为AC^0，并通过实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在本体上的受控查询评估（CQE）中，需要通过逻辑规则（如认知依赖EDs）来规范信息披露，以确保安全性。研究旨在将EDs与最优GA审查器结合，特别是采用所有最优GA审查器的交集方法来回答查询，该方法在其他语境下已被证明兼具高安全性和良好计算性能，因此希望在本体CQE中应用并验证其有效性。

Method: 1. 将认知依赖（EDs）与最优GA审查器概念结合，定义信息披露的安全机制。2. 专注于回答布尔合取查询并集（BUCQs），其答案基于所有最优GA审查器的交集。3. 理论上刻画了这种基于交集方法的安全性，并识别出“完整EDs”类别在该方法下仍是安全的。4. 对于EDs的特定子类和DL-Lite_R本体，开发了一个详细的一阶重写算法，以证明在数据复杂度上回答BUCQs属于AC^0。5. 进行了实验评估，以验证所提出的重写函数的实际可行性。

Result: 1. 成功刻画了基于所有最优GA审查器交集方法的安全性，并确定了“完整EDs”类在此框架下依然安全。2. 证明了对于EDs的某个子类和DL-Lite_R本体，在指定CQE语义下回答BUCQs在数据复杂度上属于AC^0，并通过提出的一阶重写算法实现了这一点。3. 实验结果表明，所提出的重写函数在实际应用中是可行的。

Conclusion: 本研究为本体上的受控查询评估提供了一种安全且计算效率高的方法，通过将认知依赖与最优GA审查器的交集相结合。理论证明了该方法的安全特性和在特定条件下的低数据复杂度（AC^0），并通过实验验证了其在实践中的可行性。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [50] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 本文提出一种自动化混合grounding方法，通过启发式算法智能选择ASP的grounding策略，旨在缓解ASP的grounding瓶颈，并在实验中展现出良好性能。


<details>
  <summary>Details</summary>
Motivation: Answer Set Programming (ASP) 的grounding瓶颈是其在工业中广泛应用的关键障碍。虽然混合grounding能缓解此问题，但目前尚不清楚何时应使用解耦体grounding，何时应使用标准自底向上grounding。

Method: 开发了一种自动化混合grounding方法。该方法引入了一个基于数据结构启发式（结合规则结构和实例数据估计）的分割算法，以自动检测何时使用解耦体grounding以及何时使用标准grounding更有益。

Result: 实验结果表明，该原型实现具有良好前景，在“难ground”场景下显示出改进，而在“难求解”实例上接近了当前最佳性能。

Conclusion: 通过引入智能分割算法，本文提出的自动化混合grounding方法有效解决了ASP混合grounding的策略选择问题，从而提升了其在特定复杂场景下的性能，有助于缓解grounding瓶颈。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [51] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本研究系统性地探索了LLM在RLVR框架下的多领域推理能力，揭示了领域间的交互动态和影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RLVR方面的研究主要集中在单一推理领域，但现实世界的推理任务需要多种认知技能的整合。目前，强化学习下这些推理技能之间的相互作用机制尚不清楚。

Method: 研究在RLVR框架下，对数学推理、代码生成和逻辑解谜三个主要领域进行了系统性调查。具体方法包括：1) 使用GRPO算法和Qwen-2.5-7B模型评估模型在单领域训练后的域内提升和跨域泛化能力；2) 考察组合式跨域训练中出现的相互增强与冲突；3) 分析SFT对RL的影响，比较基础模型和指令模型的性能差异；4) 深入探讨课程学习策略、奖励设计和语言特定因素对RL训练的影响。

Result: 通过大量实验，研究结果深入揭示了领域交互的动态，并揭示了影响专业化和通用化推理性能的关键因素。

Conclusion: 这些发现为优化强化学习方法以培养LLM全面的多领域推理能力提供了宝贵的指导。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [52] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 本文介绍了一种基于RAG的TAI Scan Tool，用于进行法律TAI自评估，旨在帮助AI系统符合AI法案，并通过两阶段方法准确预测风险等级并检索相关条款。


<details>
  <summary>Details</summary>
Motivation: 帮助AI系统进行法律TAI自评估，特别是促进其符合AI法案的合规性。

Method: 采用两阶段方法：预筛选和评估。评估输出包括AI系统根据AI法案的风险等级，并检索相关合规条款。

Result: 定性评估显示良好结果，能正确预测风险等级并从三个语义组中检索相关条款。工具的推理逻辑与AI法案中高风险系统的设定进行比较。

Conclusion: TAI Scan Tool能够有效评估AI系统的风险并提供合规指导，其推理机制与AI法案的核心关注点（高风险系统）高度契合，从而有效辅助合规。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: 本研究利用AI方法（特别是随机森林模型）预测假肢窝的适应性修改，以帮助标准化胫骨假肢窝的设计，减少对技师技能的依赖。


<details>
  <summary>Details</summary>
Motivation: 胫骨假肢窝的质量高度依赖技师的手动操作技能和专业知识，导致质量不一。研究旨在探索AI方法来标准化假肢窝设计。

Method: 收集了118名患者的残肢3D扫描数据和技师设计的假肢窝3D模型。数据经过预处理（对齐、标准化、压缩）。开发了三种AI算法（3D神经网络、前馈神经网络、随机森林），分别用于预测最终假肢窝形状或技师进行的适应性修改。通过表面到表面距离和误差位置图评估AI生成假肢窝与技师设计假肢窝之间的性能。

Result: 所有算法中，估计所需适应性修改的表现优于直接预测最终假肢窝形状。应用于适应性预测的随机森林模型取得了最低误差，中位表面到表面距离为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米。

Conclusion: AI技术，特别是基于随机森林模型预测适应性修改的方法，能够有效且精确地辅助标准化胫骨假肢窝的设计，有望减少对人工操作技能的依赖并提高设计一致性。

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [54] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本研究开发并评估了一个自动化工作流，用于在自动驾驶实验室（SDLs）中检测和恢复材料发现过程中的噪声数据特征，并系统性地探讨了数据集大小、噪声强度和特征分布对数据恢复的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室（SDLs）在加速材料发现方面潜力巨大，但输入参数捕获错误会导致特征受损，进而影响模型性能和实验活动的可靠性。因此，需要一种方法来系统地检测和纠正这些噪声特征，以提升数据质量和实验精度。

Method: 1. 开发了一个自动化工作流，旨在系统性地检测噪声特征、确定可纠正的样本-特征对并恢复正确的特征值。2. 进行了一项系统性研究，以探究数据集大小、噪声强度和特征值分布如何影响噪声特征的可检测性和可恢复性。

Result: 1. 高强度噪声和大型训练数据集有利于噪声特征的检测和纠正。2. 低强度噪声会降低检测和恢复效果，但可以通过更大的干净训练数据集来弥补。3. 具有连续和分散特征分布的特征比具有离散或窄分布的特征表现出更高的可恢复性。

Conclusion: 本研究不仅展示了一个模型无关的框架，用于在存在噪声、有限数据和不同特征分布的情况下进行合理的数据恢复，还为材料数据集中的kNN归因提供了一个具体的基准。这有助于提升自动材料发现中的数据质量和实验精度。

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [55] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: 引入TD-Interpreter，一个帮助工程师理解复杂时序图的机器学习工具。


<details>
  <summary>Details</summary>
Motivation: 工程师在设计和验证过程中，难以理解第三方提供的复杂时序图。

Method: TD-Interpreter是一个视觉问答（VQA）环境，通过微调轻量级7B多模态大语言模型LLaVA实现多模态学习。为解决训练数据限制，开发了合成数据生成工作流。

Result: 实验评估表明TD-Interpreter具有实用性，并在所评估的基准测试中大幅优于未经微调的GPT-4o。

Conclusion: TD-Interpreter是一个有效且实用的专业ML工具，能显著帮助工程师理解复杂时序图，并在特定任务上展现出超越通用大模型的性能。

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [56] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: 本论文将双曲Transformer集成到强化学习中，以解决多步推理任务中层次结构建模的挑战，并在准确性和计算效率上显著优于传统Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 多步推理是人工智能的核心挑战，RL虽有潜力但传统方法受信用分配、高维状态和稳定性问题困扰。Transformer和双曲几何的最新进展为克服这些挑战提供了新途径。

Method: 本文提出一个新框架，将双曲Transformer集成到强化学习中以实现多步推理。该方法利用双曲嵌入有效建模层次结构，并提供了理论见解、算法细节，通过Frontier Math和非线性最优控制问题进行实验验证。

Result: 与使用标准Transformer的强化学习相比，所提出的双曲RL方法在FrontierMath基准测试中准确率提高了32%~44%，在非线性最优控制基准测试中准确率提高了43%~45%。同时，在FrontierMath和非线性最优控制基准测试中，计算时间分别减少了16%~32%和16%~17%。

Conclusion: 该研究证明了双曲Transformer在强化学习中，特别是在处理涉及层次结构的多步推理任务方面具有显著潜力。

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [57] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: 本文提出了DiffCarl，一种碳排放和风险感知的扩散模型强化学习算法，用于多微电网系统智能运行，旨在降低运营成本和碳排放，并提高调度适应性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源日益集成和系统复杂性增加，微电网社区在不确定性下进行实时能源调度和优化面临重大挑战。

Method: DiffCarl将扩散模型整合到深度强化学习(DRL)框架中，通过去噪生成过程学习动作分布，以增强DRL策略表达能力，实现在动态不确定微电网环境中进行碳排放和风险感知的调度。

Result: 实验证明，DiffCarl的运营成本比经典算法和最先进的DRL解决方案低2.3-30.1%，碳排放比其未感知碳排放的版本低28.7%，并降低了性能变异性。

Conclusion: DiffCarl是一种实用且具有前瞻性的解决方案，其灵活设计使其能有效适应不同系统配置和目标，支持在不断发展的能源系统中进行实际部署。

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [58] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: 本文深入探讨了基于非紧对称空间U/H的卡坦神经网络（Cartan Neural Networks）的数学和几何结构，旨在确保其协变性和几何可解释性。


<details>
  <summary>Details</summary>
Motivation: 非紧对称空间U/H被认为是开发几何一致神经网络理论的有效途径。尽管已有姊妹论文初步展示了卡坦神经网络的可行性和性能，但本研究的动机是进一步扩展并详细阐述其核心的数学结构和几何属性，以支持网络的协变性和几何可解释性。

Method: 本文采用理论分析方法，详细阐述了卡坦神经网络中各层的几何特性，并解析了层间映射如何与这些几何结构相互作用，从而实现网络的协变性（covariant）和几何可解释性（geometrically interpretable）。

Result: 通过深入分析，本文揭示了卡坦神经网络的几何属性和层间映射的内在机制，阐明了这些结构如何协同作用，使得卡坦神经网络能够保持协变性并具备清晰的几何可解释性。

Conclusion: 本论文与前期的一篇姊妹论文共同构成了一项初步工作，旨在利用群论结构，为构建一个完全几何可解释的神经网络理论奠定基础。

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [59] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: 提出置信度优化概率编码（CPE）方法，通过调整距离计算和方差约束，解决概率编码中高斯噪声导致的距离扭曲问题，从而提升分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 概率编码在增强神经网络泛化能力的同时，引入的高斯噪声会扭曲分类任务中的点式距离测量，降低距离可靠性。

Method: 我们提出置信度优化概率编码（CPE）方法，主要策略有二：一是引入置信度感知机制来调整距离计算，确保概率编码分类任务中的一致性和可靠性；二是用更简单的L2正则化项直接约束方差，替代依赖不可靠先验的KL散度方差正则化。该方法具有模型无关性。

Result: 在自然语言分类任务中，针对BERT和RoBERTa模型进行的大量实验表明，我们提出的方法显著提高了性能和泛化能力。

Conclusion: CPE方法有效缓解了概率编码中高斯噪声对距离测量的负面影响，提高了距离的可靠性，从而显著增强了模型的分类性能和泛化能力。

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [60] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: 针对生成模型迭代采样计算昂贵的问题，本文提出SplitMeanFlow框架，通过推导新的代数恒等式“区间分裂一致性”来学习平均速度场，实现更高效的单步/少步生成。该方法比现有微分方法更通用、更高效，已成功应用于语音合成产品并实现20倍加速。


<details>
  <summary>Details</summary>
Motivation: 最先进的生成模型（如Flow Matching）通常依赖计算成本高昂的迭代采样过程。为了解决这一问题，近期研究转向学习平均速度场以实现少步或单步生成。然而，现有主流方法（如MeanFlow）基于微分恒等式，其表达形式可能受限。

Method: 本文从平均速度的第一性原理出发，利用定积分的加性，推导出一个新颖的、纯代数的恒等式，称为“区间分裂一致性”（Interval Splitting Consistency）。基于此原理，作者提出了SplitMeanFlow训练框架，将该代数一致性直接作为学习目标来训练平均速度场，无需微分算子。

Result: 理论上，本文证明了MeanFlow的核心微分恒等式是其代数一致性在区间分裂趋于无穷小时的极限特例，从而确立了SplitMeanFlow作为学习平均速度场更直接、更通用的基础。实践中，该代数方法显著提高了效率（消除JVP计算）、简化了实现、训练更稳定、并具有更广泛的硬件兼容性。单步和两步的SplitMeanFlow模型已成功部署于大规模语音合成产品（如豆包），实现了20倍的速度提升。

Conclusion: SplitMeanFlow提供了一种比现有微分方法更通用、更高效的学习平均速度场的代数基础。它显著加速了生成模型的推理过程，且在实际产品中展现出卓越的性能和广泛的应用潜力。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [61] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 本文提出一种简单的量化感知训练(QAT)方法，在极小训练开销下显著提升大语言模型量化性能，超越现有方法，并解决精度损失和硬件兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型量化可以降低推理延迟、模型大小和能耗，从而提升用户体验并降低成本。然而，挑战在于如何在合理时间内实现最小的精度损失，特别是要避免使用与专用推理加速器不兼容的机制。

Method: 采用一种简单、端到端的量化感知训练(QAT)方法。该方法仅引入量化操作本身，不增加额外操作，且可应用于激活、缓存和权重，并易于泛化到不同的模型架构。

Result: 在总模型训练预算增加不到0.1%的情况下，该方法在多个现代基准测试中，无论是基础模型还是指令模型变体，其性能均大幅优于现有领先的量化方法。

Conclusion: 所提出的简单、高效的量化感知训练方法有效解决了大语言模型量化中的精度损失和硬件兼容性挑战，提供了卓越的性能，且易于部署。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [62] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: 本研究利用分层强化学习（HRL）和通用价值函数（GVFs）为下肢外骨骼开发自适应控制策略，通过预测信息显著提高了系统在各种地形下的决策精度和性能。


<details>
  <summary>Details</summary>
Motivation: 旨在为运动障碍个体提升行动能力和自主性，通过开发下肢外骨骼的自适应控制策略来实现。

Method: 采用分层强化学习（HRL）方法，将外骨骼控制任务分解为地形策略适应的高层框架和提供预测信息的低层框架。低层框架通过通用价值函数（GVFs）的持续学习实现，GVFs利用可穿戴传感器（如肌电图、压力鞋垫、测角计）的数据生成未来信号的预测值。研究比较了两种将实际和预测传感器信号整合到策略网络中的方法。

Result: 关键结果显示，加入由通用价值函数（GVFs）生成的预测信息显著提高了网络的整体准确性。在平坦、不平坦地面、上下坡及转弯等通常容易误判的地形上，系统性能获得了特定地形的提升。

Conclusion: 预测信息有助于在不确定性（如易误判地形）下辅助决策。本研究为分层强化学习的细微之处以及未来外骨骼的发展提供了新见解，以促进在不同行走环境中的安全过渡和通行。

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [63] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: 论文介绍了PyG 2.0，一个PyTorch Geometric的重大更新，旨在通过增强架构、支持异构/时间图和优化存储，显著提升其在图神经网络领域的扩展性和实际应用能力。


<details>
  <summary>Details</summary>
Motivation: PyG作为图神经网络的领先框架，需要引入PyG 2.0及其后续版本，以解决日益增长的大规模图学习中的扩展性和现实应用能力挑战。

Method: 论文详细阐述了PyG 2.0的增强架构，包括对异构图和时间图的支持、可扩展的特征/图存储以及各项优化措施。文章还将总结PyG在各类应用领域的支持，并深入探讨关系深度学习和大型语言模型中的应用。

Result: PyG 2.0的更新显著提升了框架处理大规模图学习问题的效率和能力，增强了其在异构、时间图等复杂场景下的支持，使其更适用于现实世界的应用。

Conclusion: PyG 2.0是一个综合性更新，通过技术改进使其成为处理大规模和复杂图学习任务的更强大、更高效的工具，并在多个重要应用领域得到了验证和深化应用。

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [64] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文质疑OOD适应中消除偏置的必要性，提出一种理论指导下的框架，策略性地利用偏置来增强不变表示，并在实验中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD域模型适应方法普遍依赖不变表示学习来消除偏置特征的影响。然而，研究者提出疑问：偏置是否总应被消除？如果不，何时应保留它，以及如何有效利用它？

Method: 1. 进行了理论分析，探讨了偏置特征何时能被识别和有效利用的条件。
2. 基于理论基础，提出了一个新颖框架，在推理阶段策略性地利用偏置来补充不变表示。
3. 该框架包含两个核心组件：a) 利用不变性作为指导，从偏置中提取预测性成分；b) 利用已识别的偏置来估计环境条件，并据此探索合适的偏置感知预测器以弥补环境差异。
4. 通过在合成数据集和标准域泛化基准上进行实验验证。

Result: 实验结果一致表明，该方法优于现有方法，突显了其鲁棒性和适应性。

Conclusion: 本研究通过理论分析和提出的新型框架，证明了在OOD适应中策略性利用偏置特征的有效性。该方法通过在推理时巧妙结合偏置与不变表示，在各类数据集上均表现出超越现有方法的性能，展现出强大的鲁棒性和适应性。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [65] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: 介绍了一个名为laplax的开源Python包，它利用JAX实现了深度神经网络的拉普拉斯近似，旨在促进贝叶斯神经网络和不确定性量化研究。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯近似是量化深度神经网络权重空间不确定性、应用贝叶斯工具（如预测不确定性和模型选择）的有效方法。需要一个灵活、研究者友好的框架来支持相关研究。

Method: 开发了laplax，一个基于JAX的开源Python包。它采用模块化、纯函数式架构设计，并具有最少的外部依赖。

Result: laplax提供了一个灵活且便于研究人员使用的框架，可用于快速原型开发和实验。

Conclusion: laplax的目标是促进贝叶斯神经网络、深度学习不确定性量化以及改进拉普拉斯近似技术的研究和发展。

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [66] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: 本研究提出一种名为CGF-LLM的新型大语言模型框架，将GPT-2与模糊时间序列和因果图结合，首次用于多元时间序列预测，并通过将数值数据转换为可解释的文本形式，提升了预测效果和模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在时间序列预测（TSF）中的应用日益受到关注。研究旨在将复杂的数值时间序列数据转化为更具语义和结构洞察力的可解释形式，以供预训练的LLM处理。

Method: 开发了CGF-LLM框架，该框架结合了GPT-2模型、模糊时间序列（FTS）和因果图。核心方法是通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的文本表示，作为预训练GPT-2模型的输入，从而实现多元时间序列预测。

Result: 实验结果证实了所提出的基于LLM的时间序列预测模型（CGF-LLM）的有效性，并在四个不同的多元时间序列数据集上得到了验证。

Conclusion: 该研究为基于模糊时间序列的大语言模型在时间序列预测领域开辟了有前景的未来方向。

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [67] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: 本文将双层局部算子学习（BiLO）扩展到贝叶斯推断框架，用于处理偏微分方程（PDE）约束下的不确定性量化和逆问题，通过一种高效的双层方法实现了参数的准确推断和不确定性的量化。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程（PDE）主导的不确定性量化和逆问题在众多科学和工程应用中居于核心地位。现有的贝叶斯神经网络方法在高维权重空间采样和先验分布指定方面存在挑战。因此，本研究旨在将之前开发的BiLO方法扩展到贝叶斯推断框架，以克服这些限制并提高不确定性量化与参数推断的准确性。

Method: 本研究采用双层框架：下层通过最小化局部算子损失来训练神经网络以近似局部解算子；上层则利用基于梯度的马尔可夫链蒙特卡洛（MCMC）方法和低秩自适应（LoRA）技术，从后验分布中高效采样PDE参数。该方法避免了高维神经网络权重空间的采样问题，且无需指定神经网络解的先验分布，而是让不确定性通过PDE约束从数据中自然传播。此外，研究还分析了MCMC采样器中梯度的动态误差和下层问题不精确最小化导致的后验分布静态误差，并揭示了下层问题求解容差与不确定性量化精度之间的直接联系。

Result: 通过强制执行强PDE约束，所提出的方法显著提高了参数推断和不确定性量化的准确性。研究发现下层问题求解容差与不确定性量化精度之间存在直接关联。在多种PDE模型的数值实验中，该方法被证明能够提供准确的推断和不确定性量化，同时保持了较高的计算效率。

Conclusion: 本研究提出的方法成功地将BiLO扩展到贝叶斯推断框架，为偏微分方程（PDE）约束下的不确定性量化和逆问题提供了一种准确且计算高效的解决方案。

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [68] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文提出一种从行为策略中识别最常见行动并使用树模型进行策略推导的方法，以解决离线强化学习在可解释性和评估方面的挑战，并在实际医疗数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习(RL)在从观测数据中获取最优策略方面潜力巨大，但在安全关键领域，其解释性和评估面临挑战：无约束RL策略的黑箱性质阻碍了可解释性；脱策略评估对偏离数据收集行为策略的情况敏感，尤其在使用重要性采样方法时。

Method: 提出一种简单实用的替代方案：通过行为策略的可解释模型，从每个患者状态中最常选择的行动中推导治疗策略。具体使用树形模型来利用数据模式，实现状态的自然分组，并确保可解释性。通过控制考虑的行动数量来调节与行为策略的重叠度，从而实现可靠的脱策略评估。

Result: 在类风湿关节炎和败血症护理的真实世界案例中，该框架下推导出的策略能够优于当前实践，并提供了比离线RL更具可解释性的替代方案。

Conclusion: 这种务实的策略开发方法标准化了频繁的治疗模式，捕获了数据中蕴含的集体临床判断。它提供了一种既能提高性能又具有高可解释性及可靠评估能力的策略推导途径，尤其适用于安全关键领域。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [69] [LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks](https://arxiv.org/abs/2507.17188)
*Lijie Zheng,Ji He,Shih Yu Chang,Yulong Shen,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出一种分层优化框架，结合SDR和LLM-HeMARL，以在异构无人机网络中能量受限条件下最大化物理层保密速率。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分考虑异构无人机网络中无人机能力的多样性，且常忽视能量与安全之间的权衡，无法满足更实际场景的需求。

Method: 采用分层优化框架：内层使用基于半定松弛（SDR）的S2DC算法解决固定无人机位置下的保密预编码问题；外层引入大语言模型（LLM）引导的启发式多智能体强化学习（LLM-HeMARL）方法进行轨迹优化，该方法通过LLM集成专家启发式策略。

Result: 仿真结果表明，所提方法在保密速率和能量效率方面均优于现有基线，并且在不同无人机群规模和随机种子下均展现出良好的鲁棒性。

Conclusion: 所提出的分层优化框架，尤其是结合LLM的强化学习方法，能有效解决异构无人机网络中的物理层安全问题，显著提升系统保密性能和能源效率。

Abstract: This work tackles the physical layer security (PLS) problem of maximizing the
secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy
constraints. Unlike prior studies that assume uniform UAV capabilities or
overlook energy-security trade-offs, we consider a realistic scenario where
UAVs with diverse payloads and computation resources collaborate to serve
ground terminals in the presence of eavesdroppers. To manage the complex
coupling between UAV motion and communication, we propose a hierarchical
optimization framework. The inner layer uses a semidefinite relaxation
(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex
(d.c.) programming to solve the secrecy precoding problem with fixed UAV
positions. The outer layer introduces a Large Language Model (LLM)-guided
heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for
trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics
policy generated by the LLM, enabling UAVs to learn energy-aware,
security-driven trajectories without the inference overhead of real-time LLM
calls. The simulation results show that our method outperforms existing
baselines in secrecy rate and energy efficiency, with consistent robustness
across varying UAV swarm sizes and random seeds.

</details>


### [70] [Closed-Form and Boundary Expressions for Task-Success Probability in Status-Driven Systems](https://arxiv.org/abs/2507.17195)
*Jianpeng Qi,Chao Liu,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: 本文提出了一个统一的分析框架，用于计算计算优先网络中任务的端到端成功概率，解决了随机到达、有限容量和延迟等复杂因素带来的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 在计算优先网络系统中，及时高效地分发服务器状态至关重要，然而，由于任务的随机到达、服务器容量限制以及双向链路延迟等因素，对任务成功概率进行建模非常困难。

Method: 研究引入了一个统一的分析框架，该框架将接入点（AP）的转发规则抽象为一个单一概率，并通过拉普拉斯变换建模所有网络和等待延迟。该方法提供了端到端任务成功概率的闭合形式表达式，并给出了包含Erlang损失、信息过时和随机上下行链路延迟的上下界。

Result: 通过广泛参数的仿真验证了该框架，结果显示理论预测和界限始终能包络观察到的成功率。该框架仅需要两个可互换输入，易于适应不同的转发策略和延迟分布。实验表明，其上下界对经验任务成功概率的精度分别在0.01（上限）和0.016（下限）以内。

Conclusion: 该分析框架为计算优先网络中的任务成功概率分析提供了一个准确且适应性强的工具，能够有效地捕获复杂网络条件下的性能，并为替代转发策略和延迟分布的评估提供了便利。

Abstract: Timely and efficient dissemination of server status is critical in
compute-first networking systems, where user tasks arrive dynamically and
computing resources are limited and stochastic. In such systems, the access
point plays a key role in forwarding tasks to a server based on its latest
received server status. However, modeling the task-success probability
suffering the factors of stochastic arrivals, limited server capacity, and
bidirectional link delays. Therefore, we introduce a unified analytical
framework that abstracts the AP forwarding rule as a single probability and
models all network and waiting delays via their Laplace transforms. This
approach yields a closed form expression for the end to end task success
probability, together with upper and lower bounds that capture Erlang loss
blocking, information staleness, and random uplink/downlink delays. We validate
our results through simulations across a wide range of parameters, showing that
theoretical predictions and bounds consistently enclose observed success rates.
Our framework requires only two interchangeable inputs (the forwarding
probability and the delay transforms), making it readily adaptable to
alternative forwarding policies and delay distributions. Experiments
demonstrate that our bounds are able to achieve accuracy within 0.01 (upper
bound) and 0.016 (lower bound) of the empirical task success probability.

</details>


### [71] [Custody Transfer and Compressed Status Reporting for Bundle Protocol Version 7](https://arxiv.org/abs/2507.17403)
*Alice Le Bihan,Felix Flentge,Juan A. Fraire*

Main category: cs.NI

TL;DR: 本文提出并设计了针对BPv7的一种新型托管传输机制，通过序列号和专用扩展块实现捆绑包的高效识别和可靠报告，以解决BPv7核心协议中移除可靠传输功能的问题，并在模拟场景中进行了原型验证。


<details>
  <summary>Details</summary>
Motivation: 随着太空任务的增加，对高效可靠、以网络为中心的通信需求日益增长。星际通信（如LunaNet、地球观测、火星通信）中，需要使用中断/延迟容忍网络（DTN）及其捆绑协议（BP）。然而，BPv7版本移除了原有的“托管传输”机制，导致在间歇性连接和长延迟的挑战下，捆绑包（基本数据单元）的可靠性传输面临空白。

Method: 本文引入了一种BPv7的新型托管传输流程，其核心特点包括：1) 通过序列号高效识别捆绑包集合。2) 引入新的“托管传输扩展块”和对应的“压缩托管信号”管理记录，利用序列号高效报告托管的接受或拒绝。3) 引入新的“压缩报告扩展块”，结合对应的管理记录，利用序列号高效请求捆绑包处理步骤的报告。

Result: 所提出的机制已在欧洲空间局（ESA）的BP实现中进行了原型开发，并在地球观测和月球通信模拟场景中进行了测试。论文将展示这些测试结果。

Conclusion: 本文成功设计、规范并实现了一种BPv7的新型托管传输机制，弥补了BPv7核心协议中移除可靠传输功能的空白，并通过原型验证和模拟测试证明了其有效性。未来工作将继续探索DTN可靠传输领域。

Abstract: As space missions increase, there is a growing need to replace point-to-point
communication with an efficient and reliable network-centric communication
approach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol
(BP) has been selected as an interoperable network protocol in the LunaNet
Interoperability Specification. It is also considered for future Earth
Observation and Mars communication scenarios. In a DTN, the "bundle" -- the
fundamental data unit of BP -- requires dedicated mechanisms to ensure
reliability due to the challenges posed by intermittent connectivity and long
delays. The previous version of BP, BPv6, contained a mechanism for reliable
transfer between "custodial nodes" called "custody transfer". However, this
approach has been removed from the core protocol specification for BPv7, which
requires a corresponding BP reliability extension to be defined separately.
This paper introduces a new custody transfer process for BPv7 (expected to be
published by CCSDS as an experimental specification in 2025). The core features
of this new custody transfer method for BPv7 are: (1) A strategy to efficiently
identify sets of bundles by sequence numbering (2) A new Custody Transfer
Extension Block and a corresponding administrative record, Compressed Custody
Signal, to efficiently report on the acceptance or rejection of custody using
sequence numbering (3) A new Compressed Reporting Extension Block requesting
reporting on bundle processing steps using a corresponding administrative
record with sequence numbering for efficiency. The paper will describe those
concepts and their design, specification, and implementation in detail. These
mechanisms have been prototyped in the ESA BP implementation and tested in
Earth Observation and Lunar communication simulation scenarios. The results
will be presented, as will an outlook on future work in the DTN reliable
transfer domain.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [72] [Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems](https://arxiv.org/abs/2507.17736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究在图模型下复制数据库上的对称私有信息检索 (SPIR) 问题，其中服务器端公共随机性也按图复制。文章建立了SPIR容量的下界，证明了消息特有随机性的最小大小，并导出了路径图和正则图的精确SPIR容量。


<details>
  <summary>Details</summary>
Motivation: 引入了在图模型下复制数据库上进行SPIR的新问题，特别考虑了消息特定公共随机性也按图复制的场景，旨在探索此设置下的SPIR容量和相关限制。

Method: 1. 提出了一种可实现SPIR方案来建立SPIR容量的下界。 2. 通过证明必要性，确定消息特有随机性的最小大小。 3. 提供匹配的上限来推导特定图（路径图和正则图）的精确SPIR容量。

Result: 1. 建立了在一般图上SPIR容量的下界。 2. 证明了任何可行的SPIR方案中，消息特有随机性的最小大小必须等于消息大小。 3. 导出了路径图和正则图的精确SPIR容量。

Conclusion: 本研究为图模型下具有消息特定公共随机性的复制数据库上的SPIR问题提供了基础性理解，包括其容量边界以及对所需随机性大小的限制，并为特定图类型给出了精确的容量值。

Abstract: We introduce the problem of symmetric private information retrieval (SPIR) on
replicated databases modeled by a simple graph. In this model, each vertex
corresponds to a server, and a message is replicated on two servers if and only
if there is an edge between them. We consider the setting where the server-side
common randomness necessary to accomplish SPIR is also replicated at the
servers according to the graph, and we call this as message-specific common
randomness. In this setting, we establish a lower bound on the SPIR capacity,
i.e., the maximum download rate, for general graphs, by proposing an achievable
SPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the
minimum size of message-specific randomness should be equal to the size of a
message. Finally, by providing matching upper bounds, we derive the exact SPIR
capacity for the class of path and regular graphs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [73] [A Virtual Quantum Network Prototype for Open Access](https://arxiv.org/abs/2507.17495)
*Raj Kamleshkumar Madhu,Visuttha Manthamkarn,Zheshen Zhang,Jianqing Liu*

Main category: quant-ph

TL;DR: 本文提出了一个基于云的开放访问量子网络虚拟化平台，旨在解决当前量子网络规模有限、可访问性差和成本高昂的问题，实现量子硬件的远程和公平交互。


<details>
  <summary>Details</summary>
Motivation: 现有量子网络系统规模受限、应用单一，缺乏全球扩展路线图。这主要由于专业人才短缺、量子基础设施可访问性有限以及量子硬件构建和操作的复杂性与高成本。

Method: 本文提出一个开放访问、基于软件的量子网络虚拟化平台。该系统是一个云应用程序，可虚拟化实验室级量子网络测试平台的核心硬件组件（如时间标签器和光开关），使用户能够执行光子纠缠的符合计数。为确保资源公平分配，平台采用匈牙利算法为用户分配近乎相等的有效纠缠速率。

Result: 研究团队提供了实现细节和性能分析，从硬件、软件和云平台的角度证明了所开发原型的功能性和效率。

Conclusion: 该量子网络虚拟化平台成功应对了量子网络的可扩展性、可访问性和成本挑战，为量子硬件的远程和公平交互提供了有效解决方案，有望推动量子网络领域的进一步发展。

Abstract: The rise of quantum networks has revolutionized domains such as
communication, sensing, and cybersecurity. Despite this progress, current
quantum network systems remain limited in scale, are highly
application-specific (e.g., for quantum key distribution), and lack a clear
road map for global expansion. These limitations are largely driven by a
shortage of skilled professionals, limited accessibility to quantum
infrastructure, and the high complexity and cost associated with building and
operating quantum hardware. To address these challenges, this paper proposes an
open-access software-based quantum network virtualization platform designed to
facilitate scalable and remote interaction with quantum hardware. The system is
built around a cloud application that virtualizes the core hardware components
of a lab-scale quantum network testbed, including the time tagger and optical
switch, enabling users to perform coincidence counts of the photon
entanglements while ensuring fair resource allocation. The fairness is ensured
by employing the Hungarian Algorithm to allocate nearly equal effective
entanglement rates among users. We provide implementation details and
performance analysis from the perspectives of hardware, software, and cloud
platform, which demonstrates the functionality and efficiency of the developed
prototype.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: 本文全面分析了空间系统中的网络攻击向量，评估了缓解措施的有效性，并提出了风险评分框架，旨在为加强空间网络安全识别未来研究挑战。


<details>
  <summary>Details</summary>
Motivation: 现代技术日益依赖空间资产，但这些资产面临严重的网络攻击风险。现有研究缺乏对空间网络攻击向量的全面审查及对缓解措施有效性的严格评估。

Method: 本研究采取综合方法，分析了包括地面、空间、卫星和卫星星座在内的各类潜在空间网络攻击向量，评估了与空间基础设施相关的缓解措施的有效性，并提出了一个风险评分框架。

Result: 通过分析，本研究识别了开发和测试尖端网络安全技术解决方案的潜在研究挑战，并强调了在空间领域部署强大网络安全措施的必要性。

Conclusion: 本研究通过全面分析空间网络攻击向量和评估缓解措施，并提出风险评分框架，为未来空间网络安全技术的研发提供了方向，旨在促进建立更强大的空间网络安全防御体系。

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [75] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: 本文在5G网络环境下评估了NIST选定的后量子密码（PQC）算法性能，发现ML-KEM与ML-DSA组合在延迟敏感应用中效率最佳。


<details>
  <summary>Details</summary>
Motivation: 经典公钥密码系统面临量子计算威胁，需要转向后量子密码。然而，PQC在实际无线通信环境中的性能尚未充分研究。

Method: 在5G仿真堆栈（Open5GS、UERANSIM）上，通过PQC-enabled TLS 1.3（BoringSSL、liboqs）实现UE到UE通信，并评估NIST选定的PQC密钥封装机制和数字签名方案。性能指标包括握手延迟、CPU/内存使用、带宽和重传率。

Result: ML-KEM与ML-DSA组合在延迟敏感应用中表现出最佳效率。SPHINCS+和HQC组合则导致更高的计算和传输开销。

Conclusion: ML-KEM与ML-DSA组合是时间敏感5G场景的优选，而SPHINCS+和HQC组合因其高开销不适用于此类场景。

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [76] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: 本论文提出了一种增强型5G认证协议，该协议在5G-AKA基础上进行改进，实现了无状态设计并增加了完美前向保密性（PFS），在提升安全性的同时保持了低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前的5G-AKA协议尽管已被广泛采用，但在安全性和性能上存在已知局限性。它易受主动攻击，依赖序列号导致有状态设计，引发复杂性、不同步和额外通信开销，且缺乏完美前向保密性，可能导致长期密钥泄露时历史通信被破解。

Method: 研究者在5G-AKA设计基础上，首先引入了一个无状态版本，消除了对序列号的依赖，并保持与现有SIM卡和基础设施兼容。随后，在此基础上扩展增加了完美前向保密性（PFS），同时将加密开销降至最低。两种协议均使用ProVerif进行了严格的安全分析，并原型化评估了其性能，与5G-AKA和5G-AKA'（USENIX'21）进行了比较。

Result: 通过ProVerif分析确认，所提出的协议符合所有主要安全要求，包括抵御被动和主动攻击，以及符合3GPP和学术研究定义。性能评估结果显示，这些协议在提供更强安全性的同时，仅带来微小的计算开销。

Conclusion: 本论文提出的增强型5G认证协议是实用且面向未来的解决方案，能够有效应对5G及未来网络的安全挑战，提供更强的安全性和效率。

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [77] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: 随着组织向云端迁移，传统HSM和TPM的密钥管理面临云原生威胁导致的系统性漏洞。本文分析了相关安全故障、攻击向量，质疑其有效性，并提出需采用更具适应性的分层架构来保障云端加密信任。


<details>
  <summary>Details</summary>
Motivation: 组织在云迁移过程中，加密密钥管理的安全日益受到关注。尽管HSM和TPM是传统的安全标准，但其在云部署中因配置错误、API滥用和权限提升等云生态系统漏洞，而非硬件本身，导致敏感密钥材料泄露，促使重新评估其在分布式环境中的有效性。

Method: 本文通过分析涉及HSM和TPM的知名安全故障，识别常见的攻击向量，并质疑其在分布式环境中的长期有效性假设。同时，探索了机密计算、后量子密码学和去中心化密钥管理等替代方法。

Result: 研究发现，尽管HSM和TPM仍有其作用，但现代云安全需要更具适应性的分层架构来应对不断演变的威胁。

Conclusion: 本研究通过评估现有弱点和新兴模型，为云架构师和安全工程师提供了在不断变化的威胁环境中加强加密信任的策略，以提高云安全能力。

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>
