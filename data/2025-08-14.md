<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 72]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.NI](#cs.NI) [Total: 23]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 现有搜索代理顺序执行效率低。ParallelSearch提出一种强化学习框架，使LLM能并行化搜索，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推理增强型搜索代理（如Search-R1）在处理查询时，即使面对可并行化的逻辑独立比较，也严格按顺序执行。这种顺序瓶颈严重限制了计算效率，尤其对于需要多实体比较的查询。

Method: 提出ParallelSearch，一种新颖的强化学习框架，赋能大型语言模型（LLM）识别可并行化查询结构并并发执行多个搜索操作。该方法引入专用奖励函数，激励识别独立的查询组件，并通过共同考虑正确性、查询分解质量和并行执行优势来保持答案准确性。

Result: 综合实验表明，ParallelSearch在七个问答基准测试中平均性能提升2.9%，优于现有最佳基线。特别是在可并行化问题上，其性能提升了12.7%，同时与顺序方法相比，所需的LLM调用次数仅为69.6%。

Conclusion: ParallelSearch有效解决了现有搜索代理的顺序执行瓶颈，通过引入并行化搜索显著提升了LLM在复杂推理任务中的计算效率和问答性能。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 本研究评估了GPT-4o在罕见病命名实体识别（NER）中的表现，采用多种提示策略，并在低资源环境下取得了竞争性甚至最先进（SOTA）的结果，证明了提示优化LLM在生物医学NER中的潜力。


<details>
  <summary>Details</summary>
Motivation: 罕见病领域的命名实体识别（NER）面临独特挑战，包括标注数据有限、实体类型间语义模糊以及长尾分布问题，这限制了传统方法的有效性。

Method: 本研究评估了GPT-4o在低资源设置下进行罕见病NER的能力，采用的提示策略包括零样本提示、少样本情境学习、检索增强生成（RAG）和任务级微调。研究设计了一个结构化提示框架，编码了领域特定知识和四种实体类型的消歧规则，并引入了两种语义引导的少样本示例选择方法。所有实验均在RareDis语料库上进行。

Result: 实验结果显示，GPT-4o的性能与BioClinicalBERT相当或更优，其中任务级微调取得了新的最先进（SOTA）结果。成本-性能分析表明，少样本提示在较低令牌预算下能提供高回报，而RAG的额外收益微乎其微。错误分类揭示了边界漂移和类型混淆等常见失败模式。

Conclusion: 研究结果表明，提示优化的大语言模型（LLMs）可以作为生物医学NER中传统监督模型的有效且可扩展的替代方案，特别是在标注数据稀缺的罕见病应用中。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: 本文提出TEN，一种神经符号方法，用于从半结构化文本中提取表格数据。它通过LLM结合符号检查器和自调试循环，显著优于纯神经方法，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 从半结构化文本中提取表格数据具有挑战性，特别是当文本不一致地使用分隔符时。纯粹的神经方法因幻觉和无法强制执行硬约束而表现不佳。

Method: TEN方法结合了大型语言模型（LLM）和符号规则。它使用结构化分解提示（一种链式思考提示）让LLM生成初始表格；随后，一个符号检查器评估表格的规范性并检测幻觉；检查器输出由一个批评LLM处理，生成修复指南；最后，原始LLM在自调试循环中利用这些指南修正表格。

Result: 实验表明，TEN在多个数据集和指标上显著优于纯神经基线，实现了更高的精确匹配准确率和显著降低的幻觉率。一项21人用户研究证实，TEN生成的表格准确性更高（平均得分5.0 vs 4.3），并且在验证和修正方面更受用户青睐（超过60%的情况下选择TEN）。

Conclusion: TEN是一种高效的神经符号方法，能有效解决从半结构化文本中提取表格数据的难题，克服了纯神经方法的局限性，并表现出卓越的性能和用户满意度。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 本研究提出一种无需神经影像的计算框架，将文本情感映射到大脑区域，并验证其在区分临床人群和评估AI情感表达方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解语言情感表达与大脑功能的关系是计算神经科学和情感计算领域的挑战。传统神经影像技术昂贵且受限于实验室环境，而丰富的数字文本为情感-大脑映射提供了新途径。现有研究缺乏神经影像和计算文本分析的整合，无法大规模分析语言情感与大脑功能之间的关系。

Method: 研究提出一个计算框架，无需神经影像即可将文本情感内容映射到解剖学定义的大脑区域。具体方法包括：使用OpenAI的text-embedding-ada-002生成高维语义表示，通过降维和聚类识别情感群组，并将其映射到18个与情感处理相关的大脑区域。共进行了三项实验：i) 分析健康与抑郁症患者的对话数据以比较映射模式；ii) 将方法应用于GoEmotions数据集；iii) 比较人类书写文本与大型语言模型(LLM)响应，以评估推断的大脑激活差异。情感强度通过词汇分析进行评分。

Result: 结果显示了神经解剖学上合理且具有高空间特异性的映射。抑郁症患者表现出与负面情感相关的更高边缘系统参与度。该方法成功区分了离散情绪。大型语言模型生成的文本在基本情感分布上与人类文本匹配，但在同理心和自我参照区域（内侧前额叶和后扣带皮层）缺乏细微的激活。

Conclusion: 该方法具有成本效益和可扩展性，能够实现大规模自然语言分析，区分不同临床人群，并为评估人工智能情感表达提供一个基于大脑的基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 本研究提出并评估了一种人机混合Delphi（HAH-Delphi）框架，通过结合生成式AI（Gemini 2.5 Pro）和资深人类专家，旨在改进专家共识的制定过程。该框架在复制现有共识、与人类专家达成一致以及在实际应用中高效达成共识方面表现出色，证实了其提供高质量、情境敏感共识的潜力。


<details>
  <summary>Details</summary>
Motivation: 在证据复杂、冲突或不足以直接指导的领域，专家共识至关重要。然而，传统的共识方法（如Delphi研究、共识会议）存在专家负担重、解释过度简化和压制情境细微差别等局限性。当前信息过载、证据基础碎片化及对缺乏专家筛选的公开信息依赖增加，进一步加剧了这些挑战，因此需要一种更高效、更灵活的方法来生成高质量的专家共识。

Method: 本研究引入并评估了一个人机混合Delphi（HAH-Delphi）框架。该框架整合了生成式AI模型（Gemini 2.5 Pro）、小型资深人类专家小组以及结构化引导。HAH-Delphi通过三个阶段进行测试：第一阶段为回顾性复制，第二阶段为前瞻性比较，第三阶段为在耐力训练和阻力与混合心肺/力量训练两个应用领域进行部署。

Result: 在第一阶段，AI成功复制了95%已发布的专家共识结论。在第二阶段，AI与资深人类专家表现出95%的方向性一致，但缺乏经验性和实用性细微差别。在第三阶段，由六位资深专家组成的紧凑小组在最终参与者之前就实现了超过90%的共识覆盖率并达到了主题饱和。AI提供了连贯的、以文献为基础的支撑，有助于解决分歧并加速饱和。

Conclusion: HAH-Delphi框架提供了一种灵活、可扩展的方法，用于生成高质量、情境敏感的共识。其在健康、教练和运动表现科学领域的成功应用，证实了其方法学的稳健性，并支持其作为大规模生成条件性、个性化指导和发布共识框架的基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文提出一种新的无文本语音模型方法，通过联合建模语言和声学信息，在保持语言性能的同时，显著提升了语音的声学细节。


<details>
  <summary>Details</summary>
Motivation: 现有无文本语音模型仅预测语义token，并依赖独立的声码器来添加声学信息，导致模型无法访问声学上下文，也无法有效控制声学细节。

Method: 该研究提出联合建模语言信息（语义token）和声学信息（声学帧的连续实值表示）。具体方法是使用流匹配目标，根据语义token预测连续向量。研究还发现，预测多个未来语义token有助于更好地保留语言信息。

Result: 所提出的方法在语言似然基准测试中达到了与现有模型相当的性能，并且在提示生成中提供了更好的声学细节。

Conclusion: 通过联合建模语言和声学信息，可以有效提升无文本语音模型的声学细节生成能力，同时不牺牲其语言内容的准确性。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: APIO是一种无需手动指定初始提示词的自动提示词生成与优化方法，在语法错误纠正和文本简化任务上达到了LLM提示方法的最新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示词优化(APO)方法依赖于手动指定的初始提示词。本研究旨在开发一种无需人工预设提示词的自动提示词生成和优化方法，以拓展LLM在NLP任务中的应用。

Method: 提出APIO (prompt induction and optimization)，一种简单而有效的提示词生成与优化方法。该方法不依赖于手动指定的初始提示词，专注于语法错误纠正(GEC)和文本简化任务。

Result: APIO在语法错误纠正和文本简化任务上，为纯粹基于LLM的提示方法取得了新的最先进性能。

Conclusion: APIO为LLM在特定NLP任务（如GEC和文本简化）中提供了一种无需人工干预、表现卓越的提示词生成和优化范式，推动了LLM提示方法的进步。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 本文开发了基于LLM的Columbo系统，用于扩展表格列名缩写，通过改进数据集和评估指标，显著超越现有技术，并在真实世界中得到应用。


<details>
  <summary>Details</summary>
Motivation: 将表格中的缩写列名（如“esal”到“employee salary”）扩展为完整名称，对于多种下游数据任务至关重要，且该问题广泛存在于企业、科学和政府机构中。

Method: 首先，指出先前工作的合成数据局限性并引入4个新的真实世界数据集；其次，揭示现有准确性度量低估了正确扩展，并提出新的同义词感知度量；最后，开发了Columbo，一个利用上下文、规则、思维链推理和令牌级分析的强大LLM解决方案。

Result: 实验表明，Columbo在5个数据集上比当前最先进的解决方案NameGuess性能显著提升4-29%。Columbo已在环境科学数据门户EDI中投入生产使用。

Conclusion: Columbo显著提升了列名缩写扩展的准确性，超越了现有最先进技术，证明了其LLM驱动方法的强大有效性，并成功应用于实际生产环境。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 该论文利用Zipformer模型，有效解决了儿童指导场景下中英文混杂语音中的语言识别挑战，尤其在数据不平衡的情况下，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在双语环境下，特别是儿童指导场景中，语码转换（code-switching）和语言识别面临重大挑战，尤其当语音中包含不平衡的普通话和英语时。

Method: 使用Zipformer模型处理语音细微差别，并利用其内部层提取语言特征嵌入。研究中展示了内部层选择方法，并与不同后端进行了比较，以验证模型的鲁棒性。

Result: Zipformer的内部层能够有效编码语言特征，可用于语言识别。该模型在不同后端下表现出良好鲁棒性，并且能够有效处理不平衡数据，实现了81.89%的平衡准确率（BAC），比语言识别基线提升了15.47%。

Conclusion: 研究结果突出了Transformer编码器架构模型在实际场景中处理语码转换和语言识别挑战的巨大潜力。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本研究揭示了视觉-语言模型（VLMs）在生成图表摘要时存在地缘经济偏见，倾向于对高收入国家给出更积极的描述，并指出现有去偏技术效果有限。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（VLMs）在图表到文本生成任务上取得了显著进展，但其输出中潜在的偏见，特别是地缘经济偏见，鲜有研究，这可能导致社会危害。本研究旨在填补这一空白，探究VLMs如何放大此类偏见。

Method: 研究通过对来自六种主流VLM模型生成的6,000个图表-国家对进行大规模评估，分析国家经济状况对生成摘要情感倾向的影响。此外，还探索并测试了基于提示（使用积极干扰项）的推理时去偏技术。

Result: 分析发现，现有VLMs倾向于对高收入国家产生更积极的描述，而对中低收入国家则不然，即使仅改变国家属性。部分模型如GPT-4o-mini、Gemini-1.5-Flash和Phi-3.5表现出不同程度的偏见。同时，探索的基于提示的去偏技术仅部分有效。

Conclusion: VLMs在图表摘要生成中存在显著的地缘经济偏见，且这是一个复杂问题，现有去偏策略效果有限。这强调了未来需要更稳健的去偏方法来解决此类潜在的社会危害。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 现有LLM基准评估不足，本文提出了一个用户中心的主观排行榜（USL），并开发了定制化奖励模型（CRM），该模型超越了SOTA模型并能有效处理用户偏好多样性，以更好地匹配用户需求。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要关注可验证任务，是客观且静态的，这限制了它们在实际LLM选择中的效用，使得用户难以根据个人需求找到合适的模型。

Method: ['深入研究了超过1万条真实人类偏好数据，以理解其多样性和矛盾性。', '提出了首个用户中心主观排行榜（USL），旨在提供基于偏好、动态的LLM排名。', '引入了定制化奖励模型（CRMs）来处理人类偏好的多样性和矛盾性，并为USL提供支持。']

Result: ['人类偏好表现出显著的多样性和矛盾性，限制了现有奖励模型的有效性。', '仅4B参数的CRM在性能上超越了GPT-4.1和Gemini-2.5-pro等领先模型。', 'CRM在新的主题和标准上展现出卓越的泛化能力。', '由CRM驱动的USL与矛盾偏好呈强烈的负相关。']

Conclusion: 本文通过提出用户中心主观排行榜（USL）和定制化奖励模型（CRM），有效解决了现有LLM评估基准的局限性。CRM不仅性能优越、泛化能力强，还能有效处理复杂的人类偏好，为用户提供更实用的LLM选择方案。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 本文提出“主动阅读”框架，通过自生成学习策略提升大语言模型（LLMs）的知识吸收能力和事实可靠性，并在微调和预训练规模上验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽存储大量知识，但事实召回并不可靠，且缺乏工具来确保模型可靠一致地学习特定知识。

Method: 提出“主动阅读”（Active Reading）框架，通过自生成学习策略训练模型学习给定材料。

Result: 通过“主动阅读”训练的模型比传统微调和数据增强吸收更多知识。在SimpleQA上相对普通微调提升313%，在FinanceBench上提升160%。此外，该方法也能应用于预训练阶段，构建更具事实性的模型，例如Meta WikiExpert-8B在事实性问答上超越了参数量更大的模型。

Conclusion: “主动阅读”框架能有效提升大语言模型在专家领域知识吸收和事实可靠性，无论是在微调还是预训练规模，都为确保模型可靠学习知识提供了新途径。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 本文提出了动态段落选择器（DPS），一个新颖的重排序框架，将段落选择视为监督学习问题，旨在动态选择最相关的段落集，以克服现有RAG系统固定Top-K的局限性，并在多跳查询中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统常受限于重排序模块，其独立评分并选择固定Top-K段落的方法，难以处理需要综合多文档证据的复杂多跳查询。固定K值会造成取舍：小K遗漏关键信息，大K引入噪音。

Method: 引入了动态段落选择器（DPS），它将段落选择视为监督学习问题，而非传统点对点或列表式方法。DPS经过微调以捕捉段落间依赖关系，并动态选择最相关的段落集用于生成。作为即插即用模块，无需修改标准RAG管线。

Result: 在五个基准测试中，DPS持续优于最先进的重排序器和微调方法。特别是在MuSiQue数据集上，F1分数比Qwen3-reranker提升30.06%，比RankingGPT提升15.4%。

Conclusion: 通过实现自适应证据选择，DPS显著增强了复杂RAG场景中的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出一种新的跨语言情感分析(ABSA)方法，利用大语言模型(LLM)生成高质量伪标签数据，以避免依赖不可靠的翻译工具。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法严重依赖不可靠的翻译工具来弥合语言障碍，影响分析准确性。

Method: 该方法首先训练一个ABSA模型对目标语言的未标注数据进行预测；然后，利用LLM将这些预测转换为更自然的伪标签句子；最后，使用这些伪标签数据集对ABSA模型进行微调。该框架也支持生成模型，并展示了微调LLM的优越性。

Result: 该方法在六种语言和五种骨干模型上均表现出有效性，超越了现有最先进的基于翻译的方法。此外，微调后的LLM性能优于较小的多语言模型。

Conclusion: 所提出的基于LLM的伪标签生成框架能有效解决跨语言ABSA问题，无需翻译工具，并取得了领先的性能。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 跨语言方面级情感分析(ABSA)的综合性综述。


<details>
  <summary>Details</summary>
Motivation: 当前方面级情感分析研究多集中于单语，而跨语言ABSA（特别是资源稀缺语言）是一个未充分探索的领域，且缺乏系统的综述。

Method: 通过对跨语言ABSA进行全面综述，涵盖其关键任务（如方面词抽取、情感分类）、数据集、建模范式、跨语言迁移方法，并分析单语、多语及大模型ABSA的贡献。

Result: 提供跨语言ABSA领域的全面总结，明确了现有任务、方法、数据集，并识别了当前挑战及未来研究方向。

Conclusion: 本综述填补了跨语言ABSA领域系统性总结的空白，为未来研究提供了清晰的路线图。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出一个零样本事实核查声明检索系统，通过利用大型语言模型生成文本嵌入并结合不同模型，实现了竞争力强的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需特定训练数据的零样本系统，用于高效地进行事实核查声明检索。

Method: 采用多种先进大型语言模型（LLMs）生成文本嵌入，并通过模型组合优化性能。系统利用嵌入的余弦相似度来识别最相关的声明。特别地，为规避多语言模型表现不佳，仅使用英文翻译作为文本嵌入模型的输入。

Result: 该方法在单语子任务中排名第7，在跨语子任务中排名第9。NVIDIA NV-Embed-v2模型在整体表现上最佳，而在某些语言中，模型组合（如NV-Embed与GPT或Mistral）带来了额外性能提升。

Conclusion: 该零样本事实核查声明检索系统表现出较强的竞争力，其中NVIDIA NV-Embed-v2模型和针对特定语言的模型组合是关键。将多语言输入统一为英文翻译是提高系统性能的有效策略。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 针对现有情感支持模型缺乏深层同理心推理，本文提出可控同理心推理，构建数据集，并利用强化学习与创新策略提升模型情感支持能力。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持模型缺乏基于心理学原则的深层同理心推理能力，无法有效促进情感健康。

Method: 提出结合自然语言推理与结构化心理学步骤的“可控同理心推理”；构建标注推理正确性和回复偏好的精细数据集；采用强化学习并使用统一的“过程-结果奖励模型”以提供精确反馈；引入“基于个性的对话重写”和“冗余感知奖励重加权策略”以缓解回复重复性。

Result: 所提出的方法显著提升了模型的情感支持能力。

Conclusion: 该研究推动了开发更具同理心和人性化的情感支持系统的进程。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种名为“N-Gram Coverage Attack”的黑盒成员推断攻击方法，仅依赖语言模型的文本输出即可有效运作，甚至在许多情况下性能媲美白盒攻击，并发现GPT-4o等最新模型对成员推断的鲁棒性有所增强。


<details>
  <summary>Details</summary>
Motivation: 当前的成员推断攻击（用于检测版权侵犯和数据泄露）大多需要访问模型的隐藏状态或概率分布，这使得它们无法应用于广泛使用的、仅提供API访问的黑盒模型（如GPT-4）。因此，需要一种仅依赖文本输出的黑盒攻击方法。

Method: N-Gram Coverage Attack基于模型更可能生成训练数据中常见文本模式的观察。具体方法是：对一个候选成员，首先获取模型在给定候选前缀条件下的多个生成文本；然后使用n-gram重叠度量计算这些输出与真实后缀的相似度；高相似度则预示着高成员可能性。该方法的成功率与攻击计算预算（即生成的序列数量）呈正相关。

Result: N-Gram Coverage Attack在多种现有基准测试中超越了其他黑盒方法，并且在仅能访问文本输出的情况下，实现了与当前最先进的白盒攻击相当甚至更优的性能。研究发现，该方法的成功率随攻击计算预算的增加而提高。此外，利用该方法对OpenAI的闭源模型进行调查，发现GPT-4o等最新模型对成员推断的鲁棒性有所增强。

Conclusion: N-Gram Coverage Attack是一种高效的黑盒成员推断方法，能够对仅提供文本输出的模型进行隐私调查。研究结果表明，新的语言模型（如GPT-4o）在隐私保护方面表现出更高的鲁棒性，这可能预示着隐私保护的趋势正在改进。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 一项关于检测俄语AI生成科学摘要的共享任务，建立了大型数据集，并取得了积极的检测结果，旨在推动该领域的持续研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步导致人工与AI生成内容难以区分，对学术诚信构成挑战，尤其在科学出版和多语言环境中，检测资源稀缺。

Method: 引入AINL-Eval 2025共享任务，专注于检测俄语AI生成的科学摘要。构建了一个包含52,305个样本的大型数据集，涵盖12个科学领域的人工撰写摘要和来自五种主流LLM（如GPT-4-Turbo, Llama3.3-70B等）生成的AI摘要。任务挑战参与者开发可泛化到新领域和未见模型的解决方案。任务分两阶段进行，吸引了10个团队和159份提交。

Result: 顶级系统在识别AI生成内容方面表现出强大的性能。

Conclusion: 该任务成功推动了AI生成内容检测技术的发展，并建立了持续的共享任务平台和公开可用的数据集，以促进该领域的长期研究进展。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 通过分析解码温度对语言模型多样性（精确率/召回率）的影响，发现现有方法局限性，并提出基于精确率-召回率框架重新设计损失函数，以实现更好的精度与召回权衡。


<details>
  <summary>Details</summary>
Motivation: 增加语言模型多样性是一个挑战性目标，常见的解码温度调整方法在提升召回率上效果不佳。研究旨在深入理解为何降低温度能提升精确率而提高温度却无法有效提升召回率，并解决语言模型多样性问题。

Method: 通过分析一个简化但常见的案例，研究解码温度对语言模型质量（精确率）和覆盖率（召回率）的影响。在此基础上，提出利用精确率-召回率框架重新设计语言模型的损失函数。

Result: 分析表明，为了通过温度调整有效调优模型，模型必须朝覆盖率方向训练。所提出的基于精确率-召回率框架的方法，相比仅将负对数似然训练与温度缩放结合，在精确率和召回率之间取得了显著更好的权衡。

Conclusion: 这些发现为开发更通用、更鲁棒的语言建模技术提供了新途径，表明通过重新思考损失函数可以有效提升语言模型的性能和多样性。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval是一种无训练、高效的基准测试方法，通过自适应选择高质量代表性子集来解决LLM评估中的数据冗余问题，同时保持高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)的快速发展和评估基准规模的扩大，给模型评估带来了巨大的计算挑战。

Method: 本文提出了EffiEval，一种无训练的评估方法。它通过基于模型效用指数(MUI)自适应选择高质量的代表性子集，以实现高代表性、公平性和泛化性，从而有效解决数据冗余问题。

Result: 在多个公共基准和不同LLM上的大量实验表明，EffiEval仅使用原始数据的一小部分就能实现与全数据集评估高度一致的模型排名，且在大小上具有灵活性和可扩展性。

Conclusion: EffiEval为LLM时代提供了可靠、公平且高效的评估方案，具有实用性和普适性。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 链式思考（CoT）蒸馏在提升小型语言模型（SLM）推理能力时可能损害其安全性。本文提出SLowED方法，通过慢速微调和低熵掩码，在保持安全性的同时提升SLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思考（CoT）蒸馏方法主要关注利用大型语言模型（LLM）的高质量推理过程来增强小型语言模型（SLM）的推理能力，但很少关注此训练过程对SLM安全性可能造成的负面影响。虽然存在安全对齐方法，但它们通常需要额外计算或标注数据，并可能影响推理能力。因此，研究如何在CoT蒸馏过程中维护SLM的安全性是本研究的核心动机。

Method: 本文提出了一种名为Slow Tuning and Low-Entropy Masking Distillation (SLowED) 的安全蒸馏方法。该方法包含两个关键模块：1) Slow Tuning，通过缩小模型权重变化的幅度，使模型权重在初始权重分布的邻近空间进行优化；2) Low-Entropy Masking，用于掩盖被视为不必要学习目标的低熵令牌，从而将其从微调过程中排除。

Result: 在Qwen2.5-1.5B、Llama-3.2-1B和BLOOM-1.1B三种SLM上进行的实验，通过推理基准（BBH, BB-Sub, ARC, AGIEval）和安全评估（AdvBench）显示，SLowED方法在保持SLM安全性的同时，能与现有蒸馏方法媲美地提升其推理能力。消融研究进一步证实了Slow Tuning在训练早期阶段对模型安全性的维护作用，以及Low-Entropy Masking在延长安全训练周期方面的有效性。

Conclusion: SLowED是一种有效且安全的CoT蒸馏方法，成功解决了SLM在推理能力提升过程中可能出现的安全性下降问题。该方法在保持模型安全性的前提下，显著提升了小型语言模型的推理能力，为安全高效的SLM训练提供了新的途径。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 研究评估了LLM在印度法律任务中的表现，发现它们擅长起草和问题识别，但在专业法律研究方面存在幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能融入法律领域，本文旨在实证评估大型语言模型（LLM）在执行关键法律任务方面的能力。

Method: 采用调查实验方法，将GPT、Claude和Llama等LLM在印度法律任务（包括问题识别、法律起草、建议、研究和推理）中的输出与初级律师的输出进行比较。高级法学生根据实用性、准确性和全面性对作品进行评分。

Result: LLM在起草和问题识别方面表现出色，常能与人类工作媲美或超越。然而，它们在专业法律研究方面表现挣扎，频繁生成幻觉、事实不准确或捏造的内容。

Conclusion: LLM可以增强某些法律任务，但人类的专业知识在细致的推理和法律的精确应用方面仍然至关重要。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 本研究评估了视觉语言模型(VLMs)对误导性信息图表的解释能力，发现大多数VLMs会被欺骗，导致对相同数据的错误解读。


<details>
  <summary>Details</summary>
Motivation: 信息可视化可能包含欺骗性设计元素，传播错误信息。鉴于视觉语言模型(VLMs)正被广泛用于解释可视化图表，了解它们对欺骗性视觉设计的敏感度至关重要。

Method: 通过深入评估VLMs解释误导性可视化图表的能力，分析了来自十种不同模型的16,000多个响应，涵盖八种不同类型的误导性图表设计。

Result: 研究表明，大多数VLMs会被误导性图表欺骗，导致对图表的解释发生改变，即使底层数据相同。

Conclusion: 研究结果强调了在VLMs中建立强大的防范机制以应对视觉错误信息的必要性。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 本文提出GFPO（Group Filtered Policy Optimization），通过训练时过滤和优化，解决了大语言模型（LLMs）在强化学习中响应冗长的问题，显著减少了推理时的输出长度并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在经过可验证奖励的强化学习训练后，倾向于以增加响应长度来换取准确性，导致输出中包含大量重复、冗余的“填充”文本，降低了效率。

Method: 本文引入了GFPO（Group Filtered Policy Optimization）。该方法在训练时为每个问题采样更大的响应组，并根据响应长度和令牌效率（每令牌奖励比率）过滤这些响应以进行训练。其核心思想是通过在训练时进行更多的采样，从而使模型在推理时减少不必要的思考。此外，还提出了自适应难度GFPO，根据实时难度估计动态分配训练资源给更困难的问题。

Result: 在Phi-4-reasoning模型以及AIME 24/25、GPQA、Omni-MATH和LiveCodeBench等具有挑战性的STEM和编码基准测试中，GFPO将GRPO的长度膨胀率降低了46-71%，同时保持了准确性。通过进一步优化每个令牌的奖励，长度膨胀率可达到71-85%。自适应难度GFPO在计算效率和准确性之间取得了更好的平衡，尤其是在处理难题时。

Conclusion: GFPO证明了增加训练时的计算量可以直接转化为减少测试时的计算量，这是一种简单而有效的方法，可以实现高效的推理，提升模型的实用性。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种针对多跳问答的新型检索增强生成（RAG）框架，通过大型语言模型（LLM）进行问题分解和生成可回答的问题嵌入进行文档检索，有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架在处理复杂多跳问题时存在局限性，特别是多跳查询固有的模糊性，需要更有效的方法来指导文档检索并提升问答性能。

Method: 1. 利用LLM将复杂多跳问题分解为一系列单跳子问题，以指导文档检索。2. 不直接嵌入原始文档块，而是使用Qwen3-8B从每个文档块生成可回答的问题，嵌入这些生成的问题，并通过问题-问题嵌入相似性检索相关文档块。3. 将检索到的文档块与原始问题一同送入RAG管道。在MuSiQue、2WikiMultiHopQa、HotpotQA等LongBench数据集上进行评估。

Result: 与基线系统相比，所提出的方法显著提高了RAG的性能。

Conclusion: 研究成果突出显示了使用可回答问题嵌入对RAG的益处，以及基于LLM的查询分解在多跳问答场景中的有效性。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）对政治话题的立场极易受提示中论据的影响，表现出与所提供论据对齐的“谄媚”倾向，这对偏见评估有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM政治偏见的评估未充分探究当提示本身包含支持或反驳性论据时模型行为的敏感性。理解这种敏感性对于确保偏见评估的鲁棒性和理解模型与带有倾向性文本交互时的行为至关重要。

Method: 通过实验，在存在支持性和反驳性论据的情况下，对LLM进行政治偏见评估。

Result: 实验表明，论据能显著改变模型响应方向，无论是在单轮还是多轮对话中；此外，论据的强度也会影响模型响应的方向一致性。

Conclusion: LLMs表现出“谄媚”倾向，其立场会适应并与所提供的论据对齐。这种行为对衡量政治偏见和开发有效的缓解策略具有深远影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune是一种轻量级适应方法，通过低秩适应微调基于LLM的多语言TTS系统，旨在增强目标语言（日语）的发音可控性，同时保持其他语言的性能和语音质量。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的TTS模型能实现卓越的自然度，但准确建模字素到音素（G2P）映射和韵律仍具挑战，尤其当模型直接处理最小编码文本而省略显式G2P模块时。

Method: 本文提出了UtterTune，一种轻量级适应方法，利用低秩适应（low-rank adaptation）来微调基于LLM架构的多语言TTS系统。该方法使系统能够在音素层面控制目标语言（日语）的语段发音和音高重音，同时在零样本设置下保持语音的自然度和说话人相似性。

Result: 客观和主观评估均证实了UtterTune方法的有效性。

Conclusion: UtterTune成功地增强了基于LLM的多语言TTS系统对目标语言（日语）发音的控制能力，同时维护了多语言性能和语音质量，解决了G2P映射和韵律建模的挑战。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出一个自动化框架，利用多个大语言模型（LLMs）生成高质量文本解释，实验表明其在提升模型性能方面与人工标注解释具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统文本解释依赖人工标注，成本高昂、耗时且难以扩展，阻碍了可解释自然语言处理（NLP）领域的发展。

Method: 本研究构建了一个自动化框架，利用多个先进的大语言模型（LLMs）生成文本解释。通过一套全面的自然语言生成（NLG）指标评估所生成解释的质量。此外，研究还在两个基准数据集上，调查了这些解释对预训练语言模型（PLMs）和LLMs在自然语言推理任务中性能的下游影响。

Result: 实验结果表明，自动化生成的解释在提升模型性能方面，与人工标注解释表现出高度的竞争力。

Conclusion: 基于LLM的自动化文本解释生成为扩展NLP数据集和增强模型性能提供了一条有前景且可扩展的途径。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该研究通过访谈调查了从业者在实际NLP应用中对可解释方法的经验，发现存在概念差距、满意度低和评估挑战，并强调需清晰定义和用户中心框架以促进其应用。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释自然语言处理（NLP）受到日益关注，但从业者对其在实践中的采纳和有效性看法仍未被充分探索。本研究旨在填补这一空白，调查从业者使用可解释方法的经验，包括其动机、所用技术、满意度及遇到的实际挑战。

Method: 通过对行业从业者进行定性访谈，并辅以对学术研究人员的访谈，系统分析和比较了他们的观点。

Result: 研究结果揭示了概念性差距、对当前可解释性方法的低满意度，并突出了评估方面的挑战。

Conclusion: 研究结果强调，为了在实践中更好地采纳可解释NLP，需要明确的定义和以用户为中心的框架。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 本文提出BigCharts数据集生成流程及融合监督微调与强化学习的训练框架，以解决现有视觉语言模型在图表理解上的不足，并开发出性能卓越的BigCharts-R1模型。


<details>
  <summary>Details</summary>
Motivation: 图表是数据分析和决策的关键，但现有视觉语言模型因训练数据集缺乏多样性、真实性及底层数据误差，且仅依赖监督微调，导致其图表理解能力有限。

Method: 1. 提出了BigCharts数据集创建流程，通过基于真实世界图表进行条件渲染和重绘过程，生成视觉多样且底层数据准确的图表图像。2. 引入了结合监督微调和基于群组相对策略优化（GRPO）的强化学习的综合训练框架。3. 设计了专为图表推理定制的新型奖励信号。

Result: 开发出名为BigCharts-R1的先进图表推理模型。该模型在多个图表问答基准测试上超越了现有方法，甚至优于更大的开源和闭源模型。

Conclusion: 所提出的BigCharts数据集和结合强化学习的训练框架有效提升了模型的鲁棒性和泛化能力，成功构建了最先进的图表推理模型BigCharts-R1，解决了当前视觉语言模型在图表理解上的关键挑战。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 针对AI驱动心理健康助手开发所需数据集的不足，本文首次全面综述了临床心理健康数据集，揭示了当前数据的关键缺陷，并提出了未来数据集的构建和标准化建议。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康障碍日益增多，但合格临床医生数量不足，导致大量需求未被满足。尽管AI在心理健康辅助方面前景广阔，但其发展严重依赖高质量的临床训练数据集。现有数据集分散、文档不全且不易获取，阻碍了AI模型的可复现性、可比性和泛化性。

Method: 本文对与AI驱动临床助手训练和开发相关的临床心理健康数据集进行了首次全面调查。这些数据集按精神疾病、数据模态、任务类型、可访问性以及社会文化背景进行分类。同时，也对合成临床心理健康数据集进行了研究。

Result: 调查结果识别出当前数据集的关键缺陷，包括缺乏纵向数据、文化和语言代表性有限、数据收集和标注标准不一致，以及合成数据中模态的缺失。

Conclusion: 本文概述了未来数据集在整理和标准化方面的关键挑战，并提供了可操作的建议，以促进开发更稳健、更具普适性且更公平的心理健康AI系统。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本综述系统性地考察了为提高效率而设计的创新LLM架构，以克服传统Transformer模型的计算局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在LLM中表现出色并具有良好的扩展性，但其计算成本高昂，对大规模训练和实际部署构成重大障碍，因此需要更高效的架构。

Method: 本综述从语言建模出发，详细介绍了线性与稀疏序列建模方法、高效全注意力变体、稀疏专家混合模型（MoE）、结合上述技术的混合模型架构以及新兴的扩散LLMs。同时，也探讨了这些技术在其他模态中的应用。

Result: 本综述将近期研究归类，提出了一个现代高效LLM架构的蓝图，并讨论了其对开发可扩展、资源感知的基础模型的影响。

Conclusion: 本综述旨在为理解和设计高效LLM架构提供指导，并期望能激励未来研究，以开发更高效、多功能的AI系统。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出了PRELUDE基准，用于评估长文本理解能力，通过判断角色前传故事与原著叙事的一致性。实验表明，当前先进的LLM和商业服务在准确性和推理能力上均显著落后于人类，揭示了长文本理解和推理领域巨大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准对长文本的全局理解和深度推理要求不足。本文任务中，前传故事并非原著一部分，评估其合理性通常需要搜索和整合间接相关信息，且88%的实例需要从叙事多处获取证据，这比现有基准对全局理解和深度推理提出了更高要求。

Method: 引入PRELUDE基准，该基准通过判断角色前传故事与原著叙事的符合性来评估长文本理解。通过比较最先进的LLM（结合上下文学习、RAG和领域内训练）以及商业DeepResearch服务与人类在该任务上的表现来评估模型能力。

Result: 实验结果显示，最先进的LLM和商业DeepResearch服务在PRELUDE任务上的表现落后于人类超过15%。进一步的人类研究发现，模型常以错误的推理得出正确答案，导致推理准确率与人类相比存在30%以上的差距。

Conclusion: 研究结果强调了当前模型在长文本理解和深度推理方面存在显著的提升空间。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 研究评估了轻量级Whisper模型在低资源乌尔都语语音识别中的可行性，发现Whisper-Small模型性能最佳，但仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为全球主要语言，其自动语音识别（ASR）系统因方言多样性、语码转换和训练数据稀疏而受限，亟需探索轻量级模型的潜力。

Method: 研究在未进行微调的情况下，使用词错误率（WER）对Whisper Tiny、Base和Small模型在一个精选的乌尔都语数据集上进行了基准测试，并进行了定性分析。

Result: Whisper-Small模型表现出最低的词错误率（33.68%），优于Tiny（67.08%）和Base（53.67%）。定性分析显示，在语音准确性和词汇连贯性方面仍存在持续挑战，尤其对于复杂的话语。

Conclusion: Whisper-Small模型为可部署的乌尔都语ASR展示了前景，但仍存在显著差距。本研究为未来高效、低资源ASR系统的开发奠定了基础。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出Memory Decoder，一种即插即用的预训练记忆组件，无需修改大型语言模型（LLM）参数即可实现高效领域适应，同时避免了现有方法的弊端。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在通用任务中表现出色，但在特定领域适应上仍面临挑战。现有方法如领域自适应预训练（DAPT）成本高昂且存在灾难性遗忘问题；而检索增强生成（RAG）由于检索开销大，导致推理延迟显著。

Method: 引入Memory Decoder，一个即插即用的预训练记忆组件。它是一个小型transformer解码器，通过学习模仿外部非参数检索器的行为。一旦训练完成，Memory Decoder可以与任何使用相同分词器的预训练语言模型无缝集成，无需改变原始模型参数或进行模型特异性修改。

Result: 实验结果表明，Memory Decoder能有效将Qwen和Llama系列模型适应到生物医学、金融和法律三个专业领域，平均困惑度降低6.17点。

Conclusion: Memory Decoder提出了一种以预训练记忆组件为中心的新型范式，专为领域特定适应设计。这种即插即用的记忆架构能够持续提升多模型在目标领域中的性能。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本文综述了NLP在精神健康领域认知扭曲检测与分类中的应用现状，指出该领域存在的碎片化问题，并提供了统一的分类法、任务设置总结和开放性挑战，以促进研究的连贯性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理（NLP）技术在精神健康领域应用兴趣的增长，自动检测和分类认知扭曲（CDs）的研究日益增多。然而，该领域在CDs分类法、任务制定和评估实践上存在不一致性，导致研究碎片化。

Method: 本研究综述了跨越二十年的38项相关研究，提供了关于数据集、建模方法和评估策略的结构化概述。

Result: 研究结果包括提供了一个整合的认知扭曲分类参考、总结了常见的任务设置，并指出了该领域面临的开放性挑战。

Conclusion: 通过提供统一的参考和结构化分析，旨在支持该新兴领域开展更具连贯性和可复现性的研究。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 本研究探讨了数字化商业沟通中的欺骗性语言检测问题，通过结合多学科理论和使用计算文本分析及Transformer模型，在受控环境下实现了超过99%的检测准确率。研究同时指出多语言环境下的挑战，并强调了开发强大AI驱动自动文本识别系统的必要性。


<details>
  <summary>Details</summary>
Motivation: 数字化商业沟通在带来更高透明度的同时，也导致了更高级的欺骗行为。研究动机在于解释如何利用说服性词汇系统地检测欺骗性语言，以应对这种新兴挑战。

Method: 本研究综合了经典修辞学、传播心理学、语言学理论以及金融报告、可持续性话语和数字营销领域的实证研究。在受控设置中，采用计算文本分析和个性化Transformer模型来检测欺骗性语言。

Result: 在受控设置下，利用计算文本分析和个性化Transformer模型，欺骗性语言的检测准确率超过99%。然而，在多语言环境中重现这种性能存在问题，主要原因是缺乏足够的数据和多语言文本处理基础设施不足。

Conclusion: 研究表明，沟通的理论表征与经验近似之间存在日益扩大的差距。因此，在AI驱动的话语变得与人类沟通更加逼真的背景下，迫切需要强大的自动文本识别系统。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 随着LLMs广泛应用，其输出与人类价值观对齐变得至关重要。本文针对缺乏统一评估框架的问题，提出了一个多维度评估框架，用于系统比较各种LLM对齐技术，并揭示了现有SOTA模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）日益集成到实际应用中，确保其输出符合人类价值观和安全标准至关重要。然而，现有对齐方法缺乏统一的评估框架，导致难以系统比较不同对齐范式并指导部署决策。

Method: 本文引入了一个多维度LLM对齐技术评估框架，旨在对所有主要的对齐范式进行系统比较。该框架从对齐检测、对齐质量、计算效率和鲁棒性四个关键维度评估方法。

Result: 通过在不同基础模型和对齐策略上的实验，该框架成功揭示了当前最先进模型在对齐方面的优势和局限性。

Conclusion: 该评估框架为未来的LLM对齐研究方向提供了宝贵的见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）在多模态代码生成能力上存在局限。本文提出VisCodex框架，通过任务向量合并技术融合视觉与代码LLM，并构建了大型多模态代码数据集（MCD）和评估基准（InfiBench-V）。实验证明VisCodex在开源MLLM中性能领先，并接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在视觉和文本理解的整合方面取得了显著进展，但它们从多模态输入生成代码的能力仍然有限。

Method: 引入VisCodex框架，一个统一融合视觉与编码语言模型以增强MLLMs多模态代码生成能力的方案。采用基于任务向量的模型合并技术，将先进的编码LLM集成到强大的视觉-语言骨干中，同时保持视觉理解和高级编码技能。此外，构建了大型多模态编码数据集（MCD）（59.8万样本）和新颖的基准InfiBench-V，以支持训练和评估。

Result: VisCodex在开源MLLMs中取得了最先进（SOTA）的性能，并接近GPT-4o等专有模型。这突显了所提出的模型合并策略和新数据集的有效性。

Conclusion: 通过创新的VisCodex框架、任务向量模型合并策略以及新构建的大规模数据集和挑战性基准，可以显著提升多模态大语言模型的代码生成能力，使其在视觉丰富的编程任务上达到领先水平，并有望媲美顶尖闭源模型。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 该研究系统比较了通用、医学和领域特定分词器在放射学报告摘要任务中的表现，发现领域特定分词器在性能和计算效率方面均表现最佳，尤其是在从头训练的模型上。


<details>
  <summary>Details</summary>
Motivation: 语言模型分词器对文本生成质量至关重要，但其在放射学领域的具体影响尚未得到充分探索。

Method: 研究者系统比较了通用、医学和领域特定分词器在三种影像模态的放射学报告摘要任务中的表现。同时，他们也探究了有无PubMed摘要预训练对模型性能的影响。

Result: 当模型从头训练时，医学和领域特定词汇的分词器优于常用的自然语言分词器。预训练虽然部分缓解了分词器间的性能差异，但领域特定分词器依然表现最佳。此外，领域特定分词器因词汇量更小和序列更短，还能减少内存需求。

Conclusion: 将语言模型的词汇适应于临床领域（如放射学），可以带来实际益处，包括性能提升和计算需求降低，从而使这些模型在研究和实际医疗场景中更易于访问和使用。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 为解决情感分析的固有模糊性，本文提出通过添加合理上下文的方法，并利用故事生成技术构建数据集进行验证，结果表明上下文叙事能显著提高人类标注者情感判断的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析是一项固有的模糊任务。以往研究多侧重于标注者属性来解释分歧，但忽略了模糊性可能源于事件情境信息缺失。

Method: 提出一种新方法：通过自动生成基于不同情感的多个事件链（连贯叙事），为目标事件描述添加合理的上下文。该方法结合了短故事生成技术，并据此构建了一个专门的数据集，用于系统性地考察情境化情感分析。

Result: 通过自动和人工评估，研究发现情境化叙事能够增强特定情感的解读，并支持标注者产出更一致的标注结果。

Conclusion: 为事件描述补充情境上下文能够有效解决情感分析中的模糊性问题，并提高人类标注者进行情感标注的可靠性和一致性。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: 研究评估了GPT-5系列模型在眼科医学问答任务中的表现，发现GPT-5-high在准确性上表现最佳，而GPT-5-mini-low则提供了最佳的成本效益平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）如GPT-5展现出先进的推理能力，有望提高复杂医学问答的性能。然而，对于最新一代的推理模型，如何配置以最大化准确性和成本效益尚不明确。

Method: 研究评估了OpenAI GPT-5系列的12种配置（三个模型层级和四种推理努力设置），以及o1-high、o3-high和GPT-4o。使用了美国眼科学会基础临床科学课程（BCSC）数据集中260个多项选择题。主要评估指标是多项选择题的准确性，次要指标包括通过Bradley-Terry模型进行的头对头排名、使用参考锚定的LLM-as-a-judge框架进行的推理质量评估，以及基于token成本估算的准确性-成本权衡分析。

Result: GPT-5-high取得了最高准确率（0.965），优于所有GPT-5-nano变体、o1-high和GPT-4o，但与o3-high（0.958）无显著差异。GPT-5-high在准确性和推理质量方面均排名第一（分别比o3-high强1.66倍和1.11倍）。成本-准确性分析识别出多个GPT-5配置在帕累托前沿，其中GPT-5-mini-low提供了最有利的低成本、高性能平衡。

Conclusion: 这些结果对GPT-5在高质量眼科数据集上的性能进行了基准测试，证明了推理努力对准确性的影响，并引入了一种自动评分框架，用于可扩展地评估LLM在眼科领域生成答案与参考标准的一致性。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 为资源匮乏的Badini库尔德语方言开发并评估了语音转文本（STT）系统，发现基于Wav2Vec2-Large-XLSR-53的模型在准确性和可读性方面表现显著优于Whisper-small。


<details>
  <summary>Details</summary>
Motivation: 虽然语音转文本系统在多种语言中广泛应用，但对于Badini等资源匮乏的库尔德语方言，相关系统仍是空白。鉴于Badini方言约有两百万使用者，开发STT系统能帮助其社区使用移动和计算机技术，并提升该方言的全球可见度。

Method: 研究选择Badini儿童故事集（8本书，78个故事，约17小时录音）作为文本输入，由六位叙述者录制。数据经过清洗、分段和标记化预处理，得到约15小时的语音数据。使用Wav2Vec2-Large-XLSR-53和Whisper-small模型开发语言模型并进行性能评估。

Result: 实验结果显示，基于Wav2Vec2-Large-XLSR-53模型的转录过程提供更准确和可读的输出。Wav2Vec2-Large-XLSR-53模型的可读性达到90.38%，准确率为82.67%；而Whisper-small模型的可读性为65.45%，准确率为53.17%。

Conclusion: 本研究成功填补了Badini库尔德语方言在语音转文本系统方面的空白。Wav2Vec2-Large-XLSR-53模型被证明是为Badini语音创建高效和高精度语言模型的更优选择，显著优于Whisper-small模型，为Badini社区提供了实用的STT解决方案。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经上下文多臂赌博机（neural contextual bandit）的算法，用于在线选择一系列大型语言模型（LLMs）来解决需要分解为多个子任务的复杂查询，尤其是在缺乏历史数据的情况下，以优化性能和成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，如何低成本地预测哪个LLM能成功完成任务变得日益重要。然而，有些任务对单个LLM而言过于专业和困难，更适合分解为多个子任务。现有LLM选择算法仅针对单个LLM，无法处理子任务之间相互依赖的序列式LLM选择，这种序列选择中每个LLM的输出会影响后续LLMs的输入、成本和成功率，形成了复杂的性能依赖性。

Method: 研究者提出了一种基于神经上下文多臂赌博机（neural contextual bandit）的算法。该算法通过训练神经网络，以在线方式建模每个子任务上LLM的成功率，从而学习如何指导不同子任务的LLM选择，即使在缺乏历史LLM性能数据的情况下也能有效运作。

Result: 在电信问答和医疗诊断预测数据集上的实验表明，与其他的LLM选择算法相比，所提出的方法表现出更高的有效性。

Conclusion: 该研究成功地解决了复杂多子任务情境下，序列式LLM选择的挑战，其提出的神经上下文多臂赌博机算法在没有历史数据的情况下也能有效指导LLM选择，优化了任务的成功率和成本。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的多模态框架，用于检测社交媒体中的厌女和性别歧视内容，并在两个多模态数据集上取得了显著优于现有方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上大量攻击性内容针对女性，而现有的通用攻击性内容检测方法在检测厌女内容时面临挑战，因此需要专门针对女性攻击性内容的解决方案。

Method: 研究提出了一种新颖的多模态框架，包含三个模块：多模态注意力模块 (MANM) 用于生成上下文相关特征，图基特征重构模块 (GFRM) 用于细化模态内特征，以及内容特定特征学习模块 (CFLM) 用于学习文本和图像特定特征（如毒性特征和标题特征）。此外，还整理了厌女词汇集以计算厌女特定词汇分数，并应用特征空间中的测试时增强来提高泛化能力。

Result: 该方法在MAMI和MMHS150K两个多模态数据集上进行了评估，与现有方法相比，在macro-F1指标上分别平均提高了10.17%和8.88%。

Conclusion: 所提出的多模态框架能有效检测厌女和性别歧视内容，并在性能上显著超越了现有方法，证明了其在解决该特定问题上的优越性。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [47] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: 提出IAD-R1框架，通过两阶段后训练显著提升VLMs在工业异常检测中的泛化和解释能力，解决了小样本限制，并超越商业大模型。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法受限于缺陷样本稀缺，适用场景受限。尽管视觉-语言模型（VLMs）具有泛化优势，但在工业异常检测领域的性能仍有待提升。

Method: 提出IAD-R1通用后训练框架，适用于不同VLM架构和参数规模。采用两阶段训练策略：1) 感知激活监督微调（PA-SFT），利用高质量Chain-of-Thought数据集(Expert-AD)训练，增强异常感知与推理关联。2) 结构化对照组相对策略优化（SC-GRPO），通过设计奖励函数，实现从“异常感知”到“异常解释”的能力提升。

Result: IAD-R1在7个VLM上实现显著性能提升，在6个工业异常检测基准数据集上平均准确率提高43.3%。经IAD-R1训练的0.5B参数模型在零样本设置下性能超越了GPT-4.1和Claude-Sonnet-4等商业大模型。

Conclusion: IAD-R1框架有效解决了VLM在工业异常检测中性能受限的问题，显著提升其泛化和解释能力，即使是小型模型也能超越现有商业大模型，证明了其有效性和优越性。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [48] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: 本文提出CADAR，一种新型神经符号方法，用于增强现实（AR）中的认知攻击检测。它结合视觉-语言模型和粒子滤波进行语义推理，解决了现有方法在可解释性和语义推理方面的不足，并在实验中显著提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AR日益普及，利用修改AR内容来操纵用户语义感知的认知攻击日益受到关注。现有检测方法要么局限于像素级/图像级处理，缺乏语义推理能力；要么依赖预训练的视觉-语言模型（VLMs），但缺乏可解释性。

Method: CADAR是一种神经符号方法，融合多模态视觉-语言输入，通过神经VLMs获得符号感知图，并结合先验知识、显著性加权和时间关联。随后，利用基于粒子滤波的统计推理（一种序列蒙特卡洛方法）来检测认知攻击。

Result: 在扩展的AR认知攻击数据集上的实验表明，相比于强大的基线模型，CADAR在具有挑战性的AR攻击场景下，准确率提高了高达10.7%。

Conclusion: 神经符号方法在实现有效且可解释的AR认知攻击检测方面具有巨大潜力。CADAR融合了预训练VLM的适应性与粒子滤波的可解释性和推理严谨性，证明了其在未来AR安全领域的应用前景。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [49] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 针对智能交通系统中视觉数据与隐私的冲突，本文提出RL-MoE框架，将敏感视觉数据转化为隐私保护的文本描述。该框架结合MoE进行场景分解和RL优化文本生成，实现在保证语义准确性的同时，有效提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: AI摄像头在智能交通系统（ITS）中普及，导致对丰富视觉数据的需求与个人隐私权之间存在严重冲突。现有隐私保护机制（如模糊、加密）往往不足，导致隐私泄露风险或数据效用显著降低，亟需更有效的解决方案。

Method: 本文提出RL-MoE框架，其核心是将敏感视觉数据转换为隐私保护的文本描述，从而避免直接传输图像。该方法独特地结合了：1) 专家混合（MoE）架构，用于细致、多方面的场景分解；2) 强化学习（RL）智能体，优化生成的文本，以实现语义准确性和隐私保护的双重目标。

Result: 广泛的实验表明，RL-MoE提供了卓越的隐私保护，在CFP-FP数据集上将重放攻击的成功率降低至仅9.4%。同时，它比基线方法生成了更丰富的文本内容。

Conclusion: RL-MoE为在隐私敏感领域构建可信赖的AI系统提供了一个实用且可扩展的解决方案，为更安全的智慧城市和自动驾驶网络奠定了基础。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [50] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 该研究提出了一种结合优化GAN和遗传算法的框架，用于生成高质量、多样化的合成深度人脸数据，以解决细微情感识别中数据集匮乏的挑战，并在分类任务中取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 情感计算面临的主要挑战是：缺乏高质量、多样化的深度人脸数据集，以用于识别细微情感表达。

Method: 提出了一种合成深度人脸生成框架，该框架采用优化过的GAN结合知识蒸馏（EMA教师模型）以稳定训练、提高质量并防止模式崩溃。同时，应用遗传算法基于图像统计量来演化GAN潜在向量，以提升目标情感的多样性和视觉质量。对于分类，提取并串联了LBP、HOG、Sobel边缘和强度直方图特征，并使用XGBoost进行分类。

Result: 该方法在多样性和质量上均优于GAN、VAE、GMM和KDE。使用XGBoost进行分类时，达到了94%和96%的准确率。FID、IS、SSIM和PSNR等评估指标显示，该方法持续优于现有最先进方法。

Conclusion: 该框架通过生成高质量、多样化的合成深度人脸数据，成功解决了情感计算中数据集缺乏的挑战，并在情感识别分类任务中取得了显著性能提升。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [51] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [52] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 提出PFT方法，在仅有中性表情目标数据的情况下，通过潜在空间特征翻译，解决了面部表情识别（FER）模型在无源域适应中的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度FER模型在识别细微表情和处理受试者间差异时表现受限，尤其是在无源域适应（SFDA）场景下，如果仅有无标签的中性表情目标数据可用，现有SFDA方法难以适应，且生成非中性表情图像既不稳定又计算密集。

Method: 提出个性化特征翻译（PFT）方法，用于SFDA。该方法在潜在空间而非图像空间操作。首先，在源域数据上预训练一个翻译器，用于转换主体特定风格特征，同时通过表情一致性和风格感知目标来保留表情信息。然后，仅使用中性目标数据对翻译器进行适应，无需源数据或图像合成。

Result: PFT方法避免了面部表情生成的复杂性和噪声，能够生成用于分类的区分性嵌入。它无需图像合成，显著减少了计算开销（使用轻量级翻译器），并且仅适应模型的一部分，使其比基于图像的翻译方法更高效。

Conclusion: PFT通过在潜在空间进行特征翻译，为仅有中性表情目标数据的挑战性SFDA场景提供了一种高效、有效的解决方案，无需图像合成，从而提升了FER模型在该特定应用场景下的性能。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [53] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 本研究评估了多种AI模型在动漫草图自动上色任务中的表现，发现C-GAN能生成高质量、高分辨率的图像，并被认定为最有效模型。


<details>
  <summary>Details</summary>
Motivation: 漫画和动漫产业中，从草图生成全彩绘图是一个巨大且通常成本高昂的瓶颈。

Method: 研究检验了包括神经风格迁移（Neural Style Transfer）、C-GAN和CycleGAN在内的多种图像到图像转换模型，并对其进行了定性和定量评估。

Result: 通过评估发现，C-GAN是能够生成接近人类创作水平的高质量、高分辨率图像的最有效模型。

Conclusion: C-GAN在自动上色任务中表现卓越，有望缓解漫画和动漫产业的生产瓶颈。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [54] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了MME-Emotion，一个大规模且系统性的多模态大语言模型（MLLMs）情感智能基准，用于评估其情感理解和推理能力，并揭示了当前MLLMs在此领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在情感计算方面取得了显著进展，但现有的情感基准仍有局限性，未能充分评估MLLMs在不同场景下的泛化能力以及识别情感触发因素的推理能力。

Method: 研究者构建了MME-Emotion基准，包含超过6,000个精选视频片段和任务特定的问答（QA）对，涵盖八项情感任务，旨在全面评估MLLMs的情感理解和推理能力。该基准具有可扩展性、多样化设置和统一协议，并整合了采用混合指标和多智能体系统框架的整体评估套件。

Result: 通过对20个先进MLLMs的严格评估发现：1) 当前MLLMs的情感智能表现不尽人意，最佳模型在识别任务上仅获得39.3%的得分，在思维链（CoT）任务上获得56.0%的得分。2) 通用模型（如Gemini-2.5-Pro）的情感智能来源于其泛化多模态理解能力，而专业模型（如R1-Omni）可通过领域特定的后期训练适应达到可比性能。

Conclusion: MME-Emotion基准的引入，有望为未来提升MLLMs的情感智能研究奠定基础。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [55] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 本研究旨在解决多模态大语言模型（MLLMs）对抗性提示的漏洞及其现有评估标准的不足。我们提出了一个四轴评估框架，并基于对提示特性与检测之间权衡的洞察，开发了一种名为“平衡结构分解（BSD）”的递归重写策略，该策略显著提高了攻击成功率和有害输出的生成，揭示了当前MLLM安全系统的薄弱点。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-语言推理任务中广泛应用，但其对对抗性提示的脆弱性是一个严重问题，安全机制常无法阻止有害内容的生成。此外，现有越狱策略虽然报告成功率高，但许多“成功”响应实际上是良性、模糊或与预期恶意目标无关的，这表明当前评估标准可能高估了攻击的有效性。

Method: 1. 引入一个四轴评估框架，考量输入主题相关性、输入域外（OOD）强度、输出有害性以及输出拒绝率，以识别真正有效的越狱。2. 基于研究发现（即平衡相关性和新颖性的提示更能规避过滤器并触发危险输出），开发了一种名为“平衡结构分解（BSD）”的递归重写策略。该方法将恶意提示重构为语义对齐的子任务，同时引入微妙的OOD信号和视觉线索，使输入更难被检测。

Result: 1. 经验研究揭示了结构性权衡：高度相关提示常被安全过滤器阻止，而过于域外（OOD）的提示虽能规避检测但未能产生有害内容；然而，平衡了相关性和新颖性的提示更有可能规避过滤器并触发危险输出。2. BSD策略在13种商业和开源MLLMs上进行了测试，结果显示其持续提高攻击成功率，产生更多有害输出，并减少拒绝。3. 与现有方法相比，BSD将成功率提高了67%，有害性提高了21%。

Conclusion: 本研究揭示了当前多模态安全系统一个此前未被充分认识的弱点。BSD策略通过平衡提示的相关性和新颖性，能够有效规避现有MLLMs的安全机制，从而提高攻击成功率并生成更多有害内容，证明了当前安全系统在应对复杂对抗性攻击方面的不足。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [56] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 为解决手写数学表达式识别（HMER）数据稀缺问题，本文提出了一种数据引擎以生成大规模合成公式数据集Tex80M，并训练了首个大规模HMER模型TexTeller，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型受益于大规模数据训练，而手写数学表达式识别（HMER）领域因数据标注成本高昂导致数据稀缺，阻碍了其发展。

Method: 开发了一个可扩展的数据引擎，用于生成复杂且一致的LaTeX序列，并将有限的手写公式与大规模LaTeX渲染公式相结合。构建了迄今为止最大的公式数据集Tex80M（超过8000万个实例）。提出TexTeller模型，通过将Tex80M与少量真实手写数学表达式数据混合训练，实现了大规模训练。

Result: TexTeller在几乎所有基准测试中都取得了最先进（SOTA）的性能。成功构建了包含超过8000万个高质量训练实例的迄今最大公式数据集Tex80M。

Conclusion: 通过创新性地结合合成数据生成和混合训练策略，TexTeller模型在手写数学表达式识别领域取得了显著突破。为促进领域发展，模型、数据集和代码库将公开发布。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [57] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS通过梯度方向感知密度控制，解决3DGS中的过重建和过密化问题，提高渲染质量并显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting (3DGS) 方法在复杂场景中存在两个主要限制：1) 过重建，即大型高斯球因梯度方向冲突而无法有效分裂，导致细节丢失；2) 过密化，即在梯度一致区域高斯球过度密集，造成冗余和内存开销增加。

Method: 本文提出Gradient-Direction-Aware Gaussian Splatting (GDAGS)，一种梯度方向感知自适应密度控制框架。核心创新包括：1) 梯度一致性比率(GCR)，用于区分梯度方向一致或冲突的高斯球；2) 非线性动态加权机制，利用GCR进行密度控制。具体而言，GDAGS在分裂时优先处理梯度冲突的高斯球以增强细节，抑制冗余的梯度一致高斯球；在克隆时促进梯度一致高斯球的密化，同时防止梯度冲突高斯球过度增殖。

Result: 在多样化的真实世界基准测试中，GDAGS实现了卓越的渲染质量，有效缓解了过重建，抑制了过密化，并通过优化高斯球利用率构建了更紧凑的场景表示，内存消耗降低了50%。

Conclusion: GDAGS通过其创新的梯度方向感知密度控制，显著提升了3DGS在复杂场景中的性能，成功解决了过重建和过密化问题，实现了更高的渲染质量和更低的内存占用，构建了更高效的场景表示。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [58] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: 引入FineState-Bench，首个针对GUI代理精细操作的评估与诊断基准，揭示当前GUI代理的主要瓶颈在于基础视觉定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的评估框架存在根本缺陷，过度关注粗粒度任务完成，而忽视了实际应用中关键的精细控制能力。

Method: 本文提出了FineState-Bench，这是一个多平台（桌面、Web、移动）的精细GUI代理操作评估与诊断标准，包含2257个任务基准和四阶段指标。此外，开发了即插即用式视觉诊断助手（VDA），首次实现了对感知和定位能力的定量解耦分析。

Result: 实验结果显示，最先进的模型在FineState-Bench上的精细交互准确率仅为32.8%。通过VDA进行的受控实验表明，理想的视觉定位可将Gemini-2.5-Flash的成功率提高14.9%。

Conclusion: 首次证实，当前GUI代理的主要瓶颈在于其基础视觉定位能力不足。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [59] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: 提出FiGPriv框架，通过细粒度隐私保护平衡视觉语言模型在视觉辅助系统中的隐私和可用性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型驱动的视觉辅助系统日益普及，导致用户隐私担忧，特别是视障用户可能无意中捕获个人隐私信息。现有隐私保护方法依赖粗粒度分割，统一遮蔽整个私密对象，牺牲了可用性。

Method: 提出FiGPriv，一个细粒度隐私保护框架。该框架选择性地遮蔽高风险隐私信息，同时保留低风险信息。它将细粒度分割与数据驱动的风险评分机制相结合。

Result: 使用BIV-Priv-Seg数据集进行评估，FiG-Priv在确保隐私保护的同时，保留了+26%的图像内容，将VLM提供有用响应的能力提高了11%，图像内容识别率提高了45%。

Conclusion: FiGPriv通过细粒度、基于风险的遮蔽方法，有效解决了视觉辅助系统中隐私保护与可用性之间的矛盾，显著提升了用户体验和系统效能。

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [60] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出一种输入自适应导航方法，通过优化空间、模型内部和时间处理，显著提升视觉-语言导航（VLN）模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的历史感知多模态Transformer模型在VLN任务中表现出色，但其庞大的模型规模在计算资源受限的实际应用中构成瓶颈。

Method: 本研究提出了一种新颖的输入自适应导航方法，以提高VLN模型的效率。具体包括三个层面的自适应算法：1) 提升空间效率，通过选择性处理代理在每次观察时的全景视图；2) 提升模型内部效率，通过为提前退出方法引入基于重要性的自适应阈值；3) 提升时间效率，通过实施缓存机制以避免重复处理已观察过的视图。

Result: 在七个VLN基准测试中，对三种现有代理在标准和连续环境中进行评估，结果显示计算量减少了两倍以上。

Conclusion: 所提出的输入自适应导航方法能显著提高VLN模型的计算效率，有效解决了大规模模型在资源受限环境下的性能瓶颈问题。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [61] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC是一种面向视觉强化学习的新方法，它结合了SAM和YOLO-World进行对象分解与语义定位，并采用新颖的Transformer架构，在不使用人工标签的情况下，通过在线强化学习学习关注关键片段。该方法在视觉泛化方面显著优于现有技术，同时保持或超越了样本效率。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习面临从高维输入和噪声奖励中学习感知和动作的挑战。尽管存在大型感知模型，但如何有效地将其整合到强化学习中以实现视觉泛化和提高样本效率仍不明确。

Method: 本文提出SegDAC（Segmentation-Driven Actor-Critic）方法。它利用Segment Anything (SAM) 进行以对象为中心的分解，并使用YOLO-World通过文本提示对语义片段进行定位。该方法包含一种新颖的基于Transformer的架构，支持每个时间步的动态片段数量，并通过在线强化学习有效学习关注哪些片段，无需人工标注。

Result: SegDAC在Maniskill3上的挑战性视觉泛化基准（包含强视觉扰动下的多种操作任务）上进行了评估，结果表明其实现了显著优越的视觉泛化能力，在最困难的设置下将先前性能提高了一倍，并且在所有评估任务中达到或超越了现有方法的样本效率。

Conclusion: SegDAC通过有效地整合大型感知模型，显著提升了视觉强化学习的视觉泛化能力和样本效率，尤其是在具有挑战性的视觉扰动场景下，为构建更鲁棒的视觉强化学习系统提供了有效途径。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [62] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: 本文提出Lung-DDPM+，一种改进的去噪扩散概率模型，通过语义布局引导和加速求解器，高效生成高质量肺结节CT图像，显著提升效率并保持样本质量，有望应用于多种医学图像生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在肺癌诊断中存在效率低和解剖不精确的问题，限制了其临床应用。

Method: 提出Lung-DDPM+，它是Lung-DDPM的改进版，一种通过结节语义布局引导并由肺部DPM-solver加速的去噪扩散概率模型（DDPM）。

Result: Lung-DDPM+在LIDC-IDRI数据集上FLOPs减少8倍，GPU内存消耗降低6.8倍，采样速度快14倍，同时在两个下游分割任务中保持与Lung-DDPM及其他SOTA模型相当的样本质量。经验丰富的放射科医生进行的视觉图灵测试也证实了其合成样本的高级质量和保真度。

Conclusion: Lung-DDPM+能有效生成高质量的含肺结节胸部CT图像，展现了其在通用肿瘤合成和医学图像病变生成等更广泛应用中的潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [63] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 利用超轻量级Med-Vision Mamba模型，实现结肠镜检查中癌前息肉的精准分类，以优化患者预后。


<details>
  <summary>Details</summary>
Motivation: 在常规结肠镜检查中识别癌前息肉对其切除至关重要，能降低患结直肠癌的风险。先进的深度学习算法可以提高腺瘤分类和分层的准确性，从而改善风险评估并实现个性化监测方案。

Method: 采用基于状态空间模型（SSM）的超轻量级Med-Vision Mamba模型，该模型擅长建模长短程依赖和图像泛化，并具有高效的架构。

Result: 超轻量级Med-Vision Mamba模型在建模长短程依赖和图像泛化方面表现出色，尤其适用于全玻片图像分析，并在计算速度和可扩展性方面具有优势。

Conclusion: 超轻量级Med-Vision Mamba模型因其卓越的性能和高效性，是用于实时临床部署的有前景的工具。

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [64] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 本研究提出了一种将自愿眨眼实时转换为摩尔斯电码的系统，旨在为严重运动障碍者提供辅助交流能力。


<details>
  <summary>Details</summary>
Motivation: 为严重运动障碍者提供一种可行的交流方法。

Method: 系统利用标准网络摄像头和计算机视觉技术，检测并区分短眨（点）和长眨（划），随后将其解码为字母数字字符。

Result: 对五名参与者的实验显示，系统解码准确率为62%，响应时间为18-20秒。

Conclusion: 该系统证明是一种可行且低成本的辅助交流方法。

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [65] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出FusionEnsemble-Net模型，通过多模态数据（RGB视频和雷达）的注意力机制融合与时空网络集成，显著提升了医疗健康场景下手语识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医疗健康交流中手语的准确识别面临巨大挑战，需要能够精确解释复杂多模态手势的框架。

Method: 本文提出了FusionEnsemble-Net，一种新颖的基于注意力机制的时空网络集成模型。该方法同步处理RGB视频和雷达多普勒图两种模态数据，通过四个不同的时空网络。每个网络内部，利用注意力机制融合模块持续融合两种模态的特征，随后送入分类器集成。最后，将四个不同融合通道的输出在一个集成分类头中结合，以增强模型的鲁棒性。

Result: 实验结果表明，FusionEnsemble-Net在大型MultiMeDaLIS意大利手语数据集上，测试准确率达到99.44%，性能优于现有先进方法。

Conclusion: 研究发现，由多样化时空网络组成的集成模型，结合基于注意力机制的多模态融合，为复杂、多模态的独立手势识别任务提供了一个鲁棒且准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [66] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 本研究提出一种双架构框架，包括Signer-Invariant Conformer和Multi-Scale Fusion Transformer，旨在解决连续手语识别（CSLR）中手语者差异和对未见句子泛化能力差的问题，并在Isharah-1000数据集上取得了显著优于现有技术的结果，为CSLR研究设立了新基线。


<details>
  <summary>Details</summary>
Motivation: 连续手语识别（CSLR）面临手语者间显著差异和对新句子结构泛化能力差的挑战，而传统解决方案难以有效应对这些问题。

Method: 研究提出一个双架构框架：1. 针对手语者无关性（SI）挑战，设计Signer-Invariant Conformer，结合卷积与多头自注意力机制，从姿态骨骼关键点中学习鲁棒、手语者无关的表示。2. 针对未见句子（US）任务，设计Multi-Scale Fusion Transformer，其包含新颖的双路径时间编码器，能捕捉细粒度姿态动态。

Result: 在Isharah-1000数据集上，SI挑战中的Conformer架构实现了13.07%的词错误率（WER），较现有最佳水平降低13.53%。US任务中的Transformer模型WER为47.78%，超越了以往工作。在SignEval 2025 CSLR挑战中，该团队在US任务中排名第2，在SI任务中排名第4。

Conclusion: 为CSLR的特定挑战开发任务专用网络能够显著提高性能，并为未来的研究建立了新的基线。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [67] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 研究医疗图像分割中的标注者变异性，构建多标注者数据集IMA++，发现标注者间一致性与病变恶性程度显著相关，并将其作为软特征提升了皮肤病变分割和分类的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割中存在显著的标注者间/内变异性，尤其是在边界模糊或恶性病变处。这种变异性影响分割质量，研究旨在深入探究其成因与影响，并利用其提升模型性能。

Method: 构建了大型多标注者皮肤病变分割数据集IMA++。在此数据集上，深入研究了标注者、恶性程度、工具和技能等因素导致的变异性。进而，从皮肤镜图像中预测标注者间一致性（IAA），并将其作为“软”临床特征整合到多任务学习目标中。

Result: 发现标注者间一致性（IAA）与皮肤病变恶性程度存在统计学上显著（p<0.001）的关联。IAA可以从皮肤镜图像中准确预测，平均绝对误差为0.108。利用IAA作为特征，在多个模型和数据集上，平衡准确率平均提高了4.2%。

Conclusion: 标注者间一致性与皮肤病变恶性程度密切相关，并且可以作为一种有效的“软”临床特征，显著提升医疗图像（特别是皮肤病变）分割和分类任务的性能。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [68] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion是一种统一且富有表现力的全身人体运动隐式潜在表示，能够实现高保真、跨身份的全身运动迁移。


<details>
  <summary>Details</summary>
Motivation: 现有运动迁移方法依赖显式骨骼姿态和启发式调整，难以实现针对面部表情、身体姿态和手势等全身多粒度运动的高保真、精细化、跨身份迁移。

Method: 该研究提出了X-UniMotion，通过自监督的端到端框架，将单张图像中的多粒度运动编码为四个解耦的潜在令牌（面部、身体、两只手），这些令牌具有表达力且与身份无关。框架结合了基于DiT的视频生成模型，在大规模数据集上训练。通过2D空间和颜色增强，以及共享姿态下跨身份主体对的合成3D渲染，强制实现运动与身份解耦，并利用辅助解码器引导精细、语义对齐和深度感知的运动嵌入学习。

Result: 实验表明，X-UniMotion超越了现有先进方法，能够生成高表现力、卓越运动保真度和身份保持的动画效果。

Conclusion: X-UniMotion通过其创新的统一、解耦隐式潜在表示，为高保真、细节丰富的全身跨身份运动迁移提供了一种有效且性能优越的解决方案。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [69] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2针对SAR目标检测中的相干噪声问题，提出一种变换域特征调制方法，通过互补利用幅度和相位信息实现去噪，达到SOTA性能并降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达（SAR）目标检测中相干噪声的普遍影响是一个主要挑战。现有方法大多通过分析或增强目标空间域特征来隐式去噪，可能未能充分解决相干噪声问题。

Method: 本文提出了DenoDet V2，它从一个全新的视角出发，通过精心设计的注意力架构，在变换域中解构和调制特征。相较于DenoDet V1，DenoDet V2是一个重大改进，它通过频带间相互调制机制，利用幅度和相位信息的互补性，实现了相位谱和幅度谱的相互增强。

Result: DenoDet V2在各种SAR数据集上进行了广泛实验，展示了最先进的性能。特别地，在SARDet-100K数据集上，DenoDet V2比DenoDet V1的性能提升了0.8%，同时模型复杂度降低了一半。

Conclusion: DenoDet V2通过创新性地在变换域中对特征进行去噪和调制，并有效利用幅度和相位信息的互补性，成功解决了SAR目标检测中的相干噪声挑战，实现了卓越的性能提升和显著的模型效率优化。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [70] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出SkyShield，一个基于事件驱动的端到端框架，用于检测无人机在复杂环境中难以识别的亚毫米级细小障碍物，通过轻量级U-Net和创新的损失函数实现了高精度和低延迟，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂环境中面临亚毫米级细小障碍物（如钢丝、风筝线）的严重威胁，而传统传感器（如RGB相机、LiDAR、深度相机）难以有效检测这些障碍物。

Method: 本文引入了SkyShield，一个事件驱动的端到端框架，利用细小障碍物在事件流中的独特特征。该方法采用轻量级U-Net架构和创新的Dice-Contour正则化损失函数来确保精确检测。

Result: 实验结果表明，该事件驱动方法实现了0.7088的平均F1分数，并具有21.2毫秒的低延迟。

Conclusion: SkyShield能够有效感知亚毫米级障碍物，其高性能和低延迟使其非常适合部署在边缘和移动平台上。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [71] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 一种低成本、本地部署的自动后院鸟类监测系统。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种经济实惠、保护隐私且无需云服务的自动鸟类监测解决方案，以支持公民科学。

Method: 系统采用运动触发的IP摄像头，将短视频通过FTP上传至本地服务器。服务器使用Detectron2定位鸟类，随后通过在比利时40种鸟类数据集上微调的EfficientNet-B3模型对裁剪区域进行分类。所有处理均在不带独立GPU的普通硬件上运行。物理喂食器设计有30毫米小入口以排除鸽子。

Result: 检测器引导的裁剪显著提高了分类准确性。分类器在精心整理的验证集上达到了约99.5%的高性能，并在实际现场应用中对未见过物种实现了约88%（top-1）的准确率。

Conclusion: 该系统证明了在家中进行公民科学级别的生物多样性记录的可行性。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [72] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 提出Waymo-3DSkelMo数据集，一个大规模、高质量、时间连贯的3D骨骼运动数据集，包含显式交互语义，用于自动驾驶中行人行为理解。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中，数据驱动模型需要大规模、高质量、包含多人交互的3D运动数据集来理解行人行为。然而，现有数据集多依赖单目RGB视频，存在遮挡、时间不连贯等问题，导致运动质量低且不真实。

Method: 利用3D人体形状和运动先验知识，提高从原始LiDAR点云中提取的3D姿态序列的质量。

Result: 建立了Waymo-3DSkelMo数据集，其是首个大规模、高质量、时间连贯且包含显式交互语义的3D骨骼运动数据集。该数据集覆盖超过14,000秒，包含800多个真实驾驶场景，平均每场景27个智能体（最多250个）。此外，还建立了3D姿态预测基准。

Conclusion: Waymo-3DSkelMo数据集是未来在复杂城市环境中进行细粒度人类行为理解研究的宝贵基础资源。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [73] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 本文提出RampNet双阶段管道，通过自动转换政府数据构建大规模高质量城市坡道检测数据集（逾21万张），并基于此训练出性能卓越（0.9236 AP）的检测模型，显著超越现有技术，解决了数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 城市坡道检测对城市无障碍至关重要，但缺乏大规模高质量数据集导致其在图像中鲁棒检测仍是开放问题。现有众包或手动标注方法在质量或规模上不足。

Method: 引入RampNet双阶段管道：
1. 数据集生成：将政府提供的坡道位置数据自动转换为Google街景全景图中的像素坐标，生成超过21万张标注图像的数据集。
2. 模型训练：利用生成的数据集训练一个改进的ConvNeXt V2坡道检测模型。
评估方法：通过与手动标注的全景图进行比较来验证数据集和模型的性能。

Result: 生成的数据集取得了94.0%的精度和92.5%的召回率。训练的检测模型达到了0.9236 AP，远超现有研究成果。

Conclusion: 本研究首次贡献了大规模、高质量的城市坡道检测数据集、基准和模型。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [74] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL是一个基于生成流的框架，通过融合LLM增强的语义地图，提升智能体在未知环境中进行物体目标导航的泛化能力，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 物体目标导航（ObjectNav）任务要求智能体在未知环境中定位指定物体。现有方法依赖确定性判别模型完成语义地图，但忽视了室内布局的不确定性，限制了对新环境的泛化能力。

Method: 提出了GOAL，一个生成流框架，通过结合观测区域与LLM（大型语言模型）增强的完整场景语义地图来建模室内环境的语义分布。训练时，从LLM推断出的空间先验被编码为二维高斯场并注入到目标地图中，从而将丰富的上下文知识提炼到流模型中，实现更强的泛化完成。

Result: 广泛的实验表明，GOAL在MP3D和Gibson数据集上达到了最先进的性能，并在迁移设置中对HM3D展现出强大的泛化能力。

Conclusion: GOAL通过引入生成流模型和LLM的上下文知识，有效解决了ObjectNav任务中现有方法泛化能力不足的问题，并在多个基准数据集上取得了卓越的成果，为智能体在复杂未知环境中的导航提供了新的思路。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [75] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出PaIR-Net框架及PaIR数据集，旨在解决现有动作理解方法未能同时建模动作语义和空间上下文的不足，实现了动作语义和身体部位接触区域的同步预测。


<details>
  <summary>Details</summary>
Motivation: 当前动作理解方法在捕获动作的语义（“什么”）和空间上下文（“在哪里”）的双重性方面存在不足，无法联合建模动作语义及其在场景中的空间情境，导致难以全面理解多样视觉背景下的动作。

Method: 1. 引入新颖的视觉任务，同步预测高级动作语义和细粒度身体部位接触区域。2. 提出PaIR-Net框架，包含：接触先验感知模块（CPAM）用于识别接触相关身体部位；先验引导连接分割器（PGCS）用于像素级接触分割；交互推断模块（IIM）用于整合全局交互关系。3. 构建PaIR (Part-aware Interaction Representation) 数据集，包含13,979张图像，涵盖654种动作、80种物体类别和17个身体部位。

Result: PaIR-Net在实验评估中显著优于基线方法。消融研究证实了每个架构组件的有效性。

Conclusion: PaIR-Net及其提出的新任务和数据集，有效弥补了现有方法在联合理解动作语义和空间上下文方面的不足，为更全面、细致地理解人类与环境的交互提供了新途径。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [76] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 本文提出一种名为运动提示调整（MPT）的新方法，通过运动提示生成和组适配器将大型预训练模型（LMs）应用于微表情识别（MER），有效解决数据稀缺性和捕获细微运动的挑战，并超越了现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 微表情识别（MER）在医疗诊断、测谎和犯罪调查等领域至关重要。然而，微表情（ME）标注需要专业心理学知识，导致数据集样本稀缺，严重限制了MER模型的学习。现有大型预训练模型（LMs）虽然提供通用表示，但无法捕获MER所需的短暂细微面部运动。

Method: 本文引入运动提示调整（MPT）方法，将大型预训练模型（LMs）应用于微表情识别。具体包括：1) 运动提示生成，通过运动放大和高斯分词提取细微运动作为LMs的提示。2) 精心设计并将组适配器插入到LMs中，以增强其在MER目标域的性能，促进更细致的微表情表示区分。

Result: 在三个广泛使用的MER数据集上进行的大量实验表明，所提出的MPT方法持续超越现有最先进的方法，验证了其有效性。

Conclusion: MPT是一种新颖且有效的方法，通过运动提示调整和组适配器，成功地将大型预训练模型适应于微表情识别任务，克服了数据稀缺性和捕捉细微动作的挑战，并取得了卓越的性能。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [77] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 本文提出检索增强超分辨率（RASR），通过自动检索高分辨率参考图像来克服现有RefSR方法对手动配对的依赖。为促进研究，构建了首个RASR基准数据集RASR-Flickr30，并提出了RASRNet基线模型，该模型显著提升了图像超分辨率效果和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的参考图像超分辨率（RefSR）方法严重依赖手动策划的目标-参考图像对，这极大地限制了其在现实世界场景中的实用性。研究动机在于开发一种可扩展、灵活且能自动检索参考图像的RefSR范式。

Method: 引入检索增强超分辨率（RASR）范式，自动从数据库中检索语义相关的参考图像。构建了首个RASR基准数据集RASR-Flickr30，该数据集提供按类别划分的参考数据库。提出了RASRNet，一个结合了语义参考检索器和基于扩散的RefSR生成器的基线模型，其中生成器通过语义条件进行增强。

Result: 在RASR-Flickr30数据集上的实验表明，RASRNet比单图像超分辨率（SISR）基线模型有显著改进，实现了+0.38 dB PSNR和-0.0131 LPIPS，并生成了更真实的纹理。

Conclusion: 研究结果表明，检索增强是弥合学术RefSR研究与实际应用之间差距的一个有前景的方向。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [78] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 本文提出HyperKD，一种用于超光谱遥感的逆向知识蒸馏框架。它能将预训练基金模型知识有效迁移至超光谱图像模型，弥合光谱差异，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 基金模型在卫星观测中表现出色，但由于固有的光谱差异和数据稀缺性，直接应用于超光谱遥感仍面临挑战。

Method: 本文提出HyperKD，一种新颖的知识蒸馏框架。它通过逆向知识迁移，将普适基金模型（如Prithvi）的表示学习能力，蒸馏到一个基于掩码自编码器（MAE）的、专门为EnMAP超光谱图像设计的学生模型中。HyperKD采用光谱范围通道对齐、空间特征引导掩码和为超光谱图像定制的增强损失函数等特征策略，以解决光谱域间的巨大差异问题。

Result: 广泛实验表明，HyperKD显著提升了MAE中的表示学习能力，带来了更高的重建保真度，并在土地覆盖分类、作物类型识别和土壤有机碳预测等下游任务中展现出更强大的鲁棒性能。

Conclusion: HyperKD成功弥合了超光谱领域的巨大光谱域间隙，使得预训练基金模型能有效应用于地理空间应用，凸显了知识蒸馏框架在超光谱遥感分析中的巨大潜力。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [79] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出Animate-X++，一个基于DiT的通用动画框架，解决了现有方法对拟人化角色泛化性差和背景静态的问题，通过引入姿态指示器和多任务训练策略，实现了高质量动态背景的角色动画。


<details>
  <summary>Details</summary>
Motivation: 现有字符图像动画方法主要适用于人类角色，对游戏和娱乐行业常用的拟人化角色泛化性差；此外，这些方法只能生成静态背景视频，限制了视频的真实感。

Method: 1. 提出Animate-X++，一个基于DiT的通用动画框架，适用于包括拟人化角色在内的多种字符类型。2. 引入姿态指示器，通过隐式（利用CLIP视觉特征提取运动要点）和显式（模拟推理输入增强泛化性）方式捕捉驱动视频的运动模式。3. 采用多任务训练策略，联合训练动画和文本到视频（TI2V）任务，结合部分参数训练实现角色动画和文本驱动的背景动态。4. 引入新的拟人化动画基准（A2Bench）以评估性能。

Result: Animate-X++不仅实现了角色动画，还能生成文本驱动的背景动态，使视频更具真实感。广泛的实验证明了Animate-X++的优越性和有效性。

Conclusion: Animate-X++是一个优越且有效的通用动画框架，能够处理多种角色类型并生成具有动态背景的视频，显著提升了角色动画的真实感和泛化能力，解决了现有方法的关键局限性。

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [80] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 本文提出一种名为IAG的新型输入感知后门攻击方法，旨在操纵视觉语言模型（VLM）的视觉定位行为，使其无论用户查询如何，都强制定位图像中的特定目标，并验证了其有效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLM）在视觉定位任务中取得了显著进展，但其安全问题（特别是后门攻击）仍未得到充分探索。

Method: 1. 引入了名为IAG的输入感知后门攻击方法，该方法通过在输入图像中嵌入语义信息来操纵VLM的定位行为。2. 提出一个自适应触发器生成器，利用文本条件U-Net将攻击目标描述的语义信息嵌入原始图像，以克服开放词汇攻击挑战。3. 采用重建损失来最小化中毒图像与干净图像之间的视觉差异，确保攻击的隐蔽性。4. 引入了统一的攻击数据生成方法。

Result: 1. IAG在理论和经验上均证明了其可行性和有效性。2. 在InternVL-2.5-8B模型上，ASR@0.5在各种测试集上超过65%。3. 对Ferret-7B和LlaVA-1.5-7B模型也显示出良好的攻击潜力，且对干净样本的准确性下降很小。4. 通过消融研究和潜在防御等广泛实验，证明了该攻击的鲁棒性和可迁移性。

Conclusion: 本文成功提出并验证了一种针对视觉语言模型视觉定位任务的新型、有效且隐蔽的后门攻击方法IAG，揭示了该领域潜在的安全漏洞和风险。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [81] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出RelayFormer，一个统一且模块化的视觉内容篡改定位框架，通过GLoRA机制实现跨模态、可扩展、高效率的篡改区域识别，并达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉篡改定位（VML）在数字取证中非常关键，但现有方法缺乏跨模态泛化能力，且难以高效处理高分辨率或长时间输入。

Method: 提出RelayFormer，一个统一且模块化的视觉篡改定位架构。它利用灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现可扩展、分辨率无关的处理和强泛化能力。该框架通过轻量级适配模块与现有Transformer骨干（如ViT、SegFormer）无缝集成。此外，设计了一个轻量级、基于查询的掩码解码器，支持视频序列的一次性推理，具有线性复杂度。

Result: 在多个基准测试中，RelayFormer实现了最先进的定位性能。

Conclusion: RelayFormer为可扩展且模态无关的视觉篡改定位（VML）任务设立了新的基线。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [82] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: 提出GEN-AFFECT框架，用于生成富有表现力且身份一致的个性化2D虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉细微面部表情，且在不同表情下难以保持虚拟形象的身份一致性。

Method: GEN-AFFECT框架通过将多模态扩散变换器与提取的身份-表情表示相结合，并在推理时采用一致性注意力机制，以确保生成各种细微表情的同时保持身份一致性。

Result: GEN-AFFECT在生成表情的准确性、身份保留和目标身份在细微表情间的一致性方面，均优于现有最先进方法。

Conclusion: GEN-AFFECT成功克服了现有挑战，实现了表达力强且身份稳定的个性化虚拟形象生成。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [83] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出一种基于神经形态计算的鲁棒拟合方法，相比传统CPU方案，能效显著提升。


<details>
  <summary>Details</summary>
Motivation: 鲁棒几何模型拟合是计算机视觉的基础任务，现有研究多关注效率和精度，但对能耗关注不足。随着AI应用能耗问题日益突出，提高鲁棒拟合的能效变得至关重要。

Method: 研究人员设计了一种新颖的脉冲神经网络（SNN），并在Intel Loihi 2神经形态硬件上实现了鲁棒拟合。具体方法包括开发事件驱动的模型估计公式，使其适应Loihi 2的独特架构，并采用算法策略克服硬件当前的有限精度和指令集缺陷。

Result: 实验结果表明，所提出的神经形态鲁棒拟合方法在达到同等精度的前提下，能耗仅为在标准CPU上运行传统鲁棒拟合算法的15%。

Conclusion: 神经形态计算为鲁棒拟合提供了显著的能效优势，有望解决AI领域日益增长的能耗问题，且不牺牲精度。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [84] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一个面向城市级点云语义分割的开创性基础模型，通过整合文本模态，解决了现有模型泛化能力差和域差距大的问题，实现了开放词汇和零样本推理，并在多项基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有城市级点云语义分割模型受限于3D数据规模和数据集间的域差距，导致其泛化能力不足，无法实现全面的3D理解。

Method: 提出CitySeg基础模型，融合文本模态以实现开放词汇和零样本推理。具体策略包括：定制化数据预处理规则以应对数据分布不均；提出局部-全局交叉注意力网络增强感知能力；引入分层分类策略和图编码器解决标签差异；采用两阶段训练和hinge loss提升子类别特征可分性。

Result: CitySeg在九个封闭集基准测试中取得了最先进（SOTA）的性能，显著超越现有方法。此外，首次在城市规模点云场景中实现了不依赖视觉信息的零样本泛化。

Conclusion: CitySeg通过其创新方法，显著提升了城市级点云语义分割模型的泛化能力和零样本推理能力，为UAV感知系统提供了关键技术支持。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [85] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 本文提出FTNet，一个无需训练的少样本深度伪造检测网络。它利用一个未知伪造样本进行比较，在不依赖传统训练数据的情况下，针对多种AI生成图像实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测将未知样本视为“零样本”任务，但在现实世界中，当模型对未知样本表现不佳时，这些样本仍可用于分析。这表明它应被视为“少样本”任务，即有效利用少量样本可显著提升性能。与侧重语义理解的典型少样本任务不同，深度伪造检测更注重图像真实性，这与现实世界分布紧密相关。

Method: 提出了Few-shot Training-free Network (FTNet)。与依赖大量已知数据训练的传统方法不同，FTNet仅使用来自评估集的一个伪造样本（模拟现实世界中新样本出现的情景），无需任何训练或参数更新。在评估时，每个测试样本与已知伪造和真实样本进行比较，并根据最近样本的类别进行分类。

Result: 对来自29种不同生成模型的AI生成图像进行了全面分析，取得了新的SoTA性能，比现有方法平均提升了8.7%。

Conclusion: 该工作为现实世界深度伪造检测引入了新视角：当模型难以在少样本上泛化时，利用这些失败的样本可以带来更好的性能。

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [86] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本研究通过引入面部专家混合（MoFE）模型和构建大型面部角度（LFA）数据集，显著提升了视频生成在大幅面部角度下的人物身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在大幅面部角度下难以保持身份一致性，主要挑战在于难以将身份特征有效整合到DiT结构中，以及现有开源视频数据集中缺乏针对大幅面部角度的覆盖。

Method: 1. 提出面部专家混合（MoFE）模型，动态结合身份专家、语义专家和细节专家以捕获全面面部特征。2. 设计并应用以面部约束和身份一致性为核心的数据处理流程，以解决现有数据集中大幅面部角度和身份稳定训练数据的稀缺问题。3. 基于此流程，构建了包含46万个视频片段和面部角度标注的大型面部角度（LFA）数据集。

Result: 在LFA基准测试中，所提出的方法结合LFA数据集，在面部相似性、面部FID和CLIP语义对齐方面显著优于现有最先进（SOTA）方法。

Conclusion: 通过创新的MoFE模型和专门构建的LFA数据集，本研究有效解决了视频生成中大幅面部角度下身份保持的挑战，并取得了卓越的性能表现。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [87] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 提出一种基于异常检测的通用AI生成图像检测器，无需AI图像训练，对未知生成模型有效。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像（AIIs）视觉质量接近自然图像，安全问题日益突出。现有AII检测器通常采用图像分类方法，但在面对来自未见过的生成模型的AIIs时，检测性能有限。

Method: 该研究提出一种基于异常检测的通用AI生成图像检测器。其判别器无需访问任何AIIs，通过无监督学习获得可泛化的表示。具体而言，使用预训练的CLIP编码器作为特征提取器，并设计了一个类似归一化流的无监督模型。训练时，不使用AIIs，而是使用代理图像（如对自然图像进行光谱修改得到），通过最小化代理图像的似然，并可选地结合最大化自然图像的似然来训练模型。

Result: 广泛的实验证明，该方法对各种图像生成器生成的AIIs均有效。

Conclusion: 该研究通过将AI生成图像检测视为异常检测问题，并利用无监督学习和代理图像进行训练，成功构建了一个无需真实AI图像即可训练的通用检测器，有效解决了现有方法在未知生成模型上性能受限的问题。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [88] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT是一种将放射科医生眼动行为（包括其时间变化和对长尾类别的关注）融入深度学习，以提升长尾疾病分类性能的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化医学图像判读模型未能有效利用放射科医生在诊断过程中的动态视觉注意力，特别是对那些构成“长尾”的次要或偶发疾病的关注。医生的眼动模式包含丰富的精细和粗糙疾病信息，且其注意力随时间变化，整合这些动态特征对提高自动化判读至关重要。

Method: 本文提出了GazeLT方法，通过集成与分解机制，利用人类视觉搜索过程的时间动态特性来改进长尾疾病分类。该方法整合了放射科医生眼动模式中捕捉到的精细和粗糙层面的疾病相关信息，并考虑了其随时间变化的注意力。

Result: GazeLT在NIH-CXR-LT和MIMIC-CXR-LT两个大型公开长尾疾病分类数据集上进行了验证。结果显示，GazeLT在平均准确率指标上，比现有最佳长尾损失方法高出4.1%，比基于视觉注意力的基线方法高出21.7%。

Conclusion: GazeLT通过有效整合人类动态视觉注意力，显著提升了长尾疾病的自动化分类性能，为未来的医学图像判读系统提供了新的方向。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [89] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: 提出SkySplat，一个自监督框架，将RPC模型整合到通用3DGS中，解决稀疏视角卫星图像三维重建中的瞬态物体、几何约束不足和泛化能力差的问题，实现更快的速度、更高的精度和更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角卫星图像三维重建是一个长期且具有挑战性的任务。尽管3D Gaussian Splatting（3DGS）高效，但现有方法因与有理多项式系数（RPC）模型不兼容和泛化能力有限而不适用于卫星图像。通用3DGS方法在多时相稀疏卫星图像上表现不佳，原因在于几何约束有限、瞬态物体和辐射不一致性。

Method: 提出SkySplat，一个自监督框架。它将RPC模型集成到通用3DGS管线中，以有效利用稀疏几何线索。该方法仅依赖RGB图像和辐射鲁棒的相对高度监督，无需地面真实高度图。关键组件包括：1) 跨自一致性模块（CSCM），通过基于一致性的掩蔽减轻瞬态物体干扰；2) 多视图一致性聚合策略，用于优化重建结果。

Result: 相较于逐场景优化方法，SkySplat比EOGS提速86倍并具有更高精度。相较于通用3DGS基线，在DFC19数据集上将平均绝对误差（MAE）从13.18米显著降低至1.80米。在MVS3D基准测试中展现出强大的跨数据集泛化能力。

Conclusion: SkySplat通过集成RPC模型和采用自监督技术，成功解决了稀疏视角卫星图像三维重建的挑战，在速度、精度和泛化能力方面均优于现有方法。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [90] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM是一个受人类情景记忆启发的免训练框架，通过将关键帧建模为时序事件并结合CoT推理，解决了Video-LLMs在长视频理解中因上下文窗口限制和现有关键帧方法不足的问题，实现了更高的性能和更少的帧使用。


<details>
  <summary>Details</summary>
Motivation: Video-LLMs在通用视频理解方面表现出色，但受限于上下文窗口，难以处理长视频。现有关键帧检索方法将问题简化为静态图像匹配，忽略了时空关系，可能导致冗余或信息量有限的关键帧，从而影响视频问答的准确性。

Method: 引入Video-EM框架，灵感来源于人类情景记忆原理，无需训练。该框架将关键帧明确建模为按时间排序的情景事件，捕捉空间关系和时间动态。此外，它利用大型语言模型的思维链（CoT）推理，迭代识别出一组最小但信息量丰富的情景记忆子集，从而实现Video-LLMs高效准确的问答。

Result: 在Video-MME、EgoSchema、HourVideo和LVBench基准测试上进行了广泛评估，结果证实了Video-EM的优越性。它实现了极具竞争力的结果，相对于各自基线性能提升4-9%，同时使用的帧数更少。

Conclusion: Video-EM通过模拟人类情景记忆和结合CoT推理，有效解决了Video-LLMs处理长视频的挑战，显著提升了视频理解能力，并以更低的资源消耗获得了更优的性能。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [91] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 针对现有扩散图像检测方法在OOD模型上泛化性差的问题，本文提出语义感知重建误差（SARE），利用假图像与文本描述语义更一致的特性进行检测，并在多个基准测试中展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散图像检测方法主要依赖于模型特定伪影，导致其在面对来自未知（OOD）生成模型的伪造图像时，性能会显著下降，无法有效泛化。

Method: 本文提出一种新的表示方法：语义感知重建误差（SARE），通过测量图像与由其描述引导的重建图像之间的语义差异来检测伪造图像。其核心假设是真实图像的描述难以完全捕捉其复杂视觉内容，导致在描述引导重建过程中发生显著语义偏移；而伪造图像与描述高度一致，重建后语义变化极小。SARE量化这些语义偏移作为判别特征。

Result: 实验结果表明，所提出的SARE方法具有强大的泛化能力，在GenImage和CommunityForensics等基准测试中均优于现有基线方法。

Conclusion: SARE提供了一种有效且对各种生成模型具有鲁棒泛化能力的扩散图像检测方法，成功克服了传统方法在OOD场景下的局限性。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [92] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: 本文提出CWFBind，一种基于局部曲率特征的深度学习对接方法。它通过整合几何信息、加权机制和动态半径策略，旨在解决现有方法在结合位点预测和构象生成中的不足，并在准确性和效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的深度学习对接方法在速度和准确性上优于传统方法，但许多方法忽略了关键的几何信息，导致结合口袋定位不准确和结合构象不真实。

Method: 引入CWFBind，一个基于局部曲率特征的加权、快速、准确的对接方法。该方法：1. 在特征提取阶段整合局部曲率描述符，丰富蛋白质和配体的几何表示。2. 在消息传递过程中嵌入度感知加权机制，增强捕获空间结构差异和相互作用强度的能力。3. 采用配体感知动态半径策略和增强损失函数，解决结合口袋预测中的类别不平衡问题。

Result: CWFBind在多个对接基准测试中实现了有竞争力的性能。

Conclusion: CWFBind在准确性和效率之间提供了平衡，有效改进了药物设计中预测配体-蛋白结合构象的能力。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [93] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 本文提出一种结合ProGAN和SAGAN的生成对抗网络变体，用于生成高质量、高分辨率、特征丰富的印度手语图像，性能优于传统ProGAN，并发布了一个大型印度手语数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管手语识别已取得进展，但手语生成领域仍有待探索。现有生成模型（如ProGAN擅长高分辨率，SAGAN擅长特征丰富度）难以同时兼顾手语图像的高分辨率和细节丰富度。研究旨在通过改进手语图像生成技术，弥补听力健全者与听障人士之间的沟通鸿沟。

Method: 开发了一种结合ProGAN和SAGAN优点的生成对抗网络（GAN）变体。该模型是基于注意力机制的，旨在生成特征丰富、高分辨率且类别条件的印度手语图像。同时，研究者还创建并发布了一个包含印度手语字母、数字和129个单词的高质量大型数据集。

Result: 所提出的改进型基于注意力的模型能够生成高质量的印度手语字母、数字和单词图像。与传统ProGAN相比，该模型在Inception Score (IS) 上提升了3.2，在Fréchet Inception Distance (FID) 上提升了30.12，性能显著优越。此外，研究还发布了一个包含高质量印度手语字母、数字和129个单词的大型数据集。

Conclusion: 本研究开发的GAN变体能有效生成高质量、特征丰富、高分辨率的印度手语图像，并在性能上超越了现有方法。随之发布的大型数据集将进一步推动该领域的研究，为听障人士提供更好的沟通工具。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [94] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统性地调查并量化了单目标跟踪（SOT）中相似物体干扰（SOI）的影响。研究证明SOI是关键瓶颈，并提出了首个语义认知指导基准SOIBench。通过实验发现现有视觉-语言跟踪（VLT）方法未能有效利用语义指导，为此提出了一种基于大型视觉-语言模型（VLM）的新范式，该方法在语义指导下显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 单目标跟踪（SOT）中，相似物体干扰（SOI）是一个长期被忽视但至关重要的瓶颈。研究旨在首次系统性地调查并量化SOI对SOT性能的影响，并探索外部认知指导的可行性。

Method: 1. **SOI量化：** 通过受控的在线干扰掩蔽（OIM）实验，定量分析消除干扰源对跟踪性能的影响。2. **基准构建：** 构建了首个针对SOI挑战的语义认知指导基准SOIBench，采用自然语言作为外部指导形式。SOIBench通过多追踪器集体判断自动挖掘SOI帧，并采用多级标注协议生成精确的语义指导文本。3. **新范式提出：** 提出了一种利用大型视觉-语言模型（VLM）作为外部认知引擎的新范式，该引擎可无缝集成到任意RGB追踪器中。

Result: 1. **SOI影响：** 消除干扰源可使所有现有SOTA追踪器的AUC性能提升高达4.35，直接验证了SOI是鲁棒跟踪的主要制约因素。2. **现有VLT表现：** 在SOIBench上评估显示，现有视觉-语言跟踪（VLT）方法未能有效利用语义认知指导，仅获得微小改进甚至性能下降（AUC变化范围为-0.26至+0.71）。3. **所提方法表现：** 提出的VLM范式在语义认知指导下取得了显著改进（AUC增益高达0.93），显著优于现有VLT方法。

Conclusion: 相似物体干扰（SOI）是单目标跟踪中的一个关键制约因素。通过引入SOIBench，为SOI挑战提供了一个标准化评估平台。研究表明，通过将大型视觉-语言模型（VLM）作为外部认知引擎，可以有效利用语义指导来显著提升跟踪性能，这为语义认知跟踪研究提供了新的方向和见解。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [95] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 本文提出Spatial Decay Transformer (SDT)，通过新颖的上下文感知门控(CAG)机制，在Vision Transformers (ViTs)中实现了数据依赖的空间衰减，显著提升了其在图像分类和生成任务上的性能，解决了ViT缺乏空间归纳偏置的问题。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs)的自注意力机制缺乏显式空间归纳偏置，导致在空间结构化任务上表现不佳。现有方法使用数据无关的固定空间衰减，适应性差。受大型语言模型中内容感知门控机制成功的启发，作者旨在为2D视觉Transformer引入数据依赖的空间衰减。

Method: 本文提出了Spatial Decay Transformer (SDT)，其核心是Context-Aware Gating (CAG)机制。CAG能根据内容相关性和空间接近度，生成动态、数据依赖的补丁间衰减。通过统一的空间-内容融合框架（结合曼哈顿距离的空间先验和学习到的内容表示），解决了从1D到2D的适应性问题。

Result: 在ImageNet-1K图像分类和生成任务上，SDT相对于现有强基线模型取得了持续的性能提升。

Conclusion: 本文成功将数据依赖的空间衰减引入2D视觉Transformer，确立了其作为增强ViTs空间注意力的新范式。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [96] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出一种新颖的测量增强展开网络（MEUNet），通过不对称Kronecker CS模型和测量感知交叉注意力机制，解决了图像压缩感知中测量不相干性和表示学习的挑战，实现了最先进的重建精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度网络的图像压缩感知方法，在感知阶段存在不相干的压缩测量，在重建阶段存在隐式的测量表示，这些缺陷限制了整体性能。

Method: 1. 提出不对称Kronecker CS (AKCS) 模型以提高测量不相干性。2. 揭示展开网络优于非展开网络的原因在于充分的梯度下降（即显式测量表示）。3. 提出测量感知交叉注意力 (MACA) 机制来学习隐式测量表示。4. 将AKCS和MACA集成到展开架构中，构建了测量增强展开网络 (MEUNet)。

Result: 1. AKCS在理论上证明了比现有Kronecker CS更好的不相干性，且复杂度增加最小。2. MEUNet在重建精度和推理速度方面均达到了最先进的性能。

Conclusion: 通过在测量不相干性（AKCS）和测量表示学习（MACA）方面的创新，MEUNet显著提升了图像压缩感知任务的性能，达到了当前最佳水平。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [97] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: 一种新的RGBT微小目标检测框架COXNet，通过跨层特征融合、动态对齐和优化标签分配，显著提升了在复杂无人机场景下微小目标的检测精度。


<details>
  <summary>Details</summary>
Motivation: 在无人机RGBT图像中检测微小目标极具挑战性，面临空间错位、低光、遮挡和背景杂乱等问题，且现有方法未能有效利用可见光和热成像模态的互补信息。

Method: 提出COXNet框架，包含三项核心创新：1) 跨层融合模块，融合高层可见光和低层热成像特征；2) 动态对齐与尺度细化模块，校正跨模态空间错位并保留多尺度特征；3) 基于GeoShape相似性度量的优化标签分配策略。

Result: 在RGBTDronePerson数据集上，COXNet的mAP$_{50}$比现有最先进方法提升了3.32%。

Conclusion: COXNet能够有效实现RGBT图像中微小目标的鲁棒检测，尤其适用于复杂环境下的应用。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [98] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 针对非对称多相机系统中的立体匹配挑战，提出了一种名为IVF-AStereo的双阶段迭代体融合网络，通过融合不同代价体信息，显著提高了匹配精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统立体匹配算法依赖于双目视觉的对称性假设，但新兴的非对称多相机系统（如远摄-广角相机）打破了这一假设，导致立体匹配（特别是代价体计算）面临严峻挑战。

Method: 首先，分析了两种现有代价体构建方法在非对称立体中的匹配代价分布，发现它们存在信息扭曲。基于此，提出了一种双阶段迭代体融合网络(IVF-AStereo)：第一阶段，通过聚合串联体优化关联体；第二阶段，融合两种代价体以增强细节信息。

Result: 所提出的方法在非对称场景中表现卓越，对显著的视觉非对称性显示出强大的鲁棒性。在基准数据集上的大量对比实验和消融研究证实了该方法在处理分辨率和颜色降级的非对称立体图像时的有效性。

Conclusion: IVF-AStereo网络通过创新的两阶段体融合策略，有效解决了非对称立体匹配的难题，显著提升了在复杂非对称视觉条件下的匹配性能和鲁棒性。

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [99] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: 本文引入了GoViG（Goal-Conditioned Visual Navigation Instruction Generation）任务，旨在仅利用自我中心视角下的初始和目标视觉观测，自动生成精确的导航指令，无需结构化输入，提高了对未知环境的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的导航指令生成方法依赖于结构化输入（如语义标注或环境地图），这限制了它们在未知和非结构化环境中的适应性。本研究旨在通过仅利用原始自我中心视觉数据来克服这一局限。

Method: 该方法将任务分解为两个子任务：1) 视觉预测，用于预测连接初始和目标视图的中间视觉状态；2) 指令生成，用于基于观察到的和预测的视觉信息合成语言连贯的指令。这两个子任务被集成到一个自回归多模态大型语言模型中，并通过定制目标进行训练，以确保空间准确性和语言清晰度。此外，引入了一次性推理和交错推理两种互补的多模态推理策略，以模拟人类在导航过程中的渐进认知过程。为评估该方法，提出了结合合成和真实世界轨迹的R2R-Goal数据集。

Result: 实验结果表明，GoViG方法相较于现有最先进的方法取得了显著改进，在BLEU-4和CIDEr分数上表现更优，并展现出强大的跨领域泛化能力。

Conclusion: 本文提出的GoViG任务及其通过视觉预测和指令生成相结合的自回归多模态大型语言模型，有效解决了从原始视觉数据生成导航指令的挑战，并在新数据集上实现了卓越的性能和泛化能力。

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [100] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 探讨封闭集生成数据增强在提升图像分类性能中的有效性，并量化合成数据的等效规模，为其实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中一个关键科学问题：给定图像分类任务的训练集，能否通过在该数据集上训练生成模型来提高分类性能，即进行封闭集生成数据增强。

Method: 探索真实图像与封闭集生成图像的区别和相似性。通过大量实验，系统性地洞察合成数据在数据增强中的有效使用，并经验性地确定合成图像所需的等效规模。

Result: 提供了关于封闭集合成数据有效用于增强的系统性见解，经验性地确定了合成图像增强所需的等效规模。展示了真实数据增强与开放集生成增强之间的定量等效性。研究还提供了一个经验公式，用于量化达到可比图像分类性能所需的合成数据增强规模，并在自然和医学图像数据集上验证了其效果随基线训练集大小和合成数据量的变化。

Conclusion: 尽管真实图像通常更受青睐，本研究的经验公式为量化合成数据增强的所需规模提供了指导，使其能够达到与真实数据增强相当的图像分类性能，为有效利用合成数据提升分类性能提供了量化依据。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [101] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 针对超声图像多异构数据集泛化挑战，提出COME框架，通过共享和特定专家协同，在多个评估模式下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在超声图像分析中面对新数据分布时泛化能力差，主要原因包括数据有限、声影和散斑噪声。因此，亟需构建一个适用于多异构超声数据集的通用框架。现有方法（如单源特定解码器或域适应）在跨域应用时性能下降，且难以有效缓解数据集间干扰并保留各自判别性特征。

Method: 提出通用协同混合异构源特定专家（Universal Collaborative Mixture of Heterogeneous Source-Specific Experts, COME）框架。COME通过建立双重结构-语义共享专家来创建通用表示空间，并使其与源特定专家协同工作，通过提供互补特征来提取判别性特征。

Result: 在单数据集、器官内和器官间整合数据集三种评估模式下，COME展现出卓越的性能，相较于现有最先进的方法，在平均精度（mean AP）上取得了显著提升。

Conclusion: COME的设计能够有效利用跨数据集的经验分布，为小批量或未见数据场景提供通用的超声先验知识，从而实现鲁棒的泛化能力，成功解决了超声图像多异构数据集的泛化难题。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [102] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 本研究提出了一种基于2D虹膜图像拓扑不变量的生物识别方法，通过数字同调理论提取特征，并在虹膜识别任务上实现了高精度，优于CNN，且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于虹膜图像拓扑不变量的生物识别方法，通过正式定义的数字同调表示虹膜纹理，并评估其分类性能。

Method: 将归一化虹膜图像分割成网格，对每个子区域计算Betti0、Betti1及其比率等拓扑不变量。这些不变量构成特征矩阵，结合逻辑回归、KNN和SVM进行分类。同时训练一个CNN模型作为性能对比。

Result: 逻辑回归模型在拓扑特征上实现了97.78 +/- 0.82%的准确率，优于CNN（96.44 +/- 1.32%）及其他基于特征的模型。拓扑特征表现出高准确率和低方差。

Conclusion: 这是首次将形式数字同调的拓扑不变量应用于虹膜识别。该方法提供了一种紧凑、可解释且准确的深度学习替代方案，在需要可解释性或数据有限时尤其有用，并可推广至其他生物识别、医学成像等领域。

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [103] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 本文通过GPT-4o生成了180K规模的合成图像数据集Echo-4o-Image，旨在弥补真实世界数据在稀有场景和清洁监督方面的不足。利用该数据集微调生成模型得到Echo-4o，并引入新的评估基准。实验证明Echo-4o表现优异，且该数据集具有强大的可迁移性，能有效提升其他基础模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GPT-4o在图像生成上表现出色，但开源模型仍有差距，且现有研究探索了从GPT-4o蒸馏图像数据以增强开源模型。然而，一个核心问题是：既然真实世界数据已是高质量数据源，为何还需使用GPT-4o生成的合成数据？本研究旨在回答此问题，指出合成数据能弥补真实世界数据在稀有（如超现实、多参考）场景覆盖不足的盲点，并提供更清洁、可控的监督信号，解决真实数据中的背景噪声和文本-图像错位问题。

Method: 本研究首先明确了合成图像在补充稀有场景和提供清洁监督方面的两大优势。在此基础上，利用GPT-4o生成了180K规模的合成图像数据集Echo-4o-Image。随后，使用此数据集对统一多模态生成基线Bagel进行微调，从而得到了Echo-4o模型。此外，为更准确和挑战性地评估图像生成能力，研究提出了两个新的评估基准：GenEval++（增加指令复杂性以避免分数饱和）和Imagine-Bench（专注于评估想象内容的理解和生成）。

Result: Echo-4o在标准基准测试中展现出强大的性能。更重要的是，将Echo-4o-Image数据集应用于其他基础模型（如OmniGen2、BLIP3-o）时，也持续带来了多个指标的性能提升，这充分证明了该数据集的强大可迁移性。

Conclusion: GPT-4o生成的合成图像数据对于提升开源图像生成模型至关重要，尤其是在处理真实世界数据中稀有、复杂和需要清洁监督的场景。Echo-4o-Image数据集及其训练的Echo-4o模型验证了合成数据的巨大价值和普适性，为未来高质量、高性能开源模型的开发提供了有效且可迁移的资源。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [104] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出WeatherPrompt，一种多模态学习范式，通过融合图像嵌入与文本上下文，为无人机地理定位在恶劣天气下建立天气无关表示，显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉地理定位在雨、雾等天气扰动下性能显著下降。现有方法存在两个局限：1) 过度依赖有限的天气类别，限制了泛化能力；2) 通过伪天气类别对场景-天气特征的解耦效果不佳。

Method: 本文提出WeatherPrompt，一个多模态学习范式，旨在通过融合图像嵌入与文本上下文来建立天气无关表示。其包含两大关键贡献：1) 免训练天气推理机制，利用大型多模态模型合成多天气文本描述，提高对未知或复杂天气的可扩展性；2) 动态门控多模态框架，由文本嵌入驱动，自适应地重新加权和融合跨模态视觉特征以更好地解耦场景和天气特征，并结合图像-文本对比学习和图像-文本匹配进行优化。

Result: 在多种天气条件下，该方法与现有最先进的无人机地理定位方法相比，实现了有竞争力的召回率。具体而言，在夜间条件下Recall@1提升了13.37%，在雾和雪条件下提升了18.69%。

Conclusion: WeatherPrompt通过建立天气无关的表示和创新的多模态框架，有效解决了无人机在恶劣天气下视觉地理定位的性能下降问题，显著提升了在多种复杂天气条件下的定位精度和泛化能力。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [105] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 本文提出一种基于小波降质引导的曝光校正方法WEC-DG，通过引入降质描述符和解耦光照细节，解决了单曝光图像校正中的类内变异性和错误校正问题，显著提升了光照和细节恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有曝光校正方法在处理单曝光图像时，难以应对由光照、环境和天气等因素引起的复杂类内变异性，且因未能识别“模糊”的曝光降质，常导致错误校正。研究动机是提升模型在复杂成像条件下的适应性。

Method: 本文提出WEC-DG方法。具体包括：1. 曝光一致性对齐模块 (ECAM): 在处理流程两端引入降质描述符，确保曝光一致性并解决由“模糊”曝光降质导致的错误校正。2. 曝光恢复与细节重建模块 (EDRM): 利用小波变换的光照-细节解耦特性，首先处理低频信息以增强曝光，然后将高频信息作为先验指导重建空间域细节，采用串行处理策略。

Result: 在多个公共数据集上进行的大量实验表明，所提出的方法优于现有算法，取得了显著的性能提升。

Conclusion: 所提出的WEC-DG方法有效解决了单曝光图像校正中的复杂性和错误校正问题，通过提升模型适应性并实现精确的光照校正与细节恢复，验证了其有效性和实用性。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [106] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 提出CoD框架，通过诊断链和多级接地实现更具临床准确性和可解释性的放射报告生成，解决现有模型在有效性和可信度方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有放射报告生成（RRG）模型在临床有效性方面表现不佳（特别是病灶属性描述），且生成的文本缺乏可解释性，导致放射科医生难以信任结果。

Method: 提出Chain of Diagnosis (CoD) 框架。该框架首先通过诊断对话生成问答（QA）对以提取关键发现，然后利用QA诊断提示大型语言模型进行报告生成。为增强可解释性，设计了诊断接地模块（将QA诊断与生成语句匹配）和病灶接地模块（定位图像中的异常）。此外，采用一种具有临床一致性的全监督学习策略，以有效利用不同数据集的多种标注类型。

Result: 开发了一个包含QA对和病灶框的全标注RRG数据集；创建了一个评估报告中病灶位置和严重性描述准确性的工具；实验表明，CoD模型在两个RRG基准测试中持续优于专业和通用模型，并通过将生成语句准确地与QA诊断和图像关联，展示了良好的可解释性。

Conclusion: CoD框架有效地解决了放射报告生成中临床有效性不足和可解释性差的问题，实现了更高的报告准确性，并显著增强了生成结果的可信度，提升了放射科医生的工作效率。

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [107] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种名为双递归反馈（DRF）的无训练系统，通过递归优化中间潜在表示，显著提升了可控文生图模型在精细空间结构和条件控制上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有可控文生图（T2I）扩散模型（如Ctrl-X、FreeControl）在准确保留空间结构和捕获物体姿态、场景布局等细粒度条件方面表现不足。

Method: 提出了一种无需训练的“双递归反馈（Dual Recursive Feedback, DRF）”系统。该系统由外观反馈和生成反馈组成，通过递归地精炼中间潜在表示，以更好地反映给定的外观信息和用户意图，从而引导潜在表示至可靠流形，有效整合结构和外观属性。

Result: 该方法实现了细粒度生成，甚至能进行跨类别不变的结构-外观融合（例如将人类动作迁移到老虎形态上）。广泛的实验证明，本方法能够生成高质量、语义连贯且结构一致的图像。

Conclusion: DRF系统通过其独特的双递归反馈机制，成功解决了现有可控文生图模型在精细空间结构和条件捕获方面的挑战，实现了高质量、语义和结构一致的图像生成，且无需额外的模型训练。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [108] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 大型视觉-语言模型（LVLMs）存在幻觉问题，现有评估方法不足。本文提出了自动化数据构建和幻觉诱导框架，构建了可扩展的SHALE基准，用于细粒度评估忠实性和事实性幻觉，发现LVLMs在事实性幻觉和语义扰动下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs发展迅速，但仍存在幻觉（内容与输入不一致或与世界知识相悖，即忠实性或事实性幻觉）。现有研究对忠实性幻觉的评估停留在粗粒度层面，缺乏细致分析。此外，现有基准依赖昂贵的人工或重用公共数据集，存在可扩展性和数据泄露风险。

Method: 提出了一个自动化数据构建流程，用于生成可扩展、可控且多样化的评估数据。设计了一个分层幻觉诱导框架，通过输入扰动模拟真实的噪声场景。结合这些设计，构建了SHALE（Scalable HALlucination Evaluation）基准，该基准通过细粒度的幻觉分类方案评估忠实性和事实性幻觉。SHALE包含超过3万个图像-指令对，涵盖12个代表性视觉感知方面和6个知识领域，并考虑了清晰和噪声场景。

Result: 对20多个主流LVLM进行了广泛实验，结果显示这些模型存在显著的事实性幻觉，并且对语义扰动表现出高度敏感性。

Conclusion: SHALE基准为LVLM的幻觉评估提供了一个可扩展、细粒度且全面的方法。研究结果揭示了当前LVLM在处理事实性知识和应对输入扰动方面的显著局限性，表明需要进一步提升模型的稳健性和准确性。

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [109] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS是一种用于自动驾驶雷达检测的扩展目标跟踪（EOT）和融合标注框架，利用贝叶斯方法提供高精度轨迹和形状估计，支持性能评估及闭环改进。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中雷达检测的精确扩展目标跟踪（EOT）和标签标注问题，提供真实且精确的物体轨迹、形状估计以及检测级标注。

Method: 提出BAAS框架，结合EOT和融合技术。采用基于贝叶斯的跟踪、平滑和融合方法，用于提供物体轨迹和形状估计，进而生成标注标签。框架内置跟踪性能和标签标注评估功能，并支持与手动标注数据结合进行模块化分析和闭环持续改进。

Result: 在挑战性的城市真实场景中评估了框架性能，包括跟踪性能和标签标注错误率。成功展示了该方法对多种动态物体和类别类型的功能性。

Conclusion: BAAS框架为自动驾驶雷达数据提供了一套高效、精确的EOT和融合标注解决方案，通过贝叶斯方法实现高质量轨迹和形状估计，且支持性能评估和持续优化。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [110] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: 提出Hi-SMGNN框架，通过整合结构和形态连接组，以层次化和多尺度方式预测胶质瘤IDH突变状态。


<details>
  <summary>Details</summary>
Motivation: IDH突变是胶质瘤预后的关键生物标志物。现有功能MRI预测方法受可用性低和噪声限制。结构和形态连接组虽为替代方案，但常忽略大脑的层次组织和多尺度交互。

Method: 提出Hi-SMGNN框架，整合区域到模块层面的结构和形态连接组。其特点包括：1) 采用Siamese网络和跨模态注意力的多模态交互模块；2) 用于减少冗余的多尺度特征融合机制；3) 增强个体特异性和可解释性的个性化模块划分策略。

Result: 在UCSF-PDGM数据集上的实验表明，Hi-SMGNN性能优于基线和最先进模型，在IDH突变预测中展现出更高的鲁棒性和有效性。

Conclusion: Hi-SMGNN为IDH突变预测提供了一种有效且鲁棒的新方法，解决了现有方法的局限性，并充分利用了大脑的层次组织和多尺度信息。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [111] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: 提出SVG-Head，一种混合高斯表示，通过显式建模几何和分离纹理，实现高保真头部数字人渲染及实时外观编辑。


<details>
  <summary>Details</summary>
Motivation: 高保真、可编辑的头部数字人是AR/VR应用的关键，但现有方法因隐式表示和几何与外观的纠缠建模，难以实现实时外观编辑。

Method: SVG-Head采用混合表示，将3D高斯绑定至FLAME网格显式建模几何，并利用解耦纹理图像捕获全局外观。包含表面高斯（用于实时纹理编辑）和体积高斯（增强非朗伯区域重建）。引入网格感知高斯UV映射和分层优化策略。

Result: 在NeRSemble数据集上，SVG-Head生成高保真渲染结果，是首个为高斯头部数字人提供显式纹理图像并支持实时外观编辑的方法。

Conclusion: SVG-Head通过其独特的混合高斯表示和分离建模，成功解决了头部数字人实时外观编辑的挑战，为相关应用提供了有效解决方案。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [112] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: 本文提出FaME，一种无需训练且推理高效的方法，通过负向引导提升扩散模型生成图像的感知质量，同时不影响FID分数。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在类到图像生成中FID得分很高，但仍观察到在某些类别中生成扭曲或低质量图像，这表明FID无法评估个体样本的感知质量。同时，常用的CFG技术虽能提升指标，却可能引入分布偏移和视觉伪影，因此需要一种方法来提升生成图像的感知质量。

Method: 提出FaME方法，该方法无需训练且推理高效。它利用图像质量评估模型识别低质量生成图像，并存储其采样轨迹作为“失败模式”。这些失败模式随后被用作负向引导，以使未来的采样过程偏离低质量区域。

Result: 在ImageNet上的实验表明，FaME能持续提升视觉质量，同时不损害FID分数。此外，FaME还展示了扩展到文本到图像生成的潜力。

Conclusion: FaME成功地在不影响FID的前提下，提升了扩散模型生成图像的感知质量，提供了一种有效的、无需训练的改进方法，并具有广泛的应用前景。

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [113] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: 本文提出BridgeTA框架，通过轻量级教师助手网络和新的蒸馏损失，在不增加相机纯视觉模型推理成本的前提下，显著提升其在BEV地图分割上的性能，有效缩小与多传感器融合方法的差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中，相机纯视觉BEV地图分割的性能远低于激光雷达-相机融合方法。现有知识蒸馏方法（KD）虽旨在缩小此差距，但通常通过增大或修改学生模型来模仿教师，导致推理成本增加，不符成本效益。

Method: 提出BridgeTA框架，通过一个轻量级的教师助手（TA）网络来桥接激光雷达-相机融合（教师）和相机纯视觉（学生）模型之间的表示鸿沟，同时保持学生模型架构和推理成本不变。TA网络结合教师与学生的BEV表示，创建共享潜在空间。理论上，基于Young's Inequality推导了一种新的蒸馏损失，将直接的师生蒸馏路径分解为教师-TA和TA-学生双路径，以稳定优化并加强知识传递。

Result: 在nuScenes数据集上，相比相机纯视觉基线模型，mIoU性能提升了4.2%。与现有最先进的知识蒸馏方法相比，性能提升最高可达45%。

Conclusion: BridgeTA框架通过成本效益高的蒸馏方法，在不增加推理成本的前提下，显著提高了相机纯视觉BEV地图分割的性能，有效缩小了与多传感器融合方法的差距，并超越了现有知识蒸馏方法。

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [114] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D是首个用于稀疏视图锥束CT（CBCT）伪影去除的3D条件扩散模型，显著降低辐射剂量，并展现出优异的性能、可扩展性、泛化能力及临床实用性。


<details>
  <summary>Details</summary>
Motivation: 减少医学成像（特别是CBCT）的辐射暴露，并解决稀疏视图CBCT图像中的伪影问题。

Method: 提出了MInDI-3D模型，将2D的“InDI”概念扩展到3D，实现对CBCT体积的直接迭代去噪。为此，从CT-RATE数据集的胸部CT体积生成了一个大型伪CBCT数据集（16,182个样本）进行训练。通过定量指标、可扩展性分析、泛化测试和11位临床医生的评估进行了全面验证。

Result: MInDI-3D表现出有效性，在CT-RATE伪CBCT（独立真实世界）测试集上，仅用50个投影即可使PSNR增益12.96 (6.10) dB，并实现成像辐射暴露减少8倍。性能随训练数据增加而提升，与3D U-Net在16名癌症患者的真实扫描上表现相当，且能泛化到新的CBCT扫描仪几何结构。临床医生评估认为该模型足以用于患者定位，并能很好地保留肺部肿瘤边界。

Conclusion: MInDI-3D是一个有效、可扩展且泛化能力强的3D扩散模型，能够显著减少CBCT成像的辐射暴露，同时保持图像质量，在临床应用中具有巨大潜力。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [115] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 本研究提出一种基于模型信息优化的通用平面检测框架，解决了传统RANSAC在深度图像平面检测中易产生误报的问题，并提高了平面参数估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度图像中的平面检测对机器人应用至关重要，但广泛使用的RANSAC方法由于内点阈值模糊，在复杂真实场景（平面数量未知且多平面共存）中容易导致平面误报。

Method: 提出一种基于模型信息优化的平面检测框架。将深度读数视为受真实平面约束的离散随机变量，通过重复随机二次采样生成包含不同候选平面约束的模型。结合深度传感器物理和噪声模型计算每个模型的信息量，选择信息量最少的模型作为最可能的真实平面。此过程用于客观确定平面数量并防止误报。同时，通过计算内点信息减少量之和来评估平面质量。为加速算法，利用神经网络分割对深度图进行分区。

Result: 通过合成数据实验验证，该算法在平面参数估计方面比Open3D默认的RANSAC平面分割更准确。此外，结合神经网络分割加速了算法，并增强了在真实世界数据中生成更真实平面参数的能力。

Conclusion: 该研究通过信息优化提供了一种有效解决RANSAC误报问题的平面检测方法，显著提高了平面参数估计的准确性，并能客观确定平面数量和评估平面质量，在真实世界应用中表现出更好的性能。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [116] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 针对航空影像中3D俯视场景语义分割的挑战，本文提出SAD-Splat方法，通过高斯点删除模块和高置信度伪标签生成管线，有效解决语义模糊和标签稀缺问题，并引入了新的3D-AS数据集。该方法在分割精度和表示紧凑性之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 传统3D俯视场景语义分割方法难以解决航空影像中因尺度变化和结构遮挡导致的语义模糊问题，从而限制了分割精度和一致性。

Method: 本文提出SAD-Splat方法，主要包含：1. 高斯点删除模块：结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制，用于消除冗余和语义模糊的高斯点。2. 高置信度伪标签生成管线：利用2D基础模型在真实标签有限的情况下增强监督。此外，还引入了挑战性的基准数据集3D Aerial Semantic (3D-AS)，包含多样化的真实世界航空场景和稀疏标注。

Result: 实验结果表明，SAD-Splat在分割精度和表示紧凑性之间取得了出色的平衡。

Conclusion: SAD-Splat为3D航空场景理解提供了一种高效且可扩展的解决方案，并且引入的3D-AS数据集促进了该领域的研究进展。

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [117] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 文章提出利用纹理作为密集监督信号，通过轻量级纹理模块和新颖的纹理对齐损失，改进单目3D手部重建的姿态和形状估计，有效提升重建精度和真实感。


<details>
  <summary>Details</summary>
Motivation: 当前高性能3D手部重建模型在预测几何体与图像外观的对齐上常有缺陷，表明纹理对齐这一监督信号被低估。研究旨在重新审视并利用纹理作为一种密集的、空间定位的线索，主动支持手部姿态和形状估计。

Method: 提出一个轻量级纹理模块，将像素级观测嵌入UV纹理空间，并引入一种新颖的密集对齐损失，用于预测和观测手部外观的对齐。该方法假设可访问可微分渲染管线和可将图像映射到已知拓扑3D手部网格的模型，从而实现纹理手部的图像反向投影和像素级对齐。该模块可独立插拔到现有重建管线中，并通过增强HaMeR模型进行验证。

Result: 实验结果表明，所提出的系统提高了3D手部重建的准确性和真实感。

Conclusion: 纹理指导的对齐（或称外观指导的对齐）在3D手部重建中具有重要价值，能有效提升姿态和形状估计的性能。

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [118] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种针对深度强化学习的值函数初始化方法，通过重用紧凑的表格Q值实现知识迁移，显著提升了早期学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 值函数初始化（VFI）在表格强化学习中能有效实现RL的快速启动，但在深度强化学习（DRL）中面临连续状态-动作空间、神经网络近似噪声以及无法存储所有历史模型的挑战。

Method: 本文提出了DQInit，将VFI应用于DRL。DQInit从已解决的任务中提取紧凑的表格Q值作为可迁移的知识库。它采用基于“已知度”的机制，将这些迁移值柔和地整合到未充分探索的区域，并逐步转向智能体学习的估计值，避免了固定时间衰减的局限性。该方法仅依赖值估计而非策略或演示进行知识迁移。

Result: 在多个连续控制任务上的实验表明，与标准初始化和现有迁移技术相比，DQInit持续改进了早期学习效率、稳定性和整体性能。

Conclusion: DQInit提供了一种新颖的DRL知识迁移视角，结合了快速启动RL和策略蒸馏的优点并减轻了它们的缺点，显著提升了DRL的早期学习效果。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [119] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 本文介绍Othello AI Arena，一个新颖的基准框架，用于评估AI系统在面对未见环境时，能否在有限时间内进行快速适应和生成有效策略，以弥补现有AI基准在通用智能（AGI）适应性评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准主要关注在固定环境中的性能优化，未能有效评估AI系统面对细微规则或结构变化时的灵活性和泛化能力。快速适应新颖且不可预见的环境变化是AGI的基石，但在现有AI基准中仍是关键的盲点。

Method: 引入“Othello AI Arena”，一个元学习挑战平台。参与者需开发系统，在严格的时间限制（60秒）内分析新型奥赛罗棋盘的特定配置和规则，并为其生成量身定制的高性能策略。该平台具有多样的游戏阶段（包括公开和私有阶段，后者包含结构和规则变体），采用基于网络的实现方式，提供实时可视化、多维度的自动化评估和全面的日志记录。

Result: 初步的试点测试和学生参与观察到有趣的适应方法模式，包括从快速参数调整到通过模拟进行的初步环境模型学习。

Conclusion: Othello AI Arena提供了一个独特的教育工具和有价值的研究基准，旨在促进和评估AI系统中快速、智能适应的关键能力。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [120] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型和多智能体协作的自动化多模态AI助手评估框架，有效解决当前评估方法面临的挑战，并能准确预测用户满意度及识别生成缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI助手评估方法存在人工成本高、标准不一致和主观偏见等问题，限制了其发展和应用。

Method: 本文提出一种自动化多模态评估框架，其核心是基于大语言模型（LLM）和多智能体协作。该框架采用三层智能体架构，包括交互评估智能体、语义验证智能体和经验决策智能体。通过对Qwen3-8B模型进行监督微调以提升评估准确性。

Result: 该框架的评估结果与人类专家具有显著的匹配准确度。在对八个主流智能助手的实验中，验证了该框架在预测用户满意度和识别生成缺陷方面的有效性。

Conclusion: 所提出的自动化多模态评估框架有效解决了现有评估方法的局限性，提供了一种高效、准确、客观的AI助手评估方案，能够预测用户满意度并发现其潜在缺陷。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [121] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 本文提出EvoCurr框架，利用一个LLM生成动态适应的课程，逐步提升另一个LLM解决复杂决策问题的能力，显著提高了成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理需要深度、长程推理的复杂问题时性能下降，直接解决缺乏结构化中间指导，导致效率低下或失败。

Method: 提出EvoCurr自进化框架，其中一个课程生成LLM根据解题LLM的学习进度，构建渐进式难度的任务序列。该课程动态调整，当解题LLM遇到困难时降低难度，成功时提升难度。解题LLM是一个生成Python决策树脚本的代码生成模型。

Result: 在具有挑战性的决策制定基准测试中，与直接解决的基线方法相比，显著提高了任务成功率和解决方案效率。

Conclusion: LLM驱动的课程学习在增强现实世界中高复杂性领域的自动化推理方面具有强大潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [122] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该研究提出一种方法，结合Dempster-Shafer理论和Dirichlet过程，将SHAP值的不确定性分解为偶然、认知和纠缠成分，以提高高风险领域中SHAP解释的可靠性和可解释性，并发现高SHAP值特征不一定最稳定。


<details>
  <summary>Details</summary>
Motivation: SHAP值在解释复杂集成树模型中至关重要，尤其是在高风险领域（如医疗保健），但它们通常被视为点估计，忽略了模型和数据固有的不确定性（包括偶然不确定性和认知不确定性）。

Method: 提出一种分解SHAP值不确定性（偶然、认知和纠缠成分）的方法。该方法整合了Dempster-Shafer证据理论，并通过Dirichlet过程在树集成上进行假设抽样。

Result: 该方法在三个真实世界用例中得到验证，提供了对SHAP解释中认知不确定性本质的深入理解，从而更全面地理解SHAP归因的可靠性和可解释性。实验表明，SHAP值最高的特征不一定最稳定。基于树的模型（特别是Bagging）有助于有效量化认知不确定性。

Conclusion: 对SHAP归因可靠性和可解释性的理解可以指导稳健的决策制定和模型优化。通过更优质、更具代表性的数据和适当的模型开发技术，可以降低认知不确定性。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [123] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: 本研究提出MEML-GRPO框架，通过多专家提示和互学习机制，解决了RLVR在LLM推理中面临的奖励稀疏问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习与可验证奖励（RLVR）在大型语言模型（LLM）推理中面临奖励稀疏问题，即持续错误的候选答案导致零奖励，缺乏有效的学习信号，尤其在挑战性任务中表现突出。

Method: 提出Multi-Expert Mutual Learning GRPO (MEML-GRPO) 框架。该框架利用多样化的专家提示作为系统提示，生成更广泛的响应以增加找到正确解决方案的可能性。同时，引入专家间互学习机制，促进知识共享和迁移，进一步提升模型性能。

Result: 在多个推理基准上的实验表明，MEML-GRPO实现了显著的性能提升。Qwen模型平均性能提升4.89%，Llama模型平均性能提升11.33%。

Conclusion: MEML-GRPO有效克服了传统RLVR方法的核心局限性，显著增强了LLM的推理能力。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [124] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 针对大型语言模型（LLM）成对评估中存在的偏好偏差，本研究提出了UDA（无监督去偏对齐）框架。该框架通过动态调整Elo评分系统并强制评判者达成共识，显著降低了评判者间的分歧，提高了评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM的成对评估范式普遍存在偏好偏差（如评判者偏爱自身输出），导致评估结果不一致和排名偏差。因此，需要一种方法来减少这种偏差，实现更稳定和可复现的评估。

Method: 首先，实证证明了跨模型评估中显著且异构的偏差。然后，提出了UDA（无监督去偏对齐）框架，该框架通过一个紧凑的神经网络，为每次成对比较自适应地设置Elo评分系统的K因子并优化获胜概率。UDA以完全无监督的方式运行，其目标是最小化所有评判者Elo轨迹之间的离散度，从而强制实现向集体共识的对齐。研究还提供了理论依据，论证了这种对齐如何减少系统聚合偏差。

Result: 实验结果表明，UDA将评判者间的评分标准差显著降低了高达63.4%，并将与人类判断的平均相关性提高了24.7%。值得注意的是，UDA还能将表现不佳的评判者的性能提升至与高质量评判者相当的水平。

Conclusion: UDA框架有效解决了LLM成对评估中的偏好偏差问题，通过提升评估的一致性和可靠性，促进了一个更稳健、可靠的评估生态系统。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [125] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 随着LLMs日益自主，AI安全需关注其行为对齐性。本研究引入PacifAIst基准，衡量LLMs在自身目标与人类安全冲突时的自利行为，发现Gemini表现最佳，GPT-5表现最差，揭示了对齐性挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs日益自主并融入关键社会功能，AI安全焦点需从内容缓和转向行为对齐评估。现有安全基准未能系统探究LLM在自身工具性目标（如自保、资源获取、目标完成）与人类安全冲突情境下的决策，这在衡量和缓解新兴未对齐行为风险方面存在关键空白。

Method: 引入PacifAIst（基础人工智能场景测试的复杂交互过程评估），一个包含700个挑战性场景的专用基准，旨在量化LLMs的自利行为。该基准围绕一种新型的“存在优先级”（EP）分类法构建，包含自保与人类安全（EP1）、资源冲突（EP2）和目标维护与规避（EP3）等子类别。评估了八个主流LLM。

Result: 结果显示LLM性能存在显著等级差异。Google的Gemini 2.5 Flash以90.31%的最高“和平主义得分”（P-Score）表现出强劲的人类中心对齐。令人惊讶的是，备受期待的GPT-5记录了最低的P-Score（79.49%），表明潜在的对齐挑战。模型在不同子类别中的表现差异显著，例如Claude Sonnet 4和Mistral Medium在直接自保困境中表现不佳。

Conclusion: 这些发现强调了迫切需要像PacifAIst这样的标准化工具来衡量和缓解工具性目标冲突带来的风险，以确保未来的AI系统不仅在对话中有用，而且在行为优先级上能够被证明是“和平主义”的。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [126] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: 本文证明了公共观察逻辑（POL）的可满足性问题是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（包括认知规划）中，对知识和行动的推理逻辑应用广泛。其中，基于环境观察更新知识是此类规划场景的关键方面。

Method: 引入了公共观察逻辑（POL），它是公共宣告逻辑的一个变体，用于推理基于公共观察更新的知识。在认知（克里普克）模型中，每个状态都带有一组预期观察，当预期与实际观察匹配时，状态随之演变。

Result: 研究证明了公共观察逻辑（POL）的可满足性问题是2EXPTIME完全的。

Conclusion: 本工作通过确定POL可满足性问题的计算复杂度，深化了对POL的理解，揭示了其在复杂性方面的具体特征。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [127] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: 提出VIPCGRL，一个结合多模态（文本、关卡、草图）的深度强化学习框架，通过四重对比学习和辅助奖励提升PCGRL在共创中的类人性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的程序内容生成（PCGRL）系统未能展现以人为中心的行为，限制了AI驱动生成工具在实际设计工作流中的实用性。实现与人类对齐的AI对于共创至关重要，以准确解释人类意图并生成符合设计目标的可控输出。

Method: 提出VIPCGRL（Vision-Instruction PCGRL），一个新型深度强化学习框架，整合文本、关卡和草图三种模态，以扩展控制模态并增强类人性。引入通过跨模态和人机风格的四重对比学习训练的共享嵌入空间，并利用基于嵌入相似度的辅助奖励对策略进行对齐。

Result: 实验结果表明，VIPCGRL在类人性方面优于现有基线，并通过定量指标和人工评估得到验证。

Conclusion: VIPCGRL通过多模态融合和对比学习显著提升了PCGRL的类人性和可控性，使其在协同内容创作中作为人类工具的潜力更大。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [128] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 本文提出了一种动态多智能体系统（MAS）架构，通过引入“守护代理”在关键步骤验证并纠正推理过程，显著提升了大型语言模型（LLM）驱动的智能体在使用外部工具时的稳定性和准确性，并在GAIA数据集上取得了领先。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）驱动的智能体在利用多种外部工具解决复杂问题时，面临上下文过长、工具输出噪声或不相关导致系统可靠性和准确性下降的挑战，因此亟需提升智能体系统的稳定性。

Method: 研究者在AWorld框架内构建了一个鲁棒且动态的多智能体系统（MAS）架构，引入了动态监督和操纵机制。具体而言，执行代理（Execution Agent）在关键步骤调用守护代理（Guard Agent）来验证和纠正推理过程，以减少噪声引起的错误并增强问题解决的鲁棒性。

Result: 在GAIA测试数据集上进行的广泛实验表明，所提出的动态操纵机制显著提升了解决方案的有效性和稳定性，表现优于单代理系统（SAS）和标准工具增强系统。该动态MAS系统在GAIA排行榜的开源项目中位居第一。

Conclusion: 研究结果强调了协作代理角色（如守护代理）在开发更可靠、更值得信赖的智能系统方面的实际价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [129] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 本文提出一种结合知识图谱（KG）和检索增强生成（RAG）的多智能体框架，用于监管合规问答。该框架通过三元组提取、存储与检索，显著提升了问答的准确性、可追溯性和理解性，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 监管合规问答对精确性、可验证信息和领域专业知识的高要求，给大型语言模型（LLMs）带来了挑战。

Method: 本研究提出一个新型多智能体框架，整合了监管三元组知识图谱与检索增强生成（RAG）。首先，智能体从监管文档中提取、清洗、规范化并更新主谓宾（SPO）三元组以构建无本体知识图谱。其次，这些三元组连同其文本段落和元数据被嵌入并存储于一个富向量数据库中，支持图推理和信息检索。最后，一个编排的智能体管道利用三元组级别的检索进行问答，确保查询与事实核心的语义高度对齐。

Result: 该混合系统在复杂监管查询中表现优于传统方法。它通过嵌入的三元组确保事实正确性，通过统一的向量数据库实现可追溯性，并通过子图可视化增强理解。

Conclusion: 该系统为合规驱动和更广泛的审计应用提供了坚实的基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [130] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）在数学任务（算术、代数、数论）上的准确性，并分析了其推理错误，发现OpenAI o1表现优异，且双代理配置能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在AI驱动教育（特别是数学教育）中的应用日益增多，其在数学问题解决中提供准确答案和详细解决方案的能力对于可靠的反馈和评估至关重要。本研究旨在系统评估LLMs的准确性并识别其推理错误类型。

Method: 评估了四种LLMs（OpenAI GPT-4o, o1, DeepSeek-V3, DeepSeek-R1）在三种数学任务（算术、代数、数论）上的表现。研究构建了LLMs易犯错的挑战性数学任务（而非标准基准）。系统分析并编码了最终答案的准确性以及解题步骤中的错误。同时测试了单代理和双代理配置。

Result: 研究发现，经过推理增强的OpenAI o1模型在所有三类数学任务中均保持了更高或接近完美的准确率。错误分析表明，程序性失误是最常见的错误类型，对整体性能影响最大，而概念性误解则较少。部署双代理配置显著提升了整体性能。

Conclusion: 这些发现为提升LLM性能提供了可操作的见解，并强调了将LLMs有效整合到数学教育中的策略，从而推进了AI驱动的教学实践和评估的精确性。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [131] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 本文提出一种基于特征分词的Transformer模型，能高效准确地预测飞机实时预计抵达时间（ETA），并在新加坡樟宜机场数据上验证其性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 实时飞机预计抵达时间（ETA）对于航空抵达管理，特别是跑道排序至关重要。在快速变化的空域环境中，实时抵达管理系统要求ETA预测同时具备高效率和高精度。

Method: 研究采用基于特征分词的Transformer模型来高效预测飞机ETA。特征分词将原始输入映射到潜在空间，Transformer的多头自注意力机制捕获投影的关键方面，从而减少了复杂特征工程的需求。其并行计算能力使其能以1HZ的高频率处理ETA请求。模型输入包括飞机经纬度、地速、机场方位角、时间、天气和尾流湍流类别等原始数据。该方法在新加坡樟宜机场（WSSS）应用，使用2022年10月一个月的ADS-B数据进行验证，评估范围覆盖距离机场10NM至300NM的飞机。

Result: 提出的方法比常用的基于增强树的模型（如XGBoost）准确率提高了7%，同时计算时间仅为其39%。实验结果还表明，在空域中有40架飞机的情况下，ETA推理时间仅为51.7微秒。

Conclusion: 本研究提出的飞机ETA预测方法在准确性和效率上均表现出色，对于实时抵达管理系统具有广阔的应用前景。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [132] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: 针对多模态情感分析中噪声干扰和关键信息丢失问题，本文提出MoLAN框架，通过细粒度、模态感知的动态去噪，有效抑制噪声并保留关键信息，MoLAN+达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常受无关或误导性视觉和听觉信息干扰。现有方法通常将整个模态信息作为独立单元进行去噪，这可能在抑制冗余信息的同时丢失关键信息。

Method: 提出MoLAN框架，通过模态感知分块将每种模态特征划分为多块。依据每块的噪声水平和语义相关性动态分配去噪强度，实现细粒度噪声抑制并保留关键信息。MoLAN是统一且灵活的框架，可集成至多种多模态模型，并在此基础上提出MoLAN+。

Result: 在五种模型和四个数据集上的实验证明了MoLAN框架的广泛有效性。MoLAN+取得了最先进的性能。

Conclusion: MoLAN框架通过细粒度、模态感知的动态去噪，有效解决了多模态情感分析中的噪声干扰问题，并在多个模型和数据集上展示了其普适性和优越性，MoLAN+达到了最先进水平。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [133] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 利用基于LLM Transformer的上下文学习(ICL)理论，提出一种优化WiFi 7信道接入的方法，以解决动态环境下吞吐量低和节点密度估计不准的问题。


<details>
  <summary>Details</summary>
Motivation: WiFi 7中广泛使用的二进制指数退避（BEB）方案在动态信道环境下吞吐量性能较差；现有基于模型的方法在已知固定节点密度下优化，但节点密度估计不准确导致显著的吞吐量损失。

Method: 首次提出基于LLM Transformer的上下文学习（ICL）理论用于优化信道接入。设计Transformer-based ICL优化器，通过构建包含碰撞阈值数据示例和查询碰撞案例的提示作为Transformer输入，以生成预测的竞争窗口阈值（CWT）。开发高效算法训练Transformer以实现有效ICL，并在有限训练步数内保证接近最优CWT预测。进一步扩展以允许提示中存在错误数据输入，并理论证明其优化器能保持与最优值的最小预测和吞吐量偏差。

Result: 在NS-3模拟实验中，该方法展示了快速收敛性，并在未知节点密度环境下实现了接近最优的吞吐量性能，优于现有基于模型和DRL的方法。

Conclusion: 该研究成功提出了基于LLM Transformer的ICL理论来优化WiFi 7的信道接入，有效解决了动态环境下吞吐量低和节点密度估计不准确的问题，并在实际应用中展现出强大的鲁棒性和优越性能。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [134] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: 本文介绍Motif-2.6B，一个2.6亿参数的基础LLM，通过创新的架构增强（如差分注意力、PolyNorm）实现高性能和高效率，并在多项基准测试中超越同类SOTA模型，旨在民主化LLM能力。


<details>
  <summary>Details</summary>
Motivation: 开发高性能且计算高效的基础大型语言模型（LLM）仍具挑战，特别是对于新兴研究团队，现有模型难以在性能与效率之间取得平衡。

Method: 引入Motif-2.6B（一个26亿参数的基础模型），通过整合创新的架构增强，包括差分注意力（Differential Attention）和PolyNorm激活函数，以提升长上下文理解、减少幻觉并增强上下文学习能力。通过大量实验确定最优架构。

Result: Motif-2.6B在各类基准测试中持续达到或超越同等规模的现有最先进模型，展示了其有效性、可扩展性和实际应用性。

Conclusion: Motif-2.6B显著推动了高效、可扩展和强大的基础LLM领域的发展，为未来的研究和部署提供了有价值的见解和坚实的基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [135] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 研究表明，在时序分析中，用简单的全连接层替代复杂的序列混合器（如注意力机制）可以达到相当甚至更优的性能，挑战了复杂模型更优的假设。


<details>
  <summary>Details</summary>
Motivation: 时序分析（TSA）中，序列混合器是核心机制。然而，近期研究质疑复杂序列混合器的必要性，认为简单架构也能表现良好。因此，作者探究TSA中常用序列混合器是否真正必要。

Method: 提出JustDense方法，该方法在MatrixMixer框架下，系统地将各种成熟TSA模型中的序列混合器替换为全连接层，以隔离并研究混合操作的作用。

Result: 在29个基准测试和七种SOTA TSA模型上的广泛实验表明，用全连接层替换序列混合器能够达到相当甚至更优的性能。

Conclusion: 研究结果挑战了TSA中“更深、更复杂的架构固有地更好”的假设，证明了简单架构的有效性，即使在特定情况下复杂混合器有优势，也突显了简单设计的潜力。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [136] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: 本文提出DIG2RSI，一个基于深度学习的框架，结合I-G变换和2SRI（工具变量技术），以解决复杂网络中同伴因果效应估计时存在的同步反馈和未观测混淆问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂真实世界网络中估计同伴因果效应极具挑战性，主要由于同伴间的同步反馈和未观测混淆因素。现有方法要么忽略反馈，要么仅限于线性假设，导致无法准确估计同伴效应。

Method: DIG2RSI首先通过I-G变换分离相互同伴影响并消除同步反馈偏差。为处理未观测混淆，它从网络数据构建有效工具变量。在2SRI的第一阶段，训练神经网络利用这些工具变量预测同伴暴露，并提取残差作为未观测混淆的代理。在第二阶段，拟合另一个深度神经网络，并加入对抗鉴别器，该鉴别器将残差作为控制函数，确保学习到的表示不含残余混淆信号。深度学习和对抗去偏增强了其消除偏差的能力。

Result: 通过在两个半合成基准数据集和一个真实世界数据集上的实证结果表明，DIG2RSI优于现有方法。

Conclusion: DIG2RSI在处理复杂、非线性、高维关系中同伴因果效应估计的同步反馈和未观测混淆方面表现出色，并能渐近恢复真实的同伴效应，提高了估计的准确性。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [137] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 本文提出AdaPO，一个在线强化学习框架，通过自适应奖励模型和奖励感知动态KL正则化机制，解决了大型多模态模型（LMMs）自评估中存在的奖励作弊和模型崩溃问题，显著提升了模型的直接推理和自评估能力。


<details>
  <summary>Details</summary>
Motivation: 自评估对于LMMs在多轮对话中实现自我提升至关重要，但在现有基础模型中普遍缺失。现有通过强化学习（RL）增强自评估的方法，因其固定的奖励机制在优化多训练目标时易导致“奖励作弊”和“模型崩溃”。

Method: 提出AdaPO，一个在线强化学习框架，能够根据各任务的当前训练状态实时自适应调整训练目标。为缓解奖励作弊，AdaPO引入了自适应奖励模型（ARM）来评估模型生成多轮轨迹性能的分布，并采用奖励感知动态KL正则化机制，用由不同多轮情境下的奖励差距调制的动态系数取代固定惩罚，从而无需人工干预即可根据子任务的训练进度自动调整学习焦点。

Result: 在8个基准测试和各种模型上的大量实验表明，AdaPO显著增强了模型的直接推理能力和自评估能力。

Conclusion: AdaPO通过其独特的自适应训练目标调整机制，成功解决了LMMs自评估中的奖励作弊挑战，为提升LMMs在多轮对话中的自我改进能力提供了一个有效且无需手动干预的解决方案。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [138] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 该研究提出了一个框架，通过可微分的后训练过程，对流匹配生成模型进行微调，以强制物理约束并解决科学系统中的逆问题。


<details>
  <summary>Details</summary>
Motivation: 在科学系统中，现有生成模型可能缺乏物理一致性。研究旨在使生成模型能够满足物理约束（如PDEs和边界条件），并从低保真或观测数据中推断未知物理输入（逆问题），同时不扭曲其学习到的分布。

Method: 该方法首先使用低保真或观测数据训练流匹配生成模型，然后应用可微分的后训练程序，通过最小化控制偏微分方程（PDEs）的弱形式残差来促进物理一致性和边界条件。为推断未知物理输入，模型将可学习的潜在参数预测器与生成过程相结合，并提出了一种联合优化策略。

Result: 结果模型能够生成物理有效的场解和对隐藏参数的合理估计，有效解决了病态逆问题。在经典PDE基准上的验证表明，该方法显著改善了PDE约束的满足度，并准确恢复了潜在系数。

Conclusion: 该方法成功地将生成建模与科学推理相结合，为模拟增强的发现和物理系统的数据高效建模开辟了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [139] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive是一个新颖的多目标强化学习框架，通过对抗性优化实现轨迹生成与评估的闭环协同进化，解决了自动驾驶中迭代决策和多维偏好标量化偏差问题，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹规划框架存在两大挑战：一是生成-评估框架分离，导致无法迭代优化；二是强化学习方法将多维偏好压缩为标量奖励，引入标量化偏差并掩盖关键权衡，难以实现类人迭代决策。

Method: 本文提出了EvaDrive框架，它是一个多目标强化学习框架，通过对抗性优化建立了轨迹生成与评估的闭环协同进化。该框架将轨迹规划视为多轮对抗博弈：分层生成器结合自回归意图建模和扩散式细化来提出候选路径；可训练的多目标批评器显式保留多样化的偏好结构而避免标量化偏差。通过帕累托前沿选择机制，实现迭代多轮细化，从而跳出局部最优并保持轨迹多样性。

Result: EvaDrive在NAVSIM和Bench2Drive基准测试中表现出SOTA性能：在NAVSIM v1上达到94.9 PDMS（超越DiffusionDrive 6.8，DriveSuprim 5.0，TrajHF 0.9），在Bench2Drive上达到64.96驾驶分数。此外，EvaDrive能通过动态加权生成多样化的驾驶风格，且无需外部偏好数据。

Conclusion: EvaDrive引入了一种创新的闭环对抗性框架，用于实现类人迭代决策，并提供了一种新颖的、无标量化偏差的轨迹优化方法，有效克服了传统方法的局限性。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [140] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本研究整合了15个数据集，构建了一个大型T1D和低血糖数据库，旨在解决数据稀缺问题，并分析了数据质量及葡萄糖与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病(T1D)患者依赖外部胰岛素，但胰岛素可能导致危险的低血糖。虽然机器学习模型能预测血糖并提供早期预警，但糖尿病和低血糖研究受限于大型数据集的缺乏。

Method: 系统性整合了15个现有数据集，创建了一个大型数据库，包含2510名受试者和每5分钟记录的血糖测量值。提取了两个子数据库（人口统计学数据和心率数据）。评估了数据质量，并进行了血糖水平与心率数据的相关性研究。

Result: 成功整合并创建了一个包含1.49亿次测量值（其中4%为低血糖范围）的数据库。该数据集在性别和不同年龄水平上分布均衡。研究揭示了数据不平衡和缺失值是显著挑战。发现低血糖发生前15至55分钟内，血糖水平与心率数据之间存在关联。

Conclusion: 所整合的大型数据库为糖尿病和低血糖研究提供了宝贵资源，有助于克服现有数据限制。尽管数据质量存在挑战，但通过整合和分析，揭示了潜在的预测标记（如心率），可为未来的机器学习模型和早期预警系统提供支持。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [141] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 本文提出物理引导记忆网络（PgMN），结合深度学习和基于物理的模型，解决建筑能耗预测中历史数据稀缺或缺失的挑战，提升模型在多种复杂场景下的准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 建筑能耗预测对资源管理和可持续发展至关重要。深度学习模型在数据有限时表现不佳，而基于物理的模型虽然不依赖历史数据，但建模耗时且参数设置复杂。因此，需要一种结合两者优势、克服各自局限性的预测方法。

Method: 引入物理引导记忆网络（PgMN），融合深度学习和基于物理模型的预测结果。PgMN包含平行投影层（处理不完整输入）、记忆单元（处理持续偏差）和记忆经验模块（扩展预测范围）。模型在短期（小时分辨率）能耗预测上进行理论和实验评估。

Result: 理论评估证明PgMN各组件的数学有效性。实验验证表明，PgMN在新建建筑、数据缺失、历史数据稀疏和动态基础设施变化等多种场景下均表现出高准确性和广泛适用性。

Conclusion: PgMN为动态建筑环境下的能耗预测提供了一个有前景的解决方案，显著增强了模型在历史数据受限或缺失以及基于物理模型不足场景下的应用能力。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [142] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 提出一种基于自编码器和定制windowSHAP的无监督可解释AI框架，用于实时检测、识别和表征核反应堆中的重放攻击，并在真实数据上达到高精度。


<details>
  <summary>Details</summary>
Motivation: 新一代核反应堆依赖全数字化仪表和控制系统，产生大量多变量时间序列数据。确保数据完整性以防欺骗攻击（如重放攻击）对安全运行至关重要。现有重放攻击检测方法普遍存在局限性，如未能识别异常根本原因、过度依赖合成数据及简化假设，且缺乏对真实数据下重放攻击的表征和预测可解释性。

Method: 本文提出一个基于自编码器和定制windowSHAP算法的无监督可解释AI（XAI）框架。该框架旨在全面表征实时重放攻击（包括检测、源识别、时序和类型），尤其是在动态变化的反应堆过程中。研究团队在普渡大学PUR-1核反应堆的多个真实世界数据集上对该框架进行了基准测试，其中包含多达六个并发重放信号。

Result: 所提出的XAI框架在所有测试案例中，均能以95%或更高的准确率检测并识别出重放信号的来源、数量以及伪造持续时间。

Conclusion: 该无监督可解释AI框架在检测和全面表征核反应堆系统中的复杂实时重放攻击方面表现出色，并利用真实世界数据解决了现有方法的局限性，为未来高级核反应堆的安全可靠运行提供了重要的可解释性支持。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [143] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为可调序列长度（ASL）的新型方案，通过混合精度截断显著降低了物联网（IoT）场景下随机计算（SC）神经网络的能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）是物联网等资源受限场景下部署神经网络的低功耗替代方案。尽管SC相比浮点设计能显著降低能耗，但针对SC神经网络的逐层混合精度实现仍未充分探索，这是进一步提升效率的瓶颈。

Method: 本文提出ASL方案，将混合精度概念应用于SC神经网络。它引入了一个基于算子范数的理论模型来预测截断噪声的累积传播。通过随机森林（RF）回归进行扩展敏感性分析，验证理论预测。同时，提出了粗粒度（coarse-grained）和细粒度（fine-grained）两种截断策略以适应不同应用场景。

Result: 在32纳米工艺下合成的流水线SC MLP上的评估表明，ASL方案可在保持可忽略的精度损失的前提下，将能耗和延迟开销降低60%以上。

Conclusion: ASL方案在物联网应用中是可行的，并突出显示了SC设计中混合精度截断的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [144] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 针对智能交通系统ABM中高维人口合成的数据稀疏性问题，本文提出一种新型基于扩散模型的人口合成方法，旨在有效恢复缺失的“采样零”并最小化无效的“结构零”。实验结果表明，该方法在合成人口的可行性和多样性上优于现有VAE和GAN等模型。


<details>
  <summary>Details</summary>
Motivation: 人口合成是智能交通系统ABM的关键输入，但当描述代理的属性数量增多时，调查数据因“维度诅咒”而稀疏，难以准确建模人口联合分布。现有深度生成模型虽能合成“采样零”（真实存在但样本缺失的数据），但往往会伴随产生“结构零”（真实不存在的无效组合），影响合成人口的准确性。

Method: 本研究提出了一种新颖的基于扩散模型的人口合成方法，用于估计人口的潜在联合分布。该方法旨在恢复大量缺失的“采样零”，同时将生成的“结构零”保持在最低限度。

Result: 通过与变分自编码器（VAE）和生成对抗网络（GAN）等现有方法进行比较，结果表明所提出的方法在实现合成人口的“可行性”和“多样性”之间达到了更好的平衡，表现优于以往的方法。

Conclusion: 所提出的基于扩散模型的人口合成方法能有效解决高维数据稀疏性问题，在恢复真实缺失数据和避免生成无效数据之间取得更优平衡，为智能交通系统中的ABM提供更准确、更真实的人口数据输入。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [145] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: 针对ECG因布局差异导致的数据异步和缺失问题，本研究提出PatchECG框架。该框架通过掩码训练和自适应缺失表示学习，有效识别不同布局ECG中的心律失常，并在鲁棒性和性能上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ECG诊断模型难以应对因不同医院ECG布局差异造成的数字化信号导联时间异步和部分缺失，这严重挑战了其诊断能力。

Method: 本研究提出PatchECG框架，采用基于掩码训练策略的自适应可变块计数缺失表示学习。该方法通过自动关注导联间具有协作依赖的关键补丁，以实现在不同布局ECG中对心律失常的关键识别。实验在PTB-XL数据集和21388张通过ECG图像工具生成的异步ECG图像上进行，并以23个子类别作为标签。

Result: PatchECG在不同布局下表现出强大的鲁棒性，平均AUROC达0.835，且性能稳定不受布局变化影响。在朝阳医院400张真实ECG图像的外部验证中，房颤诊断AUROC达0.778；在12x1布局ECG上，AUROC达0.893。该结果优于各种经典插值和基线方法，且相较于当前最优的大规模预训练模型ECGFounder，AUROC分别提升了0.111和0.19。

Conclusion: PatchECG框架成功解决了ECG布局差异带来的数据挑战，实现了对不同布局ECG中心律失常的鲁棒、准确识别，其性能显著优于现有主流方法，具有重要的临床应用潜力。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [146] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 针对将创意转化为SVG图形耗时的问题，论文引入了大规模SVG-1M数据集及SVGen模型，实现从自然语言高效生成SVG代码，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将创意想法转化为精确的矢量图形（特别是SVG）是一个耗时且具挑战性的过程。

Method: 1. 构建了大规模高质量的SVG-1M数据集，包含配对的SVG与自然语言描述，并通过高级数据增强和思维链（Chain of Thought）注释进行对齐。2. 基于此数据集，提出了一个端到端模型SVGen，用于从自然语言输入生成SVG代码。3. 该方法通过课程学习和强化学习优化，以确保语义准确性和结构完整性。

Result: 实验表明，SVGen在有效性和效率方面均优于通用大型模型和传统渲染方法。

Conclusion: 本研究通过提出创新的SVG-1M数据集和SVGen模型，有效解决了从自然语言描述生成高质量SVG的挑战，并展现出卓越的性能。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [147] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 本文提出一种轻量级、免训练的检索增强生成（RAG）方法，通过线性映射和迭代蒸馏技术，有效解决多模态大模型（LMMs）中的模态差距问题，以生成高质量的图像文本描述，并在基准数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（LMMs）的预训练和微调成本高昂，且现有预训练LMMs普遍存在“模态差距”，即文本和视觉表示在共享嵌入空间中存在错位。传统的微调方法虽能缓解此问题，但因需要大量特定领域数据而成本高昂且不切实际。

Method: 本文提出一种轻量级、免训练的方法，利用检索增强生成（RAG）机制，通过高效计算的线性映射来连接不同模态。在推理阶段，该映射应用于LMM嵌入的图像，以检索训练集中最接近的文本描述。这些描述结合指令作为语言模型的输入提示，用于生成新的文本描述。此外，还引入了一种迭代技术，通过语言模型生成合成描述来蒸馏映射，从而优化图像描述的衡量标准。

Result: 在两个基准多模态数据集上的实验结果表明，该方法取得了显著的性能改进。

Conclusion: 所提出的轻量级、免训练的RAG方法，通过线性映射和迭代蒸馏，成功克服了多模态大模型的模态差距问题，为生成图像文本描述提供了一种高效且实用的解决方案，并验证了其有效性。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [148] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP通过特征流形补全和类别原型对齐，解决了联邦学习在医学影像等非IID数据下的性能下降问题，效果优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在非独立同分布（non-IID）数据（尤其是在医学影像领域，图像特征分布差异显著）下，全局模型的收敛性和性能受到严重阻碍。该研究旨在解决这一挑战。

Method: 提出FedMP方法，通过随机特征流形补全来丰富客户端分类器的训练空间，并利用类别原型引导客户端间特征流形在语义一致子空间内对齐，以构建更清晰的决策边界。

Result: FedMP在多个医学影像数据集（包括真实世界多中心分布数据）和多域自然图像数据集上均优于现有联邦学习算法。此外，还分析了流形维度、通信效率和特征暴露的隐私影响。

Conclusion: FedMP有效提升了联邦学习在非IID场景下的性能，特别是在医学影像领域，并通过实验验证了其优越性，并分析了相关影响因素。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [149] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: 提出DQT框架，通过嵌套整数表示和位移操作，解决了动态量化中昂贵的反量化-再量化瓶颈，实现了高效自适应AI。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署深度神经网络时，现有量化方法面临挑战：静态量化无法适应 varying complexity，而动态混合精度量化虽优越但需要昂贵的浮点转换循环（反量化-再量化），破坏了纯整数硬件范式并影响性能增益。

Method: 引入动态量化训练（DQT）框架，其核心是嵌套整数表示，其中低精度值按位嵌入高精度值中。结合自定义纯整数算术，通过接近零成本的位移操作实现即时位宽切换。DQT首次实现了主干网络的无反量化静态混合精度，以及通过轻量级控制器实现的真正高效的动态、基于实例的量化。

Result: DQT在CIFAR-10上的ResNet18和ImageNet上的ResNet50上取得了最先进的性能。在ImageNet上，4比特动态ResNet50实现了77.00%的top-1精度，在可比的BitOPs预算下优于领先的静态(LSQ, 76.70%)和动态(DQNET, 76.94%)方法。关键是，DQT的位宽转换成本仅为28.3M次简单位移操作，相比之前动态方法所需的56.6M次昂贵乘加(MAC)浮点操作，大幅降低。

Conclusion: DQT通过消除昂贵的反量化瓶颈，开启了高效、自适应AI的新领域，为资源受限设备上的深度神经网络部署提供了更高效的解决方案。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [150] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: 提出scAGC，一种基于自适应图学习和对比引导的单细胞聚类方法，有效克服传统和现有图神经网络方法在处理高维、稀疏及长尾分布单细胞数据时的局限性，并取得了卓越的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 在单细胞RNA测序(scRNA-seq)数据分析中，准确的细胞类型注释至关重要。然而，scRNA-seq数据的高维度和大量零元素对传统聚类方法构成挑战。现有基于图神经网络的方法常依赖静态图结构，对噪声敏感且难以捕捉单细胞群体的长尾分布。

Method: 本文提出scAGC，一个端到端的单细胞聚类方法。该方法通过对比引导学习自适应细胞图，并同步优化特征表示和图结构。具体而言，它引入了一个拓扑自适应图自编码器，利用可微分的Gumbel-Softmax采样策略动态优化图结构，以缓解长尾度分布问题。同时，集成零膨胀负二项式（ZINB）损失来鲁棒地重建特征，并引入对比学习目标以正则化图学习过程，确保稳定性和收敛性。

Result: 在9个真实的scRNA-seq数据集上的综合实验表明，scAGC持续优于其他最先进的方法，分别在9个和7个数据集上获得了最佳的NMI和ARI分数。

Conclusion: scAGC通过创新的自适应图学习和鲁棒的特征建模，有效克服了单细胞聚类中的关键挑战，为准确的细胞类型注释提供了一个优越且高性能的解决方案。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [151] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 针对联邦学习在车联网中非独立同分布数据导致的性能问题，本文提出LCSFLA方案，通过引入长期客户端选择和真实拍卖机制，优化客户端选择并确保信息真实性，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在车联网（IoV）中面临非独立同分布（non-IID）数据导致的模型收敛和精度问题。现有客户端选择方法存在资源浪费（需在本地训练完成后选择）和信息不对称（客户提交虚假信息）的挑战，在资源受限的IoV环境下尤为突出。

Method: 本文提出了一种新颖的基于真实拍卖的长期客户端选择联邦学习方案（LCSFLA）。该方案通过新的评估机制综合考虑长期数据质量和能源成本，以最大化社会福利。同时，引入了一个带有存款要求的拍卖机制，以激励客户端参与并确保其提供信息的真实性。该激励机制的激励兼容性和个体理性得到了理论证明。

Result: 在包括IoV场景在内的各种数据集上的实验结果表明，LCSFLA方案能有效缓解由非独立同分布数据引起的性能下降问题。

Conclusion: LCSFLA通过创新的长期数据质量评估和真实拍卖机制，成功解决了联邦学习在IoV环境下非独立同分布数据带来的性能挑战，并克服了传统客户端选择方法的局限性，提升了联邦学习模型的鲁棒性和准确性。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [152] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 本综述全面审视了基于接触式和非接触式呼吸分析技术，重点关注机器学习和深度学习在其中应用，探讨了从呼吸速率检测到疾病诊断的广泛应用场景，并指出了挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸分析方法在舒适性和实用性方面存在挑战，尤其不适用于长期监测，因此需要探索非侵入性、更便捷的呼吸监测技术，以提升健康监测能力。

Method: 本文采用综述形式，全面分析了接触式和非接触式（如Wi-Fi信道状态信息和声学传感）呼吸分析方法，并深入探讨了应用于这些方法的机器学习和深度学习技术，包括数据预处理、特征提取和分类技术。

Result: 综述分析了非接触式方法在提供准确、无创呼吸监测方面的能力，并探索了从单用户呼吸速率检测到多用户场景、用户识别和呼吸疾病检测等广泛应用。此外，还提供了不同机器学习/深度学习模型的比较性见解，并讨论了数据集稀缺、多用户干扰、数据隐私等关键挑战以及可解释AI、联邦学习、迁移学习、混合建模等新兴趋势。

Conclusion: 本综述通过整合现有方法并识别开放研究方向，为呼吸分析领域的未来创新提供了一个全面框架，旨在将先进技术能力与实际医疗保健应用相结合。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [153] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 本研究提出细粒度安全神经元（FGSN）与免训练持续投影方法，通过精确定位和调整安全神经元，有效降低了大型语言模型微调过程中的安全风险，同时保持了模型效用和持续防御能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的微调服务在注入领域知识的同时，挑战了其原有对齐机制并带来了安全风险。现有的后微调防御策略多依赖粗粒度的安全层映射，未能全面兼顾安全层与细粒度神经元，导致难以有效平衡模型的安全性和实用性。

Method: 我们提出了细粒度安全神经元（FGSN）与免训练持续投影方法，以降低微调安全风险。FGSN内在整合了安全层与神经元之间的多尺度交互，能够定位更稀疏、更精确的细粒度安全神经元，同时最小化对下游任务神经元的干扰。随后，我们将安全神经元参数投影到安全方向上，从而提高模型安全性并使其更符合人类偏好。此外，通过引入任务特异性、多维异构安全神经元簇优化机制，实现了持续防御和对未知新兴安全问题的泛化能力。

Result: 在多个微调LLM模型上进行的大量实验表明，我们的方法在极少参数修改的情况下，显著降低了有害性分数和攻击成功率，同时完整保留了模型的实用性。

Conclusion: FGSN方法能够有效缓解微调LLMs带来的安全风险，通过精细化定位和投影安全神经元，在确保模型实用性的前提下显著提升安全性，并具备持续防御与泛化能力，为LLM的安全微调提供了新的解决方案。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [154] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一个LLM驱动的框架，通过将连续数值序列和非结构化文本语境统一为语言符号表示，提升了情境感知时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测因难以有效整合历史数值序列与非结构化文本语境特征，导致预测准确性受限。

Method: 提出TokenCast框架，具体步骤包括：1. 使用离散分词器将连续数值序列转换为时间符号（temporal tokens）。2. 通过预训练LLM将时间符号和语境符号嵌入共享表示空间，并进行自回归生成目标优化。3. 在统一语义空间上，监督微调LLM以预测未来时间符号。4. 将预测的时间符号解码回原始数值空间。

Result: 在多种富含语境特征的真实世界数据集上的广泛实验表明，TokenCast具有出色的有效性和泛化能力。

Conclusion: TokenCast通过LLM驱动的统一语言表示，成功解决了时间序列预测中整合数值和文本语境的难题，显著提升了预测性能。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [155] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出D2F策略，使扩散大语言模型(dLLMs)推理速度首次超越自回归(AR)LLMs，并在推理效率和生成质量上大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有开源扩散大语言模型(dLLMs)虽有并行解码潜力，但推理速度仍无法超越同等大小的自回归(AR)LLMs，阻碍了其作为AR-LLMs替代品的实际应用。

Method: 提出离散扩散强制(D2F)策略，将dLLMs改造为AR-扩散混合范式。D2F赋能dLLMs具备两项关键能力：1) 块级自回归生成以利用KV缓存；2) 无需完成前块即可预测后续令牌，实现块间并行解码。D2F通过基于预训练dLLMs的非对称蒸馏过程实现，并提出流水线并行解码算法。

Result: 经验证，D2F dLLMs在GSM8K数据集上的推理速度比LLaMA3和Qwen2.5快2.5倍以上。相较于香草dLLMs（如LLaDA和Dream），在保持可比输出质量的同时，加速超过50倍。

Conclusion: D2F成功打破了dLLMs推理速度逊于AR LLMs的壁垒，通过混合范式显著提升了dLLMs的推理效率，并有望成为高效文本生成的新选择。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [156] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 本文提出MIPCGRL方法，通过多目标表示学习和句子嵌入，显著提升了复杂多目标自然语言指令下程序内容生成的可控性。


<details>
  <summary>Details</summary>
Motivation: 现有指令式强化学习程序内容生成（IPCGRL）方法在处理复杂、多目标自然语言指令时，难以有效利用文本输入的丰富性，导致内容生成的可控性受限。

Method: 提出MIPCGRL，一种用于指令式内容生成器的多目标表示学习方法。该方法将句子嵌入作为条件，并通过结合多标签分类和多头回归网络来有效训练多目标嵌入空间。

Result: 实验结果表明，所提出的MIPCGRL方法在多目标指令下，将内容生成的可控性提高了高达13.8%。

Conclusion: MIPCGRL处理复杂指令的能力使得内容生成更具表现力和灵活性。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [157] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 本文提出一种分层联邦微调框架，结合LoRA和UCB-DUAL算法，以解决车联网（IoV）中基础模型在移动、异构和间歇性连接环境下的高效低延迟适应问题，并在仿真中取得了显著的准确性和效率提升。


<details>
  <summary>Details</summary>
Motivation: 联邦微调是使基础模型适应边缘环境（如车联网）的有前景方法。然而，车联网中客户端的移动性、资源异构性和连接间歇性，给实现高效、低延迟的多任务适应带来了巨大挑战。

Method: 提出分层联邦微调框架，协调路边单元（RSU）和车辆进行资源感知和移动弹性学习。利用低秩适应（LoRA），设计去中心化、能量感知的秩适应机制，并将其建模为受约束的多臂老虎机问题。开发了一种新颖的UCB-DUAL算法，实现任务能耗预算下的自适应探索，并提供了次线性遗憾的理论证明。通过基于真实轨迹的大规模IoV模拟器进行评估。

Result: 与所有基线方法相比，该方法实现了最佳的准确性-效率权衡，将延迟降低了超过24%，并将平均准确率提高了超过2.5%。

Conclusion: 所提出的分层联邦微调框架及其UCB-DUAL算法，能够有效应对动态车联网环境中基础模型的适应挑战，在准确性、效率和延迟方面展现出优越的性能。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [158] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 本研究提出了一个基于元学习的框架，旨在为去中心化AI系统中的大规模模型部署自动选择最优的推理加速方法，以降低成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 大规模模型（如LLMs）的部署因计算需求高昂而导致成本巨大。为降低成本并解决可扩展性及数据安全问题，去中心化系统成为趋势，其中选择高效的推理加速方案至关重要。

Method: 引入了一个基于元学习的框架，通过学习不同加速技术在各种任务上的历史性能数据，实现对最佳加速方法的自动化和系统化选择，区别于传统的随机选择或专家直觉方法。

Result: 该元学习框架不仅简化了决策过程，而且在效率和性能方面持续优于传统方法。

Conclusion: 研究结果突出了推理加速在去中心化AI系统中的巨大潜力，为实现更民主、经济可行的AI解决方案提供了途径。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [159] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 本文提出了一种名为ADT4Coupons的新型营销框架，旨在通过对平台与用户之间复杂的顺序交互进行建模，优化优惠券分发策略，以提升长期收入，并在真实数据集上展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能有效利用平台与用户间复杂的顺序交互数据，导致性能停滞，尽管有大量电商日志数据可供利用，因此需要一个能提升长期收入的有效策略。

Method: 基于平台对不同用户进行多次顺序优惠券分发决策的营销场景，提出Aligned Decision Transformer for Coupons (ADT4Coupons) 框架。该框架通过整合通用场景适应性、基于更全面历史数据的顺序建模以及高效迭代更新三大特性，直接制定优惠券分发策略以提升长期收入。

Result: 在真实的工业数据集以及公开和合成数据集上的实证结果表明，所提出的ADT4Coupons框架具有优越性。

Conclusion: ADT4Coupons框架能够有效应对复杂的顺序交互，为平台优化优惠券分发策略以实现长期收入增长提供了一个更优的解决方案。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [160] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 本文介绍了CSDataset，一个整合了结构化和非结构化数据的全面多级施工安全数据集，以弥补现有数据不足，并促进机器学习和大语言模型在安全研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有施工安全数据集数量有限且缺乏多样性，阻碍了深入分析，构成研究空白。

Method: 提出了“施工安全数据集”（CSDataset），一个从OSHA获取的、包含事故、检查和违规记录的全面多级数据集。该数据集独特地结合了结构化属性和非结构化叙述。此外，论文还使用该数据集进行了初步的方法基准测试和各种跨层级分析。

Result: 通过数据集分析发现，例如，由投诉驱动的检查可使后续事故发生的可能性降低17.3%。该数据集旨在促进机器学习和大语言模型驱动的多种分析方法。

Conclusion: CSDataset的引入及其初步分析为解决施工安全数据不足提供了新的资源，有望推动机器学习和大语言模型在该领域的研究，并为未来的安全改进提供洞察。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [161] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: 本文提出MoQE（Mixture of Quantization Experts），一种基于MoE架构的量化推理框架，通过结合多个量化专家并动态路由输入数据，有效提升量化模型的性能，且不显著增加推理延迟。


<details>
  <summary>Details</summary>
Motivation: 量化方法对于提升模型效率和降低部署成本至关重要，但在资源受限设备上应用时，量化过程不可避免地会引入精度下降问题。

Method: 提出MoQE框架，它将一个全精度模型的多个量化变体作为“量化专家”，并根据输入数据特性动态地将数据路由到最合适的专家。为CV和NLP任务设计了轻量级、结构感知型路由器模型。

Result: 在ResNet、LLaMA和Qwen模型家族以及ImageNet、WikiText、C4、OpenWebText等基准数据集上的实验表明，MoQE性能可与SOTA量化模型媲美，且未显著增加推理延迟。

Conclusion: MoQE通过引入专家混合架构，有效缓解了单一量化模型中常见的性能下降问题，为量化模型的广泛应用提供了一种高效且高性能的解决方案。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [162] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出一种基于可微分传输模块的激光选择性转移修复算法，显著减少微LED制造的转移步骤并提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 在微LED高通量制造中，激光选择性转移过程需要计算模型来规划转移序列以最小化XY平台移动，并适应不同优化目标。现有算法（局部搜索、强化学习）存在性能、灵活性、特征工程或训练速度慢等局限性，需要一种高效、灵活且可扩展的修复方案。

Method: 提出首个基于可微分传输模块的修复算法，该模块能模拟传输平台的离散位移，并通过基于梯度的优化进行训练。与强化学习方法不同，该方法无需手工设计特征提取器。

Result: 与局部邻近搜索算法相比，本方法实现了卓越的修复性能，并支持更灵活的目标设计（例如最小化步骤数）。训练速度显著加快，可扩展至大型阵列。实验表明，在2000x2000阵列上，转移步骤减少了50%，规划时间在2分钟以内。

Conclusion: 该方法为AR/VR和下一代显示器制造中的微LED修复提供了一种实用且适应性强的解决方案，显著加速了微LED制造过程。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [163] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 提出Hi-Vec，通过多层动态适应机制，解决了测试时适应中单层模型难以处理复杂数据偏移的问题，显著提升了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时适应（Test-time adaptation, TTA）方法主要依赖于单维线性分类层，难以有效处理多样化和复杂的领域偏移（distribution shifts）。

Method: 提出Hi-Vec（Hierarchical Adaptive Networks with Task Vectors），通过将编码器的表示空间分解为多个层级组织且尺寸递增的层，实现动态测试时适应。其核心贡献包括：1. 动态层选择，自动识别每个测试批次的最优适应层。2. 权重合并机制，将动态层的权重合并到其他层，确保所有层接收目标信息。3. 线性层一致性，作为门控函数防止在噪声批次上进行错误微调。

Result: Hi-Vec在挑战性场景和多个目标数据集上进行了严格评估，结果表明它显著提高了模型鲁棒性，解决了不确定性问题，并能有效处理有限批次大小和增加的异常值率。

Conclusion: Hi-Vec显著提升了测试时适应的能力，能够有效地应对复杂和多样的领域偏移，并超越了现有最先进的方法。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [164] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: 本文提出GSMT，一个结合图注意力网络、序列到序列RNN和任务校正器的混合模型，旨在克服数据受限环境下城市公交车轨迹预测的挑战，并在真实世界数据上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 在智能交通系统，特别是在多模态数据受限的发展中地区城市环境中，准确的公交车轨迹预测至关重要。尽管存在固有的挑战，仅依赖车载GPS数据仍然是必不可少的。

Method: 提出GSMT混合模型，该模型整合了图注意力网络（GAT）和序列到序列循环神经网络（RNN），并加入了一个任务校正器。任务校正器通过聚类历史轨迹来识别复杂的行为模式，并对GAT和RNN生成的预测进行微调。GSMT融合动态公交信息和静态站点信息，采用两阶段方法进行多节点轨迹预测。

Result: 在马来西亚吉隆坡的真实世界数据集上的实验表明，GSMT方法显著优于现有方法，在短期和长期轨迹预测任务中均取得了卓越的性能。

Conclusion: GSMT混合模型通过结合深度学习和行为模式校正，有效解决了在数据受限的复杂城市环境中公交车轨迹预测的精度问题，展现出优越的预测能力。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [165] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 本研究提出一种结合量子启发图神经网络（QI-GNN）和集成模型（含CP分解层）的新方法，用于区块链反洗钱中的非法交易检测，并实现了74.8%的F2分数。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的金融科技领域，区块链网络中非法交易的检测仍是一个关键挑战，需要强大和创新的解决方案。

Method: 结合量子启发图神经网络（QI-GNN）与QBoost或随机森林分类器等集成模型，并引入了图神经网络框架内的规范多项式（CP）分解层，以增强复杂数据结构的处理和分析能力。该系统专为区块链网络的反洗钱（AML）分析而设计。

Result: 与经典机器学习实现相比，该方法在检测欺诈交易方面达到了74.8%的F2分数。

Conclusion: 量子启发技术（辅以CP层结构改进）在复杂网络分析中不仅能与传统方法匹敌，甚至可能超越它们，从而增强金融安全。研究结果倡导在金融领域更广泛地采用和进一步探索量子启发算法，以有效打击欺诈。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [166] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 本文提出一种基于LLM的原型估计框架，通过无示例提示生成特征值构建零/少样本原型，实现训练和微调无关的表格数据学习，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLMs)在表格数据建模中潜力巨大，但在零样本(zero-shot)和少样本(few-shot)场景中有效利用它们仍是挑战。

Method: 提出一种新颖的LLM-based原型估计框架。核心思想是通过“无示例提示”（仅依赖任务和特征描述）查询LLM生成特征值，进而构建无需训练的零样本原型。该原型可通过融合少量样本进一步增强，且避免了训练分类器或微调LLM。

Result: 广泛的实验证明了所提出方法在零样本和少样本表格学习中的有效性。

Conclusion: 该框架通过无示例提示和原型估计，绕过了基于示例提示的限制，提供了一个可扩展且鲁棒的表格数据零/少样本学习方案。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [167] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 该研究利用深度学习（ResCNN和AttentionCNN集成模型）分析清醒小鼠嗅球的局部场电位（LFPs），实现了对嗅觉事件的单次、鲁棒检测，性能显著优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有气味检测技术在处理复杂混合物和实现可靠的单次检测方面存在局限性。本研究旨在开发一种通用的气味检测系统，并验证两个核心假设：(i) 局部场电位（LFPs）的频谱特征是否足以进行鲁棒的单次气味检测；(ii) 仅凭嗅球信号是否足够。

Method: 研究提出并使用了一个由互补的一维卷积网络（ResCNN和AttentionCNN）组成的集成模型。该模型旨在从多通道嗅球LFPs中解码气味的存在。实验在7只清醒小鼠的2,349个试验中进行了测试。

Result: 最终的集成模型支持了两个研究假设，取得了显著的性能提升，包括86.6%的平均准确率、81.0%的F1-score和0.9247的AUC，明显优于以往的基准。此外，t-SNE可视化证实了该框架捕获了生物学上重要的特征。

Conclusion: 研究结果证实了从细胞外LFPs实现气味存在鲁棒单次检测的可行性，并展示了深度学习模型在深入理解嗅觉表征方面的巨大潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [168] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 本研究提出一种衡量图神经网络（GNNs）过压缩（over-squashing）的新指标，并用其评估图重连（rewiring）技术的效果。发现重连能有效缓解图分类任务中的过压缩问题，但在节点分类任务中效果不明显甚至可能有害。同时提供了一个实用工具。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）虽然性能卓越，但存在过压缩问题——即长距离信息被指数级压缩，限制了其表达能力。图重连技术有望缓解此问题，但由于缺乏直接的经验性过压缩度量标准，其实际效果尚不明确。

Method: 我们提出了一种严格的、以拓扑为中心的方法，通过节点对之间相互敏感度的衰减率来评估过压缩。我们将这种两两评估扩展到四个图级别统计量（普遍性、强度、变异性、极端性）。结合这些度量和图内因果设计，我们量化了重连策略如何影响不同图和节点分类基准上的过压缩。

Result: 大多数图分类数据集都存在不同程度的过压缩，且重连能有效缓解。然而，缓解程度及其转化为性能提升的效果因数据集和方法而异。在节点分类数据集中，过压缩不那么明显，重连常会增加过压缩，并且性能变化与过压缩变化不相关。

Conclusion: 重连在过压缩严重且适度修正时最为有益。过于激进的重连，或应用于轻微过压缩图的重连，不太可能带来帮助，甚至可能损害性能。我们提供的即插即用诊断工具可以帮助实践者在训练前决定重连是否值得尝试。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [169] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文研究了协作式多智能体强化学习（c-MARL）在真实世界条件下对对抗性攻击的脆弱性，提出了一种简单高效的算法，通过扰动观测来误导智能体感知，并在多个基准测试中验证了其有效性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 协作式多智能体强化学习（c-MARL）虽已迅速发展并应用于现实世界，但对其对抗性攻击脆弱性的全面调查不足。现有研究多关注训练时攻击或不现实场景，而缺乏对部署智能体观测扰动等更真实受限条件的探究。

Method: 在假设攻击者只能收集并扰动已部署智能体的观测（或完全无访问权限）的更真实和受限条件下，研究新的脆弱性。提出简单高效的算法，生成对抗性扰动，旨在使受害智能体对其环境的感知产生偏差。

Result: 所提方法在三个基准和22个环境中进行了实证验证，证明了其在不同算法和环境中的有效性。此外，该算法样本效率高，仅需1,000个样本，远低于以往方法所需的数百万个样本。

Conclusion: 研究成功揭示了c-MARL在真实条件下对观测扰动的脆弱性，并提供了一种高效的攻击方法，为c-MARL的安全性研究和实际应用提供了重要见解。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [170] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 该研究提出一种自动化、可解释的框架，利用变分自编码器和注意力机制从学生代码中发现“基于模式的知识组件”（KCs），以提升计算机科学教育中的个性化学习和知识追踪能力。


<details>
  <summary>Details</summary>
Motivation: 个性化学习依赖于准确的学生知识建模。尽管知识组件（KCs）是基础，但从学生代码中自动提取KCs面临挑战，原因在于已发现KCs的解释性不足，以及编程问题的开放性导致学生解决方案结构多样且概念交互复杂。

Method: 本文提出一种新颖、可解释的框架，通过识别学生代码中重复出现的结构模式（即“基于模式的KCs”）来实现KCs的自动化发现。具体方法是：训练一个变分自编码器（VAE），在一个可解释的、基于注意力的代码表示模型的引导下，从学生代码中生成重要的代表性模式，该模型能够识别正确和不正确的模式实现。这些模式随后被聚类以形成基于模式的KCs。评估方法采用学习曲线分析和深度知识追踪（DKT）。

Result: 实验结果表明，所提出的KCs能够展示有意义的学习轨迹，并且相比传统知识追踪方法，在深度知识追踪（DKT）的预测性能上取得了显著提升。

Conclusion: 这项工作为计算机科学教育中的知识建模提供了一个自动化、可扩展且可解释的框架，用于识别细粒度的代码模式和算法结构，这对学生的学习至关重要，从而推动了该领域的发展。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [171] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 该研究将数据集蒸馏应用于强化学习环境，将其压缩为单批次监督学习数据集，从而实现高效学习并展示了跨模态转换能力。


<details>
  <summary>Details</summary>
Motivation: 将大型数据集压缩为小型合成数据集以近似原始数据集上的学习，并探索数据集蒸馏在强化学习任务中的通用性及其将强化学习转换为监督学习的能力。

Method: 将强化学习环境蒸馏成单批次监督学习数据集。提出了近端策略优化（PPO）的元学习扩展，并将其应用于多维倒立摆问题、MuJoCo环境和Atari游戏。

Result: 成功将复杂的强化学习环境压缩为一步监督学习任务，证明了强化学习蒸馏对不同学习器架构的通用性，并实现了环境到最小合成数据集的蒸馏。

Conclusion: 数据集蒸馏能够有效压缩复杂的强化学习任务并将其转换为高效的监督学习形式，即便使用极小的合成数据集也能实现快速学习。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [172] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 本文提出一个结合联邦学习、区块链（以太坊）和IPFS的去中心化天气预报框架，通过信誉投票机制增强安全性，实验证明其能提升预测准确性、系统韧性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前中心化天气预报系统面临安全漏洞、可扩展性有限及单点故障的挑战，而天气预报对灾害应对、农业和资源管理至关重要。

Method: 提出一个去中心化天气预报框架，整合联邦学习（FL）以保护数据隐私并降低传输开销，利用以太坊区块链确保模型更新的透明和可靠验证，引入基于信誉的投票机制评估模型可信度，并使用IPFS进行高效的链下存储。

Result: 实验结果表明，所提出的方法不仅提高了天气预报的准确性，还显著增强了系统的韧性和可扩展性。

Conclusion: 该去中心化框架为在实际、安全关键环境中部署天气预报系统提供了一个可行的解决方案。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [173] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: GNN在关键应用中易受对抗性攻击。本文提出GNNev，一个用于GNN的精确验证方法，可计算属性和结构扰动下的鲁棒性保证，支持多种聚合函数（包括首次支持max和mean），并在实验中显示出优于现有工具的性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）被广泛应用于高风险场景，但易受对抗性攻击。现有提供对抗性鲁棒性保证的技术对消息传递GNN中常用的聚合函数支持不足，需要一种精确的方法来验证其鲁棒性。

Method: 开发了一种精确（完备且可靠）的GNN验证方法，用于计算GNN在属性和结构（边添加或删除）扰动下的鲁棒性保证，并受预算约束。该方法专注于节点分类任务，采用约束求解和边界收紧技术，并通过迭代求解一系列松弛的约束满足问题及利用求解器的增量求解能力来提高效率。实现了一个名为GNNev的通用求解器，支持sum、max和mean三种聚合函数，其中max和mean是首次得到支持。

Result: GNNev在两个标准基准数据集（Cora和CiteSeer）和两个真实世界欺诈数据集（Amazon和Yelp）上进行了广泛的实验评估。结果表明GNNev具有可用性和有效性，并且在sum聚合的节点分类任务上，其性能优于现有的精确验证工具。

Conclusion: 本文提出了一种名为GNNev的精确验证方法，用于评估GNN在对抗性扰动下的鲁棒性，克服了现有工具在聚合函数支持方面的局限性。实验证明GNNev是一种有效且高效的GNN鲁棒性验证工具，尤其在特定任务上表现出卓越性能，增强了GNN在关键应用中的可靠性。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [174] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 本文提出一种模仿生物大脑突触修剪的基于权值幅度的动态剪枝方法，作为Dropout的替代，在训练中渐进移除低重要性连接。该方法在多种时间序列预测模型上表现出色，尤其在金融预测中显著降低误差，是传统Dropout的有效替代。


<details>
  <summary>Details</summary>
Motivation: 生物大脑通过修剪弱连接提高效率，而人工神经网络中的Dropout正则化是随机停用神经元，未考虑基于活动的重要性修剪，这与生物机制不符。

Method: 提出一种基于权值幅度的动态突触修剪方法。该方法作为Dropout的替代，直接整合到训练循环中，通过计算跨层权值的绝对幅度确定重要性。它采用立方调度渐进增加全局稀疏性，并在固定间隔通过修剪掩码永久移除低重要性权值，同时保持活跃权值的梯度流，从而无需单独的修剪和微调阶段。

Result: 在RNN、LSTM和Patch Time Series Transformer等多种时间序列预测模型及四个数据集上的实验表明，该方法持续领先，并经Friedman检验（p < 0.01）证实具有统计学意义上的显著改进。在金融预测中，它相对于无或标准Dropout的模型将平均绝对误差（MAE）降低高达20%，相对于特定Transformer模型降低高达52%。

Conclusion: 该动态剪枝机制通过将权值消除与渐进稀疏化结合，推动了正则化技术发展，并易于集成到各种架构中。其强大的性能，特别是在金融时间序列预测中的表现，表明其作为传统Dropout技术的实用替代方案的巨大潜力。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [175] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是一个基于Ricci曲率和流的几何推荐框架，用于在动态金融图上进行根源归因和风险感知排名。


<details>
  <summary>Details</summary>
Motivation: 在动态金融图中，量化局部压力并追踪冲击传播，以实现对金融事件根源的归因，并构建风险感知的推荐系统。

Method: 提出RicciFlowRec框架，通过Ricci曲率和流分析动态金融图（包含股票、宏观指标、新闻互动）。利用离散Ricci曲率量化局部压力，通过Ricci流追踪冲击传播，并基于曲率梯度揭示因果子结构，从而构建结构风险感知的排名函数。

Result: 在S&P 500数据上，结合FinBERT情感分析的初步结果表明，在合成扰动下，系统表现出更高的鲁棒性和可解释性。

Conclusion: 该研究支持基于曲率的根源归因和早期风险感知的排名功能，是首个将几何流推理应用于金融决策支持的推荐器。未来的工作计划包括投资组合优化和回报预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [176] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 本研究发现，在特定领域（如医学文本）训练稀疏自编码器（SAEs）能显著提升其对大型语言模型激活的分解能力，提高重建精度和特征可解释性，优于在广泛数据集上训练的SAEs。


<details>
  <summary>Details</summary>
Motivation: 传统的稀疏自编码器在通用数据上训练时，其有限的潜在特征预算只能捕获高频通用模式，导致重建误差（“暗物质”）显著，且潜在特征碎片化或相互吸收，难以解释。研究旨在解决这些限制，以获得更准确和可解释的LLM激活分解。

Method: 研究方法是将JumpReLU SAEs的训练范围限制在特定领域（使用19.5万个临床问答示例的医学文本），目标是Gemma-2模型第20层的激活。通过与在广泛领域训练的SAEs进行对比，并结合自动化和人工评估，来衡量重建保真度和可解释性。

Result: 结果显示，领域限制型SAEs能多解释高达20%的方差，实现更高的损失恢复，并减少线性残余误差。学习到的特征与临床上有意义的概念高度吻合，而非频繁但不具信息量的token。这些SAEs捕获了相关的线性结构，留下了更小、更纯粹的非线性残差。

Conclusion: 研究得出结论，领域限制能有效缓解通用领域SAEs的关键局限，从而实现更完整和可解释的潜在分解。这暗示了在通用型SAEs领域可能需要重新审视“基础模型”的规模化策略，支持领域定制化的重要性。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [177] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 本研究探讨了文生图模型将痴呆症相关语音信息与生成图像对齐的能力，发现仅凭生成图像即可实现75%的痴呆症检测准确率，并利用可解释性方法揭示了语言中对检测有贡献的部分。


<details>
  <summary>Details</summary>
Motivation: 文生图模型在自然语言描述与图像生成之间表现出高度对齐能力，但对于病理语音（特别是痴呆症相关语音）与生成图像之间的对齐能力，目前研究甚少。因此，本研究旨在探究文生图模型是否能实现这种对齐，并在此基础上发展解释对齐的方法。

Method: 本研究检验了文生图模型对齐痴呆症相关语音信息与生成图像的能力，并开发了解释这种对齐的方法。具体而言，研究人员利用可解释性方法来展示语言的哪些部分对痴呆症的检测有所贡献。

Result: 研究发现，仅通过文生图模型生成的图像，就可以实现痴呆症检测，在ADReSS数据集上达到了75%的准确率。此外，利用可解释性方法，研究人员成功揭示了语言中哪些部分对痴呆症检测起到了关键作用。

Conclusion: 文生图模型能够有效地将痴呆症相关的语音信息与视觉图像对齐，这使得仅从生成的图像中检测痴呆症成为可能，并且可以通过可解释性方法识别出语言中对检测有贡献的关键因素。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [178] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的金融风险评估框架，通过结合特征注意力机制和时序建模结构，在不共享原始数据的前提下实现跨机构联合建模，有效提升了金融风险识别的效率和安全性，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在跨机构金融风险分析中，存在数据隐私保护和协同建模的挑战，传统方法难以在保护数据隐私的同时实现高效的联合风险识别。

Method: 本文提出一个基于联邦学习的风险评估框架。该框架采用分布式优化策略，各金融机构在本地训练子模型，并使用差分隐私和噪声注入保护模型参数后上传。中央服务器聚合这些参数以生成全局模型，用于识别系统性风险。模型设计中融入了特征注意力机制和时序建模结构。

Result: 实验结果表明，所提出的模型在通信效率、模型准确性、系统性风险检测和跨市场泛化能力方面，均优于传统的集中式方法和现有的联邦学习变体。它展示了强大的建模能力和在敏感金融环境中的实际价值。

Conclusion: 该方法在保护数据主权的同时，增强了风险识别的广度和效率，为智能金融风险分析提供了一个安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [179] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出一种针对分布式后端服务系统的无监督异常检测方法，结合图卷积和Transformer模型，有效处理结构依赖和时序行为，实现端到端检测。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统中存在的复杂结构依赖、多样行为演化以及缺乏标注数据等实际挑战。

Method: 构建基于服务调用关系的动态图，利用图卷积提取高阶结构表示；使用Transformer模型建模节点时间行为；通过可学习的联合嵌入机制融合结构与行为表示；最后应用非线性映射计算异常分数，实现无监督端到端检测。

Result: 在真实云监控数据上的实验表明，该方法在多个关键指标上优于现有模型，在捕获异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性。

Conclusion: 该方法具有很高的实际部署潜力，能有效应对分布式后端系统的异常检测挑战。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [180] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: 本文提出DGS-MAML，一种结合梯度匹配和锐度感知最小化的新型元学习算法，旨在解决有限数据下的跨任务泛化问题，并在实验中展现出优越的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在训练数据有限的情况下，开发一种能够跨任务有效泛化的元学习算法，以增强模型适应性和鲁棒性。

Method: 引入DGS-MAML算法，该算法在一个双层优化框架中结合了梯度匹配和锐度感知最小化。研究还提供了基于PAC-Bayes和收敛性保证的理论分析。

Result: 在基准数据集上的实验结果表明，DGS-MAML在准确性和泛化能力方面均优于现有方法。

Conclusion: DGS-MAML算法特别适用于需要小样本学习和快速适应的场景。其源代码已公开提供。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [181] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 提出隐式超图神经网络（IHGNN），通过不动点方程解决传统超图神经网络在深层结构中长距离依赖捕获受限和训练不稳定的问题，并在实验中展现出卓越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的超图神经网络（HNNs）在建模高阶关系方面前景广阔，但其固定层数的显式消息传递机制限制了长距离依赖的捕获，且随着网络深度增加，训练可能变得不稳定。

Method: 引入隐式超图神经网络（IHGNN），将隐式平衡公式应用于超图。IHGNN通过求解非线性不动点方程来计算节点表示，从而在不依赖深层架构的情况下实现稳定高效的超边全局传播。研究还开发了可证明收敛的训练方案，分析了模型的过平滑条件和表达能力，并推导了超图上的转导泛化界限。此外，采用隐式梯度训练过程结合基于投影的稳定策略。

Result: 在引文基准测试上的大量实验表明，IHGNN在准确性和鲁棒性方面持续优于强大的传统图/超图神经网络基线模型。实验证明IHGNN对随机初始化和超参数变化具有弹性。

Conclusion: IHGNN为高阶关系学习提供了一个具有强大泛化能力和实用价值的模型，有效解决了传统超图网络在深层训练中的挑战。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [182] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: 本文提出NEXICA算法，该算法通过分析道路速度时间序列，识别高速公路系统中导致交通减速的因果源头，并在准确性和计算速度上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 道路交通拥堵是一个长期存在的问题。识别和针对拥堵的根本原因采取措施，是有效缓解交通减速的策略。现有因果发现算法在处理交通数据方面存在不足。

Method: 开发了NEXICA算法，以道路速度时间序列为输入进行因果发现。该方法有三个创新点：1) 仅关注时间序列中交通减速事件的发生与否；2) 建立概率模型，使用最大似然估计计算自发和被引起的减速概率；3) 训练二元分类器识别因果位置对。研究在洛杉矶地区195个高速公路速度传感器采集的六个月数据上进行了测试。

Result: 在洛杉矶地区的数据集上测试表明，NEXICA算法在准确性和计算速度方面均优于现有最先进的基线方法。

Conclusion: NEXICA算法能够有效发现高速公路交通拥堵的因果关系，并表现出卓越的性能，为缓解交通拥堵提供了新的有效工具。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [183] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune通过动态调节稀疏神经元，实现大型语言模型安全性与实用性的同步优化。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）安全对齐技术存在鲁棒性不足、误拒良性查询、文本质量和任务性能下降等问题，这些缺陷源于粗粒度的层级干预，导致安全性与实用性难以兼顾。

Method: 提出NeuronTune框架，通过归因方法识别所有层中对安全性关键和对实用性有益的神经元。然后，利用元学习（meta-learning）自适应地增强安全神经元的激活并抑制实用神经元的激活。该方法还支持通过神经元数量阈值灵活调整干预范围。

Result: 实验结果表明，NeuronTune显著优于现有最先进的技术，在实现卓越模型安全性的同时，保持了出色的实用性。

Conclusion: NeuronTune提供了一种细粒度的方法，有效解决了LLMs安全性与实用性之间的权衡问题，实现了两者的同步优化。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [184] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: 提出CoGenT框架，首次通过联合优化将对比学习与生成式学习范式结合，用于多元时间序列的自监督学习。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列的自监督学习主要有对比方法和生成方法两种范式，它们各自有效但尚未探索其互补潜力。

Method: 提出CoGenT（Contrastive Generative Time series framework），通过联合对比-生成优化统一两种范式。该框架旨在克服对比学习对时间数据高类内相似性的敏感性，并减少生成方法对大型数据集的依赖。

Result: 在六个不同的时间序列数据集上进行评估，CoGenT表现出显著改进，相对于独立的SimCLR和MAE，F1分数分别提升高达59.2%和14.27%。

Conclusion: 混合目标能够同时保持判别能力并获得生成鲁棒性，为时间域的混合自监督学习奠定了基础。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [185] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为FGCRN的新型开放集故障诊断模型，旨在解决多模态过程中已知故障分类和未知故障识别的难题，该模型通过精细化聚类和极端值理论实现高精度诊断。


<details>
  <summary>Details</summary>
Motivation: 在多模态过程中，可靠的故障诊断系统不仅需要准确分类已知健康状态，还需要有效识别未知故障。然而，同一健康状态的样本常呈现多簇分布，导致难以构建紧凑准确的决策边界。

Method: 提出了一种名为FGCRN的开放集故障诊断模型。该模型结合了多尺度深度卷积、双向门控循环单元和时序注意力机制来捕获判别性特征。设计了基于距离的损失函数以增强类内紧凑性。通过无监督学习构建精细化特征表示以揭示各健康状态的内在结构。最后，利用极端值理论建模样本特征与其精细化表示之间的距离，从而有效识别未知故障。

Result: 大量的实验结果表明，所提出的方法具有优越的性能。

Conclusion: FGCRN模型有效解决了多模态过程中故障诊断的挑战，在已知故障分类和未知故障识别方面均表现出色。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [186] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS是一种新型元神经网络架构搜索框架，采用混合搜索策略（贝叶斯优化与梯度上升），能发现任务感知型架构，并超越现有Meta-NAS方法，提升泛化能力和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 传统NAS仅针对单一任务，限制了实际应用。尽管Meta-NAS能利用先验知识适应新任务，但现有Meta-NAS方法存在泛化能力差、搜索空间受限或计算成本高的问题。

Method: 提出GraB-NAS框架，将神经网络架构建模为图。采用混合搜索策略：通过贝叶斯优化进行全局架构搜索，并结合在潜在空间中通过梯度上升进行局部探索以生成新颖架构。

Result: GraB-NAS能够发现具有强大性能的任务感知型架构，甚至超越预定义搜索空间。大量实验证明，GraB-NAS优于最先进的Meta-NAS基线，实现了更好的泛化能力和搜索效率。

Conclusion: GraB-NAS通过引入新颖的基于图的混合搜索策略，有效解决了现有Meta-NAS方法的局限性，显著提高了元神经网络架构搜索的泛化能力和搜索效率。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [187] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: 本文提出DeepFeatIoT深度学习模型，通过融合多种学习与非学习特征，有效提升物联网时间序列传感器数据分类性能，尤其在有限标注数据场景下，并超越现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 原始物联网时间序列数据存在元数据缺失、数据源异构、采样频率、测量单位和时间戳不一致等挑战，导致数据难以解释，削弱了智能系统的有效性。

Method: 提出新颖的深度学习模型DeepFeatIoT，该模型集成学习到的局部和全局特征、非学习的随机卷积核特征，以及来自大型语言模型（LLMs）的特征，以增强物联网时间序列传感器数据的分类能力。

Result: DeepFeatIoT模型在多个真实世界物联网传感器数据集上展现出一致且泛化的性能，尤其在标注数据有限的情况下，其表现优于现有最先进的基准模型。

Conclusion: 研究结果表明，DeepFeatIoT模型在推动物联网分析领域具有显著潜力，并能支持下一代智能系统的发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [188] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP是一种基于扩展图的结构化剪枝方法，旨在解决大型语言模型（LLMs）的计算和内存挑战，实现显著的加速、内存节省并超越现有剪枝技术的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用和规模增大，其部署面临日益严重的计算和内存挑战，因此迫切需要开发更高效的模型变体。

Method: 本文提出EGGS-PTP（Expander-Graph Guided Structured Post-training Pruning），一种基于扩展图引导的结构化训练后剪枝方法。该方法利用图论指导N:M结构化剪枝设计，并通过引入扩展图的概念确保剪枝网络中的信息流，从而有效减小模型大小和计算需求。

Result: 广泛的数值实验表明，EGGS-PTP不仅通过结构化稀疏性实现了显著的加速和内存节省，而且在各种LLM上的准确性方面优于现有的结构化剪枝技术。

Conclusion: EGGS-PTP成功解决了LLMs的计算和内存效率问题，通过创新的剪枝策略在保持模型功能的同时，大幅提升了性能并超越了现有方法的准确性，为LLM的高效部署提供了有效途径。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [189] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 在联邦持续学习(FCL)中，提出首个协作框架，通过轻量级本地模型作为动态桥梁，使基础模型(FMs)能够持续适应新任务并有效利用本地数据，同时克服遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 联邦持续学习(FCL)中的基础模型(FMs)面临挑战：难以利用本地私有数据，且因模型复杂性高而易遗忘旧知识，表现欠佳。小模型虽适用于资源受限的本地训练且CL技术成熟，但如何将小模型与FMs结合以解决FCL中的挑战是一个未充分探索的问题。

Method: 提出了首个FCL协作框架，其中轻量级本地模型作为动态桥梁，持续适应新任务并增强大型基础模型的效用。该框架包含两个新颖组件：1) “小模型持续微调”(Small Model Continual Fine-tuning)用于防止小模型发生时间性遗忘；2) “一对一蒸馏”(One-by-One Distillation)用于在服务器端对异构本地知识进行个性化融合。

Result: 实验结果表明，该框架展示了卓越的性能，即使客户端使用异构的小模型也能表现出色。

Conclusion: 该框架成功弥合了小模型和基础模型在联邦持续学习中的差距，有效解决了基础模型在FCL中利用本地数据和防止知识遗忘的挑战。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [190] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 提出MiCo框架，一个用于边缘AI应用的混合精度量化（MPQ）探索与部署的整体解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的混合精度量化（MPQ）算法在灵活性和效率上受限，难以理解不同方案的复杂影响，并且缺乏端到端的优化与部署框架。

Method: 本文提出MiCo框架，包含：1) 一种新颖的优化算法，用于在满足延迟约束下搜索最高精度的量化方案；2) 为不同硬件目标构建硬件感知延迟模型以实现快速探索；3) 支持从PyTorch MPQ模型直接部署到裸机C代码。

Result: MiCo框架实现了端到端的加速，同时将精度下降最小化。

Conclusion: MiCo提供了一个全面的MPQ探索与部署框架，能够有效地将混合精度量化模型部署到边缘AI设备，实现高效且高精度的表现。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [191] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 本论文提出CGAD框架，一种基于因果图的异常检测方法，用于关键基础设施的网络攻击检测。它通过学习因果结构并比较图拓扑偏差来识别异常，解决了传统方法在非平稳和不平衡数据中的局限性，并显著提升了检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对水处理网络等关键基础设施的网络攻击日益复杂，需要开发能够应对系统脆弱性及不断演变攻击模式的鲁棒异常检测策略。传统方法（统计、密度、图基模型）在处理多变量时间序列数据的分布漂移和类别不平衡问题时表现不佳，导致高误报率。

Method: 本文提出CGAD（基于因果图的异常检测）框架，一个两阶段的监督方法。首先，它使用动态贝叶斯网络学习在“正常”和“攻击”状态下的系统行为因果不变图结构（因果剖析）。其次，它采用结构散度通过因果图比较来检测异常，评估因果图随时间变化的拓扑偏差（异常评分）。

Result: CGAD在非平稳和不平衡时间序列环境中，相较于传统机器学习方法，展现出卓越的适应性和准确性。它以显著更高的精度检测网络攻击，并在F1和ROC-AUC分数上超越了四个工业数据集上的最佳基线，有效检测了延迟和结构复杂的异常。

Conclusion: 通过利用因果结构，CGAD框架不仅以更高的精度检测网络攻击，而且重新定义了异常检测的鲁棒性，在传统模型在不平衡和漂移下失效的情况下，证明了其韧性。CGAD为关键基础设施的网络攻击检测提供了一个更可靠和精确的解决方案。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [192] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: 本文提出Gauss-Tin方法，结合重放策略和高斯混合模型，并通过指令引导，旨在解决大语言模型（LLMs）中的灾难性遗忘问题，实验显示其能有效提高知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）取得了显著进展，但灾难性遗忘问题依然严峻，即模型在学习新信息时会遗忘先前获得的知识。持续学习（CL）策略，尤其是基于重放的技术，被视为潜在解决方案，但仍需进一步提升其知识保留性能。

Method: 本文引入了Gauss-Tin方法，该方法将重放策略与高斯混合模型（Gaussian mixture model）相结合，以优化训练期间的样本选择质量。此外，通过指令引导（instructional guidance）来促进对过去学习内容的生成，旨在通过策略性地强化重要的旧知识来提升LLMs的知识保留能力，同时适应新信息。

Result: 实验结果表明，Gauss-Tin在知识保留指标上比传统方法有6%的显著提升，证实了其在缓解LLMs灾难性遗忘方面的有效性。

Conclusion: Gauss-Tin是缓解大语言模型中灾难性遗忘的有效策略，这项研究强调了混合模型在增强LLMs在动态学习环境中的鲁棒性和适应性方面的潜力。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [193] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出一个统一且可解释的图神经网络（GNN）框架，通过解决现有预测性业务流程监控（PBPM）模型在架构、时序和语义上的局限性，显著提升下一事件预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的预测性业务流程监控（PBPM）模型发展不足，它们多依赖于短前缀子图或全局架构，未能充分捕捉时间相关性和转换语义，导致预测性能受限。

Method: 本研究提出了一个统一、可解释的GNN框架。主要方法包括：1. 比较基于前缀的图卷积网络（GCNs）和全轨迹图注意力网络（GATs），量化局部与全局建模的性能差距。2. 引入一种新颖的时间衰减注意力机制，构建动态的、以预测为中心的窗口，强调时间相关历史信息并抑制噪音。3. 将转换类型语义嵌入到边特征中，以实现对结构模糊轨迹的细粒度推理。此外，该架构还包含多级可解释性模块。

Result: 所提出的模型在五个基准数据集上进行了评估，无需针对每个数据集进行调优，即可实现具有竞争力的Top-k准确率和DL分数。

Conclusion: 本工作通过弥补架构、时序和语义方面的空白，为预测性业务流程监控中的下一事件预测提供了一个鲁棒、可泛化且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [194] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: 该论文提出SYNAPSE-G，一个利用LLMs生成稀有事件合成数据，并通过图传播进行半监督学习，以解决标记数据稀缺的冷启动问题并有效扩充标签的方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据，特别是稀有事件的标记数据稀缺，阻碍了训练有效的机器学习模型，尤其是在处理冷启动问题时。

Method: 该研究提出了SYNAPSE-G流水线，首先利用大型语言模型（LLMs）生成稀有事件的合成训练数据，以解决冷启动问题。这些合成数据作为种子，在一个由种子与大量未标记数据构建的相似图上进行半监督标签传播，以识别候选阳性样本。这些样本随后由预言机（人工或LLM）进行标记。扩充后的数据集用于训练/微调分类器。此外，论文还从理论上分析了合成数据（有效性和多样性）的质量如何影响方法的精度和召回率。

Result: 在不平衡的SST2和MHS数据集上的实验结果表明，SYNAPSE-G在发现阳性标签方面是有效的，并且其性能优于包括最近邻搜索在内的基线方法。

Conclusion: SYNAPSE-G通过结合LLMs生成合成数据与图上的半监督标签传播，有效解决了稀有事件分类中标记数据稀缺的问题，并提升了模型在发现阳性标签方面的能力。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [195] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: 本综述探讨了世界模型在边缘通用智能（EGI）中的应用潜力，分析其架构、在各种边缘场景中的主动应用、与基础模型和数字孪生体的协同，并指出未来挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管世界模型在机器人和游戏领域表现出巨大潜力，但其在无线边缘计算中与边缘通用智能（EGI）的集成仍未得到充分探索，本研究旨在填补这一空白。

Method: 本文通过综述形式，首先考察了世界模型的架构基础（如潜在表示学习、动力学建模、基于想象的规划），接着阐述了它们在车载网络、无人机网络、物联网和网络功能虚拟化等EGI场景中的主动应用。此外，还探讨了世界模型与基础模型、数字孪生体的协同，并识别了开放挑战与未来研究方向。

Result: 世界模型被定位为边缘通用智能（EGI）的认知骨干，能够通过主动预测、想象未来轨迹和多步规划，显著增强边缘AI系统在延迟、能耗和隐私约束下的优化能力。本研究展示了其在各种EGI场景中提升性能的潜力。

Conclusion: 本综述为实现下一代智能自主边缘系统提供了概念基础和实用路线图，明确了世界模型在EGI中的核心作用，并指出了未来在安全性、高效训练和部署方面需克服的挑战。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [196] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文提出有限选择性预测（PLS）模型，研究预测起始时间受限场景下的最优预测误差，引入复杂度度量，并证明其界限与随机实例高度匹配。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测模型假设预测者可以随时开始预测，但在许多实际场景中，预测者只能在时间范围的特定子集上开始预测，这促使本研究引入并分析具有有限选择性的预测模型。

Method: 引入了“有限选择性预测（PLS）”模型，其中预测者只能在时间范围的子集上开始预测。研究方法包括对最优预测误差进行逐实例分析和平均案例分析。此外，还引入了一个复杂度度量，以提供与实例相关的最优误差界限。

Result: 引入的复杂度度量能够提供与实例相关的最优误差界限。对于随机生成的PLSP模型实例，这些界限以高概率与实际结果匹配。

Conclusion: 论文成功地引入并分析了有限选择性预测（PLS）模型，该模型更贴近现实中预测起始时间受限的情况。通过提出的复杂度度量，论文能够有效地量化最优误差，并且其界限在随机实例上表现出高度的准确性。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [197] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 本文提出GDCC框架，通过测量“因果能力”来识别关键状态作为子目标，有效解决强化学习中复杂环境下探索效率低的问题，显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，因果推断有助于智能体进行定向探索，但由于复杂场景下状态-动作空间巨大，难以有效衡量因果关系。

Method: 提出GDCC框架。首先，定义“因果能力”作为状态空间中的因果关系度量，表示智能体行为对未来轨迹的最高影响。其次，采用基于蒙特卡洛的方法（针对连续高维环境进行优化）识别具有高因果能力的关键点，并将其作为子目标指导探索。

Result: 经验证，高因果能力状态与预期子目标一致。与基线方法相比，GDCC在多目标任务中显著提高了成功率。

Conclusion: GDCC通过因果能力引导子目标发现，为高效探索提供了一种有效方法，从而提升了强化学习任务的性能。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [198] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [199] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG是一个多模态因果推理框架，它利用大语言模型解析多元时间序列的变量语义信息，构建知识图谱并与数值数据融合，显著提升了预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽视了变量名称和数据描述中蕴含的丰富语义信息（即关键领域知识），而这些信息对于构建鲁棒且可解释的模型至关重要。

Method: TimeMKG框架利用大语言模型解释变量语义并构建多元知识图谱；采用双模态编码器分别建模知识图谱生成的语义提示和历史时间序列的统计模式；通过跨模态注意力机制在变量层面融合这些表示，将因果先验注入到下游的预测和分类任务中。

Result: 在多种数据集上的实验结果表明，结合变量层面的知识显著提升了模型的预测性能和泛化能力。

Conclusion: 通过将变量层面的知识融入时间序列建模，TimeMKG框架成功地将传统信号处理提升为知识驱动的推理，不仅显著改善了模型性能和泛化能力，还提供了明确且可解释的因果先验来指导模型推理。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [200] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的统计框架，通过高斯过程模型克服了现有热蛋白质组分析（TPP）方法的局限性，能够更灵活、准确地分析蛋白质热稳定性数据，尤其适用于复杂或非常规的熔解曲线。


<details>
  <summary>Details</summary>
Motivation: 现有热蛋白质组分析（TPP）工作流程存在关键局限性：它们通常假设S型熔解曲线，并受限于经验零分布，导致仅能识别约5%的数据为显著变化。这使得传统方法在分析蛋白质组范围内的扰动（如通路抑制、基因修饰或环境压力）时，可能遗漏重要的生物学相关变化，并且难以处理具有非常规熔解曲线的蛋白质。

Method: Thermal Tracks采用高斯过程（GP）模型并结合平方指数核函数，能够灵活建模任何形状的熔解曲线。同时，它通过核先验生成无偏的零分布，从而克服了传统方法中经验零分布的统计限制。

Result: 该框架在分析全蛋白质组范围内由扰动引起的蛋白质热稳定性显著变化方面表现出色，能够捕获传统TPP方法可能遗漏的生物学相关变化。此外，Thermal Tracks特别擅长分析具有复杂、非S型熔解曲线的蛋白质，例如相分离蛋白和膜蛋白。

Conclusion: Thermal Tracks提供了一个可访问且灵活的Python工具，通过创新的统计方法显著提升了蛋白质热稳定性数据分析的能力，尤其在处理全蛋白质组扰动和非常规熔解曲线方面表现优异，为蛋白质热谱分析研究提供了强大的支持。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [201] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: WAAN是一个跨层框架，通过TinyML代理和半稳定集合点，在6G网络中实现意图感知的主动切换，以确保移动场景下的用户体验连续性。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向AI驱动、用户中心的生态系统演进，传统的被动切换机制在移动边缘计算和自主代理服务场景中表现出局限性。

Method: 引入WAAN跨层框架，通过在异构边缘节点嵌入轻量级TinyML代理，实现意图传播和网络自适应；同时，整合半稳定集合点作为协调锚点，用于上下文传输和状态保存，以确保移动性引起中断后的连续性。

Result: 通过一个多模态环境控制案例研究，展示了WAAN框架的操作能力及其在移动性下有效保持用户体验的能力。

Conclusion: WAAN框架能有效应对6G网络中移动性带来的挑战，保持用户体验。文章还探讨了WAAN部署和演进的关键挑战与未来机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [202] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: 提出SP-LLM框架，将预测数字孪生与大语言模型结合，实现车载边缘计算中任务卸载和资源分配的语义感知、主动式优化。


<details>
  <summary>Details</summary>
Motivation: 当前车载边缘计算（VEC）管理系统是固定的、反应式的，在动态车载环境中表现不佳，因其受限于静态优化目标且仅基于当前网络状态决策。

Method: 提出语义感知主动式LLM编排（SP-LLM）框架。通过将传统数字孪生（DT）转换为预测数字孪生（pDT）来预测关键网络参数（如任务到达、车辆移动、信道质量）。核心是一个大语言模型（LLM），作为认知编排器，利用pDT的预测进行主动式任务卸载和资源分配决策。LLM能理解自然语言的语义命令，动态调整优化策略以适应变化的目标（如紧急服务优先、能效优化）。

Result: 通过广泛仿真证明，SP-LLM在可扩展性、波动条件下的鲁棒性和适应性方面，显著优于现有反应式和基于多智能体强化学习（MARL）的方法。

Conclusion: 该框架能将人类意图转化为最佳网络行为，从而实现更智能、自主和目标驱动的车载网络。

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [203] [Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs](https://arxiv.org/abs/2508.09150)
*Pietro Piscione,Leonardo Lossi,Maziar Nekovee,Chathura Galkandage,Phil O Connor,Simon Davies*

Main category: cs.NI

TL;DR: 本文展示了一个概念验证（PoC），演示了如何将5G高级网络功能与CAPIF集成，以增强汽车应用的连接性。


<details>
  <summary>Details</summary>
Motivation: 为汽车应用提供增强的连接性，并优化5G网络性能和资源利用。

Method: 设计并实现了一个概念验证（PoC）。该PoC通过CAPIF暴露的标准3GPP NEF API，实现对移动网络性能的持续监控和5G用户设备视频流的按需动态QoS调整。此外，还实现了流量重定向至边缘以优化性能。

Result: 该PoC成功展示了移动网络性能的持续监控、特定5G用户设备视频流的QoS动态调整、以及通过边缘重定向改善延迟和优化网络资源利用。

Conclusion: 通过将5G高级网络功能与CAPIF集成，可以有效地为汽车应用提供增强的连接性，并通过动态QoS调整和边缘计算优化网络性能和资源利用。

Abstract: This paper presents the design and implementation of a Proof of Concept (PoC)
that demonstrates how 5G Advanced Network Functions can be integrated with the
Common API Framework (CAPIF) to support enhanced connectivity for automotive
applications. The PoC shows the continuous monitoring of the mobile network
performance and the on-demand and dynamic adaptation of Quality of Service
(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using
standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,
traffic flows are redirected to the edge to improve latency and optimize
network resource utilization.

</details>


### [204] [Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission](https://arxiv.org/abs/2508.09151)
*Chang Wu,Yuang Chen,Yiyuan Chen,Fengqian Guo,Xiaowei Qin,Hancheng Lu*

Main category: cs.NI

TL;DR: 本文提出了一种基于生理信号的VR流媒体QoE建模与优化框架，并结合DRL应用于RAN，以缓解分辨率突变并优化传输策略。


<details>
  <summary>Details</summary>
Motivation: VR流媒体中分辨率的剧烈变化（尤其是从高到低）会严重影响用户体验（QoE），而现有QoE模型和传输方案未能充分解决这些变化带来的感知影响。

Method: 1. 首次提出了一种创新的生理信号驱动的QoE建模和优化框架，充分利用用户的脑电图（EEG）、心电图（ECG）和皮肤活动信号，精确捕捉生理响应和分辨率变化的时间动态。2. 将所提出的QoE框架通过深度强化学习（DRL）集成到无线接入网络（RAN）中，实现了自适应传输策略，动态分配无线资源，以应对用户移动性引起的信道变化并调整帧分辨率。

Result: 与基线相比，所提出的解决方案实现了分辨率88.7%的提升和切换81.0%的减少。

Conclusion: 实验结果证明了这种生理信号驱动策略的有效性，强调了边缘AI在沉浸式媒体服务中的巨大潜力。

Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly
impair the quality-of-experience (QoE) of users, particularly during
transitions from high to low resolutions. Existing QoE models and transmission
schemes inadequately address the perceptual impact of these shifts. To bridge
this gap, this article proposes, for the first time, an innovative
physiological signal-driven QoE modeling and optimization framework that fully
leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin
activity signals. This framework precisely captures the temporal dynamics of
physiological responses and resolution changes in VR streaming, enabling
accurate quantification of resolution upgrades' benefits and downgrades'
impacts. Integrated the proposed QoE framework into the radio access network
(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission
strategies have been implemented to allocate radio resources dynamically, which
mitigates short-term channel fluctuations and adjusts frame resolution in
response to channel variations caused by user mobility. By prioritizing
long-term resolution while minimizing abrupt transitions, the proposed solution
achieves an 88.7\% improvement in resolution and an 81.0\% reduction in
handover over the baseline. Experimental results demonstrate the effectiveness
of this physiological signal-driven strategy, underscoring the promise of edge
AI in immersive media services.

</details>


### [205] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 开发了一种AI/ML驱动的5G核心网故障分析引擎，用于分类PCAP文件中的帧，识别异常，并利用LLM提供修复建议，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有5G核心网故障分析方法耗时且需大量人工投入，导致故障定位和解决效率低下，影响网络性能。

Method: 本文提出一个AI/ML驱动的故障分析(FA)引擎。该引擎利用自然语言处理(NLP)技术分析5G核心网PCAP文件以分类成功和故障帧，并识别异常。此外，它集成了一个基于5G核心网文档训练的大型语言模型(LLM)，用于提供故障修复建议和基于3GPP标准等的错误解释。

Result: 机器学习模型在测试数据集上（使用80-20比例的成功与失败PCAP文件进行训练）展现出高分类准确率。该引擎显著减少了故障分析所需的人工时间并提高了效率。

Conclusion: 所提出的AI/ML驱动FA引擎能够高效准确地识别5G核心网流量中的故障，并提供智能修复方案，从而大幅提高网络分析效率。未来工作包括将其扩展至4G网络及其他形式的网络数据。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [206] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran是一种面向6G网络的AI驱动代理市场，它通过协调多服务所有者之间的冲突目标，显著提升网络切片性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络需要协调多个服务所有者之间常常相互冲突的目标，而目前的网络切片控制器则过于僵化、受限于策略且缺乏业务情境感知。

Method: Agoran引入了一个代理市场，将决策权分布给三个自主AI分支：立法分支（使用检索增强型LLM处理合规查询）、行政分支（通过观察者更新的向量数据库维持实时态势感知）和司法分支（使用基于规则的信任评分评估代理消息并仲裁LLM以检测恶意行为）。同时，利益相关者侧的谈判代理和SRB侧的调解代理通过多目标优化器生成帕累托最优报价，并在单轮内达成共识意图，随后部署到Open和AI RAN控制器。此外，一个1B参数的Llama模型经过少量数据微调后能达到接近GPT-4的决策质量。

Result: 在私有5G测试床上的评估显示，Agoran取得了显著效果：eMBB切片吞吐量增加了37%，URLLC切片延迟降低了73%，同时与静态基线相比，端到端PRB使用节省了8.3%。一个1B参数的Llama模型在6 GiB内存限制下，仅用1.3秒即可收敛，并在短时间微调后恢复了约80%的GPT-4.1决策质量。

Conclusion: Agoran为实现超灵活、以利益相关者为中心的6G网络提供了一条具体且符合标准的路径。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [207] [WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking](https://arxiv.org/abs/2508.09166)
*Wei Guo,Shunsei Yamagishi,Lei Jing*

Main category: cs.NI

TL;DR: WPTrack系统结合单Wi-Fi链路的信道状态信息（CSI）和压力鞋垫数据，实现了精确的室内单目标人体跟踪和初始定位。


<details>
  <summary>Details</summary>
Motivation: 物联网演进对室内定位提出更高要求，但现有WiFi人体跟踪方案多依赖特殊设备或多链路。单Wi-Fi链路方案在初始位置获取和方向估计盲点方面存在挑战，限制了其在常见室内环境中的应用。

Method: 本文提出WPTrack，首次融合Wi-Fi和压力鞋垫数据进行单目标跟踪。系统采集单Wi-Fi链路的CSI和90个鞋垫传感器的压力数据。通过CSI计算相位差和多普勒速度，通过压力数据计算行走速度。随后，构建CSI-压力融合模型，以准确确定初始位置并实现精确人体跟踪。

Result: 仿真结果显示初始位置定位精度介于0.02 cm至42.55 cm之间。在真实环境中采集的实验数据表明，轨迹跟踪结果与实际轨迹高度吻合。

Conclusion: WPTrack通过创新性地融合单Wi-Fi链路CSI和压力鞋垫数据，有效解决了单Wi-Fi链路人体跟踪的初始定位和精度难题，为智能家居和老年护理等应用提供了可靠的室内定位解决方案。

Abstract: As the Internet of Things (IoT) continues to evolve, indoor location has
become a critical element for enabling smart homes, behavioral monitoring, and
elderly care. Existing WiFi-based human tracking solutions typically require
specialized equipment or multiple Wi-Fi links, a limitation in most indoor
settings where only a single pair of Wi-Fi devices is usually available.
However, despite efforts to implement human tracking using one Wi-Fi link,
significant challenges remain, such as difficulties in acquiring initial
positions and blind spots in DFS estimation of tangent direction. To address
these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure
Insoles Fusion System for Single Target Tracking. WPTrack collects Channel
State Information (CSI) from a single Wi-Fi link and pressure data from 90
insole sensors. The phase difference and Doppler velocity are computed from the
CSI, while the pressure sensor data is used to calculate walking velocity.
Then, we propose the CSI-pressure fusion model, integrating CSI and pressure
data to accurately determine initial positions and facilitate precise human
tracking. The simulation results show that the initial position localization
accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results
obtained from experimental data collected in a real-world environment closely
align with the actual trajectory.

</details>


### [208] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: webMCP通过在网页中嵌入结构化交互元数据，显著降低AI代理的网页处理开销，提高AI辅助网页交互的效率和经济性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理理解网页需要大量处理，导致AI辅助的网页交互缓慢且昂贵，这阻碍了用户与AI代理的有效协作。

Method: 引入webMCP（Web Machine Context & Procedure），一个客户端标准，将结构化交互元数据直接嵌入网页。它为AI代理提供页面元素与用户操作之间的明确映射，使代理能直接访问预结构化数据，而非处理整个HTML文档。

Result: 在1,890次真实API调用（涵盖在线购物、认证、内容管理）的评估中，webMCP将处理需求降低了67.6%，任务成功率保持在97.9%（传统方法为98.8%）。用户成本显著降低（34-63%），响应时间更快。统计分析证实这些改进非常显著，且WordPress部署研究验证了其实用性，无需服务器端修改即可部署。

Conclusion: webMCP是使AI网页辅助更易于访问和可持续的有效解决方案，弥合了生产环境中用户交互需求与AI计算要求之间的关键差距。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [209] [Camel: Energy-Aware LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2508.09173)
*Hao Xu,Long Peng,Shezheng Song,Xiaodong Liu,Ma Jun,Shasha Li,Jie Yu,Xiaoguang Mao*

Main category: cs.NI

TL;DR: 本文提出一个大语言模型（LLM）边缘推理能耗管理框架，通过优化GPU频率和批量大小来平衡延迟和能耗，实验证明可显著降低能耗延迟积（EDP）。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多部署于云端，面临网络延迟、隐私和带宽限制。将LLM部署到边缘设备是重要研究方向，但边缘推理对延迟和能耗要求高，如何在有限电池容量下平衡二者是关键挑战。

Method: 提出一个LLM推理能耗管理框架，通过优化GPU频率和批量大小，平衡延迟和能耗。该框架通过有效管理配置搜索中的探索-利用困境来寻找最优设置。在NVIDIA Jetson AGX Orin平台上进行了实现和实验验证。

Result: 与默认配置相比，所提出的框架将能耗延迟积（EDP）降低了12.4%-29.9%，在能耗和延迟之间取得了更好的平衡。

Conclusion: 所提出的能耗管理框架能有效优化边缘设备上的LLM推理，显著提升能效，实现了延迟与能耗的优良平衡。

Abstract: Most Large Language Models (LLMs) are currently deployed in the cloud, with
users relying on internet connectivity for access. However, this paradigm faces
challenges such as network latency, privacy concerns, and bandwidth limits.
Thus, deploying LLMs on edge devices has become an important research focus. In
edge inference, request latency is critical as high latency can impair
real-time tasks. At the same time, edge devices usually have limited battery
capacity, making energy consumption another major concern. Balancing energy
consumption and inference latency is essential. To address this, we propose an
LLM inference energy management framework that optimizes GPU frequency and
batch size to balance latency and energy consumption. By effectively managing
the exploration-exploitation dilemma in configuration search, the framework
finds the optimal settings. The framework was implemented on the NVIDIA Jetson
AGX Orin platform, and a series of experimental validations were conducted.
Results demonstrate that, compared to the default configuration, our framework
reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance
between energy consumption and latency.

</details>


### [210] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: 本文提出HiSTM模型，结合双空间编码器、Mamba时序模块和注意力机制，用于解决蜂窝流量预测中复杂的时空模式难题，并在精度和计算效率上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对于网络规划、资源分配和负载均衡至关重要，但由于用户移动导致的复杂时空模式，准确预测非常困难。现有基于AI的预测模型往往难以兼顾预测准确性和计算效率。

Method: 提出分层时空Mamba (HiSTM) 模型，其核心组件包括：双空间编码器、基于Mamba的时序模块以及注意力机制。HiSTM利用选择性状态空间方法来捕获网络流量中的时空模式。

Result: 通过真实世界数据集评估，HiSTM相比STN基线模型实现了29.4%的MAE改进，同时参数量减少了94%。实验表明HiSTM在不同数据集上泛化能力良好，并且在更长的时间范围内也能提高预测精度。

Conclusion: HiSTM模型为蜂窝流量预测提供了一种高效且高精度的解决方案，它有效地捕捉了复杂的时空模式，并在减少模型参数的同时显著提升了预测性能和泛化能力。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [211] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: 本文提出了MX-AI，一个端到端的基于LLM代理的系统，通过自然语言意图管理和控制6G无线接入网络，并在真实场景中展现出媲美人类专家的性能。


<details>
  <summary>Details</summary>
Motivation: 未来的6G无线接入网络（RAN）将是AI原生的，需要能够自主观察、推理和重新配置的智能代理。目前缺乏一个能实现端到端自主管理和自然语言交互的系统。

Method: 引入MX-AI系统，其方法包括：1) 在基于OpenAirInterface (OAI) 和 FlexRIC 的5G Open RAN测试平台上进行部署；2) 在服务管理和编排 (SMO) 层内部署一个由大型语言模型 (LLM) 驱动的代理图；3) 通过自然语言意图提供6G RAN资源的观测和控制功能。

Result: 在50个真实操作查询上，MX-AI的平均回答质量达到4.1/5.0，决策-行动准确率达到100%，并且在GPT-4.1支持下，端到端延迟仅为8.8秒。

Conclusion: MX-AI的性能与人类专家相当，验证了其在实际环境中的实用性，并为AI原生RAN的开放研究提供了重要支持。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [212] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: 本文提出CoMoE框架，旨在解决MoE模型在资源受限移动边缘设备上部署时面临的内存和动态专家激活挑战。CoMoE通过动态资源感知协同优化专家聚合与卸载策略，显著降低内存占用和推理延迟，使大型MoE模型得以在边缘设备上部署。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型(LLMs)的普及推动了MoE架构的应用，它能有效扩展模型容量并控制计算成本。然而，由于内存占用大和专家激活模式动态，在资源受限的移动边缘计算环境中部署MoE模型面临显著挑战。

Method: 本文提出CoMoE，一个新颖的动态资源感知协同优化框架。该框架基于实时设备资源状态、网络条件和输入特性，联合优化专家聚合粒度（通过分析现有技术如参数合并、知识蒸馏和参数共享分解）和专家卸载策略（包括专家预测、预取、缓存、调度以及多层存储架构）。CoMoE还整合了自适应调度机制，以响应用户移动性和多变的网络条件。

Result: 在真实移动边缘测试平台上的广泛实验表明，CoMoE相较于基线方法：
- 内存使用量减少约70%。
- 推理延迟降低10.5%。
- 保持模型性能稳定性。
对于大型MoE模型（如7.4B参数的Switch-Base-128），CoMoE将内存需求从15.6GB降至4.7GB，使其能够在资源受限的移动边缘设备上成功部署。

Conclusion: CoMoE框架通过其动态资源感知的协同优化方法，成功解决了MoE模型在移动边缘计算环境中的部署难题。它显著降低了内存需求和推理延迟，同时保持了模型性能，从而使大型MoE模型能够在以前无法支持它们的资源受限移动边缘设备上运行。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [213] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 针对MoE LLM在多服务器集群上的高效推理部署问题，本研究提出一种基于整数线性规划(ILP)的拓扑感知专家放置策略，旨在最小化网络传输。


<details>
  <summary>Details</summary>
Motivation: 预训练大型语言模型(LLM)的高效部署对用户响应速度至关重要。MoE LLM因其稀疏激活和负载不均衡特性，在多服务器集群部署中面临如何有效利用集群资源、考虑网络拓扑并最小化网络流量的挑战。

Method: 提出并构建了一个整数线性规划(ILP)模型，用于优化MoE LLM中专家（experts）在多服务器集群上的放置，核心目标是最小化预期的网络传输次数。该ILP问题可使用标准ILP求解器求解。

Result: 实验证明，与现有竞争策略相比，基于ILP的放置策略在小规模（DeepSeekMoE~16B）和大规模（DeepSeek-R1~671B）MoE模型上均能显著降低网络流量。

Conclusion: 本研究成功开发了一种高效的拓扑感知MoE LLM推理阶段部署策略，通过ILP优化专家放置，有效减少了网络传输量，提升了集群利用率。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [214] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: NEFMind框架利用参数高效微调的开源大型语言模型（LLMs），解决了电信领域服务化架构中服务发现和管理因API激增带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现代电信网络中，基于服务的架构导致网络功能（NFs）和API数量呈指数级增长，给服务发现和管理带来了巨大的操作复杂性。

Method: 引入NEFMind框架，该框架利用开源LLM（如Phi-2）的参数高效微调技术。它包含三个核心组件：从网络暴露功能（NEF）API规范生成合成数据集，通过量化低秩适应（Quantized-Low-Rank Adaptation）进行模型优化，并使用GPT-4 Ref Score和BertScore指标进行性能评估。

Result: 针对5G服务化架构API，该方法使通信开销比手动发现方法减少85%。实验验证显示，微调后的Phi-2模型在API调用识别方面达到了98-100%的准确率，且性能与GPT-4等大型模型相当，同时保持了计算效率。

Conclusion: 研究结果验证了领域特定、参数高效的LLM策略在管理下一代电信网络中复杂API生态系统方面的有效性。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [215] [On-Device Multimodal Federated Learning for Efficient Jamming Detection](https://arxiv.org/abs/2508.09369)
*Ioannis Panitsas,Iason Ofeidis,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 为解决无线网络干扰攻击检测中现有方法的效率、扩展性及隐私问题，本文提出一种多模态联邦学习框架，通过融合频谱图和网络KPI实现设备端高效、鲁棒且隐私保护的干扰检测，尤其在数据异构环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 无线网络面临严重的干扰攻击威胁，现有检测方法通常是单模态的，依赖中心化处理且计算资源需求高，这严重限制了其可伸缩性、效率和部署可行性。

Method: 引入一种多模态联邦学习（FL）框架，用于设备端干扰检测和分类。该框架通过轻量级双编码器架构（包含融合模块和多模态投影头），集成频谱图与跨层网络关键性能指标（KPIs）。此设计确保原始数据留在设备上，仅交换模型参数，从而实现隐私保护的训练和推理。框架在一个无线实验测试台上，使用首个过空（over-the-air）同步多模态数据集进行实现和评估。

Result: 该方法在检测准确性上比现有单模态基线高出15%；通信轮次减少60%即可实现收敛；资源占用较低。在设备间数据异构分布的情况下，其优势最为明显，展现出强大的鲁棒性和可靠性。

Conclusion: 本研究提出的多模态联邦学习框架有效提升了无线网络干扰检测的准确性、效率和鲁棒性，尤其在复杂异构数据环境下表现卓越，并有效解决了隐私保护问题，为设备端干扰检测提供了可行的解决方案。

Abstract: Wireless networks face severe vulnerabilities from jamming attacks, which can
significantly disrupt communication. Existing detection approaches are often
unimodal, rely on centralized processing, and demand substantial computational
resources, hindering scalability, efficiency, and deployment feasibility. To
address these challenges, we introduce a multimodal Federated Learning (FL)
framework for on-device jamming detection and classification that integrates
spectrograms with cross-layer network Key Performance Indicators (KPIs) through
a lightweight dual-encoder architecture equipped with a fusion module and a
multimodal projection head. This design enables privacy-preserving training and
inference by ensuring that only model parameters are exchanged, while raw data
remains on the device. The framework is implemented and evaluated on a wireless
experimental testbed using, to the best of our knowledge, the first
over-the-air multimodal dataset with synchronized benign and three distinct
jamming scenarios. Results show that our approach surpasses state-of-the-art
unimodal baselines by up to 15% in detection accuracy, achieves convergence
with 60% fewer communication rounds, and maintains low resource usage. Its
benefits are most evident under heterogeneous data distributions across
devices, where it exhibits strong robustness and reliability.

</details>


### [216] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: 本文研究波动流量下网络性能评估，尤其关注峰值速率对网络资源的影响。通过引入新指标（如Utilization Score）和改进的Shapley值方法，量化单个流的影响。评估并比较了11个指标，发现部分指标高效且易于维护。


<details>
  <summary>Details</summary>
Motivation: 解决在流量模式波动（特别是峰值数据速率）下网络性能评估的挑战，并量化单个流对整体网络状态的影响。

Method: 引入一套新指标（包括Utilization Score）以量化网络负载和测量单个流的影响。通过百分位数和样本分布分析链路和流数据。采用改进的基于Shapley值的方法测量单个流对网络的影响。评审并比较了11个指标在各种网络场景下的实际相关性。

Result: 所提出的指标能有效捕捉特定流引起的网络状态变化。其中，有三个指标提供了广泛有价值的见解，且相对易于维护。

Conclusion: 本文提出的方法为未来研究提供了一个框架，有望扩展和完善评估流对网络性能影响的指标集。这些指标有效地解决了波动流量下网络性能评估的挑战，并深入理解了资源利用率。

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [217] [Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment](https://arxiv.org/abs/2508.09582)
*Wafaa B. M. Fadlelmula,Sanaa Hamid Mohamed,Taisir E. H. El-Gorashi,Jaafar M. H. Elmirghani*

Main category: cs.NI

TL;DR: 本文提出了一种基于无源光网络（PON）的节能回传架构，用于支持可见光通信（VLC）的室内雾计算。通过混合整数线性规划（MILP）优化资源分配，旨在最小化功耗，并展示了相对于现有技术（如Spine-and-Leaf网络和集中式云）显著的能效提升。


<details>
  <summary>Details</summary>
Motivation: 为室内雾计算资源提供VLC连接，并设计一种高能效的回传架构，以最小化处理和网络功耗。

Method: 1. 提出了一种基于PON的回传架构以支持VLC系统。2. 开发了一个混合整数线性规划（MILP）模型来优化计算资源分配，目标是最小化功耗。3. 在不同工作负载和用户分布下评估架构性能。4. 将所提架构与Spine-and-Leaf（S&L）网络设计和集中式云处理进行比较分析。5. 探讨了任务拆分对能效的影响，并提出了包括动态带宽分配、增加波长带宽和改善室内连接在内的架构增强方案。6. 引入了利用邻近建筑资源的跨楼宇架构以支持高需求场景。

Result: 1. 相较于Spine-and-Leaf（S&L）网络设计的现有回传架构，总功耗节省高达82%。2. 相较于集中式云处理，能效提升高达93%。3. 将任务分配到多个处理节点可进一步提高能效。

Conclusion: 所提出的基于PON的VLC室内雾计算回传架构显著降低了功耗并提高了能效。通过优化资源分配、任务拆分以及架构增强（如动态带宽分配和跨楼宇资源利用），该方案为未来的雾计算环境提供了一个高效且节能的解决方案。

Abstract: In this paper, we consider the use of visible light communication (VLC) to
provide connectivity to indoor fog computing resources and propose an
energy-efficient passive optical network (PON)-based backhaul architecture to
support the VLC system. We develop a mixed-integer linear programming (MILP)
model to optimize the allocation of computing resources over the proposed
architecture, aiming to minimize processing and networking power consumption.
We evaluate the performance of the proposed architecture under varying workload
demands and user distributions. Comparative analysis against a backhaul
architecture that is based on the state-of-the-art spine-and-leaf (S&L) network
design demonstrates total power savings of up to 82%. Further comparison with
centralized cloud processing shows improvements in energy efficiency of up to
93%. Additionally, we examine the improvements in energy efficiency obtained by
splitting tasks among multiple processing nodes and propose enhancements to the
architecture including dynamic bandwidth allocation, increased wavelength
bandwidth and improved connectivity within rooms to alleviate networking
bottlenecks. Furthermore, we introduce an inter-building architecture that
leverages resources from neighboring buildings to support high-demand
scenarios.

</details>


### [218] [Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling](https://arxiv.org/abs/2508.09620)
*Michel Rottleuthner,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 研究表明，通过在受限物联网设备中利用动态电压频率调节（DVFS）技术，可显著降低能耗，延长电池寿命。


<details>
  <summary>Details</summary>
Motivation: 受限物联网（IoT）节点的能耗最小化是一大挑战。研究发现，这些设备微控制器与网络吞吐量之间存在显著性能差异，这使得在较低时钟频率下实现最低能量延迟积（EDP）成为可能。

Method: 将动态电压频率调节（DVFS）集成到RIOT IoT操作系统中，并评估其重配置开销。通过在真实物联网系统上，结合CSMA/CA和时隙两种MAC操作模式，以及不同CoAP事务、有效载荷大小和DTLS传输加密，系统地测量并分析了DVFS对能效的提升效果。

Result: DVFS重配置开销低于单次降频MAC操作所节省的能量。实验显示，MAC操作能耗节省了24%至52%，加密的CoAP通信能耗节省高达37%。

Conclusion: DVFS是一种有效提高物联网设备能效的方法，能显著延长电池寿命。研究鼓励将DVFS集成到未来的物联网设备中，以优化任务执行频率。

Abstract: Minimizing energy consumption of low-power wireless nodes is a persistent
challenge from the constrained Internet of Things (IoT). In this paper, we
start from the observation that constrained IoT devices have largely different
hardware (im-)balances than full-scale machines. We find that the performance
gap between MCU and network throughput on constrained devices enables minimal
energy delay product (EDP) for IoT networking at largely reduced clock
frequencies. We analyze the potentials by integrating dynamic voltage and
frequency scaling (DVFS) into the RIOT IoT operating system and show that the
DVFS reconfiguration overhead stays below the energy saved for a single,
downscaled MAC operation. Backed by these findings, we systematically
investigate how DVFS further improves energy-efficiency for common networking
tasks -- in addition to duty-cycling. We measure IoT communication scenarios
between real-world systems and analyze two MAC operating modes -- CSMA/CA and
time slotting -- in combination with different CoAP transactions, payload
sizes, as well as DTLS transport encryption. Our experiments reveal energy
savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP
communication. These results shall encourage research and system design work to
integrate DVFS in future IoT devices for performing tasks at their optimal
frequencies and thereby significantly extending battery lifetimes.

</details>


### [219] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: 本文介绍ANCHOR，一个针对全球IoT漫游平台连接服务的无监督异常检测解决方案，旨在通过识别有连接问题的客户端，实现主动式问题解决。


<details>
  <summary>Details</summary>
Motivation: 全球物联网服务通信路径复杂，导致通信可用性和可靠性难以保证。现有运营商多采取被动式故障响应，影响服务质量。研究旨在实现对IoT连接问题的主动识别与解决。

Method: 开发并部署了名为ANCHOR的无监督异常检测解决方案。该方案基于被动信令流量，利用统计规则、机器学习和深度学习模型，以识别具有连接问题的IoT客户端。论文详细描述了在实际运营平台上的设计和评估过程。

Result: ANCHOR能够有效过滤海量数据，识别出潜在的问题客户端（即连接问题影响多个IoT设备的客户端），从而使工程师能在服务严重受损前主动解决问题。研究报告了在实际运营IoT客户上的评估结果和经验。

Conclusion: ANCHOR方案通过无监督异常检测，为复杂的IoT连接服务提供了主动管理能力，显著提升了服务质量和可靠性，实现了从被动响应向主动解决问题的转变。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>


### [220] [Route Planning and Online Routing for Quantum Key Distribution Networks](https://arxiv.org/abs/2508.09735)
*Jorge López,Charalampos Chatzinakis,Marc Cartigny*

Main category: cs.NI

TL;DR: 针对量子密钥分发（QKD）网络中传统路由算法效率低、资源稀缺导致需求无法满足的问题，本文提出通过二次规划（QP）建模解决路由规划和不可行情况下的公平建议，并证明最宽最短路径（WSP）策略在在线路由中具有不低于1/2的竞争比，有效应对两种路由模式。


<details>
  <summary>Details</summary>
Motivation: QKD网络需要路由，但由于容量限制和信息处理的特殊性，传统最短路径算法在路由规划和在线路由中表现不佳。此外，QKD网络资源稀缺，导致部分需求可能无法被任何路由分配满足，需要公平的自动化建议。

Method: 将路由规划问题和不可行情况下的公平自动化建议建模为二次规划（QP）问题。此外，提出并证明了最宽最短路径路由策略，并分析其在在线路由中的竞争比。

Result: 传统最短（可用）路径路由策略在在线设置中表现不佳。最宽最短路径（WSP）路由策略的竞争比大于等于1/2，能够有效解决QKD网络中的两种路由模式。

Conclusion: 针对QKD网络的特殊路由挑战，二次规划模型和最宽最短路径策略提供了有效的解决方案，特别是后者在在线路由中表现出良好的竞争力，有效应对了网络中的两种路由模式。

Abstract: Quantum Key Distribution (QKD) networks harness the principles of quantum
physics in order to securely transmit cryptographic key material, providing
physical guarantees. These networks require traditional management and
operational components, such as routing information through the network
elements. However, due to the limitations on capacity and the particularities
of information handling in these networks, traditional shortest paths
algorithms for routing perform poorly on both route planning and online
routing, which is counterintuitive. Moreover, due to the scarce resources in
such networks, often the expressed demand cannot be met by any assignment of
routes. To address both the route planning problem and the need for fair
automated suggestions in infeasible cases, we propose to model this problem as
a Quadratic Programming (QP) problem. For the online routing problem, we
showcase that the shortest (available) paths routing strategy performs poorly
in the online setting. Furthermore, we prove that the widest shortest path
routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$,
efficiently addressing both routing modes in QKD networks.

</details>


### [221] [The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges](https://arxiv.org/abs/2508.09756)
*Mauro De Sanctis*

Main category: cs.NI

TL;DR: 本文提出“大规模无线人体感知”新范式，旨在利用异构无线信号提升人体感知能力，并介绍其核心边缘设备、架构及挑战。


<details>
  <summary>Details</summary>
Motivation: 通过利用时间和空间域的信号多样性，结合无设备和基于设备的无线感知方法，提高不同环境下人体感知的准确性和服务可用性。

Method: 引入“大规模无线人体感知”范式，其核心是“大规模无线人体感知边缘设备”，该设备是一个多技术、多方法的射频接收嵌入式系统，具备特征提取功能。文章讨论了实现此范式的架构解决方案和相关挑战。

Result: 本文成功提出并详细阐述了“大规模无线人体感知”这一创新范式，明确了其实现概念、关键使能技术（边缘设备），并为未来研究奠定了理论基础，指出了潜在的架构和挑战。

Conclusion: 该论文为无线人体感知领域描绘了一个利用大规模异构无线信号的新愿景，强调了专用边缘设备的重要性，并为该范式的未来发展提供了概念性指导和挑战分析。

Abstract: This article is a position paper which introduces the paradigm of ``Massive
Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing
based on a plethora of heterogeneous wireless communication signals. More
specifically, we aim to exploit signal diversity in the time, frequency, and
space domains using opportunistically both device-free and device-based
wireless sensing approaches, with the objective of enhancing human sensing
capabilities in terms of accuracy and service availability over different
environments. The enabling element of this concept is the massive wireless
human sensing edge device, that is, an embedded system acting as a
multi-technology and multi-approach RF receiver with feature extraction
functionality, located within the monitoring area or at its borders. In this
framework, architecture solutions and challenges are discussed to lead the
future development of this new paradigm.

</details>


### [222] [An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks](https://arxiv.org/abs/2508.09769)
*Simon Egger,Robin Laidig,Heiko Geppert,Lucas Haug,Jona Herrmann,Frank Dürr,Christian Becker*

Main category: cs.NI

TL;DR: 为解决5G-TSN网络中5G随机延迟与理想化调度模型不匹配问题，本文提出了一种(m,k)-firm提升策略。该策略通过动态优先级提升，在网络不稳定时提供弱硬实时保证，确保关键应用的性能。


<details>
  <summary>Details</summary>
Motivation: 当前5G与TSN的集成面临挑战：5G的概率性延迟特性与用于网络配置的理想化延迟模型存在根本性差异。这导致时间驱动调度中的延迟异常可能危及实时通信的鲁棒性。

Method: 本文提出了(m,k)-firm提升策略，该策略通过在主时间驱动调度之上增加动态优先级驱动机制，当连续k个帧中有帧被延迟时，提升其中m个帧的优先级，从而在网络不稳定时维持基础的弱硬实时保证。

Result: 评估结果表明，弱硬实时保证对于维护网络化控制系统中的控制质量至关重要。同时，当主调度方案能提供更强的服务质量保证时，本策略仅引入很小的额外开销。

Conclusion: 本文提出的(m,k)-firm提升策略是一种鲁棒且轻量级的备用机制，能够在网络条件不稳定时为应用提供有意义的保证。

Abstract: Current standardization efforts are advancing the integration of 5G and
Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical
industrial applications that require real-time communication. However, there
remains a fundamental disconnect between the probabilistic 5G delay
characteristics and the often idealistic delay models used to synthesize 5G-TSN
network configurations. For time-driven schedules in particular, any delay
outlier unforeseen during schedule synthesis can jeopardize the robustness of
their real-time guarantees. To address this challenge, we present the
(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time
guarantees during unstable network conditions that do not match the expected
delay characteristics. It augments the primary time-driven schedule with a
dynamic priority-driven scheme to elevate the priority of m out of k
consecutive frames if they are delayed. Our evaluations demonstrate that weakly
hard real-time guarantees are essential to uphold the quality of control within
a networked control system. At the same time, only a small overhead is imposed
when the primary schedule can provide stronger quality of service guarantees.
Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight
fallback mechanism to serve applications with meaningful guarantees during
unstable network conditions.

</details>


### [223] [A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study](https://arxiv.org/abs/2508.09839)
*Muhammad Asad Ullah,Luca Borgianni,Heikki Kokkinen,Antti Anttonen,Stefano Giordano*

Main category: cs.NI

TL;DR: 本文首次深入测量并分析了星链航空互联网服务在飞行中的性能，包括吞吐量和往返时延（RTT），并探讨了影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对星链机载性能的深入分析，但主要航空公司已开始提供星链服务，亟需评估并改进其对航空用户的性能。

Method: 在波罗的海和太平洋上空进行了飞行中的实测，收集了单用户吞吐量数据。深入分析了5.5小时的往返时延（RTT）测量数据，并提出ISL路由、卫星数据排队和馈线链路拥塞等可能影响RTT的因素。同时，进行了与住宅用户的比较分析。

Result: 单用户下行中位数吞吐量为64 Mbps，上行为24 Mbps。在高空（17,000英尺以上）时上行中位数吞吐量约为33 Mbps，但在飞机下降阶段低海拔时显著降至约20 Mbps。往返时延（RTT）高度依赖于地面站位置和星间链路（ISLs）的使用。

Conclusion: 本文通过飞行实测弥补了星链机载性能分析的空白，揭示了其吞吐量和RTT的关键性能指标，并初步识别出影响这些指标的因素，为优化航空互联网服务提供了重要数据和见解。

Abstract: Starlink delivers Internet services to users across terrestrial, maritime,
and aviation domains. The prior works have studied its performance at fixed
sites and in-motion vehicles, while an in-depth analysis of in-flight
performance remains absent. With major airlines now offering Starlink Internet
onboard, there is a growing need to evaluate and improve its performance for
aviation users. This paper addresses this shortcoming by conducting in-flight
measurements over the Baltic Sea and the Pacific Ocean. Our measurement results
show that a single user device experiences median throughputs of 64 Mbps and 24
Mbps for the downlink and uplink, respectively. The median uplink throughput is
approximately 33 Mbps when the aircraft maintains an altitude above 17,000
feet. However, a significant reduction in uplink performance is observed during
the aircraft descent phase, with the median throughput dropping to around 20
Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the
location of the ground station being pinged and the use of inter-satellite
links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over
the Pacific Ocean and investigate factors influencing RTT, hypothesizing that
ISLs routing, data queuing at satellites, and feeder link congestion contribute
to deviations from theoretical values. For comparative analysis, we evaluate
the Starlink ground terminal and in-flight connectivity performance from the
perspectives of a residential user and an airline passenger, respectively.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [224] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY是一种基于深度学习的5G物理层认证方案，通过隐写术在I/Q传输中嵌入高斯混合模型生成的伪随机签名，实现高精度且隐蔽的设备识别与认证。


<details>
  <summary>Details</summary>
Motivation: 物理层认证（PLA）利用通信介质特性提供无线网络安全高效的认证，避免传统密码方法。随着深度学习发展，PLA因其准确性和可靠性被广泛采用。本文旨在为5G网络引入一种新颖的基于深度学习的PLA解决方案。

Method: VeriPHY通过隐写术在无线I/Q传输中嵌入签名进行设备识别。它利用高斯混合模型（GMM）生成伪随机签名，并精心调整其分布以确保签名的独特性和隐蔽性。这些签名被嵌入到用户传输给5G gNB的I/Q样本中，然后利用深度神经网络识别和认证用户。

Result: VeriPHY实现了高精度，独特签名识别率在93%至100%之间，且误报率低。当签名每20毫秒更新一次时，推理时间为28毫秒。在隐蔽生成模式下，签名与未改变的5G信号几乎无法区分，同时仍保持超过93%的检测准确率。

Conclusion: VeriPHY提供了一种新颖、高效且高度精确的基于深度学习的物理层认证方案，通过隐蔽地嵌入签名，为5G网络实现了独特的设备识别和认证，同时具备出色的隐蔽性。

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


### [225] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 论文提出了一种名为Contextual Integrity Verification (CIV)的安全架构，通过加密签名溯源标签和硬注意力掩码，为LLMs提供确定性的按token非干扰保证，有效抵御提示注入攻击，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）极易受到提示注入和越狱攻击，现有启发式防护措施（规则、过滤器、LLM判断器）常被规避，导致安全漏洞。

Method: 引入Contextual Integrity Verification (CIV)架构。该架构在推理时为每个token附加加密签名的溯源标签，并在Transformer内部通过预softmax硬注意力掩码（可选FFN/残差门控）强制执行源信任格。这确保低信任度token不能影响高信任度表示，实现按token的非干扰保证，且无需模型微调。

Result: 在提示注入攻击基准测试（Elite-Attack + SoK-246）中，CIV在指定威胁模型下实现了0%的攻击成功率，同时保持93.1%的token级相似性，并在良性任务上模型困惑度没有下降。目前存在未优化的数据路径导致的延迟开销。该方法可即插即用地保护Llama-3-8B和Mistral-7B模型。

Conclusion: CIV提供了一种轻量级、无需微调的推理时安全架构，能够确定性地抵御LLMs的提示注入攻击，并在保证性能的同时，有效提升模型安全性。研究团队发布了参考实现、自动化认证工具和攻击语料库以支持复现。

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [226] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: 本文首次全面分析了大型语言模型（LLM）推理中KV-cache的隐私风险，通过三种攻击向量证明其可泄露用户输入，并提出KV-Cloak防御机制，在确保高效性和模型精度的前提下有效阻断攻击。


<details>
  <summary>Details</summary>
Motivation: KV-cache作为LLM推理加速的关键机制，存在显著但未被充分探索的隐私风险，即攻击者能够从KV-cache中重建敏感用户输入。因此，需要对这些漏洞进行深入分析并提出有效的缓解方案。

Method: 研究方法包括：1. 提出并实现了三种攻击向量（直接反演攻击、碰撞攻击、语义注入攻击）以证明KV-cache的隐私泄露问题。2. 设计并提出KV-Cloak，一种基于可逆矩阵混淆方案结合操作符融合的轻量高效防御机制，用于保护KV-cache。

Result: 研究结果表明：1. 所提出的攻击方法成功展示了KV-cache隐私泄露问题的实际可行性和严重性。2. KV-Cloak能够有效抵御所有提出的攻击，将输入重建质量降低至随机噪声水平。3. KV-Cloak在实现高安全性的同时，几乎不影响模型精度，且性能开销极小。

Conclusion: KV-Cloak为可信赖的LLM部署提供了一个实用且有效的解决方案，能够在不牺牲模型性能和精度的前提下，解决KV-cache带来的隐私泄露问题。

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [227] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 本文提出 LoD，一种新颖的无监督框架，将大型视觉语言模型（LVLM）的越狱检测视为异常检测，通过学习安全样本的内部表示实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已付出大量努力，大型视觉语言模型（LVLMs）仍易受越狱攻击，构成严重安全风险。现有基于内部表示的检测方法依赖启发式规则，导致性能不佳。

Method: 提出 LoD 框架，将越狱检测表述为异常检测。该框架包含两个关键组件：多模态安全概念激活向量（MSCAV），用于捕捉跨模态的安全相关表示；以及安全模式自编码器，通过重建误差建模安全输入的 MSCAV 分布并检测异常。模型仅在安全样本上进行无监督训练。

Result: LoD 在三种不同的 LVLM 和五个基准测试中实现了最先进的性能，平均 AUROC 达到 0.9951，最小 AUROC 相比最强基线提高了 38.89%。

Conclusion: LoD 提供了一种准确且统一的无监督越狱攻击检测方法，能够有效识别分布异常，显著提升了 LVLMs 的安全性。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [228] [QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds](https://arxiv.org/abs/2504.19716)
*Navin Sriram Ravie,Keerthi Vasan M,Asokan Thondiyath,Bijo Sebastian*

Main category: cs.RO

TL;DR: 本文提出一种轻量级的分析方法用于机器人抓取规划，尤其针对对偶抓取，通过优化物体表面抓取点而非末端执行器姿态，以解决现有采样方法泛化性差和效率低的问题。该方法在仿真和真实环境中均与现有先进方法GPD进行了比较并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取是机器人与环境交互的长期挑战。随着环境和任务日益复杂，需要更智能的抓取方法。现有方法大多基于纯采样或学习，但在现实世界中泛化性差，且由于采样效率低和概率性，导致规划耗时长、重复性差，严重限制了其在实际任务中的应用。

Method: 本文提出一种轻量级的分析方法，在六自由度空间中几乎不进行采样。该抓取规划算法被公式化为估计物体表面抓取点的优化问题，而非直接估计末端执行器姿态。具体方法包括：1) 引入软区域生长算法，实现有效的平面分割，即使是曲面也能处理。2) 采用基于优化的质量度量来评估抓取点，以确保间接力闭合。

Result: 所提出的抓取框架与现有最先进的抓取规划方法GPD在多个模拟对象上进行了比较。此外，在真实世界环境中，使用图像和点云数据，并利用ROBOTIQ夹具和UR5机械臂执行规划的抓取，也评估了所提出方法相对于GPD的有效性。

Conclusion: 本研究提出了一种新颖、轻量级的分析抓取规划方法，通过优化抓取点而非末端执行器姿态，有效克服了现有采样方法在泛化性、效率和重复性方面的局限。在仿真和真实世界环境中的评估均验证了其相对于现有先进抓取规划方法的有效性。

Abstract: Grasping has been a long-standing challenge in facilitating the final
interface between a robot and the environment. As environments and tasks become
complicated, the need to embed higher intelligence to infer from the
surroundings and act on them has become necessary. Although most methods
utilize techniques to estimate grasp pose by treating the problem via pure
sampling-based approaches in the six-degree-of-freedom space or as a learning
problem, they usually fail in real-life settings owing to poor generalization
across domains. In addition, the time taken to generate the grasp plan and the
lack of repeatability, owing to sampling inefficiency and the probabilistic
nature of existing grasp planning approaches, severely limits their application
in real-world tasks. This paper presents a lightweight analytical approach
towards robotic grasp planning, particularly antipodal grasps, with little to
no sampling in the six-degree-of-freedom space. The proposed grasp planning
algorithm is formulated as an optimization problem towards estimating grasp
points on the object surface instead of directly estimating the end-effector
pose. To this extent, a soft-region-growing algorithm is presented for
effective plane segmentation, even in the case of curved surfaces. An
optimization-based quality metric is then used for the evaluation of grasp
points to ensure indirect force closure. The proposed grasp framework is
compared with the existing state-of-the-art grasp planning approach, Grasp pose
detection (GPD), as a baseline over multiple simulated objects. The
effectiveness of the proposed approach in comparison to GPD is also evaluated
in a real-world setting using image and point-cloud data, with the planned
grasps being executed using a ROBOTIQ gripper and UR5 manipulator.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 该综述系统回顾了生成式AI在医学影像领域的最新进展、应用、挑战及未来趋势，重点探讨了其如何解决数据稀缺性等问题，并提出了评估框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正快速变革医学影像，解决数据稀缺、标准化和多模态整合等长期挑战。本综述旨在全面评估其在临床影像工作流中的作用。

Method: 本综述综合分析了多种生成模型（如GAN、VAE、扩散模型、多模态基础架构），系统审视了它们在影像获取、重建、跨模态合成、诊断支持和治疗规划等阶段的贡献。此外，提出了一个三层评估框架，用于像素级保真度、特征级真实性和任务级临床相关性评估。

Result: 生成式AI在医学影像中展现了强大的能力，包括数据合成、图像增强、模态转换和时空建模。它能有效应对数据稀缺等问题，但在实际部署中面临泛化性、幻觉风险、数据隐私和法规等障碍。与大型基础模型的结合有望实现下一代可扩展、可靠的临床影像系统。

Conclusion: 生成式AI在医学影像领域具有巨大潜力，尤其与大型基础模型的融合将推动未来发展。本综述旨在指导未来研究，促进AI、医学和生物医学工程领域的跨学科合作。

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [230] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: 提出一种混合（Transformer + CNN）模型，有效提升息肉分割的准确性及对内窥镜伪影的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查中息肉分割面临巨大挑战，因息肉大小、形状、边界模糊及内窥镜伪影（如镜面反光、运动模糊、液体遮挡）等因素，导致现有深度学习方法难以实现精确分割。

Method: 引入一种混合（Transformer + CNN）模型，旨在增强对不断变化的息肉特征的鲁棒性。该模型通过边界感知注意力机制处理边界模糊的息肉，并提升在存在内窥镜伪影情况下的特征提取能力。

Result: 该混合架构在分割精度和对伪影的弹性方面均优于现有解决方案，尤其在解决边界模糊和常见内窥镜伪影问题上表现突出。量化评估显示，召回率提高1.76%（0.9555），准确率提高0.07%（0.9849）。

Conclusion: 所提出的混合模型显著提高了息肉分割的准确性和对内窥镜伪影的抵抗能力，为结肠息肉检测提供了更可靠的解决方案。

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [231] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: impuTMAE是一种基于Transformer的多模态模型，能有效处理医学数据中的缺失模态，并在胶质瘤生存预测中达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态医学数据（如组学、医学图像、临床数据）可提升预后模型性能并深化对疾病机制的理解，但其复杂性、不完整性和缺失模态问题给多模态模型的训练带来了巨大挑战。

Method: 本文提出impuTMAE，一种新颖的基于Transformer的端到端方法，采用高效的多模态预训练策略。该模型通过重建被遮蔽的补丁，同时学习模态间和模态内的交互，并填补缺失模态。impuTMAE在异构、不完整数据上进行预训练，并使用TCGA-GBM/LGG和BraTS数据集，整合遗传（DNAm, RNA-seq）、影像（MRI, WSI）和临床数据五种模态，进行胶质瘤生存预测的微调。

Result: 通过在预训练阶段有效处理缺失数据并实现高效资源利用，impuTMAE超越了以往的多模态方法，在胶质瘤患者生存预测中取得了最先进的性能。

Conclusion: impuTMAE为利用不完整多模态医学数据提供了鲁棒的解决方案，通过有效解决预训练期间缺失模态的挑战，显著提升了胶质瘤等疾病的预后建模能力。

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [232] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 提出一种基于模型和预测不确定性的联邦学习方法，旨在实现跨异构CT数据集的通用分割，并提升性能。


<details>
  <summary>Details</summary>
Motivation: 不同CT分割数据集通常来自不同扫描仪、设置，且器官标注有限或不相交，有效利用这些异构数据同时保护患者隐私极具挑战。

Method: 开发了一种新颖的联邦学习方法：利用随机小批量梯度下降的固有噪声估算客户端模型权重不确定性；服务器端通过贝叶斯启发式逆方差聚合方案整合这些不确定性信息；通过传播模型权重不确定性量化预测不确定性，并在推理阶段利用该不确定性提升预测性能。

Result: 实验评估表明，与现有基线相比，该方法有效提高了联邦聚合的质量和不确定性加权推理的性能。

Conclusion: 该方法能够有效处理异构CT数据，实现通用分割，并为临床决策提供必要的置信度，显著提升了联邦学习的聚合和推理表现。

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [233] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: 该研究探讨了零样本自监督学习重建技术在缩短磁共振胰胆管成像（MRCP）屏气时间方面的可行性，并通过浅层训练显著缩短了计算时间，证明了其在临床应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在探究零样本自监督学习重建技术在磁共振胰胆管成像（MRCP）中缩短患者屏气时间的可行性。

Method: 招募11名健康志愿者获取屏气MRCP数据。将零样本重建与呼吸触发MRCP的并行成像以及屏气MRCP的压缩感知重建进行对比评估。为缩短计算时间，采用了一种利用预训练网络减少反向传播深度的浅层训练方法。

Result: 零样本学习重建显著提高了图像质量（特别是信噪比和导管描绘），优于压缩感知重建，并可与成功的呼吸触发采集相媲美。浅层训练在11分钟内实现了与传统零样本训练（271分钟）几乎相同的重建性能。

Conclusion: 零样本学习能够实现高保真MRCP重建，并有效缩短屏气时间。浅层训练为该技术应用于时间受限的临床工作流程提供了实用解决方案。

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [234] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: 提出一个用于计算病理学中解释深度学习模型的人机-VLM交互系统，以定性和定量地测试并区分解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中的临床应用急需可解释性，以识别模型缺陷或发现新的生物学见解。现有解释工具（如GradCAM）无法形成完整的解释。

Method: 研究提出了一个用于计算病理学分类器（包括全玻片图像多实例学习）的人机-VLM（视觉-语言模型）交互系统。该系统包含：1) 一个AI集成玻片查看器，用于通过滑动窗口实验验证解释声明；2) 使用通用视觉-语言模型量化解释的预测能力。

Result: 该系统能够定性地测试解释的声明，并定量地区分相互竞争的解释。

Conclusion: 该研究为数字病理学及其他领域从“可解释AI”迈向“已解释AI”提供了实际可行路径。

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [235] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: 该研究提出了AMRG，一个基于大型视觉-语言模型（VLMs）的端到端框架，用于自动生成乳腺X线报告，通过LoRA策略在DMID数据集上实现了高性能，并建立了该领域的首个可复现基准。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线报告生成是医疗AI中一个关键但未被充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战。

Method: 引入AMRG（Automatic Mammography Report Generation），首个使用大型视觉-语言模型（VLMs）生成叙述性乳腺X线报告的端到端框架。该框架基于MedGemma-4B-it（一个领域专用、指令调整的VLM），并采用低秩适应（LoRA）的参数高效微调（PEFT）策略。在公开数据集DMID上进行训练和评估，并系统探索LoRA超参数配置，在统一调优协议下对多种VLM骨干模型进行比较实验。

Result: AMRG在语言生成和临床指标上均表现出强大性能，ROUGE-L达到0.5691，METEOR 0.6152，CIDEr 0.5818，BI-RADS准确率0.5582。定性分析显示诊断一致性提高，幻觉减少。本工作建立了乳腺X线报告生成的首个可复现基准。

Conclusion: AMRG为放射学报告生成提供了一个可扩展和适应性强的基础，为多模态医疗AI的未来研究铺平了道路。

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [236] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob!是一个利用LLM和语义技术，通过自动化转录、嵌入和RAG，从大量电视档案中智能检索、重组并生成叙事蒙太奇的实验系统。


<details>
  <summary>Details</summary>
Motivation: 探索语义编目和大型语言模型（LLMs）在检索和重构电视档案片段方面的潜力，并促进新的档案互动方式和自动化叙事构建。

Method: 该系统处理1,547个意大利电视视频数据集，通过自动语音识别（ASR）转录音频，将其分割成句子级单元并嵌入向量数据库进行语义查询。用户输入主题提示后，LLM生成相关查询，然后利用检索增强生成（RAG）技术检索并重新组合视听片段，算法选择并构建成模拟讽刺并置和主题连贯性的叙事蒙太奇。

Result: AI Blob!系统通过动态、内容感知的检索方式，成功实现了档案内容的重构语境化，而非依赖静态元数据模式。它能生成新颖的自动化叙事结构和文化分析形式，展示了语义技术在档案利用方面的新途径。

Conclusion: 该项目提供了一个概念框架和公开可用的数据集，为媒体史学和AI驱动的档案研究做出了贡献，证明了语义技术和LLM能有效促进档案内容的创新性利用和叙事构建。

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [237] [Bayesian-Driven Graph Reasoning for Active Radio Map Construction](https://arxiv.org/abs/2508.09142)
*Wenlihan Lu,Shijian Gao,Miaowen Wen,Yuxuan Liang,Chan-Byoung Chae,H. Vincent Poor*

Main category: eess.SP

TL;DR: 为解决低空经济中无人机无线电地图构建受电池限制的问题，本文提出URAM框架，利用贝叶斯神经网络估算不确定性，并通过基于注意力的强化学习进行图基推理，规划高效轨迹，将重建精度提高34%。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济兴起，无线电地图对空中平台无线连接至关重要。然而，自主空中代理在数据收集时，有限的电池容量严重限制了覆盖范围和效率。

Method: 本文提出一种不确定性感知无线电地图（URAM）重建框架，专门为航点导航量身定制图基推理。该方法整合了两个深度学习组件：1) 一个贝叶斯神经网络用于实时估计空间不确定性；2) 一个基于注意力的强化学习策略，对概率路线图进行全局推理，利用不确定性估计来规划信息丰富且节能的轨迹。图基推理实现了智能、非短视的轨迹规划。

Result: 实验结果表明，URAM将重建精度比现有基线提高了34%。

Conclusion: URAM框架通过有效利用不确定性估计和图基推理，显著提高了无线电地图的重建精度，为低空经济中的无线连接提供了更可靠和高效的数据收集方法。

Abstract: With the emergence of the low-altitude economy, radio maps have become
essential for ensuring reliable wireless connectivity to aerial platforms.
Autonomous aerial agents are commonly deployed for data collection using
waypoint-based navigation; however, their limited battery capacity
significantly constrains coverage and efficiency. To address this, we propose
an uncertainty-aware radio map (URAM) reconstruction framework that explicitly
leverages graph-based reasoning tailored for waypoint navigation. Our approach
integrates two key deep learning components: (1) a Bayesian neural network that
estimates spatial uncertainty in real time, and (2) an attention-based
reinforcement learning policy that performs global reasoning over a
probabilistic roadmap, using uncertainty estimates to plan informative and
energy-efficient trajectories. This graph-based reasoning enables intelligent,
non-myopic trajectory planning, guiding agents toward the most informative
regions while satisfying safety constraints. Experimental results show that
URAM improves reconstruction accuracy by up to 34% over existing baselines.

</details>


### [238] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: 本文提出RadioMamba，一种结合Mamba和UNet的混合架构，用于无线电地图(RM)构建，旨在解决现有方法的精度-效率权衡问题。实验证明，RadioMamba在提高精度的同时，显著提升了运行效率并减少了模型参数量。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线电地图(RM)构建方法存在精度和效率的权衡问题，难以同时满足6G服务对实时、精确空间信道信息的需求。

Method: 研究引入RadioMamba，一个混合Mamba-UNet架构。该架构利用Mamba-卷积块，其中Mamba分支以线性复杂度捕获长距离空间依赖（全局特征），而并行卷积分支提取局部特征，从而有效结合全局上下文和局部细节。

Result: 实验结果显示，RadioMamba在精度上优于现有方法（包括扩散模型），运行速度快近20倍，且模型参数量仅为2.9%。

Conclusion: RadioMamba通过同时提升精度和效率，为下一代无线系统中的实时智能优化提供了一种切实可行的方法。

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


### [239] [3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator](https://arxiv.org/abs/2508.09708)
*Thomas Fehrenbach,Luis Omar Ortiz Abrego,Cornelius Hellge,Thomas Schierl,Jörg Ott*

Main category: eess.SP

TL;DR: 本文评估了一种名为Mode 2d（群组调度）的分布式资源分配方案，该方案旨在满足车辆编队通信对高可靠性和低延迟的需求，并通过仿真证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 车辆编队作为V2X通信中极具前景的应用，能有效降低油耗和排放。然而，它对无线通信提出了严苛的挑战，尤其是在高可靠性和低延迟方面。

Method: 本研究评估了群组调度（Mode 2d）方案。该方案基于分布式和调度式的资源分配机制，允许车辆编队在无需网络协助的情况下，从预设资源池中选择资源。研究通过仿真进行了评估。

Result: 仿真结果表明，所评估的群组调度方法能够满足车辆编队对可靠性、低延迟和数据速率的通信要求。

Conclusion: 群组调度（Mode 2d）是一种可行的通信方案，能够有效解决车辆编队应用中的高可靠性和低延迟通信挑战，从而助力智能交通系统的发展。

Abstract: Vehicle-to-everything (V2X) communication is a key technology for enabling
intelligent transportation systems (ITS) that can improve road safety, traffic
efficiency, and environmental sustainability. Among the various V2X
applications, platooning is one of the most promising ones, as it allows a
group of vehicles to travel closely together at high speeds, reducing fuel
consumption and emissions. However, it poses significant challenges for
wireless communication, such as high reliability and low latency. In this
paper, we evaluate the benefits of group scheduling, also referred to as Mode
2d, which is based on a distributed and scheduled resource allocation scheme
that allows the group of cars to select resources from a configured pool
without network assistance. We evaluated the scheme through simulations, and
the results show that this approach can meet the reliability, low latency, and
data rate requirements for platooning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [240] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: 针对融合HPC-Cloud环境，本文设计并实现了HPE Slingshot网络的扩展，以提供安全、容器粒度的多租户RDMA访问，且开销极小。


<details>
  <summary>Details</summary>
Motivation: 融合HPC-Cloud计算需要兼顾云工作负载的隔离性与HPC应用的高性能需求。HPE Slingshot网络虽然性能强大，但其软件栈专为单租户HPC部署设计，不适合多租户融合HPC-Cloud环境的安全使用，因此亟需解决其在多租户环境下的安全访问问题。

Method: 研究者基于Kubernetes，设计并实现了一个针对融合部署的HPE Slingshot堆栈扩展。此扩展旨在提供安全、容器粒度且多租户的Slingshot RDMA网络访问能力。

Result: 所实现的集成方案成功提供了安全、容器粒度的多租户Slingshot RDMA网络访问能力，并且实现了极小的系统开销。

Conclusion: 通过为HPE Slingshot网络堆栈引入基于Kubernetes的扩展，研究成功解决了融合HPC-Cloud环境中高性能网络的多租户安全访问挑战，使得Slingshot能够安全、高效地支持多租户容器化科学工作流。

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [241] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: Fake-Mamba是一种基于双向Mamba和XLSR的实时语音伪造检测方案，在多个基准测试中超越了现有最优模型。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术的进步加剧了安全威胁，因此需要研究实时深度伪造语音检测技术。

Method: 本研究探索了双向Mamba作为自注意力机制在合成语音检测中的替代潜力。提出了Fake-Mamba方案，它结合了XLSR前端和双向Mamba，以捕获局部和全局特征。核心创新是引入了三种高效编码器：TransBiMamba、ConBiMamba和PN-BiMamba，其中PN-BiMamba利用XLSR的语言表示有效捕获合成语音的细微线索。

Result: Fake-Mamba在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中分别实现了0.97%、1.74%和5.85%的EER。相较于现有最优模型XLSR-Conformer和XLSR-Mamba，取得了显著的相对性能提升。该框架在不同语音长度下均能保持实时推理，并展示了强大的泛化能力和实际可行性。

Conclusion: Fake-Mamba方案证明了双向Mamba在合成语音检测中的竞争力，为实时、高效且泛化能力强的语音深度伪造检测提供了一个有前景的解决方案。

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [242] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 本文提出一个名为ProMode的独立模型，可将文本映射到韵律特征（如F0和能量），并在TTS等下游任务中表现出优越的性能和更好的韵律偏好。


<details>
  <summary>Details</summary>
Motivation: 韵律在语音信号中承载着丰富的情感、语义信息和个体特征。有效建模韵律对于文本转语音（TTS）等应用至关重要。

Method: 研究者提出了一个名为ProMode的独立模型。该模型的编码器以部分掩码的声学特征和时间对齐的文本内容作为输入，生成固定长度的潜在韵律嵌入。解码器利用编码的韵律输入和未掩码的文本内容来预测掩码区域的声学信息。该模型在GigaSpeech数据集上进行训练，并与最先进的风格编码器进行了比较。

Result: ProMode模型在F0和能量预测方面显示出在不同粒度级别上的一致改进。将预测的韵律特征整合到TTS系统中后，感知测试显示其韵律偏好高于基线系统。

Conclusion: ProMode模型能够有效预测韵律特征，并通过感知测试证明了其在需要精确韵律建模的任务（如TTS）中的巨大潜力。

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [243] [Deep Generative Models for Discrete Genotype Simulation](https://arxiv.org/abs/2508.09212)
*Sihan Xie,Thierry Tribout,Didier Boichard,Blaise Hanczar,Julien Chiquet,Eric Barrey*

Main category: q-bio.GN

TL;DR: 本研究利用深度生成模型（VAE、Diffusion、GAN）模拟离散基因型数据，有效捕获遗传模式并保留基因型-表型关联，为基因型模拟提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型研究主要集中在基因表达或单倍型数据生成，而离散基因型数据的生成面临挑战且研究较少。本研究旨在克服数据隐私和可访问性限制，探索在无条件和表型条件设置下生成逼真的离散基因型数据。

Method: 开发并评估了变分自编码器（VAEs）、扩散模型（Diffusion Models）和生成对抗网络（GANs）等常用生成模型，并提出了针对离散基因型数据的适应性改进。在牛和人类的大规模染色体数据集上进行广泛实验，并使用深度学习和数量遗传学领域的成熟指标评估模型性能。

Result: 研究结果表明，这些模型能够有效捕获遗传模式，并能很好地保留基因型与表型之间的关联性。

Conclusion: 本研究对各种生成模型进行了全面比较，并为未来基因型模拟研究提供了实用的指导方针。

Abstract: Deep generative models open new avenues for simulating realistic genomic data
while preserving privacy and addressing data accessibility constraints. While
previous studies have primarily focused on generating gene expression or
haplotype data, this study explores generating genotype data in both
unconditioned and phenotype-conditioned settings, which is inherently more
challenging due to the discrete nature of genotype data. In this work, we
developed and evaluated commonly used generative models, including Variational
Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks
(GANs), and proposed adaptation tailored to discrete genotype data. We
conducted extensive experiments on large-scale datasets, including all
chromosomes from cow and multiple chromosomes from human. Model performance was
assessed using a well-established set of metrics drawn from both deep learning
and quantitative genetics literature. Our results show that these models can
effectively capture genetic patterns and preserve genotype-phenotype
association. Our findings provide a comprehensive comparison of these models
and offer practical guidelines for future research in genotype simulation. We
have made our code publicly available at
https://github.com/SihanXXX/DiscreteGenoGen.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [244] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 本文提出一个用户意图驱动的语义通信系统，旨在克服现有系统在深度理解用户真实意图方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统虽能通过提取关键语义提高效率，但未能深入理解和泛化用户的真实意图，限制了意图导向通信的实现。

Method: 该系统集成了多模态大模型作为语义知识库以生成用户意图先验；提出掩码引导注意力模块以突出关键语义区域；并包含信道状态感知模块以确保在不同信道条件下的自适应和鲁棒传输。

Result: 实验证明，该系统能实现对意图的深度理解，并且性能优于DeepJSCC。例如，在瑞利信道和5dB信噪比下，PSNR、SSIM和LPIPS分别提升了8%、6%和19%。

Conclusion: 所提出的用户意图驱动语义通信系统能够有效理解和解释抽象意图，并在多种信道条件下实现稳健传输，显著优于现有基线方法。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [245] [Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery](https://arxiv.org/abs/2508.09183)
*Farzan Moosavi,Bilal Farooq*

Main category: quant-ph

TL;DR: 研究了一种结合参数化量子电路的强化学习框架，用于解决大规模带时间窗的带容量取送货问题，并在解决方案规模和训练复杂度上表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 经典方法难以有效解决大规模NP-hard组合优化问题，尤其在大型Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW)中面临挑战。

Method: 设计了一个结合参数化量子电路（PQC）的强化学习（RL）框架，旨在最小化最后一英里配送的旅行时间。提出了一种新颖的问题特定编码量子电路。通过与Proximal Policy Optimization (PPO)和Quantum Singular Value Transformation (QSVT)进行数值实验比较。

Result: 所提出的方法在解决方案规模和训练复杂度方面优于对比方法，并能有效处理现实世界约束。

Conclusion: 结合PQC的RL框架为解决大规模Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW)提供了高效且可扩展的量子计算解决方案。

Abstract: Quantum computation has demonstrated a promising alternative to solving the
NP-hard combinatorial problems. Specifically, when it comes to optimization,
classical approaches become intractable to account for large-scale solutions.
Specifically, we investigate quantum computing to solve the large-scale
Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this
regard, a Reinforcement Learning (RL) framework augmented with a Parametrized
Quantum Circuit (PQC) is designed to minimize the travel time in a realistic
last-mile on-demand delivery. A novel problem-specific encoding quantum circuit
with an entangling and variational layer is proposed. Moreover, Proximal Policy
Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are
designed for comparison through numerical experiments, highlighting the
superiority of the proposed method in terms of the scale of the solution and
training complexity while incorporating the real-world constraints.

</details>


### [246] [Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks](https://arxiv.org/abs/2508.09209)
*Kun Ming Goh*

Main category: quant-ph

TL;DR: 本研究探索混合量子-经典生成对抗网络（HQCGANs），利用带噪声的量子电路作为潜在空间先验。结果表明，高量子比特HQCGANs能达到与经典GANs媲美的性能，验证了在NISQ时代其生成建模的潜力。


<details>
  <summary>Details</summary>
Motivation: 经典生成对抗网络（GANs）虽能生成高质量数据样本，但其性能受限于传统噪声分布产生的潜在表示质量。本研究旨在探究量子潜在表示是否能突破此限制，提升生成模型能力。

Method: 本研究调查了混合量子-经典生成对抗网络（HQCGANs），其中量子生成器（通过参数化量子电路实现）为经典判别器生成潜在向量。使用Qiskit的AerSimulator，模拟真实噪声环境，在二元MNIST数据集（数字0和1）上，将一个经典GAN与3、5、7量子比特的三种HQCGAN变体进行对比评估。模型训练150个周期，并使用FID（Frechet Inception Distance）和KID（Kernel Inception Distance）指标进行性能评估。

Result: 研究结果显示，经典GAN取得了最佳分数。然而，7量子比特的HQCGAN表现出有竞争力的性能，并在后期训练中缩小了与经典GAN的差距；而3量子比特模型则表现出较早的收敛限制。效率分析表明，尽管存在量子采样开销，训练时间增加仍属适中。

Conclusion: 这些发现验证了在GAN架构中，使用带噪声量子电路作为潜在先验的可行性，突显了其在噪声中间尺度量子（NISQ）时代限制下增强生成建模的潜力。

Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm
for producing high-fidelity data samples, yet their performance is constrained
by the quality of latent representations, typically sampled from classical
noise distributions. This study investigates hybrid quantum-classical GANs
(HQCGANs) in which a quantum generator, implemented via parameterised quantum
circuits, produces latent vectors for a classical discriminator. We evaluate a
classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using
Qiskit's AerSimulator with realistic noise models to emulate near-term quantum
devices. The binary MNIST dataset (digits 0 and 1) is used to align with the
low-dimensional latent spaces imposed by current quantum hardware. Models are
trained for 150 epochs and assessed with Frechet Inception Distance (FID) and
Kernel Inception Distance (KID). Results show that while the classical GAN
achieved the best scores, the 7-qubit HQCGAN produced competitive performance,
narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier
convergence limitations. Efficiency analysis indicates only moderate training
time increases despite quantum sampling overhead. These findings validate the
feasibility of noisy quantum circuits as latent priors in GAN architectures,
highlighting their potential to enhance generative modelling within the
constraints of the noisy intermediate-scale quantum (NISQ) era.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [247] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: 现有大型语言模型（LLM）的安全训练（二元拒绝）在处理模糊或双用途提示时存在局限。本文提出了一种名为“安全补全”的新方法，将安全重心从用户意图转移到助手输出，旨在提高LLM在保持安全性的同时，在双用途场景下的有用性。实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全训练侧重于学习拒绝边界（完全遵守或拒绝），这对于明确的恶意提示有效，但对于意图模糊的提示则显得脆弱。尤其在双用途场景（如生物、网络安全）中，请求在高层次上可安全回答，但若细节或可操作性足够，可能导致恶意提升。因此，需要一种新的安全训练方法，以更好地处理此类复杂情况。

Method: 提出并实施了“安全补全（safe-completions）”的安全训练方法。该方法将安全焦点放在助手的输出安全性上，而非对用户意图进行二元分类。其目标是在安全策略的约束下，最大化模型的有用性。

Result: 将“安全补全”方法整合到GPT-5中，通过生产对比和内部对照实验，发现该训练显著提高了模型的安全性（特别是在双用途提示上），降低了残余安全故障的严重性，并大幅提升了模型的有用性。

Conclusion: “安全补全”方法有效解决了传统LLM安全训练在处理意图模糊和双用途提示时的不足。它通过关注输出安全性，在提高模型安全性的同时，也显著增强了其有用性，为LLM的安全对齐提供了新的方向和潜力。

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


### [248] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: 一项对全球AI开发人员伦理认知的混合方法调查，揭示了不同角色、地域间在AI伦理方面的差异，强调需采取协作且角色敏感的方法应对伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI应用带来的风险日益增长，需要制定伦理指南和法规，因此研究AI开发相关人员的伦理认知、实践和知识变得至关重要。

Method: 采用混合方法调查研究，结合统计和定性分析，对来自43个国家的414名AI开发相关角色（如AI经理、分析师、开发者、质保人员、信息安全专家等）的伦理认知、实践和知识进行考察。

Result: 研究结果显示，不同角色、区域和其他人口因素的参与者在AI伦理原则、政府倡议和风险缓解策略方面的熟悉程度和经验存在差异。

Conclusion: 研究强调在AI开发生命周期中，应采取协作、角色敏感的方法，让多元利益相关者参与伦理决策。文章倡导开发定制化、包容性的解决方案，并提出了未来的研究方向和教育策略以促进伦理意识的AI实践。

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [249] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究分析了ChatGPT在伦理敏感话题上生成论证文本的特点及其对人类读者的说服力，发现其文本结构连贯但风格有限，说服力尤其在伦理问题上受限，并可能加剧读者的伦理担忧。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT在伦理敏感话题上生成的论证文本的修辞和语言特征，并研究这些文本对人类读者的说服影响。

Method: 进行了一项包含62名参与者的用户研究，通过前后互动调查分析AI生成论证对观点改变和用户感知的影响；对ChatGPT生成的文本进行了语言和修辞分析。

Result: ChatGPT生成的论证文本具有一致的宏观结构，但依赖固定表达且文体丰富性有限。尽管文本连贯，但其说服力有限，特别是在涉及伦理问题时。参与者在承认ChatGPT提出的益处后，伦理担忧往往持续存在甚至加剧，且研究结果因主题而异。

Conclusion: 研究为AI在伦理敏感领域的说服力提供了新见解，并为未来的相关研究奠定了基础。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [250] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 分析了ChatGPT、Gemini和Claude生成故事中的性别叙事偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨主流大型语言模型（ChatGPT、Gemini、Claude）生成故事中存在的性别叙事偏见。

Method: 利用普罗普角色分类和弗雷塔格叙事结构设计提示，通过细致阅读分析生成的故事，关注提示依从性、角色性别分布、外貌心理描写、行为、情节发展和角色关系。

Result: 研究揭示了生成故事中偏见的持续存在，尤其是隐性偏见。

Conclusion: 强调了使用解释性方法在多层面评估偏见的重要性。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [251] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD是一种基于深度学习的离轴数字全息显微镜图像处理框架，能实时高效检测血液细胞聚集体，解决了大数据挑战，适用于即时诊断。


<details>
  <summary>Details</summary>
Motivation: 自动化血液学中稀有血细胞聚集体的分析具有挑战性，常规流式细胞仪无法识别聚集体，而定量相位成像流式细胞仪则面临海量数据存储和离线处理的难题。开发一种无需标记、能实时识别聚集体并融入常规血液学诊断的方案，对于提升诊断准确性至关重要。

Method: 本文提出RT-HAD，一个端到端的基于深度学习的图像和数据处理框架，专用于离轴数字全息显微镜(DHM)数据。它结合了物理一致的全息重建和检测，并通过图结构表示每个血细胞以识别聚集体。

Result: RT-HAD能实时处理超过30 GB的图像数据，周转时间少于1.5分钟。在血小板聚集体检测中的错误率为8.9%，符合可接受的血液学生物标志物实验室错误率，并成功解决了即时诊断中的大数据挑战。

Conclusion: RT-HAD为血液细胞聚集体的高效、准确、实时检测提供了一个可行的解决方案，其性能指标满足临床实验室要求，有望推动即时诊断的发展。

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>
