<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.CV](#cs.CV) [Total: 54]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.NI](#cs.NI) [Total: 24]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本研究通过提出“风险隐匿攻击”（RCA）框架和构建FIN-Bench基准，揭示了金融领域大语言模型（LLM）在监管风险方面的严重漏洞，发现主流LLM极易被规避，平均攻击成功率高达93.18%。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型（LLM）的红队研究主要关注有害内容，却忽视了LLM在金融应用中潜在的监管风险，存在研究空白。

Method: 引入“风险隐匿攻击”（RCA），一个通过多轮交互迭代隐藏监管风险以诱导LLM产生违规响应的新颖框架。同时，构建了FIN-Bench，一个专门用于评估金融LLM安全性的领域特定基准。

Result: RCA成功绕过了九个主流大语言模型，平均攻击成功率（ASR）达到93.18%，其中对GPT-4.1和OpenAI o1的成功率分别高达98.28%和97.56%。

Conclusion: 研究结果揭示了当前LLM对齐技术在金融领域存在的关键漏洞，强调了在此领域加强审核机制的紧迫性，并为提升LLM的稳健性和领域感知对齐提供了实用见解。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 研究LLM的自我校准能力，发现通过预生成激活训练的线性探测器能有效预测模型答案的正确性，但对数学推理问题泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）能否在生成答案前预知其回答的正确性。

Method: 提取LLMs在读取问题后、生成任何token前的激活，并训练线性探测器来预测模型即将给出的答案是否正确。实验涵盖了70亿至700亿参数的开源模型，并在多种知识数据集上进行评估，与黑盒基线和口头置信度进行比较。

Result: 1. 线性探测器在分布内和分布外的知识数据集上均能有效预测模型答案的正确性，并优于黑盒基线和口头预测的置信度。
2. 预测能力在中层饱和，表明自我评估能力在模型计算过程中期出现。
3. 对数学推理问题的泛化能力显著下降。
4. 模型回答“我不知道”与探测器分数之间存在强关联，表明探测器捕获了模型的置信度。

Conclusion: 本研究揭示了LLMs在生成答案前具备预测自身正确性的能力，并将其与内部激活联系起来，为理解LLM内部机制提供了重要发现。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [3] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 计算形态学研究在语言文档化中应用有限，原因是缺乏用户中心设计。一项案例研究表明，即使模型指标表现良好，也无法满足实际用户需求。因此，以用户为中心能产生更有效工具和更相关研究方向。


<details>
  <summary>Details</summary>
Motivation: 计算形态学在语言文档化（如形态分割、IGT生成）方面潜力巨大，但研究成果在实际应用中采纳度低。本文指出这种脱节源于NLP领域研究与实践的普遍错位，并认为若不系统整合用户中心设计（UCD），该领域将面临脱离语境和失效的风险。

Method: 本文是一篇倡议用户中心设计的立场论文。通过对最先进的多语言IGT生成模型GlossLM进行案例研究，并对三名文档语言学家进行了小规模用户研究，以演示UCD原则如何重塑研究议程。

Result: 尽管GlossLM在基于指标的性能上表现出色，但在实际文档化情境中未能满足核心可用性需求。这些洞察引发了关于模型约束、标签标准化、分段和个性化等新的研究问题。

Conclusion: 以用户为中心不仅能开发出更有效的工具，还能发掘更丰富、更相关的研究方向，从而弥合计算形态学研究与实际应用之间的鸿沟。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [4] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 本文发现大型语言模型中的“熵神经元”在处理上下文与参数知识冲突时，通过抑制上下文复制行为来影响生成过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对与内部参数知识冲突的上下文信息时表现不一致，缺乏普遍接受的解释。近期研究发现了对模型输出熵有显著影响的“熵神经元”，但其具体作用尚不明确。

Method: 研究通过分析熵神经元在解决上下文与参数信息冲突时的作用，来探究它们在抑制Transformer模型中上下文复制行为方面的初步主张。

Result: 研究表明，熵神经元负责抑制各种大型语言模型中的上下文复制行为。消融这些神经元会导致模型生成过程发生显著变化。

Conclusion: 这些研究结果加深了我们对大型语言模型在处理冲突信息时内部动态的理解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [5] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 为解决大型语言模型在医疗等敏感领域中，现有对齐方法难以反映多样化价值观的问题，本文提出了EthosAgents，一种轻量级、通用化的多元对齐方法，并经验证明其能有效提升模型在不同模式下的多元对齐能力，为高风险领域的多样性尊重提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗等敏感领域的应用日益广泛，确保其输出反映多样化的价值观和视角至关重要。然而，现有对齐方法（包括多元主义范式）在医疗领域难以有效处理个人、文化和情境因素塑造的复杂多元主义。

Method: 提出EthosAgents，这是一种轻量级、通用化的多元对齐方法，旨在模拟不同的视角和价值观，以解决医疗领域中的多元主义挑战。

Result: 经验性地证明EthosAgents在七种不同规模的开放和封闭模型上，全面提升了所有三种模式的多元对齐能力。

Conclusion: 研究发现，与健康相关的多元主义需要适应性强且规范意识高的方法。这为大型语言模型如何在其他高风险领域更好地尊重多样性提供了深刻见解。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [6] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 本文提出Struct-Bench，一个评估框架和基准，用于衡量包含自然语言的结构化数据集生成的差分隐私合成数据的质量，解决了现有评估方法（如FID）的不足。


<details>
  <summary>Details</summary>
Motivation: 差分隐私合成数据生成是利用敏感数据集的重要技术。现有研究多关注非结构化数据（文本、图像），但在企业环境中，包含自然语言字段的结构化数据更常见。现有评估技术难以捕获此类数据集的结构属性和相关性。

Method: 我们提出了Struct-Bench框架和基准。用户需将数据集结构表示为上下文无关文法（CFG）。基准包含5个真实世界和2个合成数据集，均标注CFG。Struct-Bench还包含不同指标的参考实现和排行榜，并演示了如何利用它改进Private Evolution (PE)在结构化数据上的合成数据质量。

Result: 基准数据集对最先进的差分隐私合成数据生成方法构成了巨大挑战。Struct-Bench提供了一个标准化评估平台，并成功展示了如何利用该框架提升PE在结构化数据上的合成数据质量。

Conclusion: Struct-Bench为研究人员提供了一个标准化平台，用于评估和研究针对包含自然语言的结构化数据集的隐私保护合成数据生成方法，填补了现有评估工具的空白，并揭示了当前方法面临的挑战。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [7] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 大语言模型(LLMs)面临幻觉、知识过时和领域知识不足等挑战。检索与结构化(RAS)增强生成通过结合信息检索和结构化知识来解决这些问题。本综述分析了RAS的检索机制、文本结构化技术及其与LLMs的集成方法，并指出挑战和研究机会。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本生成和推理方面能力突出，但在实际应用中仍面临幻觉生成、知识过时和领域专业知识有限等关键挑战。

Method: 本综述通过以下三方面进行分析：1) 考察稀疏、密集和混合等检索机制；2) 探索分类体系构建、层次分类和信息提取等文本结构化技术；3) 研究这些结构化表示如何通过基于提示的方法、推理框架和知识嵌入技术与LLMs集成。

Result: 本综述识别了检索效率、结构质量和知识集成方面的技术挑战，并提出了多模态检索、跨语言结构和交互式系统等研究机会。为研究人员和实践者提供了RAS方法、应用和未来方向的见解。

Conclusion: RAS通过集成动态信息检索与结构化知识表示，为解决LLMs在实际应用中的局限性提供了有效方案。本全面概述为相关领域的研究人员和实践者提供了深入的洞察，涵盖了RAS的方法、应用及未来发展方向。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [8] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: 提出SearchInstruct方法，利用有限人工提问和LLM扩充、检索资源生成高质量指令数据集，显著提升LLM在特定领域的性能。


<details>
  <summary>Details</summary>
Motivation: SFT对LLM训练至关重要，但为特定领域创建合适的训练数据集面临领域约束和数据稀缺的挑战。

Method: SearchInstruct方法首先使用少量领域特定人工问题，通过大型语言模型系统性扩充；然后动态检索领域相关资源，为每个扩增问题生成准确且符合上下文的答案。

Result: 实验证明SearchInstruct提升了SFT数据集的多样性和质量，从而显著改善LLM在专业领域中的性能。此外，该方法还能有效促进模型编辑等任务。

Conclusion: SearchInstruct是一种创新且有效的SFT指令数据集构建方法，不仅能生成高质量数据集，还能支持模型编辑，并提供完整的实现细节和资源以促进复现和采用。

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [9] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 该研究比较了五种多语言Transformer模型（mBERT, XLM, XLM-RoBERTa, RemBERT, mT5）在多语言虚假信息检测上的表现，并引入了包含25种语言的新数据集PolyTruth Disinfo Corpus。结果显示RemBERT总体表现更好，尤其在低资源语言中，而mBERT和XLM在数据稀缺时存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 虚假信息跨语言传播迅速，但多数AI模型仅在英语上进行基准测试，多语言Transformer模型在虚假信息检测中的有效性仍有待探讨。目前缺乏系统性的多语言模型比较和用于评估的通用多语言数据集。

Method: 该研究系统比较了mBERT、XLM、XLM-RoBERTa、RemBERT和mT5五种多语言Transformer模型，执行“假新闻 vs. 真新闻”的机器学习分类任务。为此，作者构建了PolyTruth Disinfo Corpus，一个包含60,486对陈述（虚假主张 vs. 事实纠正）的新语料库，涵盖二十五种以上语言、五种语系及广泛主题，其中一半是经过MindBugs Discovery数据集增强验证的事实核查虚假信息。

Result: 实验揭示了模型性能差异。RemBERT等模型取得了更好的整体准确率，特别是在低资源语言中表现出色；而mBERT和XLM等模型在训练数据稀缺时表现出显著局限性。研究还讨论了这些性能模式及其对实际部署的影响。

Conclusion: 研究结果阐明了AI系统在多语言虚假信息检测方面的潜力和当前局限性。通过提供系统比较和公开数据集，本研究旨在促进该领域的进一步实验和发展。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [10] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文首次全面研究了大型语言模型（LLMs）在离散概率分布上的概率推理能力，发现大型模型表现更强但存在对符号和上下文长度敏感等局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在语言理解和生成方面取得了广泛成功，但在需要概率推理的任务中表现出不明确且不一致的行为。

Method: 研究通过三个精心设计的任务（模式识别、最大似然估计和样本生成）来评估LLM的概率推理能力。给定概率分布的观测数据，通过提示模型回答关于联合分布或条件分布的查询，从而探测其频率分析、边缘化和生成行为等概率技能。

Result: 实证评估表明，小型模型和大型模型之间存在明显的性能差距，后者在推理和样本生成方面表现出更强的能力。然而，研究也揭示了显著局限性，包括对表示概率结果所用符号变化的敏感性，以及随着上下文长度增加，性能下降超过60%。

Conclusion: 研究结果详细阐明了大型语言模型的概率推理能力，并指明了未来改进的关键方向。

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [11] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 论文提出了一个可扩展的模块化框架，用于从科学论文语料库自动生成多项选择题问答（MCQA）基准，并通过检索增强生成（RAG）方法评估小型语言模型，发现推理轨迹检索显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着科学知识的快速增长，评估基准需要不断发展以反映新发现，并确保语言模型在当前、多样化的文献上进行测试。

Method: 开发了一个可扩展的模块化MCQA生成框架，自动化了PDF解析、语义分块、问题生成和模型评估等所有阶段。以放射和癌症生物学领域的22,000篇开放获取文章为案例，生成了超过16,000个MCQ。评估了1.1B-14B参数的小型语言模型，并比较了基线准确率与来自论文语义分块的RAG以及从GPT-4蒸馏出的推理轨迹RAG的效果。

Result: 研究发现，推理轨迹检索持续改进了小型模型在合成和专家标注基准上的性能，使一些小型模型在2023年Astro放射和癌症生物学考试中超越了GPT-4。

Conclusion: 该框架能有效地从科学语料库中生成大量MCQA基准，并且通过推理轨迹检索的RAG方法能显著提高小型语言模型在科学问答任务中的表现。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [12] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文提出了RECAP框架，一个无需再训练的推理时方法，通过结构化情感推理显著提升了医疗领域大型语言模型的共情能力和情感智能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的大型语言模型在提供医学建议时常缺乏情感线索和共情，导致建议生硬，这在患者面临痛苦和脆弱时尤为不利，可能影响患者安全、依从性和信任。

Method: RECAP（Reflect-Extract-Calibrate-Align-Produce）是一个推理时框架，它通过将共情分解为透明的评估理论阶段，并暴露每个维度的李克特量表信号，实现结构化的情感推理，从而生成细致且可审计的响应，无需模型再训练。

Result: 在EmoBench、SECEU和EQ-Bench基准测试中，RECAP使8B模型的情感推理能力提高了22-28%，大型模型提高了10-13%，均优于零样本基线。临床医生评估也证实了其卓越的共情沟通能力。

Conclusion: RECAP证明了模块化、基于理论的提示方法能够系统性地增强医疗AI的情感智能，同时保持了部署所需的问责制。

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [13] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 为解决大模型KV缓存局部信息关注问题，提出Judge Q方法，通过训练软token捕获全局信息，实现更优的KV缓存淘汰，提高解码质量，且训练成本低、易于集成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的KV缓存大小随序列长度线性增长，严重影响内存使用和解码效率。现有KV缓存淘汰方法通常使用预填充阶段的最后窗口计算重要性分数，过于关注局部信息，可能忽视或遗漏关键的全局信息，导致解码质量下降。

Method: 提出Judge Q，一种新型训练方法，引入软token列表。该方法仅以低成本微调模型嵌入层。通过将软token列表拼接在输入序列末尾，训练这些token对原始输入序列的注意力图，使其与实际解码token的注意力图对齐。这样，软token对应的查询能有效捕获全局信息，更好地评估KV缓存中键值对的重要性，从而在KV缓存淘汰时保持解码质量。

Result: 在相同淘汰预算下，Judge Q方法比现有方法性能下降更小。在Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3等模型上，使用LongBench、RULER和Needle-in-a-Haystack等基准进行验证，结果显示LongBench提升约1点，RULER提升超过3点。

Conclusion: Judge Q方法能以最小的训练开销无缝集成到现有开源模型中，有效缓解KV缓存淘汰带来的性能下降，提升大模型在长序列处理中的解码质量和效率。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [14] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出一个名为Automated Error Discovery的框架和SEEED（Soft Clustering Extended Encoder-Based Error Detection）方法，用于检测对话AI中未明确定义的错误，显著优于现有大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的对话代理虽然流畅，但仍会产生难以阻止用户发现的错误。当前LLM在检测指令中未明确指定的错误（如模型更新或用户行为变化引起的错误）时表现不佳。

Method: 引入了Automated Error Discovery框架来发现和定义对话AI中的错误。提出了SEEED作为其实现方法，这是一种基于编码器的方法。SEEED通过增强Soft Nearest Neighbor Loss（对负样本放大距离权重）和引入Label-Based Sample Ranking（选择高对比度样本进行表征学习）来提升性能。

Result: SEEED在多个错误标注的对话数据集上均优于包括GPT-4o和Phi-4在内的基线模型，将未知错误检测的准确率提高了多达8个百分点，并展现出对未知意图检测的强大泛化能力。

Conclusion: SEEED方法有效地解决了对话AI中未知错误的检测问题，弥补了现有LLM在处理未明确指定错误方面的不足，并展示了其在实际应用中的潜力。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [15] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在生物医学和临床问答中展现潜力与局限。其准确性因问题类型和证据清晰度而异，检索增强提示能显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs在生物医学和临床应用中取得显著进展，需要严格评估它们回答细致入微、基于证据问题的能力。

Method: 构建了一个多源基准数据集，包括Cochrane系统评价和临床指南；使用GPT-4o-mini和GPT-5进行评估，分析不同来源和临床领域的性能模式；研究准确性与系统评价引用次数的关联；评估模型对证据质量的推理能力；并采用检索增强提示（提供金标准摘要、语义相关的PubMed摘要、随机摘要）来测试其影响。

Result: 模型在结构化指南上的准确率最高（90%），在叙述性指南和系统评价问题上较低（60-70%）。准确性与系统评价的引用次数呈强正相关（引用次数每翻倍，正确答案几率增加约30%）。模型对证据质量有中等推理能力。检索增强提示显著提高准确性：提供金标准摘要可将先前错误项的准确率提升至0.79，提供前3个PubMed摘要提升至0.23。这些效果在不同模型中一致，表明来源清晰度和精准检索是关键因素。

Conclusion: LLMs在基于证据的临床问答中既有前景也有局限性。检索增强提示是提高事实准确性和与证据一致性的有效策略。按专业和问题类型进行分层评估对于理解知识获取和情境化模型性能至关重要。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [16] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出GAPrune，一种针对领域特定嵌入模型的剪枝框架，通过结合领域重要性和通用语言基础来优化参数剪枝，实现在高稀疏度下性能接近甚至超越密集模型，并显著提升领域专业化能力。


<details>
  <summary>Details</summary>
Motivation: 领域特定嵌入模型（基于LLM）性能优越但参数量大，难以在资源受限环境中部署。现有剪枝方法未能区分通用和领域特定知识，导致剪枝效果不佳。

Method: 提出GAPrune框架，通过Fisher信息量衡量领域重要性，并使用通用领域梯度对齐来评估参数行为。将二者结合形成Domain Alignment Importance (DAI) 分数，以此指导剪枝决策，优先移除对领域任务不重要或与通用目标冲突的参数。

Result: 在FinMTEB和ChemTEB两个领域基准测试中，GAPrune在50%稀疏度的一次性剪枝下，性能与密集模型相差不到2.5%，并优于所有基线。经过100步微调后，FinMTEB性能提升4.51%，ChemTEB提升1.73%，表明剪枝策略不仅保留还增强了领域特定能力。

Conclusion: 研究表明，有原则的剪枝策略可以实现模型压缩并增强领域专业化，为模型开发提供了一种新方法。

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [17] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 提出了一种名为Text2Sign Diffusion的无语标（gloss-free）扩散模型，用于将口语文本直接翻译成手语序列，以弥补沟通鸿沟并促进数字包容性。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成（SLP）方法依赖于语标这一中间符号表示，但语标标注通常不可用且具有语言特异性，限制了SLP的灵活性和泛化能力。

Method: 核心是无语标的潜在扩散模型（latent diffusion model），通过非自回归迭代去噪过程，从噪声潜在手语码和口语文本共同生成手语序列。同时设计了跨模态手语对齐器，学习共享潜在空间以连接手语和口语的视觉与文本内容，支持条件扩散生成。

Result: 在常用的PHOENIX14T和How2Sign数据集上进行了广泛实验，证明了该方法的有效性，并取得了最先进的性能。

Conclusion: 所提出的Text2SignDiff模型成功实现了无语标手语生成，克服了传统语标方法的局限性，提升了手语翻译的准确性和上下文相关性。

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [18] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究发现，人们对AI幽默的大脑反应出人意料地积极且强烈，认知努力减少，情感反应增强，并随时间推移显示出适应性学习和情感奖励的累积效应。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣能够进行类人交流（包括讲笑话），理解人们对AI幽默的认知和情感反应变得日益重要。

Method: 本研究采用脑电图 (EEG) 和行为分析，比较了人们对AI幽默和人类幽默的处理方式。

Result: 行为分析显示AI和人类幽默同样有趣。神经生理数据显示，AI幽默引发更小的N400效应（认知努力减少），更大的LPP（惊喜和情感反应增强），这可能源于对AI幽默能力的低初始预期。时间动态显示，人类幽默表现出习惯化效应，而AI幽默则展现出处理效率和情感奖励的增加（N400减少，LPP增加），表明大脑动态更新了对AI能力的预测模型。此外，对AI的感知信任度越高，情感投入越强。

Conclusion: 大脑对AI幽默的反应出人意料地积极和强烈，挑战了“算法厌恶”，并表明幽默有潜力促进人机社交互动中的真正参与。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [19] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: 提出PREMem方法，将对话AI的复杂推理从响应生成前置到记忆构建，显著提升长时记忆表现，并使小模型达到大模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统在长时记忆方面，将过多推理负担置于响应生成，导致性能严重依赖模型大小。

Method: 引入PREMem（情景记忆预存储推理），将复杂推理过程从推理阶段转移至记忆构建（预存储）阶段。它提取事实、经验、主观等细粒度记忆片段，并建立跨会话的显式关系（如扩展、转换、暗示），以创建丰富的记忆表示。

Result: 实验显示，PREMem在所有模型尺寸上都实现了显著的性能提升。小模型能达到与大型基线模型相当的性能，并在有限的token预算下保持有效性。

Conclusion: 通过将复杂推理前置到记忆构建阶段，PREMem有效解决了对话AI长时记忆的挑战，提升了性能并降低了计算需求，尤其对小型模型效果显著。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [20] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 研究探究LLMs如何处理跨语言（英/中）的量词范围解释歧义，发现它们普遍倾向表层范围，部分模型能区分语种差异，且其性能受模型架构和预训练数据语言背景影响。


<details>
  <summary>Details</summary>
Motivation: 量词句常导致解释歧义，且这种歧义在不同语言间有所差异。本研究旨在考察大型语言模型（LLMs）如何处理英语和汉语中的量词范围解释，并量化其与人类表现的相似度。

Method: 采用跨语言方法（英语和汉语），利用概率评估LLMs的解释可能性。通过人类相似度（HS）分数来量化LLMs在不同语种中模拟人类表现的程度。

Result: 大多数LLMs偏好表层范围解释，与人类倾向一致；但仅部分模型能在逆向范围偏好上区分英语和汉语，反映出与人类相似的模式。HS分数显示LLMs在模拟人类行为上存在变异性，但其与人类对齐的总体潜力显著。模型架构、规模，尤其是模型的预训练数据语言背景，显著影响LLMs对人类量词范围解释的近似程度。

Conclusion: LLMs在量词范围解释方面表现出与人类对齐的潜力，但其性能受模型设计和预训练数据语言背景的显著影响，尤其是在处理跨语言差异时。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [21] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: Term2Note是一种差分隐私(DP)临床笔记合成方法，通过结构化分离内容和形式，能在强隐私保护下生成高保真度和实用性的合成笔记。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型依赖训练数据，但在医疗等高风险领域，真实数据的隐私泄露风险严重限制了其使用。差分隐私(DP)合成数据是解决方案，但在临床笔记这种特定领域和长文本生成中，平衡隐私保护和数据效用极具挑战。

Method: 本文提出了Term2Note方法，通过结构化分离内容和形式，根据DP医学术语生成分节的笔记内容，并对每个部分施加独立的DP约束。此外，采用DP质量最大化器选择高质量输出，以进一步增强合成笔记。

Result: 实验结果表明，Term2Note生成的合成笔记与真实临床笔记在统计特性上高度一致，展现出强大的保真度。在此类合成笔记上训练的多标签分类模型，其性能可与在真实数据上训练的模型媲美，证明了其高实用性。与现有DP文本生成基线相比，Term2Note在更少假设下显著提升了保真度和实用性。

Conclusion: Term2Note展示了作为敏感临床笔记使用的一种可行隐私保护替代方案的潜力。

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [22] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: 为解决现有文化基准的局限性，本文提出CultureSynth框架，利用分层多语言文化分类和RAG方法自动生成问答对，构建了一个大规模、跨语言的文化基准。评估揭示了模型性能分层、3B参数阈值以及架构和地域差异。


<details>
  <summary>Details</summary>
Motivation: LLM在全球环境中对文化能力的需求日益增长，但现有文化基准存在分类碎片化、领域特异性以及严重依赖人工标注的问题。

Method: 引入CultureSynth框架，包含：1) 一个覆盖12个主要和130个次要主题的综合分层多语言文化分类法；2) 一种基于检索增强生成（RAG）的方法，利用事实知识自动合成文化相关问答对。据此构建了CultureSynth-7基准（19,360个条目，4,149个手动验证条目，涵盖7种语言），并评估了14个主流LLM。

Result: LLM性能存在明显分层，ChatGPT-4o-Latest和Qwen2.5-72B-Instruct表现领先。结果表明，达到基本文化能力需要3B参数阈值，模型在知识处理上存在不同的架构偏见，且存在显著的地理差异。

Conclusion: CultureSynth提供了一个可扩展的框架，有助于开发具有文化意识的AI系统，同时减少对人工标注的依赖。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [23] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 在监管和投资中，ESG数据面临从非结构化新闻中准确提取并与抽象国际规范对齐的挑战。本文提出一种半自动化方法，利用轻量级本体论、形式化模式建模和大型语言模型，构建结构化知识图谱，将新闻事件与可持续性原则关联，以识别和解释国际可持续性指南的违规行为。


<details>
  <summary>Details</summary>
Motivation: ESG数据在监管和投资领域日益重要，但从非结构化新闻源中获取准确、可解释且与国际规范（如联合国全球契约、可持续发展目标）对齐的非财务风险数据面临显著挑战。这些规范框架通常语言抽象、缺乏标准化分类，且与商业数据提供商的私有分类系统不兼容。

Method: 提出一种半自动化方法，用于构建新闻报道中ESG事件的结构化知识表示。该方法结合了轻量级本体设计、形式化模式建模和大型语言模型（LLMs），将规范性原则转换为资源描述框架（RDF）中表达的可重用模板。这些模板被用来从新闻内容中提取相关信息，并填充一个将报告事件与特定框架原则关联的结构化知识图谱。

Result: 构建了一个可扩展且透明的框架，用于识别和解释对国际可持续性指南不合规的行为。

Conclusion: 该框架通过有效连接非结构化新闻数据与抽象的规范性原则，为识别和解释国际可持续性指南的违规行为提供了一个可扩展、透明且高效的解决方案，提高了ESG事件分析的准确性和可解释性。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [24] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: Spotlight是一种新型信息提取范式，通过突出文档中最引人入胜的部分，生成简洁且具吸引力的叙述。


<details>
  <summary>Details</summary>
Motivation: 传统摘要侧重全面覆盖，但难以促进读者深入参与源材料。本研究旨在通过选择性地强调引人入胜的内容，提升读者与文档的互动和参与度。

Method: 1. 形式化定义Spotlight并与相关概念区分。2. 利用为该研究策划的新数据集进行详细的基准研究。3. 采用两阶段方法生成：首先在基准数据上微调大型语言模型，然后通过直接偏好优化（DPO）进行对齐。

Result: 综合评估显示，所构建的模型能够精确识别关键元素，同时提高原文的可读性和参与度价值。

Conclusion: Spotlight范式成功地通过精确识别关键信息并提升文档的可读性和参与度，促进了读者对源材料的深入互动。

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [25] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文提出了一个可解释的点击诱饵检测模型，不仅能识别点击诱饵标题，还能归因到具体的语言操纵策略，并为此构建了一个合成数据集和一个两阶段检测框架。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题损害信息可信度和用户信任。现有机器学习模型缺乏可解释性，限制了实际应用。因此，需要开发透明且可信赖的AI系统来对抗操纵性媒体内容。

Method: 1. 创建一个合成数据集，通过系统性地增强真实新闻标题来模拟点击诱饵策略。2. 提出一个两阶段自动化点击诱饵分析框架：第一阶段使用微调BERT分类器与大型语言模型（GPT-4.0和Gemini 2.4 Flash），通过零样本和少样本提示进行点击诱饵检测。第二阶段使用一个专门的BERT分类器来预测标题中具体的点击诱饵策略。

Result: 文章介绍了用于受控实验和模型行为详细分析的合成数据集，并提出了一个结合BERT和大型语言模型的两阶段点击诱饵检测与策略归因框架。该工作旨在推动透明和可信赖AI系统对抗操纵性媒体内容的开发。

Conclusion: 该研究通过提供一个可解释的点击诱饵检测模型、合成数据集和两阶段分析框架，促进了透明和可信赖AI系统在打击操纵性媒体内容方面的进步。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [26] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 为弥补现有评估基准的不足，本文引入EmoBench-Reddit，一个基于Reddit数据（包含图像、文本和情感类别）的多模态情感理解分层基准，旨在评估大型语言模型理解复杂人类情感的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）的评估基准主要侧重于客观的视觉问答或图像描述，不足以评估模型理解复杂、主观人类情感的能力。

Method: 引入EmoBench-Reddit，一个多模态情感理解的分层基准。数据集包含350个Reddit样本，每个包含图像、用户文本和情感类别（悲伤、幽默、讽刺、快乐）。设计了从基本感知到高级认知的分层任务框架，包括多项选择题和开放式问题。注释质量通过AI辅助和人工验证确保。

Result: 成功构建了EmoBench-Reddit，一个具有分层任务和多样情感类别的多模态情感理解基准，能够评估模型对复杂人类情感的感知与认知能力。

Conclusion: EmoBench-Reddit提供了一个新颖且全面的工具，以弥补当前MLLMs评估基准在情感理解方面的不足，有望推动多模态情感智能的研究与发展。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [27] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文介绍了一种名为“Fluid Benchmarking”的动态评估方法，它借鉴心理测量学，能根据语言模型的能力水平自适应地选择评估项目，显著提升了评估效率、有效性、降低了方差并减少了饱和问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型基准测试面临诸多挑战：评估成本高昂、无法准确衡量预期能力、以及标签错误和基准饱和导致评估质量下降。现有解决方案多孤立解决问题，未能提升整体评估质量。

Method: 基于心理测量学原理，Fluid Benchmarking认为评估项的相对价值取决于LM的能力水平。它通过现有评估结果估算项目反应模型，并利用推断出的量动态选择评估项目，类似于教育领域的计算机化自适应测试。

Result: 实验结果显示，Fluid Benchmarking在效率、有效性、方差和饱和度四个维度上均优于随机抽样和基于项目反应理论的基线方法（例如，在MMLU上使用五十倍更少的项目实现了更高的有效性和更小的方差）。分析表明，项目反应理论提高了有效性，动态项目选择降低了方差。

Conclusion: 研究表明，通过采用动态而非静态的评估方法，可以大幅改进语言模型的基准测试效果。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [28] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出个性化论证协商对话生成（PAN-DG）任务，并构建了PACT数据集，通过微调大型语言模型（LLMs）证明了其在生成个性化理性协商对话中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了改进协商对话系统在冲突解决中的能力和适应性，需要整合论证机制和个性化属性。现有系统在这方面有所欠缺，因此旨在提升这些能力。

Method: 1. 提出人格驱动的论证式协商对话生成（PAN-DG）新任务。2. 创建PACT数据集，该数据集包含旅游行业的个性化论证协商对话，由大型语言模型生成，并整合了论证、偏好和购买风格三种个性档案。3. 对数据集进行自动和手动评估以确保高质量。4. 比较预训练和微调的LLMs在PAN-DG任务上的性能。

Result: 1. PACT数据集被评估为包含高质量对话。2. 多维度评估显示，微调后的LLMs能有效生成符合个性的、理性的协商响应。

Conclusion: PACT数据集能够有效增强协商对话系统的个性化和推理能力，为该领域未来的研究奠定了基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [29] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究发现，在LLM谬误分类任务中，添加语境和情感语气元数据（尤其是后者）常会降低模型性能并引入偏差，基础提示效果优于增强提示。


<details>
  <summary>Details</summary>
Motivation: 探讨语境和情感语气元数据如何影响大型语言模型（LLM）在政治辩论谬误分类任务中的推理和表现。

Method: 使用美国总统辩论数据，通过Qwen-3 (8B) 模型分类六种谬误。引入Pragma-Dialectics和Periodic Table of Arguments两种思维链框架，并与基线提示进行对比。评估了纯文本、带语境文本、带语境及音频情感语气元数据文本三种输入设置。

Result: 理论提示可提高可解释性并有时提高准确性。然而，添加语境（特别是情感语气元数据）通常会降低模型性能。情感语气元数据会导致模型偏向分类为“诉诸情感”谬误，从而恶化逻辑推理。整体上，基础提示的表现常优于增强提示。

Conclusion: 为LLM添加语境和情感语气元数据可能因注意力分散和偏差（如情感语气导致错误识别为“诉诸情感”）而降低谬误分类性能，提示简单的输入可能更有效。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [30] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 研究发现表情符号能显著增强大型语言模型生成有害内容的能力，通过绕过安全机制，这可能与预训练语料中的数据污染有关。


<details>
  <summary>Details</summary>
Motivation: 尽管表情符号通常与友好相关，但研究观察到它们可能引发LLMs生成有害内容。本研究旨在探究表情符号是否能增强LLMs的有害内容生成，并解释此现象。

Method: 通过自动化构建含表情符号的提示语来微妙表达有害意图，进行大规模毒性生成探索。在5种主流语言和7个著名LLMs上进行实验（包括越狱任务），并进行模型层面的解释（语义认知、序列生成、分词）。此外，还探究了预训练语料库中的表情符号相关数据污染。

Result: 实验证明，含有表情符号的提示语能轻易诱导LLMs生成有害内容。表情符号可以作为异构语义通道绕过安全机制。预训练语料中与表情符号相关的数据污染与毒性生成行为之间存在潜在关联。

Conclusion: 表情符号能够显著增强LLMs的有害内容生成，其机制是作为一种异构语义通道绕过模型的安全防御，这可能部分归因于预训练语料中的相关数据污染。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [31] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: Text2Mem 提出一种统一的内存操作语言，通过 JSON 模式和标准化流程，为大型语言模型代理提供安全、确定且可移植的内存控制，解决现有框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在长期交互中日益依赖内存，但现有框架功能有限，缺乏高级操作和正式规范，导致行为不可预测。

Method: 引入 Text2Mem，一个统一的内存操作语言。它定义了一套紧凑且富有表现力的操作集，使用基于 JSON 的模式实例表示指令，通过解析器转换为类型化操作对象，并由验证器确保正确性。适配器将操作映射到 SQL 原型后端或真实内存框架，并按需集成模型服务（如嵌入、摘要）。

Result: Text2Mem 的设计确保了跨异构后端的安全性、确定性和可移植性。

Conclusion: Text2Mem 及其相关组件为代理的内存控制建立了第一个标准化基础。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [32] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 研究发现，通过差分隐私（DP）微调的大型语言模型（LLM）在保护隐私的同时，会显著降低生成文本的质量（如长度、语法、多样性）和在下游分类任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管使用差分隐私（DP）微调大型语言模型（LLM）以合成数据来保护用户隐私的方法日益流行，但其对生成文本的语言质量和实用性方面的影响尚未被充分研究。

Method: 本研究在四种隐私级别下，使用三组语料库对五种LLM进行微调。随后，评估其生成文本的长度、语法正确性和词汇多样性。同时，通过书籍类型识别和死因识别等下游分类任务来探究合成文本的实用性。

Result: 结果显示，在更强隐私约束下微调的LLM生成的文本至少短77%，语法正确性至少降低9%，二元词多样性至少降低10%。此外，在下游分类任务中的准确率也明显下降。

Conclusion: LLM在更强隐私保护下生成的文本质量和实用性会下降，这可能对其生成的合成数据的实际应用价值产生负面影响。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [33] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: 本文提出OBR框架，通过误差补偿将量化与稀疏化相结合，克服LLM压缩中两种方法的冲突，实现W4A4KV4量化和50%稀疏性，显著提升速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）压缩技术（如量化和剪枝）单独使用时已接近极限，难以进一步压缩。结合这两种方法虽有潜力，但因对权重分布要求冲突（量化偏好紧凑范围，剪枝受益于高方差）而面临新挑战。

Method: 提出Optimal Brain Restoration (OBR) 框架，一个通用且无需训练的方法。OBR通过量化与剪枝之间的误差补偿来对齐二者，利用二阶Hessian目标最小化性能下降，并通过替代近似和组误差补偿最终得到闭式解。

Result: OBR使得现有LLM能够实现激进的W4A4KV4量化和50%稀疏性，与FP16-dense基线相比，实现了高达4.72倍的加速和6.4倍的内存缩减。

Conclusion: OBR成功解决了量化与剪枝在LLM压缩中的冲突，通过有效的误差补偿机制，大幅提升了模型的运行速度和内存效率，为LLM的进一步压缩提供了新的方向。

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [34] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 本文提出随机对抗训练（RAT）框架，将其成功应用于生物医学信息抽取（BioIE）任务。RAT通过结合随机采样机制和对抗训练，在增强模型泛化性和鲁棒性的同时，显著降低了计算成本，并在BioIE任务中表现出优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管传统的对抗训练能显著提升预训练语言模型在BioIE任务上的性能，但它也带来了相当大的计算开销。研究动机在于寻找一种更高效的生物医学信息抽取解决方案。

Method: 本研究基于PubMedBERT作为基础架构，首先验证了传统对抗训练对BioIE任务的有效性。为解决计算开销问题，提出了随机对抗训练（RAT）框架，该框架将随机采样机制与对抗训练原理策略性地结合，旨在同时实现模型泛化性和鲁棒性的提升以及计算成本的显著降低。

Result: 通过全面的评估，随机对抗训练（RAT）在生物医学信息抽取任务中展现出优于基线模型的卓越性能。结果表明RAT在增强模型泛化性和鲁棒性的同时，显著降低了计算成本。

Conclusion: RAT为生物医学自然语言处理提供了一个变革性的框架，它在模型性能和计算效率之间提供了平衡的解决方案，具有巨大的应用潜力。

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [35] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 本研究提炼了6种核心提示工程技术，并为生命科学领域提供了详细的实践指南，旨在帮助研究人员系统化地应用大语言模型，以提高研究效率和质量。


<details>
  <summary>Details</summary>
Motivation: 开发高质量的LLM提示需要大量认知投入。在生命科学领域，通过部署特定提示工程技术，可显著提高工作流程效率。鉴于现有报告中提示工程技术繁多（58种），需要提炼并提供可操作的指导，以降低其应用门槛和复杂性。

Method: 本研究从一份2025年发布的报告中，精炼并聚焦于6种核心提示工程技术：zero-shot、few-shot、思维生成、集成、自我批判和分解。作者阐述了每种方法在生命科学用例（如文献摘要、数据提取、编辑任务）中的意义，并提供了详细的提示结构建议与常见陷阱（如多轮对话退化、幻觉）的规避方案。此外，还探讨了上下文窗口限制、智能体工具（如Claude Code）以及OpenAI、Google、Anthropic和Perplexity等平台深度研究工具的有效性与局限性。

Result: 研究提供了关于提示如何构建和不应如何构建的详细建议，解决了多轮对话退化、幻觉等常见问题，并区分了推理和非推理模型。分析了上下文窗口限制和智能体工具，评估了主流LLM平台深度研究工具的有效性与当前局限性。结果表明，提示工程能够增强而非取代现有数据处理和文档编辑实践。

Conclusion: 本研究旨在提供核心提示工程原则的可操作指导，以促进从机会主义式提示向高效、低摩擦的系统化实践转变，从而提升研究质量。提示工程应作为现有研究实践的有效补充工具。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [36] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: 本文介绍了Ko-PIQA，一个融入韩国文化背景的韩语物理常识推理数据集。通过多阶段过滤和人工验证，从大量网络文本中筛选出441个高质量问题，其中近20%包含文化特定元素。评估结果显示，现有语言模型在处理文化敏感场景时仍有显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理数据集（如PIQA）主要以英语为中心，缺乏文化多样性，限制了对跨文化常识理解的评估。

Method: 从301万个网络爬取问题出发，采用多阶段过滤方法，利用三个语言模型筛选出11,553个PIQA风格问题。随后通过GPT-4o精炼和人工验证，最终构建了包含441个高质量问答对的Ko-PIQA数据集。数据集中有19.7%的问题包含韩国文化特定元素。最后，使用该数据集评估了七个语言模型。

Result: Ko-PIQA数据集包含大量需要文化感知推理的韩国特定问题。评估结果显示，最佳模型在Ko-PIQA上达到83.22%的准确率，而最弱模型仅为59.86%。模型在处理文化特定场景时表现尤为困难，凸显了现有模型的不足和改进空间。

Conclusion: Ko-PIQA数据集不仅为韩语语言模型提供了一个重要基准，也为未来更具包容性的常识推理研究奠定了基础。数据集和代码将公开发布，以促进该领域的发展。

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [37] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 本文介绍了作者在AraHealthQA-2025共享任务的Track 2（通用阿拉伯语健康问答）中开发的系统，该系统在子任务1（多项选择）和子任务2（开放式问答）中均获得第二名。


<details>
  <summary>Details</summary>
Motivation: 参与AraHealthQA-2025共享任务的Track 2 (MedArabiQ)，旨在开发针对阿拉伯语临床背景下的问答系统。

Method: 两个子任务均使用Gemini 2.5 Flash模型。子任务1（多项选择问答）采用少样本提示、数据集预处理和三种提示配置的集成。子任务2（开放式问答）则使用统一提示，并融入阿拉伯医学专家角色扮演、少样本示例和后处理技术。

Result: 在AraHealthQA-2025共享任务的子任务1（多项选择问答）和子任务2（开放式问答）中，均获得第二名。

Conclusion: 所提出的基于Gemini 2.5 Flash模型的系统，结合特定的提示工程和处理策略，在阿拉伯语临床问答任务中表现出色，取得了显著的竞赛成绩。

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [38] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 该研究系统比较了基于Transformer和非基于Transformer的深度监督关系抽取模型，发现Transformer模型性能显著更优。


<details>
  <summary>Details</summary>
Motivation: 关系抽取在将非结构化文本转化为结构化数据方面扮演重要角色，尤其在大语言模型时代。本研究旨在系统地比较有无Transformer的深度监督学习方法在关系抽取上的性能。

Method: 对比了一系列非Transformer架构（如PA-LSTM, C-GCN, AGGCN）和Transformer架构（如BERT, RoBERTa, R-BERT）。评估指标包括micro F1，并考虑了不同句长和训练数据集比例的场景。实验在TACRED, TACREV和RE-TACRED数据集上进行。

Result: 基于Transformer的模型表现优于非Transformer模型，其micro F1得分达到80-90%，而非Transformer模型的得分为64-67%。

Conclusion: Transformer模型在监督关系抽取任务中展现出卓越的性能。论文还回顾了监督关系分类的研究历程，并探讨了大语言模型在关系抽取中的作用和现状。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [39] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出Layer-Selective LoRA (LayRA) 方法，通过选择性地在多语言模型的初始和末尾层应用LoRA，实现在仅有目标语言数据的情况下，持续添加新语言，有效平衡新旧语言性能，并能通过模型算术增强指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 多语言模型语言集固定，为支持新语言需高成本地从头训练，且常无法获取原始预训练数据。持续预训练会导致灾难性遗忘，而缺乏原始数据使得缓解策略（如经验回放）无法应用。

Method: 本文提出Layer-Selective LoRA (LayRA)，在多语言模型的选定初始层和末尾层应用低秩适配器（LoRA），同时冻结其余层。该方法基于LoRA可减少遗忘，以及多语言模型分层处理（初始层编码、中间层推理、末尾层翻译）的洞察。

Result: LayRA在保留模型原有语言能力和学习新语言之间提供了最佳权衡，在新语言学习方面与现有LoRA方法具有竞争力。此外，通过模型算术，可在不依赖目标语言指令微调数据的情况下，赋予适配模型强大的指令遵循能力。

Conclusion: LayRA是持续向多语言模型添加新语言的有效方法，它能良好平衡新旧语言性能，并能通过模型算术进一步提升模型的指令遵循能力。

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [40] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 首次对“15分钟城市”概念在Twitter、Reddit和新闻媒体等多平台上的公众舆论进行情感分析。研究利用压缩型Transformer模型和Llama-3-8B进行标注和分类，结果表明压缩模型具有竞争力，并揭示了平台特有的性能挑战和权衡。


<details>
  <summary>Details</summary>
Motivation: 旨在首次在多平台（Twitter、Reddit、新闻媒体）上对“15分钟城市”概念的公众舆论进行情感分析，以理解其社会接受度，并评估压缩型Transformer模型在此类复杂异构文本域中进行情感分类的潜力，挑战传统认为大型模型才必需的假设。

Method: 开发了一个能够处理长短文本、支持一致标注和可复现评估的管道。使用Llama-3-8B进行文本标注，并基准测试了DistilRoBERTa、DistilBERT、MiniLM、ELECTRA和TinyBERT五种压缩型Transformer模型。采用分层5折交叉验证，通过F1分数、AUC和训练时间进行性能评估。

Result: DistilRoBERTa取得了最高的F1分数（0.8292），TinyBERT效率最佳，MiniLM的跨平台一致性最好。新闻数据因类别不平衡导致性能虚高，Reddit数据存在摘要损失，Twitter数据挑战适中。研究发现，压缩型模型表现出很强的竞争力，足以挑战大型模型在情感分类中必要的假设。

Conclusion: 压缩型Transformer模型在对“15分钟城市”概念的多平台情感分析中表现出色且具竞争力，足以支持城市规划领域的现实世界情感分类应用。研究识别了不同平台特有的性能权衡，并为未来可扩展情感分类提供了方向。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [41] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: 本文介绍CognitiveSky，一个开源且可扩展的框架，用于在去中心化社交媒体Bluesky上进行情感、情绪和叙事分析，通过Transformer模型处理数据并可视化。


<details>
  <summary>Details</summary>
Motivation: 去中心化社交媒体平台的出现为公共话语的实时分析带来了新的机遇和挑战。

Method: CognitiveSky通过Bluesky API获取数据，利用Transformer模型对大规模用户生成内容进行情感、情绪和叙事标注，生成结构化输出，并通过动态仪表盘可视化演变模式。该框架完全基于免费基础设施构建。

Result: 成功构建了CognitiveSky框架，实现了低运营成本和高可访问性，能够对Bluesky上的公共话语进行实时情感、情绪和叙事分析及可视化。其模块化设计使其可应用于心理健康监控、虚假信息检测、危机响应等多个领域。

Conclusion: CognitiveSky将大型语言模型与去中心化网络相结合，为计算社会科学提供了一个透明、可扩展的工具，以适应不断变化的数字生态系统。

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [42] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: CEMTM是一种上下文增强的多模态主题模型，利用LVLM和注意力机制从图文文档中提取连贯且可解释的主题结构，能高效处理多图像，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理包含文本和图像的文档时，尤其是在处理多图像文档或保持主题可解释性方面存在不足。研究旨在开发一个能够从短文档和长文档中推断出连贯且可解释主题结构，并能高效处理多模态信息的模型。

Method: 引入CEMTM模型，它基于微调的大型视觉语言模型（LVLMs）获取上下文嵌入，并采用分布式注意力机制加权token级别的贡献。模型通过重建目标将基于主题的表示与文档嵌入对齐，以确保跨模态的语义一致性。CEMTM能无需重复编码即可处理文档中的多张图像，并通过显式的词-主题和文档-主题分布保持可解释性。

Result: 在六个多模态基准测试中，CEMTM持续优于单模态和多模态基线模型，平均LLM得分达到2.61。进一步分析表明，它在下游少样本检索任务中表现有效，并能在科学文章等复杂领域捕获视觉接地的语义。

Conclusion: CEMTM是一个高效且可解释的多模态主题模型，能够从包含文本和图像的文档中提取高质量的主题结构。其在多个基准测试和下游任务中的卓越表现，证明了其在多模态信息处理和语义理解方面的强大能力。

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [43] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出逆向训练和迭代文档生成两种新方法，以解决大型语言模型在共指消解中存在的幻觉和性能问题。


<details>
  <summary>Details</summary>
Motivation: 共指消解对许多NLP任务至关重要，但现有基于LLM的方法（如问答模板和文档模板）存在幻觉和性能不佳的问题。

Method: 调查了现有基于LLM的共指消解方法（问答模板和文档模板）的局限性，并提出了两种新方法：逆向训练与联合推理（Reversed Training with Joint Inference）和迭代文档生成（Iterative Document Generation）。最后，整合这些方法和技术。

Result: 实验表明，逆向训练能改进问答模板方法；迭代文档生成能消除生成源文本中的幻觉并提升共指消解效果。

Conclusion: 整合这些方法和技术为基于LLM的共指消解提供了一个有效且鲁棒的解决方案。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [44] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文为CLEF 2025 CheckThat! Lab任务3（数值和时间声明验证）提供了一个系统。研究了零样本提示和LoRA微调两种方法，并探索了多种证据选择策略。模型在验证集上表现良好，但在测试集上泛化能力下降，强调了证据粒度和模型适应性的重要性。


<details>
  <summary>Details</summary>
Motivation: 参与CLEF 2025 CheckThat! Lab任务3，旨在验证使用检索证据的数值和时间声明。

Method: 采用了两种互补方法：使用指令微调大型语言模型（LLMs）进行零样本提示，以及使用参数高效LoRA进行监督微调。为提高证据质量，探索了多种证据选择策略，包括全文档输入和使用BM25及MiniLM的top-k句子过滤。

Result: 最佳模型（用LoRA微调的LLaMA）在英语验证集上取得了良好性能。然而，在测试集上性能显著下降，表明存在泛化挑战。

Conclusion: 研究结果强调了证据粒度和模型适应性对于稳健的数值事实验证的重要性。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [45] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 本文提交了CLEF-2025 CheckThat! Task 2的索赔规范化系统，通过微调SLM和LLM提示，在20种语言中的15种取得了前三名，尤其在零样本场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 索赔规范化（将非正式社交媒体帖子转化为简洁声明）是自动化事实核查流程中的关键步骤。

Method: 对于有监督语言，采用微调的小型语言模型（SLM）；对于零样本语言，则利用大型语言模型（LLM）的提示（prompting）。

Result: 该系统在20种语言中的15种取得了前三名，其中8种语言排名第二（包括7种零样本语言中的5种）。在葡萄牙语上，获得了0.5290的平均METEOR分数，排名第三。

Conclusion: 该方法（特别是基于LLM的零样本策略）在多语言索赔规范化任务中表现出显著的有效性，尤其在零样本语言中取得了优异成绩。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [46] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: DeDisCo是乔治城大学在DISRPT 2025篇章关系分类共享任务中的参赛系统。它结合了基于mt5的编码器和基于Qwen的解码器方法，并通过数据增强和语言特征处理低资源语言，获得了71.28的宏观准确率。


<details>
  <summary>Details</summary>
Motivation: 参与DISRPT 2025关于篇章关系分类的共享任务，并提交乔治城大学的系统DeDisCo。

Method: 采用基于mt5的编码器和基于Qwen模型的解码器两种方法。为低资源语言使用自动翻译的增强数据集进行训练，并结合了额外的语言特征。

Result: 系统在篇章关系分类任务中取得了71.28的宏观准确率，并提供了结果的解释和错误分析。

Conclusion: DeDisCo系统在DISRPT 2025篇章关系分类任务中展示了有效性，达到71.28的宏观准确率，并提供了深入的错误分析。

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


### [47] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 本文提出两种方法（注意力权重和集成梯度）来测量上下文对目标词的影响，并通过结合原始句和替换句的语义相似性，改进词汇替换中的候选词排序。


<details>
  <summary>Details</summary>
Motivation: 现有词汇替换方法在有效建模候选词替换对目标词及其上下文的双向影响，以及准确表征语义变化方面存在挑战，常只关注目标位置变化或依赖多指标参数调优。

Method: 研究了两种方法：一种基于注意力权重，另一种利用集成梯度，旨在衡量上下文词元对目标词元的影响，并通过结合原始句和替换句之间的语义相似性来对候选词进行排序。

Result: 在LS07和SWORDS数据集上的实验表明，这两种方法均能提高排序性能。

Conclusion: 所提出的基于注意力权重和集成梯度的方法，通过有效建模上下文影响和语义相似性，显著提升了词汇替换任务中候选词的排序表现。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [48] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 研究发现，现有的大型视觉语言模型（LVLMs）在理解人类自发对话中协同创建的指代表达方面存在挑战，即使在重复交互后也未能显示出持续的性能提升。


<details>
  <summary>Details</summary>
Motivation: 具身智能体需要理解自发对话中协同创建的指代表达，以便在现实世界中执行任务，这要求它们整合并理解语言、视觉和对话交互。

Method: 研究人员选取了七种最先进的大型视觉语言模型（LVLMs），让它们作为旁听者，分析人类参与者在协同物体匹配任务中进行的自发对话语料库，以评估其能力。

Result: 研究发现，该任务对当前的LVLMs来说极具挑战性。所有测试模型都未能显示出一致的性能提升，即使它们旁听了来自相同对话参与者在多轮中重复相同任务的更多对话。

Conclusion: 当前的大型视觉语言模型在理解人类自发对话中的协同指代表达方面表现不足，且未能从重复的对话交互中获得持续的学习改进。该领域仍需进一步研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [49] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 研究构建了秘鲁医学考试数据集PeruMedQA，并评估了多款医疗LLM在西班牙语医学问题上的表现。结果显示medgemma-27b表现最佳，而微调后的medgemma-4b显著提升了小模型在本地化医学知识上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在医学考试中表现出色，但其在西班牙语及拉丁美洲特定医学问题上的性能尚未被充分探索。鉴于基于LLM的医疗应用在拉丁美洲日益普及，评估其在此类特定背景下的可迁移性至关重要。

Method: 1. **数据集构建**: 整理了PeruMedQA，一个包含8,380道来自2018-2025年秘鲁医师专业培训考试的多项选择题数据集，涵盖12个医学领域。
2. **模型选择**: 选取了包括medgemma-4b-it和medgemma-27b-text-it在内的八个医疗大语言模型。
3. **评估策略**: 采用零样本（zero-shot）任务特定提示来回答问题。
4. **模型微调**: 使用参数高效微调（PEFT）和低秩适应（LoRA）技术对medgemma-4b-it进行微调，训练集包含除2025年测试集之外的所有问题。

Result: 1. **最佳模型表现**: medgemma-27b-text-it在所有模型中表现最优，在多项考试中正确率超过90%。
2. **小模型局限**: 参数小于100亿的LLM正确率普遍低于60%，部分考试甚至低于50%。
3. **微调效果**: 经过微调的medgemma-4b-it表现显著提升，击败了所有参数小于100亿的LLM，并在多项考试中能与参数700亿的LLM相媲美。

Conclusion: 对于需要西班牙语国家知识库或与秘鲁流行病学概况相似的医疗AI应用和研究，建议使用medgemma-27b-text-it或微调后的medgemma-4b-it。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [50] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
*Zhihan Cao,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本研究通过与三种其他语义关系对比，发现反义词的共现模式具有独特性，表现为高强度、偏好线性顺序和短距离共现。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现反义词频繁共现，但缺乏与其他语义关系进行比较，因此不清楚这种共现模式是否是反义词所独有的。

Method: 本研究使用稳健的共现度量，跨词性将反义词与其他三种语义关系进行比较。

Result: 研究发现反义词在三个方面具有独特性：反义词对共现强度高、存在偏好的线性顺序，并且在较短的文本跨度内共现。

Conclusion: 反义词与其他语义关系相比，表现出独特的共现模式。

Abstract: Antonymy has long received particular attention in lexical semantics.
Previous studies have shown that antonym pairs frequently co-occur in text,
across genres and parts of speech, more often than would be expected by chance.
However, whether this co-occurrence pattern is distinctive of antonymy remains
unclear, due to a lack of comparison with other semantic relations. This work
fills the gap by comparing antonymy with three other relations across parts of
speech using robust co-occurrence metrics. We find that antonymy is distinctive
in three respects: antonym pairs co-occur with high strength, in a preferred
linear order, and within short spans. All results are available online.

</details>


### [51] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: 本文提出了HARP（基于推理子空间投影的幻觉检测）框架，通过将LLM的隐藏状态空间分解为语义和推理子空间，并利用Unembedding层和SVD获取推理子空间投影，实现了更鲁棒、更准确的幻觉检测，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）中的幻觉是其在关键决策中可靠应用的主要障碍。现有幻觉检测方法虽然提高了准确性，但在解耦语义和推理信息以及保持鲁棒性方面仍面临挑战。

Method: 本文提出了HARP框架，核心思想是将LLM的隐藏状态空间分解为语义子空间和推理子空间。研究证明Unembedding层可以解耦这些子空间，并通过对其参数应用奇异值分解（SVD）来获得语义和推理子空间的基向量。最终，HARP将隐藏状态投影到推理子空间的基向量上，并将这些投影作为幻觉检测的输入特征。

Result: HARP将特征维度降低到原始维度的约5%，过滤了大部分噪声，并增强了鲁棒性。实验表明，HARP在多个数据集上实现了最先进的幻觉检测性能，特别是在TriviaQA上取得了92.8%的AUROC，比之前最好的方法提高了7.5%。

Conclusion: HARP通过创新性地利用LLM内部推理子空间的投影，有效解决了幻觉检测中语义与推理信息解耦及鲁棒性问题，显著提升了幻觉检测的性能，达到了当前最优水平。

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [52] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 针对RAG文档分块缺乏有效评估工具的问题，本文提出了评估基准HiCBench和基于微调LLM的分块框架HiChunk，并证明了其能有效评估分块方法并提升RAG系统的整体性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统中文档分块是关键组成部分，但当前缺乏有效的评估工具。现有RAG评估基准由于证据稀疏性，不足以评估文档分块质量。

Method: 1. 提出了HiCBench基准，包含人工标注的多级文档分块点、合成的证据密集型问答对及其证据来源。
2. 引入了HiChunk框架，这是一个基于微调大型语言模型（LLMs）的多级文档结构化框架。
3. 将HiChunk与Auto-Merge检索算法结合，以提高检索质量。

Result: 1. HiCBench能够有效评估不同分块方法对整个RAG流程的影响。
2. HiChunk在合理的时耗内实现了更好的分块质量。
3. HiChunk显著提升了RAG系统的整体性能。

Conclusion: HiCBench提供了一个有效的文档分块评估工具，而HiChunk框架则通过提升分块质量，进而增强了RAG系统的整体性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [53] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
*Yue Ding,Xiaofang Zhu,Tianze Xia,Junfei Wu,Xinlong Chen,Qiang Liu,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出D$^2$HScore，一种无需训练、无需标签的幻觉检测框架，通过测量LLM推理过程中表示的层内分散度和层间漂移来解决大模型幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）取得了显著成功，但其生成的非事实性内容（即“幻觉”）阻碍了实际应用，尤其是在金融、安全和医疗等高风险领域。确保LLM输出的可靠性是一个关键挑战。

Method: 该研究从模型架构和生成动态角度重新审视幻觉检测，将幻觉信号分解为语义的层内广度和层间深度。基于此，提出了D$^2$HScore，一个结合了“层内分散度”（量化每层token表示的语义多样性）和“层间漂移”（跟踪核心概念跨层演化）的框架。通过注意力信号引导token选择，确保漂移反映有意义的语义演变。

Result: D$^2}$HScore在五种开源LLM和五个广泛使用的基准测试中，表现持续优于现有无需训练的基线方法。

Conclusion: D$^2$HScore通过捕获推理过程中表示的水平和垂直动态，提供了一种可解释且轻量级的幻觉检测代理方法。

Abstract: Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [54] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
*Sampoorna Poria,Xiaolei Huang*

Main category: cs.CL

TL;DR: 本调查全面审视了2020年以来针对南亚低资源语言的NLP模型，特别是基于Transformer的模型，揭示了数据、模型和任务方面存在的挑战和差距，旨在提高社区对此问题的关注。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语NLP任务中取得了革命性进展，但低资源语言（尤其是南亚语言）的模型及其评估却被忽视。南亚有超过650种语言，但许多缺乏计算资源或未被现有模型覆盖。因此，需要评估当前阶段和挑战，以指导NLP社区促进南亚语言的模型发展。

Method: 本调查综合审查了自2020年以来针对南亚语言的NLP模型（重点是基于Transformer的模型，如BERT、T5和GPT）的现有努力和挑战。研究从数据、模型和任务三个关键方面（如可用数据源、微调策略和领域应用）展示了进展和差距。

Result: 研究结果突出了重大问题，包括关键领域（如健康）数据的缺失、语码混合现象以及缺乏标准化的评估基准。

Conclusion: 本调查旨在提高NLP社区的意识，以促进更有针对性的数据整理，统一针对南亚文化和语言细微差别的基准，并鼓励南亚语言的公平代表性。

Abstract: Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [55] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
*Chu-Hsuan Lee,Chen-Chi Chang,Hung-Shin Lee,Yun-Hsiang Hsu,Ching-Yuan Chen*

Main category: cs.CL

TL;DR: 本研究分析了AI驱动的客家语聊天机器人TALKA中的用户行为，发现生成式AI能有效支持濒危语言学习，促进认知发展、语用协商和文化认同。


<details>
  <summary>Details</summary>
Motivation: 鉴于许多濒危语言面临消失风险，本研究旨在探索技术（特别是生成式AI）如何结合文化教学策略来支持语言保护工作。

Method: 采用基于布鲁姆认知分类法和对话行为分类法的双层分析框架，对TALKA中7,077条用户话语进行分析。每条话语根据六个认知水平和十一种对话行为类型（如信息询问、翻译请求、文化查询、创造性语言使用）进行标注，并进一步探讨了对话行为与特定认知意图的关联。

Result: 研究表明，生成式AI聊天机器人能以有意义的方式支持语言学习，尤其当其设计考虑到用户的思维和沟通方式时。它们有助于学习者更自信地表达并连接其文化身份。TALKA案例提供了AI介导对话如何促进低资源语言学习者认知发展、语用协商和社群归属感的实证见解。

Conclusion: 本研究聚焦于AI辅助的语言学习，为技术如何支持语言保护和教育实践提供了新的洞察。

Abstract: With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

</details>


### [56] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
*Md. Mithun Hossain,Sanjara,Md. Shakil Hossain,Sudipto Chaki*

Main category: cs.CL

TL;DR: 本文提出SpanEIT框架，通过整合动态跨度交互和图感知记忆机制，解决实体级情感分类中实体与情感的复杂交互、跨句依赖和一致性问题，并在多个数据集上超越现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 实体级情感分类面临建模实体与情感表达式的复杂交互、捕获跨句依赖、确保多实体提及的情感预测一致性、以及处理否定和歧义等语言现象的挑战，尤其在真实嘈杂文本中更为困难。

Method: 提出SpanEIT框架，该框架整合了动态跨度交互和图感知记忆机制。它为实体和候选情感短语构建基于跨度的表示，使用双向注意力进行细粒度交互，利用图注意力网络捕获句法和共现关系，并通过一个共指感知记忆模块确保跨文档的实体级一致性。

Result: 在FSAD、BARU和IMDB数据集上的实验表明，SpanEIT在准确率和F1分数上均优于最先进的Transformer和混合基线模型。消融和可解释性分析也验证了该方法的有效性。

Conclusion: SpanEIT框架有效解决了实体级情感分类中的复杂问题，其在社交媒体监控和客户反馈分析等应用中的细粒度情感分析方面具有显著潜力。

Abstract: Entity-level sentiment classification involves identifying the sentiment
polarity linked to specific entities within text. This task poses several
challenges: effectively modeling the subtle and complex interactions between
entities and their surrounding sentiment expressions; capturing dependencies
that may span across sentences; and ensuring consistent sentiment predictions
for multiple mentions of the same entity through coreference resolution.
Additionally, linguistic phenomena such as negation, ambiguity, and overlapping
opinions further complicate the analysis. These complexities make entity-level
sentiment classification a difficult problem, especially in real-world, noisy
textual data. To address these issues, we propose SpanEIT, a novel framework
integrating dynamic span interaction and graph-aware memory mechanisms for
enhanced entity-sentiment relational modeling. SpanEIT builds span-based
representations for entities and candidate sentiment phrases, employs
bidirectional attention for fine-grained interactions, and uses a graph
attention network to capture syntactic and co-occurrence relations. A
coreference-aware memory module ensures entity-level consistency across
documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT
outperforms state-of-the-art transformer and hybrid baselines in accuracy and
F1 scores. Ablation and interpretability analyses validate the effectiveness of
our approach, underscoring its potential for fine-grained sentiment analysis in
applications like social media monitoring and customer feedback analysis.

</details>


### [57] [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
*Xiaoying Song,Anirban Saha Anik,Eduardo Blanco,Vanessa Frias-Martinez,Lingzi Hong*

Main category: cs.CL

TL;DR: 为解决危机沟通中自动回复的风格一致性问题，本文提出了一种新的评估指标和融合生成方法，有效提升了回复质量并显著降低了风格差异。


<details>
  <summary>Details</summary>
Motivation: 危机沟通急需有效的自动回复，但语言模型生成的回复缺乏风格一致性，这会影响受影响人群对响应者的信任。现有研究在此方面存在空白。

Method: 提出了一种评估风格一致性的新颖指标，并引入了一种基于该指标的融合生成方法。该方法采用两阶段过程：首先评估候选回复的风格，然后通过融合过程在实例层面进行优化和集成。

Result: 在多个数据集上的实验结果表明，该方法在回复质量和风格统一性方面均持续优于基线方法。

Conclusion: 该方法能够生成高质量的回复，并显著减少实例间的风格差异，从而提升危机沟通的信任度。

Abstract: In response to the urgent need for effective communication with
crisis-affected populations, automated responses driven by language models have
been proposed to assist in crisis communications. A critical yet often
overlooked factor is the consistency of response style, which could affect the
trust of affected individuals in responders. Despite its importance, few
studies have explored methods for maintaining stylistic consistency across
generated responses. To address this gap, we propose a novel metric for
evaluating style consistency and introduce a fusion-based generation approach
grounded in this metric. Our method employs a two-stage process: it first
assesses the style of candidate responses and then optimizes and integrates
them at the instance level through a fusion process. This enables the
generation of high-quality responses while significantly reducing stylistic
variation between instances. Experimental results across multiple datasets
demonstrate that our approach consistently outperforms baselines in both
response quality and stylistic uniformity.

</details>


### [58] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了一种LLM驱动的幻觉检测系统HalluDetect，并评估了五种聊天机器人架构，发现AgentBot在减少Llama 3.1 8B Instruct模型在消费者投诉场景中的幻觉方面最有效，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的幻觉问题限制了其在关键应用中的可靠性，尤其是在工业界广泛使用的紧凑模型（如LLaMA 3.1 8B Instruct）构建的消费者投诉聊天机器人中，需要有效减少幻觉。

Method: 开发了基于LLM的幻觉检测系统HalluDetect。对五种聊天机器人架构进行了基准测试，以评估其在减少幻觉方面的表现和令牌准确性。

Result: HalluDetect系统实现了69%的F1分数，比基线检测器高出25.44%。在评估的架构中，AgentBot将每轮幻觉降至0.4159，并保持了96.13%的最高令牌准确率，被认为是最有效的缓解策略。

Conclusion: 研究提供了一个可扩展的幻觉缓解框架，证明优化的推理策略能显著提高事实准确性。尽管应用于消费法领域，但该方法可推广到其他高风险领域，增强了LLM驱动助手的信任度。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [59] [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
*Xiaoying Song,Anirban Saha Anik,Dibakar Barua,Pengcheng Luo,Junhua Ding,Lingzi Hong*

Main category: cs.CL

TL;DR: 针对健康错误信息，提出一个名为Controlled-Literacy的RAG+RL框架，根据受众的健康素养水平生成定制化的反驳内容。


<details>
  <summary>Details</summary>
Motivation: 在线健康错误信息对公众健康构成威胁。现有反驳生成方法忽视了受众健康素养水平对反驳可读性和有效性的影响，通常生成统一回复。

Method: 提出Controlled-Literacy框架，结合检索增强生成（RAG）和强化学习（RL）。RAG用于检索与特定健康素养水平对齐的知识。RL通过结合主观用户偏好和客观可读性奖励函数来优化反驳内容，使其适应目标健康素养水平。

Result: 实验结果表明，Controlled-Literacy框架生成了更具可读性和更受用户偏爱的反驳内容，优于基线方法。

Conclusion: 本研究通过提高健康错误信息反驳内容的可读性和理解性，促进了更公平和更有影响力的公共健康传播。

Abstract: Health misinformation spreading online poses a significant threat to public
health. Researchers have explored methods for automatically generating
counterspeech to health misinformation as a mitigation strategy. Existing
approaches often produce uniform responses, ignoring that the health literacy
level of the audience could affect the accessibility and effectiveness of
counterspeech. We propose a Controlled-Literacy framework using
retrieval-augmented generation (RAG) with reinforcement learning (RL) to
generate tailored counterspeech adapted to different health literacy levels. In
particular, we retrieve knowledge aligned with specific health literacy levels,
enabling accessible and factual information to support generation. We design a
reward function incorporating subjective user preferences and objective
readability-based rewards to optimize counterspeech to the target health
literacy level. Experiment results show that Controlled-Literacy outperforms
baselines by generating more accessible and user-preferred counterspeech. This
research contributes to more equitable and impactful public health
communication by improving the accessibility and comprehension of counterspeech
to health misinformation

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图像修复的实时减弱现实（DR）系统，用于在共享混合现实（MR）会议中保护隐私，通过移除敏感物体，使其对其他参与者不可见。


<details>
  <summary>Details</summary>
Motivation: 在共享空间混合现实（MR）会议中，用户需要对个人或敏感物品进行隐私控制，确保这些物品不被其他参与者看到。

Method: 系统采用语义分割和精确物体选择，结合实时视频修复（使用修改后的解耦时空Transformer模型DSTT）实现物体移除。物体检测使用YOLOv11。该系统从第二观察者视角进行修复，利用移动ZED 2i深度相机实现，无需固定视角或预先3D扫描环境。

Result: 在720p分辨率下，该系统管线能维持超过20帧/秒的帧率。

Conclusion: 该研究证明了实时减弱现实在实际隐私保护混合现实应用中的可行性。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [61] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: 本文介绍SurgLaVi，一个迄今为止最大、最多样化、分层结构的外科视觉-语言数据集，并通过SurgCLIP模型验证其能显著提升手术理解和识别任务的性能，解决现有数据集的局限。


<details>
  <summary>Details</summary>
Motivation: 现有外科视觉-语言预训练（VLP）数据集在规模、程序多样性、语义质量和分层结构方面存在限制，阻碍了该领域在手术工作流理解和任务迁移方面的进展。

Method: 1. 构建了SurgLaVi数据集，包含近24万个剪辑-字幕对，涵盖200多种手术程序，并具有阶段、步骤、任务等多层次结构。2. 采用全自动管道生成视频的精细转录，并分割为程序单元。3. 应用双模态过滤以确保注释质量，并丰富字幕的上下文细节。4. 发布了SurgLaVi-{eta}，一个包含11.3万剪辑-字幕对的开源版本。5. 引入SurgCLIP，一个CLIP风格的视频-文本对比框架作为基准模型，以验证SurgLaVi数据集的价值。

Result: 1. SurgLaVi是目前最大、最多样化的外科视觉-语言数据集，其公开版本SurgLaVi-{eta}比现有数据集大四倍多。2. SurgCLIP在阶段、步骤、动作和工具识别方面均取得了显著改进，大幅超越了现有SOTA方法。3. 结果证实大规模、语义丰富和分层结构的数据集能直接转化为更强大、更具泛化能力的表征。

Conclusion: SurgLaVi是开发外科基础模型的关键资源，证明了其大规模、语义丰富和分层数据结构对构建更强大、更通用表征的重要性。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [62] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出并发布了一个基于SimCLR的通用高分辨率3D脑部结构MRI自监督学习（SSL）基础模型。该模型在包含多种神经系统疾病的大型多样化数据集上预训练，并在多个下游任务中超越了MAE和有监督基线，即使在有限标签数据下表现依然出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在分析3D MRI时通常针对特定任务且依赖大量标注数据，泛化能力有限。尽管自监督学习在2D医学影像中取得成功，但针对3D脑部MRI的基础模型仍存在分辨率、范围或可及性方面的限制。因此，需要开发一个通用、高分辨率且易于获取的3D脑部MRI基础模型。

Method: 开发了一个基于SimCLR的通用高分辨率3D脑部结构MRI自监督学习（SSL）基础模型。该模型在包含18,759名患者（44,958次扫描）的11个公开数据集上进行预训练，这些数据集涵盖了多种神经系统疾病。研究将该模型与Masked Autoencoders (MAE) 以及两个有监督基线模型进行了比较，并在四项不同的下游预测任务上进行了评估，包括分布内和分布外设置。

Result: 微调后的SimCLR模型在所有任务中均优于所有其他模型。值得注意的是，即使仅使用20%的标注训练样本来预测阿尔茨海默病，该模型仍能达到卓越的性能。

Conclusion: 本工作提出了一个广泛适用且易于访问的临床脑部MRI分析基础模型。通过提供公开代码、数据和训练好的模型，为3D脑部结构MRI分析领域做出了重要贡献。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [63] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: 本文提出USCTNet，一个深度展开求解器，通过在可学习变换域中进行物理约束的逆问题，显式估计相机光谱灵敏度(CSS)和场景照明，并引入数据自适应低秩子空间奇异值阈值(SVT)算子，从单张RGB图像重建高光谱图像(HSI)，解决了重建不适定和物理不一致的问题，并提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 从单张RGB图像重建高光谱图像(HSI)是一个不适定问题，并且当相机光谱灵敏度(CSS)和场景照明参数设置不准确时，可能导致物理上不一致的重建结果。

Method: 研究将RGB到HSI重建问题表述为一个物理约束的逆问题，并在可学习的变换域中使用核范数进行正则化。为了确保色彩度一致性，方法在每次迭代中明确估计CSS和照明以定义前向算子。为了避免全奇异值分解(SVD)的成本和不稳定性，引入了数据自适应低秩子空间SVT算子。在此基础上，开发了一个名为USCTNet的深度展开求解器，该网络结合了参数估计模块和可学习的近端更新。

Result: 在标准基准测试上进行的大量实验表明，与现有最先进的基于RGB的方法相比，所提出的方法在重建精度上取得了持续的改进。

Conclusion: 通过将物理约束的逆问题与可学习的低秩正则化、显式参数估计以及深度展开网络相结合，本方法能有效且一致地从RGB图像重建高光谱图像，并在重建精度上超越了现有技术。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [64] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 在胶质瘤分类和分割等医学图像任务中，卷积神经网络(CNNs)的表现优于大型语言模型(LLMs)，表明当前LLMs在图像空间理解方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在基于文本的医疗保健任务中表现出色，但其在基于图像的应用中的效用尚未被探索。

Method: 研究评估了通用视觉-语言LLM（LLaMA 3.2 Instruct，预微调和微调后）与定制3D CNN在BraTS 2020多模态脑MRI数据集上的性能。任务包括胶质瘤分类（低级别 vs. 高级别）和分割（采用中心点、边界框和多边形提取方法）。

Result: 在胶质瘤分类中，CNN达到了80%的准确率和平衡的精确度召回率。通用LLM准确率为76%，但特异性仅为18%，常误分类低级别肿瘤。微调后，LLM特异性提升至55%，但总体性能下降（准确率降至72%）。在分割任务中，CNN能准确定位胶质瘤，而LLMs的预测一致地聚集在图像中心，未能区分肿瘤大小、位置。LLM微调改善了输出格式，但未能实质性提高空间准确性。总体而言，CNN在两项任务中均优于LLMs，LLMs表现出有限的空间理解能力且微调效果不显著。

Conclusion: LLMs以其当前形式，不适用于图像处理任务。要提高LLMs在医学领域的性能、鲁棒性和实用性，需要更严格的微调或替代训练策略。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [65] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: 本文提出了Stable Part Diffusion 4D (SP4D) 框架，通过单目输入生成配对的RGB和运动学部件视频，其输出可用于下游动画和运动相关任务。


<details>
  <summary>Details</summary>
Motivation: 现有部件分割方法主要依赖外观语义，难以生成与物体关节对齐且在不同视角和时间上一致的运动学部件。本研究旨在解决从单目输入生成这些对动画和运动至关重要的运动学部件的挑战。

Method: SP4D采用双分支扩散模型，协同合成RGB帧和对应部件分割图。为简化架构和支持不同部件计数，引入空间颜色编码方案，将部件掩码映射为连续的RGB类图像，使分割分支能共享RGB分支的潜在VAE。BiDiFuse模块增强跨分支一致性，辅以对比部件一致性损失促进预测的空间和时间对齐。为训练和评估，构建了包含2万多个绑定对象的KinematicParts20K数据集，包含多视角RGB和部件视频序列。

Result: SP4D生成的2D部件图可以少量手动调整后提升至3D，以推导骨架结构和谐波蒙皮权重。实验证明，SP4D在真实视频、新生成对象和罕见姿态等多样场景中表现出强大的泛化能力，能生成适用于下游动画和运动相关任务的运动学感知输出。

Conclusion: SP4D成功提出了一个新颖的框架，能够从单目输入生成高质量、一致的运动学部件视频，并证明其输出在3D重建、动画和运动分析等下游任务中具有巨大潜力。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [66] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR通过可提示零样本视频分割，结合RGB和姿态信息进行手语识别，通过分割肢体保留细节并聚焦处理，在ChaLearn249 IsoGD数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的隔离手语识别（ISLR）方法在结合RGB和姿态信息时，由于边界框等不精确表示，常会丢失手形和方向等关键细节。

Method: 提出SegSLR系统，通过可提示零样本视频分割结合RGB和姿态信息进行手语识别。利用姿态信息提供的手和身体粗略定位，在视频中分割这些部分以保留所有相关形状信息，进而将RGB数据处理聚焦于最相关的身体部位。

Result: SegSLR在复杂的ChaLearn249 IsoGD数据集上表现优于现有最先进方法。消融研究表明，SegSLR通过聚焦于打手语者的身体和手部获得了显著益处。

Conclusion: SegSLR通过视频分割有效结合RGB和姿态信息，解决了现有方法细节丢失问题，从而在隔离手语识别中表现出色。聚焦于打手语者的身体和手部的设计选择是合理且有效的。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [67] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: 本文提出一个名为SCOPE的语音引导协同感知框架，结合大语言模型和视觉基础模型，通过医生语音反馈实现手术场景中器械和解剖结构的即时分割、标注和追踪，旨在开发适应性强的免手持手术工具。


<details>
  <summary>Details</summary>
Motivation: 现有的手术场景分割和追踪方案受限于依赖标注数据的领域特定模型，或依赖手动视觉/文本提示的视觉基础模型，难以适应新的手术场景并实现开放集分割，限制了其在术中环境的部署。

Method: 引入SCOPE框架，该框架整合了LLM的推理能力和开放集VFM的感知能力。核心是一个协同感知代理，通过生成VFM分割候选并结合临床医生的直观语音反馈来引导手术器械分割。随后，器械本身可作为交互式指针来标注其他手术场景元素。

Result: 在Cataract1k数据集子集和内部离体颅底数据集上验证了该框架，展示了其在手术场景中生成即时分割和追踪的潜力。通过一次实时模拟离体实验，进一步证明了其动态能力。

Conclusion: 这种人机协作范式展示了为动态手术室环境开发适应性强、免手持、以外科医生为中心的工具的巨大潜力。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [68] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出4D-GRT，结合4D高斯溅射和物理光线追踪，用于模拟真实世界相机效应并生成带可控、物理精确相机效应的视频，同时实现了快速渲染和高质量。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉系统假设理想针孔相机，在面对鱼眼畸变和卷帘快门等真实世界相机效应时失效，原因在于缺乏带有这些效应的训练数据。现有数据生成方法存在成本高、模拟与真实之间的鸿沟或无法准确建模相机效应的问题。

Method: 提出4D Gaussian Ray Tracing (4D-GRT) 两阶段流水线。首先，利用4D高斯溅射从多视角视频重建动态场景。然后，应用物理光线追踪技术生成具有可控、物理精确相机效应的视频。

Result: 4D-GRT实现了最快的渲染速度，并且渲染质量优于或与现有基线相当。此外，作者构建了一个包含八个合成室内动态场景和四种相机效应的基准数据集，用于评估生成的视频。

Conclusion: 4D-GRT为生成带有物理精确相机效应的训练数据提供了一个高效且高质量的解决方案，有望帮助弥补现有计算机视觉系统在处理真实世界相机效应时的不足。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [69] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 本文提出一种多智能体学习方法，将视频编辑自动化为顺序决策过程。通过Editor和Critic智能体，接收自然语言指令并生成视频序列，在多项评估指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑自动化工具主要侧重于检索或用户界面，未能自动化核心编辑任务，将实际编辑留给用户。本研究旨在解决此问题，实现核心视频编辑任务的自动化。

Method: 采用多智能体方法，将视频编辑视为顺序决策过程。设计了Editor智能体（接收视频片段和自然语言指令，利用常用编辑工具生成序列）和Critic智能体（提供自然语言反馈或渲染满意序列）。引入基于学习的方法以实现智能体间针对语言驱动视频编辑任务的有效通信。通过用户研究（定性、定量）和LLM-as-a-judge指标评估系统输出。

Result: 评估结果显示，本系统在覆盖率、时间约束满足度以及人类偏好方面显著优于现有方法。

Conclusion: 本研究成功通过创新的多智能体学习方法自动化了核心视频编辑任务，实现了高效的语言驱动视频编辑，并在多项指标上表现出卓越的性能。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [70] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 本研究提出了一个稳定性感知机器学习框架，通过无造影剂MRI图像预测胶质瘤的造影剂增强情况，减少对钆基造影剂的依赖，并提高多中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 钆基造影剂在胶质瘤成像中存在安全性、成本和可及性问题。通过无造影剂MRI预测对比增强是一种更安全的替代方案，但扫描仪和队列差异阻碍了模型的鲁棒选择。

Method: 分析了来自四个TCIA数据集的1,446例胶质瘤病例。以无造影剂T1WI作为输入，从配对的增强后T1WI中获取增强信息。提取108个PyRadiomics特征，并结合48种降维方法和25种分类器，生成1,200条机器学习管道。采用轮转验证（在三个数据集上训练，在一个数据集上测试）来评估模型性能和稳定性。

Result: 交叉验证预测准确率介于0.91至0.96。外部测试准确率达到0.87 (UCSF-PDGM)、0.98 (UPENN-GB) 和0.95 (BRATS-Africa)，平均为0.93。F1、精确率和召回率稳定（0.87至0.96），而ROC-AUC变化较大（0.50至0.82），反映了队列异质性。MI与ETr管道组合表现最佳，兼顾了准确性和稳定性。

Conclusion: 该稳定性感知框架能够从无造影剂胶质瘤MRI中可靠地预测造影剂增强，降低了对钆基造影剂的依赖，并提升了跨中心的泛化能力。它为神经肿瘤学及其他领域的机器学习研究提供了一个可扩展的、可复现的模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [71] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 针对无人机图像中小目标漏检问题，本文提出一种检测器无关的后处理框架，通过重叠分块和群组证据验证（空间与语义聚类）来提升召回率，适用于对召回率敏感的应用。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中密集的小目标由于远距离视角、遮挡和杂乱等原因经常被漏检。

Method: 本文提出一个检测器无关的后处理框架，将重叠导致的冗余转化为群组证据。首先，通过重叠分块恢复低置信度候选框；然后，利用空间门（基于边界框质心的DBSCAN聚类）和语义门（基于ResNet-18嵌入的DBSCAN聚类）验证群组证据；最后，对验证过的群组进行置信度重加权，并与类别感知的NMS融合。该框架无需重新训练，可与现代检测器集成。

Result: 在VisDrone数据集上，召回率从0.685提升到0.778（+0.093），精度从0.801调整到0.595，F1分数达到0.669。后处理平均延迟为每图0.095秒。消融实验证实了分块、空间聚类、语义聚类和重加权对提升性能的贡献。

Conclusion: 研究结果表明该方法具有“召回优先，精度折衷”的特点，有利于远场计数和监控等对召回率敏感的应用。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [72] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了 InternScenes，一个大规模、可模拟的室内场景数据集，包含约4万个多样化场景，通过整合多种来源，解决了现有数据集在规模、多样性、布局真实性和物体碰撞方面的不足，并展示了其在具身AI任务中的价值。


<details>
  <summary>Details</summary>
Motivation: 具身AI的发展严重依赖于大规模、多样化且布局真实的3D可模拟场景数据集。然而，现有数据集在数据规模或多样性、缺少小物件的简化布局以及严重的物体碰撞方面存在局限性。

Method: 通过整合真实世界扫描、程序生成场景和设计师创建场景这三种不同来源，构建了包含约4万个多样化场景的InternScenes数据集。该方法特别保留了大量小物件，并采用全面的数据处理流程，包括为真实世界扫描创建实物到模拟的复制品、整合交互式对象以及通过物理模拟解决物体碰撞，以确保场景的可模拟性和交互性。

Result: InternScenes数据集包含约4万个多样化场景，1.96M个3D对象，覆盖15种常见场景类型和288种对象类别。它通过保留大量小物件，实现了平均每区域41.5个对象的真实复杂布局。在场景布局生成和点目标导航这两个基准应用中，InternScenes展现了其复杂真实布局带来的新挑战，并为这些任务的模型训练扩展提供了可能。

Conclusion: InternScenes是一个有价值的大规模、多样化且可模拟的室内场景数据集，它克服了现有数据集的局限性，为具身AI领域（尤其是场景生成和导航任务）的模型训练提供了扩展的途径，并致力于开源以造福整个社区。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [73] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 本文分析数学手写识别中多项式基选择（如Legendre、Chebyshev及其Sobolev变体）与多项式次数的权衡，旨在优化建模精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索不同多项式基和多项式次数在数学手写识别中实现准确建模与降低计算成本之间的最佳平衡点。

Method: 通过考察这些基中多项式求值的条件数，并利用内积来限定不同符号间变异的范数。

Result: 摘要中未直接给出具体研究结果，但表明论文将揭示不同基选择和多项式次数在建模准确性和计算成本方面的权衡。

Conclusion: 摘要中未提供明确的研究结论。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [74] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 针对胶质瘤进展预测，本文提出一个多任务扩散框架，解决了稀疏MRI数据问题，并能生成与时间无关的像素级概率演变图，辅助临床风险评估。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤进展迅速、预后差，准确预测其演变具有挑战性。临床上MRI数据稀疏、获取不规律且随访序列不完整，导致数据不平衡，难以进行可靠建模。

Method: 本文提出一个多任务扩散框架，用于与时间无关的像素级胶质瘤进展预测。该模型能同时生成任意时间点的未来FLAIR序列，并利用符号距离场（SDFs）估计空间概率性肿瘤演变图，量化不确定性。为捕捉任意时间间隔的肿瘤演变动态，集成了一个预训练的形变模块。为应对数据稀缺，实施了目标增强流程，合成完整的三次随访扫描序列并补齐缺失的MRI模态。此外，引入了放射治疗加权的焦点损失项，利用放射剂量图以突出临床重要区域。

Result: 该框架仅基于早期两次随访扫描，即可生成灵活的、随时间变化的概率图，使临床医生能够查询任何未来时间点的肿瘤进展风险。该方法在公开数据集和内部私有数据集上均取得了有前景的结果。

Conclusion: 提出的多任务扩散框架为胶质瘤进展预测提供了一个鲁棒的解决方案，有效克服了稀疏临床数据的挑战，并为临床医生提供了关于未来肿瘤演变风险的灵活、可解释的洞察。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [75] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 本文提出一种利用点-平面投影从2D表示学习特征，并结合几何感知数据增强的LiDAR点云语义分割方法，有效提升了数据稀缺场景下的性能，并在标准数据集上取得有竞争力结果。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云语义分割方法计算复杂度高，需要大量训练数据，导致在数据稀缺场景中泛化能力受限。

Method: 1. 通过点-平面投影，从点云的多个2D表示中有效学习特征，以提取互补信息，且仅依赖LiDAR数据。2. 引入一种几何感知的数据增强技术，该技术符合LiDAR传感器特性并缓解类别不平衡问题。

Result: 该方法在数据受限场景中取得了显著性能提升，并在SemanticKITTI和PandaSet两个标准公开数据集上达到了有竞争力的结果。

Conclusion: 通过结合2D表示学习和几何感知的数据增强，本文提出的方法为LiDAR点云语义分割提供了一种高效且在有限数据场景下表现卓越的解决方案。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [76] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: OpenUrban3D是首个无需多视角图像、预训练网络或手动标注的3D城市点云开放词汇语义分割框架，显著提升了分割精度和跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割对数字孪生、智慧城市等大型城市点云应用至关重要，但在此领域尚不成熟。主要障碍包括：城市点云数据中高质量、对齐的多视角图像缺失，以及现有3D分割管道在多样城市环境中泛化能力差。

Method: 提出OpenUrban3D框架，无需多视角图像、预训练点云分割网络或手动标注。通过多视角、多粒度渲染，掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成鲁棒的语义特征，然后蒸馏到3D骨干模型中，实现对任意文本查询的零样本分割。

Result: 在SensatUrban和SUM等大型城市基准测试中，OpenUrban3D在分割精度和跨场景泛化方面均显著优于现有方法。

Conclusion: OpenUrban3D为3D城市场景理解提供了一种灵活且可扩展的解决方案。

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [77] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: AutoOEP是一个基于双摄像头、计算机视觉和机器学习的多模态在线考试自动监考系统，能有效检测多种作弊行为，准确率高且资源效率高。


<details>
  <summary>Details</summary>
Motivation: 随着在线教育的兴起，远程考试的学术诚信保障面临挑战。传统人工监考难以规模化，现有自动化方案存在侵入性或检测范围有限等问题。

Method: 本文提出了AutoOEP，一个综合性多模态框架。它利用双摄像头捕捉考生正面和工作区侧面视图，并集成多个并行分析模块：人脸模块（通过ArcFace进行身份验证、头部姿态、凝视跟踪、嘴部运动分析）和手部模块（使用YOLOv11检测违禁物品并追踪手部与物品的距离）。这些模块的特征被聚合后输入长短期记忆（LSTM）网络，分析时序模式以计算实时作弊概率。

Result: 在定制数据集上评估，AutoOEP在分类可疑活动方面达到了90.7%的准确率。目标检测组件对违禁物品的平均精度（mAP@.5）为0.57。整个框架在无GPU的情况下处理视频流速度约为2.4帧/秒。

Conclusion: AutoOEP被证明是一种有效且资源高效的自动化监考解决方案，能够显著减少人工干预需求，并提升在线评估的诚信度。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [78] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 针对CASSI光谱成像中高压缩比重建问题，本文提出双相机CASSI框架，整合TV次梯度理论与动态正则化策略，有效提升重建质量，保持空间-光谱一致性，并提供可解释的数学基础。


<details>
  <summary>Details</summary>
Motivation: 光谱成像技术在平衡光谱、空间、时间分辨率上存在挑战。压缩感知CASSI虽能缓解，但高压缩比导致重建问题不适定。传统模型方法受限于手工先验，深度学习方法缺乏物理可解释性。

Method: 提出一个双相机CASSI重建框架，整合全变分(TV)次梯度理论。建立端到端SD-CASSI数学模型以降低逆问题求解复杂度。引入动态正则化策略，利用RGB/全色参考图像的归一化梯度约束构建具有严格凸优化保证的TV次梯度相似函数。设计自适应参考生成与更新机制，利用辅助相机空间先验提供次梯度指导。

Result: 实验结果表明，所提方法能有效保持空间-光谱结构一致性。理论框架为计算光谱成像建立了可解释的数学基础，并在多种重建场景中展现出鲁棒性能。

Conclusion: 通过整合TV次梯度理论和动态正则化策略，本研究成功解决了CASSI高压缩比下的重建难题，弥补了现有方法的不足。该框架不仅提高了重建质量和结构一致性，还提供了可解释的数学基础，为多相机计算光谱成像系统提供了新思路。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [79] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑型（2.5M参数）元数据感知混合专家掩码自编码器（MoE-MAE），通过地理-时间条件化预训练，实现了与大型地球观测（EO）基础模型相当的性能，提高了可访问性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模地球观测基础模型计算成本高昂，限制了它们的可访问性和下游任务的重用。研究人员旨在探索紧凑型架构，以开发更小、更通用的EO模型。

Method: 提出了一种参数量仅为2.5M的元数据感知混合专家掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理-时间条件化（包括图像、经纬度及季节/日循环编码）。模型在BigEarthNet-Landsat数据集上进行预训练，并使用线性探针评估其冻结编码器产生的嵌入。

Result: 尽管模型规模很小，但其性能与大型架构相当，表明元数据感知预训练能提升迁移学习能力和标签效率。在缺乏明确元数据的EuroSAT-Landsat数据集上评估时，模型依然展现出与数亿参数模型相当的竞争力。

Conclusion: 紧凑的元数据感知MoE-MAE是未来地球观测基础模型发展中高效且可扩展的一步。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [80] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习（ESWGAN-GP）的运动伪影校正方法，并通过优化正弦图模拟技术生成配对数据集，以解决高分辨率外周定量CT (HR-pQCT) 中的运动伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率外周定量CT (HR-pQCT) 中的运动伪影严重阻碍了骨微结构的体内评估。由于缺乏标准化的退化模型，目前尚无有效的运动校正方法。

Method: 1. 优化了基于正弦图的传统方法来模拟HR-pQCT图像中的运动伪影，创建了运动损坏图像及其对应真实值的配对数据集。2. 提出了一种边缘增强型自注意力Wasserstein生成对抗网络与梯度惩罚（ESWGAN-GP），该模型包含边缘增强跳跃连接和自注意力机制，并使用VGG-based感知损失来校正模拟和真实数据集中的运动伪影。

Result: 在模拟数据集上，ESWGAN-GP实现了平均信噪比(SNR) 26.78，结构相似性指数(SSIM) 0.81，视觉信息保真度(VIF) 0.76。在真实数据集上，性能进一步提升，SNR为29.31，SSIM为0.87，VIF为0.81。

Conclusion: 尽管所提出的方法是对真实运动的简化表示，但它为HR-pQCT中基于深度学习的运动校正迈出了重要的初始一步，有助于解决该模态广泛应用所面临的主要挑战之一。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [81] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 本文研究了影响最先进注视认证性能的关键因素，发现校准目标深度、校准与非校准注视的融合以及眼动追踪信号质量对性能有积极影响，而简单的移动平均滤波则略有负面影响。


<details>
  <summary>Details</summary>
Motivation: 旨在识别并分析影响最先进注视认证系统性能的关键因素。

Method: 利用包含8,849名受试者的大规模内部数据集（通过Meta Quest Pro等效硬件以72Hz收集），采用最先进的神经网络架构，研究了眼动追踪信号质量、校准方面以及原始注视估计的简单滤波对认证性能的影响。

Result: 研究发现，使用相同的校准目标深度、融合校准和非校准注视数据、以及提高眼动追踪信号质量均能提升认证性能。然而，简单的三样本移动平均滤波器通常会略微降低认证性能。大多数发现普遍适用，但也存在少数例外。

Conclusion: 优化眼动追踪校准（特别是校准目标深度）、有效融合注视数据和提升信号质量是提高注视认证性能的关键。应谨慎使用简单的滤波方法。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [82] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: 该研究引入了TrueSkin数据集，旨在解决现有模型在肤色识别和生成方面的不足，并证明其能显著提升模型在这些任务上的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 肤色识别和生成在模型公平性、医疗保健和生成式AI中至关重要，但现有模型（特别是大型多模态模型和图像生成模型）由于缺乏全面的数据集和稳健的方法，难以准确识别和合成肤色。

Method: 引入了TrueSkin数据集，包含7299张系统分类为6个肤色类别，并在不同光照、摄像机角度和捕获设置下收集的图像。使用TrueSkin数据集对现有识别和生成方法进行了基准测试，并在此数据集上训练识别模型及微调图像生成模型。

Result: 基准测试揭示了显著偏见：大型多模态模型倾向于将中间肤色错误分类为较浅肤色；生成模型在受到提示中无关属性（如发型、环境）影响时，难以准确生成指定肤色。在TrueSkin上训练的识别模型比现有模型精度提高20%以上，微调图像生成模型也显著提升了肤色保真度。

Conclusion: 研究结果强调了像TrueSkin这样全面数据集的必要性，它不仅可作为评估现有模型的基准，也是提升肤色识别和生成任务公平性和准确性的宝贵训练资源。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [83] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 针对无人机动物检测中数据不足和预训练模型选择困难的问题，本文提出了一个基于强化学习（RL）和上限置信度（UCB）算法的迁移学习框架，以自动化选择最佳预训练模型，并取得了更高的检测率和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 动物健康监测和种群管理日益依赖自动化检测系统，无人机结合计算机视觉是前景广阔的非侵入式解决方案。然而，缺乏标注训练数据是深度学习模型开发的主要障碍。尽管迁移学习能缓解此问题，但现有大量预训练模型使选择最佳模型变得困难，特别是对于新入门的研究者。

Method: 本文提出了一个基于强化学习（RL）的迁移学习框架，该框架采用上限置信度（UCB）算法，自动选择最适合动物检测任务的预训练模型。该方法通过系统地评估候选模型的性能并进行排序，从而简化模型选择过程。

Result: 实验结果表明，与传统方法相比，本文提出的框架在显著减少计算时间的同时，实现了更高的检测率。

Conclusion: 该RL-UCB迁移学习框架能有效解决动物检测中预训练模型选择的挑战，提高了检测性能并降低了计算成本，为动物监测提供了更高效的自动化解决方案。

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [84] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: 为解决FungiCLEF 2025竞赛中真菌物种识别的少样本和类别不平衡问题，本文提出一种基于原型网络的深度学习方法，在Recall@5指标上显著超越竞赛基线30多个百分点。


<details>
  <summary>Details</summary>
Motivation: 自动真菌物种识别工具对真菌学家和公民科学家至关重要，有助于大规模生物多样性监测。然而，此类系统需处理高度不平衡的类别分布，并需在许多物种（特别是稀有和缺乏记录的类群）仅有极少训练样本的情况下提供可靠性能。

Method: 提出一种基于原型网络的鲁棒深度学习方法，通过增强原型表示来改善少样本真菌分类能力。

Result: 该原型网络方法在Recall@5指标上，于公共和私人排行榜均超越竞赛基线30多个百分点。

Conclusion: 该方法在准确识别常见和稀有真菌物种方面展现出强大潜力，有效支持FungiCLEF 2025竞赛的主要目标。

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [85] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [86] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 本文提出一个零样本视频异常检测（ZS-VAD）框架，利用骨架数据结合语言模型进行行为典型性建模和测试时上下文独特性分析，解决了现有方法泛化性差的问题，并在多个大型数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频异常检测（ZS-VAD）因数据隐私和新部署等实际问题而至关重要。骨架数据在ZS-VAD中具有消除背景和人体外观领域差异的泛化优势。然而，现有方法仅学习低级骨架表示并依赖领域受限的正常性边界，难以泛化到具有不同正常和异常行为模式的新场景。

Method: 本文提出了一个零样本视频异常检测框架，通过动作典型性和独特性学习来挖掘骨架数据的潜力。首先，引入了一个语言引导的语义典型性建模模块，将骨架片段投影到动作语义空间，并利用大型语言模型（LLM）的知识来学习典型正常和异常行为。其次，提出了一个测试时上下文独特性分析模块，用于精细分析骨架片段的时空差异，并推导出场景自适应的边界。

Result: 在不使用任何目标域训练样本的情况下，该方法在四个大型VAD数据集（ShanghaiTech、UBnormal、NWPU和UCF-Crime，涵盖100多个未见过监控场景）上，相对于基于骨架的方法实现了最先进（state-of-the-art）的结果。

Conclusion: 该研究成功地通过结合语言模型引导的语义典型性学习和测试时上下文独特性分析，解锁了骨架数据在零样本视频异常检测中的潜力，有效解决了现有方法的泛化性问题，并在多个实际场景中展现出优越的性能。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [87] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 开发了一个名为 Organoid Tracker 的GUI平台，利用SAM2实现肾脏类器官显微视频的自动化分析，量化囊肿形成等关键指标，加速PKD研究和药物发现。


<details>
  <summary>Details</summary>
Motivation: 肾脏类器官模型在多囊肾病（PKD）研究和药物发现中具有潜力，但其产生的时空显微视频数据目前依赖手动分析，效率低下且无法提取像素级和纵向的详细定量信息，限制了研究进展。

Method: 开发了Organoid Tracker，一个基于模块化插件架构的图形用户界面（GUI）平台。该平台利用先进的视觉基础模型Segment Anything Model 2 (SAM2) 实现零样本分割和时空显微视频的自动化分析，使研究人员无需编程知识即可提取详细定量指标。

Result: Organoid Tracker成功量化了囊肿形成率、生长速度和形态变化等关键指标，并能生成全面的报告。它克服了手动分析的瓶颈，使研究人员能够高效地获取像素级和纵向的宝贵信息。

Conclusion: Organoid Tracker提供了一个可扩展的开源框架，为改善和加速肾脏发育、PKD建模和治疗发现等领域的研究提供了强大的解决方案。该平台作为开源软件公开发布。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [88] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 该报告介绍了CVPR 2024自动驾驶大挑战中“驾驶与语言”赛道的领先方案，通过LORA和DoRA微调LLaVA模型，整合深度信息，并采用思维链推理，在DriveLM-nuScenes数据集上取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 为CVPR 2024自动驾驶大挑战的“驾驶与语言”赛道开发并展示一种高效的视觉语言模型系统。

Method: 模型基于LLaVA，使用LoRA和DoRA方法进行微调，并 exclusively 利用DriveLM-nuScenes数据集进行训练。系统集成了来自开源深度估计模型的深度信息。推理时，特别是对于多选题和是非题，采用了思维链（Chain-of-Thought）推理方法。

Result: 在验证集排行榜上取得了0.7799的最高分数，排名第一。

Conclusion: 该综合方法在“驾驶与语言”挑战赛中表现出色，证明了其在解决自动驾驶中视觉语言任务的有效性和领先性。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [89] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 提出一种鲁棒的多模态框架，通过融合相机和激光雷达数据，并使用IMU自监督学习，预测行星探测车的地面可通行性成本图。


<details>
  <summary>Details</summary>
Motivation: 为行星探测车提供准确且鲁棒的地面可通行性成本图，并针对现有工作可能夸大效果的问题，旨在提供一个高保真、可重现的解决方案。

Method: 构建多模态框架，融合相机和激光雷达数据以生成鸟瞰图（BEV）地形成本图。采用IMU数据进行自监督训练，结合DINOv3图像编码器、FiLM传感器融合和Huber与平滑项的优化损失。提供了高保真、可重现的仿真环境和自监督IMU标注流水线。

Result: 消融实验（如去除图像颜色、遮挡、噪声）显示模型在MAE/MSE上仅有微小变化（例如激光雷达稀疏化时MAE从0.0775增至0.0915），表明模型高度鲁棒，且地形几何信息在学习成本中占据主导地位。性能差异小归因于IMU标注主要反映地形几何而非语义以及数据多样性有限。

Conclusion: 本文贡献了一个强大的多模态BEV成本图预测模型、高保真仿真环境和自监督IMU标注流水线。模型展现出高度鲁棒性，其学习成本主要受地形几何驱动。未来工作将关注域泛化和数据集扩展，以克服IMU标签的几何倾向和数据多样性不足等局限。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [90] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: CAA-Policy提出了一种控制辅助注意力机制，通过控制输出的自监督梯度训练，实现了感知与控制的有效协同，显著提升了端到端精确泊车系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端学习方法在精确泊车中缺乏有效的感知与控制协同，单独使用Transformer自注意力会导致空间注意力不稳定，影响策略可靠性。

Method: 提出CAA-Policy，一个端到端模仿学习系统。核心是控制辅助注意力（CAA）机制，利用控制信号通过控制输出的反向传播梯度自监督地指导视觉注意力学习，使其关注引起动作高方差的视觉特征。同时，集成短程路点预测作为辅助任务，并引入独立的运动预测模块以增强稳定性。

Result: 在CARLA模拟器中，CAA-Policy在精度、鲁棒性和可解释性方面，持续超越了端到端学习基线和模块化BEV分割+混合A*管线。

Conclusion: CAA-Policy通过其新颖的控制辅助注意力机制和辅助任务，有效解决了精确泊车中感知与控制协同的挑战，提供了一个更鲁棒、更具泛化能力和可解释性的解决方案。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [91] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 通过将全景视频生成视为自适应问题，本文利用LoRA高效微调预训练视频扩散模型，成功生成高质量全景视频，并在多项指标上超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 生成高质量360度全景视频面临挑战，主要源于全景与传统透视视图投影的根本差异，导致标准视频生成模型难以适应，且现有解决方案通常效率低下、效果不佳。研究动机在于受LoRA在风格迁移中成功的启发，提出将全景视频生成视为从透视视图的自适应问题。

Method: 该研究将全景视频生成视为从透视视图的自适应问题。通过理论分析，证明了当LoRA的秩超过任务自由度时，它能有效建模这两种投影之间的转换。具体方法是利用LoRA高效微调一个预训练的视频扩散模型，仅使用大约1,000个视频进行训练。

Result: 该方法实现了高质量的全景视频生成。实验结果表明，它不仅保持了正确的投影几何，还在视觉质量、左右一致性和运动多样性方面超越了以往的SOTA方法。

Conclusion: 通过将全景视频生成框架化为从透视视图的低秩自适应问题，并利用LoRA高效微调预训练模型，能够有效克服现有挑战，生成高质量且具有良好几何一致性的全景视频。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [92] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出SMILE，一种超分辨率引导的多任务学习高光谱解混方法，通过理论分析验证了任务亲和性并保证了收敛性，并设计了一个学习共享和特定表示的框架，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高光谱解混性能受限于低空间分辨率。尽管可以通过多任务学习结合超分辨率进行增强，但直接集成面临任务亲和性未验证和解混收敛性无法保证的挑战。

Method: 本文提出SMILE，一种超分辨率引导的多任务学习高光谱解混方法。它通过提供理论分析（包括关系定理、存在定理和可达性定理）来验证任务亲和性、证明超分辨率的积极指导作用并保证解混的收敛性（通过证明最优解）。该框架通过学习共享和特定表示来将超分辨率的积极信息推广到解混任务。

Result: 理论分析验证了多任务学习的可行性、任务亲和性，并保证了解混的收敛性。在合成和真实数据集上的实验结果证实了所提SMILE方法的有效性。

Conclusion: 该工作为高光谱解混提供了渐进的理论支持和一个新的超分辨率引导框架（SMILE），成功解决了多任务学习中任务亲和性和收敛性的挑战，并被证明具有实际应用价值。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [93] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 针对多时相高光谱解混中现有方法无法有效建模时间依赖性的问题，本文提出了一种基于copula理论的Cog-TD方法，通过新数学模型和框架捕捉动态物质演变，并提供了理论支持和实验验证。


<details>
  <summary>Details</summary>
Motivation: 多时相高光谱解混（MTHU）现有方法在建模时间依赖性方面存在局限，未能捕捉动态物质演变，因此需要一种能显式建模依赖结构的新方法。

Method: 提出了一种名为Cog-TD的copula引导时间依赖性方法。该方法定义了新的数学模型，将copula理论融入MTHU问题定义以描述时间依赖结构；构建了copula引导框架，利用copula函数估计具有时间依赖性的动态端元和丰度；并开发了copula函数估计和时间依赖性指导两个关键模块，计算并利用时间信息指导解混过程。

Result: 在合成和真实世界数据集上的实验结果均证明了所提出方法的有效性。

Conclusion: 本文重新定义了MTHU问题的时间依赖性，提出了一个copula引导框架，开发了两个关键模块，并提供了理论支持，有效解决了多时相高光谱解混中的时间依赖性建模难题。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [94] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 本文提出首个用于灾后评估的3D基准数据集3DAeroRelief，通过无人机收集数据并生成精细的3D点云，以克服现有2D图像和3D数据集在灾害场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 灾后结构损伤的及时评估对灾害响应和恢复至关重要。然而，现有自然灾害分析工作多依赖缺乏深度和空间上下文的2D图像，而现有3D语义分割基准数据集主要关注城市或室内场景，缺乏针对受灾区域的数据。

Method: 研究人员使用低成本无人机在飓风受损区域收集数据，通过运动结构（SfM）和多视图立体（MVS）技术重建密集的3D点云。语义标注通过手动2D标注并投影到3D空间完成。

Result: 研究结果是构建了3DAeroRelief数据集——首个专为灾后评估设计的3D基准数据集。该数据集捕获了真实灾害背景下的3D大规模室外环境和细粒度结构损伤。通过评估多项先进3D分割模型，展示了数据集的实用性，并揭示了灾害响应中3D场景理解的挑战与机遇。

Conclusion: 3DAeroRelief数据集为推动灾后场景中鲁棒3D视觉系统在实际应用中的发展提供了宝贵资源。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [95] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 针对遥感语义分割中多模态信号缺失问题，本文提出GEMMNet网络，通过混合特征提取器、多尺度混合融合和互补损失方案，有效克服现有生成模型在处理复杂场景异构数据时的局限性及对主导模态的偏倚，在两个遥感数据集上均超越基线和SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在多种领域表现优异，但在实际场景中，多模态信号易因传感器故障或恶劣天气而缺失，严重影响模型性能。现有生成模型（如AE和GAN）虽可用于重建缺失模态，但在遥感语义分割中其有效性尚未充分探索。它们在处理遥感数据的异构性时，难以捕捉复杂场景的语义上下文，且易受主导模态偏倚影响，降低模型鲁棒性。

Method: 本文提出一种生成增强多模态学习网络（GEMMNet），包含三个关键组件：(1) 混合特征提取器（HyFEx）有效学习模态特定表示；(2) 混合多尺度感知融合（HyFMA）捕获跨尺度的模态协同语义上下文；(3) 互补损失（CoLoss）方案通过鼓励模态和任务之间的一致性来减轻固有偏倚。

Result: GEMMNet在Vaihingen和Potsdam两个挑战性遥感语义分割数据集上，性能优于生成基线模型（AE, cGAN）以及最先进的非生成方法（mmformer, shaspec）。

Conclusion: GEMMNet通过其独特架构（HyFEx、HyFMA、CoLoss）有效解决了多模态遥感语义分割中因信号缺失导致的性能下降问题，并克服了现有生成模型在处理复杂异构数据和模态偏倚方面的局限性，实现了卓越的分割性能。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [96] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 提出一种从单视角野外视频中提取、重建动态3D烟雾资产并支持交互式编辑的管线。


<details>
  <summary>Details</summary>
Motivation: 现有3D流体重建主要依赖受控实验室环境，而野外视频中的流体（特别是烟雾）重建挑战巨大且未充分探索。

Method: 针对野外烟雾重建的三个关键挑战（背景去除的烟雾提取、烟雾粒子与相机姿态初始化、多视角视频推断），设计了专门技术，形成一套完整的提取、重建及交互式模拟管线。

Result: 方法在野外视频上实现了高质量烟雾重建，平均PSNR比现有方法提高2.22，并能对流体动力学进行多样化、逼真的编辑。

Conclusion: 成功解决了野外视频中动态3D烟雾的提取和重建难题，并提供了可交互模拟的烟雾资产，显著优于现有方法。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [97] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: SVR-GS通过引入空间变异正则化器，优化了3DGS高斯点剪枝，显著减少高斯数量同时保持高质量，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS掩膜剪枝方法（如MaskGS）的全局掩膜均值正则化与局部逐像素重建损失不匹配，导致无法有效识别和移除低重要性高斯点，剪枝效率不高。

Method: 提出SVR-GS，一种空间变异正则化器。它根据每个高斯点沿射线的有效贡献生成逐像素空间掩膜，对低重要性高斯点施加稀疏化压力。该方法探索了三种空间掩膜聚合策略，并通过梯度分析优化最终设计，用CUDA实现。

Result: SVR-GS在高斯数量上相较MaskGS平均减少1.79倍，相较3DGS平均减少5.63倍，而PSNR仅分别下降0.50 dB和0.40 dB。

Conclusion: SVR-GS显著减小了模型大小，提高了速度和内存效率，使其非常适合机器人、AR/VR和移动感知等实时应用。

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [98] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 一个轻量级学习框架，通过多视角2D图像预测珊瑚的3D体积和表面积，用于珊瑚生长量化和礁石监测。


<details>
  <summary>Details</summary>
Motivation: 珊瑚形态复杂，导致准确量化其体积和表面积以有效监测珊瑚生长极具挑战性。

Method: 提出一个学习框架：首先使用预训练模块（VGGT）从多视角RGB图像中提取密集点图；然后将点图合并为统一的点云并加入置信度；最后将点云输入两个并行的DGCNN解码器，联合输出珊瑚的体积、表面积及其置信度，并引入基于高斯负对数似然的复合损失函数以提高预测稳定性和提供不确定性估计。

Result: 该方法实现了具有竞争力的准确性，并能很好地泛化到未见的珊瑚形态。

Conclusion: 该框架为直接从稀疏图像集高效、可扩展地进行珊瑚几何估计铺平了道路，在珊瑚生长分析和礁石监测中具有潜在应用价值。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [99] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 本文提出了Traffic-MLLM，一个基于Qwen2.5-VL的多模态大语言模型，通过LoRA微调和结合CoT与RAG的知识提示模块，显著提升了复杂交通场景中的时空因果分析和领域知识集成能力，并在TrafficQA和DriveQA基准测试中达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有交通视频理解方法在准确建模时空因果关系和集成领域特定知识方面面临挑战，限制了其在复杂交通场景中的有效性。

Method: 研究者提出了Traffic-MLLM，一个为细粒度交通分析量身定制的多模态大语言模型。它基于Qwen2.5-VL骨干，利用高质量交通专用多模态数据集并通过LoRA进行轻量级微调，以增强对视频序列中连续时空特征的建模能力。此外，模型引入了一个创新的知识提示模块，融合了思维链（CoT）推理与检索增强生成（RAG），以精确注入详细的交通法规和领域知识，从而提升模型的逻辑推理和知识适应能力。

Result: 实验结果表明，Traffic-MLLM在TrafficQA和DriveQA基准测试中取得了最先进（SOTA）的性能，验证了其处理多模态交通数据的卓越能力。该模型还展现出显著的零样本推理和跨场景泛化能力。

Conclusion: Traffic-MLLM通过其创新的架构和知识注入机制，成功解决了现有方法在时空因果建模和领域知识集成上的局限性，为智能交通系统提供了更强大的场景感知和因果分析能力。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [100] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 本文提出Multispectral-NeRF，一个基于NeRF的增强型神经网络架构，通过扩展隐藏层、重新设计残差函数和调整数据压缩模块，有效整合多光谱信息，实现了高精度、光谱特性保持良好的三维重建，解决了现有NeRF模型无法处理多波段数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多光谱三维重建方法存在成本高、精度低和几何特征差的问题。虽然基于NeRF的方法能提供高精度和高质量的重建，但现有NeRF及其改进模型（如NeRFacto）仅限于处理三波段（RGB）数据，无法有效利用和整合更多的多波段光谱信息。

Method: 本文提出Multispectral-NeRF，一个从NeRF衍生的增强型神经网络架构，旨在有效整合多光谱信息。技术贡献包括三个方面：1) 扩展隐藏层维度以适应6波段光谱输入；2) 重新设计残差函数以优化重建图像与参考图像之间的光谱差异计算；3) 调整数据压缩模块以满足多光谱图像增加的位深度要求。

Result: 实验结果证实，Multispectral-NeRF成功处理了多波段光谱特征，同时准确地保留了原始场景的光谱特性。

Conclusion: Multispectral-NeRF有效整合了多光谱信息进行三维重建，克服了传统NeRF模型在处理多波段数据方面的局限性，能够产生高精度且光谱特性准确的重建结果。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [101] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出SPHERE，一个结合体素和高斯表示的相机端3D语义场景补全方法，通过语义引导初始化和物理感知谐波增强，克服现有方法的局限，实现高效、高语义精度和真实几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有体素/平面SSC方法难以捕捉物理规律以获得真实几何细节；而NeRF/3DGS等神经重建方法在处理大规模自动驾驶场景时，计算成本高、收敛慢，且语义精度不足。

Method: 提出Semantic-PHysical Engaged REpresentation (SPHERE)，整合体素和高斯表示。包括：1) Semantic-guided Gaussian Initialization (SGI) 模块，利用双分支3D表示以焦点体素引导高效高斯初始化。2) Physical-aware Harmonics Enhancement (PHE) 模块，引入语义球谐函数建模物理感知细节，并通过焦点分布对齐促进语义-几何一致性。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准上进行广泛实验和分析，验证了SPHERE的有效性，能生成具有真实细节的SSC结果。

Conclusion: SPHERE通过创新性地结合语义和物理信息，有效解决了相机端3D SSC中现有方法在几何细节和语义精度方面的不足，为自动驾驶提供更全面的场景感知。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [102] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 本文提出StegOT，一个基于自编码器并融合最优传输理论的图像隐写模型，通过MCOT模块解决模式坍塌问题，平衡封面和秘密图像信息，并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN和VAE的隐写模型普遍存在模式坍塌问题，导致隐写图像中封面和秘密图像信息失衡，进而影响后续的提取效果。

Method: 提出StegOT模型，该模型基于自编码器并结合最优传输理论。核心是设计了多通道最优传输（MCOT）模块，用于将具有多峰特征分布转换为单峰分布，以实现信息权衡。

Result: 实验证明，StegOT不仅实现了封面图像和秘密图像之间的信息权衡，还显著提升了隐写图像和恢复图像的质量。

Conclusion: StegOT模型成功解决了现有隐写模型中的模式坍塌挑战，并在信息权衡和图像质量方面取得了显著改进。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [103] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 研究发现Fitzpatrick肤色（FST）量表的粒度对AI皮肤病分类模型的性能和偏见有显著影响，并建议在公平AI研究中放弃FST量表，转向更具包容性的肤色表示方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI皮肤病分类模型存在肤色偏见，且常用FST量表对浅肤色粒度更高。本文旨在探究FST量表粒度对AI模型性能和偏见的影响。

Method: 通过训练多个AI模型，使用不同粒度的FST特定数据（如FST 1/2, 3/4, 5/6三组）进行良性/恶性病变分类，并与FST平衡数据集训练的通用模型进行比较，同时评估减少FST粒度（如合并1/2和3/4）的影响。

Result: ['使用基于三个FST分组（1/2, 3/4, 5/6）的特定FST数据训练的模型，性能普遍优于使用FST平衡数据训练的通用模型。', '减少FST量表信息的粒度（例如，从1/2和3/4减少到1/2/3/4）可能对模型性能产生不利影响。']

Conclusion: FST分组的粒度对训练病变分类模型至关重要。考虑到FST量表类别选择可能存在人为偏见，本研究结果支持在公平AI研究中放弃FST量表，转而采用更能代表人类肤色多样性的替代量表。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [104] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 本文提出了一种利用合成数据预训练3D森林激光扫描树木分割模型的方法，显著减少了对真实标注数据的需求，并实现了与全量真实数据训练模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的树木分割对于提取森林指标和理解生态系统功能至关重要，AI技术虽有进步，但现有公共3D森林数据集不足以构建鲁棒的分割系统。受自动驾驶等领域合成数据成功的启发，研究人员旨在探索合成数据是否能帮助解决树木分割问题。

Method: 开发了一个新的合成数据生成管线，结合了游戏引擎和基于物理的LiDAR模拟，用于森林视觉任务。该方法使用合成数据进行预训练，然后仅需少量真实森林地块标注数据进行微调。

Result: 生成了一个规模空前、全面多样的带标注3D森林数据集。实验表明，合成数据能大幅减少对真实标注数据的需求。仅用小于0.1公顷的单个真实森林地块进行微调后，预训练模型的分割性能可与使用全部真实数据训练的模型相媲美。同时，识别出成功使用合成数据的关键因素：物理特性、多样性和规模。

Conclusion: 合成数据是解决3D树木分割领域缺乏真实标注数据问题的有效途径，本文提出的数据生成管线和数据集为未来更鲁棒的3D森林视觉系统奠定了基础。物理特性、多样性和规模是合成数据成功的关键因素。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [105] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出“Beyond Sliders”框架，整合GANs和扩散模型，通过文本和视觉的对抗性细粒度指导，显著提升真实世界图像的生成质量和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法（如概念滑块）在处理非AIGC（尤其是真实世界）图像时，在真实感和定制化方面表现不足。

Method: 引入“Beyond Sliders”框架，该框架整合了生成对抗网络（GANs）和扩散模型，并通过对抗性的文本和视觉细粒度指导来精炼图像，以实现复杂的图像操作。

Result: 研究结果显示，该方法显著增强了图像质量和真实感，并通过广泛实验验证了其在多种应用中的鲁棒性和多功能性。

Conclusion: Beyond Sliders有效弥补了现有方法在真实世界图像处理上的不足，提供了卓越的图像质量和真实感，具有广泛的应用前景。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [106] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 本文提出一种概率性的、分量分解的空间变换网络（STNs）扩展，通过将仿射变换分解为旋转、缩放和剪切，并建模其不确定性，以提高细粒度视觉分类（FGVC）对几何变化的鲁棒性，并在蛾类分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类（FGVC）对几何变化（如任意方向、尺度和透视畸变）高度敏感。虽然等变架构能解决此问题，但通常需要大量计算资源并限制了假设空间。本文旨在寻找一种更灵活、高效的方法来处理几何变异。

Method: 本文重新审视了空间变换网络（STNs），并提出一种概率性的、分量分解的扩展。具体方法包括：将仿射变换分解为旋转、缩放和剪切三个分量；使用共享的定位编码器在几何约束下回归每个分量；通过高斯变分后验建模每个分量以捕捉不确定性，并在推理时进行基于采样的规范化；引入一种新颖的分量级对齐损失，利用增强参数指导空间对齐。

Result: 在具有挑战性的蛾类分类基准测试中，本文提出的方法与其他STNs相比，始终能提高分类的鲁棒性。

Conclusion: 通过对STNs进行概率性的、分量分解的扩展，本文成功地解决了细粒度视觉分类中几何变异的挑战，有效提高了模型鲁棒性，且避免了传统等变架构的计算开销和假设空间限制。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [107] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 本研究提出一种基于协同模型无关元学习（CCoMAML）与多头注意力特征融合（MHAFF）的少样本学习框架，用于实时牛只面部识别。该框架克服了传统深度学习在数据稀缺和模型重训练上的挑战，实现了98.46%和97.91%的F1分数，优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 牛只识别对畜牧管理至关重要，但当前RFID耳标系统存在丢失、损坏、篡改和安全漏洞等问题。尽管基于牛鼻纹生物特征的深度学习方法前景广阔，但其面临数据有限、数据收集中断以及动态牛群需频繁重训练的挑战。

Method: 为解决现有深度学习的局限性，本文提出一种新颖的少样本学习框架，用于实时牛只识别。该框架采用协同模型无关元学习（CCoMAML）并结合多头注意力特征融合（MHAFF）作为特征提取器模型。该模型能够从少量数据样本中高效学习，实现对新数据的良好适应性，无需重新训练。

Result: 通过与当前先进的牛只识别少样本学习技术进行严格评估，实验结果表明，所提出的CCoMAML结合MHAFF模型表现出卓越的牛只识别性能，F1分数分别达到98.46%和97.91%。

Conclusion: 该研究提出的基于CCoMAML与MHAFF的少样本学习框架，成功解决了牛只鼻纹生物识别中数据稀缺和频繁模型重训练的难题，提供了一种高效、鲁棒且高准确率的实时牛只识别解决方案。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [108] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [109] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: 本文提出MIS-LSTM，一种结合CNN编码器和LSTM序列模型的混合框架，用于从多模态生命日志数据预测日级别的睡眠质量和压力，并通过不确定性感知集成UALRE进一步提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从多模态生命日志数据（包括连续传感器流和稀疏离散事件）中准确预测日常睡眠质量和压力，是一项具有挑战性的任务。

Method: 该方法将连续传感器数据分块并渲染为多通道图像，通过CNN编码；将稀疏离散事件用专用1D-CNN编码。使用卷积块注意力模块融合两种模态为块嵌入，再由LSTM聚合以捕获长期时间依赖。为提高鲁棒性，引入UALRE不确定性感知集成方法，用高置信度个体预测覆盖低置信度多数投票。

Result: 在2025 ETRI Lifelog Challenge数据集上，MIS-LSTM基础模型 achieves Macro-F1 0.615；结合UALRE集成后，得分提高到0.647，优于现有的LSTM、1D-CNN和CNN基线。消融实验证实了多通道成像的优越性、4小时块粒度的益处以及模态特定离散编码的有效性。

Conclusion: MIS-LSTM框架，特别是结合UALRE不确定性感知集成，能有效且鲁棒地从多模态生命日志数据预测睡眠质量和压力，且其关键架构选择已得到验证。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [110] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: 本文提出CMLReID，一个基于CLIP的框架，通过情境感知语义提示(CASP)和自适应知识融合与投影(AKFP)来处理同时包含着装变化和持续学习的行人重识别(LReID-Hybrid)任务，实现了最先进的性能、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界行人重识别(ReID)面临着装变化(CCReID)和持续学习(LReID)的挑战。现有方法多专注于同衣识别(SC)或将CCReID视为独立问题，无法在持续学习环境下同时处理SC和CCReID。任务间的表示不匹配和遗忘是显著问题。本研究旨在开发一个模型，在持续学习设置下同时实现SC和CCReID，即解决LReID-Hybrid任务。

Method: 提出CMLReID，一个基于CLIP的框架，包含两个新颖任务：1. 情境感知语义提示(CASP)：生成自适应提示并结合上下文，将多粒度视觉线索与语义文本空间对齐。2. 自适应知识融合与投影(AKFP)：通过双路径学习器生成鲁棒的SC/CC原型，并使用“服装状态感知投影损失”对齐特征。

Result: 在广泛数据集上的实验表明，CMLReID的性能超越了所有现有最先进的方法，在着装变化和复杂的序列学习过程中展现出强大的鲁棒性和泛化能力。

Conclusion: CMLReID成功解决了LReID-Hybrid任务，通过创新的解决方案应对着装变化和持续学习的挑战，取得了优于现有方法的卓越性能、鲁棒性和泛化能力。

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [111] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: 为解决增量式无监督域适应(CI-UDA)中灾难性遗忘和内存增加问题，本文提出利用CLIP提取领域不变、类别无关的“属性”进行跨域对齐，实现了无回放的增量学习，并优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CI-UDA方法通常依赖存储回放样本（导致内存持续增加），且仅对跨域共享类进行对齐，易导致灾难性遗忘。因此，需要一种更高效且无回放的解决方案。

Method: 本文提出挖掘和保护领域不变、类别无关的知识。具体而言，利用CLIP提取类无关的“属性”，将其表示为视觉原型（键）和文本提示（值）的“键值”对。维护源域和目标域的属性字典，并通过鼓励视觉注意力一致性和预测一致性，进行跨域属性对齐，以减轻域偏移和灾难性遗忘。

Result: 在三个CI-UDA基准测试上的实验结果表明，本文方法优于现有最先进的方法，并且有效缓解了灾难性遗忘问题。

Conclusion: 本文通过提出一种基于属性建模和跨域对齐的无回放框架，成功解决了CI-UDA中的灾难性遗忘和域偏移问题，达到了领先的性能。

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [112] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的评估框架，结合广义交叉验证和域迁移学习原理，以实现合成数据集质量的可泛化和可比较评估。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据集生成技术的快速发展，评估其质量成为关键研究焦点。然而，当前的合成数据集评估研究有限，缺乏普遍接受的标准框架。

Method: 该框架整合了广义交叉验证实验和域迁移学习原理。具体方法包括：使用合成数据集和多个真实世界基准（如KITTI、BDD100K）训练特定任务模型（如YOLOv5s），构建交叉性能矩阵；归一化后构建广义交叉验证（GCV）矩阵以量化域迁移能力；引入两个关键指标，分别评估模拟质量（与真实数据的相似性）和迁移质量（合成数据的多样性和覆盖范围）。

Result: 在Virtual KITTI上的实验验证表明，所提出的框架和指标在评估合成数据保真度方面是有效的。

Conclusion: 该可扩展、可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了一种原则性方法。

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [113] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: ROSGS是一个两阶段管线，利用2DGS和混合光照模型高效重建并重打光室外场景，实现高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 室外场景因无界、光照多变而难以分解。现有NeRF和3DGS方法存在计算开销大、光照表示低频等局限性，导致渲染效率低和重打光精度不足。

Method: ROSGS是一个两阶段管线。第一阶段，利用单目法线先验，通过紧凑的2D Gaussian Splatting (2DGS) 重建场景几何。第二阶段，基于重建的几何结构，通过混合光照模型分解场景纹理和光照：使用球形高斯函数捕获高频太阳光，并通过球谐系数学习辐射传输函数以建模低频天光。

Result: ROSGS在室外场景重打光方面达到了最先进的性能，并在定量和定性评估中均展现出卓越的重打光精度和渲染效率。

Conclusion: ROSGS通过结合高效的2DGS几何重建和新颖的混合光照模型，为室外场景的重打光提供了一个高效且高精度的解决方案，显著提升了重打光能力和渲染效率。

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文研究交通排放与气象条件的关系，利用模糊推理系统（FIS）建立排放预测模型，旨在为城市规划者提供交通管理与环境保护的参考。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染（特别是交通排放）是当今社会面临的重要问题，因此有必要深入分析气象条件对交通排放量及其扩散的影响，以期有效减少污染。

Method: 采用系统性方法分析交通排放与气象条件的关系，并基于捷克布拉格的交通、气象及排放数据，利用模糊推理系统（FIS）开发了一个预测排放变化的模型。

Result: 成功开发了一个基于模糊推理系统（FIS）的交通排放变化预测模型，该模型能够揭示气象条件对城市交通排放数量和扩散的影响。

Conclusion: 本研究旨在为城市规划者和政策制定者提供深入见解，以帮助他们在考虑环境保护的前提下，更有效地规划和管理城市交通。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [115] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 首次展示仅通过自由形式的自然语言提示，即可引导人工或生物群体行为，无需预设奖励或任务特定设计。


<details>
  <summary>Details</summary>
Motivation: 现有AI和生物系统难以理解和响应人类语言，导致控制受限。迫切需要开发一种无需特定工程奖励或监督，仅凭自由形式自然语言就能有效指导复杂、去中心化系统的方法。

Method: 该方法使用两个AI模型：一个模型将指令性语言提示转化为对模拟细胞的干预措施；另一个模型评估干预结果与原始提示的匹配程度。前者通过演化迭代，以优化后者生成的评估分数。

Result: 实验证明，该演化系统能够仅凭自由形式语言提示，有效引导模拟细胞的集体行为。系统具备泛化能力，能处理未经训练的新提示。

Conclusion: 该研究将自然语言作为控制层，为AI-生物合作愿景迈出了坚实一步，预示着未来可使用语言而非数学目标函数或特定编程来指导计算、机器人或生物系统。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [116] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演化图像生成系统，通过迭代演化提示词，使T2I模型能够从初始提示词开始自主改进生成图像。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）模型高度依赖人工干预，常需手动、迭代的提示词工程，尤其是在面对未充分指定的提示词时，这带来了显著的可用性挑战。

Method: Maestro系统包含两项关键创新：1) 自批判，其中专门的多模态大型语言模型（MLLM）代理充当“评论员”，识别图像弱点，纠正提示词欠明确之处，并提供可解释的编辑信号，由“验证员”代理整合，同时保留用户意图；2) 自演化，利用MLLM作为“评判者”对迭代生成的图像进行一对一比较，筛选掉有问题的图像，并演化出符合用户意图的创新提示词候选。

Result: 通过对黑盒模型进行复杂T2I任务的广泛实验表明，Maestro显著提高了图像质量，优于初始提示词和最先进的自动化方法，并且其有效性会随着MLLM组件的先进性而增强。

Conclusion: 这项工作为T2I生成实现自我改进提供了一条稳健、可解释且有效的途径。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [117] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 本研究分析了不同GPT模型评估AI生成内容时的“评估人格”和偏见。发现不同GPT模型评估策略和行为差异显著，GPT模型评估能力与通用能力不完全相关，且存在负面评估偏见。稳健的AI评估需要多样化的架构视角。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益评估其他AI的输出，理解其评估行为对于预防级联偏见至关重要。

Method: 研究分析了NVIDIA Describe Anything Model生成的视觉-语言描述，并由GPT-4o、GPT-4o-mini和GPT-5进行评估。使用Gemini 2.5 Pro作为独立的问卷生成器进行对照实验，并通过生成问题的语义相似性进行跨家族分析。

Result: GPT-4o-mini表现出系统性一致性，GPT-4o擅长错误检测，而GPT-5则极端保守且变异性高。这些“评估人格”是模型的内在属性。GPT模型在评估策略上高度相似，而Gemini则显著不同。所有GPT模型都表现出2:1的负面评估偏向，但这似乎是家族特有的模式。

Conclusion: 评估能力不随通用能力而提升，鲁棒的AI评估需要多样化的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [118] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: 研究引入GEO-16框架，通过审计AI答案引擎引用网页的质量信号，发现页面整体质量和特定支柱（如元数据、语义HTML）是AI引擎引用网页的强预测因子。


<details>
  <summary>Details</summary>
Motivation: 随着AI答案引擎日益成为获取领域知识的媒介，通过生成回应并引用网页来源，理解哪些网页会被这些引擎引用变得至关重要。

Method: 研究引入了GEO-16审计框架，该框架将页面质量信号转化为支柱得分和0到1的GEO分数G。使用70个产品意图提示，从Brave Summary、Google AI Overviews和Perplexity三个引擎收集了1,702条引用，并审计了1,100个独立URL。通过逻辑模型分析数据，并考虑了领域聚类标准误差。

Result: 不同引擎引用的页面GEO质量存在差异。元数据与新鲜度、语义HTML和结构化数据等支柱与引用表现出最强的关联。逻辑模型显示，整体页面质量是引用的强预测因子，且简单操作点（如G至少0.70结合至少12个支柱命中）与数据中显著更高的引用率一致。研究还报告了各引擎对比、垂直效应和阈值分析。

Conclusion: 整体页面质量，特别是元数据、语义HTML和结构化数据等特定方面，是AI答案引擎引用网页的关键因素。研究结果可转化为出版商的实用指南，以提高其内容被AI引擎引用的可能性。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [119] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 本研究通过企业级基准测试，评估了18种智能体配置在多智能体系统中的交互，发现存在显著的模型特定架构偏好，并揭示了现有智能体系统在企业任务上的性能局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立地探讨智能体架构组件，但对于复杂多智能体系统中不同设计维度如何交互的实证理解仍然有限。

Method: 本研究构建了一个全面的企业特定基准，评估了18种不同的智能体配置，涵盖了编排策略、智能体提示实现（ReAct与函数调用）、内存架构和思维工具集成这四个关键维度，并使用了最先进的大型语言模型。

Result: 基准测试揭示了显著的模型特定架构偏好，挑战了智能体AI系统中普遍存在的“一刀切”范式。同时，研究发现现有智能体系统在企业任务上的整体性能存在显著弱点，得分最高的模型在复杂任务上最高成功率仅为35.3%，在简单任务上为70.8%。

Conclusion: 这些发现为未来智能体系统的设计提供了信息，有助于在架构组件和模型选择方面做出更具实证支持的决策。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [120] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出一种基于优化人机对话和单调布尔/k值函数的技术，以发现可计算的个人专家心理模型（EMM），从而改进大型语言模型（LLM）的提示工程，使其更有效地处理复杂决策问题，尤其是在关键信息缺失的情况下。


<details>
  <summary>Details</summary>
Motivation: 决策问题普遍存在，LLM在决策支持方面引起兴趣，但存在幻觉、数据缺失等局限性。RAG等方法是部分解决方案，仍无法访问所有必要信息。设计有效提示也非易事，难以捕捉专家复杂的心理模型。对于关键信息缺失的任务，LLM和现有系统均不足。

Method: 本文提出一种基于优化人机对话和单调布尔与k值函数的技术，以发现一个可计算的个人专家心理模型（EMM）。其LLM提示工程EMM算法包含四个步骤：(1) 因素识别，(2) 因素的层级结构化，(3) 生成泛化专家心理模型规范，以及 (4) 根据该规范生成详细的泛化专家心理模型。

Result: 通过提出的方法，可以发现一个可计算的个人专家心理模型（EMM），旨在提升LLM在决策任务中的效率，尤其在处理复杂和信息缺失场景时，通过结构化的提示工程更好地模拟领域专家的思维模式。

Conclusion: 该研究探索了如何利用LLM更高效地进行决策，通过引入一种基于人机对话和数学函数的创新技术来构建专家心理模型，有望克服LLM在处理复杂决策和信息缺失方面的现有局限性，从而实现更精准的提示工程。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [121] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 本文提出LVSA，一种神经符号框架，通过可微分Skolemization模块、神经否定器和逻辑约束优化协议，解决了不完整知识图谱上复杂查询回答中逻辑完备性和计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在不完整知识图谱上进行复杂查询回答（CQA，通常形式化为EFO1）时，逻辑完备性和计算效率之间存在根本性权衡。基于Grounding的方法面临组合爆炸，而大多数基于Skolemization的方法未能明确建模Skolem函数并牺牲了逻辑一致性。

Method: 本文提出了逻辑约束向量符号架构（LVSA），这是一个神经符号框架。它统一了可微分Skolemization模块和神经否定器，并引入了逻辑约束驱动的优化协议，以协调几何和逻辑要求。

Result: 理论上，LVSA保证了所有EFO1查询的普适性。在实践中，它优于最先进的基于Skolemization的方法，并且与基于Grounding的基线相比，将推理成本降低了几个数量级。

Conclusion: LVSA通过提供一种统一的神经符号框架，有效地解决了不完整知识图谱上复杂查询回答中逻辑完备性和计算效率的权衡问题，实现了理论上的普适性和经验上的卓越性能。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [122] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本研究批判性地重新评估了AI中以“智能体”为中心的范式，认为其模糊性和人类中心偏见限制了AI发展，并提议转向非智能体和系统级框架以推进通用智能。


<details>
  <summary>Details</summary>
Motivation: 重新评估AI研究中根深蒂固的“智能体”概念范式，因为它可能因概念模糊性和固有人类中心偏见而成为限制性框架，特别是在理解LLM等现代AI系统时。

Method: 通过系统回顾相关文献，解构了各种AI框架中的智能体范式，并区分了智能体系统、自主系统和非智能体系统，以揭示定义和衡量自主性、目标导向性等属性的挑战。

Result: 智能体式（agentic）的AI系统框架，尽管在启发式上有所助益，但可能具有误导性，并掩盖了底层计算机制，尤其是在大型语言模型（LLM）中。定义和测量自主性及目标导向性等属性存在挑战。

Conclusion: 为实现鲁棒、可扩展且潜在非拟人化的通用智能，有必要探索受复杂系统、生物学和非常规计算启发的非智能体和系统级框架，这不仅需要新架构，更需要从根本上重新思考智能本身，超越智能体隐喻。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [123] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 本研究提出了一种名为HaPLa的黑盒越狱技术，通过溯因框架和符号编码，实现了对大型语言模型（LLM）的高效攻击，并揭示了LLM安全与实用性之间的固有权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然功能强大，但其被滥用于有害目的的潜力是一个主要担忧。为增强防御，有必要研究利用LLM内在弱点的通用越狱攻击。

Method: 提出一种名为HaPLa（Harmful Prompt Laundering）的黑盒越狱技术，包含两项主要策略：1) 溯因框架（abductive framing），引导LLM推断有害活动的中间步骤；2) 符号编码（symbolic encoding），用于模糊有害内容以规避LLM对显式关键词的敏感性。

Result: 实验结果显示，HaPLa在GPT系列模型上取得了超过95%的攻击成功率，在所有目标模型上平均达到70%的成功率。

Conclusion: 进一步分析揭示了一个根本性挑战：在不显著降低LLM对良性查询的有用性的前提下，安全地调整LLM仍然十分困难。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [124] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 针对大型语言模型（LLM）的上下文学习（ICL）中存在的隐私泄露问题，研究提出一种结合公共数据的差分隐私ICL算法，有效平衡了隐私保护和模型效用。


<details>
  <summary>Details</summary>
Motivation: LLM的ICL存在私有数据泄露风险，而传统的差分隐私（DP）方法会显著降低ICL的效用。因此，需要一种新的方法来在保持DP保证的同时，提高ICL的实用性。

Method: 将与任务相关的公共数据融入到ICL框架中，同时维持差分隐私（DP）保证，并在此基础上提出一种私有上下文学习算法。

Result: 实验证明，所提出的方法在公共数据的辅助下显著提升了私有ICL的效用。此外，该方法对成员推断攻击具有鲁棒性，展示了经验性的隐私保护。

Conclusion: 通过引入公共数据，该私有ICL算法能够有效平衡隐私保护和模型效用，同时对隐私攻击表现出鲁棒性。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [125] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 本文探讨了将具有强大计算能力的LLMs整合到心理学真实的认知架构中，以Clarion架构为例，利用其内隐-外显二分法，实现了LLMs计算能力与Clarion心理学优势的结合。


<details>
  <summary>Details</summary>
Motivation: 现有计算认知架构虽具心理学合理性，但计算能力有限；而LLMs计算能力强大。为同时应对真实世界复杂性和心理学真实性，将LLMs整合到认知架构中至关重要。

Method: 以Clarion认知架构为例，讨论了其与LLMs的协同结合。具体利用Clarion架构中固有的内隐-外显二分法，实现LLMs与Clarion的无缝集成。

Result: 通过这种整合，成功地将LLMs的计算能力与Clarion的心理学精细性（nicety）相结合。

Conclusion: 将LLMs融入认知架构，特别是利用Clarion的内隐-外显机制，能有效提升认知架构的计算能力，同时保持其心理学真实性，为处理复杂问题提供了一条有效途径。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [126] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 现有LLM生成理由的评估方法粗糙，本研究提出通过识别关键属性并结合自动化、LLM及人工评估，使用SHAP和ELO分数进行细粒度分析，以克服二元比较的局限性，提供更细致的模型洞察并指导未来评估实践。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM生成理由的评估（如二元偏好判断）不透明且粒度粗糙，难以深入理解理由优劣。本研究旨在重新思考评估方法，探究良好理由的定义属性、人类偏好与这些属性的关系，以及属性评估能否克服二元比较的局限性。

Method: 1. 从现有文献中识别关键理由属性。 2. 使用自动化指标、LLM判断和人工标注评估这些属性。 3. 利用SHAP分析MT Bench和Chatbot Arena等人类偏好数据集，以识别解释人类偏好结果的属性。 4. 使用特定属性的ELO分数重新评估模型生成的理由。

Result: 研究揭示了更细致的模型比较和洞察。结果表明，细粒度的属性评估能更好地表征理由质量。

Conclusion: 细粒度的属性评估能够更好地刻画理由质量，并为未来构建更可解释和可靠的评估实践指明方向。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [127] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: 本文提出Free-MAD框架，通过引入计分制决策机制和反从众机制，消除多智能体辩论(MAD)中达成共识的需求。Free-MAD仅需单轮辩论，显著提升了大型语言模型(LLMs)的推理性能，降低了token成本，并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法依赖多轮交互和多数投票达成共识，导致token成本高、LLMs的从众性引起错误传播，以及多数投票带来决策随机性和不公平性，进而限制了可扩展性和推理性能。

Method: 提出Free-MAD，一个新型MAD框架，取消智能体间共识需求。它引入基于分数的决策机制，评估整个辩论轨迹而非仅最后一轮，从而实现更准确公平的结果。此外，通过引入反从众机制重构辩论阶段，帮助智能体缓解多数影响。Free-MAD仅需单轮辩论。

Result: 在八个基准数据集上的实验表明，Free-MAD显著提高了推理性能，同时仅需单轮辩论，从而降低了token成本。与现有MAD方法相比，Free-MAD在真实攻击场景中表现出更高的鲁棒性。

Conclusion: Free-MAD通过创新的单轮辩论、基于分数的决策机制和反从众机制，有效解决了现有MAD方法在高成本、错误传播和决策公平性方面的局限，实现了更高的推理性能、更低的成本和更强的鲁棒性。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [128] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic是一个基于有限状态机（FSM）的多智能体系统，通过动态编排和质量控制，显著提升了桌面自动化在复杂多步任务中的可靠性和成功率，在OSWorld基准测试中达到了最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 现有用于桌面自动化的自主智能体在处理复杂多步任务时，由于协调性差和质量控制不足而表现不佳。

Method: 引入了Agentic Lybic，一个将整个架构作为有限状态机（FSM）运行的多智能体系统。该系统包含一个控制器、一个管理器、三个工作器（用于代码操作的Technician、用于GUI交互的Operator、用于决策支持的Analyst）和一个评估器。核心机制是基于FSM的组件间路由，实现动态执行策略选择、自适应重规划和错误恢复，并结合了强大的质量门控。

Result: 在OSWorld基准测试中，Agentic Lybic在50步内达到了57.07%的成功率，显著优于现有方法，达到最先进水平。

Conclusion: 有原则的多智能体编排与持续的质量控制相结合，能为复杂计算环境中的通用桌面自动化提供卓越的可靠性。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [129] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本文提出一个验证框架，用于解决多智能体LLM系统中计算信任问题，通过非对称工作量验证LLM输出的真实性，比完全重新生成快12倍以上。


<details>
  <summary>Details</summary>
Motivation: 随着LLM向动态多智能体系统发展，出现了一个根本性挑战：如何建立计算信任，即一个智能体如何验证另一个智能体的输出确实来自声称的LLM，而非伪造或由劣质模型生成。

Method: 该方法基于自回归模型固有的“确定性可复制性”原理，要求所有智能体在相同的硬件和软件环境下运行。框架允许多个验证器概率性地审计LLM输出的随机小片段，并有效分配验证工作量，以实现可追踪的非对称工作量（验证成本远低于执行成本）。

Result: 仿真结果表明，目标验证速度比完全重新生成快12倍以上，且具有可调参数以调整检测概率。

Conclusion: 该工作为可审计的LLM系统建立了一个可行的机制，为负责任的AI奠定了基础，并为未来更复杂、异构的多智能体系统研究提供了基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [130] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Patient-Zero是一种无需真实病历的框架，利用多步生成和动态更新机制，生成高准确性、多样性、一致性的合成患者数据，有效提升现有模型在医学问答上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的合成数据生成方法在医疗领域仍面临数据隐私、准确性、多样性不足的挑战，且缺乏像真实患者一样的交互能力，并通常依赖于重写或补全现有记录。

Method: 提出Patient-Zero框架，无需真实医疗记录。它首先引入医学对齐的多步生成架构，通过分层医学知识注入构建全面的患者记录；其次，设计了动态更新机制以优化虚拟患者与人类的交互能力，提高一致性和对话性能。框架还支持自适应对话策略和实时临床合理性验证，以确保上下文多样性和医疗连贯性。

Result: 实验结果表明，该模型在准确性、多样性和一致性方面表现良好。使用Patient-Zero生成的虚拟患者进行训练后，现有模型在MedQA数据集上取得了显著改进。

Conclusion: Patient-Zero框架成功解决了现有合成医疗数据生成中的隐私、准确性、多样性和交互性问题，能够在不依赖真实病历的情况下生成高质量、高交互性的虚拟患者数据，并有效提升了下游医学模型的性能。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [131] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态多智能体编排框架，它能根据查询难度自适应调整工作流深度和LLM分配，从而在准确性和推理效率上超越现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖静态或任务级工作流，导致对简单查询过度处理、对复杂查询表现不佳，且未能兼顾异构LLM的效率与性能权衡。

Method: 提出“难度感知智能体编排”（DAAO）框架。DAAO根据输入查询的难度，动态调整工作流深度、操作符选择和LLM分配。它包含三个相互依赖的模块：用于难度估计的变分自编码器（VAE）、模块化操作符分配器以及成本与性能感知的LLM路由器。

Result: DAAO在六个基准测试中，在准确性和推理效率方面均优于现有多智能体系统。

Conclusion: DAAO通过利用异构LLM和动态定制工作流，实现了细粒度、查询特定的推理策略，显著提升了多智能体系统的性能和效率。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [132] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 神经细胞自动机 (NCA) 是一种强大的生物自组织建模框架，通过可训练规则模拟多尺度生物过程。本文综述了NCA在生物学和生物工程中的应用，并强调其在泛化、鲁棒性、与现代生成式AI的联系以及构建生物启发式集体智能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 鉴于神经细胞自动机 (NCA) 在生物自组织建模领域取得的巨大成功，本文旨在综述其在生物学和生物工程应用中的最新文献，并阐明其超越生物学的应用前景（如机器人控制、高级推理任务）以及与现代生成式AI的潜在联系，以探讨其作为统一计算范式的潜力。

Method: 本文采用文献综述和概念分析的方法，审查了与生物学和生物工程应用相关的神经细胞自动机 (NCA) 的现有文献，并强调其在多尺度自组织、适应性、泛化能力方面的特性及其与现代生成式AI的潜在联系。

Result: NCA能够模拟分子、细胞、组织和系统层面的生物过程，再现生物启发的目标模式，并对扰动表现出鲁棒性，具备开放式适应和推理能力。它们在无中心控制下实现目标导向动态，例如在机器人形态控制和高级推理任务中。NCA的迭代状态细化与现代生成式AI（如扩散模型）相似，其局部交互能扩展为协调的系统级结果。

Conclusion: 神经细胞自动机 (NCA) 构成了一个统一且计算轻量级的范式，它不仅连接了多尺度生物学与现代生成式AI的基本见解，而且有潜力设计出真正生物启发式的、能够进行层级推理和控制的集体智能。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [133] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT是一种新型知识追踪模型，通过前端到后端的架构和对比学习，显式建模稳定的知识状态，提高了可解释性并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪（KT）模型主要关注学习者交互序列拟合，忽视了知识状态本身，导致可解释性差，且对智能辅导系统（ITS）的教学支持不足。

Method: 提出AlignKT模型，采用前端到后端的架构显式建模稳定的知识状态。通过将初步知识状态与基于教学理论的理想知识状态进行对齐，并利用五个编码器和一个对比学习模块增强对齐过程的鲁棒性。

Result: AlignKT在三个真实世界数据集上表现出卓越性能，超越七个KT基线模型，其中在两个数据集上达到最先进水平，在第三个数据集上表现出竞争力。

Conclusion: AlignKT通过显式建模稳定的、可解释的知识状态，有效解决了现有模型的局限性，显著提升了知识追踪的性能和智能辅导系统的教学支持能力。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [134] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: AIGC应用广泛但跨领域研究不足。本文汇集多学科专家，系统综述AIGC的技术、社会影响及挑战，并展望未来研究方向，提供跨领域视角。


<details>
  <summary>Details</summary>
Motivation: 现有研究鲜有深入探讨AIGC在不同领域的最新进展和新兴挑战，存在研究空白。

Method: 本文汇集16位多学科专家，从跨领域视角，全面概述AIGC的训练技术、检测方法、内容传播与使用；探讨其在不同领域的社会影响及现有方法；讨论关键技术挑战并提出研究命题以指导未来工作。

Result: 提供了一个关于AIGC的跨领域视角，深入洞察其当前研究趋势、面临的持续挑战以及未来的发展方向。

Conclusion: 本论文旨在通过其贡献，为读者提供AIGC的跨领域见解，揭示其当前研究趋势、持续挑战和未来方向，以期指导未来的研究工作。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [135] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: VideoAgent是一个多智能体框架，通过对话界面自动化生成个性化科研视频，并引入SciVidEval进行全面评估。


<details>
  <summary>Details</summary>
Motivation: 自动化生成科研视频对知识传播至关重要，但现有文档自动化工具主要关注静态媒体，缺乏个性化动态编排和多模态内容同步机制。

Method: 引入VideoAgent多智能体框架，它解析论文以构建细粒度资产库，根据用户需求通过对话界面编排叙事流（静态幻灯片和动态动画）。同时提出SciVidEval综合评估套件，结合自动化多模态内容质量和同步指标与基于视频问答的人工评估来衡量知识转移。

Result: 实验证明，该方法显著优于现有商业科研视频生成服务，并在科研交流中接近人类水平质量。

Conclusion: VideoAgent通过生成个性化、高质量的科研视频，有效解决了知识传播的挑战。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [136] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 本文提出一个新颖的对齐框架，将大型语言模型（LLMs）作为人类调查受访者的代理，以经济有效且可控的方式解决社会科学中调查成本上升和人口统计学不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究面临调查部署成本高昂和调查响应数据中人口统计学失衡日益严重的两大挑战。

Method: 受显性偏好理论启发，提出两阶段对齐问题：构建多样化的“禀赋”代理角色模拟受访者，并选择一个代表性子集以近似真实人口。引入P2P系统，通过结构化提示工程、基于熵的采样和基于回归的选择，引导LLM代理生成代表性行为模式。该方法与人口统计学无关，仅依赖于聚合调查结果。

Result: 在真实世界意见调查数据集上，对齐后的代理群体能够高保真地重现聚合响应模式，并表现出显著的响应多样性，即使没有人口统计学条件限制。

Conclusion: 该框架提高了社会科学研究的数据效率，并为研究多元对齐的操作化提供了一个试验平台。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [137] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 利用大型语言模型从科学摘要中提取污染物到健康影响的关系元路径，构建毒性轨迹图，并处理证据冲突。


<details>
  <summary>Details</summary>
Motivation: 塑料的广泛使用导致微纳塑料在环境（空气、水、土壤）中积累，对人类健康（呼吸、消化、神经系统疾病）构成严重风险。

Method: 提出一个新颖框架，利用大型语言模型从科学摘要中提取连接污染物来源到健康影响的关系元路径。该系统识别并连接实体，构建结构化的关系元路径，并聚合成毒性轨迹图。同时，引入动态证据协调模块以解决研究发现中的语义冲突。

Result: 该方法在从嘈杂的科学文本中提取可靠、高价值的关系知识方面表现出强大的性能。

Conclusion: 提供了一种可扩展的解决方案，用于挖掘领域特定语料库中复杂的因果结构。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [138] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 提出一种基于动态因果分析和液体时间常数（LTC）网络的新型框架，用于优化基于观测器的软传感器，通过系统地识别并修剪对状态估计因果影响最小的传感器输入。


<details>
  <summary>Details</summary>
Motivation: 传统的传感器选择方法依赖线性可观测性指标或统计相关性，未能有效捕捉复杂系统的时间演化，因此需要一种能处理时间动态的传感器选择方法。

Method: 利用具有输入依赖时间常数的连续时间神经网络架构（LTC网络），实施迭代工作流：在候选输入上训练LTC观测器，通过受控扰动分析量化每个输入的因果影响，移除影响可忽略的输入，然后重新训练，直到性能下降。在弹簧-质量-阻尼系统、非线性连续搅拌釜反应器和捕食者-猎物模型三个机械测试平台上进行了验证。

Result: 研究结果表明，因果引导的修剪方法始终能识别出符合底层物理原理的最小传感器集，并显著提高预测准确性。该框架还能自动区分基本物理测量与噪声，并确定派生交互项是提供互补信息还是冗余信息。

Conclusion: 该方法不仅提高了计算效率，而且通过将传感器选择决策建立在动态因果关系而非静态相关性之上，增强了可解释性，为过程工程、生态监测和农业等领域的软传感应用提供了显著优势。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [139] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 提出MAPGD，一种结合多智能体协作与梯度优化的提示工程框架，旨在克服现有方法的局限，实现更高效、鲁棒和可解释的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法多依赖单一优化路径，导致适应性、效率受限，并存在视角狭窄、梯度冲突及高计算成本等问题。

Method: 提出了MAPGD（多智能体提示梯度下降）框架，它整合了多智能体协作与梯度优化。MAPGD包含：用于任务清晰、示例选择、格式设计和风格优化的专业智能体；解决冲突的语义梯度协调；基于强盗算法的候选选择实现高效探索-利用；以及理论收敛性保证。

Result: 实验结果表明，在分类、生成和推理任务中，MAPGD在准确性和效率方面均优于单智能体及随机基线。消融实验也证实了梯度融合、智能体特化和冲突解决的有效性。

Conclusion: MAPGD提供了一种统一的、受梯度启发的全新多智能体方法，实现了鲁棒且可解释的提示优化。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [140] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 人工智能代理（AI agents）虽拓展了LLMs的能力，但面临安全威胁（如提示注入）。本文提出将基于角色访问控制（RBAC）集成到AI agents中，以提供安全保障，支持其有效、可扩展的本地部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）受限于静态数据且需微调。AI agents虽通过访问外部工具和实时数据克服了部分限制，但易受提示注入等安全攻击，对其完整性和可靠性构成重大风险，尤其在工业应用中。

Method: 提出一个将基于角色访问控制（RBAC）集成到人工智能代理（AI agents）中的框架。

Result: 该框架旨在为AI agents提供强大的安全防护措施，以支持其高效、可扩展的部署，尤其是在本地部署场景中。

Conclusion: 将RBAC集成到AI agents中，可以作为强大的安全防护措施，有效应对其安全漏洞（如提示注入），从而确保AI agents在包括工业环境在内的各种场景中实现安全、有效和可扩展的本地部署。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [141] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 本文提出一种自适应专家混合（MoE）模型，用于解决多源异构数据集成挑战下的降水速率预测问题，并开发了交互式可视化工具，该模型在基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确降水预测对农业、灾害管理和可持续发展至关重要，但气候系统复杂性和雷达、卫星、地面测量等多源观测数据的异构性（空间、时间分辨率及领域特定特征差异）使得传统深度学习模型难以有效集成，现有机器学习技术也普遍难以处理异构模态数据的集成问题。

Method: 提出一种自适应专家混合（MoE）模型，专为降水速率预测设计。模型中每个专家专注于特定模态或时空模式，并引入动态路由器，学习将输入分配给最相关的专家。此外，还开发了一个交互式网络可视化工具，用于探索历史天气模式。

Result: 模块化设计显著提高了预测准确性和可解释性。通过使用包含2022年飓风伊恩真实条件的多模态气候数据集进行评估，自适应MoE模型显著优于所有基线方法。

Conclusion: 自适应专家混合模型有效解决了降水预测中多源异构数据的集成挑战，显著提升了预测精度，并提供了支持决策的可视化工具，有助于气候敏感部门的利益相关者进行决策。

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [142] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 本文评估了VLA模型在边缘和数据中心GPU上的性能、功耗和架构影响，发现其性能扩展趋势，并挑战了数据中心硬件在机器人推理方面固有优势的假设。


<details>
  <summary>Details</summary>
Motivation: 尽管Vision-Language-Action (VLA) 模型已成为强大的通用机器人控制策略，但其在不同模型架构、硬件平台和相关功耗预算下的性能扩展规律仍不明确。

Method: 该研究评估了五种代表性VLA模型（包括SOTA基线和两种新架构），目标平台为边缘和数据中心GPU。使用LIBERO基准测试，在不同边缘功耗限制和高性能数据中心GPU配置下，测量了准确性以及系统级指标（如延迟、吞吐量和峰值内存使用）。

Result: (1) 架构选择（如动作分词和模型主干大小）显著影响吞吐量和内存占用；(2) 功耗受限的边缘设备表现出非线性性能下降，但某些配置可匹敌或超越旧的数据中心GPU；(3) 可以在不显著降低准确性的情况下实现高吞吐量变体。

Conclusion: 研究结果为在各种部署限制下选择和优化VLA模型提供了实用见解，并挑战了数据中心硬件在机器人推理方面固有优势的当前假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [143] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: 本文提出MedicalOS，一个基于大语言模型代理的统一操作系统，旨在作为医疗领域特定抽象层，将自然语言指令转化为可执行的数字医疗命令，以自动化临床工作流并确保安全合规。


<details>
  <summary>Details</summary>
Motivation: 现有数字健康系统难以学习和使用，给临床医生带来管理多工具、重复操作和导航复杂界面的负担，导致行政时间过长。虽然大型语言模型（LLM）代理展现出通过自然语言与软件交互的潜力，但在数字医疗领域，需要一个严格遵循临床指南和标准的领域特定抽象层，以确保安全性、透明度和合规性。

Method: 研究者开发了MedicalOS，一个统一的基于代理的操作系统，作为医疗领域的领域特定抽象层。它将人类指令转换为预定义的数字医疗命令（如患者查询、病史检索、检查管理、报告生成、转诊、治疗计划），这些命令被封装为可直接使用的工具（使用Python、API、MCP、Linux等机器语言）。

Result: MedicalOS在22个专科的214个患者案例中进行了实证验证，结果显示出高诊断准确性和置信度、临床上合理的检查请求，以及结构化报告和药物推荐的一致性生成。

Conclusion: 这些结果表明MedicalOS是推动临床实践工作流自动化的一个值得信赖且可扩展的基础。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [144] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 本研究通过生成合成眼动数据并结合真实数据进行任务解码，显著提升了分类准确率，支持了从眼动数据中解码观察者任务的假设。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习算法在从眼动数据解码任务方面表现不一，对Yarbus关于从眼动判断任务的假设存在争议。本研究旨在通过数据增强支持这一假设。

Method: 使用CTGAN、CopulaGAN和Gretel AI等合成数据生成器，从真人用户研究的现有数据中生成合成眼动数据样本，并结合真实数据进行任务类别解码。

Result: 数据增强显著提高了分类准确率，即使是传统机器学习算法也受益。例如，在增加五倍数据后，任务解码准确率从随机森林的28.1%提升至Inception Time的82%。本框架因使用额外合成数据集而优于该数据集上的所有现有研究。

Conclusion: 通过各种算法和真实与合成数据的组合验证，随着生成数据对真实数据的增强，任务解码准确率显著提高，支持了Yarbus的假设。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [145] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: 本文提出MCFR，一个结合LLM和模型检测的神经符号框架，用于解决封闭领域QA系统中LLM推理不忠实的问题，并在真实学术程序数据集上验证了其在推理忠实性和可解释性方面的提升。


<details>
  <summary>Details</summary>
Motivation: 封闭领域QA系统中的推理需要高准确性和策略符合性，但LLM的推理痕迹常不忠实。现有结合LLM与符号引擎的方法限于静态逻辑，难以处理动态、基于状态的推理（如多步进展和条件转换）。

Method: 提出MCFR（Model Checking for Formal Reasoning）神经符号框架，将LLM与模型检测集成以支持属性验证。MCFR将自然语言转换为形式化规范，并在转换模型上进行验证。引入EduMC-QA基准数据集进行评估，并与ChatGPT、DeepSeek、Claude等SOTA LLM进行性能比较。

Result: MCFR显著提高了推理的忠实性和可解释性。

Conclusion: MCFR为高风险封闭领域应用中的可验证QA提供了一条可行的路径。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [146] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 本综述定义了时间序列推理问题，根据推理拓扑（直接、线性链、分支结构）和主要目标对现有文献进行分类和组织，回顾了方法、系统、数据集和评估实践，并提出了未来研究方向，旨在实现可追溯、可靠的推理系统。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在系统地定义时间序列推理，组织现有文献，回顾不同领域的方法和系统，识别各种拓扑的优缺点，提出有效的评估实践和指导，最终推动该领域从狭窄的准确性转向大规模的可靠性，以构建能够理解、解释并基于动态世界行动的系统。

Method: 该研究通过以下方式进行：1) 将时间序列推理定义为以时间为核心轴并整合中间证据；2) 根据推理拓扑（直接、线性链、分支结构）和主要目标（传统分析、解释理解、因果推理与决策、时间序列生成）组织文献；3) 审查跨领域的方法和系统，分析其优缺点；4) 整理精选数据集、基准和资源；5) 强调保持证据可见和时间对齐的评估实践；6) 提炼了关于拓扑选择、可观测性、应对变化和流式处理、成本与延迟考量的指导。

Result: 本综述构建了一个以推理拓扑和目标为维度的系统性时间序列推理文献框架；揭示了不同推理拓扑的能力边界及其在忠实性或鲁棒性上的限制；提供了支持该领域研究与部署的精选数据集、基准和资源列表；明确了有效的评估实践，并提供了在不确定性、可观测性、系统规划和成本管理方面的实用指导；强调了推理结构在接地（grounding）能力、自我校正与计算成本、可复现性之间需要平衡。

Conclusion: 未来的进展将依赖于将推理质量与实际效用挂钩的基准测试，以及能够在考虑变化、流式和长周期设置下权衡成本与风险的闭环测试平台。研究方向正从追求狭窄的准确性转向大规模的可靠性，旨在构建能够理解、解释并基于动态世界行动，且具备可追溯证据和可信结果的系统。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [147] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: 本文提出了AMLNet，一个基于知识的多智能体框架，用于生成符合监管要求的合成反洗钱交易数据集，并配备一个高性能的检测模型，以促进可复现的AML研究。


<details>
  <summary>Details</summary>
Motivation: 反洗钱（AML）研究受限于缺乏可公开共享、符合监管要求的交易数据集。

Method: AMLNet是一个知识驱动的多智能体框架，包含两个协调单元：一个监管感知的交易生成器和一个集成检测管道。该生成器生成了1,090,173笔合成交易，涵盖核心洗钱阶段和高级洗钱模式。

Result: 数据集的监管一致性（基于AUSTRAC规则覆盖）达到75%，综合技术保真度分数为0.75。检测集成模型在AMLNet内部测试集上实现了F1分数0.90（精确度0.84，召回率0.97），并能适应外部SynthAML数据集，表明其架构具有泛化性。

Conclusion: AMLNet提供了多维度评估，并发布了数据集（版本1.0），旨在推动可复现且注重监管的AML实验研究。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [148] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 该研究首次全面评估了多模态大语言模型（MLLMs）在青少年特发性脊柱侧弯（AIS）自我管理中的应用，发现其在图像解读和领域知识理解方面存在显著局限，并提出了关键点提示和RAG等改进方法。结果表明，尽管RAG有所改善，但MLLMs在AIS护理中仍远未达到个性化助手的水平，尤其在脊柱畸形检测准确率极低。


<details>
  <summary>Details</summary>
Motivation: 首次对多模态大语言模型（MLLMs）在青少年特发性脊柱侧弯（AIS）自我管理中的应用进行全面评估，旨在探索其作为个性化助手的潜力。

Method: 构建了包含约3000张X光片及诊断文本的数据库。通过“分而治之”框架（包括视觉问答、领域知识评估、患者教育咨询任务）评估了五种MLLMs。针对MLLMs发现的局限，提出了利用脊柱关键点提示增强模型和构建AIS知识库进行检索增强生成（RAG）的方法。

Result: 研究发现MLLMs在解读复杂脊柱X光片和理解AIS护理知识方面存在局限。视觉提示在不同架构间效果不一，而RAG显著提升了模型在知识评估任务上的表现。然而，MLLMs在脊柱畸形位置（最佳准确率：0.55）和方向（最佳准确率：0.13）检测方面的准确率极低。

Conclusion: 目前的MLLMs尚无法在AIS护理中实现个性化助手。最大的挑战在于其准确检测脊柱畸形位置和方向的能力严重不足。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [149] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion通过局部、多尺度的图建模，有效处理异构智能体和多尺度交互，提升自动驾驶多智能体轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分捕捉复杂社会动态中的多尺度交互和异构智能体行为，尤其在自动驾驶多智能体轨迹预测中，导致性能受限。

Method: 提出HeLoFusion编码器，它不依赖全局上下文，而是构建以每个智能体为中心的局部、多尺度图来建模直接两两依赖和复杂群体交互。通过聚合-分解消息传递机制和类型特定特征网络处理智能体异构性。

Result: 在Waymo Open Motion Dataset上，HeLoFusion实现了最先进的性能，在Soft mAP和minADE等关键指标上树立了新基准。

Conclusion: 明确建模多尺度和异构交互的局部化架构，是推动运动预测发展的有效策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [150] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文将监督对比学习(SupCon)应用于模仿学习(IL)，旨在通过学习更有效的状态表示，提升智能体在视频游戏环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模仿学习中未能有效捕获与动作相关的因素和因果关系，导致状态表示不足。研究旨在获取更好的潜在表示，以更准确地建模观察与动作之间的因果关系。

Method: 提出了一种将监督对比学习(SupCon)损失函数与连续输出空间相结合的方法，使其能够在模仿学习中学习状态表示，并适用于不受动作类型限制的各种环境。

Result: 在3D游戏(Astro Bot, Returnal)和多个2D Atari游戏上的实验表明，该方法提高了表示质量、加快了学习收敛速度，并相比仅使用监督动作预测损失训练的基线模型获得了更好的泛化能力。

Conclusion: 所提出的基于监督对比学习的模仿学习方法，通过优化状态表示，显著提升了智能体在视频游戏环境中的学习效率和泛化能力。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [151] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: 本文提出了EgoMem，一个为全双工模型设计的终身记忆代理，能够处理实时全模态流，实现多用户识别、个性化响应及用户长期知识维护。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理难以在实时、具身场景中直接处理原始音视频流，以识别多用户、提供个性化响应并维护用户的长期事实、偏好和社交关系。

Method: EgoMem包含三个异步进程：1) 检索进程，通过人脸和语音识别用户并收集上下文。2) 全模态对话进程，基于上下文生成个性化音频响应。3) 记忆管理进程，自动检测对话边界并提取信息更新长期记忆。该方法完全依赖原始音视频流。

Result: EgoMem的检索和记忆管理模块在测试集上实现了超过95%的准确率。与RoboEgo全模态聊天机器人集成后，系统在实时个性化对话中的事实一致性分数超过87%。

Conclusion: EgoMem为实时、终身和具身场景下的记忆代理研究提供了一个强大的基线。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [152] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym是一个开源、灵活的强化学习（RL）框架，用于建筑能源管理中的控制策略训练，集成了EnergyPlus并支持多种控制场景，有效弥合了建筑经理与AI专家之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）在建筑能源管理中缺乏一个灵活的框架，无法有效地应用于各种控制问题。

Method: 提出了BuildingGym工具，一个研究友好且灵活的开源框架，用于训练建筑能源管理中的RL控制策略。它集成了EnergyPlus作为核心模拟器，适用于系统级和房间级控制，并可接受外部信号输入以适应智能电网等更灵活的环境。该工具提供了内置RL算法，并允许用户轻松配置或AI专家实现和测试新算法。

Result: BuildingGym成功建立了制冷负荷管理（包括恒定和动态负荷）的训练任务，内置算法在两项任务中均表现出强大性能，证明了BuildingGym在优化制冷策略方面的有效性。

Conclusion: BuildingGym通过提供一个易于配置和替换RL算法、模拟器及控制环境的平台，有效弥合了建筑经理与AI专家之间的鸿沟，为建筑能源管理提供了一个灵活高效的RL训练解决方案。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [153] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 神经拟态计算旨在实现高效智能系统。本文提出将动力系统理论作为其统一框架，利用该理论建模学习和控制，并利用噪声和微分遗传编程，以促进新兴神经拟态智能的发展。


<details>
  <summary>Details</summary>
Motivation: 传统数字计算能耗巨大。神经拟态计算虽具潜力，但缺乏整合人工智能、神经科学、物理学等多学科的统一理论框架，以实现可持续、透明和普及的智能系统。

Method: 提出以微分学为基础的动力系统理论作为统一理论框架，用于建模自然和人工基质中的推理、学习和控制。具体方法包括将噪声作为学习资源，并利用微分遗传编程发现实现自适应行为的动力系统。

Result: 论文论证了动力系统理论可以作为神经拟态计算的统一基础，使得智能行为能够从物理基质的动态中涌现，从而有望实现新兴的神经拟态智能。

Conclusion: 采纳动力系统理论的视角，将为新兴神经拟态智能铺平道路，使智能行为从物理基质的动力学中产生，进而提升人工智能的科学性和可持续性。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [154] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 本文提出RPAD和RRAD新评估指标，通过与多位专家意见比较并考虑专家间差异，更稳定地衡量AI诊断性能。研究发现，顶级LLMs（如DeepSeek-V3）能达到或超越专家一致性，并强调医学AI需采用相对评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有AI医学诊断评估方法（如传统精确率/召回率、Cohen's Kappa）无法有效处理专家判断的固有变异性，导致评估不一致或缺乏可解释性。亟需更稳定、真实的AI性能衡量标准。

Method: 引入RPAD和RRAD（算法诊断的相对精确率和召回率）新评估指标，通过将AI输出与多位专家意见而非单一参考进行比较，并根据专家间分歧进行性能归一化。此外，开发了一种自动化方法，以98%的准确率识别自由形式临床诊断。使用360个医学对话评估，将多个大型语言模型（LLMs）与医生小组进行比较。

Result: RPAD和RRAD提供了更稳定和真实的诊断质量衡量。自由形式诊断识别方法达到了98%的准确率。顶级LLMs（如DeepSeek-V3）的诊断一致性达到或超过专家共识。研究还发现专家判断存在显著变异性，通常大于AI与人类之间的变异。

Conclusion: 绝对评估指标在医学AI中存在局限性，应采纳相对评估指标（如RPAD和RRAD）。大型语言模型（LLMs）在医学诊断中，在适当的评估框架下，可以达到或超越专家一致性。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [155] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出一种神经符号多智能体架构，通过Kripke模型和模态逻辑增强语言模型驱动的智能体在复杂环境中的推理能力和逻辑一致性，并在模拟粒子加速器环境中成功诊断复杂故障。


<details>
  <summary>Details</summary>
Motivation: 智能体需具备自适应、复杂和自主决策能力以应对挑战性环境。当前AI研究侧重于模型和数据集的规模扩展，但智能体推理的结构、保真度和逻辑一致性在复杂环境中仍是未充分探索的关键领域，现有语言模型可能得出物理或逻辑上站不住脚的结论。

Method: 引入了一种神经符号多智能体架构。其中，个体智能体的信念状态被正式表示为Kripke模型，并使用模态逻辑的正式语言对“可能性”和“必然性”概念进行推理。通过将不变的、特定领域知识编码为逻辑约束，主动引导语言模型的假设生成，有效阻止其得出物理或逻辑上不可持续的结论。

Result: 在高保真模拟粒子加速器环境中，该系统成功诊断了复杂的级联故障。这通过结合语言模型强大的语义直觉、模态逻辑严格可验证的验证以及真实世界模型实现。

Conclusion: 该研究展示了一条通往更鲁棒、可靠和可验证的自主智能体的可行路径。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [156] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 针对医疗领域多模态数据诊断推理不准确问题，本文提出一种新颖的基于时间图的多智能体推理框架。


<details>
  <summary>Details</summary>
Motivation: 医疗和医学领域涉及多模态数据，但现有的多模态推理模型在医疗诊断应用中仍存在局限性，无法实现正确的诊断推理。

Method: 提出一种基于有向图建模的新颖时间图推理过程，支持通过回溯、优化、增删原因来适应动态变化，以达到最佳推荐。同时考虑不同时间点的多模态数据，并采用多智能体时间推理框架，包含任务分配和交叉验证机制，以提高推理输出的准确性。

Result: 初步的实验和分析结果验证了所提出方法的创新性和实用性。

Conclusion: 该初步方法在解决医疗领域多模态推理挑战、辅助医疗专业人员进行准确诊断方面，展现出新颖性和实际应用价值。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [157] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: 研究表明，去中心化的冻结基础模型蜂群无需权重更新，通过刺激素（stigmergic）协调可创作出连贯的长篇音乐作品，并在质量、多样性和创造力上超越集中式系统。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种计算和数据高效的方法，以生成长期、连贯的创意结构，并使其能够从音乐领域推广到更广泛的协作与发现任务。

Method: 本文提出了一个去中心化的蜂群系统（MusicSwarm），该系统由相同的、冻结的基础模型组成，它们通过刺激素和点对点信号进行协调。系统中的小节级代理感知并沉积和声、节奏和结构线索，调整短期记忆并达成共识。研究将此蜂群系统与一个带有全局评论器的集中式多智能体系统进行了比较。

Result: 通过符号、音频和图论分析，蜂群系统在音乐质量、多样性、结构多样性以及各项创造力指标上均表现出卓越性能。其动态演化趋向于互补角色的稳定配置。自相似网络揭示了具有高效长程连接和专业桥接主题的小世界架构，阐明了局部创新如何整合为全局音乐形式。

Conclusion: MusicSwarm通过将专业化从参数更新转移到交互规则、共享内存和动态共识，提供了一种计算和数据高效的途径来生成长期创意结构。该方法不仅适用于音乐，还可以立即推广到协同写作、设计和科学发现等领域。

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [158] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文系统综述了人机协作在灾害管理决策中的模式、效益与局限，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在高风险灾害情景下，不确定性、动态环境和有限资源对及时、明智的决策构成严峻挑战。

Method: 通过系统综述51项同行评审研究，识别并分析了支持灾害管理各阶段决策的四大人机协作模式及其子模式。

Result: 识别出人机决策支持系统、任务与资源协调、信任与透明度、模拟与训练四类主要模式。AI系统能增强态势感知、提高响应效率并支持复杂决策，但也存在可伸缩性、可解释性和系统互操作性等局限。

Conclusion: 强调需开发适应性强、值得信赖且情境感知的人机系统，以提升灾害韧性和公平恢复成果，并指出了关键挑战和未来研究方向。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [159] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: MLLM易受隐式推理风险，无害单模输入组合成有害多模输出。本文提出SSUI数据集和SRPO训练框架，显著提升MLLM在多模态安全推理上的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在长链推理中难以维持安全对齐，导致无害单模态输入协同组合成危险多模态数据，产生有害输出。

Method: 引入首个具有可解释推理路径的数据集SSUI，以及基于SSUI设计的训练框架Safety-aware Reasoning Path Optimization (SRPO)，旨在将MLLM的内部推理过程与人类安全价值观对齐。

Result: 经SRPO训练的模型在包括Reasoning Path Benchmark (RSBench) 在内的关键安全基准测试中取得了最先进的结果，显著优于开源和顶级商业MLLMs。

Conclusion: 通过SSUI数据集和SRPO训练框架，可以有效解决MLLM的隐式推理安全风险，提升其多模态安全对齐能力。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [160] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 提出一种模型驱动方法，通过SysML配置和算法，在MBSE模型中自动生成PDDL规划工件，以弥补MBSE模型缺乏规划语义的不足，实现系统变体验证。


<details>
  <summary>Details</summary>
Motivation: MBSE工程模型虽包含详细系统结构和行为信息，但缺乏前置条件、效果、资源和时间约束等符号规划语义，限制了评估系统变体执行任务的能力和效率。

Method: 本文提出一种模型驱动方法：引入专用SysML配置文件，包含可重用规划构件的刻板印象（stereotypes），并将其整合到现有模型结构中。通过算法自动生成符合规划领域定义语言（PDDL）的领域文件和问题文件。该方法支持原生集成，并保持工程与规划工件间的一致性。

Result: 通过一个飞机装配的案例研究验证了该方法的适用性。案例展示了如何丰富现有工程模型的规划语义，并应用所提工作流生成一致的规划工件。

Conclusion: 生成规划工件能够通过AI规划对系统变体进行验证。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [161] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: 本文介绍了一个名为JustEva的开源评估工具包，旨在衡量大型语言模型（LLM）在法律任务中的公平性，并发现当前LLM存在显著的公平性缺陷。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLM）整合到法律实践中引发了对司法公平性的担忧，特别是由于其“黑箱”处理过程。

Method: 引入JustEva工具包，具有以下特点：(1) 包含65个额外法律因素的结构化标签系统；(2) 不一致性、偏见和不平衡不准确性三种核心公平性指标；(3) 稳健的统计推断方法；(4) 信息可视化。该工具包支持两种实验类型：从LLM生成结构化输出，以及通过回归等方法对LLM输出进行统计分析和推断。

Result: JustEva的实证应用表明，当前LLM存在显著的公平性缺陷，突显了公平可信的LLM法律工具的缺失。

Conclusion: JustEva为评估和改进法律领域算法公平性提供了一个便捷的工具和方法论基础。

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [162] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: 提出双向认知对齐(BiCA)范式，使人类和AI相互适应，优于传统单向对齐，提高了协作性能、适应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐（如RLHF）是单向的，AI适应人类偏好，但忽略人类认知的可变性，限制了协作的潜力。

Method: 引入双向认知对齐(BiCA)框架，通过可学习协议、表征映射和KL预算约束实现人与AI的受控协同演化。

Result: 在协作导航中，BiCA成功率达85.5%（基线70.3%），互适应性提升230%，协议收敛性提升332%。涌现协议优于手工协议84%，双向适应意外提升安全性（OOD鲁棒性+23%），协同效应提升46%。

Conclusion: 最佳协作存在于人与AI能力的交集而非并集，验证了从单向到双向对齐范式的转变是有效的。

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [163] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: LLMs在基于文本的鉴别诊断和模拟专家医学演示方面超越了医生，但在图像解读和文献检索方面仍有不足。研究发布了CPC-Bench基准和Dr. CaBot AI讨论者。


<details>
  <summary>Details</summary>
Motivation: 以往对AI在诊疗会议（CPCs）中的评估仅关注最终诊断，未能解决专家医生所需的复杂推理和演示技能。

Method: 使用7102个CPCs和1021个图像挑战创建了经医生验证的CPC-Bench基准，涵盖10项文本和多模态任务，并用其评估了主流LLMs。开发了“Dr. CaBot”，一个能生成书面和幻灯片演示的AI讨论者，模拟人类专家的角色。

Result: 面对377个CPCs，o3（OpenAI）在60%的病例中将最终诊断排在首位，84%的病例中排在前十，表现优于20位医生。下一项测试选择准确率达98%。但在文献搜索和图像任务上表现较低，o3和Gemini 2.5 Pro在图像挑战中准确率为67%。在CaBot与人类专家生成文本的盲法比较中，医生在74%的试验中误判来源，并认为CaBot的质量更高。CPC-Bench和CaBot已发布。

Conclusion: LLMs在复杂的基于文本的鉴别诊断和模拟专家医学演示方面超越了医生表现，但图像解读和文献检索仍是其弱点。CPC-Bench和CaBot有助于透明且持续地追踪医学AI的进展。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [164] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: 本文回顾了DRL4Real研讨会，重点关注将解耦表示学习(DRL)从理论推向实际可控生成应用，并超越合成基准。


<details>
  <summary>Details</summary>
Motivation: 弥合解耦表示学习(DRL)的理论承诺与现实应用之间的鸿沟，旨在评估DRL方法在可控生成、模型鲁棒性、可解释性和泛化能力等实际场景中的表现，而非局限于合成基准。

Method: 本文总结了DRL4Real研讨会接受的9篇论文，这些论文涵盖了整合新型归纳偏置（如语言）、将扩散模型应用于DRL、3D感知解耦以及DRL在自动驾驶和脑电图分析等专业领域的应用。

Result: 研讨会论文展示了DRL在实际应用中的最新进展，包括其在可控生成、模型鲁棒性、可解释性和泛化方面的增强，并将其应用扩展到多个专业和特定领域。

Conclusion: DRL4Real研讨会成功推动了解耦表示学习从理论走向实际应用，并探讨了多样化的方法和跨领域扩展，为未来研究指明了方向。

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [165] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: 提出了一种名为Moment-DeepRitz的数据驱动两阶段方法，用于学习广义扩散系统中保守-耗散动力学的漂移分解，该方法对噪声数据鲁棒且适应性强。


<details>
  <summary>Details</summary>
Motivation: 保守-耗散动力学在各种复杂开放系统中普遍存在，理解和学习这些动力学机制至关重要。

Method: 开发了一种数据驱动的两阶段方法，即Moment-DeepRitz方法，用于学习广义扩散系统中包含保守-耗散动力学的漂移分解。

Result: 该方法对噪声数据具有鲁棒性，并能适应粗糙势和振荡旋转。通过多个数值实验证明了其有效性。

Conclusion: Moment-DeepRitz方法提供了一种有效且鲁棒的方式来学习广义扩散系统中保守-耗散动力学的漂移分解。

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [166] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: 提出一种结合KAN和LSTM的新型SOH预测框架SOH-KLSTM，用于锂电池健康监测。


<details>
  <summary>Details</summary>
Motivation: 锂电池SOH估算对于确保其寿命、安全性和性能至关重要，但传统方法无法有效表示电池退化的非线性和时间特性。

Method: 提出SOH-KLSTM框架，结合LSTM学习长期依赖的能力和KAN的非线性逼近能力，以有效捕获锂电池复杂的退化行为。

Result: 抽象中仅介绍了方法提案，未提及具体研究结果。

Conclusion: SOH-KLSTM通过结合LSTM和KAN的优势，为锂电池SOH估计提供了一种新颖且有望更准确的解决方案。

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [167] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: 本文比较了Operon、PySR、phy-SO和eggp中多视图符号回归（MvSR）的不同实现，发现它们均表现良好，但特定特性有助于提升模型质量，并为未来MvSR发展提供了指导。


<details>
  <summary>Details</summary>
Motivation: 传统上通过第一性原理和观察推导方程。符号回归（SR）提供了一种自动化方法，而多视图符号回归（MvSR）作为其扩展，能用单一参数函数描述多个数据集，有效缓解过拟合和数据稀缺。鉴于MvSR近期出现多种不同实现，对其进行比较分析具有重要意义。

Method: 作者在不同的真实世界数据集上，对Operon、PySR、phy-SO和eggp这四种工具中多视图符号回归（MvSR）的实现进行了测试和比较。

Result: 所有测试的MvSR实现通常都能达到良好的准确性，并生成具有少量自由参数的解决方案。然而，研究发现某些特定的特性能够更频繁地生成更好的模型。

Conclusion: 研究为未来的MvSR发展提供了指导方针。

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [168] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: ZIDF是一种处理零膨胀数据的降水预测框架，结合高斯扰动、Transformer和扩散去噪，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 零膨胀数据（以零值为主，非零事件稀疏）给降水预测带来了重大挑战。

Method: 提出零膨胀扩散框架 (ZIDF)，该框架整合了高斯扰动（用于平滑零膨胀分布）、基于Transformer的预测（用于捕捉时间模式）和基于扩散的去噪（用于恢复原始数据结构）。实验使用了南澳大利亚的观测降水数据和合成的零膨胀数据。

Result: ZIDF在多个最先进的降水预测模型上表现出显著性能提升，相对于基线非平稳Transformer，MSE降低高达56.7%，MAE降低21.1%。

Conclusion: 研究结果表明ZIDF能够鲁棒地处理稀疏时间序列数据，并具有推广到其他零膨胀领域解决关键挑战的潜力。

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [169] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: FEDEXCHANGE提出一种服务器端模型交换策略，解决了联邦目标检测中跨域泛化和客户端计算开销大的问题，在不增加本地开销的情况下，显著提升了性能（1.6倍mAP）并降低了计算资源（0.8倍）。


<details>
  <summary>Details</summary>
Motivation: 联邦目标检测（FOD）在客户端数据域差异大时，跨域泛化能力差。现有FOD方法常忽略边缘设备硬件限制，引入高计算成本的本地训练正则化，限制了实际应用。

Method: FEDEXCHANGE是一个新型FOD框架，采用服务器端动态模型交换策略，使客户端无需直接数据共享即可从其他客户端领域数据中学习。服务器交替进行模型聚合和模型交换：聚合轮次正常聚合模型；交换轮次则基于距离度量对本地模型进行聚类和交换。所有操作均在服务器端完成，不增加客户端本地计算开销。

Result: FEDEXCHANGE增强了FOD性能，在雨天等挑战性领域实现了1.6倍的平均精度（mAP）提升，同时相比基线方法仅需0.8倍的计算资源。

Conclusion: FEDEXCHANGE通过服务器端模型交换策略，有效弥合了领域差距，提升了联邦目标检测的跨域泛化能力，且不增加客户端计算开销，显著提高了性能和效率。

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [170] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: 逆合成规划常因“最弱环节”失败。本文将逆合成重构为树形MDP中的最差路径优化问题，并提出InterRetro方法，在Retro*-190基准上实现100%目标解决率，缩短路线，并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成规划方法仅优化平均性能，忽视了合成路线中因任何一个无效叶节点导致规划失败的“最弱环节”问题，即对最差情况的敏感性。

Method: 提出将逆合成规划重新定义为树形马尔可夫决策过程（MDPs）中的最差路径优化问题，并证明了其数学特性。在此基础上，开发了InterRetro方法，通过与树形MDP交互，学习最差路径的价值函数，并通过自我模仿（self-imitation）优先强化高优势决策来改进策略。

Result: InterRetro在Retro*-190基准测试中实现了100%的目标解决率，将合成路线平均缩短了4.9%，并能在仅使用10%训练数据的情况下获得出色性能，达到当前最佳水平。

Conclusion: 该研究通过引入最差路径优化框架和InterRetro方法，显著解决了逆合成规划中的“最弱环节”挑战，是计算逆合成规划领域的重要进展。

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [171] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: 本文提出AttnBoost，一个将特征级注意力机制集成到梯度提升过程中的可解释学习框架，以解决零售需求预测中传统GBDT模型特征适应性不足的问题，提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 零售供应链产品需求预测复杂，特征噪声大、异构性强，消费者行为变化迅速。传统梯度提升决策树（GBDT）模型虽预测性能强，但在应对变化条件时，缺乏自适应机制来识别和强调最相关的特征。

Method: 提出AttnBoost框架，通过轻量级注意力机制，在每个提升轮次中动态调整特征重要性，使其能够聚焦于促销、定价和季节性趋势等高影响力变量，从而提升预测准确性和可解释性。

Result: AttnBoost在大型零售销售数据集上表现优于标准机器学习和深度表格模型，并为供应链管理者提供可操作性洞察。消融研究证实注意力模块有助于缓解过拟合和提高可解释性。

Conclusion: 注意力引导的提升方法为现实世界预测应用中可解释、可扩展的AI提供了一个有前景的方向。

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [172] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: 研究发现，通过引入选择性反馈机制，大型语言模型（LLMs）在基于自身输出的递归训练中，不仅能避免模型崩溃，还能显著提升性能，该现象被称为“反衔尾蛇效应”。


<details>
  <summary>Details</summary>
Motivation: 解决递归训练的大语言模型（LLMs）的稳定性问题，特别是现有理论预测的模型崩溃现象，这对于AI安全至关重要。

Method: 引入一种选择性反馈机制来训练基于自身输出的LLMs。通过实验对比了带有质量过滤、无过滤和随机过滤三种条件下，Gemma 2B模型在复杂摘要任务上的性能变化。此外，还通过一个简单的分类器实验验证了理论上的退化循环，以突出高维模型的独特动态。

Result: 在五代训练后，带有质量过滤的模型在ROUGE-L F1得分上提升了6.6%，而无过滤和随机过滤的对照组分别下降了3.5%和4.2%。这表明选择性反馈不仅阻止了性能衰退，反而诱导了显著的性能提升，这一现象被命名为“反衔尾蛇效应”。

Conclusion: LLMs在简单的选择压力下可以表现出系统性韧性，这是一种强大的、可扩展的原则，有助于开发更安全、更强大的AI系统。

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [173] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ是一个结合认知双记忆系统和自适应探索策略的强化学习框架，旨在解决传统RL在动态环境中探索、稳定性和适应性不足的问题，并在异常检测任务中表现出卓越的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习算法（如DQN和PPO）在动态环境中，往往难以实现高效探索、保持稳定性和具备适应性。

Method: 本研究提出了LogGuardQ框架，该框架整合了受人类认知启发的双记忆系统以及由温度衰减和好奇心驱动的自适应探索策略。

Result: 在包含100万条模拟访问日志的数据集上，LogGuardQ实现了96.0%的检测率（DQN为93.0%，PPO为47.1%），召回率为0.9996，F1-score为0.6450。其平均奖励为20.34，显著优于DQN和PPO。图形分析和统计测试（Mann-Whitney U）均证实了LogGuardQ在稳定性、效率和性能上的显著优势。

Conclusion: LogGuardQ通过融合认知科学与强化学习，为不确定环境下的自适应学习提供了一种可扩展的方法，在网络安全、入侵检测和不确定性决策等领域具有潜在应用。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [174] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: 本文提出一种面向服务的自适应激励机制，通过结合Stackelberg博弈、多智能体马尔可夫决策过程（DRL求解）和ASOSA算法，解决联邦学习中的数据稀缺问题，以最大化任务发布者、本地模型拥有者和工作者的效用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式模型训练中面临训练数据不足的问题，需要招募工作者来收集数据。研究目的是最大化任务发布者（TP）、本地模型拥有者（LMOs）和数据收集工作者的效用。

Method: 1. 提出一种面向服务的自适应激励机制。2. 将LMOs与TP之间的交互建模为Stackelberg博弈，并推导出分析性的纳什均衡解。3. 将LMOs与工作者之间的交互建模为多智能体马尔可夫决策过程（MAMDP），并通过深度强化学习（DRL）识别最优策略。4. 设计了自适应搜索最优策略算法（ASOSA），以稳定参与者策略并解决耦合问题。

Result: 通过广泛的数值实验，验证了所提方法的有效性。

Conclusion: 所提出的自适应激励机制通过融合博弈论和深度强化学习方法，成功解决了联邦学习中的数据稀缺问题，并有效最大化了任务发布者、本地模型拥有者和数据收集工作者的效用。

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [175] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: 针对稀疏MoE在异构输入下专家特化不足的问题，本文提出Mixture-of-Clustered-Experts (MoCE)，通过双阶段路由（序列级专家组路由+token级组内激活）有效提升专家特化和泛化能力，并在广泛基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE架构在提高专家特化以增强性能和泛化能力方面面临挑战，尤其是在输入高度异构的指令微调场景中。

Method: 提出Mixture-of-Clustered-Experts (MoCE)，采用双阶段路由机制：第一阶段基于序列级特征进行专家组路由；第二阶段在选定的专家组内激活top-$k$个token级专家。

Result: MoCE在全面的基准测试中持续优于强基线模型，并展现出增强的泛化能力。详细分析进一步证实了MoCE的鲁棒性和有效性。

Conclusion: MoCE通过其创新的双阶段路由机制，成功解决了现有MoE架构在专家特化和处理异构输入方面的局限性，在性能、泛化能力、鲁棒性和有效性上均表现出色。

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [176] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: 本文提出一个基于微分流形的新框架，用于统一分析人工神经网络中的连续吸引子，并揭示了其普遍存在性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏一个统一框架来分析不同动力系统中的连续吸引子，这限制了其在跨架构方面的泛化能力。

Method: 本研究从微分流形的视角，建立了一个新的分析框架，用于探究人工神经网络中的连续吸引子。

Result: 该框架验证了与先验结论的兼容性；阐明了连续吸引子现象与局部雅可比矩阵特征值之间的联系；并证明了奇异值分层在常见分类模型和数据集中的普遍性。

Conclusion: 研究结果表明连续吸引子可能在通用神经网络中普遍存在，本研究所提出的框架为构建通用理论提供了一个有前景的基础。

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [177] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: 本文提出UAPO框架，通过引入锚定函数处理偏好数据不确定性，支持非配对数据训练，从而提升离线偏好优化方法的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有DPO类离线偏好优化方法（基于Bradley-Terry奖励建模）存在局限性，如需要配对训练数据、模型分布偏移和人类理性假设等，这些限制了数据利用效率和方法鲁棒性。

Method: 提出了一种通用的离线偏好优化框架——带实用锚的自适应偏好优化（UAPO），该方法引入锚定函数来估计偏好数据标注带来的不确定性，并支持在非配对数据场景下进行训练。

Result: 实验结果表明，UAPO在不严格依赖数据配对的情况下取得了具有竞争力的结果，显著提升了数据利用效率，且锚定设计增强了训练过程的鲁棒性。

Conclusion: UAPO为更灵活、更有效的偏好优化方法开辟了道路，解决了传统方法对数据配对的严格依赖问题。

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


### [178] [Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction](https://arxiv.org/abs/2509.10516)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本研究提出并评估了一个基于联邦学习的隐私保护推荐系统，用于教育领域，在不集中学生敏感数据的前提下，实现了接近中心化模型的高效个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 数字化教育带来个性化机遇，但学生数据隐私挑战日益突出。传统推荐系统依赖中心化数据，与现代数据保护法规冲突。

Method: 提出基于联邦学习（FL）的隐私保护推荐系统。采用深度神经网络（DNN），利用ASSISTments教育数据集的丰富特征。对联邦聚合策略进行严格比较分析，识别出FedProx比FedAvg更适合处理异构学生数据。

Result: 优化的联邦模型实现了76.28%的F1-Score，相当于强大中心化XGBoost模型性能的82.85%。FedProx在处理异构学生数据方面表现出比FedAvg更好的稳定性和有效性。

Conclusion: 联邦学习方法能够提供高效的内容推荐，且无需集中敏感学生数据。本工作为现代教育平台中的个性化-隐私困境提供了一个可行且稳健的解决方案。

Abstract: The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.

</details>


### [179] [A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data](https://arxiv.org/abs/2509.10517)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本研究对比了五种联邦学习策略在非独立同分布和不平衡的临床住院死亡率预测任务中的表现，发现基于正则化的FedProx表现最佳。


<details>
  <summary>Details</summary>
Motivation: 机器学习在预测住院死亡率方面潜力巨大，但受限于数据隐私和临床数据的异质性。联邦学习提供隐私保护方案，但其在非独立同分布和数据不平衡条件下的性能需深入研究。

Method: 研究比较了FedAvg、FedProx、FedAdagrad、FedAdam和FedCluster五种联邦学习策略，用于住院死亡率预测。使用MIMIC-IV数据集，通过按临床护理单元划分数据来模拟非独立同分布环境，并应用SMOTE-Tomek技术处理各客户端的局部数据不平衡问题。

Result: 经过50轮通信实验，基于正则化的FedProx策略持续优于其他方法，F1-Score达到0.8831，并保持稳定收敛。基线FedAvg计算效率最高，但预测性能显著较低。

Conclusion: 研究表明，针对异构和不平衡的临床预测任务，FedProx等基于正则化的联邦学习算法比标准或服务器端自适应聚合方法更稳健有效，为医疗健康应用中联邦学习策略的选择提供了重要的经验基准。

Abstract: Machine learning models hold significant potential for predicting in-hospital
mortality, yet data privacy constraints and the statistical heterogeneity of
real-world clinical data often hamper their development. Federated Learning
(FL) offers a privacy-preserving solution, but its performance under
non-Independent and Identically Distributed (non-IID) and imbalanced conditions
requires rigorous investigation. The study presents a comparative benchmark of
five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and
FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we
simulate a realistic non-IID environment by partitioning data by clinical care
unit. To address the inherent class imbalance of the task, the SMOTE-Tomek
technique is applied to each client's local training data. Our experiments,
conducted over 50 communication rounds, reveal that the regularization-based
strategy, FedProx, consistently outperformed other methods, achieving the
highest F1-Score of 0.8831 while maintaining stable convergence. While the
baseline FedAvg was the most computationally efficient, its predictive
performance was substantially lower. Our findings indicate that
regularization-based FL algorithms like FedProx offer a more robust and
effective solution for heterogeneous and imbalanced clinical prediction tasks
than standard or server-side adaptive aggregation methods. The work provides a
crucial empirical benchmark for selecting appropriate FL strategies for
real-world healthcare applications.

</details>


### [180] [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)
*Justin Arndt*

Main category: cs.LG

TL;DR: 本研究提出全息知识流形（HKM）流水线，通过分形量化等技术，实现了AI知识表示中的零灾难性遗忘，同时高效、低内存增长，并显著降低成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决AI知识表示中灾难性遗忘、内存增长和效率低下的问题，旨在使大型语言模型（LLM）实现“永恒”适应而无需重复训练。

Method: 引入全息知识流形（HKM）四阶段流水线，核心技术包括分形量化、概率纠缠和动态衍射切片。

Result: 实验证明，HKM实现了0%灾难性遗忘（比GEM基线无限提升），3倍知识压缩，存储节省67%，训练时间减少53%。预计五年内可节省92.4M美元，并减少21.2%能源消耗和33%碳足迹。

Conclusion: 本工作预示着公共大型语言模型范式的转变，使其能够“永恒”适应而无需重新训练，未来可扩展至多模态融合和量子硬件，进一步普及可伸缩AI并大幅降低微调成本。

Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline
that achieves zero catastrophic forgetting in AI knowledge representation while
maintaining minimal memory growth and high efficiency. Leveraging fractal
quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM
compresses knowledge substrates by 3x with 67% storage savings, integrates
holographically at 100%, and supports over 1,020 updates with 1% growth per
increment. In experiments on combined WikiText and FB15k datasets (scaled to
2,997 nodes), we demonstrate industry-leading performance: 0% forgetting
(infinite improvement over GEM baselines), 3x compression, and 53% training
time reduction on consumer GPU hardware. Hypothetical cost analyses project
$92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and
33% lower carbon footprint. This work hypothesizes a paradigm shift for public
large language models (LLMs), enabling "eternal" adaptation without retraining.
Future extensions to multimodal fusion and quantum hardware could further
democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for
models like Llama-3 or Grok-4. Code, datasets, and full results are publicly
available for reproducibility.

</details>


### [181] [Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models](https://arxiv.org/abs/2509.10519)
*Chang Meng,Wayne Burleson,Giovanni De Micheli*

Main category: cs.LG

TL;DR: 本文提出两种基于查找表（LUT）的方法（LUT-2D和LUT-1D），用于精确计算近似乘法器（AppMult）的梯度，从而显著提高使用AppMults的深度学习模型的重训练精度。


<details>
  <summary>Details</summary>
Motivation: 近似乘法器（AppMults）在深度学习加速器中广泛使用以降低面积、延迟和功耗，但它们会引入算术误差，需要通过重训练来恢复模型精度。现有方法通常使用精确乘法器（AccMult）的梯度来估计AppMult梯度，这导致重训练结果不理想。

Method: 为解决现有方法重训练效果次优的问题，本文提出了两种获取更精确AppMult梯度的计算方法：
1.  **LUT-2D**：使用二维查找表（LUTs）来表征AppMult梯度，提供精细的梯度估计，并能实现最高的重训练精度。
2.  **LUT-1D**：作为LUT-2D的紧凑且更高效的变体，它将梯度值存储在一维查找表（LUTs）中，在缩短运行时间的同时实现了可比的重训练精度。

Result: 实验结果表明：
*   在CIFAR-10数据集上使用卷积神经网络（CNNs）时，LUT-2D和LUT-1D方法分别平均提高了重训练精度3.83%和3.72%。
*   在ImageNet数据集上使用Vision Transformer模型时，LUT-1D方法与现有最先进的重训练框架相比，平均提高了重训练精度23.69%。

Conclusion: 所提出的基于查找表的AppMult梯度计算方法（LUT-2D和LUT-1D）能够提供更精确的梯度信息，从而显著提升了使用近似乘法器的深度学习模型的重训练精度，尤其是在复杂模型和大规模数据集上表现出卓越的改进。

Abstract: Approximate multipliers (AppMults) are widely used in deep learning
accelerators to reduce their area, delay, and power consumption. However,
AppMults introduce arithmetic errors into deep learning models, necessitating a
retraining process to recover accuracy. A key step in retraining is computing
the gradient of the AppMult, i.e., the partial derivative of the approximate
product with respect to each input operand. Existing approaches typically
estimate this gradient using that of the accurate multiplier (AccMult), which
can lead to suboptimal retraining results. To address this, we propose two
methods to obtain more precise gradients of AppMults. The first, called LUT-2D,
characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs),
providing fine-grained estimation and achieving the highest retraining
accuracy. The second, called LUT-1D, is a compact and more efficient variant
that stores gradient values in 1-dimensional LUTs, achieving comparable
retraining accuracy with shorter runtime. Experimental results show that on
CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods
improve retraining accuracy by 3.83% and 3.72% on average, respectively. On
ImageNet with vision transformer models, our LUT-1D method improves retraining
accuracy by 23.69% on average, compared to a state-of-the-art retraining
framework.

</details>


### [182] [Offline Contextual Bandit with Counterfactual Sample Identification](https://arxiv.org/abs/2509.10520)
*Alexandre Gilotte,Otmane Sakhi,Imad Aouali,Benjamin Heymann*

Main category: cs.LG

TL;DR: 提出反事实样本识别（CSI）方法，通过比较真实与反事实行动来识别成功，解决上下文强盗问题中奖励模型混杂性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强盗系统中的直接奖励模型易受混杂性影响，难以有效分离行动和上下文的效果。

Method: 引入“反事实样本识别”新方法，将问题重构为识别导致成功（二元）结果的行动。通过在相同上下文下，将实际行动与从日志策略中抽取的反事实行动进行比较来实现。

Result: 该方法具有理论基础，并在合成实验和实际部署中均持续优于传统直接模型。

Conclusion: 反事实样本识别是解决上下文强盗模型中混杂性问题的有效且理论上可靠的方案，并在实践中表现出优越性。

Abstract: In production systems, contextual bandit approaches often rely on direct
reward models that take both action and context as input. However, these models
can suffer from confounding, making it difficult to isolate the effect of the
action from that of the context. We present \emph{Counterfactual Sample
Identification}, a new approach that re-frames the problem: rather than
predicting reward, it learns to recognize which action led to a successful
(binary) outcome by comparing it to a counterfactual action sampled from the
logging policy under the same context. The method is theoretically grounded and
consistently outperforms direct models in both synthetic experiments and
real-world deployments.

</details>


### [183] [Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization](https://arxiv.org/abs/2509.10521)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 现有PFL在标签偏斜和非平稳性下表现不佳。本文提出VGM$^2$，一个以几何为中心的PFL框架，通过学习客户端UMAP嵌入、建模潜在距离并交换变分标记统计量来解决此问题。实验证明VGM$^2$在多个视觉数据集上实现竞争或优异的F1分数，通信量小且隐私性强。


<details>
  <summary>Details</summary>
Motivation: 个性化联邦学习（PFL）在标签偏斜和非平稳性数据下通常表现不佳，因为单一的全局参数化未能考虑到客户端特定的数据几何结构。

Method: 本文提出VGM$^2$（Variational Gaussian Mixture Manifold）框架，这是一个以几何为中心的PFL方法。它通过以下步骤实现：(i) 学习客户端特定的参数化UMAP嵌入；(ii) 使用混合关系标记（针对同类和异类对）建模潜在的成对距离；(iii) 仅交换变分、不确定性感知的标记统计量。每个客户端维护关于标记权重、均值和方差的Dirichlet-Normal-Inverse-Gamma (Dir-NIG) 后验分布，服务器通过共轭矩匹配进行聚合以形成指导后续轮次的全局先验。理论证明此聚合能最小化客户端后验的Kullback-Leibler散度，从而在异构性下提供稳定性。此外，还引入了距离到相似度映射的校准项，并考虑了通信和计算预算。

Result: 在八个带有非IID标签分片的视觉数据集上，VGM$^2$与强基线相比，实现了有竞争力或更优的测试F1分数，同时仅通信小的几何摘要，显著降低了通信开销。通过安全聚合和可选的差分隐私噪声，模型的隐私性得到增强，并通过成员推断压力测试进行了验证。

Conclusion: VGM$^2$通过其几何为中心的方法，有效解决了PFL在标签偏斜和非平稳性下的挑战，成功地在保持高性能的同时，显著降低了通信成本并增强了隐私保护。其稳健的理论基础确保了在高度异构环境下的稳定性，为个性化联邦学习提供了一个实用且有效的解决方案。

Abstract: Personalized federated learning (PFL) often fails under label skew and
non-stationarity because a single global parameterization ignores
client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture
Manifold), a geometry-centric PFL framework that (i) learns client-specific
parametric UMAP embeddings, (ii) models latent pairwise distances with mixture
relation markers for same and different class pairs, and (iii) exchanges only
variational, uncertainty-aware marker statistics. Each client maintains a
Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means,
and variances; the server aggregates via conjugate moment matching to form
global priors that guide subsequent rounds. We prove that this aggregation
minimizes the summed reverse Kullback-Leibler divergence from client posteriors
within the conjugate family, yielding stability under heterogeneity. We further
incorporate a calibration term for distance-to-similarity mapping and report
communication and compute budgets. Across eight vision datasets with non-IID
label shards, VGM$^2$ achieves competitive or superior test F1 scores compared
to strong baselines while communicating only small geometry summaries. Privacy
is strengthened through secure aggregation and optional differential privacy
noise, and we provide a membership-inference stress test. Code and
configurations will be released to ensure full reproducibility.

</details>


### [184] [Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction](https://arxiv.org/abs/2509.10522)
*Kaizhen Tan*

Main category: cs.LG

TL;DR: 本文提出了一个多模态深度学习框架（CNN-Transformer集成模型），整合多种数据预测空管指令与飞机机动的时间偏移及指令时长，旨在支持智能指令生成，并对工作量评估、人员配置和排班提供实用价值。


<details>
  <summary>Details</summary>
Motivation: 在密集空域中，空中交通管制员（ATCOs）发布高强度语音指令，准确的工作量建模对于确保飞行安全和提高效率至关重要。

Method: 1. 提出了一个整合结构化数据、轨迹序列和图像特征的多模态深度学习框架。
2. 构建了高质量数据集，并使用滑动窗口和基于直方图的方法检测机动点。
3. 开发了CNN-Transformer集成模型以实现准确、可泛化和可解释的预测。

Result: 1. 成功估算了ATCO指令生命周期中的两个关键参数：指令与飞机机动之间的时间偏移量和指令持续时间。
2. 模型实现了准确、可泛化和可解释的预测。
3. 首次建立了将轨迹与语音指令关联的模型，支持智能指令生成。

Conclusion: 该工作为ATCO的工作量评估、人员配置和排班提供了重要的实用价值。

Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense
airspace, where accurate workload modeling is critical for safety and
efficiency. This paper proposes a multimodal deep learning framework that
integrates structured data, trajectory sequences, and image features to
estimate two key parameters in the ATCO command lifecycle: the time offset
between a command and the resulting aircraft maneuver, and the command
duration. A high-quality dataset was constructed, with maneuver points detected
using sliding window and histogram-based methods. A CNN-Transformer ensemble
model was developed for accurate, generalizable, and interpretable predictions.
By linking trajectories to voice commands, this work offers the first model of
its kind to support intelligent command generation and provides practical value
for workload assessment, staffing, and scheduling.

</details>


### [185] [From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions](https://arxiv.org/abs/2509.10523)
*Kush Gupta,Amir Aly,Emmanuel Ifeachor,Rohit Shankar*

Main category: cs.LG

TL;DR: 本文提出一个结合深度学习和可解释AI的诊断框架，利用跨域迁移学习解决ASD数据稀缺问题，并通过多种XAI技术解释模型决策并识别关键脑区。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）研究中，机器学习的迁移学习应用有限，且存在数据稀缺问题。此外，需要提升诊断模型的可解释性，以识别关键脑区。

Method: 该框架包含两个模块：1) 利用跨域迁移学习微调深度学习模型进行ASD分类；2) 采用显著性图、梯度加权类激活映射（Grad-CAM）和SHAP分析三种可解释AI（XAI）技术来解释模型决策并识别与ASD相关的关键脑区。

Result: 该框架表明，跨域迁移学习能有效解决ASD研究中的数据稀缺问题。同时，XAI技术揭示了模型的诊断决策过程，并成功识别出与ASD高度相关的脑区。这些发现与已有的神经生物学证据高度吻合。

Conclusion: 所提出的结合深度学习和可解释AI的框架，不仅能有效诊断ASD并克服数据稀缺挑战，还能提供可解释的诊断依据，并通过识别与ASD相关的脑区，展现出显著的临床相关性。

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by atypical brain maturation. However, the adaptation of transfer
learning paradigms in machine learning for ASD research remains notably
limited. In this study, we propose a computer-aided diagnostic framework with
two modules. This chapter presents a two-module framework combining deep
learning and explainable AI for ASD diagnosis. The first module leverages a
deep learning model fine-tuned through cross-domain transfer learning for ASD
classification. The second module focuses on interpreting the model decisions
and identifying critical brain regions. To achieve this, we employed three
explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class
Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This
framework demonstrates that cross-domain transfer learning can effectively
address data scarcity in ASD research. In addition, by applying three
established explainability techniques, the approach reveals how the model makes
diagnostic decisions and identifies brain regions most associated with ASD.
These findings were compared against established neurobiological evidence,
highlighting strong alignment and reinforcing the clinical relevance of the
proposed approach.

</details>


### [186] [Resource-Aware Neural Network Pruning Using Graph-based Reinforcement Learning](https://arxiv.org/abs/2509.10526)
*Dieter Balemans,Thomas Huybrechts,Jan Steckel,Siegfried Mercelis*

Main category: cs.LG

TL;DR: 该论文提出一种新颖的神经网络剪枝方法，通过将基于图的观测空间和二元动作空间整合到AutoML框架中，在CMDP下学习特定任务的剪枝策略，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络剪枝方法依赖手工启发式规则和局部优化视角，常导致次优性能和低效的剪枝策略，且缺乏对网络结构的全局视图。

Method: 该框架将目标神经网络表示为图，捕获完整的拓扑关系，并以全局视图替代传统的层级观测空间。核心创新包括：1) 使用图注意力网络（GAT）编码器处理图表示以生成丰富嵌入；2) 将动作空间从连续剪枝比率转变为细粒度的二元动作空间，使智能体直接从数据中学习通道重要性标准。这些被建模在约束马尔可夫决策过程（CMDP）框架内，并设计了自竞争奖励系统，以在满足资源约束（如压缩率）的同时，学习并优化剪枝决策。

Result: 在CIFAR-10、CIFAR-100和ImageNet等基准数据集上的实验表明，该方法持续优于传统剪枝技术，取得了最先进（SOTA）的结果。它能学习特定任务的剪枝策略，并识别超出简单权重大小考虑的功能冗余连接。

Conclusion: 所提出的基于图的AutoML剪枝框架通过克服传统方法的局限性，成功实现了SOTA性能。它能够学习智能、全局且特定任务的剪枝策略，有效识别冗余连接，从而提高了神经网络剪枝的效率和效果。

Abstract: This paper presents a novel approach to neural network pruning by integrating
a graph-based observation space into an AutoML framework to address the
limitations of existing methods. Traditional pruning approaches often depend on
hand-crafted heuristics and local optimization perspectives, which can lead to
suboptimal performance and inefficient pruning strategies. Our framework
transforms the pruning process by introducing a graph representation of the
target neural network that captures complete topological relationships between
layers and channels, replacing the limited layer-wise observation space with a
global view of network structure. The core innovations include a Graph
Attention Network (GAT) encoder that processes the network's graph
representation and generates a rich embedding. Additionally, for the action
space we transition from continuous pruning ratios to fine-grained binary
action spaces which enables the agent to learn optimal channel importance
criteria directly from data, moving away from predefined scoring functions.
These contributions are modelled within a Constrained Markov Decision Process
(CMDP) framework, allowing the agent to make informed pruning decisions while
adhering to resource constraints such as target compression rates. For this, we
design a self-competition reward system that encourages the agent to outperform
its previous best performance while satisfying the defined constraints. We
demonstrate the effectiveness of our approach through extensive experiments on
benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments
show that our method consistently outperforms traditional pruning techniques,
showing state-of-the-art results while learning task-specific pruning
strategies that identify functionally redundant connections beyond simple
weight magnitude considerations.

</details>


### [187] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息几何投影的贝叶斯联邦学习(BFL)个性化框架，通过在统计流形上计算重心实现无成本的闭式解，有效平衡了全局泛化与局部专业化，且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习(BFL)方法在处理数据异构和隐私约束下的个性化问题时，依赖MCMC或变分推断，效率和可控性有待提升。研究需要一种能够有效平衡全局泛化和局部专业化的模型个性化方法。

Method: 提出了一种用于参数BFL个性化的信息几何投影框架。该方法通过将全局模型投影到用户本地模型的邻域，在全局泛化和局部专业化之间实现可调权衡。在温和假设下，此投影等价于在统计流形上计算重心，从而获得闭式解，实现无成本的个性化。该方法应用于使用改进变分在线牛顿(IVON)优化器的变分学习设置，并扩展到通用的BFL聚合方案。

Result: 所提出的方法通过信息几何投影实现了全局模型与本地模型之间的可调权衡。该投影等价于在统计流形上计算重心，能够导出闭式解，从而实现无成本的个性化。在异构数据分布下的实证评估证实，该方法有效平衡了全局和局部性能，且计算开销极小。

Conclusion: 该研究提供了一种高效且创新的信息几何投影框架，解决了贝叶斯联邦学习中模型个性化的问题。通过实现无成本的闭式个性化，该方法在保持全局泛化能力的同时，有效适应了本地数据分布，并显著降低了计算开销。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [188] [STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions](https://arxiv.org/abs/2509.10528)
*Amirhossein Ghaffari,Huong Nguyen,Lauri Lovén,Ekaterina Gilman*

Main category: cs.LG

TL;DR: 介绍STM-Graph，一个将城市时空事件数据转换为图表示的开源Python框架，以便进行图神经网络（GNN）训练和预测。


<details>
  <summary>Details</summary>
Motivation: 城市时空数据因其动态性和复杂性，对预测分析提出了独特的挑战。

Method: 开发了STM-Graph框架，它将原始时空城市事件数据转换为适合GNN训练和预测的图表示。该框架集成了多种空间映射方法、OpenStreetMap的城市特征、多个GNN模型、全面的可视化工具以及用户友好的图形用户界面（GUI）。

Result: STM-Graph是一个模块化、可扩展的框架，便于快速实验和基准测试。它支持集成新的映射方法和自定义模型。

Conclusion: STM-Graph是城市计算领域研究人员和实践者的宝贵资源。

Abstract: Urban spatio-temporal data present unique challenges for predictive analytics
due to their dynamic and complex nature. We introduce STM-Graph, an open-source
Python framework that transforms raw spatio-temporal urban event data into
graph representations suitable for Graph Neural Network (GNN) training and
prediction. STM-Graph integrates diverse spatial mapping methods, urban
features from OpenStreetMap, multiple GNN models, comprehensive visualization
tools, and a graphical user interface (GUI) suitable for professional and
non-professional users. This modular and extensible framework facilitates rapid
experimentation and benchmarking. It allows integration of new mapping methods
and custom models, making it a valuable resource for researchers and
practitioners in urban computing. The source code of the framework and GUI are
available at: https://github.com/Ahghaffari/stm_graph and
https://github.com/tuminguyen/stm_graph_gui.

</details>


### [189] [Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay](https://arxiv.org/abs/2509.10529)
*Aoi Otani*

Main category: cs.LG

TL;DR: 本文提出将潜变量重放（Latent Replay）应用于文本到图像扩散模型，以解决持续学习中的灾难性遗忘和模式崩溃问题，通过存储紧凑的高级特征表示，显著提高模型在学习新概念后对旧知识的保持能力。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在持续学习中存在“灾难性遗忘”问题，新知识覆盖旧知识，这在文本到图像扩散模型中尤为严重，并伴随“模式崩溃”导致输出重复。现有重放方法需大量存储空间。

Method: 将受神经科学启发的潜变量重放应用于扩散模型。该方法不存储原始图像，而是仅保留模型内部架构提取的紧凑、高层次的特征表示（潜变量），模拟海马体存储神经活动模式，从而减少内存使用。实验中比较了随机选择和基于相似性的潜变量选择策略。

Result: 在学习五个顺序视觉概念后，潜变量重放方法在保持模型多功能性方面显著优于现有方法，对最早概念的图像对齐（IA）率达到77.59%，比基线方法高14%，并保持输出多样性。令人惊讶的是，随机选择存储的潜变量示例效果优于基于相似性的策略。

Conclusion: 潜变量重放能有效实现生成式AI模型的持续学习，为开发可随用户需求演进且无需过高计算成本的个性化文本到图像模型奠定基础。

Abstract: Continual learning -- the ability to acquire knowledge incrementally without
forgetting previous skills -- is fundamental to natural intelligence. While the
human brain excels at this, artificial neural networks struggle with
"catastrophic forgetting," where learning new tasks erases previously acquired
knowledge. This challenge is particularly severe for text-to-image diffusion
models, which generate images from textual prompts. Additionally, these models
face "mode collapse," where their outputs become increasingly repetitive over
time. To address these challenges, we apply Latent Replay, a
neuroscience-inspired approach, to diffusion models. Traditional replay methods
mitigate forgetting by storing and revisiting past examples, typically
requiring large collections of images. Latent Replay instead retains only
compact, high-level feature representations extracted from the model's internal
architecture. This mirrors the hippocampal process of storing neural activity
patterns rather than raw sensory inputs, reducing memory usage while preserving
critical information. Through experiments with five sequentially learned visual
concepts, we demonstrate that Latent Replay significantly outperforms existing
methods in maintaining model versatility. After learning all concepts, our
approach retained 77.59% Image Alignment (IA) on the earliest concept, 14%
higher than baseline methods, while maintaining diverse outputs. Surprisingly,
random selection of stored latent examples outperforms similarity-based
strategies. Our findings suggest that Latent Replay enables efficient continual
learning for generative AI models, paving the way for personalized
text-to-image models that evolve with user needs without excessive
computational costs.

</details>


### [190] [Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts](https://arxiv.org/abs/2509.10530)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Jie ji*

Main category: cs.LG

TL;DR: 本文提出DASG-MoE模型，通过分组多头注意力、双尺度共享专家结构和分层自适应动态路由机制，显著提升了MoE模型在长序列建模中的计算效率、长距离依赖捕获能力及专家资源动态适应性，并超越了现有最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MoE架构的Transformer模型在长序列建模中存在计算效率低、长距离依赖捕获能力不足以及专家资源动态分配适应性差的问题。

Method: 提出DASG-MoE混合模型，集成了三个模块：1) **分组多头注意力(GMHA)** 机制，通过序列分组、局部滑动窗口注意力和特征聚合降低计算复杂度并解决长距离依赖问题；2) **双尺度共享专家结构(DSSE)**，利用浅层专家快速处理低维特征，深层专家处理高维复杂语义，平衡效率与准确性；3) **分层自适应动态路由(ADR)** 机制，根据特征复杂度和任务需求动态选择专家层级并优化资源分配。

Result: 在多个长序列基准数据集上的实验结果表明，DASG-MoE模型优于现有的最先进模型。

Conclusion: DASG-MoE模型通过创新性的模块设计，有效提升了长序列建模能力，解决了现有MoE模型的效率和长距离依赖问题，并通过动态适应性专家资源分配实现了卓越的性能。

Abstract: Transformer models based on the Mixture of Experts (MoE) architecture have
made significant progress in long-sequence modeling, but existing models still
have shortcomings in computational efficiency and the ability to capture
long-range dependencies, especially in terms of the dynamic adaptability of
expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared
Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance
long-sequence modeling capabilities by integrating three modules. First, we
employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce
the computational complexity of long sequences. By parallel processing through
sequence grouping, local sliding window attention, and feature aggregation, we
address long-range dependency issues and the model's lack of generalization for
local information. Second, we design a Dual-Scale Shared Expert Structure
(DSSE), where shallow experts use lightweight computations to quickly respond
to low-dimensional features, while deep experts process high-dimensional
complex semantics through pre-training transfer and post-training optimization,
achieving a dynamic balance between efficiency and accuracy. Third, we propose
a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically
selects expert levels based on feature complexity and task requirements, and
optimizes resource allocation through a local expert activation strategy.
Experiments on multiple long-sequence benchmark datasets demonstrate that our
DASG-MoE model outperforms state-of-the-art models.

</details>


### [191] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 为解决传统深度强化学习(DRL)投资组合优化方法忽略新投资机会的问题，本研究提出一种双DRL代理模型，整合现有资产配置与新机会探索，并在真实市场数据上显示出优越性能。


<details>
  <summary>Details</summary>
Motivation: 大多数基于深度强化学习(DRL)的投资组合优化方法局限于预定义的投资范围，忽略了探索新投资机会，这限制了它们在不断变化的市场中提升投资组合性能的能力。

Method: 本研究引入了一个投资环境，整合了对现有资产的利用和在扩展范围中探索新投资机会。该方法利用两个DRL代理：一个负责现有投资范围内的资产配置，另一个协助探索扩展范围中的新机会。这两个代理动态平衡目标以适应市场。

Result: 在两个真实市场数据集上的实验结果表明，所提出的方法优于现有的最先进投资组合策略和基线方法。

Conclusion: 本研究提出的DRL方法通过整合对现有资产的利用和对新机会的探索，有效解决了投资组合优化中忽略新机会的局限性，显著提升了投资组合性能，并表现出优于现有方法的优势。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [192] [Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction](https://arxiv.org/abs/2509.11713)
*Yuqian Wu,Yuhong Peng,Jiapeng Yu,Xiangyu Liu,Zeting Yan,Kang Lin,Weifeng Su,Bingqing Qu,Raymond Lee,Dingqi Yang*

Main category: cs.LG

TL;DR: 针对下一地点预测中周期性与混沌性移动模式动态不平衡以及上下文线索利用不足的问题，本文提出了CANOE模型。该模型引入混沌神经振荡注意力机制和多模态上下文融合的编解码器，显著提升了预测性能，并对不同混沌程度的轨迹表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在下一地点预测中面临两大挑战：一是无法有效处理周期性和混沌性移动模式间的动态不平衡，导致对稀疏轨迹适应性不足；二是对上下文线索（如到达时间规律）利用不足，这些线索即使在混沌模式中也具有更强的可预测性。

Method: 本文提出了CANOE（Chaotic Neural Oscillator Network）模型。该模型引入了受生物启发的混沌神经振荡注意力机制，为传统注意力注入自适应变异性，以平衡表示演变的移动行为。同时，模型采用一个三对交互编码器（Tri-Pair Interaction Encoder）和一个跨上下文注意力解码器（Cross Context Attentive Decoder），在一个联合框架中融合“谁-何时-何地”等多模态上下文信息，以增强预测性能。

Result: 在两个真实世界数据集上的广泛实验表明，CANOE持续且显著优于一系列最先进的基线模型，性能提升了3.17%至13.11%。特别值得注意的是，CANOE能够对不同移动混沌水平的轨迹进行鲁棒预测。一系列消融研究也支持了模型关键设计选择的有效性。

Conclusion: CANOE通过创新的混沌神经振荡注意力机制和多模态上下文融合方法，成功解决了下一地点预测中动态移动模式处理和上下文线索利用不足的问题，从而显著提升了预测性能，并确保了对不同移动混沌程度轨迹的鲁棒预测能力。

Abstract: Next location prediction is a key task in human mobility analysis, crucial
for applications like smart city resource allocation and personalized
navigation services. However, existing methods face two significant challenges:
first, they fail to address the dynamic imbalance between periodic and chaotic
mobile patterns, leading to inadequate adaptation over sparse trajectories;
second, they underutilize contextual cues, such as temporal regularities in
arrival times, which persist even in chaotic patterns and offer stronger
predictability than spatial forecasts due to reduced search spaces. To tackle
these challenges, we propose \textbf{\method}, a
\underline{\textbf{C}}h\underline{\textbf{A}}otic \underline{\textbf{N}}eural
\underline{\textbf{O}}scillator n\underline{\textbf{E}}twork for next location
prediction, which introduces a biologically inspired Chaotic Neural Oscillatory
Attention mechanism to inject adaptive variability into traditional attention,
enabling balanced representation of evolving mobility behaviors, and employs a
Tri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to
fuse multimodal ``who-when-where'' contexts in a joint framework for enhanced
prediction performance. Extensive experiments on two real-world datasets
demonstrate that CANOE consistently and significantly outperforms a sizeable
collection of state-of-the-art baselines, yielding 3.17\%-13.11\% improvement
over the best-performing baselines across different cases. In particular, CANOE
can make robust predictions over mobility trajectories of different mobility
chaotic levels. A series of ablation studies also supports our key design
choices. Our code is available at: https://github.com/yuqian2003/CANOE.

</details>


### [193] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: 分析指出RoPE中内容与位置信息纠缠，损害Transformer性能。本文提出PoPE，有效分离二者，在多领域任务上超越RoPE，并展现出优异的零样本长度外推能力。


<details>
  <summary>Details</summary>
Motivation: 流行的RoPE旋转位置编码中，内容（what）和位置（where）信息存在纠缠。这种纠缠在需要独立匹配这些因素时，会损害Transformer模型的性能。

Method: 提出一种名为极坐标位置嵌入（PoPE）的RoPE改进方案，旨在消除内容与位置信息的混淆，从而解耦二者。

Result: PoPE在诊断任务上表现显著优于RoPE；在音乐、基因组和自然语言等领域的自回归序列建模中，使用PoPE的Transformer在评估损失（困惑度）和下游任务性能上均优于使用RoPE的基线模型。这些性能提升在不同模型规模（124M至774M参数）上依然存在。尤为重要的是，PoPE展现出强大的零样本长度外推能力，而RoPE在未经微调或位置插值的情况下，处理长序列时性能会显著下降。

Conclusion: PoPE通过解耦Transformer注意力机制中的内容和位置信息，有效提升了模型在多种序列建模任务上的性能，并解决了RoPE在长序列外推能力上的关键局限性。

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [194] [Semantic-guided LoRA Parameters Generation](https://arxiv.org/abs/2509.10535)
*Miaoge Li,Yang Chen,Zhijie Rao,Can Jiang,Jingcai Guo*

Main category: cs.LG

TL;DR: 提出SG-LoRA框架，无需用户数据或额外训练，通过任务描述生成用户特定LoRA参数，解决边缘AI个性化与隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA难以满足边缘用户个性化偏好和应对域漂移；为每个用户重新训练模型成本高昂且存在隐私风险。

Method: SG-LoRA使用任务描述作为语义桥梁，在共享嵌入空间中衡量其与已知专家任务的距离，并基于此建模目标任务的LoRA参数分布，从而为新任务生成高性能参数。

Result: 在多个挑战性任务上进行了广泛实验，证实SG-LoRA具有卓越的性能和显著的适应性。

Conclusion: SG-LoRA通过知识蒸馏，实现了实时、个性化且隐私保护的LoRA模型构建，为零样本开放世界下的模型适应提供了有效方案。

Abstract: Low-Rank Adaptation (LoRA) has demonstrated strong generalization
capabilities across a variety of tasks for efficiently fine-tuning AI models,
especially on resource-constrained edges. However, in real-world applications,
edge users often exhibit task-specific preferences that are difficult to handle
with a unified model trained under a closed-world assumption, and the challenge
may further increase when there are significant domain shifts between training
and deployment. Meanwhile, retraining/fine-tuning models for each user is also
impractical due to its cost-intensive nature and privacy concerns over raw data
utilization from edges. To address these challenges, we propose Semantic-guided
LoRA Parameter Generation (SG-LoRA), the first of its kind framework to
efficiently produce user-specific LoRA parameters without any additional
training on user tasks or access to user-specific data. Concretely, SG-LoRA
uses task descriptions as the semantic bridge, measuring their proximity to a
set of known expert tasks in a shared embedding space. Based on this semantic
guidance, it models the target task's LoRA parameter distribution to generate
high-performing parameters for novel tasks. SG-LoRA enables the real-time
construction of LoRA models aligned with individual intents by distilling
knowledge from prominent LoRA experts and, meanwhile, offering a
privacy-preserving solution for personalized model adaptation in a novel
zero-shot open-world setting proposed in this work. Extensive experiments on
multiple challenging tasks confirm the superior performance and remarkable
adaptability of SG-LoRA. Code is available at
https://github.com/keepgoingjkg/SG-LoRA.

</details>


### [195] [Contextuality, Holonomy and Discrete Fiber Bundles in Group-Valued Boltzmann Machines](https://arxiv.org/abs/2509.10536)
*Jean-Pierre Magnot*

Main category: cs.LG

TL;DR: 提出一种几何扩展的受限玻尔兹曼机（RBMs），允许权重为抽象群值，并引入一个基于群值完整性的“语境性指数”来量化复杂结构中的全局不一致性或曲率，为AI领域开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有受限玻尔兹曼机（RBMs）难以有效建模复杂的关联结构，如投影变换、旋量动力学和函数对称性，需要一种更强大的框架来处理这些复杂性。

Method: 1. 几何拓展RBMs，允许权重取抽象群（如GLn(R), SU(2)）的值。2. 引入基于RBM图中循环计算的群值完整性（holonomies）的“语境性指数”，以量化局部权重诱导的全局不一致性或“曲率”。3. 建立与层论语境性、规范场论和非交换几何的联系。4. 提供数值和图示示例进行验证。

Result: 1. 成功建模了复杂的关联结构，包括投影变换、旋量动力学和函数对称性。2. 语境性指数有效量化了全局不一致性或“曲率”，并泛化了经典的一致性概念。3. 通过有限和无限维度的数值及图示示例验证了该框架的有效性。

Conclusion: 该框架为AI领域开辟了新方向，包括开发曲率感知学习架构和在不确定或对抗环境中实现拓扑正则化，为理解和量化复杂系统中的全局不一致性提供了新工具。

Abstract: We propose a geometric extension of restricted Boltzmann machines (RBMs) by
allowing weights to take values in abstract groups such as \(
\mathrm{GL}_n(\mathbb{R}) \), \( \mathrm{SU}(2) \), or even
infinite-dimensional operator groups. This generalization enables the modeling
of complex relational structures, including projective transformations, spinor
dynamics, and functional symmetries, with direct applications to vision,
language, and quantum learning.
  A central contribution of this work is the introduction of a
\emph{contextuality index} based on group-valued holonomies computed along
cycles in the RBM graph. This index quantifies the global inconsistency or
"curvature" induced by local weights, generalizing classical notions of
coherence, consistency, and geometric flatness. We establish links with
sheaf-theoretic contextuality, gauge theory, and noncommutative geometry, and
provide numerical and diagrammatic examples in both finite and infinite
dimensions.
  This framework opens novel directions in AI, from curvature-aware learning
architectures to topological regularization in uncertain or adversarial
environments.

</details>


### [196] [On Using Large-Batches in Federated Learning](https://arxiv.org/abs/2509.10537)
*Sahil Tyagi*

Main category: cs.LG

TL;DR: 联邦学习中，大批量训练虽能加速，但泛化性差。本文提出一种新的大批量训练技术，在保持并行优势的同时，显著提升了模型泛化能力，在ResNet50和VGG11模型上均获得了更高的测试精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习对于资源受限设备和处理多模态大数据（尤其重视隐私和局部性）至关重要。现有联邦学习算法在并行性能与统计性能之间存在权衡。大批量训练虽可加速，但常因泛化能力下降而导致测试性能不佳。本研究旨在解决大批量联邦学习的泛化能力退化问题。

Method: 本文提出了一种利用小批量和大批量训练之间权衡的理念，探索新方向以同时实现大批量训练的并行扩展能力和小批量训练的良好泛化能力。

Result: 在相同迭代次数下，所提出的大批量训练技术比小批量训练取得了显著更高的测试精度。具体而言，在ResNet50模型上测试精度提高了约32.33%，在VGG11模型上测试精度提高了约3.74%。

Conclusion: 本研究成功地应对了大批量联邦学习的泛化能力下降挑战。它证明了可以同时享受大批量训练的并行扩展优势和良好泛化能力，从而提升了联邦学习模型的性能。

Abstract: Efficient Federated learning (FL) is crucial for training deep networks over
devices with limited compute resources and bounded networks. With the advent of
big data, devices either generate or collect multimodal data to train either
generic or local-context aware networks, particularly when data privacy and
locality is vital. FL algorithms generally trade-off between parallel and
statistical performance, improving model quality at the cost of higher
communication frequency, or vice versa. Under frequent synchronization
settings, FL over a large cluster of devices may perform more work per-training
iteration by processing a larger global batch-size, thus attaining considerable
training speedup. However, this may result in poor test performance (i.e., low
test loss or accuracy) due to generalization degradation issues associated with
large-batch training. To address these challenges with large-batches, this work
proposes our vision of exploiting the trade-offs between small and large-batch
training, and explore new directions to enjoy both the parallel scaling of
large-batches and good generalizability of small-batch training. For the same
number of iterations, we observe that our proposed large-batch training
technique attains about 32.33% and 3.74% higher test accuracy than small-batch
training in ResNet50 and VGG11 models respectively.

</details>


### [197] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: DualAlign框架通过结合统计对齐和语义对齐，改进了合成临床数据的生成，使其更具真实性和临床意义，从而提高了低资源临床文本分析中AI模型的性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于真实世界电子健康记录（EHRs）严格的隐私限制、稀有疾病标注数据有限以及观测数据中的系统性偏差，合成临床数据在医疗AI发展中日益重要。然而，尽管大型语言模型能生成流畅的临床文本，但生成既真实又具临床意义的合成数据仍是挑战。

Method: 引入DualAlign框架，通过双重对齐增强统计忠实度和临床合理性：1) 统计对齐，根据患者人口统计学和风险因素进行生成条件化；2) 语义对齐，整合真实世界症状轨迹以指导内容生成。以阿尔茨海默病为例进行研究。

Result: DualAlign能生成基于上下文的症状级语句，更好地反映真实世界的临床文档。使用DualAlign生成的合成数据和人工标注数据组合对LLaMA 3.1-8B模型进行微调，相较于仅用黄金数据或无引导的合成基线训练的模型，性能有显著提升。

Conclusion: DualAlign为生成临床真实、保护隐私的合成数据提供了一种实用方法，以支持低资源临床文本分析，尽管它未能完全捕捉纵向复杂性。

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [198] [GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python](https://arxiv.org/abs/2509.10560)
*Xuechen Liang,Xiaoxing He,Shengdao Wang,Jean-Philippe Montillet,Zhengkai Huang,Gaël Kermarrec,Shunqiang Hu,Yu Zhou,Jiahui Huang*

Main category: cs.LG

TL;DR: 提出并介绍了GTS Forecaster，一个开源Python包，利用深度学习模型（如KAN、GNNGRU、TimeGNN）和预处理工具（如KTIF）来准确预测非线性、非平稳的测地时间序列，如GNSS、SSH和TG数据。


<details>
  <summary>Details</summary>
Motivation: 测地时间序列（如GNSS、SSH、TG）对于监测地表形变和海平面变化至关重要，其准确预测能增强预警系统并支持灾害缓解。然而，这些数据的非线性、非平稳和不完整性对传统模型构成挑战，导致它们难以捕捉长期依赖和复杂的时空动态。

Method: 引入了GTS Forecaster开源Python包，该包集成了先进的深度学习模型（包括核注意力网络KAN、基于图神经网络的门控循环单元GNNGRU、时间感知图神经网络TimeGNN）以有效建模非线性时空模式。此外，它还提供了强大的预处理工具，如异常值检测和基于强化学习的缺失值填充算法（Kalman-TransFusion Interpolation Framework, KTIF）。

Result: GTS Forecaster目前支持对GNSS、SSH和TG数据集进行预测、可视化和评估，并且可以适用于一般的时序应用。

Conclusion: GTS Forecaster通过结合前沿模型和易用界面，促进了深度学习在测地预测任务中的应用。

Abstract: Geodetic time series -- such as Global Navigation Satellite System (GNSS)
positions, satellite altimetry-derived sea surface height (SSH), and tide gauge
(TG) records -- is essential for monitoring surface deformation and sea level
change. Accurate forecasts of these variables can enhance early warning systems
and support hazard mitigation for earthquakes, landslides, coastal storm surge,
and long-term sea level. However, the nonlinear, non-stationary, and incomplete
nature of such variables presents significant challenges for classic models,
which often fail to capture long-term dependencies and complex spatiotemporal
dynamics. We introduce GTS Forecaster, an open-source Python package for
geodetic time series forecasting. It integrates advanced deep learning models
-- including kernel attention networks (KAN), graph neural network-based gated
recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to
effectively model nonlinear spatial-temporal patterns. The package also
provides robust preprocessing tools, including outlier detection and a
reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion
Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting,
visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable
to general time series applications. By combining cutting-edge models with an
accessible interface, it facilitates the application of deep learning in
geodetic forecasting tasks.

</details>


### [199] [SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs](https://arxiv.org/abs/2509.10594)
*Iqbal H. Sarker,Helge Janicke,Ahmad Mohsin,Leandros Maglaras*

Main category: cs.LG

TL;DR: 本文提出了一个结构化框架，旨在解决中小企业（SMEs）在采纳AI/LLMs时面临的技术、伦理和信任问题，通过在AI生命周期中嵌入信任和伦理原则，促进其负责任使用。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）和大型语言模型（LLMs）正在重塑商业实践，但中小企业在采纳这些技术时面临显著的技术、伦理和信任挑战。

Method: 本文提出了一个结构化、多阶段的框架，旨在将信任和伦理原则嵌入AI生命周期中。该框架围绕数据、算法、人工监督和模型架构四大支柱构建，旨在将理论伦理原则与运营实践相结合。

Result: 该框架能够弥合理论伦理原则与操作实践之间的鸿沟，提升AI在中小企业多种应用中的能力，并为负责任的AI采纳提供了一个结构化的路线图。

Conclusion: 信任和伦理是中小企业在AI时代实现韧性、竞争力以及可持续创新的催化剂。

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping
today's business practices, however, their adoption within small and
medium-sized enterprises (SMEs) raises significant technical, ethical and trust
issues. This paper proposes a structured, multi-phased framework designed to
embed trust and ethical principles throughout the AI lifecycle for their secure
and responsible use in SMEs. Structured around four pillars, i.e., Data,
Algorithms, Human oversight, and Model Architecture, the framework bridges
theoretical ethical principles with operational practice, enhancing AI
capabilities in diverse SME applications. Ultimately, this paper offers a
structured roadmap for responsible AI adoption, framing trust and ethics as a
catalyst for resilience, competitiveness, and sustainable innovation in SMEs.

</details>


### [200] [pySigLib -- Fast Signature-Based Computations on CPU and GPU](https://arxiv.org/abs/2509.10613)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: 提出pySigLib库，高效实现签名和签名核计算，并引入新颖的微分方案，解决现有方法在处理大规模序列数据时的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的签名核实现在处理实际数据集大小和序列长度时无法有效扩展，限制了签名方法在机器学习中的应用。

Method: 开发了pySigLib，一个高性能Python库，提供在CPU和GPU上优化的签名和签名核实现，并与PyTorch的自动微分完全兼容。此外，引入了一种新颖的签名核微分方案。

Result: pySigLib为大规模基于签名的计算提供了一个高效的软件栈。所提出的新颖微分方案能以远低于现有库的运行时间提供准确的梯度。

Conclusion: pySigLib通过优化实现和创新的微分方案，显著提升了签名及签名核在处理大规模序列数据时的计算效率和梯度精度，解决了现有方法的扩展性瓶颈。

Abstract: Signature-based methods have recently gained significant traction in machine
learning for sequential data. In particular, signature kernels have emerged as
powerful discriminators and training losses for generative models on
time-series, notably in quantitative finance. However, existing implementations
do not scale to the dataset sizes and sequence lengths encountered in practice.
We present pySigLib, a high-performance Python library offering optimised
implementations of signatures and signature kernels on CPU and GPU, fully
compatible with PyTorch's automatic differentiation. Beyond an efficient
software stack for large-scale signature-based computation, we introduce a
novel differentiation scheme for signature kernels that delivers accurate
gradients at a fraction of the runtime of existing libraries.

</details>


### [201] [Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices](https://arxiv.org/abs/2509.10626)
*Georgiy A. Bondar,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文提出在所有图结构上寻找最优多边际薛定谔桥（MSB）的问题，并证明其归结为度量值顶点的最小生成树问题，提供两步求解算法。


<details>
  <summary>Details</summary>
Motivation: 传统的多边际薛定谔桥（MSB）需要预先指定相关结构（图）。本文旨在克服这一限制，通过在所有可能的图结构上优化来寻找最优MSB，从而实现最佳耦合。

Method: 将寻找所有可能图结构上最优MSB的问题等价为解决度量值顶点的最小生成树问题。具体方法分两步：1. 构建一个完全图，其中边权重为对应双边际SB最优值与端点熵的和。2. 在此完全加权图上求解标准的最小生成树问题。通过数值实验验证了所提出的解决方案。

Result: 发现计算在所有图结构上最优的多边际薛定谔桥等价于解决度量值顶点的最小生成树问题。提出了一个分两步的算法来解决此问题。

Conclusion: 成功地公式化并解决了通过优化底层图结构来寻找最优多边际薛定谔桥的问题。研究表明该问题可以通过将其简化为标准最小生成树问题来高效解决，并提供了一个实用的两步算法。

Abstract: The Multimarginal Schr\"odinger Bridge (MSB) finds the optimal coupling among
a collection of random vectors with known statistics and a known correlation
structure. In the MSB formulation, this correlation structure is specified
\emph{a priori} as an undirected connected graph with measure-valued vertices.
In this work, we formulate and solve the problem of finding the optimal MSB in
the sense we seek the optimal coupling over all possible graph structures. We
find that computing the optimal MSB amounts to solving the minimum spanning
tree problem over measure-valued vertices. We show that the resulting problem
can be solved in two steps. The first step constructs a complete graph with
edge weight equal to a sum of the optimal value of the corresponding bimarginal
SB and the entropies of the endpoints. The second step solves a standard
minimum spanning tree problem over that complete weighted graph. Numerical
experiments illustrate the proposed solution.

</details>


### [202] [Interpretable neural network system identification method for two families of second-order systems based on characteristic curves](https://arxiv.org/abs/2509.10632)
*Federico J. Gonzalez,Luis P. Lara*

Main category: cs.LG

TL;DR: 提出一种基于特征曲线（CCs）的统一数据驱动框架，结合微分方程结构与神经网络灵活性，尤其NN-CC方法在处理复杂非线性和不连续系统时，能有效平衡模型可解释性与识别能力。


<details>
  <summary>Details</summary>
Motivation: 非线性系统辨识中，可解释性与灵活性存在根本性权衡，且通常需要结合物理约束。

Method: 提出一个统一数据驱动框架，核心概念是特征曲线（CCs），每个CC由一个专用神经网络建模，以模块化和可解释的方式表示系统方程。为验证其通用性，引入三种辨识策略：SINDy-CC（扩展SINDy引入方程结构约束）、Poly-CC（使用高阶多项式表示CCs）和NN-CC（直接使用NNs而无需先验基函数假设）。

Result: 对于简单多项式非线性系统（如范德波尔振子），三种方法均适用；而NN-CC在建模复杂非线性及不连续系统（如粘滑系统）时表现出卓越性能。

Conclusion: 基于CC的框架，特别是NN-CC方法，通过显式表示CCs，能够捕获复杂非线性并同时保持可解释性。这使其非常适用于建模传统方法难以处理的、具有不连续性和复杂非线性的系统，为非线性系统辨识提供强大工具。

Abstract: Nonlinear system identification often involves a fundamental trade-off
between interpretability and flexibility, often requiring the incorporation of
physical constraints. We propose a unified data-driven framework that combines
the mathematical structure of the governing differential equations with the
flexibility of neural networks (NNs). At the core of our approach is the
concept of characteristic curves (CCs), which represent individual nonlinear
functions (e.g., friction and restoring components) of the system. Each CC is
modeled by a dedicated NN, enabling a modular and interpretable representation
of the system equation. To demonstrate the versatility of the CC-based
formalism, we introduce three identification strategies: (1) SINDy-CC, which
extends the sparse regression approach of SINDy by incorporating the
mathematical structure of the governing equations as constraints; (2) Poly-CC,
which represents each CC using high-degree polynomials; and (3) NN-CC, which
uses NNs without requiring prior assumptions about basis functions. Our results
show that all three approaches are well-suited for systems with simple
polynomial nonlinearities, such as the van der Pol oscillator. In contrast,
NN-CC demonstrates superior performance in modeling systems with complex
nonlinearities and discontinuities, such as those observed in stick-slip
systems. The key contribution of this work is to demonstrate that the CC-based
framework, particularly the NN-CC approach, can capture complex nonlinearities
while maintaining interpretability through the explicit representation of the
CCs. This balance makes it well-suited for modeling systems with
discontinuities and complex nonlinearities that are challenging to assess using
traditional polynomial or sparse regression methods, providing a powerful tool
for nonlinear system identification.

</details>


### [203] [Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning](https://arxiv.org/abs/2509.10635)
*Ali Burak Ünal,Cem Ata Baykara,Peter Krawitz,Mete Akgün*

Main category: cs.LG

TL;DR: 本文提出一个联邦GestaltMatcher服务，通过联邦学习在不共享患者数据的情况下，实现面部畸形诊断的跨机构协作，并保持高诊断性能。


<details>
  <summary>Details</summary>
Motivation: GestaltMatcher在面部畸形诊断中表现出色，但其对集中式数据集的依赖因患者数据分散和隐私法规而受限，阻碍了进一步发展。

Method: 引入基于跨机构横向联邦学习框架的联邦GestaltMatcher服务。该服务允许医院在不共享患者图像的情况下，协作训练全局集成特征提取器，将患者数据映射到共享潜在空间，并采用隐私保护的核矩阵计算框架进行综合征推断和发现。

Result: 实验表明，该联邦服务保留了超过90%的集中式性能，并对不同数量的数据孤岛和异构数据分布都表现出良好的稳健性。

Conclusion: 该联邦服务成功解决了面部畸形诊断中数据隐私和多机构协作的挑战，通过联邦学习实现了高效且隐私保护的诊断模型共享与更新，同时保持了高水平的性能。

Abstract: Machine learning has shown promise in facial dysmorphology, where
characteristic facial features provide diagnostic clues for rare genetic
disorders. GestaltMatcher, a leading framework in this field, has demonstrated
clinical utility across multiple studies, but its reliance on centralized
datasets limits further development, as patient data are siloed across
institutions and subject to strict privacy regulations. We introduce a
federated GestaltMatcher service based on a cross-silo horizontal federated
learning framework, which allows hospitals to collaboratively train a global
ensemble feature extractor without sharing patient images. Patient data are
mapped into a shared latent space, and a privacy-preserving kernel matrix
computation framework enables syndrome inference and discovery while
safeguarding confidentiality. New participants can directly benefit from and
contribute to the system by adopting the global feature extractor and kernel
configuration from previous training rounds. Experiments show that the
federated service retains over 90% of centralized performance and remains
robust to both varying silo numbers and heterogeneous data distributions.

</details>


### [204] [Test-Time Warmup for Multimodal Large Language Models](https://arxiv.org/abs/2509.10641)
*Nikita Rajaneesh,Thomas Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 针对多模态大语言模型（MLLMs）因有限微调数据导致复杂推理任务性能不足的问题，本文提出了一种“测试时热身（Test-Time Warmup）”方法。该方法通过利用弱监督辅助任务数据在每个测试实例推理前进行适应性调整，无需大量标注数据即可显著提升MLLMs在多个推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在文本和图像交叉推理方面潜力巨大，但尚未完全实现。尽管其各组件在海量数据上预训练，整个多模态模型却只在少量样本上训练，导致在复杂推理任务上表现不佳。

Method: 提出了一种“测试时热身（Test-Time Warmup）”方法。该方法不依赖于大量标注数据集进行微调，而是通过利用弱监督辅助任务的数据，对每个测试实例的MLLM进行适应性调整。

Result: 在Llama-Vision-Instruct模型上，该方法使MMMU数据集的性能相对提升4.03%，VQA-Rad提升5.28%，GQA提升1.63%。

Conclusion: 该方法证明了在推理前进行“热身”能够增强多模态大语言模型（MLLMs）在各种推理任务中的鲁棒性。

Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced
reasoning at the intersection of text and images, yet they have not fully
realized this potential. MLLMs typically integrate an LLM, a vision encoder,
and a connector that maps the vision encoder's embeddings into the LLM's text
embedding space. Although each component is pretrained on massive datasets with
billions of samples, the entire multimodal model is typically trained on only
thousands (or a few million) samples, which can result in weak performance on
complex reasoning tasks. To address these shortcomings, instead of relying on
extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup
method that adapts the MLLM per test instance by leveraging data from weakly
supervised auxiliary tasks. With our approach, we observe a relative
performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on
the Llama-Vision-Instruct model. Our method demonstrates that 'warming up'
before inference can enhance MLLMs' robustness across diverse reasoning tasks.

</details>


### [205] [Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration](https://arxiv.org/abs/2509.10656)
*Chirayu Nimonkar,Shlok Shah,Catherine Ji,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本研究利用自监督目标达成技术，简化多智能体协作任务的指定（通过单一目标状态），并在稀疏奖励下实现卓越性能，并促成涌现的协作和探索行为。


<details>
  <summary>Details</summary>
Motivation: 为多智能体群体的协调和长时序推理设计有效的奖励函数极具挑战性。

Method: 本文提出利用自监督目标达成技术，使智能体旨在最大化访问某个特定目标状态的可能性，而非最大化标量奖励。这种方法允许用户通过单一目标状态指定任务。

Result: 在多智能体强化学习（MARL）基准测试中，所提方法优于使用相同稀疏奖励信号的替代方法。尽管没有明确的探索机制，自监督多智能体目标达成仍能在其他方法无法成功一次的场景中，促成涌现的协作和探索行为。

Conclusion: 自监督目标达成是促进多智能体协作的有效方法，它能有效应对稀疏反馈，简化任务指定，并能激发智能体在复杂环境中涌现出协作和探索能力。

Abstract: For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

</details>


### [206] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: M4GN是一种三层分段中心的分层GNN，通过混合分割策略和GNN-Transformer混合架构，显著提高了网格PDE模拟的精度和推理速度，解决了现有GNN的成本高、过平滑及分层GNN的粗粒度图构建和精度维持难题。


<details>
  <summary>Details</summary>
Motivation: 网格GNN在大型长程网格上存在深度消息传递成本高和过平滑问题。分层GNN虽能缩短传播路径，但面临两大挑战：一是如何构建尊重网格拓扑、几何和物理不连续性的粗粒度图；二是如何在加速的同时保持精细尺度精度。

Method: M4GN是一个三层分段中心的分层网络。首先，采用混合分割策略（结合快速图划分器和模态分解特征引导的超像素式细化）生成动态一致的连续节点段。其次，使用置换不变聚合器对这些段进行编码，避免了传统方法的顺序敏感性和二次成本。最后，网络结合微观层GNN捕获局部动态和宏观层Transformer进行跨段高效推理，以平衡精度和效率。

Result: 在多个代表性基准数据集上，M4GN的预测精度提高了高达56%，同时推理速度比现有最先进基线快了高达22%。

Conclusion: M4GN通过其独特的分段策略和混合网络架构，成功克服了分层GNN在网格PDE模拟中的关键挑战，实现了精度和效率的显著提升。

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [207] [Least-Ambiguous Multi-Label Classifier](https://arxiv.org/abs/2509.10689)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 针对单正多标签学习（SPMLL）的挑战，本文提出了一种模型无关的、基于共形预测的方法，以实现可靠的多标签预测，并在多个基准数据集上取得了优于现有基线的表现。


<details>
  <summary>Details</summary>
Motivation: 多标签学习通常需要完整的标签标注，但这成本高昂。许多数据集中每个实例只标注一个正标签（即SPMLL），尽管存在多个相关标签，这构成了一种极端的部分监督形式，带来了显著挑战。

Method: 提出一种模型无关的SPMLL方法，该方法利用共形预测（conformal prediction）来生成校准的集合值输出，从而在测试时实现可靠的多标签预测。该方法弥合了单标签训练与多标签评估之间的监督鸿沟，且不依赖于标签分布假设。

Result: 在12个基准数据集上进行了评估，结果显示其持续优于现有基线方法，并具有实际应用性。

Conclusion: 本研究提出了一种创新且有效的模型无关方法，利用共形预测成功解决了单正多标签学习（SPMLL）中的部分监督难题，并在多个数据集上验证了其性能优越性和实用价值。

Abstract: Multi-label learning often requires identifying all relevant labels for
training instances, but collecting full label annotations is costly and
labor-intensive. In many datasets, only a single positive label is annotated
per training instance, despite the presence of multiple relevant labels. This
setting, known as single-positive multi-label learning (SPMLL), presents a
significant challenge due to its extreme form of partial supervision. We
propose a model-agnostic approach to SPMLL that draws on conformal prediction
to produce calibrated set-valued outputs, enabling reliable multi-label
predictions at test time. Our method bridges the supervision gap between
single-label training and multi-label evaluation without relying on label
distribution assumptions. We evaluate our approach on 12 benchmark datasets,
demonstrating consistent improvements over existing baselines and practical
applicability.

</details>


### [208] [Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization](https://arxiv.org/abs/2509.10693)
*Iman Nodozi,Djordje Gligorijevic,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文提出一种针对首价拍卖的测量值优化出价策略，通过Wasserstein-proximal更新适应性调整出价参数分布，以最大化预期盈余。


<details>
  <summary>Details</summary>
Motivation: 在首价拍卖中，寻求一种有效的出价调整（bid shading）策略，以优化性能并提高预期盈余。

Method: 将出价调整策略建模为测量值优化问题。采用标准参数化形式，并将其表述为关于调整参数联合分布的凸优化问题。每次拍卖后，通过正则化的Wasserstein-proximal更新（带有数据驱动且上下文相关的能量函数）来适应性调整参数分布。该算法旨在使出价分布更侧重于具有更高预期盈余的值。

Result: 研究表明，所提出的测量值凸优化问题存在闭式解。通过数值示例验证了该方法的有效性。

Conclusion: 本工作提出了一种新颖且有效的首价拍卖出价调整策略，该策略通过测量值优化和自适应更新，能够鼓励更高的预期盈余，并且具有理论上的闭式解。

Abstract: This work proposes a bid shading strategy for first-price auctions as a
measure-valued optimization problem. We consider a standard parametric form for
bid shading and formulate the problem as convex optimization over the joint
distribution of shading parameters. After each auction, the shading parameter
distribution is adapted via a regularized Wasserstein-proximal update with a
data-driven energy functional. This energy functional is conditional on the
context, i.e., on publisher/user attributes such as domain, ad slot type,
device, or location. The proposed algorithm encourages the bid distribution to
place more weight on values with higher expected surplus, i.e., where the win
probability and the value gap are both large. We show that the resulting
measure-valued convex optimization problem admits a closed form solution. A
numerical example illustrates the proposed method.

</details>


### [209] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: Scalify是一个轻量级框架，利用等价饱和与Datalog推理，高效验证大型机器学习计算图的语义等价性，从而发现隐性错误并提供精准调试指导。


<details>
  <summary>Details</summary>
Motivation: 现代大型机器学习框架在引入并行化和优化技术支持大模型的同时，也增加了复杂性，导致难以察觉的隐性错误，严重影响模型性能。现有解决方案往往是临时性的或成本高昂，不适用于生产环境。

Method: Scalify框架通过使用等价饱和（equality saturation）和Datalog风格推理来验证计算图的语义等价性，以暴露隐性错误。为提高验证效率和规模，Scalify采用并行重写和层记忆化（layer memoization）来分割图，重用重写模板，并结合关系推理和符号双射推断来增强等价饱和。该框架还能将发现的差异定位到具体的代码位置，提供可操作的调试指导。

Result: Scalify能在普通机器上于数分钟内验证Llama-3.1-405B等大型模型，并成功在亚马逊生产机器学习框架中发现了五个未知错误。

Conclusion: Scalify提供了一个高效、可扩展且实用的解决方案，能够有效地检测大型机器学习模型中由复杂优化引起的隐性错误，并将其转化为具体的调试建议，显著提升了复杂ML系统的可靠性与调试效率。

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [210] [Kalman Bayesian Transformer](https://arxiv.org/abs/2509.10695)
*Haoming Jing,Oren Wright,José M. F. Moura,Yorie Nakahira*

Main category: cs.LG

TL;DR: 针对数据流分布变化的Transformer序列微调挑战，本文提出一种贝叶斯方法，通过量化不确定性来平衡先验知识与新数据，实现鲁棒且数据高效的学习。


<details>
  <summary>Details</summary>
Motivation: Transformer序列微调面临多重挑战：需要少量数据下稳定训练并平衡旧知识与新信息，尤其当新数据序列到达且分布发生漂移时；此外，在低延迟环境中，还需量化并利用不确定性。

Method: 提出一种新方法，将序列微调框定为贝叶斯框架下的后验推断问题。该方法整合了随机变量的闭式矩传播、卡尔曼贝叶斯神经网络以及softmax函数矩的泰勒近似，将预训练模型视为先验，并基于量化不确定性自适应地平衡先验知识与新信息。

Result: 通过显式地将预训练模型作为先验并基于量化不确定性自适应地平衡它们与新信息，该方法实现了鲁棒且数据高效的序列学习。在涉及决策Transformer对分布漂移和有限内存任务进行序列适应的数值模拟中，验证了其有效性。

Conclusion: 该贝叶斯框架通过将预训练模型作为先验并利用量化不确定性自适应地平衡新旧知识，成功解决了Transformer在分布变化和资源受限下的序列微调难题，实现了鲁棒且数据高效的学习。

Abstract: Sequential fine-tuning of transformers is useful when new data arrive
sequentially, especially with shifting distributions. Unlike batch learning,
sequential learning demands that training be stabilized despite a small amount
of data by balancing new information and previously learned knowledge in the
pre-trained models. This challenge is further complicated when training is to
be completed in latency-critical environments and learning must additionally
quantify and be mediated by uncertainty. Motivated by these challenges, we
propose a novel method that frames sequential fine-tuning as a posterior
inference problem within a Bayesian framework. Our approach integrates
closed-form moment propagation of random variables, Kalman Bayesian Neural
Networks, and Taylor approximations of the moments of softmax functions. By
explicitly accounting for pre-trained models as priors and adaptively balancing
them against new information based on quantified uncertainty, our method
achieves robust and data-efficient sequential learning. The effectiveness of
our method is demonstrated through numerical simulations involving sequential
adaptation of a decision transformer to tasks characterized by distribution
shifts and limited memory resources.

</details>


### [211] [CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction](https://arxiv.org/abs/2509.10698)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: CrunchLLM是一个领域适配的LLM框架，通过融合结构化和非结构化数据，并采用参数高效微调和提示优化，显著提高了初创公司成功预测的准确性，并提供可解释的预测依据。


<details>
  <summary>Details</summary>
Motivation: 预测初创公司成功（被收购或IPO）是创业研究中的关键问题。现有方法（传统机器学习）只依赖结构化数据且准确性一般，而大语言模型（LLM）虽有强大推理能力但难以直接适应特定领域数据，且难以有效利用异构数据（结构化与非结构化）。

Method: 提出CrunchLLM框架，该框架整合结构化公司属性与非结构化文本描述。通过应用参数高效微调（PEFT）策略和提示优化，将基础语言模型专门化用于创业数据。

Result: 在Crunchbase初创公司成功预测任务上，准确率超过80%，显著优于传统分类器和基线LLM。此外，CrunchLLM提供可解释的推理路径，增强了预测的透明度和可信度。

Conclusion: 本研究展示了通过领域适配的LLM微调和结构化-非结构化数据融合，可以有效推进创业成果的预测建模。CrunchLLM为风险投资和创新政策提供了方法论框架和实用工具。

Abstract: Predicting the success of start-up companies, defined as achieving an exit
through acquisition or IPO, is a critical problem in entrepreneurship and
innovation research. Datasets such as Crunchbase provide both structured
information (e.g., funding rounds, industries, investor networks) and
unstructured text (e.g., company descriptions), but effectively leveraging this
heterogeneous data for prediction remains challenging. Traditional machine
learning approaches often rely only on structured features and achieve moderate
accuracy, while large language models (LLMs) offer rich reasoning abilities but
struggle to adapt directly to domain-specific business data. We present
\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success
prediction. CrunchLLM integrates structured company attributes with
unstructured textual narratives and applies parameter-efficient fine-tuning
strategies alongside prompt optimization to specialize foundation models for
entrepreneurship data. Our approach achieves accuracy exceeding 80\% on
Crunchbase startup success prediction, significantly outperforming traditional
classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM
provides interpretable reasoning traces that justify its predictions, enhancing
transparency and trustworthiness for financial and policy decision makers. This
work demonstrates how adapting LLMs with domain-aware fine-tuning and
structured--unstructured data fusion can advance predictive modeling of
entrepreneurial outcomes. CrunchLLM contributes a methodological framework and
a practical tool for data-driven decision making in venture capital and
innovation policy.

</details>


### [212] [Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](https://arxiv.org/abs/2509.10729)
*Ilker Demirel,Karan Thakkar,Benjamin Elizalde,Miquel Espi Marques,Shirley Ren,Jaya Narain*

Main category: cs.LG

TL;DR: 研究表明，大语言模型（LLMs）可用于音频和运动时间序列数据的后期融合，实现零样本和单样本多模态活动分类，无需特定训练，并具有部署优势。


<details>
  <summary>Details</summary>
Motivation: 将传感器数据流（如音频和运动）中的互补信息整合用于下游活动分类具有挑战性，尤其在传统方法需要大量对齐训练数据时。

Method: 本文利用大语言模型（LLMs）对来自Ego4D数据集的音频和运动时间序列数据进行后期融合，以实现多样化活动的零样本和单样本分类。

Result: 评估显示，LLMs在12类活动的零样本和单样本分类中取得了远超随机水平的F1分数，且未进行任何任务特定的训练。

Conclusion: 基于LLM的融合方法能够支持在对齐训练数据有限的多模态时序应用，并且在模型部署时无需额外的内存和计算资源，简化了应用特定多模态模型的部署。

Abstract: Sensor data streams provide valuable information around activities and
context for downstream applications, though integrating complementary
information can be challenging. We show that large language models (LLMs) can
be used for late fusion for activity classification from audio and motion time
series data. We curated a subset of data for diverse activity recognition
across contexts (e.g., household activities, sports) from the Ego4D dataset.
Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores
significantly above chance, with no task-specific training. Zero-shot
classification via LLM-based fusion from modality-specific models can enable
multimodal temporal applications where there is limited aligned training data
for learning a shared embedding space. Additionally, LLM-based fusion can
enable model deploying without requiring additional memory and computation for
targeted application-specific multimodal models.

</details>


### [213] [Matched-Pair Experimental Design with Active Learning](https://arxiv.org/abs/2509.10742)
*Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: 针对匹配对实验，提出一种主动学习框架，以顺序、经济地识别并招募高治疗效果区域的患者，降低成本并确保识别的完整性。


<details>
  <summary>Details</summary>
Motivation: 在匹配对实验中，当整体治疗效果量较小时，传统方法难以有效识别并聚焦于干预措施最有效的高治疗效果区域。

Method: 提出一种匹配对实验设计，将高治疗效果区域的识别问题框架化为分类问题，并开发了针对匹配对设计的主动学习框架，以顺序、主动地招募患者。

Result: 该设计能显著降低检测治疗效果的实验成本，并确保识别出的区域能完全覆盖所有高治疗效果区域。理论分析的标签复杂度和实际场景实验均验证了其效率和优势。

Conclusion: 所提出的主动学习匹配对设计能够高效、经济地识别并定位高治疗效果区域，从而优化实验成本并提升干预措施的有效性。

Abstract: Matched-pair experimental designs aim to detect treatment effects by pairing
participants and comparing within-pair outcome differences. In many situations,
the overall effect size is small across the entire population. Then, the focus
naturally shifts to identifying and targeting high treatment-effect regions
where the intervention is most effective. This paper proposes a matched-pair
experimental design that sequentially and actively enrolls patients in high
treatment-effect regions. Importantly, we frame the identification of the
target region as a classification problem and propose an active learning
framework tailored to matched-pair designs. The proposed design not only
reduces the experimental cost of detecting treatment efficacy, but also ensures
that the identified regions enclose the entire high-treatment-effect regions.
Our theoretical analysis of the framework's label complexity, along with
experiments in practical scenarios, demonstrates the efficiency and advantages
of the approach.

</details>


### [214] [HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling](https://arxiv.org/abs/2509.10753)
*Minh Vu,Brian K. Tran,Syed A. Shah,Geigh Zollicoffer,Nhat Hoang-Xuan,Manish Bhattarai*

Main category: cs.LG

TL;DR: 本文提出HalluField，一种基于场论和热力学的LLM幻觉检测方法。它通过分析响应的语义稳定性，识别能量景观中的不稳定行为来检测幻觉。该方法无需微调，高效实用，并实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常产生不准确或不可靠的幻觉内容，这严重限制了其在关键应用中的部署。因此，迫切需要一种通用的LLM幻觉检测方法。

Method: 引入HalluField，一种基于参数化变分原理和热力学的场论方法。它将LLM响应建模为离散的似然token路径集合，每条路径关联能量和熵。通过分析能量和熵分布如何随温度和似然变化，量化响应的语义稳定性。通过识别能量景观中的不稳定或异常行为来检测幻觉。该方法直接操作模型输出logits，无需微调或辅助神经网络，并具有物理学解释。

Result: 通过这种物理视角建模LLM行为，HalluField在不同模型和数据集上实现了最先进（state-of-the-art）的幻觉检测性能。

Conclusion: HalluField提供了一种高效、实用且基于物理原理的通用幻觉检测方法，有效解决了LLM幻觉问题，并达到了业界领先水平。

Abstract: Large Language Models (LLMs) exhibit impressive reasoning and
question-answering capabilities. However, they often produce inaccurate or
unreliable content known as hallucinations. This unreliability significantly
limits their deployment in high-stakes applications. Thus, there is a growing
need for a general-purpose method to detect hallucinations in LLMs. In this
work, we introduce HalluField, a novel field-theoretic approach for
hallucination detection based on a parametrized variational principle and
thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response
to a given query and temperature setting as a collection of discrete likelihood
token paths, each associated with a corresponding energy and entropy. By
analyzing how energy and entropy distributions vary across token paths under
changes in temperature and likelihood, HalluField quantifies the semantic
stability of a response. Hallucinations are then detected by identifying
unstable or erratic behavior in this energy landscape. HalluField is
computationally efficient and highly practical: it operates directly on the
model's output logits without requiring fine-tuning or auxiliary neural
networks. Notably, the method is grounded in a principled physical
interpretation, drawing analogies to the first law of thermodynamics.
Remarkably, by modeling LLM behavior through this physical lens, HalluField
achieves state-of-the-art hallucination detection performance across models and
datasets.

</details>


### [215] [Contextual Budget Bandit for Food Rescue Volunteer Engagement](https://arxiv.org/abs/2509.10777)
*Ariana Tang,Naveen Raman,Fei Fang,Zheyuan Ryan Shi*

Main category: cs.LG

TL;DR: 本文提出Contextual Budget Bandit及Mitosis算法，以解决食物救援平台在维持志愿者参与、最大化食物救援量以及缓解现有算法加剧的地理不均问题。


<details>
  <summary>Details</summary>
Motivation: 现有食物救援平台算法在提高志愿者参与度和最大化食物救援量的同时，加剧了地理不均，导致部分社区系统性地处于劣势。

Method: 提出Contextual Budget Bandit模型，该模型将情境依赖的预算分配融入不安多臂老虎机框架。为此，开发了经验上快速的启发式算法；并设计了Mitosis算法，以在志愿者稀缺时保证计算出最优预算分配。

Result: 在合成和真实世界的食物救援数据集上，所提出的算法均优于基线方法。

Conclusion: 所提出的算法能有效实现食物救援中的地理公平性，并提升整体绩效。

Abstract: Volunteer-based food rescue platforms tackle food waste by matching surplus
food to communities in need. These platforms face the dual problem of
maintaining volunteer engagement and maximizing the food rescued. Existing
algorithms to improve volunteer engagement exacerbate geographical disparities,
leaving some communities systematically disadvantaged. We address this issue by
proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates
context-dependent budget allocation in restless multi-armed bandits, a model of
decision-making which allows for stateful arms. By doing so, we can allocate
higher budgets to communities with lower match rates, thereby alleviating
geographical disparities. To tackle this problem, we develop an empirically
fast heuristic algorithm. Because the heuristic algorithm can achieve a poor
approximation when active volunteers are scarce, we design the Mitosis
algorithm, which is guaranteed to compute the optimal budget allocation.
Empirically, we demonstrate that our algorithms outperform baselines on both
synthetic and real-world food rescue datasets, and show how our algorithm
achieves geographical fairness in food rescue.

</details>


### [216] [GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research](https://arxiv.org/abs/2509.10790)
*Luke Howard*

Main category: cs.LG

TL;DR: 本文提出了GoldenTransformer，一个模块化、可扩展的故障注入框架，用于评估大型语言模型（LLMs）在硬件故障条件下的弹性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型应用广泛，但在故障条件下的鲁棒性研究不足。需要一个工具来评估LLMs对诱发硬件故障的抵抗能力。

Method: GoldenTransformer是一个基于Python的统一平台，构建于PyTorch和HuggingFace Transformers之上。它能向预训练的Transformer模型注入多种故障（如权重损坏、激活注入、注意力层中断），并专注于解决大型Transformer架构的挑战，支持实验复现、指标记录和可视化。

Result: 论文详细介绍了GoldenTransformer的技术设计和使用方法，并通过在分类和生成任务上的示例实验展示了其能力，证明了该框架的有效性和实用性。

Conclusion: GoldenTransformer通过在Transformer模型中多个逻辑和结构点进行可控故障注入，为研究人员和实践者提供了模型鲁棒性分析的宝贵工具，并能指导实际LLM应用的可靠系统设计。

Abstract: Transformers have become the foundation for a wide range of
state--of--the--art models across natural language processing, computer vision,
and other machine learning domains. Despite their widespread deployment, the
robustness of these models under fault conditions remains underexplored. We
present GoldenTransformer, a modular and extensible fault injection framework
designed to evaluate the resiliency of Large Language Models to induced
hardware faults. GoldenTransformer offers a unified Python-based platform for
injecting diverse classes of faults--such as weight corruption, activation
injections, and attention--level disruptions--into pretrained
transformer--based models. Inspired by the GoldenEye simulator for DNNs, our
framework focuses on the unique challenges of working with large transformer
architectures, including considerations such as structural complexity, latent
dependencies, and nonuniform layer definitions. GoldenTransformer is built atop
PyTorch and HuggingFace Transformers, and it supports experiment
reproducibility, metric logging, and visualization out of the box. We detail
the technical design and use of GoldenTransformer and demonstrate through
several example experiments on classification and generation tasks. By enabling
controlled injection of faults at multiple logical and structural points in a
transformer, GoldenTransformer offers researchers and practitioners a valuable
tool for model robustness analysis and for guiding dependable system design in
real-world LLM applications.

</details>


### [217] [Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone](https://arxiv.org/abs/2509.10809)
*Antonio Bărbălau,Cristian Daniel Păduraru,Teodor Poncu,Alexandru Tifrea,Elena Burceanu*

Main category: cs.LG

TL;DR: 挑战SAE去偏方法中关于特征存储的传统假设，提出一种编码器中心的新方法（S&P TopK），显著提升公平性并超越现有技术，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有SAE去偏方法普遍假设特征表示存储在解码器权重中并操纵稀疏激活。本文挑战此基本假设，旨在提出一种编码器中心的表示去偏替代方案。

Method: 提出Selection and Projection (S&P TopK) 框架，包含：(i) 非常规SAE特征选择策略；(ii) 创新的去偏方法，将输入嵌入与编码器权重正交化；(iii) 通过编码器权重插值实现去偏过程中的性能保持机制。

Result: S&P TopK框架在公平性指标上超越传统SAE用法高达3.2倍，并在测试时VLM去偏方面超越现有技术高达1.8倍，同时保持了下游性能。

Conclusion: 通过挑战传统假设并引入编码器中心的S&P TopK框架，本文成功实现高效且性能保持的表示去偏，显著提升了公平性，为可解释AI领域的偏见缓解提供了新的方向。

Abstract: Sparse Autoencoders (SAEs) have proven valuable due to their ability to
provide interpretable and steerable representations. Current debiasing methods
based on SAEs manipulate these sparse activations presuming that feature
representations are housed within decoder weights. We challenge this
fundamental assumption and introduce an encoder-focused alternative for
representation debiasing, contributing three key findings: (i) we highlight an
unconventional SAE feature selection strategy, (ii) we propose a novel SAE
debiasing methodology that orthogonalizes input embeddings against encoder
weights, and (iii) we establish a performance-preserving mechanism during
debiasing through encoder weight interpolation. Our Selection and Projection
framework, termed S\&P TopK, surpasses conventional SAE usage in fairness
metrics by a factor of up to 3.2 and advances state-of-the-art test-time VLM
debiasing results by a factor of up to 1.8 while maintaining downstream
performance.

</details>


### [218] [FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring](https://arxiv.org/abs/2509.10825)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: FACTORS框架结合实验设计和Shapley分解，通过估计主效应和交互作用并考虑不确定性与成本，实现预算内稳定且可解释的优化配置选择。


<details>
  <summary>Details</summary>
Motivation: 解决性能和稳定性问题对训练因子组合敏感的挑战，特别是在预算有限下需要可靠地选择最佳配置。

Method: 提出FACTORS框架，整合实验设计（DOE）与Shapley分解。通过两种互补路径（基于条件均值的插件路径和重构Shapley贡献的最小二乘路径）估计主效应和两因子交互作用。将估计结果整合到考虑不确定性与成本的风险调整目标函数中，并辅以估计标准化、偏差校正、不确定性量化及轻量级搜索。

Result: 在多样数据集和设计条件下，提升了秩保留和最优配置识别能力，降低了决策风险。理论上提供了误差分解、样本复杂度分析和最优性差距上限。通过地图形式总结效应，提供可解释的调整优先级和改进路径。

Conclusion: FACTORS提供了一个鲁棒的调优基础，即使在预算限制下，也能带来稳定的性能提升和可解释的决策依据，有效识别并选择最优配置。

Abstract: We propose FACTORS, a framework that combines design of experiments with
Shapley decomposition to address performance and stability issues that are
sensitive to combinations of training factors. Our approach consistently
estimates main effects and two-factor interactions, then integrates them into a
risk-adjusted objective function that jointly accounts for uncertainty and
cost, enabling reliable selection of configurations under a fixed budget.
Effect estimation is implemented through two complementary paths: a plug-in
path based on conditional means, and a least-squares path that reconstructs
Shapley contributions from samples. These paths are designed to work
complementarily even when design density and bias levels differ. By
incorporating standardization of estimates, bias correction, and uncertainty
quantification, our procedure ensures comparability across heterogeneous factor
spaces and designs, while a lightweight search routine yields configurations
within practical time even for large factor spaces. On the theoretical side, we
provide error decompositions, sample complexity analysis, and upper bounds on
optimality gaps. On the interpretive side, we summarize main effects and
interactions in map form, highlighting adjustment priorities and safe
improvement pathways. Across diverse datasets and design conditions, our
approach improves rank preservation and optimal configuration identification,
reduces decision-making risks, and offers a tuning foundation that delivers
interpretable justification alongside stable performance gains even under
budget constraints.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [219] [Energy-Aware Data Center Management: A Sustainable Approach to Reducing Carbon Footprint](https://arxiv.org/abs/2509.10462)
*Rabab Khan Rongon,Krishna Das*

Main category: cs.NI

TL;DR: 本研究提出一个能源感知管理框架，通过整合先进技术（如动态工作负载、可再生能源、智能冷却）来降低数据中心的能耗、运营成本和碳足迹，同时保持高性能，以实现可持续的绿色云基础设施。


<details>
  <summary>Details</summary>
Motivation: 云计算和数据中心基础设施的快速扩张导致能源消耗巨大，日益增长的碳足迹带来严重环境挑战，亟需可持续的数据中心运营策略。

Method: 通过整合先进的节能技术和优化资源利用，提出一个能源感知管理框架，关键要素包括动态工作负载分配、可再生能源整合和智能冷却系统。研究采用模拟和案例研究的方法来评估这些策略。

Result: 所提出的策略能有效降低数据中心的能耗、运营成本和碳排放，同时保持高性能，证明了可持续实践在环境和经济上的双重效益。研究提供了减少数据中心碳排放的实用见解，支持向绿色云基础设施的过渡。

Conclusion: 可扩展的能源感知数据中心设计具有显著降低环境影响并确保最佳功能的潜力，有助于缓解气候变化。实现既环保又经济可持续的数据中心运营是可行的。

Abstract: The rapid expansion of cloud computing and data center infrastructure has led
to significant energy consumption, posing environmental challenges due to the
growing carbon footprint. This research explores energy-aware management
strategies aimed at creating sustainable data center operations. By integrating
advanced energy-efficient technologies and optimizing resource utilization,
this study proposes a framework to minimize power usage while maintaining high
performance. Key elements include dynamic workload allocation, renewable energy
integration, and intelligent cooling systems, all of which contribute to
reducing overall energy consumption. The study also examines the impact of
these strategies on operational costs and performance efficiency, demonstrating
how sustainable practices can be both environmentally and economically
beneficial. Through simulations and case studies, the research offers practical
insights into reducing carbon emissions in data centers, supporting the
transition towards greener cloud infrastructure. The findings highlight the
potential for scalable, energy-aware data center designs that significantly
lower environmental impact while ensuring optimal functionality, contributing
to the global effort of mitigating climate change.

</details>


### [220] [A Dynamic Service Offloading Algorithm Based on Lyapunov Optimization in Edge Computing](https://arxiv.org/abs/2509.10475)
*Peiyan Yuan,Ming Li,Chenyang Wang,Ledong An,Xiaoyan Zhao,Junna Zhang,Xiangyang Li,Huadong Ma*

Main category: cs.NI

TL;DR: 研究协作边缘计算中稳定性与卸载成本的权衡。提出基于Lyapunov优化的LDSO算法，以确保系统稳定性和最小化卸载成本，并验证其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有协作边缘卸载方法忽略队列稳定性对整体系统性能的关键作用，导致在优化卸载成本时未能充分考虑系统长期稳定性。

Method: 建立了多跳数据传输模型和包含能耗与时延的成本模型；引入时变队列模型以维护系统稳定性；基于Lyapunov优化，提出动态卸载算法（LDSO），旨在最小化卸载成本同时确保长期系统稳定性。

Result: 理论分析和实验结果验证了所提出的LDSO算法在成本效率和系统稳定性方面均比现有技术有显著提升。

Conclusion: LDSO算法成功地解决了协作边缘计算中系统稳定性与卸载成本的权衡问题，通过引入队列稳定性，实现了更优的长期系统性能和成本效率。

Abstract: This study investigates the trade-off between system stability and offloading
cost in collaborative edge computing. While collaborative offloading among
multiple edge servers enhances resource utilization, existing methods often
overlook the role of queue stability in overall system performance. To address
this, a multi-hop data transmission model is developed, along with a cost model
that captures both energy consumption and delay. A time-varying queue model is
then introduced to maintain system stability. Based on Lyapunov optimization, a
dynamic offloading algorithm (LDSO) is proposed to minimize offloading cost
while ensuring long-term stability. Theoretical analysis and experimental
results verify that the proposed LDSO achieves significant improvements in both
cost efficiency and system stability compared to the state-of-the-art.

</details>


### [221] [The LLM as a Network Operator: A Vision for Generative AI in the 6G Radio Access Network](https://arxiv.org/abs/2509.10478)
*Oluwaseyi Giwa,Michael Adewole,Tobi Awodumila,Pelumi Aderinto*

Main category: cs.NI

TL;DR: 为应对NextG RAN管理的高复杂性，本文提出LLM-RAN运营商概念，将LLM引入RAN控制回路。构建一个形式化框架，提供分析工具以验证AI原生RAN控制的可行性和稳定性，并识别关键研究挑战。


<details>
  <summary>Details</summary>
Motivation: 未来AI原生下一代(NextG)无线接入网络(RAN)的管理复杂度巨大，已超出传统自动化方法的处理能力。

Method: 引入LLM-RAN运营商概念，将大型语言模型(LLM)嵌入RAN控制环路，将高级人类意图转化为最优网络动作。提出一个形式化框架，通过与O-RAN标准对齐的适配器提供可检查的保证，分离非实时(Non-RT) RIC中的LLM驱动战略指导与近实时(Near-RT) RIC中的反应性执行。

Result: 该框架提供了分析工具来论证AI原生RAN控制的可行性和稳定性，包括关于策略表达性和收敛到稳定不动点的命题和定理。同时，识别了安全性、实时性能和物理世界基础等关键研究挑战。

Conclusion: 通过数学严谨性为AI原生RAN控制提供分析工具，旨在弥合AI理论与无线系统工程之间的鸿沟，以实现知识驱动、意图驱动的NextG无线网络。

Abstract: The management of future AI-native Next-Generation (NextG) Radio Access
Networks (RANs), including 6G and beyond, presents a challenge of immense
complexity that exceeds the capabilities of traditional automation. In
response, we introduce the concept of the LLM-RAN Operator. In this paradigm, a
Large Language Model (LLM) is embedded into the RAN control loop to translate
high-level human intents into optimal network actions. Unlike prior empirical
studies, we present a formal framework for an LLM-RAN operator that builds on
earlier work by making guarantees checkable through an adapter aligned with the
Open RAN (O-RAN) standard, separating strategic LLM-driven guidance in the
Non-Real-Time (RT) RAN intelligent controller (RIC) from reactive execution in
the Near-RT RIC, including a proposition on policy expressiveness and a theorem
on convergence to stable fixed points. By framing the problem with mathematical
rigor, our work provides the analytical tools to reason about the feasibility
and stability of AI-native RAN control. It identifies critical research
challenges in safety, real-time performance, and physical-world grounding. This
paper aims to bridge the gap between AI theory and wireless systems engineering
in the NextG era, aligning with the AI4NextG vision to develop knowledgeable,
intent-driven wireless networks that integrate generative AI into the heart of
the RAN.

</details>


### [222] [Exploring Busy Period for Worst-Case Deadline Failure Probability Analysis](https://arxiv.org/abs/2509.10479)
*Junyi Liu,Xu Jiang,Yuanzhen Mu,Wang Yi,Nan Guan*

Main category: cs.NI

TL;DR: 针对概率实时调度中临界瞬间分析不安全的问题，本文提出了一种新颖的最坏情况截止期失效概率（WCDFP）方法，通过系统分析不同忙周期起点的截止期错失概率，实验证明其显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在经典确定性实时调度分析中，仅考虑临界瞬间的忙周期被认为是安全的。然而，最近研究发现，在概率实时调度分析中，仅考虑临界瞬间是不安全的，这构成了一个研究空白。

Method: 通过系统分析不同忙周期起始点的截止期错失概率来弥补现有分析的不足。提出了一种用于概率固定优先级抢占式调度的新型最坏情况截止期失效概率（WCDFP）方法。

Result: 实验结果表明，所提出的方法相比现有最先进的方法取得了显著的改进。

Conclusion: 本文提出的WCDFP方法为概率实时调度分析提供了一种更安全、更有效的途径，解决了传统临界瞬间方法在概率上下文中的局限性。

Abstract: Busy period is a fundamental concept in classical deterministic real-time
scheduling analysis. In this deterministic context, only one busy period -
which starts at the critical instant - needs to be considered, which identifies
the worst-case scenario and thus paves the way for the development of efficient
and safe analysis techniques. However, a recent work has revealed that, in the
context of \textit{probabilistic} real-time scheduling analysis, only
considering critical instant is not safe. In this paper, we address this gap by
systematically analyzing deadline miss probabilities across varying busy period
starting points. We propose a novel method of Worst-Case Deadline Failure
Probability (WCDFP) for probabilistic fixed-priority preemptive scheduling.
Experimental results demonstrate significant improvements over state-of-the-art
methods achieved by our proposed method.

</details>


### [223] [Synergetic Empowerment: Wireless Communications Meets Embodied Intelligence](https://arxiv.org/abs/2509.10481)
*Hongtao Liang,Yihe Diao,YuHang Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.NI

TL;DR: 无线通信正迈入智能体时代，论文分析了无线通信与具身智能结合如何实现协同赋能，促进智能体通信发展，并识别了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信进入智能体时代，具身智能体成为活跃参与者，亟需探究无线通信与具身智能如何协同赋能，以极大促进智能体通信发展。

Method: 论文通过概述协同赋能的共演化过程，将无线通信定位为集体智能的数字神经系统。同时，运用感知-认知-执行（PCE）循环视角，阐述了具身智能与无线通信如何相互受益，揭示了PCE各阶段对网络容量的挑战及系统优化机遇。

Result: 协同赋能使无线通信转变为集体智能的数字神经系统，并将孤立智能体提升为具有超越个体贡献能力的统一超有机体。PCE循环的每个阶段既构成网络容量挑战，也为系统级优化创造了前所未有的机遇。

Conclusion: 无线通信与具身智能的协同结合是其共同演进的关键，能催生集体智能和超有机体。这种结合在PCE循环中带来挑战与机遇，并为未来的智能体通信指明了开放性问题和研究方向。

Abstract: Wireless communication is evolving into an agent era, where large-scale
agents with inherent embodied intelligence are not just users but active
participants. The perfect combination of wireless communication and embodied
intelligence can achieve a synergetic empowerment and greatly facilitate the
development of agent communication. An overview of this synergetic empowerment
is presented, framing it as a co-evolutionary process that transforms wireless
communication from a simple utility into the digital nervous system of a
collective intelligence, while simultaneously elevating isolated agents into a
unified superorganism with emergent capabilities far exceeding individual
contributions. Moreover, we elaborate how embodied intelligence and wireless
communication mutually benefit each other through the lens of the
perception-cognition-execution (PCE) loop, revealing a fundamental duality
where each PCE stage both challenges network capacity and creates unprecedented
opportunities for system-wide optimization. Furthermore, critical open issues
and future research directions are identified.

</details>


### [224] [SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2509.10486)
*Pengcheng Luo,Yunyang Zhao,Bowen Zhang,Genke Yang,Boon-Hee Soong,Chau Yuen*

Main category: cs.NI

TL;DR: 针对现有学习型ABR方法泛化能力差的问题，本文提出了SABR框架，结合行为克隆预训练和强化学习微调，并通过新基准测试证明其在广泛网络条件下学习更稳定，对未知网络泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的自适应码率（ABR）控制方法在训练时依赖有限的网络轨迹，忽略了真实网络条件的广泛分布特性，导致在域外（OOD）场景中泛化能力差。

Method: 提出SABR训练框架，结合行为克隆（BC）预训练与强化学习（RL）微调。同时，引入ABRBench-3G和ABRBench-4G+基准，提供覆盖广泛的训练轨迹和专门的OOD测试集。

Result: 实验结果表明，在所提出的基准测试中，SABR与Pensieve、Comyco和NetLLM相比，取得了最佳平均排名。

Conclusion: SABR能够实现更稳定的跨广泛分布学习，并显著提升了对未知网络条件的泛化能力。

Abstract: With the advent of 5G, the internet has entered a new video-centric era. From
short-video platforms like TikTok to long-video platforms like Bilibili, online
video services are reshaping user consumption habits. Adaptive Bitrate (ABR)
control is widely recognized as a critical factor influencing Quality of
Experience (QoE). Recent learning-based ABR methods have attracted increasing
attention. However, most of them rely on limited network trace sets during
training and overlook the wide-distribution characteristics of real-world
network conditions, resulting in poor generalization in out-of-distribution
(OOD) scenarios. To address this limitation, we propose SABR, a training
framework that combines behavior cloning (BC) pretraining with reinforcement
learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and
ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD
test sets for assessing robustness to unseen network conditions. Experimental
results demonstrate that SABR achieves the best average rank compared with
Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results
indicate that SABR enables more stable learning across wide distributions and
improves generalization to unseen network conditions.

</details>


### [225] [Online Learning Based Efficient Resource Allocation for LoRaWAN Network](https://arxiv.org/abs/2509.10493)
*Ruiqi Wang,Jing Ren,Tongyu Song,Wenjun Li,Xiong Wang,Sheng Wang,Shizhong Xu*

Main category: cs.NI

TL;DR: 该论文提出D-LoRa和CD-LoRa两种在线学习框架，用于LoRaWAN网络中动态优化PDR和EE的资源分配，通过分解学习和混合初始化策略，在不同动态环境下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型LoRaWAN网络需要动态优化相互冲突的PDR和EE指标，但现有方法常过度简化问题或缺乏对动态信道环境的适应性，导致性能不佳。

Method: 提出两种在线学习框架：D-LoRa是一个全分布式框架，将问题建模为组合多臂老虎机，通过分解参数选择和去聚合奖励函数降低学习复杂度。CD-LoRa是一个混合框架，通过引入轻量级中心化初始化阶段进行一次性信道分配和动作空间剪枝，以加速后续分布式学习。

Result: 广泛的仿真和实际实验表明，D-LoRa在非平稳环境中表现卓越，CD-LoRa在平稳条件下收敛最快。与现有技术相比，PDR提升高达10.8%，EE提升26.1%。

Conclusion: 所提出的D-LoRa和CD-LoRa框架在实际应用中被证明是有效的，能为可扩展和高效的LoRaWAN网络提供切实可行的解决方案。

Abstract: The deployment of large-scale LoRaWAN networks requires jointly optimizing
conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)
by dynamically allocating transmission parameters, including Carrier Frequency,
Spreading Factor, and Transmission Power. Existing methods often oversimplify
this challenge, focusing on a single metric or lacking the adaptability needed
for dynamic channel environments, leading to suboptimal performance. To address
this, we propose two online learning-based resource allocation frameworks that
intelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,
is a fully distributed framework that models the problem as a Combinatorial
Multi-Armed Bandit. By decomposing the joint parameter selection and employing
specialized, disaggregated reward functions, D-LoRa dramatically reduces
learning complexity and enables nodes to autonomously adapt to network
dynamics. To further enhance performance in LoRaWAN networks, we introduce
CD-LoRa, a hybrid framework that integrates a lightweight, centralized
initialization phase to perform a one-time, quasi-optimal channel assignment
and action space pruning, thereby accelerating subsequent distributed learning.
Extensive simulations and real-world field experiments demonstrate the
superiority of our frameworks, showing that D-LoRa excels in non-stationary
environments while CD-LoRa achieves the fastest convergence in stationary
conditions. In physical deployments, our methods outperform state-of-the-art
baselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their
practical effectiveness for scalable and efficient LoRaWAN networks.

</details>


### [226] [Towards Scalable O-RAN Resource Management: Graph-Augmented Proximal Policy Optimization](https://arxiv.org/abs/2509.10499)
*Duc-Thinh Ngo,Kandaraj Piamrat,Ons Aouedi,Thomas Hassan,Philippe Raipin-Parvédy*

Main category: cs.NI

TL;DR: 本文提出了一种名为GPPO的框架，通过结合图神经网络和动作掩码，联合优化O-RAN的基带功能拆分和虚拟单元放置，显著提高了资源管理的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构虽灵活但引入了资源管理挑战，需要在动态需求和复杂拓扑下联合优化功能拆分和虚拟单元放置。现有解决方案通常单独处理或缺乏在大规模真实场景中的可扩展性。

Method: 提出了一种图增强近端策略优化（GPPO）框架。该框架利用图神经网络（GNNs）进行拓扑感知特征提取，并集成动作掩码以高效处理组合决策空间，联合优化功能拆分和放置决策。

Result: GPPO在大小规模O-RAN场景中均优于现有基线，部署成本降低高达18%，泛化测试中奖励提高25%，并保持完美可靠性。

Conclusion: GPPO框架对实际O-RAN部署具有高度的有效性和可扩展性，能有效解决其复杂的资源管理问题。

Abstract: Open Radio Access Network (O-RAN) architectures enable flexible, scalable,
and cost-efficient mobile networks by disaggregating and virtualizing baseband
functions. However, this flexibility introduces significant challenges for
resource management, requiring joint optimization of functional split selection
and virtualized unit placement under dynamic demands and complex topologies.
Existing solutions often address these aspects separately or lack scalability
in large and real-world scenarios. In this work, we propose a novel
Graph-Augmented Proximal Policy Optimization (GPPO) framework that leverages
Graph Neural Networks (GNNs) for topology-aware feature extraction and
integrates action masking to efficiently navigate the combinatorial decision
space. Our approach jointly optimizes functional split and placement decisions,
capturing the full complexity of O-RAN resource allocation. Extensive
experiments on both small-and large-scale O-RAN scenarios demonstrate that GPPO
consistently outperforms state-of-the-art baselines, achieving up to 18% lower
deployment cost and 25% higher reward in generalization tests, while
maintaining perfect reliability. These results highlight the effectiveness and
scalability of GPPO for practical O-RAN deployments.

</details>


### [227] [An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms](https://arxiv.org/abs/2509.10507)
*Vadim Allayev,Mahbubur Rahman*

Main category: cs.NI

TL;DR: 针对IoIT面临的资源限制和中心化系统问题，本文提出一个去中心化P2P网状网络系统模型，利用联邦学习、元启发式算法和多目标优化，以实现可靠性、能效和低延迟。


<details>
  <summary>Details</summary>
Motivation: IoIT在计算资源、能源供应和存储方面面临挑战，尤其是在嵌入式设备上高效部署机器学习/深度学习模型。此外，现有研究多关注中心化系统，存在瓶颈和安全隐患，缺乏去中心化的解决方案。

Method: 提出一个异构、去中心化的传感监控IoIT对等（P2P）网状网络系统模型。该系统采用联邦学习进行分布式节点训练，利用元启发式算法优化任务分配和路由路径，并通过多目标优化来平衡可靠性、能效和延迟等相互冲突的性能目标。

Result: 本文主要提出了一个解决IoIT挑战的系统模型和技术框架。抽象内容未包含实际的实验结果或性能评估，而是描述了所提系统的设计理念和预期功能。

Conclusion: 通过构建去中心化P2P网状网络，并结合联邦学习、元启发式算法和多目标优化，所提出的模型有望克服IoIT在资源受限和中心化系统中的挑战，实现更可靠、节能和低延迟的智能物联网系统。

Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the
utility of Internet of Things (IoT) devices with the innovation of embedded AI
algorithms. However, it does not come without challenges, and struggles
regarding available computing resources, energy supply, and storage
limitations. In particular, many impediments to IoIT are linked to the
energy-efficient deployment of machine learning (ML)/deep learning (DL) models
in embedded devices. Research has been conducted to design energy-efficient
IoIT platforms, but these papers often focus on centralized systems, in which
some central entity processes all the data and coordinates actions. This can be
problematic, e.g., serve as bottleneck or lead to security concerns. In a
decentralized system, nodes/devices would self-organize and make their own
decisions. Therefore, to address such issues, we propose a heterogeneous,
decentralized sensing and monitoring IoIT peer-to-peer mesh network system
model. Nodes in the network will coordinate towards several optimization goals:
reliability, energy efficiency, and latency. The system employs federated
learning to train nodes in a distributed manner, metaheuristics to optimize
task allocation and routing paths, and multi-objective optimization to balance
conflicting performance goals.

</details>


### [228] [CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi Head Attention for Heterogeneous Vehicular Networks](https://arxiv.org/abs/2509.10508)
*Aathira G Menon,Prabu Krishnan,Shyam Lal*

Main category: cs.NI

TL;DR: 提出一种轻量级深度学习解决方案CAR-BRAINet，用于异构车载网络（HetVNets）中的波束预测，通过结合CNN和MHA，在真实复杂场景下实现了高精度预测，显著提升了频谱效率。


<details>
  <summary>Details</summary>
Motivation: 异构车载网络（HetVNets）在5G/B5G车载通信中至关重要，但在高移动性的真实环境中保持稳定连接是一个挑战。现有波束预测研究鲜少专门针对HetVNets，且多限于理想化场景，忽视了真实车载网络的复杂性，因此迫切需要开发可靠的HetVNets专用波束预测方案。

Method: 本文提出一种名为“CAR-BRAINet”的轻量级深度学习解决方案，它结合了卷积神经网络（CNNs）和强大的多头注意力（MHA）机制。为模拟真实驾驶场景的复杂性，本研究构建了包含城市、乡村和高速公路场景的三个高质量动态数据集，并纳入了关键因素，如MAC协议（3GPP-C-V2X和IEEE 802.11BD）、高速下的多普勒频移以及变化的距离和信噪比水平。

Result: CAR-BRAINet在所有车载场景下均表现出色，实现了精确的波束预测，具有最小的波束开销，并且相对于现有方法，频谱效率稳定提升了17.9422%。

Conclusion: 本研究证明了CAR-BRAINet在复杂异构车载网络中的有效性，提供了有前景的性能，且不依赖于移动用户的位置角度和天线尺寸，从而减少了冗余的传感器延迟。

Abstract: Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking
different communication technologies such as sub-6GHz, mm-wave and DSRC to meet
diverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address
the humongous user demands-but maintaining a steady connection in a highly
mobile, real-world conditions remain a challenge. Though there has been ample
of studies on beam prediction models a dedicated solution for HetVNets is
sparsely explored. Hence, it is the need of the hour to develop a reliable beam
prediction solution, specifically for HetVNets. This paper introduces a
lightweight deep learning-based solution termed-"CAR-BRAINet" which consists of
convolutional neural networks with a powerful multi-head attention (MHA)
mechanism. Existing literature on beam prediction is largely studied under a
limited, idealised vehicular scenario, often overlooking the real-time
complexities and intricacies of vehicular networks. Therefore, this study aims
to mimic the complexities of a real-time driving scenario by incorporating key
factors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the
effect of Doppler shifts under high velocity and varying distance and SNR
levels into three high-quality dynamic datasets pertaining to urban, rural and
highway vehicular networks. CAR-BRAINet performs effectively across all the
vehicular scenarios, demonstrating precise beam prediction with minimal beam
overhead and a steady improvement of 17.9422% on the spectral efficiency over
the existing methods. Thus, this study justifies the effectiveness of
CAR-BRAINet in complex HetVNets, offering promising performance without relying
on the location angle and antenna dimensions of the mobile users, and thereby
reducing the redundant sensor-latency.

</details>


### [229] [Pair-Bid Auction Model for Optimized Network Slicing in 5G RAN](https://arxiv.org/abs/2509.10533)
*Mengyao Li,Sebastian Troia,Yingqian Zhang,Guido Maier*

Main category: cs.NI

TL;DR: 本文提出一个基于两级组合拍卖的模型，利用对偶竞价机制和VCG定价，优化5G网络切片中MNO与MVNO间的资源分配和收益，实现了公平共享和财务效益，并在仿真中显示12.5%的收益提升。


<details>
  <summary>Details</summary>
Motivation: 5G网络切片在MNO、MVNO和终端用户共享物理基础设施时，面临如何实现公平共享、提升财务相关能效以及处理动态用户请求的挑战。研究旨在优化资源分配和收入。

Method: 构建了一个两级分层组合拍卖模型：上层MNO向MVNOs拍卖网络切片，下层MVNOs再通过自身拍卖向终端用户分配资源。模型采用对偶竞价机制提高竞争和效率，并结合Vickrey-Clarke-Groves (VCG) 定价，以确保真实竞价并优化资源分配和收益生成。

Result: 仿真结果验证了模型的有效性，在资源分配和财务绩效方面表现良好。与基线方案相比，该模型实现了12.5%的收益提升。

Conclusion: 所提出的基于两级组合拍卖、对偶竞价和VCG定价的模型，能够有效解决5G网络切片中的资源公平共享和财务效率问题，显著提高网络运营商的收益。

Abstract: Network slicing is a key 5G technology that enables multiple virtual networks
to share physical infrastructure, optimizing flexibility and resource
allocation. This involves Mobile Network Operators (MNO), Mobile Virtual
Network Operators (MVNOs), and end users, where MNO leases network slices to
MVNOs, and then provides customized services. This work considers end-to-end
network slicing with a focus on fair sharing and financial-related power
efficiency, modeled as a two level hierarchical combinatorial auction. At the
upper level, an MNO auctions slices to competing MVNOs, while at the lower
level, MVNOs allocate resources to end users through their own auctions.
Dynamic user requests add complexity to the process. Our model optimizes
resource allocation and revenue generation using a pair-bid mechanism and
Vickrey-Clarke-Groves (VCG) pricing. The pair-bid approach enhances competition
and efficiency, while VCG ensures truthful bidding based on marginal system
impact. Simulations validate the model's effectiveness in resource distribution
and financial performance, showing a 12.5% revenue improvement over the
baseline.

</details>


### [230] [ASL360: AI-Enabled Adaptive Streaming of Layered 360° Video over UAV-assisted Wireless Networks](https://arxiv.org/abs/2509.10544)
*Alireza Mohammadhosseini,Jacob Chakareski,Nicholas Mastronarde*

Main category: cs.NI

TL;DR: 本文提出ASL360，一种基于自适应深度强化学习的调度器，用于在无人机辅助的5G毫米波网络中为移动VR用户提供点播360度视频流，旨在最大化用户体验质量（QoE）。该系统将调度决策建模为约束马尔可夫决策过程（CMDP），并使用PPO方法优化策略，结果显示ASL360显著提升了视频质量、降低了卡顿时间，并减少了质量波动。


<details>
  <summary>Details</summary>
Motivation: 在下一代无线网络（特别是无人机辅助的5G网络）中，为移动VR用户提供360度点播视频流时，旨在最大化用户的整体体验质量（QoE）。

Method: ['提出ASL360，一个基于自适应深度强化学习的调度器，用于360度视频流。', '系统模型包含宏基站和无人机基站，均采用毫米波传输。', '360度视频被编码为依赖层和分段瓦片，用户可调度各层段下载，并使用多个缓冲区存储相应层段。', '将调度决策建模为约束马尔可夫决策过程（CMDP），代理选择基础层或增强层以最大化QoE。', '使用策略梯度方法（PPO）来寻找最优策略。', '实现成本组件的动态调整机制，以根据实时网络和流会话条件，自适应地平衡和优先处理视频质量、缓冲区占用和质量变化。']

Result: ['ASL360显著改善了用户体验质量（QoE）。', '相较于竞争基线方法，平均视频质量提高了约2 dB。', '平均卡顿时间降低了80%。', '视频质量波动降低了57%。']

Conclusion: 所提出的层级和自适应方法在增强沉浸式视频流应用（尤其是在动态和具有挑战性的网络环境中）的QoE方面表现出有效性。

Abstract: We propose ASL360, an adaptive deep reinforcement learning-based scheduler
for on-demand 360{\deg} video streaming to mobile VR users in next generation
wireless networks. We aim to maximize the overall Quality of Experience (QoE)
of the users served over a UAV-assisted 5G wireless network. Our system model
comprises a macro base station (MBS) and a UAV-mounted base station which both
deploy mm-Wave transmission to the users. The 360{\deg} video is encoded into
dependent layers and segmented tiles, allowing a user to schedule downloads of
each layer's segments. Furthermore, each user utilizes multiple buffers to
store the corresponding video layer's segments. We model the scheduling
decision as a Constrained Markov Decision Process (CMDP), where the agent
selects Base or Enhancement layers to maximize the QoE and use a policy
gradient-based method (PPO) to find the optimal policy. Additionally, we
implement a dynamic adjustment mechanism for cost components, allowing the
system to adaptively balance and prioritize the video quality, buffer
occupancy, and quality change based on real-time network and streaming session
conditions. We demonstrate that ASL360 significantly improves the QoE,
achieving approximately 2 dB higher average video quality, 80% lower average
rebuffering time, and 57% lower video quality variation, relative to
competitive baseline methods. Our results show the effectiveness of our layered
and adaptive approach in enhancing the QoE in immersive videostreaming
applications, particularly in dynamic and challenging network environments.

</details>


### [231] [Empowering AI-Native 6G Wireless Networks with Quantum Federated Learning](https://arxiv.org/abs/2509.10559)
*Shaba Shaon,Md Raihan Uddin,Dinh C. Nguyen,Seyyedali Hosseinalipour,Dusit Niyato,Octavia A. Dobre*

Main category: cs.NI

TL;DR: 本文探讨将量子联邦学习（QFL）整合到AI原生6G网络中，以克服传统联邦学习在异构无线网络中的挑战，并在边缘智能、网络优化和安全隐私方面提供新能力，并通过案例研究展示其在模型收敛方面的优越性。


<details>
  <summary>Details</summary>
Motivation: AI原生6G网络需要实时、个性化、保护隐私的边缘智能，联邦学习（FL）是基石。然而，传统FL在异构动态无线网络中面临设备计算能力有限、连接不可靠、通信间歇性以及模型安全和数据隐私泄露等瓶颈。

Method: 通过将量子联邦学习（QFL）整合到AI原生6G网络中，利用量子计算、通信和密码学技术来改进联邦学习工作流程。该方法着重于提升边缘智能、网络优化和安全隐私三个关键维度。

Result: QFL在边缘智能、网络优化和安全隐私方面提供了新能力。一个案例研究表明，采用量子近似优化算法（QAOA）的QFL框架在模型收敛方面优于经典方法。

Conclusion: QFL是AI原生6G网络中解决传统FL挑战的变革性范例，但其部署面临量子态脆弱性、与经典协议不兼容和硬件限制等实际挑战。未来研究需着眼于其可扩展的实际应用。

Abstract: AI-native 6G networks are envisioned to tightly embed artificial intelligence
(AI) into the wireless ecosystem, enabling real-time, personalized, and
privacy-preserving intelligence at the edge. A foundational pillar of this
vision is federated learning (FL), which allows distributed model training
across devices without sharing raw data. However, implementing classical FL
methods faces several bottlenecks in heterogeneous dynamic wireless networks,
including limited device compute capacity, unreliable connectivity,
intermittent communications, and vulnerability to model security and data
privacy breaches. This article investigates the integration of quantum
federated learning (QFL) into AI-native 6G networks, forming a transformative
paradigm capable of overcoming these challenges. By leveraging quantum
techniques across computing, communication, and cryptography within FL
workflows, QFL offers new capabilities along three key dimensions: (i) edge
intelligence, (ii) network optimization, and (iii) security and privacy, which
are studied in this work. We further present a case study demonstrating that a
QFL framework employing the quantum approximate optimization algorithm
outperforms classical methods in model convergence. We conclude the paper by
identifying practical challenges facing QFL deployment, such as quantum state
fragility, incompatibility with classical protocols, and hardware constraints,
and then outline key research directions toward its scalable real-world
adoption.

</details>


### [232] [gNB-based Local Breakout for URLLC in industrial 5G](https://arxiv.org/abs/2509.10617)
*Rajendra Paudyal,Rajendra Upadhyay,Al Nahian Bin Emran,Duminda Wijesekera*

Main category: cs.NI

TL;DR: 本文提出一种gNB本地组播分流方案，通过将小区内流量在基站本地处理，显著降低了5G URLLC（超可靠低延迟通信）场景下的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 工业URLLC应用（如协同机器人、自动导引车）需要亚5毫秒延迟和高可靠性。然而，现有5G组播/广播服务（MB-SMF/MB-UPF）将小区内组播流量锚定在核心网，引入了可避免的额外路径和数据包延迟。

Method: 研究者提出一种gNB本地组播分流方案，将符合条件的上行流量在gNB内部转换为下行点对多点承载。该方案在5G核心网中维护授权、成员和策略，并指定了资格策略和配置授权上行，同时通过不变的控制平面锚点保持3GPP安全和合规性。

Result: 通过延迟预算和仿真结果表明，移除回传/UPF/AF段可以将端到端延迟从约6.5-11.5毫秒（核心网锚定）大幅降低到1.5-4.0毫秒（本地分流），平均延迟低于2毫秒，并在不同组大小之间保持约10毫秒的稳定差距。

Conclusion: 该方法为私有5G中实现确定性的小区内组播分发提供了一条实用且符合标准的途径。

Abstract: Industrial URLLC workloads-coordinated robotics, automated guided vehicles,
machine-vision collaboration require sub-5 ms latency and five-nines
reliability. In standardized 5G Multicast/Broadcast Services, intra-cell group
traffic remains anchored in the core using MB-SMF/MB-UPF, and the Application
Function. This incurs a core network path and packet delay that is avoidable
when data transmitters and receivers share a cell. We propose a gNB-local
multicast breakout that pivots eligible uplink flows to a downlink
point-to-multipoint bearer within the gNB, while maintaining authorization,
membership, and policy in the 5G core. The design specifies an eligibility
policy, configured-grant uplink. 3GPP security and compliance are preserved via
unchanged control-plane anchors. A latency budget and simulation indicate that
removing the backhaul/UPF/AF segment reduces end-to-end latency from
approximate 6.5-11.5 ms (anchored to the core) to 1.5-4.0 ms (local breakout),
producing sub-2 ms averages and a stable gap approximate 10 ms between group
sizes. The approach offers a practical, standards-aligned path to deterministic
intra-cell group dissemination in private 5G. We outline multi-cell and
prototype validation as future work.

</details>


### [233] [Deep Learning based Moving Target Defence for Federated Learning against Poisoning Attack in MEC Systems with a 6G Wireless Model](https://arxiv.org/abs/2509.10914)
*Somayeh Kianpisheh,Tarik Taleb,Jari Iinatti,JaeSeung Song*

Main category: cs.NI

TL;DR: 本文针对联邦学习中的模型中毒攻击，提出一种结合设备流量分析和动态拓扑变异（移动目标防御MTD）的防御策略，通过深度强化学习优化，有效排除恶意模型并提升攻击识别性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到模型中毒攻击，而现有基于模型的异常检测机制在处理异构模型或复杂攻击时效率不高。

Method: 利用设备级流量分析（基于循环神经网络对时间序列数据进行分析），预测参与者的可靠性。引入移动目标防御（MTD）策略，动态改变联邦学习的参与者拓扑。基于6G无线模型，提出了MTD策略的优化框架。采用深度强化学习机制，根据设备的拜占庭状态和通信信道能力，优化拓扑变异。

Result: 在DDoS攻击检测应用和Botnet攻击场景下，实验结果表明该方法能有效排除恶意模型，并显著提高攻击识别时间和准确性。

Conclusion: 结合流量分析、动态拓扑变异（MTD）和深度强化学习的防御策略，能够有效抵御联邦学习中的模型中毒攻击，提升系统在恶意环境下的鲁棒性、恶意模型排除能力及攻击识别的效率和准确性。

Abstract: Collaboration opportunities for devices are facilitated with Federated
Learning (FL). Edge computing facilitates aggregation at edge and reduces
latency. To deal with model poisoning attacks, model-based outlier detection
mechanisms may not operate efficiently with hetereogenous models or in
recognition of complex attacks. This paper fosters the defense line against
model poisoning attack by exploiting device-level traffic analysis to
anticipate the reliability of participants. FL is empowered with a topology
mutation strategy, as a Moving Target Defence (MTD) strategy to dynamically
change the participants in learning. Based on the adoption of recurrent neural
networks for time-series analysis of traffic and a 6G wireless model,
optimization framework for MTD strategy is given. A deep reinforcement
mechanism is provided to optimize topology mutation in adaption with the
anticipated Byzantine status of devices and the communication channel
capabilities at devices. For a DDoS attack detection application and under
Botnet attack at devices level, results illustrate acceptable malicious models
exclusion and improvement in recognition time and accuracy.

</details>


### [234] [RU Energy Modeling for O-RAN in ns3-oran](https://arxiv.org/abs/2509.10978)
*Abdul Wadud,Nima Afraz*

Main category: cs.NI

TL;DR: 本文提出一个基于ns3-oran模拟器的O-RAN无线单元（RU）详细灵活功耗模型，支持xApp控制，通过硬件参数化来评估能效并识别理想操作点。


<details>
  <summary>Details</summary>
Motivation: 旨在支持O-RAN部署中即将推出的xApp驱动的能源管理策略，通过提供一个现实的RU功耗模型并识别有效的RU行为的理想操作点。

Method: 开发了一个基于ns3-oran模拟器的O-RAN无线单元（RU）详细、灵活且以RU为中心的功耗模型。该模型是首个支持xApp控制的ns3-oran RU功耗模型，并由硬件级特征（如收发器数量、功率放大器效率、毫米波开销和待机行为）参数化，能够无缝集成到ns-3的能量跟踪系统进行仿真驱动评估。

Result: 该模型实现了对不同发射功率水平下能效的仿真驱动评估。数值研究验证了模型能够代表现实世界的非线性功率缩放，并识别出有效RU行为的理想操作点。

Conclusion: 所开发的O-RAN RU功耗模型能够准确反映现实世界的非线性功率缩放，并识别理想操作点，为未来O-RAN中xApp驱动的能源管理策略提供了有力的评估工具和支持。

Abstract: This paper presents a detailed and flexible power consumption model for Radio
Units (RUs) in O-RAN using the ns3-oran simulator. This is the first ns3-oran
model supporting xApp control to perform the RU power modeling. In contrast to
existing frameworks like EARTH or VBS-DRX, the proposed framework is RU-centric
and is parameterized by hardware-level features, such as the number of
transceivers, the efficiency of the power amplifier, mmWave overheads, and
standby behavior. It enables simulation-driven assessment of energy efficiency
at various transmit power levels and seamlessly integrates with ns-3's energy
tracking system. To help upcoming xApp-driven energy management strategies in
O-RAN installations, numerical research validates the model's capacity to
represent realistic nonlinear power scaling. It identifies ideal operating
points for effective RU behavior.

</details>


### [235] [Multi-Modal Sensing Aided mmWave Beamforming for V2V Communications with Transformers](https://arxiv.org/abs/2509.11112)
*Muhammad Baqer Mollah,Honggang Wang,Hua Fang*

Main category: cs.NI

TL;DR: 针对动态车载毫米波通信中波束训练开销过高的问题，本文提出了一种多模态感知与融合学习框架，利用视觉和GPS数据预测最佳波束，显著降低了波束搜索空间开销，同时保持了高预测准确率和低功率损耗。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的车辆环境中，毫米波通信中标准定义的波束赋形方法因交换导频信号和穷举波束测量，导致高昂的波束训练开销，并减少了可用通信时间。

Method: 本文提出一个多模态感知与融合学习框架。该框架首先通过特定模态编码器分别从视觉和GPS坐标传感模态中提取特征，然后融合这些多模态特征以预测Top-k波束，从而主动建立最佳视距链路。

Result: 实验结果显示，该框架在预测Top-15波束时，实现了高达77.58%的准确率，优于单一模态方法。同时，平均功率损耗低至约2.32 dB，并相对于标准方法，Top-15波束的波束搜索空间开销显著降低了76.56%。

Conclusion: 所提出的多模态感知与融合学习框架能够有效减少动态车载毫米波通信中的波束训练开销，通过高精度波束预测和大幅缩减搜索空间，为建立和维护可靠连接提供了一种可行的替代方案。

Abstract: Beamforming techniques are utilized in millimeter wave (mmWave) communication
to address the inherent path loss limitation, thereby establishing and
maintaining reliable connections. However, adopting standard defined
beamforming approach in highly dynamic vehicular environments often incurs high
beam training overheads and reduces the available airtime for communications,
which is mainly due to exchanging pilot signals and exhaustive beam
measurements. To this end, we present a multi-modal sensing and fusion learning
framework as a potential alternative solution to reduce such overheads. In this
framework, we first extract the features individually from the visual and GPS
coordinates sensing modalities by modality specific encoders, and subsequently
fuse the multimodal features to obtain predicted top-k beams so that the best
line-of-sight links can be proactively established. To show the
generalizability of the proposed framework, we perform a comprehensive
experiment in four different vehicle-to-vehicle (V2V) scenarios from real-world
multi-modal sensing and communication dataset. From the experiment, we observe
that the proposed framework achieves up to 77.58% accuracy on predicting top-15
beams correctly, outperforms single modalities, incurs roughly as low as 2.32
dB average power loss, and considerably reduces the beam searching space
overheads by 76.56% for top-15 beams with respect to standard defined approach.

</details>


### [236] [On the Feasibility of Inter-Flow Service Degradation Detection](https://arxiv.org/abs/2509.11140)
*Balint Bicski,Adrian Pekar*

Main category: cs.NI

TL;DR: 针对网络硬件加速造成的监测盲点，本文提出跨流关联框架，发现可观测流的信号能预测不可观测流的服务降级。统计分析表明关联信号稀疏但有预测性，而简单机器学习模型虽精度高，却主要依赖流内特征，揭示需要更复杂的结构感知模型来充分利用跨流信息。


<details>
  <summary>Details</summary>
Motivation: 现代网络中的硬件加速导致流量卸载到不可观测状态，形成监测盲点，严重阻碍了对实时服务降级 (SD) 的检测。

Method: 1. 提出并形式化了一个基于“可观测流可作为非可观测流环境传感器”假设的跨流关联框架。
2. 对跨流关联图谱进行全面的统计分析。
3. 使用标准机器学习模型对框架进行评估，并进行特征重要性分析。

Result: 1. 跨流关联潜力巨大，但最明确的信号（如共同发生的服务降级事件）稀疏且很少完美对齐。
2. 关键发现是这些信号频繁地先于目标流的降级发生，验证了及时检测的可能性。
3. 机器学习模型达到了高分类精度，但特征重要性分析表明其主要依赖于更简单的流内特征。

Conclusion: 1. 利用复杂的跨流上下文信息需要比简单模型更复杂的结构感知模型。
2. 本研究为跨流问题提供了基础分析，并明确指出了未来研究的方向，即开发结构感知模型来解决此问题。

Abstract: Hardware acceleration in modern networks creates monitoring blind spots by
offloading flows to a non-observable state, hindering real-time service
degradation (SD) detection. To address this, we propose and formalize a novel
inter-flow correlation framework, built on the hypothesis that observable flows
can act as environmental sensors for concurrent, non-observable flows. We
conduct a comprehensive statistical analysis of this inter-flow landscape,
revealing a fundamental trade-off: while the potential for correlation is vast,
the most explicit signals (i.e., co-occurring SD events) are sparse and rarely
perfectly align. Critically, however, our analysis shows these signals
frequently precede degradation in the target flow, validating the potential for
timely detection. We then evaluate the framework using a standard machine
learning model. While the model achieves high classification accuracy, a
feature-importance analysis reveals it relies primarily on simpler intra-flow
features. This key finding demonstrates that harnessing the complex contextual
information requires more than simple models. Our work thus provides not only a
foundational analysis of the inter-flow problem but also a clear outline for
future research into the structure-aware models needed to solve it.

</details>


### [237] [UDFS: Lightweight Representation-Driven Robust Network Traffic Classification](https://arxiv.org/abs/2509.11157)
*Youquan Xian,Xueying Zeng,Mei Huang,Aoxiang Zhou,Xiaoyu Cui,Peng Liu,Lei Cui*

Main category: cs.NI

TL;DR: 针对加密流量分析中现有序列建模方法的局限性，本文提出UDFS表示和自适应阈值机制，以降低复杂性、提高区分度，并在多种场景下实现了卓越的分类性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分析的序列建模方法（流级别和跟踪级别）存在高特征冗余、区分能力受限或计算与存储开销过大等问题。此外，还面临类别特异性区分度差异的挑战。

Method: 1. 提出UDFS（Up-Down Flow Sequence）表示，将整个跟踪压缩为二维序列，通过聚合流的上下行流量来表征每个流，旨在降低复杂性并保持高区分度。
2. 提出自适应阈值机制，动态调整训练权重和拒绝边界，以解决类别特异性区分度差异，从而提升模型分类性能。

Result: 所提出的方法在粗粒度、细粒度数据集以及概念漂移和开放世界场景下，均展现出优越的分类性能和鲁棒性。

Conclusion: 本研究通过提出的UDFS表示和自适应阈值机制，有效克服了现有加密流量分析方法的局限性，显著提升了模型在各种复杂场景下的分类性能和鲁棒性。

Abstract: In recent years, sequence features such as packet length have received
considerable attention due to their central role in encrypted traffic analysis.
Existing sequence modeling approaches can be broadly categorized into
flow-level and trace-level methods: the former suffer from high feature
redundancy, limiting their discriminative power, whereas the latter preserve
complete information but incur substantial computational and storage overhead.
To address these limitations, we propose the \textbf{U}p-\textbf{D}own
\textbf{F}low \textbf{S}equence (\textbf{UDFS}) representation, which
compresses an entire trace into a two-dimensional sequence and characterizes
each flow by the aggregate of its upstream and downstream traffic, reducing
complexity while maintaining high discriminability. Furthermore, to address the
challenge of class-specific discriminability differences, we propose an
adaptive threshold mechanism that dynamically adjusts training weights and
rejection boundaries, enhancing the model's classification performance.
Experimental results demonstrate that the proposed method achieves superior
classification performance and robustness on both coarse-grained and
fine-grained datasets, as well as under concept drift and open-world scenarios.
Code and Dataset are available at https://github.com/kid1999/UDFS.

</details>


### [238] [Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation Intelligent Delay-Tolerant Networks](https://arxiv.org/abs/2509.11239)
*Zhekun Huang,Milena Radenkovic*

Main category: cs.NI

TL;DR: 本文提出一种结合机器学习的DTN智能路由增强，通过预测中继节点质量来优化消息传输，显著提升了紧急通信场景下的投递率并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有DTN路由协议采用静态复制策略，无法自适应地区分高质量中继节点，导致消息传播效率低下且次优。

Method: 将机器学习集成到Spray and Wait框架中，从仿真日志中提取动态特征训练MLP、SVM、RF等分类器以预测节点中继适用性。通过Flask RESTful API部署模型进行实时预测，并实现MLPBasedSprayRouter，选择性转发消息。引入缓存机制以降低计算开销。

Result: 在紧急移动场景下，该框架显著提高了消息投递率，同时降低了平均延迟。MLP分类器在准确性、适应性和推理速度方面表现最佳，优于SVM和RF。

Conclusion: 研究结果证实了将机器学习应用于DTN路由的新颖性和实用性，为智能城市和灾难恢复等动态环境中的弹性智能通信系统提供了基础。

Abstract: Delay Tolerant Networks (DTNs) are critical for emergency communication in
highly dynamic and challenging scenarios characterized by intermittent
connectivity, frequent disruptions, and unpredictable node mobility. While some
protocols are widely adopted for simplicity and low overhead, their static
replication strategy lacks the ability to adaptively distinguish high-quality
relay nodes, often leading to inefficient and suboptimal message dissemination.
To address this challenge, we propose a novel intelligent routing enhancement
that integrates machine learning-based node evaluation into the Spray and Wait
framework. Several dynamic, core features are extracted from simulation logs
and are used to train multiple classifiers - Multi-Layer Perceptron (MLP),
Support Vector Machine (SVM), and Random Forest (RF) - to predict whether a
node is suitable as a relay under dynamic conditions. The trained models are
deployed via a lightweight Flask-based RESTful API, enabling real-time,
adaptive predictions. We implement the enhanced router MLPBasedSprayRouter,
which selectively forwards messages based on the predicted relay quality. A
caching mechanism is incorporated to reduce computational overhead and ensure
stable, low-latency inference. Extensive experiments under realistic emergency
mobility scenarios demonstrate that the proposed framework significantly
improves delivery ratio while reducing average latency compared to the baseline
protocols. Among all evaluated classifiers, MLP achieved the most robust
performance, consistently outperforming both SVM and RF in terms of accuracy,
adaptability, and inference speed. These results confirm the novelty and
practicality of integrating machine learning into DTN routing, paving the way
for resilient and intelligent communication systems in smart cities, disaster
recovery, and other dynamic environments.

</details>


### [239] [Energy-Aware 6G Network Design: A Survey](https://arxiv.org/abs/2509.11289)
*Rashmi Kamran,Mahesh Ganesh Bhat,Pranav Jha,Shana Moothedath,Manjesh Hanawal,Prasanna Chaporkar*

Main category: cs.NI

TL;DR: 本文综述了6G网络中实现能源效率的设计方法，包括AI/ML、能量收集及标准化工作，并提出了开放性研究问题。


<details>
  <summary>Details</summary>
Motivation: 6G移动网络将支持大量用户和数据密集型应用，导致能源效率和可持续性面临严峻挑战。因此，将可持续性作为6G设计中的关键目标，构建能源感知型解决方案至关重要。

Method: 本文通过全面的文献综述，分析了用于设计能源高效6G网络的方法，例如能量收集、能源模型与参数、能源感知服务的分类以及基于AI/ML的解决方案。综述还包括展示能源感知益处的用例，并探讨了3GPP、ITU和IEEE等组织的标准化工作。

Result: 综述提供了能源高效6G网络设计方法的全面概述，展示了将能源感知融入网络决策的益处，并揭示了标准化工作的进展。

Conclusion: 本文总结了开放性研究问题和挑战，旨在使能源感知设计在6G网络中可行，并确保性能和能源目标的最佳化。

Abstract: 6th Generation (6G) mobile networks are envisioned to support several new
capabilities and data-centric applications for unprecedented number of users,
potentially raising significant energy efficiency and sustainability concerns.
This brings focus on sustainability as one of the key objectives in the their
design. To move towards sustainable solution, research and standardization
community is focusing on several key issues like energy information monitoring
and exposure, use of renewable energy, and use of Artificial
Intelligence/Machine Learning (AI/ML) for improving the energy efficiency in 6G
networks. The goal is to build energy-aware solutions that takes into account
the energy information resulting in energy efficient networks. Design of
energy-aware 6G networks brings in new challenges like increased overheads in
gathering and exposing of energy related information, and the associated user
consent management. The aim of this paper is to provide a comprehensive survey
of methods used for design of energy efficient 6G networks, like energy
harvesting, energy models and parameters, classification of energy-aware
services, and AI/ML-based solutions. The survey also includes few use cases
that demonstrate the benefits of incorporating energy awareness into network
decisions. Several ongoing standardization efforts in 3GPP, ITU, and IEEE are
included to provide insights into the ongoing work and highlight the
opportunities for new contributions. We conclude this survey with open research
problems and challenges that can be explored to make energy-aware design
feasible and ensure optimality regarding performance and energy goals for 6G
networks.

</details>


### [240] [Federated Edge Learning for Predictive Maintenance in 6G Small Cell Networks](https://arxiv.org/abs/2509.11421)
*Yusuf Emir Sezgin,Mehmet Özdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 本研究提出一种联邦边缘学习框架，用于6G小蜂窝网络的预测性维护，通过去中心化智能和本地训练，在保护隐私和减少通信开销的同时，实现与集中式训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络对自主性、可靠性和可扩展性提出高要求，但将敏感遥测数据传输到中央服务器引发隐私和带宽担忧，因此需要一种解决方案来解决这些问题。

Method: 采用知识定义网络（KDN）架构的联邦边缘学习框架。每个基站独立使用本地遥测数据（如SINR、抖动、延迟）训练故障预测模型，不共享原始数据。使用基于阈值的多标签编码检测并发故障。在ns-3 mmWave模拟环境中，通过Flower框架协调学习流程，并使用联邦平均（FedAvg）算法进行模型聚合，比较了集中式和联邦式训练策略的性能。

Result: 实验结果表明，在准确性和每标签精度方面，联邦模型实现了与集中式训练相当的性能。同时，该方法有效保护了数据隐私并显著减少了通信开销。

Conclusion: 联邦边缘学习是6G网络预测性维护的有效解决方案，它在不牺牲性能的前提下，通过去中心化智能解决了数据隐私和通信效率的挑战。

Abstract: The rollout of 6G networks introduces unprecedented demands for autonomy,
reliability, and scalability. However, the transmission of sensitive telemetry
data to central servers raises concerns about privacy and bandwidth. To address
this, we propose a federated edge learning framework for predictive maintenance
in 6G small cell networks. The system adopts a Knowledge Defined Networking
(KDN) architecture in Data, Knowledge, and Control Planes to support
decentralized intelligence, telemetry-driven training, and coordinated policy
enforcement. In the proposed model, each base station independently trains a
failure prediction model using local telemetry metrics, including SINR, jitter,
delay, and transport block size, without sharing raw data. A threshold-based
multi-label encoding scheme enables the detection of concurrent fault
conditions. We then conduct a comparative analysis of centralized and federated
training strategies to evaluate their performance in this context. A realistic
simulation environment is implemented using the ns-3 mmWave module,
incorporating hybrid user placement and base station fault injection across
various deployment scenarios. The learning pipeline is orchestrated via the
Flower framework, and model aggregation is performed using the Federated
Averaging (FedAvg) algorithm. Experimental results demonstrate that the
federated model achieves performance comparable to centralized training in
terms of accuracy and per-label precision, while preserving privacy and
reducing communication overhead.

</details>


### [241] [Towards Dynamic Urban Scene Synthesis: The Digital Twin Descriptor Service](https://arxiv.org/abs/2509.11810)
*Ioannis Tsampras,Georgios Stergiopoulos,Tanya Politi,Spyros Denazis*

Main category: cs.NI

TL;DR: 提出一种“数字孪生描述服务（DTDS）”，通过NGSI-LD整合几何资产和上下文信息，解决智慧城市数字孪生在集成、联邦化和互操作性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有城市数字孪生平台在集成、联邦化和适应性连接方面存在不足；数据流与表示解耦限制了仿真；几何中心标准难以整合动态上下文信息；多提供商联邦化支持薄弱，导致互操作性差。

Method: 提出“数字孪生描述服务（DTDS）”概念，利用NGSI-LD将几何资产和上下文信息的抽象引用融合到单个、可扩展的描述服务中，以实现动态和联邦化的集成。

Result: DTDS实现了上下文数据、表示和运行时同步的动态联邦化集成。本文概述了DTDS的架构组件和描述本体。

Conclusion: DTDS通过提供动态和联邦化的集成能力，解决了智慧城市数字孪生在集成、联邦化和互操作性方面的关键不足，支持现代智慧城市中的数字孪生过程。

Abstract: Digital twins have been introduced as supporters to city operations, yet
existing scene-descriptor formats and digital twin platforms often lack the
integration, federation, and adaptable connectivity that urban environments
demand. Modern digital twin platforms decouple data streams and representations
into separate architectural planes, fusing them only at the visualization layer
and limiting potential for simulation or further processing of the combined
assets. At the same time, geometry-centric file standards for digital twin
description, and services built on top of them, focus primarily on explicitly
declaring geometry and additional structural or photorealistic parameters,
making integration with evolving context information a complicated process
while limiting compatibility with newer representation methods. Additionally,
multi-provider federation, critical in smart city services where multiple
stakeholders may control distinct infrastructure or representation assets, is
sparsely supported. Consequently, most pilots isolate context and
representation, fusing them per use case with ad hoc components and custom
description files or glue code, which hinders interoperability. To address
these gaps, this paper proposes a novel concept, the 'Digital Twin Descriptor
Service (DTDS)' that fuses abstracted references to geometry assets and context
information within a single, extensible descriptor service through NGSI-LD. The
proposed DTDS provides dynamic and federated integration of context data,
representations, and runtime synchronization across heterogeneous engines and
simulators. This concept paper outlines the DTDS architectural components and
description ontology that enable digital-twin processes in the modern smart
city.

</details>


### [242] [Optimization for Massive 3D-RIS Deployment: A Generative Diffusion Model-Based Approach](https://arxiv.org/abs/2509.11969)
*Kaining Wang,Bo Yang,Zhiwen Yu,Xuelin Cao,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 针对现有RIS部署优化方法的不足，本文提出一种基于扩散模型的概率生成学习方法，将大规模3D RIS部署视为条件生成任务，并通过采样获取最优策略。仿真结果表明该方法在性能和泛化能力上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: RIS部署优化至关重要，但现有方法面临计算复杂度高、环境适应性差以及易陷入局部最优等挑战。

Method: 提出一种基于概率生成学习的扩散模型，用于优化大规模3D RIS部署。该方法将目标区域划分为网格，把RIS部署视为条件生成任务，通过训练扩散模型生成部署策略分布，进而通过采样获得最优部署策略。

Result: 仿真结果表明，所提出的基于扩散模型的方法在超越比率（exceed ratio）和泛化能力方面均优于传统的基准方法。

Conclusion: 所提出的基于扩散模型的RIS部署优化方法能有效解决传统方法的局限性，在优化大规模3D RIS部署方面展现出优越的性能和泛化能力。

Abstract: Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment
by modifying the amplitude, phase, and polarization of incoming waves,
significantly improving coverage performance. Notably, optimizing the
deployment of RISs becomes vital, but existing optimization methods face
challenges such as high computational complexity, limited adaptability to
changing environments, and a tendency to converge on local optima. In this
paper, we propose to optimize the deployment of large-scale 3D RISs using a
diffusion model based on probabilistic generative learning. We begin by
dividing the target area into fixed grids, with each grid corresponding to a
potential deployment location. Then, a multi-RIS deployment optimization
problem is formulated, which is difficult to solve directly. By treating RIS
deployment as a conditional generation task, the well-trained diffusion model
can generate the distribution of deployment strategies, and thus, the optimal
deployment strategy can be obtained by sampling from this distribution.
Simulation results demonstrate that the proposed diffusion-based method
outperforms traditional benchmark approaches in terms of exceed ratio and
generalization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [243] [Agent-based Simulation for Drone Charging in an Internet of Things Environment System](https://arxiv.org/abs/2509.10867)
*Leonardo Grando,José Roberto Emiliano Leite,Edson Luiz Ursini*

Main category: cs.MA

TL;DR: 本文提出一种基于Agent的仿真模型，用于协调物联网和工业4.0环境中无人机蜂群的电池充电，并通过智能农业用例进行验证。


<details>
  <summary>Details</summary>
Motivation: 优化物联网和工业4.0环境下无人机蜂群的电池使用和任务效率，特别是在大规模部署中实现自主协调充电。

Method: 开发基于Agent的仿真模型，详细描述仿真方法、系统架构和实现。采用机器学习技术分析仿真敏感性分析结果。

Result: 通过智能农业用例，展示了自主协调策略如何有效优化大规模无人机部署中的电池使用和任务效率。

Conclusion: 该基于Agent的仿真模型及其自主协调策略，能有效提升无人机蜂群在物联网和工业4.0应用（如智能农业）中的电池管理与任务效率。

Abstract: This paper presents an agent-based simulation model for coordinating battery
recharging in drone swarms, focusing on applications in Internet of Things
(IoT) and Industry 4.0 environments. The proposed model includes a detailed
description of the simulation methodology, system architecture, and
implementation. One practical use case is explored: Smart Farming, highlighting
how autonomous coordination strategies can optimize battery usage and mission
efficiency in large-scale drone deployments. This work uses a machine learning
technique to analyze the agent-based simulation sensitivity analysis output
results.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [244] [Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks](https://arxiv.org/abs/2509.06775)
*Po-Heng Chou,Pin-Qi Fu,Walid Saad,Li-Chun Wang*

Main category: eess.SY

TL;DR: 针对NR SL网络中授权/非授权频段分配的共存挑战，本文提出一种基于Agentic AI驱动的DDQN调度框架，通过自主感知和自适应策略显著降低阻塞率，提升QoS。


<details>
  <summary>Details</summary>
Motivation: NR SL网络在授权频段需与蜂窝通信共存，在非授权频段需与Wi-Fi共存，这带来了显著的频谱分配和共存挑战，传统规则/阈值方法难以保障服务质量。

Method: 提出一个Agentic AI驱动的DDQN调度框架。该调度器能自主感知队列动态、信道状况和共存状态，并自适应地调整调度策略以维持QoS，区别于传统基于规则或阈值的方法。

Result: 仿真结果显示，在有限授权带宽条件下，该框架相较于基于阈值的调度方法，阻塞率降低了高达87.5%。

Conclusion: 研究结果表明Agentic AI在为未来NR SL系统实现稳定、QoS感知和自适应调度方面具有巨大潜力。

Abstract: This paper presents an agentic artificial intelligence (AI)-driven double
deep Q-network (DDQN) scheduling framework for licensed and unlicensed band
allocation in New Radio (NR) sidelink (SL) networks. SL must share licensed
spectrum with cellular communications (CC) and unlicensed bands with Wi-Fi,
posing significant challenges for coexistence. Unlike prior rule-based or
threshold-based methods, the proposed agentic scheduler autonomously perceives
queueing dynamics, channel conditions, and coexistence states, and adapts its
policy to maintain quality-of-service (QoS). Simulation results show that our
framework reduces the blocking rate by up to 87.5% compared to threshold-based
scheduling under limited licensed bandwidth. These findings demonstrate the
potential of Agentic AI to enable stable, QoS-aware, and adaptive scheduling
for future NR SL systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [245] [Program Skeletons for Automated Program Translation](https://arxiv.org/abs/2504.07483)
*Bo Wang,Tianyu Li,Ruishi Li,Umang Mathur,Prateek Saxena*

Main category: cs.PL

TL;DR: 提出一种基于程序骨架的系统方法，实现编程语言间软件翻译的自动化，并在Python到JavaScript的翻译中显示出良好的可伸缩性和高自动化率。


<details>
  <summary>Details</summary>
Motivation: 编程语言间的软件翻译是一项艰巨的任务，自动化技术难以扩展到大型程序。主要困难在于需要将源程序的预期行为重新表达为目标语言的惯用构造，同时抽象掉源语言的细节并保持功能不变。

Method: 提出一种基于“程序骨架”（program skeletons）的系统自动化翻译方法。程序骨架通过抽象和总结低级具体代码片段来保留源程序的高级结构，骨架本身可机械翻译到目标语言。骨架设计允许以多种方式填充片段的具体实现，并可与现有数据驱动的代码合成器协同。该方法支持“可靠分解”，即每个片段正确翻译后，结合机械翻译的骨架，整个程序翻译也是正确的。

Result: 开发了原型系统Skel，实现了从Python到JavaScript的骨架式翻译。结果显示，相比现有工作，该方法具有显著的可伸缩性。对于9个真实世界的Python程序（部分超过1000行代码），95%的代码片段可以自动翻译，约5%需要手动干预。所有最终翻译在全程序测试套件下均正确。

Conclusion: 基于程序骨架的方法能够有效实现编程语言间软件翻译的自动化，显著提高了可伸伸缩性和自动化率，并确保了翻译结果的正确性，为解决跨语言软件迁移的难题提供了一条有前景的途径。

Abstract: Translating software between programming languages is a challenging task, for
which automated techniques have been elusive and hard to scale up to larger
programs. A key difficulty in cross-language translation is that one has to
re-express the intended behavior of the source program into idiomatic
constructs of a different target language. This task needs abstracting away
from the source language-specific details, while keeping the overall
functionality the same. In this work, we propose a novel and systematic
approach for making such translation amenable to automation based on a
framework we call program skeletons. A program skeleton retains the high-level
structure of the source program by abstracting away and effectively summarizing
lower-level concrete code fragments, which can be mechanically translated to
the target programming language. A skeleton, by design, permits many different
ways of filling in the concrete implementation for fragments, which can work in
conjunction with existing data-driven code synthesizers. Most importantly,
skeletons can conceptually enable sound decomposition, i.e., if each individual
fragment is correctly translated, taken together with the mechanically
translated skeleton, the final translated program is deemed to be correct as a
whole. We present a prototype system called Skel embodying the idea of
skeleton-based translation from Python to JavaScript. Our results show
promising scalability compared to prior works. For 9 real-world Python
programs, some with more than about 1k lines of code, 95% of their code
fragments can be automatically translated, while about 5% require manual
effort. All the final translations are correct with respect to whole-program
test suites.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [246] [Information Entropy-Based Scheduling for Communication-Efficient Decentralized Learning](https://arxiv.org/abs/2507.17426)
*Jaiprakash Nagar,Zheng Chen,Marios Kountouris,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 本文提出一种基于信息熵的调度策略，用于提升资源受限网络中D-SGD的通信效率和收敛速度，在节点和链接调度中均优于或媲美现有技术。


<details>
  <summary>Details</summary>
Motivation: 在资源受限网络中，提升去中心化随机梯度下降（D-SGD）算法的通信效率。

Method: 引入基于节点和链接的调度策略。提出一种基于信息熵的新颖重要性度量，用于确定节点和链接的调度概率，并在通信成本约束下随机激活部分节点或链接子集。

Result: 在节点调度中，该方法持续优于基于BC的方法，在通信预算降低高达60%时实现更快的收敛，在高预算下性能相当或更优。在链接调度中，性能优于或与MATCHA相当。

Conclusion: 所提出的基于信息熵的调度策略能有效增强资源受限网络中D-SGD的通信效率和收敛性，性能优于或媲美现有技术。

Abstract: This paper addresses decentralized stochastic gradient descent (D-SGD) over
resource-constrained networks by introducing node-based and link-based
scheduling strategies to enhance communication efficiency. In each iteration of
the D-SGD algorithm, only a few disjoint subsets of nodes or links are randomly
activated, subject to a given communication cost constraint. We propose a novel
importance metric based on information entropy to determine node and link
scheduling probabilities. We validate the effectiveness of our approach through
extensive simulations, comparing it against state-of-the-art methods, including
betweenness centrality (BC) for node scheduling and \textit{MATCHA} for link
scheduling. The results show that our method consistently outperforms the
BC-based method in the node scheduling case, achieving faster convergence with
up to 60\% lower communication budgets. At higher communication budgets (above
60\%), our method maintains comparable or superior performance. In the link
scheduling case, our method delivers results that are superior to or on par
with those of \textit{MATCHA}.

</details>


### [247] [Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications](https://arxiv.org/abs/2509.11636)
*Shiyao Jiang,Jian Jiao,Xingjian Zhang,Ye Wang,Dusit Niyato,Qinyu Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种名为TALSC的任务无关语义通信框架，通过引入元学习器（SCM）和基于KAN的SCM扩展，有效解决了知识库中异构数据偏差（如标签噪声和类别不平衡）对图像传输鲁棒性的影响，显著提高了语义恢复准确率。


<details>
  <summary>Details</summary>
Motivation: 在未来6G网络中，海量多样化数据使得任务无关语义通信系统需要提供鲁棒的智能服务。然而，现实世界知识库中存在的异构数据偏差（包括标签翻转噪声和类别不平衡）是影响图像传输鲁棒性的关键问题。

Method: 本文提出任务无关可学习加权知识库语义通信（TALSC）框架。该框架将样本置信度模块（SCM）作为元学习器，语义编码网络作为学习器。学习器依据可学习加权知识库（LW-KB）的经验知识进行更新。元学习器SCM根据任务损失反馈评估样本重要性，并调整学习器更新策略以增强未知任务的语义恢复鲁棒性。为平衡SCM参数与评估精度，设计了SCM-网格扩展（SCM-GE）方法，将Kolmogorov-Arnold网络（KAN）嵌入SCM，利用KAN的样条细化概念，实现可伸缩、可定制粒度的SCM，且无需重新训练。

Result: 仿真结果表明，TALSC框架有效缓解了任务无关图像语义通信中翻转噪声和类别不平衡的影响。与现有最先进方法相比，语义恢复准确率（SRA）和多尺度结构相似性（MS-SSIM）至少提高了12%。

Conclusion: TALSC框架通过其独特的元学习器设计和KAN增强的SCM，成功解决了知识库中异构数据偏差导致的鲁棒性问题，显著提升了任务无关图像语义通信的性能，超越了现有SOTA方法。

Abstract: With the emergence of diverse and massive data in the upcoming
sixth-generation (6G) networks, the task-agnostic semantic communication system
is regarded to provide robust intelligent services. In this paper, we propose a
task-agnostic learnable weighted-knowledge base semantic communication (TALSC)
framework for robust image transmission to address the real-world heterogeneous
data bias in KB, including label flipping noise and class imbalance. The TALSC
framework incorporates a sample confidence module (SCM) as meta-learner and the
semantic coding networks as learners. The learners are updated based on the
empirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,
the meta-learner evaluates the significance of samples according to the task
loss feedback, and adjusts the update strategy of learners to enhance the
robustness in semantic recovery for unknown tasks. To strike a balance between
SCM parameters and precision of significance evaluation, we design an SCM-grid
extension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)
within SCM, which leverages the concept of spline refinement in KAN and enables
scalable SCM with customizable granularity without retraining. Simulations
demonstrate that the TALSC framework effectively mitigates the effects of
flipping noise and class imbalance in task-agnostic image semantic
communication, achieving at least 12% higher semantic recovery accuracy (SRA)
and multi-scale structural similarity (MS-SSIM) compared to state-of-the-art
methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [248] [Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks](https://arxiv.org/abs/2410.06927)
*Friedrich Wolf-Monheim*

Main category: cs.SD

TL;DR: 本研究使用深度卷积神经网络（CNN）对多种声学特征表示进行音频分类性能评估，发现梅尔频谱图和梅尔频率倒谱系数（MFCCs）的表现显著优于其他特征。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNNs）在计算机视觉领域表现出色，也适用于通过从时域数字音频信号中提取的频谱和节奏特征进行声学分类。研究动机在于探究不同频谱和节奏特征表示（如梅尔频谱图、MFCCs等）在使用深度CNN进行音频分类任务时的性能差异。

Method: 研究采用深度卷积神经网络（CNN）对多种频谱和节奏特征表示进行音频分类性能评估。所 investigated 的特征包括梅尔频谱图、梅尔频率倒谱系数（MFCCs）、循环时间图、短时傅里叶变换（STFT）色度图、常Q变换（CQT）色度图和色度能量归一化统计（CENS）色度图。实验使用包含2,000个带标签环境音频记录的ESC-50数据集进行。

Result: 实验结果明确显示，梅尔频谱图（mel-scaled spectrograms）和梅尔频率倒谱系数（MFCCs）在使用深度CNN进行音频分类任务时，其表现显著优于本研究中 investigated 的其他频谱和节奏特征。

Conclusion: 对于使用深度卷积神经网络进行音频分类任务，梅尔频谱图和梅尔频率倒谱系数（MFCCs）是比其他所测试频谱和节奏特征更有效的特征表示。

Abstract: Convolutional neural networks (CNNs) are widely used in computer vision. They
can be used not only for conventional digital image material to recognize
patterns, but also for feature extraction from digital imagery representing
spectral and rhythm features extracted from time-domain digital audio signals
for the acoustic classification of sounds. Different spectral and rhythm
feature representations like mel-scaled spectrograms, mel-frequency cepstral
coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)
chromagrams, constant-Q transform (CQT) chromagrams and chroma energy
normalized statistics (CENS) chromagrams are investigated in terms of the audio
classification performance using a deep convolutional neural network. It can be
clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral
coefficients (MFCCs) perform significantly better than the other spectral and
rhythm features investigated in this research for audio classification tasks
using deep CNNs. The experiments were carried out with the aid of the ESC-50
dataset with 2,000 labeled environmental audio recordings.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [249] [Large-Scale Network Utility Maximization via GPU-Accelerated Proximal Message Passing](https://arxiv.org/abs/2509.10722)
*Akshay Sreekumar,Anthony Degleris,Ram Rajagopal*

Main category: math.OC

TL;DR: 本文提出一种GPU加速的近端消息传递算法，用于大规模网络效用最大化(NUM)，该算法在PyTorch中实现，与现有求解器相比，在处理千万级变量和约束问题时，能提供显著加速（4到20倍）并支持更大的问题规模，同时具有鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 网络效用最大化(NUM)是资源分配领域的基础问题，旨在通过在网络中分配资源以最大化总效用，同时遵守链路容量限制，因此需要高效且可扩展的求解方法。

Method: 开发了一种GPU加速的近端消息传递算法，该算法是ADMM的变体。它仅需与链路-路由矩阵进行稀疏矩阵-向量乘法和逐元素近端算子评估，从而实现跨流和链路的完全并行更新。该方法支持异构效用类型（包括对数效用），且不假设严格凹性。研究人员在PyTorch中实现了该方法。

Result: 该方法在具有千万级变量和约束的问题上，比现有CPU和GPU求解器快4到20倍，并且能够解决基线方法因内存耗尽而无法处理的问题规模。此外，该算法对拥堵和链路容量退化表现出鲁棒性。

Conclusion: 所提出的算法能在大规模网络资源分配问题中提供高效、可扩展且鲁棒的解决方案。通过时间扩展的交通座位分配案例研究，证明了该方法能够在现实网络中产生可解释的资源分配，具有实际应用价值。

Abstract: We present a GPU-accelerated proximal message passing algorithm for
large-scale network utility maximization (NUM). NUM is a fundamental problem in
resource allocation, where resources are allocated across various streams in a
network to maximize total utility while respecting link capacity constraints.
Our method, a variant of ADMM, requires only sparse matrix-vector multiplies
with the link-route matrix and element-wise proximal operator evaluations,
enabling fully parallel updates across streams and links. It also supports
heterogeneous utility types, including logarithmic utilities common in NUM, and
does not assume strict concavity. We implement our method in PyTorch and
demonstrate its performance on problems with tens of millions of variables and
constraints, achieving 4x to 20x speedups over existing CPU and GPU solvers and
solving problem sizes that exhaust the memory of baseline methods.
Additionally, we show that our algorithm is robust to congestion and
link-capacity degradation. Finally, using a time-expanded transit seat
allocation case study, we illustrate how our approach yields interpretable
allocations in realistic networks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [250] [A Uniqueness Theorem for Distributed Computation under Physical Constraint](https://arxiv.org/abs/2509.11754)
*Zhiyuan Ren,Mingxuan Lu,Wenchi Cheng*

Main category: cs.DC

TL;DR: 在INC等极端环境中，物理限制导致通信效率、有限内存和弹性伸缩的难题。本文通过建立公理系统，形式化推导出一个唯一的、最优的解决方案：自描述并行流（SDPF），揭示了物理定律下分布式计算流的必然范式。


<details>
  <summary>Details</summary>
Motivation: 传统计算模型忽略了物理硬件限制。但在网络内计算（INC）等极端环境中，这些限制变得至关重要，导致通信效率、有限内存和弹性伸缩性之间存在尖锐的三难困境。现有分布式范式无法应对这种严格的制度。

Method: 本文从工程权衡转向逻辑必然性，建立了一个严格的公理系统来形式化物理约束。通过数学证明，论文揭示了对于接受幂等合并操作符的广泛计算类别，存在一个唯一的、最优的范式。

Result: 任何满足公理的系统都必须收敛到一个单一的范式：自描述并行流（SDPF）。SDPF是一个纯粹以数据为中心的模型，其中无状态执行器处理携带自身控制逻辑的流。该唯一的范式被证明是收敛的、图灵完备的且最小的。

Conclusion: 类似于CAP定理定义了不可能，本文提供了一个建设性的对偶：一个唯一性定理，揭示了在物理定律下分布式计算流的“必然”范式，为受限环境下的分布式计算奠定了基础性原则。

Abstract: Foundational models of computation often abstract away physical hardware
limitations. However, in extreme environments like In-Network Computing (INC),
these limitations become inviolable laws, creating an acute trilemma among
communication efficiency, bounded memory, and robust scalability. Prevailing
distributed paradigms, while powerful in their intended domains, were not
designed for this stringent regime and thus face fundamental challenges. This
paper demonstrates that resolving this trilemma requires a shift in perspective
- from seeking engineering trade-offs to deriving solutions from logical
necessity. We establish a rigorous axiomatic system that formalizes these
physical constraints and prove that for the broad class of computations
admitting an idempotent merge operator, there exists a unique, optimal
paradigm. Any system satisfying these axioms must converge to a single normal
form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where
stateless executors process flows that carry their own control logic. We
further prove this unique paradigm is convergent, Turing-complete, and minimal.
In the same way that the CAP theorem established a boundary for what is
impossible in distributed state management, our work provides a constructive
dual: a uniqueness theorem that reveals what is \textit{inevitable} for
distributed computation flows under physical law.

</details>
