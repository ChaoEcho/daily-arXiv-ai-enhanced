<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
*Shaun Baek,Shaun Esua-Mensah,Cyrus Tsui,Sejan Vigneswaralingam,Abdullah Alali,Michael Lu,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为 Rosetta-PL 的基准测试，通过将 Lean 逻辑命题翻译成自定义逻辑语言来评估和提升大语言模型在低资源环境下的逻辑推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型主要在高资源自然语言上训练，这限制了它们在低资源环境和需要深度逻辑推理的任务中的效果。

Method: 构建了 Rosetta-PL 基准测试，将 Lean 的逻辑命题数据集翻译成自定义逻辑语言，并使用该数据集微调大语言模型（如 GPT-4o），分析数据集大小和翻译方法对模型性能的影响。

Result: 研究发现，在翻译过程中保持逻辑关系能显著提高模型精确度；当训练样本量超过约 20,000 条后，准确率趋于稳定。

Conclusion: 研究结果为优化大语言模型在形式推理任务中的训练以及提升在各种低资源语言应用中的表现提供了有价值的指导。

Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural
languages, limiting their effectiveness in low-resource settings and in tasks
requiring deep logical reasoning. This research introduces Rosetta-PL, a
benchmark designed to evaluate LLMs' logical reasoning and generalization
capabilities in a controlled environment. We construct Rosetta-PL by
translating a dataset of logical propositions from Lean into a custom logical
language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our
experiments analyze the impact of the size of the dataset and the translation
methodology on the performance of the model. Our results indicate that
preserving logical relationships in the translation process significantly
boosts precision, with accuracy plateauing beyond roughly 20,000 training
samples. These insights provide valuable guidelines for optimizing LLM training
in formal reasoning tasks and improving performance in various low-resource
language applications.

</details>


### [2] [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: 该论文指出计算主义在解释符号接地时存在悖论，无论心智计算是有意义符号还是无意义符号，最终都会导向语义天生论。


<details>
  <summary>Details</summary>
Motivation: 挑战计算主义（认为心智是数字计算机的观点），论证其无法解释符号接地（符号如何获得意义）的问题。

Method: 采用逻辑分析方法，提出一个两难推理：分析心智计算是基于“有意义符号”还是“无意义符号”这两种可能性及其对符号接地的影响。

Result: 如果心智基于有意义符号进行计算，则其功能预设了意义的存在，即导向语义天生论。如果心智基于无意义符号计算，则在符号接地之前不存在意向性认知过程，而任何接地过程都需要意向性认知过程，因此符号接地无法发生。

Conclusion: 无论计算主义假设心智计算是基于有意义符号还是无意义符号，它都蕴含着语义天生论，无法在非天生论的立场上成功解释符号接地。

Abstract: The paper presents a paradoxical feature of computational systems that
suggests that computationalism cannot explain symbol grounding. If the mind is
a digital computer, as computationalism claims, then it can be computing either
over meaningful symbols or over meaningless symbols. If it is computing over
meaningful symbols its functioning presupposes the existence of meaningful
symbols in the system, i.e. it implies semantic nativism. If the mind is
computing over meaningless symbols, no intentional cognitive processes are
available prior to symbol grounding. In this case, no symbol grounding could
take place since any grounding presupposes intentional cognitive processes. So,
whether computing in the mind is over meaningless or over meaningful symbols,
computationalism implies semantic nativism.

</details>


### [3] [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
*Zizhou Liu,Ziwei Gong,Lin Ai,Zheng Hui,Run Chen,Colin Wayne Leach,Michelle R. Greene,Julia Hirschberg*

Main category: cs.CL

TL;DR: 这篇论文综述了心理学理论如何应用于大型语言模型（LLM）开发的各个阶段（数据、预训练、后训练、评估与应用），以提升其类人认知、行为和交互能力。


<details>
  <summary>Details</summary>
Motivation: 鉴于心理学见解曾推动关键的NLP突破，并且随着LLM日益复杂，研究者认为心理学对于实现LLM的类人能力至关重要。

Method: 通过文献综述的方式，整合认知心理学、发展心理学、行为心理学、社会心理学、人格心理学和心理语言学的见解，分析它们在LLM开发不同阶段的应用现状。

Result: 分析揭示了当前心理学理论在LLM中应用的趋势和不足之处，并识别了心理学与NLP之间的跨领域联系和潜在冲突点。

Conclusion: 该研究旨在促进心理学与NLP的交叉融合，推动未来研究更深入、更审慎地将心理学理论整合到LLM的开发中。

Abstract: Psychological insights have long shaped pivotal NLP breakthroughs, including
the cognitive underpinnings of attention mechanisms, formative reinforcement
learning, and Theory of Mind-inspired social modeling. As Large Language Models
(LLMs) continue to grow in scale and complexity, there is a rising consensus
that psychology is essential for capturing human-like cognition, behavior, and
interaction. This paper reviews how psychological theories can inform and
enhance stages of LLM development, including data, pre-training, post-training,
and evaluation\&application. Our survey integrates insights from cognitive,
developmental, behavioral, social, personality psychology, and
psycholinguistics. Our analysis highlights current trends and gaps in how
psychological theories are applied. By examining both cross-domain connections
and points of tension, we aim to bridge disciplinary divides and promote more
thoughtful integration of psychology into future NLP research.

</details>


### [4] [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
*Danilo S. Carvalho,Yingji Zhang,Harriet Unsworth,André Freitas*

Main category: cs.CL

TL;DR: 提出了 LangVAE 框架，用于在预训练大语言模型 (LLM) 之上构建变分自编码器 (VAE)，以及 LangSpace 框架用于分析这些 VAE 产生的表示。


<details>
  <summary>Details</summary>
Motivation: 旨在将预训练 LLM 的知识编码成更紧凑、语义解耦的表示，并系统化地实验和理解文本表示。

Method: 构建 LangVAE（基于 LLM 的 VAE）和 LangSpace（包含向量遍历、插值、解耦度量和聚类可视化等探测方法）。进行了不同编码器/解码器组合和标注输入的实验。

Result: LangVAE 和 LangSpace 提供了一种灵活、高效、可扩展的方式来构建和分析文本表示，易于与 HuggingFace Hub 集成。实验揭示了不同架构和尺寸在泛化和解耦方面的相互作用。

Conclusion: 该框架为系统化实验和理解文本表示提供了一个有前景的方法。

Abstract: We present LangVAE, a novel framework for modular construction of variational
autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such
language model VAEs can encode the knowledge of their pre-trained components
into more compact and semantically disentangled representations. The
representations obtained in this way can be analysed with the LangVAE companion
framework: LangSpace, which implements a collection of probing methods, such as
vector traversal and interpolation, disentanglement measures, and cluster
visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable
way of building and analysing textual representations, with simple integration
for models available on the HuggingFace Hub. Additionally, we conducted a set
of experiments with different encoder and decoder combinations, as well as
annotated inputs, revealing a wide range of interactions across architectural
families and sizes w.r.t. generalisation and disentanglement. Our findings
demonstrate a promising framework for systematising the experimentation and
understanding of textual representations.

</details>


### [5] [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
*Hayden Helm,Tianyi Chen,Harvey McGuinness,Paige Lee,Brandon Duderstadt,Carey E. Priebe*

Main category: cs.CL

TL;DR: 论文证明基于语言模型的美国国会议员虚拟模型可作为数字孪生，其生成的推文可预测投票行为。


<details>
  <summary>Details</summary>
Motivation: 验证基于大量推文数据训练的语言模型能否满足国会议员数字孪生的定义，并探索其在政治分析中的应用价值。

Method: 收集每日更新的美国国会议员推文数据集，使用现代语言模型针对特定议员的数据进行训练，生成模拟推文，并利用这些推文预测议员的投票行为和跨党派倾向。

Result: 模型生成的推文与议员真实发布的推文难以区分，且能有效预测议员的投票行为和量化其跨党派的可能性。

Conclusion: 基于推文的语言模型可以构建有效的国会议员数字孪生，有助于利益相关者分配资源并可能影响立法动态，但该分析存在局限性。

Abstract: In this paper we provide evidence that a virtual model of U.S.
congresspersons based on a collection of language models satisfies the
definition of a digital twin. In particular, we introduce and provide
high-level descriptions of a daily-updated dataset that contains every Tweet
from every U.S. congressperson during their respective terms. We demonstrate
that a modern language model equipped with congressperson-specific subsets of
this data are capable of producing Tweets that are largely indistinguishable
from actual Tweets posted by their physical counterparts. We illustrate how
generated Tweets can be used to predict roll-call vote behaviors and to
quantify the likelihood of congresspersons crossing party lines, thereby
assisting stakeholders in allocating resources and potentially impacting
real-world legislative dynamics. We conclude with a discussion of the
limitations and important extensions of our analysis.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors](https://arxiv.org/abs/2505.00044)
*Richard Schmit*

Main category: cs.CV

TL;DR: 提出一种新框架，通过让小目标实例“借用”同类大目标的特征来提升单阶段检测器的小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 单阶段目标检测器在检测小目标时面临挑战，因为卷积特征图在空间分辨率和语义丰富度之间存在固有的权衡。

Method: 在SSD框架基础上引入三个关键模块：特征匹配块（FMB）用于跨层识别语义相似的描述符；特征表示块（FRB）通过加权聚合生成增强的浅层特征；特征融合块（FFB）通过整合原始、借用和上下文信息来优化特征图。

Result: 实验结果表明，该方法显著提高了小目标检测的准确率，优于基线方法，并保持了实时检测性能。

Conclusion: 该方法为复杂视觉环境下的鲁棒目标检测（特别是小目标检测）提供了一个有前景的方向。

Abstract: Detecting small objects remains a significant challenge in single-shot object
detectors due to the inherent trade-off between spatial resolution and semantic
richness in convolutional feature maps. To address this issue, we propose a
novel framework that enables small object representations to "borrow"
discriminative features from larger, semantically richer instances within the
same class. Our architecture introduces three key components: the Feature
Matching Block (FMB) to identify semantically similar descriptors across
layers, the Feature Representing Block (FRB) to generate enhanced shallow
features through weighted aggregation, and the Feature Fusion Block (FFB) to
refine feature maps by integrating original, borrowed, and context information.
Built upon the SSD framework, our method improves the descriptive capacity of
shallow layers while maintaining real-time detection performance. Experimental
results demonstrate that our approach significantly boosts small object
detection accuracy over baseline methods, offering a promising direction for
robust object detection in complex visual environments.

</details>


### [7] [Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design](https://arxiv.org/abs/2505.00134)
*Vasudev Sharma,Ahmed Alagha,Abdelhakim Khellaf,Vincent Quoc-Huy Trinh,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 本文研究了提示工程对三种视觉语言模型在大型消化道病理数据集上诊断癌症侵袭性和不典型增生状态性能的影响。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在计算病理学中潜力巨大，但其对大规模临床数据、任务制定和提示设计的敏感性，尤其是在诊断准确性方面，仍是未解问题。

Method: 研究者在一个包含3507张千兆像素WSI的内部消化道病理数据集上，系统地评估了三种先进的VLM（Quilt-Net, Quilt-LLAVA, CONCH）。通过结构化的消融研究，开发并应用了一个全面的提示工程框架，系统地调整领域特异性、解剖精确性、指令框架和输出约束。

Result: 提示工程显著影响模型性能。CONCH模型在获得精确解剖参考时准确率最高。降低解剖精确性会导致性能持续下降。模型复杂性本身并不保证高性能，领域对齐和特定领域训练至关重要。

Conclusion: 提示工程在计算病理学中至关重要，恰当的、包含精确解剖背景的领域提示能够显著提升VLM的诊断准确性。该研究为计算病理学中的提示工程建立了基础指南。

Abstract: Vision-language models (VLMs) have gained significant attention in
computational pathology due to their multimodal learning capabilities that
enhance big-data analytics of giga-pixel whole slide image (WSI). However,
their sensitivity to large-scale clinical data, task formulations, and prompt
design remains an open question, particularly in terms of diagnostic accuracy.
In this paper, we present a systematic investigation and analysis of three
state of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and
CONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each
in giga-pixel form, across distinct tissue types. Through a structured ablative
study on cancer invasiveness and dysplasia status, we develop a comprehensive
prompt engineering framework that systematically varies domain specificity,
anatomical precision, instructional framing, and output constraints. Our
findings demonstrate that prompt engineering significantly impacts model
performance, with the CONCH model achieving the highest accuracy when provided
with precise anatomical references. Additionally, we identify the critical
importance of anatomical context in histopathological image analysis, as
performance consistently degraded when reducing anatomical precision. We also
show that model complexity alone does not guarantee superior performance, as
effective domain alignment and domain-specific training are critical. These
results establish foundational guidelines for prompt engineering in
computational pathology and highlight the potential of VLMs to enhance
diagnostic accuracy when properly instructed with domain-appropriate prompts.

</details>


### [8] [Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis](https://arxiv.org/abs/2505.00135)
*Michal Geyer,Omer Tov,Linyi Jin,Richard Tucker,Inbar Mosseri,Tali Dekel,Noah Snavely*

Main category: cs.CV

TL;DR: 提出一种将文本到视频生成器转换为视频到立体视频生成器的方法，直接合成新的视点以创建3D效果，无需估计深度或进行图像扭曲。


<details>
  <summary>Details</summary>
Motivation: 立体3D视频生成因数据稀缺以及现有方法（估计视差/深度->扭曲->修复）在处理镜面或透明物体时的局限性而面临挑战。

Method: 利用预训练视频模型的先验知识（几何、材质、光学、语义），直接从输入视频合成一个偏移视点的视频帧，从而将2D视频转换为立体视频，避免了显式的几何估计和扭曲步骤。

Result: 该方法成功生成了具有令人信服的3D效果的立体视频，并在包含多样化材质和复杂成分的真实场景中展现了优势，避免了传统多阶段方法中因视差估计不准确导致的伪影。

Conclusion: 通过直接合成新视点，该方法有效克服了现有视频转立体视频技术的局限性，尤其是在处理具有挑战性的光学现象（如镜面反射和透明度）时表现更佳。

Abstract: The rising popularity of immersive visual experiences has increased interest
in stereoscopic 3D video generation. Despite significant advances in video
synthesis, creating 3D videos remains challenging due to the relative scarcity
of 3D video data. We propose a simple approach for transforming a text-to-video
generator into a video-to-stereo generator. Given an input video, our framework
automatically produces the video frames from a shifted viewpoint, enabling a
compelling 3D effect. Prior and concurrent approaches for this task typically
operate in multiple phases, first estimating video disparity or depth, then
warping the video accordingly to produce a second view, and finally inpainting
the disoccluded regions. This approach inherently fails when the scene involves
specular surfaces or transparent objects. In such cases, single-layer disparity
estimation is insufficient, resulting in artifacts and incorrect pixel shifts
during warping. Our work bypasses these restrictions by directly synthesizing
the new viewpoint, avoiding any intermediate steps. This is achieved by
leveraging a pre-trained video model's priors on geometry, object materials,
optics, and semantics, without relying on external geometry models or manually
disentangling geometry from the synthesis process. We demonstrate the
advantages of our approach in complex, real-world scenarios featuring diverse
object materials and compositions. See videos on
https://video-eye2eye.github.io

</details>


### [9] [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
*Minh-Hao Van,Xintao Wu*

Main category: cs.CV

TL;DR: 该研究利用视觉语言模型（VLM）来检测和消除（减轻）网络表情包（meme）中的仇恨内容，提出了一种定义引导的提示方法用于检测，以及一个名为 UnHateMeme 的统一框架用于消除仇恨。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多模态表情包常被滥用以传播仇恨言论，虽然仇恨表情包检测已有研究，但如何有效转化或消除这些仇恨内容仍是一个重大挑战。

Method: 研究人员利用视觉语言模型（VLM）的生成和推理能力。首先，提出了一种“定义引导的提示技术”来检测仇恨表情包。其次，构建了一个名为 UnHateMeme 的统一框架，通过替换仇恨文本和/或视觉元素来消除表情包中的仇恨内容。

Result: 使用定义引导的提示，VLM 在仇恨表情包检测任务上表现出色。UnHateMeme 框架结合 VLM，能够有效地将仇恨表情包转化为非仇恨形式，同时保持图像与文本的多模态连贯性，并符合人类对仇恨言论的判断标准。实验证明了 LLaVA、Gemini 和 GPT-4o 等先进预训练 VLM 在这些任务上的有效性。

Conclusion: 该研究表明，视觉语言模型（VLM）是检测和消除仇恨表情包的有效工具，其提出的方法有助于创建更安全、更尊重的在线环境。

Abstract: The rapid evolution of social media has provided enhanced communication
channels for individuals to create online content, enabling them to express
their thoughts and opinions. Multimodal memes, often utilized for playful or
humorous expressions with visual and textual elements, are sometimes misused to
disseminate hate speech against individuals or groups. While the detection of
hateful memes is well-researched, developing effective methods to transform
hateful content in memes remains a significant challenge. Leveraging the
powerful generation and reasoning capabilities of Vision-Language Models
(VLMs), we address the tasks of detecting and mitigating hateful content. This
paper presents two key contributions: first, a definition-guided prompting
technique for detecting hateful memes, and second, a unified framework for
mitigating hateful content in memes, named UnHateMeme, which works by replacing
hateful textual and/or visual components. With our definition-guided prompts,
VLMs achieve impressive performance on hateful memes detection task.
Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a
strong capability to convert hateful memes into non-hateful forms that meet
human-level criteria for hate speech and maintain multimodal coherence between
image and text. Through empirical experiments, we show the effectiveness of
state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the
proposed tasks, providing a comprehensive analysis of their respective
strengths and limitations for these tasks. This paper aims to shed light on
important applications of VLMs for ensuring safe and respectful online
environments.

</details>


### [10] [V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving](https://arxiv.org/abs/2505.00156)
*Jannik Lübberstedt,Esteban Rivera,Nico Uhlemann,Markus Lienkamp*

Main category: cs.CV

TL;DR: 提出 V3LMA 方法，通过融合大型语言模型 (LLM) 和大型视觉语言模型 (LVLM)，利用目标检测和视频生成的文本描述，无需微调即可提升自动驾驶场景下的 3D 理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型 (LVLM) 在理解 3D 环境方面能力有限，难以满足自动驾驶对动态环境全面、安全理解的需求。

Method: 引入 V3LMA 方法：1) 将 LLM 与 LVLM 集成；2) 利用从目标检测和视频输入生成的文本描述作为输入；3) 通过专门的预处理流程提取 3D 对象数据；4) 探索不同的融合策略和 token 组合。

Result: 该方法显著提升了性能，改善了复杂交通场景中的态势感知和决策能力，并在 LingoQA 基准测试中取得了 0.56 分。

Conclusion: 将 LLM 与 LVLM 相结合，利用文本描述增强 3D 场景理解，是提升交通场景解读能力、实现更安全自动驾驶系统的有效途径。

Abstract: Large Vision Language Models (LVLMs) have shown strong capabilities in
understanding and analyzing visual scenes across various domains. However, in
the context of autonomous driving, their limited comprehension of 3D
environments restricts their effectiveness in achieving a complete and safe
understanding of dynamic surroundings. To address this, we introduce V3LMA, a
novel approach that enhances 3D scene understanding by integrating Large
Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions
generated from object detections and video inputs, significantly boosting
performance without requiring fine-tuning. Through a dedicated preprocessing
pipeline that extracts 3D object data, our method improves situational
awareness and decision-making in complex traffic scenarios, achieving a score
of 0.56 on the LingoQA benchmark. We further explore different fusion
strategies and token combinations with the goal of advancing the interpretation
of traffic scenes, ultimately enabling safer autonomous driving systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TL;DR: 该立场文件回顾了人机协作的最新实证研究，指出现有工作缺乏统一理论框架，并提出了一个名为“分层探索-利用网络”的新概念架构来整合不同方法并指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 观察到当前人机协作领域的实证研究虽然众多，但缺乏一个统一的理论框架来系统地整合这些不同的成果，特别是在应对开放式、复杂任务时。

Method: 批判性地调研了人机协作领域的最新实证进展；提出了一个新的概念架构（分层探索-利用网络），该架构整合了多智能体协调、知识管理、控制反馈和高层控制机制；将现有的各种贡献（包括符号AI、基于LLM的智能体、混合组织实践等）映射到此框架上。

Result: 提出了一个统一的概念架构（分层探索-利用网络），该架构有助于梳理现有研究方法，促进对传统方法的修订，并启发融合定性与定量范式的新工作。

Conclusion: 所提出的框架为更深入地理解和设计人机共生系统提供了一个基础，促进了不同技术方法的融合，并为人类认知与人工智能能力的未来共同进化提供了方向。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [12] [UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces](https://arxiv.org/abs/2505.00472)
*Alaa Saleh,Sasu Tarkoma,Praveen Kumar Donta,Naser Hossein Motlagh,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: 提出了一种名为UserCentrix的智能记忆增强型AI框架，通过结合个性化大语言模型(LLM)代理和混合控制系统来优化智能空间。


<details>
  <summary>Details</summary>
Motivation: 当前的智能环境需要更动态、更适应用户偏好、更高效的AI系统来管理数据和资源。

Method: 开发了UserCentrix框架，该框架集成了：1) 个性化、具有记忆能力的LLM代理；2) 结合中心化和分布式处理的混合分层控制系统；3) 基于信息价值(VoI)的决策过程；4) 自我组织的任务紧急性主动扩展；5) 元推理个人LLM代理；6) 智能多代理协调系统。

Result: 实验证明，UserCentrix在提高响应准确性、系统效率和计算资源管理方面表现出色。

Conclusion: UserCentrix框架通过其动态、上下文感知和资源高效的特性，有效增强了智能空间的性能和用户体验。

Abstract: Agentic AI, with its autonomous and proactive decision-making, has
transformed smart environments. By integrating Generative AI (GenAI) and
multi-agent systems, modern AI frameworks can dynamically adapt to user
preferences, optimize data management, and improve resource allocation. This
paper introduces UserCentrix, an agentic memory-augmented AI framework designed
to enhance smart spaces through dynamic, context-aware decision-making. This
framework integrates personalized Large Language Model (LLM) agents that
leverage user preferences and LLM memory management to deliver proactive and
adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control
system, balancing centralized and distributed processing to optimize real-time
responsiveness while maintaining global situational awareness. UserCentrix
achieves resource-efficient AI interactions by embedding memory-augmented
reasoning, cooperative agent negotiation, and adaptive orchestration
strategies. Our key contributions include (i) a self-organizing framework with
proactive scaling based on task urgency, (ii) a Value of Information
(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM
agent, and (iv) an intelligent multi-agent coordination system for seamless
environment adaptation. Experimental results across various models confirm the
effectiveness of our approach in enhancing response accuracy, system
efficiency, and computational resource management in real-world application.

</details>


### [13] [First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images](https://arxiv.org/abs/2505.00173)
*Isabelle Bloch,Enzo Bonnot,Pietro Gori,Giammarco La Barbera,Sabine Sarnacki*

Main category: cs.AI

TL;DR: 本文提出一种基于模糊逻辑和形式化解剖学知识的方法，用于识别医学图像（特别是弥散磁共振图像）中的神经纤维束。


<details>
  <summary>Details</summary>
Motivation: 旨在解决解剖学描述中固有的不精确性问题，以便在医学图像中准确描述和识别神经纤维束，辅助外科医生进行手术规划。

Method: 采用结合模糊语义的一阶逻辑来形式化描述神经纤维轨迹的解剖学知识，定义了一种包含空间实体、关系和量词的语言，并基于此开发了用于分割和识别神经的空间推理算法。

Result: 该方法已成功应用于儿童骨盆神经的弥散磁共振图像，实现了神经的分割与识别。

Conclusion: 提出的基于模糊逻辑的形式化方法能够有效地利用解剖学知识来识别和分割医学图像中的神经结构，为手术规划提供了支持。

Abstract: This article deals with the description and recognition of fiber bundles, in
particular nerves, in medical images, based on the anatomical description of
the fiber trajectories. To this end, we propose a logical formalization of this
anatomical knowledge. The intrinsically imprecise description of nerves, as
found in anatomical textbooks, leads us to propose fuzzy semantics combined
with first-order logic. We define a language representing spatial entities,
relations between these entities and quantifiers. A formula in this language is
then a formalization of the natural language description. The semantics are
given by fuzzy representations in a concrete domain and satisfaction degrees of
relations. Based on this formalization, a spatial reasoning algorithm is
proposed for segmentation and recognition of nerves from anatomical and
diffusion magnetic resonance images, which is illustrated on pelvic nerves in
pediatric imaging, enabling surgeons to plan surgery.

</details>


### [14] [Real-World Gaps in AI Governance Research](https://arxiv.org/abs/2505.00174)
*Ilan Strauss,Isobel Moure,Tim O'Reilly,Sruly Rosenblat*

Main category: cs.AI

TL;DR: 对比顶尖AI公司与大学在生成式AI安全领域的研究，发现公司更关注部署前（对齐、测试），而部署阶段（偏见、特定风险领域）的研究关注度下降，存在知识鸿沟。


<details>
  <summary>Details</summary>
Motivation: 了解并比较领先AI公司和顶尖大学在生成式AI安全与可靠性方面的研究重点和潜在差距。

Method: 分析了2020年1月至2025年3月期间，从9439篇生成式AI论文中筛选出的1178篇安全与可靠性论文，对比了五家主要AI公司（Anthropic, Google DeepMind, Meta, Microsoft, OpenAI）和六所顶尖AI大学（CMU, MIT, NYU, Stanford, UC Berkeley, UW）的研究产出。

Result: 企业AI研究越来越集中在模型部署前的领域（如模型对齐、测试与评估），而对部署阶段问题（如模型偏见）的关注有所减弱。在高风险部署领域（如医疗、金融、虚假信息、诱导性与成瘾性特征、幻觉、版权）存在显著的研究空白。

Conclusion: 企业研究的日益集中可能导致对已部署AI系统行为的认知不足。建议扩大外部研究人员对部署数据的访问权限，并对市场上的AI行为进行系统性的观察。

Abstract: Drawing on 1,178 safety and reliability papers from 9,439 generative AI
papers (January 2020 - March 2025), we compare research outputs of leading AI
companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI
universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of
Washington). We find that corporate AI research increasingly concentrates on
pre-deployment areas -- model alignment and testing & evaluation -- while
attention to deployment-stage issues such as model bias has waned. Significant
research gaps exist in high-risk deployment domains, including healthcare,
finance, misinformation, persuasive and addictive features, hallucinations, and
copyright. Without improved observability into deployed AI, growing corporate
concentration could deepen knowledge deficits. We recommend expanding external
researcher access to deployment data and systematic observability of in-market
AI behaviors.

</details>


### [15] [RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset](https://arxiv.org/abs/2505.00204)
*Sumit Verma,Pritam Prasun,Arpit Jaiswal,Pritish Kumar*

Main category: cs.AI

TL;DR: 本文提出使用RAIL框架系统性评估大型语言模型（LLM）在真实世界应用中的伦理行为。


<details>
  <summary>Details</summary>
Motivation: 现有AI伦理框架虽强调公平、透明和问责，但缺乏可操作的评估方法来衡量AI系统（特别是LLM）是否符合伦理标准。

Method: 采用负责任AI实验室（RAIL）框架，该框架包含八个可衡量维度。将此框架应用于Anthropic的“Values in the Wild”数据集（包含Claude的匿名对话和标注的价值表达），将数据集中的价值映射到RAIL维度，并计算综合得分。

Result: 研究成功将真实世界对话中的价值表达映射到RAIL框架的维度，计算了综合得分，并为理解LLM在实际使用中的伦理行为提供了见解。

Conclusion: RAIL框架提供了一种系统化且可衡量的方法，能够利用真实世界数据评估LLM的规范行为，填补了现有伦理评估方法在可操作性上的不足。

Abstract: As AI systems become embedded in real-world applications, ensuring they meet
ethical standards is crucial. While existing AI ethics frameworks emphasize
fairness, transparency, and accountability, they often lack actionable
evaluation methods. This paper introduces a systematic approach using the
Responsible AI Labs (RAIL) framework, which includes eight measurable
dimensions to assess the normative behavior of large language models (LLMs). We
apply this framework to Anthropic's "Values in the Wild" dataset, containing
over 308,000 anonymized conversations with Claude and more than 3,000 annotated
value expressions. Our study maps these values to RAIL dimensions, computes
synthetic scores, and provides insights into the ethical behavior of LLMs in
real-world use.

</details>


### [16] [DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing](https://arxiv.org/abs/2505.00278)
*Lo Pang-Yun Ting,Yu-Hao Chiang,Yi-Tung Tsai,Hsu-Chao Lai,Kun-Ta Chuang*

Main category: cs.AI

TL;DR: 提出了一种名为 DeCo 的 AI 方法，通过构建缺陷感知图和基于对比学习的任务分配机制，优化集成电路（IC）测试任务分配给工程师，以提高处理成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的 IC 缺陷研究侧重于故障定位或分类，忽视了缺陷特征、历史故障和工程师经验的整合，导致在提高 IC 处理效率方面效果有限。需要一种整合这些因素以优化任务分配的方法。

Method: 1. 从 IC 测试报告构建新颖的缺陷感知图，捕捉共同失效关系以区分缺陷。 2. 利用图的局部和全局结构为工程师和任务生成缺陷感知表示。 3. 采用基于对比的分配机制，根据工程师的技能水平和当前工作量将测试任务分配给 QA 工程师。

Result: 在真实世界数据集上的实验表明，DeCo 在不同场景下均取得了最高的任务处理成功率（超过 80%），并且无论缺陷数据稀疏还是充足，都能保持工程师工作负载的均衡。案例研究还显示，DeCo 能够将任务分配给有潜力处理该任务的工程师，即使是他们不熟悉的缺陷类型。

Conclusion: DeCo 是一个有效的 AI 驱动解决方案，能够应用于实际的 IC 失效分析和任务处理场景，显著提高任务分配的效率和成功率，并平衡工程师的工作负载。

Abstract: In the semiconductor industry, integrated circuit (IC) processes play a vital
role, as the rising complexity and market expectations necessitate improvements
in yield. Identifying IC defects and assigning IC testing tasks to the right
engineers improves efficiency and reduces losses. While current studies
emphasize fault localization or defect classification, they overlook the
integration of defect characteristics, historical failures, and the insights
from engineer expertise, which restrains their effectiveness in improving IC
handling. To leverage AI for these challenges, we propose DeCo, an innovative
approach for optimizing task assignment in IC testing. DeCo constructs a novel
defect-aware graph from IC testing reports, capturing co-failure relationships
to enhance defect differentiation, even with scarce defect data. Additionally,
it formulates defect-aware representations for engineers and tasks, reinforced
by local and global structure modeling on the defect-aware graph. Finally, a
contrasting-based assignment mechanism pairs testing tasks with QA engineers by
considering their skill level and current workload, thus promoting an equitable
and efficient job dispatch. Experiments on a real-world dataset demonstrate
that DeCo achieves the highest task-handling success rates in different
scenarios, exceeding 80\%, while also maintaining balanced workloads on both
scarce or expanded defect data. Moreover, case studies reveal that DeCo can
assign tasks to potentially capable engineers, even for their unfamiliar
defects, highlighting its potential as an AI-driven solution for the real-world
IC failure analysis and task handling.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling](https://arxiv.org/abs/2505.00101)
*Barak Gahtan,Sanketh Vedula,Gil Samuelly Leichtag,Einat Kodesh,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 提出了一种利用消费级可穿戴设备数据（心率）和生理模型来实时预测跑步过程中瞬时摄氧量（VO2）的新框架。


<details>
  <summary>Details</summary>
Motivation: 理解跑步时的生理反应（如VO2）对优化表现、定制训练和健康管理至关重要，但现有方法往往局限于实验室。需要一种能通过普通可穿戴设备进行实时VO2预测的方法。

Method: 该框架包含两个互补模型：1) 使用生理约束的常微分方程（ODE）和神经卡尔曼滤波器精确建模心率（HR）动态，基于超过300万次HR观测数据训练；2) 利用精确的HR建模原理，构建一种新的VO2预测架构，仅需初始1秒的VO2数据进行校准，即可根据可穿戴设备数据（智能手表和心率胸带）进行序列到序列的代谢需求估计。

Result: 心率模型在1秒间隔预测上实现了低至2.81 bpm的平均绝对误差（相关性0.87）。VO2预测模型仅依赖消费级可穿戴设备数据，平均绝对百分比误差约为13%，能有效捕捉不同跑步强度下的快速生理转变和稳态条件。还建立了一个包含血乳酸测量的同步数据集。

Conclusion: 该框架将生理约束嵌入现代机器学习中，成功实现了通过消费级可穿戴设备预测VO2轨迹，使先进的代谢监测更加普及，弥合了实验室精度与日常可及性之间的差距，惠及精英运动员和健身爱好者。

Abstract: Understanding physiological responses during running is critical for
performance optimization, tailored training prescriptions, and athlete health
management. We introduce a comprehensive framework -- what we believe to be the
first capable of predicting instantaneous oxygen consumption (VO$_{2}$)
trajectories exclusively from consumer-grade wearable data. Our approach
employs two complementary physiological models: (1) accurate modeling of heart
rate (HR) dynamics via a physiologically constrained ordinary differential
equation (ODE) and neural Kalman filter, trained on over 3 million HR
observations, achieving 1-second interval predictions with mean absolute errors
as low as 2.81\,bpm (correlation 0.87); and (2) leveraging the principles of
precise HR modeling, a novel VO$_{2}$ prediction architecture requiring only
the initial second of VO$_{2}$ data for calibration, enabling robust,
sequence-to-sequence metabolic demand estimation. Despite relying solely on
smartwatch and chest-strap data, our method achieves mean absolute percentage
errors of approximately 13\%, effectively capturing rapid physiological
transitions and steady-state conditions across diverse running intensities. Our
synchronized dataset, complemented by blood lactate measurements, further lays
the foundation for future noninvasive metabolic zone identification. By
embedding physiological constraints within modern machine learning, this
framework democratizes advanced metabolic monitoring, bridging laboratory-grade
accuracy and everyday accessibility, thus empowering both elite athletes and
recreational fitness enthusiasts.

</details>


### [18] [Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter](https://arxiv.org/abs/2505.00131)
*Dalton Durant,Renato Zanetti*

Main category: cs.LG

TL;DR: 提出了一种基于核的集合高斯混合概率假设密度(EnGM-PHD)滤波器，用于多目标滤波，结合了GM-PHD和SMC-PHD的优点。


<details>
  <summary>Details</summary>
Motivation: 旨在结合GM-PHD滤波器的高斯混合技术和SMC-PHD滤波器的粒子技术，开发一种性能更优的多目标滤波器。

Method: 提出EnGM-PHD滤波器：从后验强度函数获取粒子，通过系统动力学传播粒子，然后使用核密度估计(KDE)技术来近似先验强度函数的高斯混合。

Result: 实验表明，在分量或粒子数量相同的情况下，EnGM-PHD滤波器比GM-PHD和SMC-PHD滤波器实现了更好的多目标滤波性能。

Conclusion: EnGM-PHD滤波器是一种有效的多目标滤波方法，其性能优于标准的GM-PHD和SMC-PHD滤波器，并保证了分量数趋于无穷时收敛到真实强度函数。

Abstract: In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis
Density (EnGM-PHD) filter is presented for multi-target filtering applications.
The EnGM-PHD filter combines the Gaussian-mixture-based techniques of the
Gaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the
particle-based techniques of the Sequential Monte Carlo Probability Hypothesis
Density (SMC-PHD) filter. It achieves this by obtaining particles from the
posterior intensity function, propagating them through the system dynamics, and
then using Kernel Density Estimation (KDE) techniques to approximate the
Gaussian mixture of the prior intensity function. This approach guarantees
convergence to the true intensity function in the limit of the number of
components. Moreover, in the special case of a single target with no births,
deaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces
to the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented
experiment, the results indicate that the EnGM-PHD filter achieves better
multi-target filtering performance than both the GM-PHD and SMC-PHD filters
while using the same number of components or particles.

</details>


### [19] [GPRat: Gaussian Process Regression with Asynchronous Tasks](https://arxiv.org/abs/2505.00136)
*Maksim Helmann,Alexander Strack,Dirk Pflüger*

Main category: cs.LG

TL;DR: 通过pybind11将基于HPX异步运行时的C++代码绑定到Python，开发了并行高斯过程库GPRat，展示了其在多核CPU上优于GPyTorch和GPflow的训练和预测性能及扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前Python AI库依赖底层BLAS并行化，可能导致性能和扩展性瓶颈。

Method: 使用pybind11将基于C++异步运行时模型HPX的任务型代码绑定到Python API，并以此开发了一个并行高斯过程库GPRat。

Result: 绑定开销几乎为零。GPRat在多核CPU（最多64核）训练时展现出优越的扩展性。预测速度上，GPRat比GPyTorch快7.63倍，比GPflow快25.25倍，并且随着特征数量增加，优势更加明显。

Conclusion: 在Python AI应用中使用异步任务可以显著提升性能和扩展性。

Abstract: Python is the de-facto language for software development in artificial
intelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow,
rely on parallelization built into their BLAS backends to achieve speedup on
CPUs. However, only applying parallelization in a low-level backend can lead to
performance and scaling degradation. In this work, we present a novel way of
binding task-based C++ code built on the asynchronous runtime model HPX to a
high-level Python API using pybind11. We develop a parallel Gaussian process
(GP) li- brary as an application. The resulting Python library GPRat combines
the ease of use of commonly available GP libraries with the performance and
scalability of asynchronous runtime systems. We evaluate the per- formance on a
mass-spring-damper system, a standard benchmark from control theory, for
varying numbers of regressors (features). The results show almost no binding
overhead when binding the asynchronous HPX code using pybind11. Compared to
GPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD
EPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction
speedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number
of features from eight to 128, we observe speedups of 29.62 and 21.19,
respectively. These results showcase the potential of using asynchronous tasks
within Python-based AI applications.

</details>


### [20] [Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search](https://arxiv.org/abs/2505.00162)
*Nuojin Cheng,Alireza Doostan,Stephen Becker*

Main category: cs.LG

TL;DR: 提出了一种名为 BF-SSD 的新型零阶优化算法，通过结合低保真度和高保真度函数评估来构建代理模型，以减少计算昂贵优化问题中的函数查询成本。


<details>
  <summary>Details</summary>
Motivation: 在目标函数和梯度评估计算成本高昂的场景下，优化是一个挑战。虽然零阶方法在梯度不可用时有效，但其函数查询的高成本限制了实际性能。

Method: 引入双保真度随机子空间下降 (BF-SSD) 算法。该方法利用低成本的低保真度 (LF) 和准确的高保真度 (HF) 函数评估构建代理模型，并基于此模型进行高效的回溯线搜索来确定步长。

Result: 在四个不同问题（综合基准、核岭回归、对抗攻击、语言模型微调）上的实验表明，与基线方法相比，BF-SSD 在显著减少高保真度函数评估次数的同时，取得了更优的优化性能。

Conclusion: BF-SSD 通过将双保真度策略整合到零阶优化中，为解决大规模、高维度的计算昂贵优化问题提供了一种有前景且计算高效的方法。

Abstract: Efficient optimization remains a fundamental challenge across numerous
scientific and engineering domains, especially when objective function and
gradient evaluations are computationally expensive. While zeroth-order
optimization methods offer effective approaches when gradients are
inaccessible, their practical performance can be limited by the high cost
associated with function queries. This work introduces the bi-fidelity
stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order
optimization method designed to reduce this computational burden. BF-SSD
leverages a bi-fidelity framework, constructing a surrogate model from a
combination of computationally inexpensive low-fidelity (LF) and accurate
high-fidelity (HF) function evaluations. This surrogate model facilitates an
efficient backtracking line search for step size selection, for which we
provide theoretical convergence guarantees under standard assumptions. We
perform a comprehensive empirical evaluation of BF-SSD across four distinct
problems: a synthetic optimization benchmark, dual-form kernel ridge
regression, black-box adversarial attacks on machine learning models, and
transformer-based black-box language model fine-tuning. Numerical results
demonstrate that BF-SSD consistently achieves superior optimization performance
while requiring significantly fewer HF function evaluations compared to
relevant baseline methods. This study highlights the efficacy of integrating
bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as
a promising and computationally efficient approach for tackling large-scale,
high-dimensional problems encountered in various real-world applications.

</details>


### [21] [GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation](https://arxiv.org/abs/2505.00169)
*Filipp Nikitin,Ian Dunn,David Ryan Koes,Olexandr Isayev*

Main category: cs.LG

TL;DR: 本文指出现有3D分子生成评估方法的缺陷，并提出修正后的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估深度生成模型生成3D分子结构的GEOM-Drugs基准测试协议存在严重缺陷，包括错误的价态定义、键级计算错误以及使用的力场与参考数据不一致。

Method: 重新审视GEOM-Drugs数据集，修正数据预处理中的问题，构建化学上准确的价态表，并引入基于GFN2-xTB的几何结构和能量基准测试。使用此新框架重新训练和评估了几个主流模型。

Result: 提供了更新后的模型性能指标，并为未来的基准测试提出了实用建议。

Conclusion: 研究结果强调了在3D分子生成领域采用化学上严谨的评估实践的必要性。

Abstract: Deep generative models have shown significant promise in generating valid 3D
molecular structures, with the GEOM-Drugs dataset serving as a key benchmark.
However, current evaluation protocols suffer from critical flaws, including
incorrect valency definitions, bugs in bond order calculations, and reliance on
force fields inconsistent with the reference data. In this work, we revisit
GEOM-Drugs and propose a corrected evaluation framework: we identify and fix
issues in data preprocessing, construct chemically accurate valency tables, and
introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and
re-evaluate several leading models under this framework, providing updated
performance metrics and practical recommendations for future benchmarking. Our
results underscore the need for chemically rigorous evaluation practices in 3D
molecular generation. Our recommended evaluation methods and GEOM-Drugs
processing scripts are available at
https://github.com/isayevlab/geom-drugs-3dgen-evaluation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [Q Cells in Wireless Networks](https://arxiv.org/abs/2505.00138)
*Martin Haenggi*

Main category: cs.NI

TL;DR: 论文提出使用“Q单元”（少量圆盘的交集）作为一种解析方法，来界定和估计由一组无线发射器提供最低服务质量保障的覆盖区域。


<details>
  <summary>Details</summary>
Motivation: 确定是否有可能解析地刻画一组发射器（如蜂窝基站或WiFi接入点）能够保证用户最低服务质量的覆盖位置集合。

Method: 引入“Q单元”（Q cells）作为关键几何元素，即少量圆盘的交集。单个发射器的Q单元是其服务区域的外边界，所有Q单元的并集构成了整个覆盖区域的外边界。在无限网络中，利用信号干扰比（SINR）的元分布（meta distribution）来缩放Q单元，以更精确地估计覆盖区域。

Result: 论文肯定地回答了研究动机中的问题，提供了覆盖区域的显式、简单的外边界和估计方法。证明了Q单元是有效的界定工具，并且在无限网络模型中可以通过结合元分布信息进行精确估计。

Conclusion: 该研究成功提供了一种基于Q单元的解析方法，用于表征和估计无线发射器网络的覆盖区域。

Abstract: For a given set of transmitters such as cellular base stations or WiFi access
points, is it possible to analytically characterize the set of locations that
are "covered" in the sense that users at these locations experience a certain
minimum quality of service? In this paper, we affirmatively answer this
question, by providing explicit simple outer bounds and estimates for the
coverage manifold. The key geometric elements of our analytical method are the
Q cells, defined as the intersections of a small number of disks. The Q cell of
a transmitter is an outer bound to the service region of the transmitter, and,
in turn, the union of Q cells is an outer bound to the coverage manifold. In
infinite networks, connections to the meta distribution of the
signal-to-interference ratio allow for a scaling of the Q cells to obtain
accurate estimates of the coverage manifold.

</details>


### [23] [Edge Large AI Models: Revolutionizing 6G Networks](https://arxiv.org/abs/2505.00321)
*Zixin Wang,Yuanming Shi,Yong Zhou,Jingyang Zhu,Khaled. B. Letaief*

Main category: cs.NI

TL;DR: 论文探讨了在资源受限的6G边缘网络中部署大型AI模型（LAM）的机遇与挑战，提出了协同训练和推理框架，并探索了其在空口设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型（LAM）潜力巨大，但其庞大的模型尺寸和高昂的通信开销给在资源有限的边缘设备上部署（尤其为支持6G实时智能服务）带来了巨大挑战，传统边缘AI方法难以满足需求。

Method: 研究了模型分解和资源管理策略。提出了协同微调、全参数训练框架，以及一个微服务辅助的推理架构来促进边缘LAM在无线网络上的部署。同时，探讨了边缘LAM在信道预测和波束赋形等空口设计中的应用。

Result: 识别了边缘LAM部署面临的主要障碍（通信、计算、存储资源限制，模型参数多，通信开销大），并提出了创新的框架（协同微调、全参数训练、微服务推理）和具体应用（信道预测、波束赋形）作为应对这些挑战的解决方案。

Conclusion: 边缘LAM是赋能6G各项服务的关键技术，但其部署需要克服资源和模型复杂性的挑战。本文提出的框架和应用为在6G中成功部署和利用边缘LAM提供了有价值的见解和解决方案。

Abstract: Large artificial intelligence models (LAMs) possess human-like abilities to
solve a wide range of real-world problems, exemplifying the potential of
experts in various domains and modalities. By leveraging the communication and
computation capabilities of geographically dispersed edge devices, edge LAM
emerges as an enabling technology to empower the delivery of various real-time
intelligent services in 6G. Unlike traditional edge artificial intelligence
(AI) that primarily supports a single task using small models, edge LAM is
featured by the need of the decomposition and distributed deployment of large
models, and the ability to support highly generalized and diverse tasks.
However, due to limited communication, computation, and storage resources over
wireless networks, the vast number of trainable neurons and the substantial
communication overhead pose a formidable hurdle to the practical deployment of
edge LAMs. In this paper, we investigate the opportunities and challenges of
edge LAMs from the perspectives of model decomposition and resource management.
Specifically, we propose collaborative fine-tuning and full-parameter training
frameworks, alongside a microservice-assisted inference architecture, to
enhance the deployment of edge LAM over wireless networks. Additionally, we
investigate the application of edge LAM in air-interface designs, focusing on
channel prediction and beamforming. These innovative frameworks and
applications offer valuable insights and solutions for advancing 6G technology.

</details>


### [24] [Deterministic Scheduling over Wi-Fi 6 using Target Wake Time: An Experimental Approach](https://arxiv.org/abs/2505.00447)
*Govind Rajendran,Samar Agnihotri*

Main category: cs.NI

TL;DR: 利用Wi-Fi 6的TWT功能实现确定性调度，以改善拥塞网络中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi的CSMA/CA机制在拥塞场景下存在争用开销大、无法提供性能保证的问题。

Method: 通过在Wi-Fi 6测试平台上进行TWT实验，分析其性能影响因素；构建并求解优化问题以获得确定性调度的最优参数；在测试平台上配置最优参数并与传统CSMA/CA进行比较。

Result: 基于TWT的确定性调度在支持TWT的客户端和整体系统性能方面，始终优于传统的CSMA/CA调度。

Conclusion: 使用Wi-Fi 6的TWT功能进行确定性调度，可以有效提升Wi-Fi网络性能，特别是在拥塞环境下。

Abstract: Wi-Fi networks traditionally use Distributed Coordination Function (DCF) that
employs CSMA/CA along with the binary backoff mechanism for channel access.
This causes unavoidable contention overheads and does not provide performance
guarantees. In this work, we outline some issues that occur with the
probabilistic channel access in highly congested scenarios and how those can be
mitigated using deterministic scheduling. Towards this, we propose to use
Target Wake Time (TWT) - a feature introduced in Wi-Fi 6 as a power-saving
mechanism, to improve the performance of Wi-Fi. To gain insights into the
workings of the TWT over commercially available off-the-shelf components and to
analyze the factors that affect its performance, we carry out various
experiments with it over our Wi-Fi 6 testbed. Using these insights and
analysis, we formulate and solve an optimization problem to synthesize
deterministic schedules and obtain the optimal values of various system
parameters. Lastly, we configure our testbed with these optimal parameter
values and show that the TWT based deterministic scheduling consistently
results in better performance of the TWT-capable clients and overall system
performance compared to traditional CSMA/CA based scheduling.

</details>


### [25] [Surviving the Storm: The Impacts of Open RAN Disaggregation on Latency and Resilience](https://arxiv.org/abs/2505.00605)
*Sotiris Chatzimiltis,Mohammad Shojafar,Mahdi Boloursaz Mashhadi,Rahim Tafazolli*

Main category: cs.NI

TL;DR: 分析了 Open RAN 解耦架构对 UE 接入时延和信令风暴风险的影响，并提出一种自适应机制以提高系统韧性。


<details>
  <summary>Details</summary>
Motivation: Open RAN 的解耦架构带来了灵活性但也引入了额外的初始接入时延，增加了信令风暴的风险。本研究旨在量化这种时延影响，并探究如何利用 Open RAN 的灵活性来缓解风险。

Method: 对基带单元 (BBU) 解耦为中央单元 (CU) 和分布单元 (DU) 造成的 UE 接入时延进行建模；分析不同负载和处理时间下的性能；引入新的效用函数来量化韧性；提出一种新的自适应机制来增强 Open RAN 对抗信令风暴的鲁棒性，并通过仿真进行评估。

Result: 仿真结果表明，Open RAN 解耦增加了接入时延和对信令拥塞的敏感性。但提出的自适应机制能显著增强系统韧性，相比固定配置提升高达 286%，最优条件下韧性评分接近 0.96。

Conclusion: 虽然 Open RAN 解耦增加了时延和拥塞风险，但其架构的灵活性允许通过自适应机制等手段有效缓解这些问题，从而提高系统在高负载下的整体韧性。

Abstract: The development of Open Radio Access Networks (Open RAN), with their
disaggregated architectures and virtualization of network functions, has
brought considerable flexibility and cost savings to mobile networks. However,
these architectural advancements introduce additional latency during the
initial attachment procedure of User Equipment (UE), increasing the risk of
signaling storms. This paper investigates the latency impact due to
disaggregation of the Base-band Unit (BBU) into the Central Unit (CU) and
Distributed Unit (DU). Specifically, we model the delays induced due to
disaggregation on UE attachment, analyzing the performance under varying load
conditions, and sensitivity to processing times. We demonstrate that while both
monolithic and Open RAN architectures experience performance degradation under
high-load conditions, Open RAN's added overheads can increase its
susceptibility to congestion and signaling storms. However, Open RAN's inherent
flexibility, enabled by disaggregation and virtualization, allows efficient
deployment of resources, faster service deployment, and adaptive congestion
control mechanisms to mitigate these risks and enhance overall system
resilience. Thereby, we quantify resilience by introducing a new utility
function and propose a novel adaptation mechanism to reinforce Open RAN's
robustness against signaling storms. Our results show that the proposed
adaptive mechanism significantly enhances resilience, achieving improvements of
up to 286% over fixed configurations, with resilience scores approaching 0.96
under optimal conditions. While simulation results show that Open RAN
disaggregation increases attachment latency and susceptibility to signaling
congestion, they also highlight that its architectural flexibility can mitigate
these effects, improving resilience under high-load conditions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [26] [RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks](https://arxiv.org/abs/2505.00618)
*Gurjot Singh,Alim Dhanani,Diogo Barradas*

Main category: cs.CR

TL;DR: 提出RevealNet，一个使用P4可编程交换机进行流量关联的去中心化攻击溯源框架，解决了传统方法的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 网络攻击者利用代理、VPN等隐藏身份，现有的中心化流量关联溯源方法在大型高速网络中扩展性差。

Method: 采用名为RevealNet的去中心化框架，协调P4可编程交换机群组。利用基于流草图（flow sketches）的关联原语，在网络内部高效、分布式地执行流量关联。

Result: RevealNet在准确性上与中心化系统相当，同时显著降低了计算复杂度和带宽开销。

Conclusion: RevealNet为网络攻击溯源提供了一种高效、可扩展的分布式解决方案，优于传统的中心化方法。

Abstract: Network attackers have increasingly resorted to proxy chains, VPNs, and
anonymity networks to conceal their activities. To tackle this issue, past
research has explored the applicability of traffic correlation techniques to
perform attack attribution, i.e., to identify an attacker's true network
location. However, current traffic correlation approaches rely on
well-provisioned and centralized systems that ingest flows from multiple
network probes to compute correlation scores. Unfortunately, this makes
correlation efforts scale poorly for large high-speed networks.
  In this paper, we propose RevealNet, a decentralized framework for attack
attribution that orchestrates a fleet of P4-programmable switches to perform
traffic correlation. RevealNet builds on a set of correlation primitives
inspired by prior work on computing and comparing flow sketches -- compact
summaries of flows' key characteristics -- to enable efficient, distributed,
in-network traffic correlation. Our evaluation suggests that RevealNet achieves
comparable accuracy to centralized attack attribution systems while
significantly reducing both the computational complexity and bandwidth
overheads imposed by correlation tasks.

</details>
