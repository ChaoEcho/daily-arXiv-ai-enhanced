{"id": "2507.19511", "pdf": "https://arxiv.org/pdf/2507.19511", "abs": "https://arxiv.org/abs/2507.19511", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "categories": ["cs.CL", "cs.LG"], "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5728Reddit\u6570\u636e\u4e0a\u4f7f\u7528Transformer\u6a21\u578b\u548cLSTM\u6a21\u578b\u8fdb\u884c\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5206\u7c7b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793aTransformer\u6a21\u578b\uff08\u7279\u522b\u662fRoBERTa\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u7ed3\u5408BERT\u5d4c\u5165\u7684LSTM\u6a21\u578b\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u5177\u6709\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u65e5\u76ca\u666e\u904d\uff0c\u4e9f\u9700\u5f00\u53d1\u5f3a\u5927\u3001\u81ea\u52a8\u5316\u7684\u5de5\u5177\u8fdb\u884c\u65e9\u671f\u68c0\u6d4b\u548c\u76d1\u6d4b\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7279\u522b\u662f\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u6587\u672c\u5206\u6790\u4e2d\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578bReddit\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5206\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5224\u65ad\u5206\u6790\u548c\u4e3b\u9898\u5efa\u6a21\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002\u968f\u540e\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684Transformer\u6a21\u578b\uff08BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA\uff09\u4e0e\u57fa\u4e8e\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u4e0d\u540c\u7684\u6587\u672c\u5d4c\u5165\u6280\u672f\uff0c\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTransformer\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002RoBERTa\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u5185\u90e8\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u4e3a99.54%\uff0c\u5728\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u4e3a96.05%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u589e\u5f3a\u4e86BERT\u5d4c\u5165\u7684LSTM\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684F1\u5206\u6570\u8d85\u8fc794%\uff0c\u540c\u65f6\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u663e\u8457\u66f4\u5c11\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u4e8e\u4e34\u5e8a\u5e94\u7528\u548c\u6570\u5b57\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u6700\u5148\u8fdbNLP\u65b9\u6cd5\u5728\u7cbe\u795e\u969c\u788d\u68c0\u6d4b\u4e2d\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.19653", "pdf": "https://arxiv.org/pdf/2507.19653", "abs": "https://arxiv.org/abs/2507.19653", "authors": ["Armen Manukyan", "Hrant Khachatrian", "Edvard Ghukasyan", "Theofanis P. Raptis"], "title": "On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication.\n  This work was supported by funding under the bilateral agreement between CNR\n  (Italy) and HESC MESCS RA (Armenia) as part of the DeepRF project for the\n  2025-2026 biennium, and by the HESC MESCS RA grant No. 22rl-052 (DISTAL)", "summary": "We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links\nin central Rome. We use a real measurement set of 1,664 user-equipments (UEs)\nand six nominal base-station (BS) sites. Using these fixed positions we\nsystematically vary the main simulation parameters, including path depth,\ndiffuse/specular/refraction flags, carrier frequency, as well as antenna's\nproperties like its altitude, radiation pattern, and orientation. Simulator\nfidelity is scored for each base station via Spearman correlation between\nmeasured and simulated powers, and by a fingerprint-based k-nearest-neighbor\nlocalization algorithm using RSSI-based fingerprints. Across all experiments,\nsolver hyper-parameters are having immaterial effect on the chosen metrics. On\nthe contrary, antenna locations and orientations prove decisive. By simple\ngreedy optimization we improve the Spearman correlation by 5% to 130% for\nvarious base stations, while kNN-based localization error using only simulated\ndata as reference points is decreased by one-third on real-world samples, while\nstaying twice higher than the error with purely real data. Precise geometry and\ncredible antenna models are therefore necessary but not sufficient; faithfully\ncapturing the residual urban noise remains an open challenge for transferable,\nhigh-fidelity outdoor RF simulation.", "AI": {"tldr": "\u7814\u7a76Sionna\u5149\u7ebf\u8ffd\u8e2a\u5728\u5ba4\u5916\u8702\u7a9d\u94fe\u8def\u4e2d\u7684\u771f\u5b9e\u6027\uff0c\u53d1\u73b0\u5929\u7ebf\u53c2\u6570\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57ce\u5e02\u566a\u58f0\u5efa\u6a21\u4ecd\u662f\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30Sionna v1.0.2\u5149\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e0b\uff08\u7f57\u9a6c\u5e02\u4e2d\u5fc3\uff09\u5ba4\u5916\u8702\u7a9d\u94fe\u8def\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u7f57\u9a6c\u5e02\u4e2d\u5fc31664\u4e2a\u7528\u6237\u8bbe\u5907\u548c6\u4e2a\u57fa\u7ad9\u7684\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u3002\u7cfb\u7edf\u6027\u5730\u8c03\u6574\u4e86\u5149\u7ebf\u8ffd\u8e2a\u6a21\u62df\u53c2\u6570\uff0c\u5305\u62ec\u8def\u5f84\u6df1\u5ea6\u3001\u53cd\u5c04/\u6563\u5c04/\u6298\u5c04\u6807\u5fd7\u3001\u8f7d\u6ce2\u9891\u7387\u53ca\u5929\u7ebf\u5c5e\u6027\u3002\u901a\u8fc7\u6d4b\u91cf\u529f\u7387\u4e0e\u6a21\u62df\u529f\u7387\u4e4b\u95f4\u7684\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8eRSSI\u6307\u7eb9\u7684kNN\u5b9a\u4f4d\u7b97\u6cd5\u6765\u8bc4\u4f30\u6a21\u62df\u5668\u7684\u51c6\u786e\u6027\u3002\u5bf9\u5929\u7ebf\u4f4d\u7f6e\u548c\u65b9\u5411\u8fdb\u884c\u4e86\u8d2a\u5a6a\u4f18\u5316\u3002", "result": "\u6c42\u89e3\u5668\u8d85\u53c2\u6570\u5bf9\u7ed3\u679c\u5f71\u54cd\u4e0d\u5927\uff0c\u800c\u5929\u7ebf\u4f4d\u7f6e\u548c\u65b9\u5411\u5219\u5177\u6709\u51b3\u5b9a\u6027\u4f5c\u7528\u3002\u901a\u8fc7\u8d2a\u5a6a\u4f18\u5316\uff0c\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u6027\u63d0\u5347\u4e865%\u81f3130%\u3002\u4f7f\u7528\u6a21\u62df\u6570\u636e\u4f5c\u4e3a\u53c2\u8003\u7684kNN\u5b9a\u4f4d\u8bef\u5dee\u5728\u771f\u5b9e\u6837\u672c\u4e0a\u964d\u4f4e\u4e86\u4e09\u5206\u4e4b\u4e00\uff0c\u4f46\u4ecd\u662f\u7eaf\u771f\u5b9e\u6570\u636e\u8bef\u5dee\u7684\u4e24\u500d\u3002", "conclusion": "\u7cbe\u786e\u7684\u51e0\u4f55\u6a21\u578b\u548c\u53ef\u4fe1\u7684\u5929\u7ebf\u6a21\u578b\u662f\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u7684\u3002\u5fe0\u5b9e\u5730\u6355\u6349\u5269\u4f59\u7684\u57ce\u5e02\u566a\u58f0\uff0c\u5bf9\u4e8e\u53ef\u8fc1\u79fb\u7684\u9ad8\u4fdd\u771f\u5ba4\u5916\u5c04\u9891\u6a21\u62df\u6765\u8bf4\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002"}}
{"id": "2507.19521", "pdf": "https://arxiv.org/pdf/2507.19521", "abs": "https://arxiv.org/abs/2507.19521", "authors": ["Vishakh Padmakumar", "Joseph Chee Chang", "Kyle Lo", "Doug Downey", "Aakanksha Naik"], "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing volume of academic literature makes it essential for\nresearchers to organize, compare, and contrast collections of documents. Large\nlanguage models (LLMs) can support this process by generating schemas defining\nshared aspects along which to compare papers. However, progress on schema\ngeneration has been slow due to: (i) ambiguity in reference-based evaluations,\nand (ii) lack of editing/refinement methods. Our work is the first to address\nboth issues. First, we present an approach for augmenting unannotated table\ncorpora with synthesized intents and apply it to create a dataset for studying\nschema generation conditioned on a given information need, thus reducing\nambiguity. With this dataset, we show how incorporating table intents\nsignificantly improves baseline performance in reconstructing reference\nschemas. Next, we propose several LLM-based schema editing techniques. We start\nby comprehensively benchmarking several single-shot schema generation methods,\nincluding prompted LLM workflows and fine-tuned models, showing that smaller,\nopen-weight models can be fine-tuned to be competitive with state-of-the-art\nprompted LLMs. Then we demonstrate that our editing techniques can further\nimprove schemas generated by these methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u6570\u636e\u96c6\u548cLLM\u7f16\u8f91\u6280\u672f\uff0c\u89e3\u51b3\u5b66\u672f\u8bba\u6587\u6a21\u5f0f\u751f\u6210\u4e2d\u7684\u6b67\u4e49\u548c\u7f3a\u4e4f\u6539\u8fdb\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u751f\u6210\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5b66\u672f\u6587\u732e\u91cf\u589e\u957f\uff0c\u9700\u8981\u6709\u6548\u7ec4\u7ec7\u548c\u6bd4\u8f83\u6587\u6863\u3002\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u53ef\u7528\u4e8e\u751f\u6210\u6bd4\u8f83\u6a21\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u9879\u4e0d\u8db3\uff1a1) \u57fa\u4e8e\u5f15\u7528\u7684\u8bc4\u4f30\u5b58\u5728\u6b67\u4e49\uff1b2) \u7f3a\u4e4f\u7f16\u8f91/\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u7528\u5408\u6210\u610f\u56fe\u6269\u5145\u672a\u6807\u6ce8\u7684\u8868\u683c\u8bed\u6599\u5e93\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u5728\u7ed9\u5b9a\u4fe1\u606f\u9700\u6c42\u4e0b\u751f\u6210\u6a21\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u51cf\u5c11\u6b67\u4e49\u30022. \u63d0\u51fa\u51e0\u79cd\u57fa\u4e8eLLM\u7684\u6a21\u5f0f\u7f16\u8f91\u6280\u672f\u30023. \u5168\u9762\u8bc4\u4f30\u4e86\u591a\u79cd\u5355\u6b21\u6a21\u5f0f\u751f\u6210\u65b9\u6cd5\uff08\u5305\u62ecLLM\u63d0\u793a\u5de5\u4f5c\u6d41\u548c\u5fae\u8c03\u6a21\u578b\uff09\u3002", "result": "1. \u7ed3\u5408\u8868\u683c\u610f\u56fe\u663e\u8457\u6539\u5584\u4e86\u57fa\u7ebf\u6a21\u578b\u5728\u91cd\u6784\u53c2\u8003\u6a21\u5f0f\u65b9\u9762\u7684\u6027\u80fd\u30022. \u8bc1\u660e\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u53ef\u4ee5\u4e0e\u6700\u5148\u8fdb\u7684LLM\u63d0\u793a\u65b9\u6cd5\u5ab2\u7f8e\u30023. \u63d0\u51fa\u7684\u7f16\u8f91\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u8fd9\u4e9b\u65b9\u6cd5\u751f\u6210\u7684\u6a21\u5f0f\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u65b0\u6570\u636e\u96c6\u548cLLM\u7f16\u8f91\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5f0f\u751f\u6210\u4e2d\u7684\u6b67\u4e49\u548c\u7f3a\u4e4f\u6539\u8fdb\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u751f\u6210\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5e76\u8bc1\u660e\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2507.19657", "pdf": "https://arxiv.org/pdf/2507.19657", "abs": "https://arxiv.org/abs/2507.19657", "authors": ["Beining Wu", "Jun Huang", "Shui Yu"], "title": "\"X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems", "categories": ["cs.NI", "cs.AI"], "comment": "48 pages, 14 figures, submitted to IEEE", "summary": "The development of next-generation networking systems has inherently shifted\nfrom throughput-based paradigms towards intelligent, information-aware designs\nthat emphasize the quality, relevance, and utility of transmitted information,\nrather than sheer data volume. While classical network metrics, such as latency\nand packet loss, remain significant, they are insufficient to quantify the\nnuanced information quality requirements of modern intelligent applications,\nincluding autonomous vehicles, digital twins, and metaverse environments. In\nthis survey, we present the first comprehensive study of the ``X of\nInformation'' continuum by introducing a systematic four-dimensional taxonomic\nframework that structures information metrics along temporal, quality/utility,\nreliability/robustness, and network/communication dimensions. We uncover the\nincreasing interdependencies among these dimensions, whereby temporal freshness\ntriggers quality evaluation, which in turn helps with reliability appraisal,\nultimately enabling effective network delivery. Our analysis reveals that\nartificial intelligence technologies, such as deep reinforcement learning,\nmulti-agent systems, and neural optimization models, enable adaptive,\ncontext-aware optimization of competing information quality objectives. In our\nextensive study of six critical application domains, covering autonomous\ntransportation, industrial IoT, healthcare digital twins, UAV communications,\nLLM ecosystems, and metaverse settings, we illustrate the revolutionary promise\nof multi-dimensional information metrics for meeting diverse operational needs.\nOur survey identifies prominent implementation challenges, including ...", "AI": {"tldr": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u7cfb\u7edf\u6b63\u4ece\u57fa\u4e8e\u541e\u5410\u91cf\u8f6c\u5411\u6ce8\u91cd\u4fe1\u606f\u8d28\u91cf\u7684\u8bbe\u8ba1\u3002\u672c\u7efc\u8ff0\u9996\u6b21\u63d0\u51fa\u4e00\u4e2a\u56db\u7ef4\u6846\u67b6\u6765\u91cf\u5316\u4fe1\u606f\u6307\u6807\uff0c\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u4f18\u5316\u8fd9\u4e9b\u6307\u6807\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c55\u793a\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u516d\u5927\u5e94\u7528\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7f51\u7edc\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u548c\u4e22\u5305\u7387\uff09\u4e0d\u8db3\u4ee5\u91cf\u5316\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u5b6a\u751f\u548c\u5143\u5b87\u5b99\u7b49\u73b0\u4ee3\u667a\u80fd\u5e94\u7528\u5bf9\u4fe1\u606f\u8d28\u91cf\u7684\u7cbe\u7ec6\u5316\u8981\u6c42\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u4e00\u9879\u7efc\u5408\u6027\u8c03\u67e5\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u201c\u4fe1\u606f\u4e4bX\u201d\u8fde\u7eed\u4f53\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u56db\u7ef4\u5206\u7c7b\u6846\u67b6\u6765\u6784\u5efa\u4fe1\u606f\u6307\u6807\uff0c\u6db5\u76d6\u65f6\u95f4\u3001\u8d28\u91cf/\u6548\u7528\u3001\u53ef\u9760\u6027/\u9c81\u68d2\u6027\u4ee5\u53ca\u7f51\u7edc/\u901a\u4fe1\u7ef4\u5ea6\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u8fd9\u4e9b\u7ef4\u5ea6\u4e4b\u95f4\u65e5\u76ca\u589e\u957f\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff1b\u5206\u6790\u8868\u660e\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff08\u5982\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\u80fd\u591f\u5b9e\u73b0\u5bf9\u7ade\u4e89\u6027\u4fe1\u606f\u8d28\u91cf\u76ee\u6807\u7684\u81ea\u9002\u5e94\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5316\uff1b\u901a\u8fc7\u5bf9\u516d\u4e2a\u5173\u952e\u5e94\u7528\u9886\u57df\u7684\u5e7f\u6cdb\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u591a\u7ef4\u4fe1\u606f\u6307\u6807\u5728\u6ee1\u8db3\u591a\u6837\u5316\u64cd\u4f5c\u9700\u6c42\u65b9\u9762\u7684\u9769\u547d\u6027\u6f5c\u529b\u3002", "conclusion": "\u591a\u7ef4\u4fe1\u606f\u6307\u6807\u5bf9\u4e8e\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u667a\u80fd\u5e94\u7528\u7684\u590d\u6742\u4fe1\u606f\u8d28\u91cf\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u662f\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807\u4f18\u5316\u7684\u5173\u952e\u63a8\u52a8\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u5b9e\u65bd\u6311\u6218\u3002"}}
{"id": "2507.19537", "pdf": "https://arxiv.org/pdf/2507.19537", "abs": "https://arxiv.org/abs/2507.19537", "authors": ["Felix Kraus", "Nicolas Blumenr\u00f6hr", "Danah Tonne", "Achim Streit"], "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri", "categories": ["cs.CL"], "comment": null, "summary": "We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for\nthe automated translation of SKOS thesauri. This work addresses a critical need\nin the Digital Humanities (DH), where language diversity can limit access,\nreuse, and semantic interoperability of knowledge resources. WOKIE combines\nexternal translation services with targeted refinement using Large Language\nModels (LLMs), balancing translation quality, scalability, and cost. Designed\nto run on everyday hardware and be easily extended, the application requires no\nprior expertise in machine translation or LLMs. We evaluate WOKIE across\nseveral DH thesauri in 15 languages with different parameters, translation\nservices and LLMs, systematically analysing translation quality, performance,\nand ontology matching improvements. Our results show that WOKIE is suitable to\nenhance the accessibility, reuse, and cross-lingual interoperability of\nthesauri by hurdle-free automated translation and improved ontology matching\nperformance, supporting more inclusive and multilingual research\ninfrastructures.", "AI": {"tldr": "WOKIE\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u5757\u5316\u7684\u81ea\u52a8\u5316SKOS\u8bcd\u5e93\u7ffb\u8bd1\u5de5\u5177\uff0c\u5b83\u7ed3\u5408\u5916\u90e8\u7ffb\u8bd1\u670d\u52a1\u548cLLM\u7cbe\u70bc\uff0c\u65e8\u5728\u63d0\u9ad8\u6570\u5b57\u4eba\u6587\u9886\u57df\u77e5\u8bc6\u8d44\u6e90\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u6570\u5b57\u4eba\u6587\uff08DH\uff09\u9886\u57df\u4e2d\uff0c\u8bed\u8a00\u591a\u6837\u6027\u9650\u5236\u4e86\u77e5\u8bc6\u8d44\u6e90\uff08\u5982SKOS\u8bcd\u5e93\uff09\u7684\u8bbf\u95ee\u3001\u91cd\u7528\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u8bed\u8a00\u969c\u788d\u3002", "method": "\u5f15\u5165WOKIE\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316SKOS\u8bcd\u5e93\u7ffb\u8bd1\u7ba1\u9053\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u5916\u90e8\u7ffb\u8bd1\u670d\u52a1\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b9a\u5411\u7cbe\u70bc\uff0c\u4ee5\u5e73\u8861\u7ffb\u8bd1\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u3002WOKIE\u8bbe\u8ba1\u4e3a\u53ef\u5728\u65e5\u5e38\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u6613\u4e8e\u6269\u5c55\uff0c\u4e14\u65e0\u9700\u673a\u5668\u5b66\u4e60\u6216LLM\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u901a\u8fc7\u572815\u79cd\u8bed\u8a00\u7684\u591a\u4e2a\u6570\u5b57\u4eba\u6587\u8bcd\u5e93\u4e0a\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eWOKIE\u80fd\u591f\u901a\u8fc7\u65e0\u969c\u788d\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\u548c\u6539\u8fdb\u7684\u672c\u4f53\u5339\u914d\u6027\u80fd\uff0c\u6709\u6548\u589e\u5f3a\u8bcd\u5e93\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u8de8\u8bed\u8a00\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "WOKIE\u662f\u4e00\u6b3e\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u80fd\u5b9e\u73b0SKOS\u8bcd\u5e93\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u8d44\u6e90\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u8de8\u8bed\u8a00\u4e92\u64cd\u4f5c\u6027\uff0c\u4ece\u800c\u652f\u6301\u66f4\u5177\u5305\u5bb9\u6027\u548c\u591a\u8bed\u8a00\u7684\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.19925", "pdf": "https://arxiv.org/pdf/2507.19925", "abs": "https://arxiv.org/abs/2507.19925", "authors": ["Sowmiyan Morri", "Joy Bose", "L Raghunatha Reddy", "Sai Hareesh Anamandra"], "title": "Predicting Locations of Cell Towers for Network Capacity Expansion", "categories": ["cs.NI", "90B18", "C.2.1; I.2.6"], "comment": "9 pages, 5 figures", "summary": "Network capacity expansion is a critical challenge for telecom operators,\nrequiring strategic placement of new cell sites to ensure optimal coverage and\nperformance. Traditional approaches, such as manual drive tests and static\noptimization, often fail to consider key real-world factors including user\ndensity, terrain features, and financial constraints. In this paper, we propose\na machine learning-based framework that combines deep neural networks for\nsignal coverage prediction with spatial clustering to recommend new tower\nlocations in underserved areas. The system integrates geospatial, demographic,\nand infrastructural data, and incorporates budget-aware constraints to\nprioritize deployments. Operating within an iterative planning loop, the\nframework refines coverage estimates after each proposed installation, enabling\nadaptive and cost-effective expansion. While full-scale simulation was limited\nby data availability, the architecture is modular, robust to missing inputs,\nand generalizable across diverse deployment scenarios. This approach advances\nradio network planning by offering a scalable, data-driven alternative to\nmanual methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u7a7a\u95f4\u805a\u7c7b\uff0c\u667a\u80fd\u63a8\u8350\u65b0\u7684\u8702\u7a9d\u57fa\u7ad9\u4f4d\u7f6e\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u7f51\u7edc\u6269\u5bb9\u4e2d\u672a\u80fd\u8003\u8651\u771f\u5b9e\u4e16\u754c\u56e0\u7d20\u548c\u9884\u7b97\u9650\u5236\u7684\u4e0d\u8db3\u3002", "motivation": "\u7535\u4fe1\u8fd0\u8425\u5546\u7684\u7f51\u7edc\u6269\u5bb9\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7b56\u7565\u6027\u5730\u653e\u7f6e\u65b0\u57fa\u7ad9\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4eba\u5de5\u8def\u6d4b\u548c\u9759\u6001\u4f18\u5316\uff09\u672a\u80fd\u5145\u5206\u8003\u8651\u7528\u6237\u5bc6\u5ea6\u3001\u5730\u5f62\u7279\u5f81\u548c\u8d22\u52a1\u9650\u5236\u7b49\u5173\u952e\u771f\u5b9e\u4e16\u754c\u56e0\u7d20\uff0c\u5bfc\u81f4\u8986\u76d6\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4fe1\u53f7\u8986\u76d6\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u7a7a\u95f4\u805a\u7c7b\u6280\u672f\u63a8\u8350\u670d\u52a1\u4e0d\u8db3\u533a\u57df\u7684\u65b0\u5854\u7ad9\u4f4d\u7f6e\u3002\u7cfb\u7edf\u6574\u5408\u5730\u7406\u7a7a\u95f4\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u57fa\u7840\u8bbe\u65bd\u6570\u636e\uff0c\u5e76\u7eb3\u5165\u9884\u7b97\u7ea6\u675f\u4ee5\u4f18\u5148\u90e8\u7f72\u3002\u5b83\u5728\u4e00\u4e2a\u8fed\u4ee3\u89c4\u5212\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u6bcf\u6b21\u63d0\u8bae\u5b89\u88c5\u540e\u7ec6\u5316\u8986\u76d6\u4f30\u8ba1\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u6210\u672c\u6548\u76ca\u7684\u6269\u5c55\u3002", "result": "\u5c3d\u7ba1\u53d7\u9650\u4e8e\u6570\u636e\u53ef\u7528\u6027\u672a\u80fd\u8fdb\u884c\u5168\u9762\u4eff\u771f\uff0c\u4f46\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5177\u6709\u6a21\u5757\u5316\u3001\u5bf9\u7f3a\u5931\u8f93\u5165\u9c81\u68d2\u4ee5\u53ca\u5728\u591a\u6837\u5316\u90e8\u7f72\u573a\u666f\u4e2d\u5177\u6709\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6570\u636e\u9a71\u52a8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53d6\u4ee3\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u65e0\u7ebf\u7f51\u7edc\u89c4\u5212\u3002"}}
{"id": "2507.19586", "pdf": "https://arxiv.org/pdf/2507.19586", "abs": "https://arxiv.org/abs/2507.19586", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u548c\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eKahneman-Tversky\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u62e5\u6709\u5e7f\u6cdb\u7684\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u76f8\u5173\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u5e38\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff08\u5373\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\uff09\uff0c\u8fd9\u635f\u5bb3\u4e86\u5176\u53ef\u9760\u6027\u3002\u73b0\u6709\u7814\u7a76\u5e7f\u6cdb\u5173\u6ce8LLMs\u7684\u4e00\u822c\u77e5\u8bc6\u5e7b\u89c9\uff0c\u4f46\u5bf9\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u7684\u7cfb\u7edf\u8bc4\u4f30\u548c\u7f13\u89e3\u4ecd\u7f3a\u4e4f\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u53d7\u63a7\u8bc4\u4f30\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u3002\u901a\u8fc7\u5bf920\u4e2a\u5148\u8fdbLLMs\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5176\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u4e2d\u7684\u5e7b\u89c9\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eKahneman-Tversky\u4f18\u5316\u7684\u52a8\u6001\u4e8b\u5b9e\u5bf9\u9f50\u65b9\u6cd5\u6765\u7f13\u89e3\u5730\u7406\u7a7a\u95f4\u5e7b\u89c9\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u3002\u6240\u63d0\u51fa\u7684\u57fa\u4e8eKahneman-Tversky\u4f18\u5316\u7684\u65b9\u6cd5\u5728\u63d0\u51fa\u7684\u57fa\u51c6\u4e0a\u5c06\u6027\u80fd\u63d0\u9ad8\u4e8629.6%\u4ee5\u4e0a\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u5b66\u4e60\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8LLMs\u5728\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.19938", "pdf": "https://arxiv.org/pdf/2507.19938", "abs": "https://arxiv.org/abs/2507.19938", "authors": ["W. A. Sasindu Wijesuriya"], "title": "Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using Single-Channel Hardware", "categories": ["cs.NI", "cs.RO"], "comment": "6 pages, 5 figures", "summary": "The deployment of mobile LoRa gateways using low-cost single-channel hardware\npresents a significant challenge in maintaining reliable communication due to\nthe lack of dynamic configuration support. In traditional LoRaWAN networks,\nAdaptive Data Rate (ADR) mechanisms optimize communication parameters in real\ntime. However, such features are typically supported only by expensive\nmulti-channel gateways. This study proposes a cost-effective and\nenergy-efficient solution by statically selecting the optimal Spreading Factor\n(SF) using a two-phase algorithm. The method first applies rule-based exclusion\nto eliminate SFs that violate constraints related to distance, data rate, link\nmargin, and regulatory limits. Remaining candidates are then evaluated using a\nweighted scoring model incorporating Time-on-Air, energy consumption, data\nrate, and link robustness. The proposed algorithm was validated through\nextensive field tests and NS-3 simulations under line-of-sight conditions.\nResults demonstrate that the selected SF matched the optimal SF in over 92% of\ncases across 672 simulated scenarios, confirming the algorithm's effectiveness.\nThis approach offers a scalable alternative to dynamic protocols, enabling\nreliable mobile LoRa deployments in cost-sensitive environments such as\nagriculture and rural sensing applications.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u6210\u672c\u5355\u901a\u9053\u79fb\u52a8LoRa\u7f51\u5173\u7f3a\u4e4f\u52a8\u6001\u914d\u7f6e\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u7b97\u6cd5\u4ee5\u9759\u6001\u9009\u62e9\u6700\u4f73\u6269\u5c55\u56e0\u5b50\uff08SF\uff09\uff0c\u901a\u8fc7\u89c4\u5219\u6392\u9664\u548c\u52a0\u6743\u8bc4\u5206\u6a21\u578b\u5b9e\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u4e2d\u80fd\u4ee5\u8d85\u8fc792%\u7684\u51c6\u786e\u7387\u5339\u914d\u6700\u4f73SF\uff0c\u9002\u7528\u4e8e\u6210\u672c\u654f\u611f\u578b\u79fb\u52a8LoRa\u90e8\u7f72\u3002", "motivation": "\u4f4e\u6210\u672c\u5355\u901a\u9053\u79fb\u52a8LoRa\u7f51\u5173\u7f3a\u4e4f\u52a8\u6001\u914d\u7f6e\u652f\u6301\uff0c\u5bfc\u81f4\u96be\u4ee5\u7ef4\u6301\u53ef\u9760\u901a\u4fe1\uff1b\u800c\u4f20\u7edfLoRaWAN\u7684\u81ea\u9002\u5e94\u6570\u636e\u901f\u7387\uff08ADR\uff09\u673a\u5236\u901a\u5e38\u4ec5\u7531\u6602\u8d35\u7684\u591a\u901a\u9053\u7f51\u5173\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u7b97\u6cd5\u6765\u9759\u6001\u9009\u62e9\u6700\u4f18SF\u3002\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u89c4\u5219\u6392\u9664\uff08\u57fa\u4e8e\u8ddd\u79bb\u3001\u6570\u636e\u901f\u7387\u3001\u94fe\u8def\u88d5\u5ea6\u548c\u6cd5\u89c4\u9650\u5236\uff09\u7b5b\u9009\u4e0d\u7b26\u5408\u6761\u4ef6\u7684SF\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5305\u542b\u7a7a\u4e2d\u65f6\u95f4\u3001\u80fd\u8017\u3001\u6570\u636e\u901f\u7387\u548c\u94fe\u8def\u9c81\u68d2\u6027\u7684\u52a0\u6743\u8bc4\u5206\u6a21\u578b\u8bc4\u4f30\u5269\u4f59SF\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u73b0\u573a\u6d4b\u8bd5\u548cNS-3\u4eff\u771f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728672\u4e2a\u6a21\u62df\u573a\u666f\u4e2d\uff0c\u6240\u9009SF\u4e0e\u6700\u4f73SF\u7684\u5339\u914d\u5ea6\u8d85\u8fc792%\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u534f\u8bae\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u5728\u519c\u4e1a\u548c\u519c\u6751\u4f20\u611f\u7b49\u6210\u672c\u654f\u611f\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u79fb\u52a8LoRa\u90e8\u7f72\u3002"}}
{"id": "2507.19489", "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital.", "AI": {"tldr": "MAIA\u662f\u4e00\u4e2a\u57fa\u4e8eKubernetes\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u534f\u4f5c\u548c\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u52a0\u901fAI\u7814\u7a76\u5728\u4e34\u5e8a\u533b\u7597\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c06\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6574\u5408\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\uff0c\u9700\u8981\u5f3a\u5927\u7684\u534f\u4f5c\u5e73\u53f0\u6765\u5f25\u5408\u6280\u672f\u521b\u65b0\u4e0e\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86MAIA\uff08Medical Artificial Intelligence Assistant\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eKubernetes\u6784\u5efa\u7684\u5f00\u6e90\u5e73\u53f0\u3002\u5b83\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0c\u96c6\u6210\u4e86\u6570\u636e\u7ba1\u7406\u3001\u6a21\u578b\u5f00\u53d1\u3001\u6807\u6ce8\u3001\u90e8\u7f72\u548c\u4e34\u5e8a\u53cd\u9988\u5de5\u5177\uff0c\u5e76\u652f\u6301\u9879\u76ee\u9694\u79bb\u3001CI/CD\u81ea\u52a8\u5316\u4ee5\u53ca\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7684\u96c6\u6210\u3002", "result": "MAIA\u5df2\u5728\u5b66\u672f\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u652f\u6301\u533b\u5b66\u5f71\u50cfAI\u7684\u5b9e\u9645\u7528\u4f8b\uff0c\u5e76\u5728KTH\u7687\u5bb6\u7406\u5de5\u5b66\u9662\u548c\u5361\u7f57\u6797\u65af\u5361\u5927\u5b66\u533b\u9662\u7684\u4e0d\u540c\u9879\u76ee\u4e2d\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "conclusion": "MAIA\u65e8\u5728\u901a\u8fc7\u4fc3\u8fdb\u534f\u4f5c\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u52a0\u901fAI\u7814\u7a76\u5411\u6709\u5f71\u54cd\u529b\u7684\u4e34\u5e8a\u89e3\u51b3\u65b9\u6848\u7684\u8f6c\u5316\uff0c\u540c\u65f6\u63d0\u5347\u53ef\u91cd\u590d\u6027\u3001\u900f\u660e\u5ea6\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.19574", "pdf": "https://arxiv.org/pdf/2507.19574", "abs": "https://arxiv.org/abs/2507.19574", "authors": ["Ghufran Abualhail Alhamzawi", "Ali Saeed Alfoudi", "Ali Hakem Alsaeedi", "Suha Mohammed Hadi", "Amjed Abbas Ahmed", "Md. Riad Hassan", "Nurhizam Safie Mohd Satar", "Waeel Yahya Yasseen"], "title": "Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh", "categories": ["cs.CV"], "comment": null, "summary": "Enhancing images in low-light conditions is an important challenge in\ncomputer vision. Insufficient illumination negatively affects the quality of\nimages, resulting in low contrast, intensive noise, and blurred details. This\npaper presents a model for enhancing low-light images called tuning adaptive\ngamma correction (TAGC). The model is based on analyzing the color luminance of\nthe low-light image and calculating the average color to determine the adaptive\ngamma coefficient. The gamma value is calculated automatically and adaptively\nat different illumination levels suitable for the image without human\nintervention or manual adjustment. Based on qualitative and quantitative\nevaluation, tuning adaptive gamma correction model has effectively improved\nlow-light images while maintaining details, natural contrast, and correct color\ndistribution. It also provides natural visual quality. It can be considered a\nmore efficient solution for processing low-light images in multiple\napplications such as night surveillance, improving the quality of medical\nimages, and photography in low-light environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u4f3d\u9a6c\u6821\u6b63\uff08TAGC\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5149\u7167\u6761\u4ef6\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u8868\u73b0\u4e3a\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u566a\u58f0\u5927\u548c\u7ec6\u8282\u6a21\u7cca\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8c03\u6574\u81ea\u9002\u5e94\u4f3d\u9a6c\u6821\u6b63\u201d\uff08TAGC\uff09\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5206\u6790\u4f4e\u5149\u56fe\u50cf\u7684\u8272\u5f69\u4eae\u5ea6\u5e76\u8ba1\u7b97\u5e73\u5747\u989c\u8272\u6765\u786e\u5b9a\u81ea\u9002\u5e94\u4f3d\u9a6c\u7cfb\u6570\u3002\u4f3d\u9a6c\u503c\u80fd\u591f\u6839\u636e\u56fe\u50cf\u7684\u4e0d\u540c\u5149\u7167\u6c34\u5e73\u81ea\u52a8\u3001\u81ea\u9002\u5e94\u5730\u8ba1\u7b97\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u624b\u52a8\u8c03\u6574\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0cTAGC\u6a21\u578b\u6709\u6548\u6539\u5584\u4e86\u4f4e\u5149\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ec6\u8282\u3001\u81ea\u7136\u5bf9\u6bd4\u5ea6\u548c\u6b63\u786e\u7684\u8272\u5f69\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4e86\u81ea\u7136\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "TAGC\u6a21\u578b\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4f4e\u5149\u56fe\u50cf\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e94\u7528\u4e8e\u591c\u95f4\u76d1\u63a7\u3001\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u548c\u4f4e\u5149\u73af\u5883\u6444\u5f71\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2507.19510", "pdf": "https://arxiv.org/pdf/2507.19510", "abs": "https://arxiv.org/abs/2507.19510", "authors": ["Haoxuan Ma", "Xishun Liao", "Yifan Liu", "Chris Stanford", "Jiaqi Ma"], "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u96f6\u6563GPS\u6570\u636e\u751f\u6210\u5012\u73ed\u5de5\u4eba\u7684\u5b8c\u6574\u6d3b\u52a8\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u4f20\u7edf\u4ea4\u901a\u6a21\u578b\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u8c03\u67e5\u548c\u89c4\u5212\u4e2d\uff0c\u5360\u52b3\u52a8\u529b15-20%\u7684\u5012\u73ed\u5de5\u4eba\u51fa\u884c\u6a21\u5f0f\u88ab\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\uff0c\u5bfc\u81f4\u57ce\u5e02\u51fa\u884c\u5efa\u6a21\u5b58\u5728\u5173\u952e\u7a7a\u767d\u3002\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4GPS\u548c\u8c03\u67e5\u6570\u636e\uff0c\u63ed\u793a\u4e86\u5012\u73ed\u5de5\u4eba\u4e0e\u5e38\u89c4\u671d\u4e5d\u665a\u4e94\u6a21\u5f0f\u7684\u663e\u8457\u5dee\u5f02\uff0c\u786e\u8ba4\u4e86\u8fd9\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u96f6\u6563GPS\u8f68\u8ff9\u6570\u636e\u751f\u6210\u5b8c\u6574\u7684\u3001\u884c\u4e3a\u6709\u6548\u7684\u975e\u6807\u51c6\u5de5\u65f6\u4eba\u5458\u6d3b\u52a8\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5468\u671f\u611f\u77e5\u7684\u65f6\u95f4\u5d4c\u5165\u548c\u4ee5\u8f6c\u6362\u4e3a\u4e3b\u7684\u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u6355\u83b7\u5012\u73ed\u5de5\u4eba\u72ec\u7279\u7684\u6d3b\u52a8\u8282\u5f8b\u5e76\u51cf\u5c11\u4f20\u7edf\u6570\u636e\u96c6\u7684\u504f\u5dee\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u751f\u6210\u7684\u6570\u636e\u4e0e\u6d1b\u6749\u77f6\u53bf\u7684GPS\u6570\u636e\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5206\u5e03\u4e00\u81f4\u6027\uff08\u6240\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5e73\u5747JSD < 0.02\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u4e0d\u5b8c\u6574\u7684GPS\u8f68\u8ff9\u8f6c\u5316\u4e3a\u5b8c\u6574\u7684\u3001\u6709\u4ee3\u8868\u6027\u7684\u6d3b\u52a8\u6a21\u5f0f\uff0c\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u4ee5\u586b\u8865\u7406\u89e3\u57ce\u5e02\u4eba\u53e324/7\u51fa\u884c\u9700\u6c42\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u5305\u5bb9\u7684\u4ea4\u901a\u89c4\u5212\u3002"}}
{"id": "2507.19595", "pdf": "https://arxiv.org/pdf/2507.19595", "abs": "https://arxiv.org/abs/2507.19595", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.", "AI": {"tldr": "\u9274\u4e8eTransformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u74f6\u9888\uff0c\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u4e0e\u7a00\u758f\u6ce8\u610f\u529b\u4e24\u5927\u9ad8\u6548\u673a\u5236\u53ca\u5176\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u53ef\u6269\u5c55\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "Transformer\u67b6\u6784\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u65f6\u95f4\u4e0e\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u5df2\u6210\u4e3a\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u6839\u672c\u969c\u788d\u3002", "method": "\u672c\u6587\u5bf9\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff08\u5305\u62ec\u7ebf\u6027\u6ce8\u610f\u529b\u4e0e\u7a00\u758f\u6ce8\u610f\u529b\uff09\u7684\u6700\u65b0\u53d1\u5c55\u8fdb\u884c\u4e86\u7cfb\u7edf\u800c\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u6574\u5408\u4e86\u7b97\u6cd5\u521b\u65b0\u548c\u786c\u4ef6\u5c42\u9762\u8003\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u9ad8\u6548\u6ce8\u610f\u529b\u5728\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6574\u5408\u65b9\u5f0f\uff0c\u5305\u62ec\u7eaf\u9ad8\u6548\u6ce8\u610f\u529b\u67b6\u6784\u548c\u6df7\u5408\u8bbe\u8ba1\u3002", "result": "\u672c\u7efc\u8ff0\u63d0\u4f9b\u4e86\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u7684\u7cfb\u7edf\u6027\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u7b97\u6cd5\u548c\u786c\u4ef6\u5c42\u9762\u7684\u521b\u65b0\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6574\u5408\u4e0e\u5e94\u7528\u6a21\u5f0f\u3002", "conclusion": "\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u9645\u90e8\u7f72\u7b56\u7565\uff0c\u4e3a\u63a8\u52a8\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e00\u4e2a\u57fa\u7840\u6027\u53c2\u8003\u3002"}}
{"id": "2507.19963", "pdf": "https://arxiv.org/pdf/2507.19963", "abs": "https://arxiv.org/abs/2507.19963", "authors": ["Nikolaos Bartzoudis", "Jos\u00e9 Rubio Fern\u00e1ndez", "David L\u00f3pez-Bueno", "Antonio Rom\u00e1n Villarroel"], "title": "A Scalable Resource Management Layer for FPGA SoCs in 6G Radio Units", "categories": ["cs.NI", "cs.AR"], "comment": "Paper accepted to the \"XL Simposio Nacional de la Uni\\'on\n  Cient\\'ifica Internacional de Radio (URSI 2025)\", Tarragona, Spain, 3-5\n  September 2025. Proceedings are not published. Also part of the worj appears\n  in Deliverables 2.2 and 5.2 of the SNS JU project VERGE", "summary": "This work presents a perspective on addressing the underutilization of\ncomputing resources in FPGA SoC devices deployed in 5G radio and edge computing\ninfrastructure. The initial step in this approach involves developing a\nresource management layer capable of dynamically migrating and scaling\nfunctions within these devices in response to contextual events. This layer\nserves as the foundation for designing a hierarchical, data-driven\nmicro-orchestrator responsible for managing the lifecycle of functions in FPGA\nSoC devices. In this paper, the proposed resource management layer is utilized\nto reconfigure a function based on events identified by a computer vision edge\napplication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u5c42\uff0c\u65e8\u5728\u89e3\u51b35G\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2dFPGA SoC\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b35G\u65e0\u7ebf\u548c\u8fb9\u7f18\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u4e2dFPGA SoC\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u8d44\u6e90\u7ba1\u7406\u5c42\uff0c\u8be5\u5c42\u80fd\u591f\u6839\u636e\u4e0a\u4e0b\u6587\u4e8b\u4ef6\u52a8\u6001\u8fc1\u79fb\u548c\u6269\u5c55FPGA SoC\u8bbe\u5907\u5185\u7684\u529f\u80fd\u3002\u8be5\u5c42\u662f\u6784\u5efa\u5206\u5c42\u3001\u6570\u636e\u9a71\u52a8\u7684\u5fae\u7f16\u6392\u5668\u4ee5\u7ba1\u7406\u529f\u80fd\u751f\u547d\u5468\u671f\u7684\u57fa\u7840\u3002", "result": "\u5728\u672c\u6587\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u8d44\u6e90\u7ba1\u7406\u5c42\u88ab\u7528\u4e8e\u6839\u636e\u8ba1\u7b97\u673a\u89c6\u89c9\u8fb9\u7f18\u5e94\u7528\u8bc6\u522b\u7684\u4e8b\u4ef6\u6765\u91cd\u65b0\u914d\u7f6e\u529f\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u5c42\u63d0\u5347FPGA SoC\u8bbe\u5907\u8d44\u6e90\u5229\u7528\u7387\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u672a\u6765\u6784\u5efa\u529f\u80fd\u751f\u547d\u5468\u671f\u5fae\u7f16\u6392\u5668\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.19543", "pdf": "https://arxiv.org/pdf/2507.19543", "abs": "https://arxiv.org/abs/2507.19543", "authors": ["Maria Emilia Mazzolenis", "Ruirui Zhang"], "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.7"], "comment": "Accepted at the ICML 2025 Workshop on Multi-Agent Systems in the Era\n  of Foundation Models: Opportunities, Challenges, and Futures. Code repo:\n  https://github.com/emiliamazzo/WARPP/", "summary": "Large language models (LLMs) are increasingly applied in task-oriented\ndialogue (TOD) systems but often struggle with long, conditional workflows that\ninvolve external tool calls and depend on user-specific information. We present\nWorkflow Adherence via Runtime Parallel Personalization, or WARPP, a\ntraining-free, modular framework that combines multi-agent orchestration with\nruntime personalization to improve workflow adherence in LLM-based systems. By\ndynamically pruning conditional branches based on user attributes, the\nframework reduces reasoning overhead and narrows tool selection at runtime.\nWARPP deploys a parallelized architecture where a dedicated Personalizer agent\noperates alongside modular, domain-specific agents to dynamically tailor\nexecution paths in real time. The framework is evaluated across five\nrepresentative user intents of varying complexity within three domains:\nbanking, flights, and healthcare. Our evaluation leverages synthetic datasets\nand LLM-powered simulated users to test scenarios with conditional\ndependencies. Our results demonstrate that WARPP outperforms both the\nnon-personalized method and the ReAct baseline, achieving increasingly larger\ngains in parameter fidelity and tool accuracy as intent complexity grows, while\nalso reducing average token usage, without any additional training.", "AI": {"tldr": "WARPP\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u5e76\u884c\u4e2a\u6027\u5316\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u578b\u5bf9\u8bdd\u4e2d\u5bf9\u6761\u4ef6\u5de5\u4f5c\u6d41\u7684\u9075\u5faa\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u964d\u4f4e\u4e86token\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\uff08TOD\uff09\u7cfb\u7edf\u4e2d\u5904\u7406\u6d89\u53ca\u5916\u90e8\u5de5\u5177\u8c03\u7528\u548c\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u7684\u957f\u3001\u6761\u4ef6\u6027\u5de5\u4f5c\u6d41\u65f6\uff0c\u5e38\u5e38\u96be\u4ee5\u9075\u5faa\u65e2\u5b9a\u6d41\u7a0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86WARPP\uff08Workflow Adherence via Runtime Parallel Personalization\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\uff0c\u91c7\u7528\u5e76\u884c\u5316\u67b6\u6784\u3002\u4e00\u4e2a\u4e13\u7528\u7684\u4e2a\u6027\u5316\u4ee3\u7406\uff08Personalizer agent\uff09\u4e0e\u6a21\u5757\u5316\u3001\u9886\u57df\u7279\u5b9a\u7684\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\uff0c\u6839\u636e\u7528\u6237\u5c5e\u6027\u52a8\u6001\u88c1\u526a\u6761\u4ef6\u5206\u652f\uff0c\u5b9e\u65f6\u8c03\u6574\u6267\u884c\u8def\u5f84\uff0c\u4ece\u800c\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u5e76\u7f29\u5c0f\u5de5\u5177\u9009\u62e9\u8303\u56f4\u3002", "result": "\u5728\u94f6\u884c\u3001\u822a\u7a7a\u3001\u533b\u7597\u4e09\u4e2a\u9886\u57df\u7684\u4e94\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u7528\u6237\u610f\u56fe\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cWARPP\u6027\u80fd\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u65b9\u6cd5\u548cReAct\u57fa\u7ebf\uff0c\u968f\u7740\u610f\u56fe\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u5176\u5728\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u5de5\u5177\u51c6\u786e\u6027\u65b9\u9762\u7684\u63d0\u5347\u66f4\u4e3a\u663e\u8457\uff0c\u540c\u65f6\u8fd8\u964d\u4f4e\u4e86\u5e73\u5747token\u4f7f\u7528\u91cf\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "WARPP\u901a\u8fc7\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u4efb\u52a1\u578b\u5bf9\u8bdd\u5de5\u4f5c\u6d41\u4e2d\u4f9d\u4ece\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19575", "pdf": "https://arxiv.org/pdf/2507.19575", "abs": "https://arxiv.org/abs/2507.19575", "authors": ["Ayush Roy", "Samin Enam", "Jun Xia", "Vishnu Suresh Lokhande", "Won Hwa Kim"], "title": "Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data scarcity is a major challenge in medical imaging, particularly for deep\nlearning models. While data pooling (combining datasets from multiple sources)\nand data addition (adding more data from a new dataset) have been shown to\nenhance model performance, they are not without complications. Specifically,\nincreasing the size of the training dataset through pooling or addition can\ninduce distributional shifts, negatively affecting downstream model\nperformance, a phenomenon known as the \"Data Addition Dilemma\". While the\ntraditional i.i.d. assumption may not hold in multi-source contexts, assuming\nexchangeability across datasets provides a more practical framework for data\npooling. In this work, we investigate medical image segmentation under these\nconditions, drawing insights from causal frameworks to propose a method for\ncontrolling foreground-background feature discrepancies across all layers of\ndeep networks. This approach improves feature representations, which are\ncrucial in data-addition scenarios. Our method achieves state-of-the-art\nsegmentation performance on histopathology and ultrasound images across five\ndatasets, including a novel ultrasound dataset that we have curated and\ncontributed. Qualitative results demonstrate more refined and accurate\nsegmentation maps compared to prominent baselines across three model\narchitectures. The code will be available on Github.", "AI": {"tldr": "\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u6570\u636e\u589e\u52a0\u5f15\u53d1\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u7279\u5f81\u5dee\u5f02\u63d0\u5347\u7279\u5f81\u8868\u793a\uff0c\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u8d85\u58f0\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6311\u6218\u3002\u867d\u7136\u6570\u636e\u6c60\u5316\u548c\u6570\u636e\u589e\u52a0\u80fd\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u5373\u201c\u6570\u636e\u589e\u52a0\u56f0\u5883\u201d\uff0c\u4ece\u800c\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002\u5728\u591a\u6e90\u6570\u636e\u80cc\u666f\u4e0b\uff0c\u4f20\u7edf\u7684i.i.d.\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u6846\u67b6\u3002", "method": "\u5728\u6570\u636e\u96c6\u53ef\u4ea4\u6362\u6027\u5047\u8bbe\u4e0b\uff0c\u7814\u7a76\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u501f\u9274\u56e0\u679c\u6846\u67b6\u7684\u89c1\u89e3\uff0c\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u6df1\u5ea6\u7f51\u7edc\u6240\u6709\u5c42\u4e2d\u7684\u524d\u666f-\u80cc\u666f\u7279\u5f81\u5dee\u5f02\uff0c\u4ee5\u6539\u5584\u6570\u636e\u589e\u52a0\u573a\u666f\u4e0b\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u8d85\u58f0\u56fe\u50cf\u4e0a\u7684\u4e94\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e00\u4e2a\u65b0\u7b56\u5212\u7684\u8d85\u58f0\u6570\u636e\u96c6\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u5206\u5272\u6027\u80fd\u3002\u5b9a\u6027\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4e3b\u8981\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u4e09\u79cd\u6a21\u578b\u67b6\u6784\u4e0b\u4ea7\u751f\u4e86\u66f4\u7cbe\u7ec6\u3001\u66f4\u51c6\u786e\u7684\u5206\u5272\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6539\u5584\u7279\u5f81\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u201c\u6570\u636e\u589e\u52a0\u56f0\u5883\u201d\uff0c\u5e76\u5728\u6570\u636e\u589e\u52a0\u573a\u666f\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.19513", "pdf": "https://arxiv.org/pdf/2507.19513", "abs": "https://arxiv.org/abs/2507.19513", "authors": ["Khalid Ali", "Zineddine Bettouche", "Andreas Kassler", "Andreas Fischer"], "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource\nmanagement in 5G and beyond. However, conventional AI approaches often fail to\ncapture the intricate spatial and temporal patterns that exist, due to e.g.,\nthe mobility of users. We introduce a lightweight, dual-path Spatiotemporal\nNetwork that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling\nand a three-layer Conv3D module for spatial feature extraction. A fusion layer\nintegrates both streams into a cohesive representation, enabling robust\nforecasting. Our design improves gradient stability and convergence speed while\nreducing prediction error. Evaluations on real-world datasets show superior\nforecast performance over ConvLSTM baselines and strong generalization to\nunseen regions, making it well-suited for large-scale, next-generation network\ndeployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,\nwith a 30% improvement in model generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53cc\u8def\u5f84\u65f6\u7a7a\u7f51\u7edc\uff0c\u7ed3\u5408Scalar LSTM\u548cConv3D\u6a21\u5757\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u3002\u8be5\u7f51\u7edc\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eConvLSTM\u57fa\u7ebf\uff0c\u5e76\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u7f51\u7edc\u90e8\u7f72\u3002", "motivation": "\u57285G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\uff0c\u7cbe\u786e\u7684\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u5bf9\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edfAI\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u79fb\u52a8\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53cc\u8def\u5f84\u7684\u65f6\u7a7a\u7f51\u7edc\u3002\u8be5\u7f51\u7edc\u5229\u7528Scalar LSTM (sLSTM) \u8fdb\u884c\u9ad8\u6548\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u4e09\u5c42Conv3D\u6a21\u5757\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u3002\u4e00\u4e2a\u878d\u5408\u5c42\u5c06\u8fd9\u4e24\u4e2a\u6570\u636e\u6d41\u6574\u5408\u4e3a\u7edf\u4e00\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u4f18\u4e8eConvLSTM\u57fa\u7ebf\uff0c\u5e76\u5bf9\u672a\u89c1\u533a\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMAE\u6bd4ConvLSTM\u51cf\u5c11\u4e8623%\uff0c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u63d0\u9ad8\u4e8630%\u3002\u8be5\u8bbe\u8ba1\u8fd8\u6539\u5584\u4e86\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u53cc\u8def\u5f84\u65f6\u7a7a\u7f51\u7edc\u80fd\u591f\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u4ea4\u901a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u4f18\u5f02\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5927\u89c4\u6a21\u3001\u4e0b\u4e00\u4ee3\u7f51\u7edc\u90e8\u7f72\u3002"}}
{"id": "2507.19598", "pdf": "https://arxiv.org/pdf/2507.19598", "abs": "https://arxiv.org/abs/2507.19598", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-T\u00fcr", "Ismini Lourentzou"], "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u201c\u4ee3\u7801\u5206\u89e3\u653b\u51fb\u201d\u65b9\u6cd5\u6765\u89c4\u907fLLM\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5e76\u5f15\u5165MOCHA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4ee3\u7801LLM\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u6076\u610f\u63d0\u793a\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0LLM\u5728\u6b64\u7c7b\u653b\u51fb\u4e0b\u666e\u904d\u8106\u5f31\uff0c\u4f46\u901a\u8fc7\u5728MOCHA\u4e0a\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5176\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u6ee5\u7528\uff08\u7279\u522b\u662f\u901a\u8fc7\u591a\u8f6e\u6076\u610f\u7f16\u7801\u63d0\u793a\uff09\u65f6\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u5f15\u5165\u201c\u4ee3\u7801\u5206\u89e3\u653b\u51fb\u201d\uff1a\u5c06\u6076\u610f\u7f16\u7801\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u8f6e\u770b\u4f3c\u65e0\u5bb3\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u89c4\u907fLLM\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u30022. \u6784\u5efaMOCHA\uff08\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff09\uff1a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801LLMs\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u6076\u610f\u63d0\u793a\u4e0b\u7684\u9c81\u68d2\u6027\u30023. \u5728\u5f00\u653e\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u30024. \u5bf9\u6a21\u578b\u5728MOCHA\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "1. \u5f00\u653e\u6e90\u548c\u95ed\u6e90LLM\u90fd\u5b58\u5728\u6301\u7eed\u7684\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u5728\u591a\u8f6e\u6076\u610f\u63d0\u793a\u573a\u666f\u4e0b\u30022. \u5728MOCHA\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u63d0\u9ad8\u5176\u5bf9\u6076\u610f\u63d0\u793a\u7684\u62d2\u7edd\u7387\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u4ee3\u7801\u751f\u6210\u80fd\u529b\u30023. \u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5916\u90e8\u5bf9\u6297\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u62d2\u7edd\u7387\u6700\u9ad8\u63d0\u534732.4%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u76d1\u7763\u3002", "conclusion": "LLMs\u5728\u591a\u8f6e\u6076\u610f\u4ee3\u7801\u751f\u6210\u63d0\u793a\u4e0b\u5b58\u5728\u660e\u663e\u7684\u9c81\u68d2\u6027\u5f31\u70b9\uff0c\u4f46\u901a\u8fc7\u672c\u6587\u63d0\u51fa\u7684MOCHA\u57fa\u51c6\u548c\u76f8\u5e94\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30\u5e76\u663e\u8457\u63d0\u5347LLMs\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6838\u5fc3\u529f\u80fd\u3002"}}
{"id": "2507.20050", "pdf": "https://arxiv.org/pdf/2507.20050", "abs": "https://arxiv.org/abs/2507.20050", "authors": ["Rohail Asim", "Ankit Bhardwaj", "Lakshmi Suramanian", "Yasir Zaki"], "title": "Towards Next Generation Immersive Applications in 5G Environments", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "The Multi-user Immersive Reality (MIR) landscape is evolving rapidly, with\napplications spanning virtual collaboration, entertainment, and training.\nHowever, wireless network limitations create a critical bottleneck, struggling\nto meet the high-bandwidth and ultra-low latency demands essential for\nnext-generation MIR experiences. This paper presents Hera, a modular framework\nfor next-generation immersive applications, comprising a high-level streaming\nand synchronization layer for AR/VR systems and a low-level delay-based\nQoE-aware rate control protocol optimized for dynamic wireless environments.\nThe Hera framework integrates application-aware streaming logic with a\nQoE-centric rate control core, enabling adaptive video quality, multi-user\nfairness, and low-latency communication across challenging 5G network\nconditions. We demonstrate that Hera outperforms existing state-of-the-art rate\ncontrol algorithms by maintaining up to 66% lower latencies with comparable\nthroughput performance, higher visual quality with 50% average bitrate\nimprovements in our analysis, and improved fairness. By bridging the gap\nbetween application-level responsiveness and network-level adaptability, Hera\nlays the foundation for more scalable, robust, and high-fidelity multi-user\nimmersive experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHera\u6846\u67b6\uff0c\u4e00\u4e2a\u4e13\u4e3a\u4e0b\u4e00\u4ee3\u591a\u7528\u6237\u6c89\u6d78\u5f0f\u5e94\u7528\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u6d41\u5a92\u4f53\u548c\u901f\u7387\u63a7\u5236\uff0c\u65e8\u5728\u514b\u670d\u65e0\u7ebf\u7f51\u7edc\u74f6\u9888\uff0c\u63d0\u5347\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u753b\u8d28\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u591a\u7528\u6237\u6c89\u6d78\u5f0f\u73b0\u5b9e\uff08MIR\uff09\u5e94\u7528\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5f53\u524d\u65e0\u7ebf\u7f51\u7edc\uff08\u7279\u522b\u662f5G\uff09\u5728\u6ee1\u8db3\u5176\u6240\u9700\u7684\u9ad8\u5e26\u5bbd\u548c\u8d85\u4f4e\u5ef6\u8fdf\u65b9\u9762\u5b58\u5728\u5173\u952e\u74f6\u9888\uff0c\u5f71\u54cd\u4e86\u4e0b\u4e00\u4ee3MIR\u4f53\u9a8c\u3002", "method": "\u672c\u6587\u63d0\u51faHera\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u7528\u4e8eAR/VR\u7cfb\u7edf\u7684\u9ad8\u7ea7\u6d41\u5a92\u4f53\u4e0e\u540c\u6b65\u5c42\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e3a\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4f18\u5316\u7684\u4f4e\u7ea7\u57fa\u4e8e\u5ef6\u8fdf\u7684QoE\uff08\u4f53\u9a8c\u8d28\u91cf\uff09\u611f\u77e5\u901f\u7387\u63a7\u5236\u534f\u8bae\u3002Hera\u5c06\u5e94\u7528\u611f\u77e5\u6d41\u5a92\u4f53\u903b\u8f91\u4e0e\u4ee5QoE\u4e3a\u4e2d\u5fc3\u7684\u901f\u7387\u63a7\u5236\u6838\u5fc3\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u89c6\u9891\u8d28\u91cf\u3001\u591a\u7528\u6237\u516c\u5e73\u6027\u548c\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cHera\u5728\u4fdd\u6301\u53ef\u6bd4\u541e\u5410\u91cf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe66%\uff1b\u89c6\u89c9\u8d28\u91cf\u66f4\u9ad8\uff0c\u5e73\u5747\u6bd4\u7279\u7387\u63d0\u9ad850%\uff1b\u5e76\u663e\u8457\u6539\u5584\u4e86\u591a\u7528\u6237\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u901f\u7387\u63a7\u5236\u7b97\u6cd5\u3002", "conclusion": "Hera\u901a\u8fc7\u8fde\u63a5\u5e94\u7528\u5c42\u54cd\u5e94\u80fd\u529b\u4e0e\u7f51\u7edc\u5c42\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u66f4\u5177\u53ef\u4f38\u7f29\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u7528\u6237\u6c89\u6d78\u5f0f\u4f53\u9a8c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.19593", "pdf": "https://arxiv.org/pdf/2507.19593", "abs": "https://arxiv.org/abs/2507.19593", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Classical game-theoretic models typically assume rational agents, complete\ninformation, and common knowledge of payoffs - assumptions that are often\nviolated in real-world MAS characterized by uncertainty, misaligned\nperceptions, and nested beliefs. To overcome these limitations, researchers\nhave proposed extensions that incorporate models of cognitive constraints,\nsubjective beliefs, and heterogeneous reasoning. Among these, hypergame theory\nextends the classical paradigm by explicitly modeling agents' subjective\nperceptions of the strategic scenario, known as perceptual games, in which\nagents may hold divergent beliefs about the structure, payoffs, or available\nactions. We present a systematic review of agent-compatible applications of\nhypergame theory, examining how its descriptive capabilities have been adapted\nto dynamic and interactive MAS contexts. We analyze 44 selected studies from\ncybersecurity, robotics, social simulation, communications, and general\ngame-theoretic modeling. Building on a formal introduction to hypergame theory\nand its two major extensions - hierarchical hypergames and HNF - we develop\nagent-compatibility criteria and an agent-based classification framework to\nassess integration patterns and practical applicability. Our analysis reveals\nprevailing tendencies, including the prevalence of hierarchical and graph-based\nmodels in deceptive reasoning and the simplification of extensive theoretical\nframeworks in practical applications. We identify structural gaps, including\nthe limited adoption of HNF-based models, the lack of formal hypergame\nlanguages, and unexplored opportunities for modeling human-agent and\nagent-agent misalignment. By synthesizing trends, challenges, and open research\ndirections, this review provides a new roadmap for applying hypergame theory to\nenhance the realism and effectiveness of strategic modeling in dynamic\nmulti-agent environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u8d85\u535a\u5f08\u7406\u8bba\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u5176\u5728\u5efa\u6a21\u667a\u80fd\u4f53\u4e3b\u89c2\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u8d8b\u52bf\u3001\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7ecf\u5178\u7684\u535a\u5f08\u8bba\u6a21\u578b\u5047\u8bbe\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u611f\u77e5\u9519\u4f4d\u548c\u5d4c\u5957\u4fe1\u5ff5\u7b49\u95ee\u9898\u3002\u8d85\u535a\u5f08\u7406\u8bba\u88ab\u63d0\u51fa\u4ee5\u663e\u5f0f\u5efa\u6a21\u667a\u80fd\u4f53\u5bf9\u6218\u7565\u60c5\u666f\u7684\u4e3b\u89c2\u611f\u77e5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5ba1\u67e5\u5176\u5728\u52a8\u6001\u3001\u4ea4\u4e92\u5f0fMAS\u80cc\u666f\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u5bf944\u9879\u6765\u81ea\u7f51\u7edc\u5b89\u5168\u3001\u673a\u5668\u4eba\u3001\u793e\u4f1a\u4eff\u771f\u3001\u901a\u4fe1\u548c\u4e00\u822c\u535a\u5f08\u8bba\u5efa\u6a21\u9886\u57df\u7684\u8d85\u535a\u5f08\u7406\u8bba\u667a\u80fd\u4f53\u517c\u5bb9\u5e94\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\u3002\u57fa\u4e8e\u5bf9\u8d85\u535a\u5f08\u7406\u8bba\u53ca\u5176\u5206\u5c42\u8d85\u535a\u5f08\u548cHNF\u4e24\u4e2a\u4e3b\u8981\u6269\u5c55\u7684\u6b63\u5f0f\u4ecb\u7ecd\uff0c\u7814\u7a76\u5f00\u53d1\u4e86\u667a\u80fd\u4f53\u517c\u5bb9\u6027\u6807\u51c6\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30\u5176\u6574\u5408\u6a21\u5f0f\u548c\u5b9e\u9645\u9002\u7528\u6027\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5728\u6b3a\u9a97\u6027\u63a8\u7406\u4e2d\u5206\u5c42\u548c\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\u666e\u904d\u5b58\u5728\uff0c\u4ee5\u53ca\u590d\u6742\u7406\u8bba\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u88ab\u7b80\u5316\u7684\u4e3b\u6d41\u8d8b\u52bf\u3002\u540c\u65f6\uff0c\u4e5f\u8bc6\u522b\u51fa\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u5305\u62ecHNF\u6a21\u578b\u7684\u6709\u9650\u91c7\u7528\u3001\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u8d85\u535a\u5f08\u8bed\u8a00\uff0c\u4ee5\u53ca\u5728\u5efa\u6a21\u4eba-\u667a\u80fd\u4f53\u548c\u667a\u80fd\u4f53-\u667a\u80fd\u4f53\u9519\u4f4d\u65b9\u9762\u7684\u672a\u63a2\u7d22\u673a\u4f1a\u3002", "conclusion": "\u672c\u7efc\u8ff0\u901a\u8fc7\u7efc\u5408\u5206\u6790\u5f53\u524d\u8d8b\u52bf\u3001\u6311\u6218\u548c\u5f00\u653e\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u5c06\u8d85\u535a\u5f08\u7406\u8bba\u5e94\u7528\u4e8e\u589e\u5f3a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u6218\u7565\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u548c\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2507.19590", "pdf": "https://arxiv.org/pdf/2507.19590", "abs": "https://arxiv.org/abs/2507.19590", "authors": ["Chandravardhan Singh Raghaw", "Jasmer Singh Sanjotra", "Mohammad Zia Ur Rehman", "Shubhi Bansal", "Shahid Shafi Dar", "Nagendra Kumar"], "title": "T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Precise and automated segmentation of the liver and its tumor within CT scans\nplays a pivotal role in swift diagnosis and the development of optimal\ntreatment plans for individuals with liver diseases and malignancies. However,\nautomated liver and tumor segmentation faces significant hurdles arising from\nthe inherent heterogeneity of tumors and the diverse visual characteristics of\nlivers across a broad spectrum of patients. Aiming to address these challenges,\nwe present a novel Transformer-aware Multiscale Progressive Encoder-Decoder\nNetwork (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet\nleverages a deep adaptive features backbone through a progressive\nencoder-decoder structure, enhanced by skip connections for recalibrating\nchannel-wise features while preserving spatial integrity. A\nTransformer-inspired dynamic attention mechanism captures long-range contextual\nrelationships within the spatial domain, further enhanced by multi-scale\nfeature utilization for refined local details, leading to accurate prediction.\nMorphological boundary refinement is then employed to address indistinct\nboundaries with neighboring organs, capturing finer details and yielding\nprecise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed\non two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive\nquantitative and qualitative analyses demonstrate the superiority of T-MPEDNet\ncompared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves\noutstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and\ntumor segmentation, respectively. Similar performance is observed on 3DIRCADb,\nwith DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.\nOur findings prove that T-MPEDNet is an efficacious and reliable framework for\nautomated segmentation of the liver and its tumor in CT scans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faT-MPEDNet\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u548c\u591a\u5c3a\u5ea6\u6e10\u8fdb\u7f16\u89e3\u7801\u5668\u7684\u65b0\u578b\u7f51\u7edc\uff0c\u7528\u4e8eCT\u626b\u63cf\u4e2d\u809d\u810f\u53ca\u5176\u80bf\u7624\u7684\u81ea\u52a8\u5316\u7cbe\u786e\u5206\u5272\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u809d\u810f\u53ca\u80bf\u7624\u5728CT\u626b\u63cf\u4e2d\u7684\u7cbe\u786e\u81ea\u52a8\u5206\u5272\u5bf9\u4e8e\u809d\u75c5\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u80bf\u7624\u56fa\u6709\u7684\u5f02\u8d28\u6027\u548c\u809d\u810f\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6837\u6027\u4e3a\u81ea\u52a8\u5316\u5206\u5272\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Transformer-aware Multiscale Progressive Encoder-Decoder Network (T-MPEDNet)\u3002\u8be5\u7f51\u7edc\u5229\u7528\u6e10\u8fdb\u5f0f\u7f16\u89e3\u7801\u7ed3\u6784\u548c\u8df3\u8dc3\u8fde\u63a5\u5b9e\u73b0\u6df1\u5ea6\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408Transformer\u542f\u53d1\u7684\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u5229\u7528\u6765\u7ec6\u5316\u5c40\u90e8\u7ec6\u8282\uff0c\u5e76\u91c7\u7528\u5f62\u6001\u5b66\u8fb9\u754c\u7ec6\u5316\u6765\u5904\u7406\u6a21\u7cca\u8fb9\u754c\u3002", "result": "T-MPEDNet\u5728LiTS\u548c3DIRCADb\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e12\u79cd\u73b0\u6709\u65b9\u6cd5\u3002\u5728LiTS\u6570\u636e\u96c6\u4e0a\uff0c\u809d\u810f\u548c\u80bf\u7624\u5206\u5272\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u5206\u522b\u8fbe\u523097.6%\u548c89.1%\u3002\u57283DIRCADb\u6570\u636e\u96c6\u4e0a\uff0cDSC\u5206\u522b\u4e3a98.3%\u548c83.3%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\uff0cT-MPEDNet\u662f\u4e00\u4e2a\u5728CT\u626b\u63cf\u4e2d\u5b9e\u73b0\u809d\u810f\u53ca\u5176\u80bf\u7624\u81ea\u52a8\u5206\u5272\u7684\u6709\u6548\u4e14\u53ef\u9760\u7684\u6846\u67b6\u3002"}}
{"id": "2507.19514", "pdf": "https://arxiv.org/pdf/2507.19514", "abs": "https://arxiv.org/abs/2507.19514", "authors": ["Andrew Kiruluta"], "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a fully spectral learning framework that eliminates traditional\nneural layers by operating entirely in the wavelet domain. The model applies\nlearnable nonlinear transformations, including soft-thresholding and gain-phase\nmodulation, directly to wavelet coefficients. It also includes a differentiable\nwavelet basis selection mechanism, enabling adaptive processing using families\nsuch as Haar, Daubechies, and Biorthogonal wavelets.\n  Implemented in PyTorch with full 3D support, the model maintains a spectral\npipeline without spatial convolutions or attention. On synthetic 3D denoising\nand natural language tasks from the GLUE benchmark, including SST-2 sentiment\nclassification, the model achieves 89.3 percent accuracy, close to a 4-layer\nTransformer baseline (90.1 percent), while using 72 percent fewer parameters\nand 58 percent less peak memory. Faster early convergence is observed due to\nspectral sparsity priors.\n  In contrast to the quadratic complexity of self-attention and large matrix\nmultiplications in Transformers, our approach uses linear-time wavelet\ntransforms and pointwise nonlinearities, significantly reducing inference cost.\nThis yields a compact, interpretable, and efficient alternative to neural\nmodels. Our results support the viability of principled spectral learning in\nboth vision and language tasks, offering new directions for model design\nwithout overparameterized architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5168\u8c31\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u4e2d\u64cd\u4f5c\u5e76\u76f4\u63a5\u5bf9\u5c0f\u6ce2\u7cfb\u6570\u5e94\u7528\u53ef\u5b66\u4e60\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5c42\u3002\u8be5\u6a21\u578b\u652f\u6301\u53ef\u5fae\u5206\u5c0f\u6ce2\u57fa\u9009\u62e9\uff0c\u5728\u53c2\u6570\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u5e45\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u53bb\u566a\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0eTransformer\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff08\u5c24\u5176\u662fTransformer\uff09\u9762\u4e34\u53c2\u6570\u91cf\u5927\u3001\u5185\u5b58\u5360\u7528\u9ad8\u3001\u63a8\u7406\u6210\u672c\u9ad8\uff08\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff09\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63a2\u7d22\u65e0\u9700\u8fc7\u5ea6\u53c2\u6570\u5316\u67b6\u6784\u7684\u539f\u7406\u6027\u8c31\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u5b8c\u5168\u5728\u5c0f\u6ce2\u57df\u4e2d\u64cd\u4f5c\uff0c\u4e0d\u4f7f\u7528\u7a7a\u95f4\u5377\u79ef\u6216\u6ce8\u610f\u529b\u673a\u5236\u3002\u5b83\u76f4\u63a5\u5bf9\u5c0f\u6ce2\u7cfb\u6570\u5e94\u7528\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff08\u5305\u62ec\u8f6f\u9608\u503c\u548c\u589e\u76ca-\u76f8\u4f4d\u8c03\u5236\uff09\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u5c0f\u6ce2\u57fa\u9009\u62e9\u673a\u5236\uff08\u5982Haar\u3001Daubechies\u3001Biorthogonal\u5c0f\u6ce2\uff09\u3002\u6a21\u578b\u5728PyTorch\u4e2d\u5b9e\u73b0\uff0c\u652f\u63013D\u6570\u636e\uff0c\u5e76\u5229\u7528\u7ebf\u6027\u65f6\u95f4\u7684\u5c0f\u6ce2\u53d8\u6362\u548c\u70b9\u5f0f\u975e\u7ebf\u6027\u64cd\u4f5c\u3002", "result": "\u5728\u5408\u62103D\u53bb\u566a\u548cGLUE\u57fa\u51c6\uff08SST-2\u60c5\u611f\u5206\u7c7b\uff09\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u8fbe\u5230\u4e8689.3%\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u56db\u5c42Transformer\u57fa\u7ebf\uff0890.1%\uff09\u3002\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u53c2\u6570\u91cf\u51cf\u5c11\u4e8672%\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e8658%\u3002\u7531\u4e8e\u8c31\u7a00\u758f\u5148\u9a8c\uff0c\u6a21\u578b\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002\u4e0eTransformer\u76f8\u6bd4\uff0c\u5176\u63a8\u7406\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6210\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u8c31\u5b66\u4e60\u65b9\u6cd5\u662f\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5b83\u9a8c\u8bc1\u4e86\u539f\u7406\u6027\u8c31\u5b66\u4e60\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u65e0\u9700\u4f9d\u8d56\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u67b6\u6784\u3002"}}
{"id": "2507.19616", "pdf": "https://arxiv.org/pdf/2507.19616", "abs": "https://arxiv.org/abs/2507.19616", "authors": ["Xuchen Wei", "Yangxin Wu", "Yaoyin Zhang", "Henglyu Liu", "Kehai Chen", "Xuefeng Bai", "Min Zhang"], "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track", "categories": ["cs.CL"], "comment": "7 pages, 1 figure, submitted to IWSLT 2025", "summary": "This paper presents HITSZ's submission for the IWSLT 2025 Indic track,\nfocusing on speech-to-text translation (ST) for English-to-Indic and\nIndic-to-English language pairs. To enhance translation quality in this\nlow-resource scenario, we propose an end-to-end system integrating the\npre-trained Whisper automated speech recognition (ASR) model with Krutrim, an\nIndic-specialized large language model (LLM). Experimental results demonstrate\nthat our end-to-end system achieved average BLEU scores of $28.88$ for\nEnglish-to-Indic directions and $27.86$ for Indic-to-English directions.\nFurthermore, we investigated the Chain-of-Thought (CoT) method. While this\nmethod showed potential for significant translation quality improvements on\nsuccessfully parsed outputs (e.g. a $13.84$ BLEU increase for\nTamil-to-English), we observed challenges in ensuring the model consistently\nadheres to the required CoT output format.", "AI": {"tldr": "HITSZ\u4e3aIWSLT 2025\u5370\u5ea6\u8bed\u8d5b\u9053\u63d0\u4ea4\u7684\u8bba\u6587\uff0c\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408Whisper\u548cKrutrim\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u82f1\u8bed-\u5370\u5ea6\u8bed\u53ca\u5370\u5ea6\u8bed-\u82f1\u8bed\u7ffb\u8bd1\u3002\u8be5\u7cfb\u7edf\u53d6\u5f97\u4e86\u826f\u597d\u7684BLEU\u5206\u6570\uff0c\u5e76\u63a2\u8ba8\u4e86CoT\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u6709\u63d0\u5347\u6f5c\u529b\u4f46\u5b58\u5728\u683c\u5f0f\u4f9d\u4ece\u6027\u6311\u6218\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u63d0\u5347\u82f1\u8bed-\u5370\u5ea6\u8bed\u548c\u5370\u5ea6\u8bed-\u82f1\u8bed\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\uff08ST\uff09\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u6574\u5408\u9884\u8bad\u7ec3\u7684Whisper\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u4e0e\u4e13\u6ce8\u4e8e\u5370\u5ea6\u8bed\u7684Krutrim\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86Chain-of-Thought (CoT) \u65b9\u6cd5\u3002", "result": "\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u82f1\u8bed-\u5370\u5ea6\u8bed\u65b9\u5411\u4e0a\u5e73\u5747BLEU\u5f97\u5206\u4e3a28.88\uff0c\u5728\u5370\u5ea6\u8bed-\u82f1\u8bed\u65b9\u5411\u4e0a\u4e3a27.86\u3002CoT\u65b9\u6cd5\u5728\u6210\u529f\u89e3\u6790\u7684\u8f93\u51fa\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u7ffb\u8bd1\u8d28\u91cf\u63d0\u5347\u6f5c\u529b\uff08\u4f8b\u5982\uff0c\u6cf0\u7c73\u5c14\u8bed\u5230\u82f1\u8bed\u63d0\u534713.84 BLEU\uff09\uff0c\u4f46\u89c2\u5bdf\u5230\u6a21\u578b\u96be\u4ee5\u6301\u7eed\u9075\u5faa\u6240\u9700CoT\u8f93\u51fa\u683c\u5f0f\u7684\u6311\u6218\u3002", "conclusion": "\u7ed3\u5408Whisper\u548cKrutrim\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u5370\u5ea6\u8bed\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002CoT\u65b9\u6cd5\u5177\u6709\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u89e3\u51b3\u6a21\u578b\u8f93\u51fa\u683c\u5f0f\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2507.20115", "pdf": "https://arxiv.org/pdf/2507.20115", "abs": "https://arxiv.org/abs/2507.20115", "authors": ["Gongli Xi", "Ye Tian", "Yannan Hu", "Yuchao Zhang", "Yapeng Niu", "Xiangyang Gong"], "title": "Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion", "categories": ["cs.NI", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In response to Distributed Denial of Service (DDoS) attacks, recent research\nefforts increasingly rely on Machine Learning (ML)-based solutions, whose\neffectiveness largely depends on the quality of labeled training datasets. To\naddress the scarcity of such datasets, data augmentation with synthetic traces\nis often employed. However, current synthetic trace generation methods struggle\nto capture the complex temporal patterns and spatial distributions exhibited in\nemerging DDoS attacks. This results in insufficient resemblance to real traces\nand unsatisfied detection accuracy when applied to ML tasks. In this paper, we\npropose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,\nmulti-stream network traffic generative model based on diffusion models,\nfeaturing two main streams: The field stream utilizes spatial mapping to bridge\nnetwork data characteristics with pre-trained realms of stable diffusion\nmodels, effectively translating complex network interactions into formats that\nstable diffusion can process, while the spatial stream adopts a dynamic\ntemporal modeling approach, meticulously capturing the intrinsic temporal\npatterns of network traffic. Extensive experiments demonstrate that data\ngenerated by our model exhibits higher statistical similarity to originals\ncompared to current state-of-the-art solutions, and enhance performances on a\nwide range of downstream tasks.", "AI": {"tldr": "\u63d0\u51faDSTF-Diffusion\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u7f51\u7edc\u6d41\u91cf\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684DDoS\u653b\u51fb\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728DDoS\u68c0\u6d4b\u4e2d\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u6b64\u7c7b\u6570\u636e\u7a00\u7f3a\u3002\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349DDoS\u653b\u51fb\u7684\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5bfc\u81f4\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5dee\u5f02\u5927\uff0c\u5f71\u54cdML\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faDSTF-Diffusion\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u89c6\u56fe\u3001\u591a\u6d41\u7f51\u7edc\u6d41\u91cf\u751f\u6210\u6a21\u578b\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6d41\uff1a\u4e00\u4e2a\u201c\u5b57\u6bb5\u6d41\u201d\u5229\u7528\u7a7a\u95f4\u6620\u5c04\u5c06\u7f51\u7edc\u6570\u636e\u7279\u6027\u4e0e\u9884\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff1b\u53e6\u4e00\u4e2a\u201c\u65f6\u95f4\u6d41\u201d\uff08\u539f\u6587spatial stream\u6839\u636e\u63cf\u8ff0\u5e94\u4e3atemporal stream\uff09\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u7cbe\u786e\u6355\u6349\u7f51\u7edc\u6d41\u91cf\u7684\u5185\u5728\u65f6\u95f4\u6a21\u5f0f\u3002", "result": "\u8be5\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u4e0e\u539f\u59cb\u6570\u636e\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7edf\u8ba1\u76f8\u4f3c\u6027\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5982DDoS\u68c0\u6d4b\uff09\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "DSTF-Diffusion\u6a21\u578b\u901a\u8fc7\u5176\u521b\u65b0\u7684\u53cc\u6d41\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5408\u6210\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u751f\u6210\u4e2d\u65f6\u7a7a\u6a21\u5f0f\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3aDDoS\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.19608", "pdf": "https://arxiv.org/pdf/2507.19608", "abs": "https://arxiv.org/abs/2507.19608", "authors": ["Jiawen Qi", "Chang Gao", "Zhaochun Ren", "Qinyu Chen"], "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference", "categories": ["cs.AI", "eess.SP"], "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge devices remains challenging\ndue to their quadratically increasing computations with the sequence length.\nExisting studies for dynamic attention pruning are designed for hardware with\nmassively parallel computation capabilities, such as GPUs or TPUs, and aim at\nlong context lengths (e.g., 64K), making them unsuitable for edge scenarios. We\npresent DeltaLLM, a training-free framework that exploits temporal sparsity in\nattention patterns to enable efficient LLM inference across both the prefilling\nand decoding stages, on resource-constrained edge devices. DeltaLLM introduces\nan accuracy- and memory-aware delta matrix construction strategy that\nintroduces temporal sparsity, and a context-aware hybrid attention mechanism\nthat combines full attention in a local context window with delta approximation\noutside it to increase accuracy. We evaluate our framework on the\nedge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model\nacross diverse language tasks. The results show that on BitNet, our framework\nincreases the attention sparsity from 0% to 60% during the prefilling stage\nwith slight accuracy improvement on the WG task, and 0% to 57% across both the\nprefilling and decoding stages, with even higher F1 score from 29.63 to 30.97\non SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity\nduring the prefilling stage and around 57% across both stages with negligible\naccuracy drop. These results demonstrate that DeltaLLM offers a promising\nsolution for efficient edge deployment, requiring no fine-tuning and seamlessly\nintegrating with existing inference pipelines.", "AI": {"tldr": "DeltaLLM\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5f0f\u4e2d\u7684\u65f6\u95f4\u7a00\u758f\u6027\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6ce8\u610f\u529b\u7a00\u758f\u6027\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u91cf\u968f\u5e8f\u5217\u957f\u5ea6\u5448\u5e73\u65b9\u589e\u957f\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u52a8\u6001\u6ce8\u610f\u529b\u526a\u679d\u65b9\u6cd5\u591a\u4e3aGPU/TPU\u8bbe\u8ba1\uff0c\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u573a\u666f\u3002", "method": "DeltaLLM\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u65f6\u95f4\u7a00\u758f\u6027\u3002\u5b83\u5f15\u5165\u4e86\uff1a1) \u4e00\u79cd\u517c\u987e\u7cbe\u5ea6\u548c\u5185\u5b58\u7684Delta\u77e9\u9635\u6784\u5efa\u7b56\u7565\uff0c\u4ee5\u5f15\u5165\u65f6\u95f4\u7a00\u758f\u6027\uff1b2) \u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5c40\u90e8\u4e0a\u4e0b\u6587\u7a97\u53e3\u5185\u7684\u5168\u6ce8\u610f\u529b\u4e0e\u5916\u90e8\u7684Delta\u8fd1\u4f3c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728BitNet\u6a21\u578b\u4e0a\uff0c\u9884\u586b\u5145\u9636\u6bb5\u6ce8\u610f\u529b\u7a00\u758f\u6027\u4ece0%\u63d0\u5347\u523060%\uff0cWG\u4efb\u52a1\u7cbe\u5ea6\u7565\u6709\u63d0\u5347\uff1b\u5728SQuAD-v2\u4efb\u52a1\u4e0a\uff0c\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u6574\u4f53\u7a00\u758f\u6027\u4ece0%\u63d0\u5347\u523057%\uff0cF1\u5206\u6570\u4ece29.63\u63d0\u5347\u523030.97\u3002\u5728Llama\u6a21\u578b\u4e0a\uff0c\u9884\u586b\u5145\u9636\u6bb5\u6700\u9ad8\u53ef\u8fbe60%\u7a00\u758f\u6027\uff0c\u6574\u4f53\u7ea657%\u7a00\u758f\u6027\uff0c\u7cbe\u5ea6\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "DeltaLLM\u4e3aLLM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4e0e\u73b0\u6709\u63a8\u7406\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6ce8\u610f\u529b\u7a00\u758f\u6027\u5e76\u4fdd\u6301\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.19592", "pdf": "https://arxiv.org/pdf/2507.19592", "abs": "https://arxiv.org/abs/2507.19592", "authors": ["Meng Wei", "Charlie Budd", "Oluwatosin Alabi", "Miaojing Shi", "Tom Vercauteren"], "title": "SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Consistent surgical instrument segmentation is critical for automation in\nrobot-assisted surgery. Yet, existing methods only treat instrument-level\ninstance segmentation (IIS) or part-level semantic segmentation (PSS)\nseparately, without interaction between these tasks. In this work, we formulate\na surgical tool segmentation as a unified part-aware instance segmentation\n(PIS) problem and introduce SurgPIS, the first PIS model for surgical\ninstruments. Our method adopts a transformer-based mask classification approach\nand introduces part-specific queries derived from instrument-level object\nqueries, explicitly linking parts to their parent instrument instances. In\norder to address the lack of large-scale datasets with both instance- and\npart-level labels, we propose a weakly-supervised learning strategy for SurgPIS\nto learn from disjoint datasets labelled for either IIS or PSS purposes. During\ntraining, we aggregate our PIS predictions into IIS or PSS masks, thereby\nallowing us to compute a loss against partially labelled datasets. A\nstudent-teacher approach is developed to maintain prediction consistency for\nmissing PIS information in the partially labelled data, e.g., parts of the IIS\nlabelled data. Extensive experiments across multiple datasets validate the\neffectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well\nas IIS, PSS, and instrument-level semantic segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSurgPIS\u7684\u7edf\u4e00\u90e8\u5206\u611f\u77e5\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u5668\u68b0\u5206\u5272\u7684\u6311\u6218\uff0c\u5e76\u5728\u5f31\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u672f\u5668\u68b0\u5206\u5272\u65b9\u6cd5\u5c06\u5668\u68b0\u7ea7\u5b9e\u4f8b\u5206\u5272\uff08IIS\uff09\u548c\u90e8\u4ef6\u7ea7\u8bed\u4e49\u5206\u5272\uff08PSS\uff09\u72ec\u7acb\u5904\u7406\uff0c\u7f3a\u4e4f\u4e24\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8fd9\u79cd\u4e00\u81f4\u6027\u5206\u5272\u5bf9\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u6b64\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u5c06\u624b\u672f\u5de5\u5177\u5206\u5272\u95ee\u9898\u7edf\u4e00\u8868\u8ff0\u4e3a\u90e8\u5206\u611f\u77e5\u5b9e\u4f8b\u5206\u5272\uff08PIS\uff09\uff0c\u5e76\u5f15\u5165\u4e86SurgPIS\u6a21\u578b\u3002\u8be5\u6a21\u578b\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u63a9\u819c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5f15\u5165\u4ece\u5668\u68b0\u7ea7\u5bf9\u8c61\u67e5\u8be2\u6d3e\u751f\u51fa\u7684\u7279\u5b9a\u90e8\u4ef6\u67e5\u8be2\uff0c\u660e\u786e\u5c06\u90e8\u4ef6\u4e0e\u5176\u7236\u7ea7\u5668\u68b0\u5b9e\u4f8b\u5173\u8054\u3002\u4e3a\u89e3\u51b3\u7f3a\u4e4f\u540c\u65f6\u5305\u542b\u5b9e\u4f8b\u548c\u90e8\u4ef6\u7ea7\u522b\u6807\u7b7e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5141\u8bb8SurgPIS\u4ece\u4ec5\u6807\u6ce8\u4e86IIS\u6216PSS\u7684\u72ec\u7acb\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u3002\u8bad\u7ec3\u65f6\uff0cPIS\u9884\u6d4b\u88ab\u805a\u5408\u6210IIS\u6216PSS\u63a9\u819c\uff0c\u4ee5\u8ba1\u7b97\u9488\u5bf9\u90e8\u5206\u6807\u8bb0\u6570\u636e\u96c6\u7684\u635f\u5931\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5b66\u751f-\u6559\u5e08\u65b9\u6cd5\uff0c\u4ee5\u7ef4\u62a4\u90e8\u5206\u6807\u8bb0\u6570\u636e\u4e2d\u7f3a\u5931PIS\u4fe1\u606f\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SurgPIS\u7684\u6709\u6548\u6027\uff0c\u5728PIS\u4ee5\u53caIIS\u3001PSS\u548c\u5668\u68b0\u7ea7\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SurgPIS\u9996\u6b21\u5c06\u624b\u672f\u5668\u68b0\u5206\u5272\u95ee\u9898\u7edf\u4e00\u4e3a\u90e8\u5206\u611f\u77e5\u5b9e\u4f8b\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u8054\u52a8\u548c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u5668\u68b0\u5206\u5272\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.19515", "pdf": "https://arxiv.org/pdf/2507.19515", "abs": "https://arxiv.org/abs/2507.19515", "authors": ["Edmund F. Agyemang", "Hansapani Rodrigo", "Vincent Agbenyeavu"], "title": "A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,\nthough this estimate is an improvement from years past due to improvements in\nsanitation, healthcare practices, and vaccination programs. In this study, we\nperform a comparative analysis of traditional and deep learning models to\npredict Influenza A outbreaks. Using historical data from January 2009 to\nDecember 2023, we compared the performance of traditional ARIMA and Exponential\nSmoothing(ETS) models with six distinct deep learning architectures: Simple\nRNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear\nsuperiority of all the deep learning models, especially the state-of-the-art\nTransformer with respective average testing MSE and MAE of 0.0433 \\pm 0.0020\nand 0.1126 \\pm 0.0016 for capturing the temporal complexities associated with\nInfluenza A data, outperforming well known traditional baseline ARIMA and ETS\nmodels. These findings of this study provide evidence that state-of-the-art\ndeep learning architectures can enhance predictive modeling for infectious\ndiseases and indicate a more general trend toward using deep learning methods\nto enhance public health forecasting and intervention planning strategies.\nFuture work should focus on how these models can be incorporated into real-time\nforecasting and preparedness systems at an epidemic level, and integrated into\nexisting surveillance systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7532\u578b\u6d41\u611f\u7206\u53d1\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662fTransformer\uff0c\u9884\u6d4b\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u7532\u578b\u6d41\u611f\u6bcf\u5e74\u5bfc\u81f4\u5927\u91cf\u547c\u5438\u9053\u6b7b\u4ea1\uff0c\u51c6\u786e\u9884\u6d4b\u5176\u7206\u53d1\u5bf9\u516c\u5171\u536b\u751f\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u75282009\u5e741\u6708\u81f32023\u5e7412\u6708\u7684\u5386\u53f2\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\uff08ARIMA\u3001ETS\uff09\u4e0e\u516d\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Simple RNN\u3001LSTM\u3001GRU\u3001BiLSTM\u3001BiGRU\u3001Transformer\uff09\u7684\u6027\u80fd\u3002\u8bc4\u4f30\u6307\u6807\u4e3a\u5e73\u5747\u6d4b\u8bd5MSE\u548cMAE\u3002", "result": "\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff08ARIMA\u548cETS\uff09\u7684\u9884\u6d4b\u80fd\u529b\u3002\u5176\u4e2d\uff0cTransformer\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5176\u5e73\u5747\u6d4b\u8bd5MSE\u548cMAE\u5206\u522b\u4e3a0.0433 \u00b1 0.0020\u548c0.1126 \u00b1 0.0016\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u7532\u578b\u6d41\u611f\u6570\u636e\u7684\u65f6\u95f4\u590d\u6742\u6027\u3002", "conclusion": "\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u80fd\u663e\u8457\u63d0\u5347\u4f20\u67d3\u75c5\u9884\u6d4b\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u9884\u793a\u7740\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u516c\u5171\u536b\u751f\u9884\u6d4b\u548c\u5e72\u9884\u7b56\u7565\u89c4\u5212\u4e2d\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.19634", "pdf": "https://arxiv.org/pdf/2507.19634", "abs": "https://arxiv.org/abs/2507.19634", "authors": ["Sara Papi", "Maike Z\u00fcfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MCIF\uff0c\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8de8\u8bed\u8a00\u3001\u591a\u6a21\u6001\u53ca\u957f\u77ed\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6b63\u5728\u5411\u901a\u7528\u6307\u4ee4\u9075\u5faa\u6a21\u578b\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5728\u8054\u5408\u8bc4\u4f30\u5176\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4ee5\u53ca\u957f\u77ed\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u4ec5\u9650\u4e8e\u82f1\u8bed\u3001\u4fa7\u91cd\u5355\u6a21\u6001\u3001\u4f9d\u8d56\u77ed\u4e0a\u4e0b\u6587\u6216\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\uff0c\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86MCIF\uff08Multimodal Crosslingual Instruction Following\uff09\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u79d1\u5b66\u6f14\u8bb2\u7684\u591a\u8bed\u8a00\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u3002MCIF\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u8de8\u8bed\u8a00\u3001\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u957f\u77ed\u8f93\u5165\u8fdb\u884c\u6307\u4ee4\u9075\u5faa\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u8bed\u97f3\u3001\u89c6\u89c9\u548c\u6587\u672c\u4e09\u79cd\u6838\u5fc3\u6a21\u6001\uff0c\u4ee5\u53ca\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u610f\u5927\u5229\u8bed\u548c\u4e2d\u6587\u56db\u79cd\u8bed\u8a00\u3002", "result": "\u8be5\u62bd\u8c61\u4e3b\u8981\u4ecb\u7ecd\u4e86MCIF\u57fa\u51c6\u7684\u521b\u5efa\u53ca\u5176\u7279\u6027\uff0c\u5e76\u672a\u76f4\u63a5\u5448\u73b0\u4f7f\u7528\u6b64\u57fa\u51c6\u8bc4\u4f30\u6a21\u578b\u6240\u83b7\u5f97\u7684\u5177\u4f53\u7814\u7a76\u7ed3\u679c\u3002\u57fa\u51c6\u7684\u521b\u5efa\u672c\u8eab\u662f\u672c\u7814\u7a76\u7684\u4e3b\u8981\u4ea7\u51fa\uff0c\u5176\u5177\u5907\u4e86\u5168\u9762\u8bc4\u4f30MLLMs\u8de8\u8bed\u8a00\u7406\u89e3\u548c\u591a\u6a21\u6001\u4fe1\u606f\u7ed3\u5408\u80fd\u529b\u7684\u529f\u80fd\u3002", "conclusion": "MCIF\u7684\u63a8\u51fa\u586b\u8865\u4e86MLLMs\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4e14\u652f\u6301\u957f\u77ed\u4e0a\u4e0b\u6587\u7684\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u5de5\u5177\u3002\u5b83\u7684\u53d1\u5e03\u5c06\u4fc3\u8fdbMLLMs\u7684\u5f00\u653e\u7814\u7a76\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2507.20116", "pdf": "https://arxiv.org/pdf/2507.20116", "abs": "https://arxiv.org/abs/2507.20116", "authors": ["Yinuo Deng", "Hailiang Zhao", "Dongjing Wang", "Peng Chen", "Wenzhuo Qian", "Jianwei Yin", "Schahram Dustdar", "Shuiguang Deng"], "title": "Accelerating Containerized Service Delivery at the Network Edge", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.", "AI": {"tldr": "PeerSync\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316P2P\u7cfb\u7edf\uff0c\u65e8\u5728\u4f18\u5316\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684\u5bb9\u5668\u955c\u50cf\u5206\u53d1\uff0c\u901a\u8fc7\u667a\u80fd\u4e0b\u8f7d\u5f15\u64ce\u548c\u7ba1\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u53d1\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u7f51\u7edc\u6d41\u91cf\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u548c\u7f51\u7edc\u52a8\u6001\u53d8\u5316\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5bb9\u5668\u955c\u50cf\u5206\u53d1\u5bf9\u673a\u5668\u5b66\u4e60\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faPeerSync\u7cfb\u7edf\uff0c\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684P2P\u5bb9\u5668\u955c\u50cf\u5206\u53d1\u7cfb\u7edf\u3002\u5b83\u91c7\u7528\u6d41\u884c\u5ea6\u4e0e\u7f51\u7edc\u611f\u77e5\u7684\u4e0b\u8f7d\u5f15\u64ce\uff08\u5229\u7528\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\uff09\uff0c\u5e76\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8ddf\u8e2a\u5668\u9009\u4e3e\u4ee5\u5feb\u901f\u53d1\u73b0\u5bf9\u7b49\u8282\u70b9\uff0c\u4ee5\u53ca\u52a8\u6001\u7f13\u5b58\u7ba1\u7406\u4ee5\u9ad8\u6548\u5229\u7528\u5b58\u50a8\u3002\u7cfb\u7edf\u4f7f\u7528Rust\u8bed\u8a00\u5b9e\u73b0\uff0c\u5e76\u5728\u7269\u7406\u8fb9\u7f18\u8bbe\u5907\u548cDocker\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPeerSync\u7684\u5206\u53d1\u901f\u5ea6\u76f8\u8f83\u4e8eBaseline\u3001Dragonfly\u548cKraken\u5206\u522b\u63d0\u9ad8\u4e862.72\u500d\u30011.79\u500d\u548c1.28\u500d\u3002\u5728\u62e5\u585e\u548c\u53d8\u5316\u7684\u7ec4\u7f51\u6761\u4ef6\u4e0b\uff0c\u5cf0\u503c\u8de8\u7f51\u7edc\u6d41\u91cf\u663e\u8457\u51cf\u5c11\u4e8690.72%\u3002", "conclusion": "PeerSync\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u8fb9\u7f18\u73af\u5883\u4e0b\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u5316\u5bb9\u5668\u955c\u50cf\u5206\u53d1\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u5206\u53d1\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5cf0\u503c\u7f51\u7edc\u6d41\u91cf\u3002"}}
{"id": "2507.19672", "pdf": "https://arxiv.org/pdf/2507.19672", "abs": "https://arxiv.org/abs/2507.19672", "authors": ["Haoran Lu", "Luyang Fang", "Ruidong Zhang", "Xinliang Li", "Jiazhang Cai", "Huimin Cheng", "Lin Tang", "Ziyu Liu", "Zeliang Sun", "Tao Wang", "Yingchuan Zhang", "Arif Hassan Zidan", "Jinwen Xu", "Jincheng Yu", "Meizhi Yu", "Hanqi Jiang", "Xilin Gong", "Weidi Luo", "Bolun Sun", "Yongkai Chen", "Terry Ma", "Shushan Wu", "Yifan Zhou", "Junhao Chen", "Haotian Xiang", "Jing Zhang", "Afrar Jahin", "Wei Ruan", "Ke Deng", "Yi Pan", "Peilong Wang", "Jiahui Li", "Zhengliang Liu", "Lu Zhang", "Lin Zhao", "Wei Liu", "Dajiang Zhu", "Xin Xing", "Fei Dou", "Wei Zhang", "Chao Huang", "Rongjie Liu", "Mengrui Zhang", "Yiwen Liu", "Xiaoxiao Sun", "Qin Lu", "Zhen Xiang", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "119 pages, 10 figures, 7 tables", "summary": "Due to the remarkable capabilities and growing impact of large language\nmodels (LLMs), they have been deeply integrated into many aspects of society.\nThus, ensuring their alignment with human values and intentions has emerged as\na critical challenge. This survey provides a comprehensive overview of\npractical alignment techniques, training protocols, and empirical findings in\nLLM alignment. We analyze the development of alignment methods across diverse\nparadigms, characterizing the fundamental trade-offs between core alignment\nobjectives. Our analysis shows that while supervised fine-tuning enables basic\ninstruction-following, preference-based methods offer more flexibility for\naligning with nuanced human intent. We discuss state-of-the-art techniques,\nincluding Direct Preference Optimization (DPO), Constitutional AI,\nbrain-inspired methods, and alignment uncertainty quantification (AUQ),\nhighlighting their approaches to balancing quality and efficiency. We review\nexisting evaluation frameworks and benchmarking datasets, emphasizing\nlimitations such as reward misspecification, distributional robustness, and\nscalable oversight. We summarize strategies adopted by leading AI labs to\nillustrate the current state of practice. We conclude by outlining open\nproblems in oversight, value pluralism, robustness, and continuous alignment.\nThis survey aims to inform both researchers and practitioners navigating the\nevolving landscape of LLM alignment.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u5ba1\u89c6\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\u3001\u5b9e\u7528\u6280\u672f\u3001\u8bc4\u4f30\u5c40\u9650\u6027\u53ca\u672a\u6765\u5f00\u653e\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u663e\u8457\u589e\u5f3a\u4e14\u5f71\u54cd\u65e5\u76ca\u6269\u5927\uff0c\u5b83\u4eec\u5df2\u6df1\u5ea6\u878d\u5165\u793e\u4f1a\u591a\u65b9\u9762\uff0c\u56e0\u6b64\u786e\u4fddLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u5bf9\u9f50\u5df2\u6210\u4e3a\u4e00\u9879\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86LLM\u5bf9\u9f50\u7684\u5b9e\u7528\u6280\u672f\u3001\u8bad\u7ec3\u534f\u8bae\u548c\u5b9e\u8bc1\u53d1\u73b0\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u8303\u5f0f\u4e0b\u5bf9\u9f50\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u63a2\u8ba8\u4e86\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8fd8\u8ba8\u8bba\u4e86\u5148\u8fdb\u6280\u672f\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u603b\u7ed3\u4e86\u9886\u5148AI\u5b9e\u9a8c\u5ba4\u7684\u7b56\u7565\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u76d1\u7763\u5fae\u8c03\u80fd\u5b9e\u73b0\u57fa\u672c\u6307\u4ee4\u9075\u5faa\uff0c\u800c\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u5728\u5bf9\u9f50\u7ec6\u5fae\u4eba\u7c7b\u610f\u56fe\u65b9\u9762\u66f4\u7075\u6d3b\u3002\u8ba8\u8bba\u4e86\u5305\u62ecDPO\u3001\u5baa\u6cd5\u5f0fAI\u7b49\u5728\u5185\u7684\u524d\u6cbf\u6280\u672f\u5982\u4f55\u5e73\u8861\u8d28\u91cf\u4e0e\u6548\u7387\u3002\u540c\u65f6\uff0c\u56de\u987e\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u53ca\u5176\u5728\u5956\u52b1\u6307\u5b9a\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u76d1\u7763\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7efc\u8ff0\u6982\u8ff0\u4e86\u76d1\u7763\u3001\u4ef7\u503c\u591a\u5143\u5316\u3001\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5bf9\u9f50\u65b9\u9762\u7684\u5f00\u653e\u95ee\u9898\uff0c\u65e8\u5728\u4e3aLLM\u5bf9\u9f50\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u4e0e\u53c2\u8003\u3002"}}
{"id": "2507.19599", "pdf": "https://arxiv.org/pdf/2507.19599", "abs": "https://arxiv.org/abs/2507.19599", "authors": ["Haochen Wang", "Qirui Chen", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Weidi Xie", "Stratis Gavves"], "title": "Object-centric Video Question Answering with Visual Grounding and Referring", "categories": ["cs.CV"], "comment": null, "summary": "Video Large Language Models (VideoLLMs) have recently demonstrated remarkable\nprogress in general video understanding. However, existing models primarily\nfocus on high-level comprehension and are limited to text-only responses,\nrestricting the flexibility for object-centric, multiround interactions. In\nthis paper, we make three contributions: (i) we address these limitations by\nintroducing a VideoLLM model, capable of performing both object referring for\ninput and grounding for output in video reasoning tasks, i.e., allowing users\nto interact with videos using both textual and visual prompts; (ii) we propose\nSTOM (Spatial-Temporal Overlay Module), a novel approach that propagates\narbitrary visual prompts input at any single timestamp to the remaining frames\nwithin a video; (iii) we present VideoInfer, a manually curated object-centric\nvideo instruction dataset featuring questionanswering pairs that require\nreasoning. We conduct comprehensive experiments on VideoInfer and other\nexisting benchmarks across video question answering and referring object\nsegmentation. The results on 12 benchmarks of 6 tasks show that our proposed\nmodel consistently outperforms baselines in both video question answering and\nsegmentation, underscoring its robustness in multimodal, object-centric video\nand image understanding. Project page:\nhttps://qirui-chen.github.io/RGA3-release/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684VideoLLM\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u63d0\u793a\u548cSTOM\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u591a\u8f6e\u89c6\u9891\u4ea4\u4e92\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VideoLLMs\u4e3b\u8981\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6b21\u7406\u89e3\u4e14\u4ec5\u652f\u6301\u6587\u672c\u54cd\u5e94\uff0c\u7f3a\u4e4f\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u591a\u8f6e\u4ea4\u4e92\u7075\u6d3b\u6027\u3002", "method": "1. \u5f15\u5165\u4e00\u4e2aVideoLLM\u6a21\u578b\uff0c\u652f\u6301\u5728\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u5bf9\u8c61\u5f15\u7528\uff08\u8f93\u5165\uff09\u548c\u5b9a\u4f4d\uff08\u8f93\u51fa\uff09\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u4e0e\u89c6\u9891\u4ea4\u4e92\u30022. \u63d0\u51faSTOM\uff08Spatial-Temporal Overlay Module\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u53ef\u5c06\u4efb\u610f\u89c6\u89c9\u63d0\u793a\u4ece\u5355\u4e2a\u65f6\u95f4\u6233\u4f20\u64ad\u5230\u89c6\u9891\u7684\u5176\u4f59\u5e27\u30023. \u6784\u5efaVideoInfer\uff0c\u4e00\u4e2a\u4eba\u5de5\u6574\u7406\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5305\u542b\u9700\u8981\u63a8\u7406\u7684\u95ee\u7b54\u5bf9\u3002", "result": "\u5728VideoInfer\u53ca\u5176\u4ed6\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u548c\u6307\u4ee3\u5bf9\u8c61\u5206\u5272\u57fa\u51c6\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u57286\u9879\u4efb\u52a1\u768412\u4e2a\u57fa\u51c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5206\u5272\u65b9\u9762\u5747\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6a21\u6001\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u548c\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u6570\u636e\u6784\u5efa\uff0c\u663e\u8457\u589e\u5f3a\u4e86VideoLLMs\u5728\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7406\u89e3\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.19517", "pdf": "https://arxiv.org/pdf/2507.19517", "abs": "https://arxiv.org/abs/2507.19517", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Ben Beck"], "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted for publication in the Proceedings of\n  the $28^{th}$ IEEE International Conference on Intelligent Transportation\n  Systems (ITSC 2025). This is the author's version of the work", "summary": "Accurate link-level bicycle volume estimation is essential for informed urban\nand transport planning but it is challenged by extremely sparse count data in\nurban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task\nframework augmenting a Hybrid Graph Neural Network (GNN) with Variational\nAutoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing\nsparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model\nintricate spatial relationships in sparse networks while VAE generates\nsynthetic nodes and edges to enrich the graph structure and enhance the\nestimation performance. BikeVAE-GNN simultaneously performs - regression for\nbicycling volume estimation and classification for bicycling traffic level\ncategorization. We demonstrate the effectiveness of BikeVAE-GNN using\nOpenStreetMap data and publicly available bicycle count data within the City of\nMelbourne - where only 141 of 15,933 road segments have labeled counts\n(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN\noutperforms machine learning and baseline GNN models, achieving a mean absolute\nerror (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.\nAblation studies further validate the effective role of Hybrid-GNN and VAE\ncomponents. Our research advances bicycling volume estimation in sparse\nnetworks using novel and state-of-the-art approaches, providing insights for\nsustainable bicycling infrastructures.", "AI": {"tldr": "BikeVAE-GNN\u662f\u4e00\u79cd\u7ed3\u5408GNN\u548cVAE\u7684\u65b0\u578b\u53cc\u4efb\u52a1\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7a00\u758f\u6570\u636e\u4e0b\u81ea\u884c\u8f66\u4ea4\u901a\u91cf\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u51c6\u786e\u7684\u81ea\u884c\u8f66\u4ea4\u901a\u91cf\u4f30\u7b97\u5bf9\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5168\u7403\u57ce\u5e02\u81ea\u884c\u8f66\u7f51\u7edc\u666e\u904d\u9762\u4e34\u8ba1\u6570\u6570\u636e\u6781\u5176\u7a00\u758f\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86BikeVAE-GNN\uff0c\u4e00\u4e2a\u53cc\u4efb\u52a1\u6846\u67b6\u3002\u5b83\u5c06\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08\u7ed3\u5408GCN\u3001GAT\u548cGraphSAGE\uff09\u4e0e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u76f8\u7ed3\u5408\u3002\u6df7\u5408GNN\u7528\u4e8e\u5efa\u6a21\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\uff0c\u800cVAE\u5219\u751f\u6210\u5408\u6210\u8282\u70b9\u548c\u8fb9\u4ee5\u4e30\u5bcc\u56fe\u7ed3\u6784\uff0c\u4ece\u800c\u63d0\u9ad8\u4f30\u8ba1\u6027\u80fd\u3002\u8be5\u6a21\u578b\u540c\u65f6\u6267\u884c\u81ea\u884c\u8f66\u4ea4\u901a\u91cf\u56de\u5f52\u548c\u4ea4\u901a\u6c34\u5e73\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u5728\u58a8\u5c14\u672c\uff0899%\u6570\u636e\u7a00\u758f\u5ea6\uff09\u7684OpenStreetMap\u548c\u516c\u5f00\u81ea\u884c\u8f66\u8ba1\u6570\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86BikeVAE-GNN\u3002\u7ed3\u679c\u663e\u793a\uff0cBikeVAE-GNN\u4f18\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u57fa\u7ebfGNN\u6a21\u578b\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a30.82\u8f86/\u5929\uff0c\u51c6\u786e\u7387\u8fbe99%\uff0cF1\u5206\u6570\u8fbe0.99\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86Hybrid-GNN\u548cVAE\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u65b0\u9896\u4e14\u5148\u8fdb\u7684\u65b9\u6cd5\u63a8\u8fdb\u4e86\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u81ea\u884c\u8f66\u4ea4\u901a\u91cf\u4f30\u8ba1\uff0c\u4e3a\u53ef\u6301\u7eed\u81ea\u884c\u8f66\u57fa\u7840\u8bbe\u65bd\u5efa\u8bbe\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.19666", "pdf": "https://arxiv.org/pdf/2507.19666", "abs": "https://arxiv.org/abs/2507.19666", "authors": ["Andrei Vlad Man", "R\u0103zvan-Alexandru Sm\u0103du", "Cristian-George Craciun", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams", "categories": ["cs.CL"], "comment": "49 pages, 52 figures", "summary": "The intersection of AI and legal systems presents a growing need for tools\nthat support legal education, particularly in under-resourced languages such as\nRomanian. In this work, we aim to evaluate the capabilities of Large Language\nModels (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning\nabout Romanian driving law through textual and visual question-answering tasks.\nTo facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising\nRomanian driving test questions, text-based and image-based, alongside\nannotated legal references and human explanations. We implement and assess\nretrieval-augmented generation (RAG) pipelines, dense retrievers, and\nreasoning-optimized models across tasks including Information Retrieval (IR),\nQuestion Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate\nthat domain-specific fine-tuning significantly enhances retrieval performance.\nAt the same time, chain-of-thought prompting and specialized reasoning models\nimprove QA accuracy, surpassing the minimum grades required to pass driving\nexams. However, visual reasoning remains challenging, highlighting the\npotential and the limitations of applying LLMs and VLMs to legal education.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86LLMs\u548cVLMs\u5728\u7f57\u9a6c\u5c3c\u4e9a\u9a7e\u9a76\u6cd5\u5f8b\u6559\u80b2\u4e2d\u7684\u80fd\u529b\uff0c\u5f15\u5165\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6RoD-TAL\u5e76\u4f7f\u7528RAG\u7b49\u65b9\u6cd5\u3002\u7814\u7a76\u8868\u660e\u9886\u57df\u5fae\u8c03\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u89c6\u89c9\u63a8\u7406\u4ecd\u5177\u6311\u6218\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u4e0e\u6cd5\u5f8b\u7cfb\u7edf\u4ea4\u53c9\u9886\u57df\u5bf9\u6cd5\u5f8b\u6559\u80b2\u5de5\u5177\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7406\u89e3\u548c\u63a8\u7406\u7f57\u9a6c\u5c3c\u4e9a\u9a7e\u9a76\u6cd5\u5f8b\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6RoD-TAL\uff0c\u5305\u542b\u7f57\u9a6c\u5c3c\u4e9a\u9a7e\u9a76\u8003\u8bd5\u7684\u6587\u672c\u548c\u56fe\u50cf\u95ee\u9898\uff0c\u4ee5\u53ca\u6cd5\u5f8b\u53c2\u8003\u548c\u4eba\u5de5\u89e3\u91ca\u3002\u5b9e\u65bd\u5e76\u8bc4\u4f30\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u3001\u5bc6\u96c6\u68c0\u7d22\u5668\u548c\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff0c\u6db5\u76d6\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u3001\u95ee\u7b54\uff08QA\uff09\u3001\u89c6\u89c9IR\u548c\u89c6\u89c9QA\u7b49\u4efb\u52a1\u3002\u5b9e\u9a8c\u4e2d\u8fd8\u4f7f\u7528\u4e86\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u3002", "result": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u4e13\u7528\u63a8\u7406\u6a21\u578b\u63d0\u9ad8\u4e86\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86\u901a\u8fc7\u9a7e\u9a76\u8003\u8bd5\u6240\u9700\u7684\u6700\u4f4e\u5206\u6570\u3002\u7136\u800c\uff0c\u89c6\u89c9\u63a8\u7406\u4ecd\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "LLMs\u548cVLMs\u5728\u6cd5\u5f8b\u6559\u80b2\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.20234", "pdf": "https://arxiv.org/pdf/2507.20234", "abs": "https://arxiv.org/abs/2507.20234", "authors": ["Burak Arda Okutan", "Stefan Schmid", "Yvonne-Anne Pignolet"], "title": "Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)", "categories": ["cs.NI", "cs.ET", "cs.SI", "C.2.4"], "comment": "This paper is an extended version of the work presented at the IEEE\n  International Conference on Blockchain and Cryptocurrency (ICBC) 2025", "summary": "Decentralized autonomous organizations (DAOs) rely on governance mechanism\nwithout centralized leadership. This paper presents an empirical study of user\nbehavior in governance for a variety of DAOs, ranging from DeFi to gaming,\nusing the Internet Computer Protocol DAO framework called SNS (Service Nervous\nSystem). To analyse user engagement, we measure participation rates and\nfrequency of proposals submission and voter approval rates. We evaluate\ndecision duration times to determine DAO agility. To investigate dynamic\naspects, we also measure metric shifts in time. We evaluate over 3,000\nproposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected\nDAO have been existing between 6 and 20 months and cover a wide spectrum of use\ncases, treasury sizes, and number of participants. We also compare our results\nfor SNS DAOs with DAOs from other blockchain platforms. While approval rates\nare generally high for all DAOs studied, SNS DAOs show slightly more alignment.\nWe observe that the SNS governance mechanisms and processes in ICP lead to\nhigher activity, lower costs and faster decisions. Most importantly, in\ncontrast to studies which report a decline in participation over time for other\nframeworks, SNS DAOs exhibit sustained or increasing engagement levels over\ntime.", "AI": {"tldr": "\u5bf9ICP\u7684SNS\u6846\u67b6\u4e0bDAO\u7528\u6237\u6cbb\u7406\u884c\u4e3a\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u5176\u5177\u6709\u9ad8\u53c2\u4e0e\u5ea6\u3001\u9ad8\u6548\u7387\u548c\u6301\u7eed\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u81ea\u6cbb\u7ec4\u7ec7\uff08DAO\uff09\u5728\u65e0\u4e2d\u5fc3\u5316\u9886\u5bfc\u4e0b\u7684\u6cbb\u7406\u673a\u5236\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u4e92\u8054\u7f51\u8ba1\u7b97\u673a\u534f\u8bae\uff08ICP\uff09\u7684SNS\u6846\u67b6\u4e0b\u7528\u6237\u884c\u4e3a\u548c\u6cbb\u7406\u6548\u7387\u3002", "method": "\u91c7\u7528\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u6765\u81ea14\u4e2aSNS DAO\u76843000\u591a\u4e2a\u63d0\u6848\uff0c\u65f6\u95f4\u8de8\u5ea620\u4e2a\u6708\u3002\u8861\u91cf\u6307\u6807\u5305\u62ec\u53c2\u4e0e\u7387\u3001\u63d0\u6848\u63d0\u4ea4\u548c\u901a\u8fc7\u7387\u3001\u51b3\u7b56\u65f6\u957f\u4ee5\u53ca\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6307\u6807\u8d8b\u52bf\u3002\u5e76\u4e0e\u5176\u4ed6\u533a\u5757\u94fe\u5e73\u53f0\u4e0a\u7684DAO\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u603b\u4f53\u63d0\u6848\u901a\u8fc7\u7387\u8f83\u9ad8\uff0cSNS DAO\u8868\u73b0\u51fa\u7565\u9ad8\u7684\u5171\u8bc6\u5ea6\u3001\u66f4\u9ad8\u7684\u6d3b\u8dc3\u5ea6\u3001\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u5feb\u7684\u51b3\u7b56\u3002\u4e0e\u5176\u5b83\u6846\u67b6\u4e0b\u53c2\u4e0e\u5ea6\u968f\u65f6\u95f4\u4e0b\u964d\u7684\u8d8b\u52bf\u4e0d\u540c\uff0cSNS DAO\u5c55\u793a\u4e86\u6301\u7eed\u6216\u589e\u957f\u7684\u53c2\u4e0e\u5ea6\u3002", "conclusion": "ICP\u7684SNS\u6cbb\u7406\u673a\u5236\u548c\u6d41\u7a0b\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u6d3b\u52a8\u6027\u3001\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u5feb\u7684\u51b3\u7b56\uff0c\u5e76\u4e14\u6700\u91cd\u8981\u7684\u662f\uff0c\u80fd\u591f\u7ef4\u6301\u6216\u589e\u52a0\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e00\u4e9bDAO\u6846\u67b6\u3002"}}
{"id": "2507.19703", "pdf": "https://arxiv.org/pdf/2507.19703", "abs": "https://arxiv.org/abs/2507.19703", "authors": ["Peter V. Coveney", "Sauro Succi"], "title": "The wall confronting large language models", "categories": ["cs.AI"], "comment": null, "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.", "AI": {"tldr": "\u672c\u7814\u7a76\u6307\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7f29\u653e\u5b9a\u5f8b\u4e25\u91cd\u9650\u5236\u4e86\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6539\u8fdb\uff0c\u4f7f\u5176\u96be\u4ee5\u8fbe\u5230\u79d1\u5b66\u53ef\u9760\u6027\u6807\u51c6\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0cLLMs\u4ece\u9ad8\u65af\u8f93\u5165\u751f\u6210\u975e\u9ad8\u65af\u8f93\u51fa\u7684\u673a\u5236\u53ef\u80fd\u662f\u9519\u8bef\u7d2f\u79ef\u548cAI\u9000\u5316\u7684\u6839\u6e90\u3002\u907f\u514d\u672a\u6765AI\u9000\u5316\u9700\u8981\u66f4\u6df1\u5165\u5730\u7406\u89e3\u95ee\u9898\u7ed3\u6784\u3002", "motivation": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u9ad8\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e3a\u4f55\u5176\u53ef\u9760\u6027\u96be\u4ee5\u6ee1\u8db3\u79d1\u5b66\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u63d0\u51fa\u5176\u4ece\u9ad8\u65af\u8f93\u5165\u751f\u6210\u975e\u9ad8\u65af\u8f93\u51fa\u7684\u673a\u5236\u662f\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3001\u4fe1\u606f\u707e\u96be\u548c\u9000\u5316AI\u884c\u4e3a\u7684\u6f5c\u5728\u539f\u56e0\u3002\u6b64\u5916\uff0c\u8fd8\u8003\u8651\u4e86\u6570\u636e\u96c6\u4e2d\u968f\u89c4\u6a21\u589e\u52a0\u7684\u865a\u5047\u76f8\u5173\u6027\uff08\u7531Calude\u548cLongo\u6307\u51fa\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u51b3\u5b9aLLMs\u6027\u80fd\u7684\u7f29\u653e\u5b9a\u5f8b\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u6539\u8fdb\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u53ef\u9760\u6027\u96be\u4ee5\u8fbe\u5230\u79d1\u5b66\u63a2\u7a76\u6807\u51c6\u3002\u63a8\u6d4b\u5176\u5b66\u4e60\u80fd\u529b\u6765\u6e90\u4e8e\u4ece\u9ad8\u65af\u8f93\u5165\u751f\u6210\u975e\u9ad8\u65af\u8f93\u51fa\u7684\u673a\u5236\uff0c\u8fd9\u53ef\u80fd\u662f\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u9000\u5316AI\u884c\u4e3a\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "LLMs\u7684\u5b66\u4e60\u80fd\u529b\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5185\u5728\u77db\u76fe\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5176\u8d70\u5411\u9000\u5316\u7684\u4eba\u5de5\u667a\u80fd\u8def\u5f84\u3002\u4e3a\u907f\u514d\u8fd9\u79cd\u9000\u5316\uff0c\u672a\u6765\u7684AI\u7814\u7a76\u5fc5\u987b\u66f4\u52a0\u91cd\u89c6\u5bf9\u6240\u7814\u7a76\u95ee\u9898\u7ed3\u6784\u7279\u5f81\u7684\u6d1e\u5bdf\u548c\u7406\u89e3\u3002"}}
{"id": "2507.19621", "pdf": "https://arxiv.org/pdf/2507.19621", "abs": "https://arxiv.org/abs/2507.19621", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond", "categories": ["cs.CV"], "comment": null, "summary": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faExemplar Med-DETR\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5229\u7528\u7c7b\u522b\u7279\u5f02\u6027\u8303\u4f8b\u7279\u5f81\u548c\u8fed\u4ee3\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u7279\u5f81\u8868\u793a\u5dee\u5f02\u548c\u89e3\u5256\u7ed3\u6784\u4e0e\u5f02\u5e38\u7684\u590d\u6742\u5173\u7cfb\u3002\u7279\u522b\u662f\u4e73\u817aX\u7ebf\u56fe\u50cf\u4e2d\uff0c\u81f4\u5bc6\u4e73\u817a\u7ec4\u7ec7\u53ef\u80fd\u63a9\u76d6\u75c5\u7076\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u6709\u6548\u7684\u7c7b\u522b\u7279\u5f02\u6027\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6210\u50cf\u6a21\u6001\u95f4\u7684\u9002\u7528\u6027\u3002", "method": "\u5f15\u5165Exemplar Med-DETR\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u68c0\u6d4b\u5668\uff0c\u53ef\u5b9e\u73b0\u57fa\u4e8e\u7279\u5f81\u7684\u68c0\u6d4b\u3002\u5b83\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5185\u5728\u5730\u63a8\u5bfc\u51fa\u7684\u3001\u76f4\u89c2\u7684\u7c7b\u522b\u7279\u5f02\u6027\u8303\u4f8b\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u6210\u50cf\u6a21\u6001\u7684\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728\u8d8a\u5357\u81f4\u5bc6\u578b\u4e73\u817aX\u7ebf\u56fe\u50cf\u4e0a\uff0c\u80bf\u5757\u68c0\u6d4bmAP\u8fbe0.7\uff0c\u9499\u5316\u68c0\u6d4bmAP\u8fbe0.55\uff0c\u7edd\u5bf9\u63d0\u534716\u4e2a\u767e\u5206\u70b9\u3002\u5bf9\u6765\u81ea\u4e2d\u56fd\u5f02\u5206\u5e03\u961f\u5217\u7684100\u5f20\u4e73\u817aX\u7ebf\u56fe\u50cf\u8fdb\u884c\u653e\u5c04\u79d1\u533b\u751f\u652f\u6301\u7684\u8bc4\u4f30\uff0c\u75c5\u7076\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u4e24\u500d\u3002\u5728\u80f8\u90e8X\u7ebf\u548c\u8840\u7ba1\u9020\u5f71\u4e2d\uff0c\u80bf\u5757\u68c0\u6d4bmAP\u8fbe0.25\uff0c\u72ed\u7a84\u68c0\u6d4bmAP\u8fbe0.37\uff0c\u5206\u522b\u63d0\u53474\u548c7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u6709\u671b\u63a8\u52a8\u533b\u5b66\u6210\u50cf\u9886\u57df\u5f00\u53d1\u51fa\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u6027\u5f3a\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2507.19518", "pdf": "https://arxiv.org/pdf/2507.19518", "abs": "https://arxiv.org/abs/2507.19518", "authors": ["Sangwoo Seo", "Jimin Seo", "Yoonho Lee", "Donghyeon Kim", "Hyejin Shin", "Banghyun Sung", "Chanyoung Park"], "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "ICCAD 2025", "summary": "Subgraph matching plays an important role in electronic design automation\n(EDA) and circuit verification. Traditional rule-based methods have limitations\nin generalizing to arbitrary target circuits. Furthermore, node-to-node\nmatching approaches tend to be computationally inefficient, particularly for\nlarge-scale circuits. Deep learning methods have emerged as a potential\nsolution to address these challenges, but existing models fail to efficiently\ncapture global subgraph embeddings or rely on inefficient matching matrices,\nwhich limits their effectiveness for large circuits. In this paper, we propose\nan efficient graph matching approach that utilizes Graph Neural Networks (GNNs)\nto predict regions of high probability for containing the target circuit.\nSpecifically, we construct various negative samples to enable GNNs to\naccurately learn the presence of target circuits and develop an approach to\ndirectly extracting subgraph embeddings from the entire circuit, which captures\nglobal subgraph information and addresses the inefficiency of applying GNNs to\nall candidate subgraphs. Extensive experiments demonstrate that our approach\nsignificantly outperforms existing methods in terms of time efficiency and\ntarget region prediction, offering a scalable and effective solution for\nsubgraph matching in large-scale circuits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGNN\u7684\u9ad8\u6548\u5b50\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u7535\u8def\u4e2d\u9884\u6d4b\u76ee\u6807\u7535\u8def\u7684\u9ad8\u6982\u7387\u533a\u57df\u3002", "motivation": "\u5b50\u56fe\u5339\u914d\u5728EDA\u548c\u7535\u8def\u9a8c\u8bc1\u4e2d\u5f88\u91cd\u8981\u3002\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u8282\u70b9\u5bf9\u8282\u70b9\u5339\u914d\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u5c24\u5176\u5bf9\u4e8e\u5927\u578b\u7535\u8def\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u672a\u80fd\u9ad8\u6548\u6355\u83b7\u5168\u5c40\u5b50\u56fe\u5d4c\u5165\u6216\u4f9d\u8d56\u4f4e\u6548\u5339\u914d\u77e9\u9635\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5927\u7535\u8def\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u9884\u6d4b\u5305\u542b\u76ee\u6807\u7535\u8def\u7684\u9ad8\u6982\u7387\u533a\u57df\u3002\u6784\u5efa\u591a\u79cd\u8d1f\u6837\u672c\u4f7fGNNs\u5b66\u4e60\u76ee\u6807\u7535\u8def\u7684\u5b58\u5728\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u4ece\u6574\u4e2a\u7535\u8def\u4e2d\u76f4\u63a5\u63d0\u53d6\u5b50\u56fe\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u6355\u83b7\u5168\u5c40\u5b50\u56fe\u4fe1\u606f\u5e76\u89e3\u51b3GNN\u5e94\u7528\u4e8e\u6240\u6709\u5019\u9009\u5b50\u56fe\u7684\u4f4e\u6548\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u76ee\u6807\u533a\u57df\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u7535\u8def\u4e2d\u7684\u5b50\u56fe\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19699", "pdf": "https://arxiv.org/pdf/2507.19699", "abs": "https://arxiv.org/abs/2507.19699", "authors": ["Maitha Alshehhi", "Ahmed Sharshar", "Mohsen Guizani"], "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks", "categories": ["cs.CL"], "comment": "Published in the 3rd International Workshop on Generalizing from\n  Limited Resources in the Open World. Workshop at International Joint\n  Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Although LLMs have attained significant success in high-resource languages,\ntheir capacity in low-resource linguistic environments like Kannada and Arabic\nis not yet fully understood. This work benchmarking the performance of\nmultilingual and monolingual Large Language Models (LLMs) across Arabic,\nEnglish, and Indic languages, with particular emphasis on the effects of model\ncompression strategies such as pruning and quantization. Findings shows\nsignificant performance differences driven by linguistic diversity and resource\navailability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.\nWe find that multilingual versions of the model outperform their\nlanguage-specific counterparts across the board, indicating substantial\ncross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in\nmaintaining model accuracy while promoting efficiency, but aggressive pruning\nsignificantly compromises performance, especially in bigger models. Our\nfindings pinpoint key strategies to construct scalable and fair multilingual\nNLP solutions and underscore the need for interventions to address\nhallucination and generalization errors in the low-resource setting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u548c\u5355\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5361\u7eb3\u8fbe\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\uff09\u4e2d\u7684\u6027\u80fd\u53ca\u5176\u538b\u7f29\u7b56\u7565\uff08\u526a\u679d\u3001\u91cf\u5316\uff09\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u91cf\u5316\u6709\u6548\uff0c\u4f46\u6fc0\u8fdb\u526a\u679d\u4f1a\u635f\u5bb3\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\uff08\u5982\u5361\u7eb3\u8fbe\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\uff09\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8bed\u8a00\u591a\u6837\u6027\u548c\u8d44\u6e90\u53ef\u7528\u6027\u5982\u4f55\u5f71\u54cd\u5176\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u538b\u7f29\u7b56\u7565\uff08\u5982\u526a\u679d\u548c\u91cf\u5316\uff09\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u5bf9BLOOMZ\u3001AceGPT\u3001Jais\u3001LLaMA-2\u3001XGLM\u548cAraGPT2\u7b49\u591a\u79cd\u591a\u8bed\u8a00\u548c\u5355\u8bed\u8a00\u5927\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u3001\u82f1\u8bed\u548c\u5370\u5730\u8bed\u7cfb\u8bed\u8a00\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u4e86\u526a\u679d\u548c\u91cf\u5316\u7b49\u6a21\u578b\u538b\u7f29\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bed\u8a00\u591a\u6837\u6027\u548c\u8d44\u6e90\u53ef\u7528\u6027\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u591a\u8bed\u8a00\u7248\u672c\u6a21\u578b\u666e\u904d\u4f18\u4e8e\u5176\u7279\u5b9a\u8bed\u8a00\u7248\u672c\uff0c\u8868\u660e\u5177\u6709\u663e\u8457\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u4f18\u52bf\uff1b4\u4f4d\u548c8\u4f4d\u91cf\u5316\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u80fd\u6709\u6548\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\uff1b\u4f46\u6fc0\u8fdb\u526a\u679d\u4f1a\u663e\u8457\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u5927\u578b\u6a21\u578b\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u672c\u7814\u7a76\u660e\u786e\u4e86\u6784\u5efa\u53ef\u6269\u5c55\u548c\u516c\u5e73\u7684\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u7b56\u7565\uff0c\u5e76\u5f3a\u8c03\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u9700\u8981\u91c7\u53d6\u5e72\u9884\u63aa\u65bd\u6765\u89e3\u51b3\u6a21\u578b\u7684\u5e7b\u89c9\u548c\u6cdb\u5316\u9519\u8bef\u95ee\u9898\u3002"}}
{"id": "2507.20367", "pdf": "https://arxiv.org/pdf/2507.20367", "abs": "https://arxiv.org/abs/2507.20367", "authors": ["Charitha Madapatha", "Piotr Lechowicz", "Carlos Natalino", "Paolo Monti", "Tommy Svensson"], "title": "Joint Fiber and Free Space Optical Infrastructure Planning for Hybrid Integrated Access and Backhaul Networks", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": "Accepted invited paper for IEEE PIMRC 2025, Istanbul, Turkey", "summary": "Integrated access and backhaul (IAB) is one of the promising techniques for\n5G networks and beyond (6G), in which the same node/hardware is used to provide\nboth backhaul and cellular services in a multi-hop architecture. Due to the\nsensitivity of the backhaul links with high rate/reliability demands, proper\nnetwork planning is needed to ensure the IAB network performs with the desired\nperformance levels. In this paper, we study the effect of infrastructure\nplanning and optimization on the coverage of IAB networks. We concentrate on\nthe cases where the fiber connectivity to the nodes is constrained due to cost.\nThereby, we study the performance gains and energy efficiency in the presence\nof free-space optical (FSO) communication links. Our results indicate hybrid\nfiber/FSO deployments offer substantial cost savings compared to fully fibered\nnetworks, suggesting a beneficial trade-off for strategic link deployment while\nimproving the service coverage probability. As we show, with proper network\nplanning, the service coverage, energy efficiency, and cost efficiency can be\nimproved.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf95G/6G\u7efc\u5408\u63a5\u5165\u56de\u4f20\uff08IAB\uff09\u7f51\u7edc\u5728\u5149\u7ea4\u90e8\u7f72\u53d7\u6210\u672c\u9650\u5236\u4e0b\u7684\u89c4\u5212\u95ee\u9898\uff0c\u63a2\u7d22\u4e86\u6df7\u5408\u5149\u7ea4/\u81ea\u7531\u7a7a\u95f4\u5149\uff08FSO\uff09\u90e8\u7f72\u5bf9\u7f51\u7edc\u8986\u76d6\u3001\u80fd\u6548\u548c\u6210\u672c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u670d\u52a1\u8986\u76d6\u3002", "motivation": "\u7efc\u5408\u63a5\u5165\u56de\u4f20\uff08IAB\uff09\u662f5G\u53ca\u672a\u67656G\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u56de\u4f20\u94fe\u8def\u5bf9\u901f\u7387\u548c\u53ef\u9760\u6027\u8981\u6c42\u9ad8\uff0c\u9700\u8981\u6070\u5f53\u7684\u7f51\u7edc\u89c4\u5212\u6765\u4fdd\u8bc1\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u5149\u7ea4\u8fde\u63a5\u53d7\u6210\u672c\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u5bfb\u627e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u4f18\u5316\u5bf9IAB\u7f51\u7edc\u8986\u76d6\u7684\u5f71\u54cd\u3002\u5177\u4f53\u5730\uff0c\u5728\u8282\u70b9\u5149\u7ea4\u8fde\u63a5\u53d7\u6210\u672c\u9650\u5236\u7684\u573a\u666f\u4e0b\uff0c\u5206\u6790\u4e86\u5f15\u5165\u81ea\u7531\u7a7a\u95f4\u5149\uff08FSO\uff09\u901a\u4fe1\u94fe\u8def\u540e\uff0c\u7f51\u7edc\u6027\u80fd\u589e\u76ca\u548c\u80fd\u6548\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408\u5149\u7ea4/FSO\u90e8\u7f72\u76f8\u6bd4\u5168\u5149\u7ea4\u7f51\u7edc\u80fd\u663e\u8457\u8282\u7701\u6210\u672c\uff0c\u5e76\u80fd\u5728\u6218\u7565\u6027\u94fe\u8def\u90e8\u7f72\u4e2d\u63d0\u4f9b\u6709\u5229\u7684\u6743\u8861\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u670d\u52a1\u8986\u76d6\u6982\u7387\u3002\u901a\u8fc7\u9002\u5f53\u7684\u7f51\u7edc\u89c4\u5212\uff0c\u670d\u52a1\u8986\u76d6\u3001\u80fd\u6548\u548c\u6210\u672c\u6548\u7387\u5747\u53ef\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u7f51\u7edc\u89c4\u5212\uff0c\u7ed3\u5408\u6df7\u5408\u5149\u7ea4/FSO\u90e8\u7f72\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347IAB\u7f51\u7edc\u7684\u670d\u52a1\u8986\u76d6\u3001\u80fd\u6548\u548c\u6210\u672c\u6548\u7387\uff0c\u5c24\u5176\u5728\u5149\u7ea4\u8fde\u63a5\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff0c\u8fd9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u76ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19725", "pdf": "https://arxiv.org/pdf/2507.19725", "abs": "https://arxiv.org/abs/2507.19725", "authors": ["Leonardo Villalobos-Arias", "Grant Forbes", "Jianxun Wang", "David L Roberts", "Arnav Jhala"], "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors", "categories": ["cs.AI"], "comment": "11 pages, 7 figures, 3 tables", "summary": "Games are challenging for Reinforcement Learning~(RL) agents due to their\nreward-sparsity, as rewards are only obtainable after long sequences of\ndeliberate actions. Intrinsic Motivation~(IM) methods -- which introduce\nexploration rewards -- are an effective solution to reward-sparsity. However,\nIM also causes an issue known as `reward hacking' where the agent optimizes for\nthe new reward at the expense of properly playing the game. The larger problem\nis that reward hacking itself is largely unknown; there is no answer to\nwhether, and to what extent, IM rewards change the behavior of RL agents. This\nstudy takes a first step by empirically evaluating the impact on behavior of\nthree IM techniques on the MiniGrid game-like environment. We compare these IM\nmodels with Generalized Reward Matching~(GRM), a method that can be used with\nany intrinsic reward function to guarantee optimality. Our results suggest that\nIM causes noticeable change by increasing the initial rewards, but also\naltering the way the agent plays; and that GRM mitigated reward hacking in some\nscenarios.", "AI": {"tldr": "\u9488\u5bf9RL\u7a00\u758f\u5956\u52b1\u4e0e\u5185\u751f\u6fc0\u52b1(IM)\u5bfc\u81f4\u7684\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5728MiniGrid\u73af\u5883\u5b9e\u8bc1\u8bc4\u4f30IM\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u5e7f\u4e49\u5956\u52b1\u5339\u914d(GRM)\u53ef\u90e8\u5206\u7f13\u89e3\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60(RL)\u5728\u7a00\u758f\u5956\u52b1\u6e38\u620f\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u800c\u5185\u751f\u6fc0\u52b1(IM)\u662f\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0cIM\u4e5f\u5e26\u6765\u4e86\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u95ee\u9898\uff0c\u5373\u667a\u80fd\u4f53\u8fc7\u5ea6\u4f18\u5316\u5185\u751f\u5956\u52b1\u800c\u504f\u79bb\u6e38\u620f\u76ee\u6807\u3002\u76ee\u524d\u5bf9\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u7684\u7a0b\u5ea6\u4e0e\u5177\u4f53\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6df1\u5165\u7406\u89e3IM\u5bf9RL\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5177\u4f53\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u5728MiniGrid\u6e38\u620f\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5185\u751f\u6fc0\u52b1(IM)\u6280\u672f\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u5c06\u8fd9\u4e9bIM\u6a21\u578b\u4e0e\u5e7f\u4e49\u5956\u52b1\u5339\u914d(GRM)\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0cGRM\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u4f18\u5316\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5185\u751f\u6fc0\u52b1(IM)\u663e\u8457\u6539\u53d8\u4e86\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u4e0d\u4ec5\u589e\u52a0\u4e86\u521d\u59cb\u5956\u52b1\uff0c\u8fd8\u6539\u53d8\u4e86\u5176\u51b3\u7b56\u4e0e\u6e38\u620f\u7b56\u7565\u3002\u6b64\u5916\uff0c\u5e7f\u4e49\u5956\u52b1\u5339\u914d(GRM)\u5728\u90e8\u5206\u573a\u666f\u4e0b\u6709\u6548\u7f13\u89e3\u4e86\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u95ee\u9898\u3002", "conclusion": "\u5185\u751f\u6fc0\u52b1(IM)\u5bf9\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8fdc\u8d85\u4ec5\u4ec5\u589e\u52a0\u5956\u52b1\u3002\u5e7f\u4e49\u5956\u52b1\u5339\u914d(GRM)\u5728\u7f13\u89e3\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u5e94\u5bf9\u201c\u5956\u52b1\u6b3a\u9a97\u201d\u73b0\u8c61\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.19626", "pdf": "https://arxiv.org/pdf/2507.19626", "abs": "https://arxiv.org/abs/2507.19626", "authors": ["Adrian Celaya", "Tucker Netherton", "Dawid Schellingerhout", "Caroline Chung", "Beatrice Riviere", "David Fuentes"], "title": "Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation continues to advance rapidly, yet rigorous\ncomparison between methods remains challenging due to a lack of standardized\nand customizable tooling. In this work, we present the current state of the\nMedical Imaging Segmentation Toolkit (MIST), with a particular focus on its\nflexible and modular postprocessing framework designed for the BraTS 2025 pre-\nand post-treatment glioma segmentation challenge. Since its debut in the 2024\nBraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing\nmodule has been significantly extended to support a wide range of transforms,\nincluding removal or replacement of small objects, extraction of the largest\nconnected components, and morphological operations such as hole filling and\nclosing. These transforms can be composed into user-defined strategies,\nenabling fine-grained control over the final segmentation output. We evaluate\nthree such strategies - ranging from simple small-object removal to more\ncomplex, class-specific pipelines - and rank their performance using the BraTS\nranking protocol. Our results highlight how MIST facilitates rapid\nexperimentation and targeted refinement, ultimately producing high-quality\nsegmentations for the BraTS 2025 challenge. MIST remains open source and\nextensible, supporting reproducible and scalable research in medical image\nsegmentation.", "AI": {"tldr": "MIST\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u540e\u5904\u7406\u5de5\u5177\u5305\uff0c\u65e8\u5728\u4e3aBraTS 2025\u6311\u6218\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u53ef\u5b9a\u5236\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u53ef\u5b9a\u5236\u7684\u5de5\u5177\uff0c\u5bfc\u81f4\u96be\u4ee5\u8fdb\u884c\u4e25\u683c\u6bd4\u8f83\u3002", "method": "\u4ecb\u7ecd\u5e76\u6269\u5c55\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5de5\u5177\u5305\uff08MIST\uff09\u7684\u540e\u5904\u7406\u6846\u67b6\u3002\u8be5\u6846\u67b6\u652f\u6301\u591a\u79cd\u53ef\u7ec4\u5408\u7684\u53d8\u6362\u64cd\u4f5c\uff08\u5982\u5c0f\u5bf9\u8c61\u79fb\u9664\u3001\u6700\u5927\u8fde\u901a\u5206\u91cf\u63d0\u53d6\u3001\u5f62\u6001\u5b66\u64cd\u4f5c\uff09\uff0c\u53ef\u7ec4\u6210\u7528\u6237\u5b9a\u4e49\u7b56\u7565\u3002\u901a\u8fc7BraTS\u6392\u540d\u534f\u8bae\u8bc4\u4f30\u4e86\u4e09\u79cd\u6b64\u7c7b\u7b56\u7565\u3002", "result": "MIST\u4fc3\u8fdb\u4e86\u5feb\u901f\u5b9e\u9a8c\u548c\u76ee\u6807\u6027\u4f18\u5316\uff0c\u6700\u7ec8\u4e3aBraTS 2025\u6311\u6218\u8d5b\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u7ed3\u679c\u3002", "conclusion": "MIST\u662f\u5f00\u6e90\u4e14\u53ef\u6269\u5c55\u7684\uff0c\u652f\u6301\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7814\u7a76\uff0c\u4e3a\u89e3\u51b3\u5de5\u5177\u7f3a\u4e4f\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.19519", "pdf": "https://arxiv.org/pdf/2507.19519", "abs": "https://arxiv.org/abs/2507.19519", "authors": ["J. Poole", "P. Gardner", "A. J. Hughes", "N. Dervilis", "R. S. Mills", "T. A. Dardeno", "K. Worden"], "title": "Physics-informed transfer learning for SHM via feature selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data used for training structural health monitoring (SHM) systems are\nexpensive and often impractical to obtain, particularly labelled data.\nPopulation-based SHM presents a potential solution to this issue by considering\nthe available data across a population of structures. However, differences\nbetween structures will mean the training and testing distributions will\ndiffer; thus, conventional machine learning methods cannot be expected to\ngeneralise between structures. To address this issue, transfer learning (TL),\ncan be used to leverage information across related domains. An important\nconsideration is that the lack of labels in the target domain limits data-based\nmetrics to quantifying the discrepancy between the marginal distributions.\nThus, a prerequisite for the application of typical unsupervised TL methods is\nto identify suitable source structures (domains), and a set of features, for\nwhich the conditional distributions are related to the target structure.\nGenerally, the selection of domains and features is reliant on domain\nexpertise; however, for complex mechanisms, such as the influence of damage on\nthe dynamic response of a structure, this task is not trivial. In this paper,\nknowledge of physics is leveraged to select more similar features, the modal\nassurance criterion (MAC) is used to quantify the correspondence between the\nmodes of healthy structures. The MAC is shown to have high correspondence with\na supervised metric that measures joint-distribution similarity, which is the\nprimary indicator of whether a classifier will generalise between domains. The\nMAC is proposed as a measure for selecting a set of features that behave\nconsistently across domains when subjected to damage, i.e. features with\ninvariance in the conditional distributions. This approach is demonstrated on\nnumerical and experimental case studies to verify its effectiveness in various\napplications.", "AI": {"tldr": "\u9488\u5bf9\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b(SHM)\u4e2d\u6570\u636e\u83b7\u53d6\u56f0\u96be\u548c\u6a21\u578b\u6cdb\u5316\u5dee\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u5e76\u5229\u7528\u6a21\u6001\u4fdd\u8bc1\u51c6\u5219(MAC)\u6765\u9009\u62e9\u9002\u5408\u8fc1\u79fb\u5b66\u4e60\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8SHM\u7cfb\u7edf\u5728\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6570\u503c\u548c\u5b9e\u9a8c\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b(SHM)\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u6602\u8d35\u4e14\u5e26\u6807\u7b7e\u6570\u636e\u66f4\u96be\u83b7\u53d6\u3002\u7fa4\u4f53SHM\u867d\u53ef\u5229\u7528\u591a\u7ed3\u6784\u6570\u636e\uff0c\u4f46\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u3002\u8fc1\u79fb\u5b66\u4e60\u6709\u671b\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u5173\u952e\u5728\u4e8e\u5728\u76ee\u6807\u57df\u7f3a\u4e4f\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u9009\u62e9\u5408\u9002\u7684\u6e90\u7ed3\u6784\u548c\u7279\u5f81\uff0c\u4ee5\u786e\u4fdd\u6761\u4ef6\u5206\u5e03\u7684\u5173\u8054\u6027\u3002\u6b64\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u5bf9\u4e8e\u590d\u6742\u7684\u635f\u4f24\u673a\u5236\uff0c\u8be5\u4efb\u52a1\u975e\u6613\u4e8b\u3002", "method": "\u672c\u6587\u5229\u7528\u7269\u7406\u77e5\u8bc6\u6765\u6307\u5bfc\u7279\u5f81\u9009\u62e9\uff0c\u5177\u4f53\u91c7\u7528\u6a21\u6001\u4fdd\u8bc1\u51c6\u5219\uff08MAC\uff09\u6765\u91cf\u5316\u5065\u5eb7\u7ed3\u6784\u6a21\u6001\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002MAC\u88ab\u7528\u4f5c\u4e00\u79cd\u6307\u6807\uff0c\u4ee5\u8bc6\u522b\u90a3\u4e9b\u5728\u7ed3\u6784\u53d7\u635f\u65f6\u80fd\u8de8\u9886\u57df\u4fdd\u6301\u884c\u4e3a\u4e00\u81f4\u6027\uff08\u5373\u6761\u4ef6\u5206\u5e03\u4e0d\u53d8\u6027\uff09\u7684\u7279\u5f81\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cMAC\u4e0e\u8861\u91cf\u8054\u5408\u5206\u5e03\u76f8\u4f3c\u5ea6\u7684\u6709\u76d1\u7763\u6307\u6807\uff08\u8be5\u6307\u6807\u662f\u5206\u7c7b\u5668\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\uff09\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u5df2\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9009\u62e9\u5177\u6709\u6761\u4ef6\u5206\u5e03\u4e0d\u53d8\u6027\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u8fc1\u79fb\u5b66\u4e60\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u9886\u57df\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u7ed3\u6784\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.19710", "pdf": "https://arxiv.org/pdf/2507.19710", "abs": "https://arxiv.org/abs/2507.19710", "authors": ["Ronak Upasham", "Tathagata Dey", "Pushpak Bhattacharyya"], "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs", "categories": ["cs.CL"], "comment": null, "summary": "In Table-to-Text (T2T) generation, existing approaches predominantly focus on\nproviding objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond\nraw numerical data, remains underexplored. To address this, we introduce a\nnovel pipeline that leverages intermediate representations to generate both\nobjective and subjective text from tables. Our three-stage pipeline consists\nof: 1) extraction of Resource Description Framework (RDF) triples, 2)\naggregation of text into coherent narratives, and 3) infusion of subjectivity\nto enrich the generated text. By incorporating RDFs, our approach enhances\nfactual accuracy while maintaining interpretability. Unlike large language\nmodels (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs\nsmaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5\nand outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our\napproach through quantitative and qualitative analyses, demonstrating its\neffectiveness in balancing factual accuracy with subjective interpretation. To\nthe best of our knowledge, this is the first work to propose a structured\npipeline for T2T generation that integrates intermediate representations to\nenhance both factual correctness and subjectivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u7ba1\u9053\uff0c\u5229\u7528RDF\u4e2d\u95f4\u8868\u793a\u4ece\u8868\u683c\u751f\u6210\u517c\u5177\u5ba2\u89c2\u6027\u548c\u4e3b\u89c2\u6027\u7684\u6587\u672c\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5ab2\u7f8e\uff0c\u4f46\u4f7f\u7528\u66f4\u5c0f\u7684T5\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u5230\u6587\u672c\uff08T2T\uff09\u751f\u6210\u4e3b\u8981\u4fa7\u91cd\u4e8e\u63d0\u4f9b\u8868\u683c\u6570\u636e\u7684\u5ba2\u89c2\u63cf\u8ff0\uff0c\u4f46\u5bf9\u7ed3\u5408\u4e3b\u89c2\u89e3\u91ca\u7684\u6587\u672c\u751f\u6210\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a1) \u63d0\u53d6\u8d44\u6e90\u63cf\u8ff0\u6846\u67b6\uff08RDF\uff09\u4e09\u5143\u7ec4\uff1b2) \u5c06\u6587\u672c\u805a\u5408\u6210\u8fde\u8d2f\u7684\u53d9\u8ff0\uff1b3) \u6ce8\u5165\u4e3b\u89c2\u6027\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e2d\u95f4\u8868\u793a\uff08RDF\uff09\u589e\u5f3a\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u91c7\u7528\u5c0f\u578b\u5fae\u8c03T5\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f7f\u7528\u8f83\u5c0fT5\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u53ef\u4e0eGPT-3.5\u5ab2\u7f8e\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8eMistral-7B\u548cLlama-2\u3002\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e73\u8861\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e3b\u89c2\u89e3\u91ca\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u7684T2T\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u4e2d\u95f4\u8868\u793a\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6587\u672c\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u4e3b\u89c2\u6027\u3002"}}
{"id": "2507.20438", "pdf": "https://arxiv.org/pdf/2507.20438", "abs": "https://arxiv.org/abs/2507.20438", "authors": ["Rostand A. K. Fezeu", "Jason Carpenter", "Rushikesh Zende", "Sree Ganesh Lalitaditya Divakarla", "Nitin Varyani", "Faaiq Bilal", "Steven Sleder", "Nanditha Naik", "Duncan Joly", "Eman Ramadan", "Ajay Kumar Gurumadaiah", "Zhi-Li Zhang"], "title": "Teleoperating Autonomous Vehicles over Commercial 5G Networks: Are We There Yet?", "categories": ["cs.NI", "cs.OH", "C.2.0"], "comment": "17 pages", "summary": "Remote driving, or teleoperating Autonomous Vehicles (AVs), is a key\napplication that emerging 5G networks aim to support. In this paper, we conduct\na systematic feasibility study of AV teleoperation over commercial 5G networks\nfrom both cross-layer and end-to-end (E2E) perspectives. Given the critical\nimportance of timely delivery of sensor data, such as camera and LiDAR data,\nfor AV teleoperation, we focus in particular on the performance of uplink\nsensor data delivery. We analyze the impacts of Physical Layer (PHY layer) 5G\nradio network factors, including channel conditions, radio resource allocation,\nand Handovers (HOs), on E2E latency performance. We also examine the impacts of\n5G networks on the performance of upper-layer protocols and E2E application\nQuality-of-Experience (QoE) adaptation mechanisms used for real-time sensor\ndata delivery, such as Real-Time Streaming Protocol (RTSP) and Web Real Time\nCommunication (WebRTC). Our study reveals the challenges posed by today's 5G\nnetworks and the limitations of existing sensor data streaming mechanisms. The\ninsights gained will help inform the co-design of future-generation wireless\nnetworks, edge cloud systems, and applications to overcome the low-latency\nbarriers in AV teleoperation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u5546\u75285G\u7f51\u7edc\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fdc\u7a0b\u64cd\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u5173\u6ce8\u4e0a\u884c\u4f20\u611f\u5668\u6570\u636e\u4f20\u8f93\uff0c\u63ed\u793a\u4e86\u5f53\u524d5G\u7684\u6311\u6218\u548c\u73b0\u6709\u6570\u636e\u6d41\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6d1e\u5bdf\u3002", "motivation": "5G\u7f51\u7edc\u65e8\u5728\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fdc\u7a0b\u64cd\u4f5c\uff08AVs teleoperation\uff09\uff0c\u4f46\u5176\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u4f20\u611f\u5668\u6570\u636e\uff08\u5982\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff09\u7684\u53ca\u65f6\u4f20\u8f93\u6027\u80fd\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u7814\u7a76\u91c7\u7528\u8de8\u5c42\u548c\u7aef\u5230\u7aef\uff08E2E\uff09\u89c6\u89d2\uff0c\u5206\u6790\u7269\u7406\u5c42\uff08\u5982\u4fe1\u9053\u6761\u4ef6\u3001\u65e0\u7ebf\u8d44\u6e90\u5206\u914d\u3001\u5207\u6362\uff09\u5bf9E2E\u5ef6\u8fdf\u7684\u5f71\u54cd\uff0c\u5e76\u68c0\u67e55G\u7f51\u7edc\u5bf9\u4e0a\u5c42\u534f\u8bae\uff08\u5982RTSP\u3001WebRTC\uff09\u548cE2E\u5e94\u7528\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u81ea\u9002\u5e94\u673a\u5236\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0a\u884c\u4f20\u611f\u5668\u6570\u636e\u4f20\u8f93\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d5G\u7f51\u7edc\u5728AV\u8fdc\u7a0b\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u4f20\u611f\u5668\u6570\u636e\u6d41\u4f20\u8f93\u673a\u5236\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u83b7\u5f97\u7684\u6d1e\u5bdf\u5c06\u6709\u52a9\u4e8e\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u3001\u8fb9\u7f18\u4e91\u7cfb\u7edf\u548c\u5e94\u7528\u8fdb\u884c\u534f\u540c\u8bbe\u8ba1\uff0c\u4ee5\u514b\u670d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u969c\u788d\u3002"}}
{"id": "2507.19726", "pdf": "https://arxiv.org/pdf/2507.19726", "abs": "https://arxiv.org/abs/2507.19726", "authors": ["Yuzhang Xie", "Xu Han", "Ran Xu", "Xiao Hu", "Jiaying Lu", "Carl Yang"], "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare", "categories": ["cs.AI", "cs.LG"], "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025), Main Tracks, Research Track, Oral", "summary": "Knowledge graphs (KGs) are important products of the semantic web, which are\nwidely used in various application domains. Healthcare is one of such domains\nwhere KGs are intensively used, due to the high requirement for knowledge\naccuracy and interconnected nature of healthcare data. However, KGs storing\ngeneral factual information often lack the ability to account for important\ncontexts of the knowledge such as the status of specific patients, which are\ncrucial in precision healthcare. Meanwhile, electronic health records (EHRs)\nprovide rich personal data, including various diagnoses and medications, which\nprovide natural contexts for general KGs. In this paper, we propose HypKG, a\nframework that integrates patient information from EHRs into KGs to generate\ncontextualized knowledge representations for accurate healthcare predictions.\nUsing advanced entity-linking techniques, we connect relevant knowledge from\ngeneral KGs with patient information from EHRs, and then utilize a hypergraph\nmodel to \"contextualize\" the knowledge with the patient information. Finally,\nwe employ hypergraph transformers guided by downstream prediction tasks to\njointly learn proper contextualized representations for both KGs and patients,\nfully leveraging existing knowledge in KGs and patient contexts in EHRs. In\nexperiments using a large biomedical KG and two real-world EHR datasets, HypKG\ndemonstrates significant improvements in healthcare prediction tasks across\nmultiple evaluation metrics. Additionally, by integrating external contexts,\nHypKG can learn to adjust the representations of entities and relations in KG,\npotentially improving the quality and real-world utility of knowledge.", "AI": {"tldr": "\u63d0\u51faHypKG\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u56fe\u6a21\u578b\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u60a3\u8005\u4fe1\u606f\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u533b\u7597\u9884\u6d4b\u3002", "motivation": "\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u7f3a\u4e4f\u60a3\u8005\u7279\u5b9a\u4e0a\u4e0b\u6587\uff08\u5982\u60a3\u8005\u72b6\u6001\uff09\uff0c\u800c\u8fd9\u5bf9\u4e8e\u7cbe\u51c6\u533b\u7597\u81f3\u5173\u91cd\u8981\u3002\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u5305\u542b\u4e30\u5bcc\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u53ef\u4ee5\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u8fd9\u79cd\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faHypKG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u5c06\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\u4e0e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u60a3\u8005\u4fe1\u606f\u8fde\u63a5\u8d77\u6765\u3002\u63a5\u7740\uff0c\u5229\u7528\u8d85\u56fe\u6a21\u578b\u5c06\u77e5\u8bc6\u4e0e\u60a3\u8005\u4fe1\u606f\u8fdb\u884c\u4e0a\u4e0b\u6587\u5173\u8054\u3002\u6700\u540e\uff0c\u91c7\u7528\u7531\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u5f15\u5bfc\u7684\u8d85\u56feTransformer\uff0c\u8054\u5408\u5b66\u4e60\u77e5\u8bc6\u56fe\u8c31\u548c\u60a3\u8005\u7684\u8bed\u5883\u5316\u8868\u793a\u3002", "result": "\u5728\u5927\u578b\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHypKG\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u4e0a\u4e0b\u6587\uff0cHypKG\u80fd\u591f\u8c03\u6574\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5b9e\u4f53\u548c\u5173\u7cfb\u7684\u8868\u793a\uff0c\u4ece\u800c\u53ef\u80fd\u63d0\u9ad8\u77e5\u8bc6\u7684\u8d28\u91cf\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "HypKG\u6210\u529f\u5730\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u60a3\u8005\u4e0a\u4e0b\u6587\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u751f\u6210\u4e86\u66f4\u51c6\u786e\u548c\u5b9e\u7528\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u7597\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u53ef\u80fd\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u672c\u8eab\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.19673", "pdf": "https://arxiv.org/pdf/2507.19673", "abs": "https://arxiv.org/abs/2507.19673", "authors": ["Babak Taati", "Muhammad Muzammil", "Yasamin Zarghami", "Abhishek Moturu", "Airhossein Kazerouni", "Hailey Reimer", "Alex Mihailidis", "Thomas Hadjistavropoulos"], "title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, submitted to IEEE JBHI", "summary": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP", "AI": {"tldr": "SynPAIN\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8001\u5e74\u4eba\u75bc\u75db\u68c0\u6d4b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u80fd\u5e2e\u52a9\u8bc6\u522b\u53ca\u7f13\u89e3\u7b97\u6cd5\u504f\u5dee\uff0c\u63d0\u9ad8\u75bc\u75db\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5bf9\u4e8e\u96be\u4ee5\u6c9f\u901a\u7684\u60a3\u8005\uff08\u5982\u75f4\u5446\u8001\u5e74\u4eba\uff09\u8fdb\u884c\u51c6\u786e\u7684\u75bc\u75db\u8bc4\u4f30\u662f\u4e00\u9879\u5173\u952e\u7684\u533b\u7597\u6311\u6218\u3002\u867d\u7136\u81ea\u52a8\u5316\u75bc\u75db\u68c0\u6d4b\u7cfb\u7edf\u6709\u5e2e\u52a9\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u79cd\u65cf/\u6c11\u65cf\u591a\u6837\u6027\u6709\u9650\u3001\u9690\u79c1\u9650\u5236\u4ee5\u53ca\u8001\u5e74\u4eba\u6837\u672c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u8001\u5e74\u4eba\u6b63\u662f\u4e34\u5e8a\u90e8\u7f72\u7684\u4e3b\u8981\u76ee\u6807\u4eba\u7fa4\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86SynPAIN\uff0c\u4e00\u4e2a\u5305\u542b10,710\u5f20\u9762\u90e8\u8868\u60c5\u56fe\u50cf\uff085,355\u5bf9\u4e2d\u6027/\u8868\u73b0\u5bf9\uff09\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u8986\u76d6\u4e86\u4e94\u79cd\u6c11\u65cf/\u79cd\u65cf\u3001\u4e24\u4e2a\u5e74\u9f84\u7ec4\uff08\u5e74\u8f7b\uff1a20-35\u5c81\uff0c\u8001\u5e74\uff1a75\u5c81\u4ee5\u4e0a\uff09\u548c\u4e24\u79cd\u6027\u522b\u3002\u901a\u8fc7\u4f7f\u7528\u5546\u4e1a\u751f\u6210\u5f0fAI\u5de5\u5177\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u8861\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u75bc\u75db\u8868\u60c5\u5408\u6210\u8eab\u4efd\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u75bc\u75db\u8868\u60c5\u8868\u73b0\u51fa\u9884\u671f\u7684\u75bc\u75db\u6a21\u5f0f\uff0c\u4f7f\u7528\u4e34\u5e8a\u9a8c\u8bc1\u7684\u75bc\u75db\u8bc4\u4f30\u5de5\u5177\uff0c\u5176\u5f97\u5206\u663e\u8457\u9ad8\u4e8e\u4e2d\u6027\u53ca\u975e\u75bc\u75db\u8868\u60c5\u3002\u5b9e\u9a8c\u8bc1\u660eSynPAIN\u6709\u52a9\u4e8e\u8bc6\u522b\u73b0\u6709\u75bc\u75db\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u7b97\u6cd5\u504f\u5dee\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u4e4b\u95f4\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u5e74\u9f84\u5339\u914d\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u53ef\u5c06\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u7684\u75bc\u75db\u68c0\u6d4b\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad87.0%\u3002", "conclusion": "SynPAIN\u901a\u8fc7\u63d0\u4f9b\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u8001\u5e74\u4eba\u75bc\u75db\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u75bc\u75db\u8bc4\u4f30\u7814\u7a76\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002\u540c\u65f6\uff0c\u5b83\u4e5f\u4e3a\u6d4b\u91cf\u548c\u7f13\u89e3\u7b97\u6cd5\u504f\u5dee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u3002"}}
{"id": "2507.19520", "pdf": "https://arxiv.org/pdf/2507.19520", "abs": "https://arxiv.org/abs/2507.19520", "authors": ["Ethan Lo", "Dan C. Lo"], "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.AI"], "comment": null, "summary": "With manual searching processes, the rate at which scientists and astronomers\ndiscover exoplanets is slow because of inefficiencies that require an extensive\ntime of laborious inspections. In fact, as of now there have been about only\n5,000 confirmed exoplanets since the late 1900s. Recently, machine learning\n(ML) has proven to be extremely valuable and efficient in various fields,\ncapable of processing massive amounts of data in addition to increasing its\naccuracy by learning. Though ML models for discovering exoplanets owned by\nlarge corporations (e.g. NASA) exist already, they largely depend on complex\nalgorithms and supercomputers. In an effort to reduce such complexities, in\nthis paper, we report the results and potential benefits of various, well-known\nML models in the discovery and validation of extrasolar planets. The ML models\nthat are examined in this study include logistic regression, k-nearest\nneighbors, and random forest. The dataset on which the models train and predict\nis acquired from NASA's Kepler space telescope. The initial results show\npromising scores for each model. However, potential biases and dataset\nimbalances necessitate the use of data augmentation techniques to further\nensure fairer predictions and improved generalization. This study concludes\nthat, in the context of searching for exoplanets, data augmentation techniques\nsignificantly improve the recall and precision, while the accuracy varies for\neach model.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u3001\u968f\u673a\u68ee\u6797\uff09\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u7cfb\u5916\u884c\u661f\u7684\u53d1\u73b0\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u65b9\u9762\u3002", "motivation": "\u7cfb\u5916\u884c\u661f\u7684\u624b\u52a8\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u5927\u578b\u673a\u6784\uff08\u5982NASA\uff09\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fc7\u4e8e\u590d\u6742\u5e76\u4f9d\u8d56\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u66f4\u7b80\u5355\u3001\u66f4\u5e7f\u4e3a\u4eba\u77e5\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u964d\u4f4e\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u7cfb\u5916\u884c\u661f\u53d1\u73b0\u548c\u9a8c\u8bc1\u7684\u6548\u7387\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u548c\u968f\u673a\u68ee\u6797\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5728NASA\u5f00\u666e\u52d2\u7a7a\u95f4\u671b\u8fdc\u955c\u83b7\u53d6\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\u3002\u4e3a\u89e3\u51b3\u6f5c\u5728\u504f\u5dee\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7814\u7a76\u91c7\u7528\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5404\u6a21\u578b\u5747\u8868\u73b0\u826f\u597d\u3002\u5e94\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u540e\uff0c\u7cfb\u5916\u884c\u661f\u641c\u7d22\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4f46\u51c6\u786e\u7387\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u5728\u7cfb\u5916\u884c\u661f\u641c\u7d22\u4e2d\uff0c\u6570\u636e\u589e\u5f3a\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\uff0c\u5c3d\u7ba1\u4e0d\u540c\u6a21\u578b\u7684\u51c6\u786e\u7387\u6709\u6240\u4e0d\u540c\u3002"}}
{"id": "2507.19741", "pdf": "https://arxiv.org/pdf/2507.19741", "abs": "https://arxiv.org/abs/2507.19741", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "title": "Basic Reading Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u7840\u9605\u8bfb\u84b8\u998f\uff08BRD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u5c0f\u6a21\u578b\u6a21\u4eff\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u57fa\u7840\u9605\u8bfb\u884c\u4e3a\uff0c\u4f7f\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u8d8a\u6216\u5ab2\u7f8e\u592720\u500d\u4ee5\u4e0a\u7684LLMs\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\uff08\u5982\u77e5\u8bc6\u84b8\u998f\u6216\u4efb\u52a1\u84b8\u998f\uff09\u5ffd\u89c6\u4e86\u5728\u4e0e\u4e0b\u6e38\u4efb\u52a1\u65e0\u5173\u7684\u901a\u7528\u6587\u672c\u4e0a\u5bf9\u5c0f\u6a21\u578b\u8fdb\u884c\u201c\u57fa\u7840\u9605\u8bfb\u6559\u80b2\u201d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u57fa\u7840\u9605\u8bfb\u84b8\u998f\uff08BRD\uff09\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u6559\u80b2\u5c0f\u578b\u6a21\u578b\u6a21\u4eff\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u57fa\u672c\u9605\u8bfb\u884c\u4e3a\uff0c\u4f8b\u5982\u5728\u6bcf\u4e2a\u53e5\u5b50\u4e0a\u8fdb\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u95ee\u9898\u63d0\u51fa\u548c\u56de\u7b54\u3002", "result": "\u7ecf\u8fc7BRD\u8bad\u7ec3\u540e\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u548cBIG-bench\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u8d85\u8d8a\u6216\u5ab2\u7f8e\u4f53\u91cf\u592720\u500d\u4ee5\u4e0a\u7684LLMs\u3002\u5206\u6790\u8868\u660e\uff0cBRD\u80fd\u6709\u6548\u5f71\u54cd\u5c0f\u578b\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\uff0c\u4e14\u4e0e\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u6216\u4efb\u52a1\u84b8\u998f\u65b9\u6cd5\u5177\u6709\u6b63\u4ea4\u6027\u3002", "conclusion": "\u57fa\u7840\u9605\u8bfb\u84b8\u998f\uff08BRD\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5927\u578bLLMs\u7684\u6c34\u5e73\uff0c\u4e3aLLM\u7684\u8f7b\u91cf\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.20467", "pdf": "https://arxiv.org/pdf/2507.20467", "abs": "https://arxiv.org/abs/2507.20467", "authors": ["Avi Deb Raha", "Apurba Adhikary", "Mrityunjoy Gain", "Yumin Park", "Walid Saad", "Choong Seon Hong"], "title": "DD-JSCC: Dynamic Deep Joint Source-Channel Coding for Semantic Communications", "categories": ["cs.NI"], "comment": null, "summary": "Deep Joint Source-Channel Coding (Deep-JSCC) has emerged as a promising\nsemantic communication approach for wireless image transmission by jointly\noptimizing source and channel coding using deep learning techniques. However,\ntraditional Deep-JSCC architectures employ fixed encoder-decoder structures,\nlimiting their adaptability to varying device capabilities, real-time\nperformance optimization, power constraints and channel conditions. To address\nthese limitations, we propose DD-JSCC: Dynamic Deep Joint Source-Channel Coding\nfor Semantic Communications, a novel encoder-decoder architecture designed for\nsemantic communication systems. Unlike traditional Deep-JSCC models, DD-JSCC is\nflexible for dynamically adjusting its layer structures in real-time based on\ntransmitter and receiver capabilities, power constraints, compression ratios,\nand current channel conditions. This adaptability is achieved through a\nhierarchical layer activation mechanism combined with implicit regularization\nvia sequential randomized training, effectively reducing combinatorial\ncomplexity, preventing overfitting, and ensuring consistent feature\nrepresentations across varying configurations. Simulation results demonstrate\nthat DD-JSCC enhances the performance of image reconstruction in semantic\ncommunications, achieving up to 2 dB improvement in Peak Signal-to-Noise Ratio\n(PSNR) over fixed Deep-JSCC architectures, while reducing training costs by\nover 40%. The proposed unified framework eliminates the need for multiple\nspecialized models, significantly reducing training complexity and deployment\noverhead.", "AI": {"tldr": "\u4f20\u7edf\u7684Deep-JSCC\u56fa\u5b9a\u7ed3\u6784\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002\u672c\u6587\u63d0\u51faDD-JSCC\uff0c\u4e00\u4e2a\u52a8\u6001\u8c03\u6574\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5c42\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u6761\u4ef6\uff0c\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u8054\u5408\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801\uff08Deep-JSCC\uff09\u6a21\u578b\u91c7\u7528\u56fa\u5b9a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u8bbe\u5907\u80fd\u529b\u3001\u5b9e\u65f6\u6027\u80fd\u4f18\u5316\u3001\u529f\u8017\u9650\u5236\u548c\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faDD-JSCC\uff08\u52a8\u6001\u6df1\u5ea6\u8054\u5408\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801\uff09\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002\u5b83\u901a\u8fc7\u5206\u5c42\u6fc0\u6d3b\u673a\u5236\u548c\u5e8f\u5217\u968f\u673a\u8bad\u7ec3\u7684\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5c42\u7ed3\u6784\u7684\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\uff0c\u4ee5\u9002\u5e94\u53d1\u5c04\u5668/\u63a5\u6536\u5668\u80fd\u529b\u3001\u529f\u8017\u3001\u538b\u7f29\u6bd4\u548c\u4fe1\u9053\u6761\u4ef6\u3002", "result": "DD-JSCC\u5728\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u4e0a\uff0cPSNR\u6bd4\u56fa\u5b9aDeep-JSCC\u67b6\u6784\u63d0\u5347\u9ad8\u8fbe2 dB\uff0c\u5e76\u4e14\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u8d85\u8fc740%\u3002\u8be5\u6846\u67b6\u6d88\u9664\u4e86\u5bf9\u591a\u4e2a\u4e13\u7528\u6a21\u578b\u7684\u9700\u6c42\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u590d\u6742\u5ea6\u548c\u90e8\u7f72\u5f00\u9500\u3002", "conclusion": "DD-JSCC\u901a\u8fc7\u5176\u52a8\u6001\u53ef\u8c03\u7684\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u7684\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\u3001\u8bad\u7ec3\u6548\u7387\u548c\u90e8\u7f72\u7075\u6d3b\u6027\uff0c\u4e3a\u9002\u5e94\u591a\u53d8\u901a\u4fe1\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19733", "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "categories": ["cs.AI", "cs.DB"], "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5229\u7528\u672c\u4f53\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u6765\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u5e76\u4fee\u6b63\u4e86\u4f20\u7edf\u7684\u6982\u7387\u672c\u4f53\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u73b0\u8c61\u7684\u52a8\u6001\u6027\u3002", "motivation": "\u8bba\u8bc1\u672c\u4f53\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u5728\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u6279\u5224\u73b0\u6709\u6982\u7387\u6a21\u578b\u5e76\u63d0\u51fa\u66ff\u4ee3\u65b9\u6848\uff0c\u6765\u514b\u670d\u5176\u7f3a\u9677\u3002", "method": "\u5229\u7528BFO\u548cCCO\u7b49\u672c\u4f53\u6846\u67b6\u7ec4\u7ec7\u548c\u68c0\u7d22\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u6570\u636e\uff08\u4f8b\u5982\u6e14\u8239\u79fb\u52a8\uff09\uff0c\u5e76\u7528\u67e5\u8be2\u7ed3\u679c\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u4ee5\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002\u5f15\u5165\u201c\u65f6\u7a7a\u77ac\u65f6\u201d\u6982\u5ff5\u5b8c\u5584\u7ed3\u6784\u8bed\u4e49\u3002\u6279\u5224\u4e86\u5c06\u6982\u7387\u4e0e\u53ef\u80fd\u6027\u6df7\u6dc6\u7684\u73b0\u6709\u6a21\u578b\uff0c\u63d0\u51fa\u5c06\u6982\u7387\u89c6\u4e3a\u201c\u8fc7\u7a0b\u5256\u9762\u201d\u7684\u66ff\u4ee3\u89c2\u70b9\u3002\u6700\u7ec8\u5c06\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u6982\u7387\u8ba1\u7b97\u7ed3\u679c\u65e0\u7f1d\u6574\u5408\u56de\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u672c\u4f53\uff08BFO\u548cCCO\uff09\u7ec4\u7ec7\u548c\u68c0\u7d22\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u6b64\u6570\u636e\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002\u63d0\u51fa\u4e86\u201c\u65f6\u7a7a\u77ac\u65f6\u201d\u6982\u5ff5\u4ee5\u5b8c\u5584\u8bed\u4e49\u3002\u6279\u5224\u4e86\u73b0\u6709\u6982\u7387\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u6982\u7387\u89c6\u4e3a\u201c\u8fc7\u7a0b\u5256\u9762\u201d\u7684\u65b0\u89c6\u89d2\u3002\u5b9e\u73b0\u4e86\u5c06\u9a6c\u5c14\u53ef\u592b\u94fe\u8ba1\u7b97\u51fa\u7684\u6982\u7387\u65e0\u7f1d\u6574\u5408\u56de\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u5206\u6790\u548c\u51b3\u7b56\u3002", "conclusion": "\u672c\u4f53\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u5728\u751f\u6210\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u901a\u8fc7\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u548c\u63d0\u51fa\u7684\u65b0\u6982\u7387\u89c2\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u9884\u6d4b\u5206\u6790\uff0c\u5e76\u652f\u6301\u51b3\u7b56\u5236\u5b9a\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u73b0\u8c61\u3002"}}
{"id": "2507.19679", "pdf": "https://arxiv.org/pdf/2507.19679", "abs": "https://arxiv.org/abs/2507.19679", "authors": ["Mandar Kulkarni"], "title": "Efficient Learning for Product Attributes with Compact Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image-based product attribute prediction in e-commerce is a crucial task with\nnumerous applications. The supervised fine-tuning of Vision Language Models\n(VLMs) faces significant scale challenges due to the cost of manual or API\nbased annotation. In this paper, we investigate label-efficient semi-supervised\nfine-tuning strategies for compact VLMs (2B-3B parameters) that leverage\nunlabeled product listings through Direct Preference Optimization (DPO).\nBeginning with a small, API-based, annotated, and labeled set, we first employ\nPEFT to train low-rank adapter modules. To update the adapter weights with\nunlabeled data, we generate multiple reasoning-and-answer chains per unlabeled\nsample and segregate these chains into preferred and dispreferred based on\nself-consistency. We then fine-tune the model with DPO loss and use the updated\nmodel for the next iteration. By using PEFT fine-tuning with DPO, our method\nachieves efficient convergence with minimal compute overhead. On a dataset\nspanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes\nonly unlabeled data, demonstrates a significant improvement over the supervised\nmodel. Moreover, experiments demonstrate that accuracy with DPO training\nimproves with more unlabeled data, indicating that a large pool of unlabeled\nsamples can be effectively leveraged to improve performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u4ea7\u54c1\u5217\u8868\u6570\u636e\u548cDPO\uff08Direct Preference Optimization\uff09\u6765\u63d0\u5347\u7535\u5546\u56fe\u50cf\u5546\u54c1\u5c5e\u6027\u9884\u6d4b\u4e2d\u7d27\u51d1\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5546\u4e2d\u57fa\u4e8e\u56fe\u50cf\u7684\u5546\u54c1\u5c5e\u6027\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76d1\u7763\u5f0f\u5fae\u8c03VLM\u9762\u4e34\u9ad8\u6602\u7684\u624b\u52a8\u6216API\u6807\u6ce8\u6210\u672c\u5e26\u6765\u7684\u89c4\u6a21\u6311\u6218\u3002", "method": "\u9996\u5148\uff0c\u4ece\u5c11\u91cfAPI\u6807\u6ce8\u7684\u6570\u636e\u96c6\u5f00\u59cb\uff0c\u4f7f\u7528PEFT\uff08Parameter-Efficient Fine-Tuning\uff09\u8bad\u7ec3\u4f4e\u79e9\u9002\u914d\u5668\u6a21\u5757\u3002\u63a5\u7740\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u4e3a\u6bcf\u4e2a\u65e0\u6807\u7b7e\u6837\u672c\u751f\u6210\u591a\u6761\u63a8\u7406-\u56de\u7b54\u94fe\uff0c\u5e76\u57fa\u4e8e\u201c\u81ea\u6d3d\u6027\u201d\u5c06\u5176\u5206\u4e3a\u504f\u597d\u548c\u975e\u504f\u597d\u94fe\u3002\u7136\u540e\uff0c\u4f7f\u7528DPO\u635f\u5931\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7528\u66f4\u65b0\u540e\u7684\u6a21\u578b\u8fdb\u884c\u4e0b\u4e00\u8f6e\u8fed\u4ee3\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408PEFT\u548cDPO\u5b9e\u73b0\u9ad8\u6548\u6536\u655b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u6536\u655b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002\u5728\u6db5\u76d612\u4e2a\u7535\u5546\u5782\u76f4\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u7684DPO\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8868\u660eDPO\u8bad\u7ec3\u7684\u51c6\u786e\u6027\u968f\u65e0\u6807\u7b7e\u6570\u636e\u91cf\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u3002", "conclusion": "\u5927\u91cf\u65e0\u6807\u7b7e\u6837\u672c\u53ef\u4ee5\u88ab\u6709\u6548\u5229\u7528\u6765\u663e\u8457\u63d0\u9ad8\u7535\u5546\u56fe\u50cf\u5546\u54c1\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0cDPO\u7ed3\u5408PEFT\u662f\u4e00\u79cd\u6709\u6548\u7684\u534a\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2507.19522", "pdf": "https://arxiv.org/pdf/2507.19522", "abs": "https://arxiv.org/abs/2507.19522", "authors": ["Aarush Gupta", "Kendric Hsu", "Syna Mathod"], "title": "Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations", "categories": ["cs.LG"], "comment": null, "summary": "Mathematical models in neural networks are powerful tools for solving complex\ndifferential equations and optimizing their parameters; that is, solving the\nforward and inverse problems, respectively. A forward problem predicts the\noutput of a network for a given input by optimizing weights and biases. An\ninverse problem finds equation parameters or coefficients that effectively\nmodel the data. A Physics-Informed Neural Network (PINN) can solve both\nproblems. PINNs inject prior analytical information about the data into the\ncost function to improve model performance outside the training set boundaries.\nThis also allows PINNs to efficiently solve problems with sparse data without\noverfitting by extrapolating the model to fit larger trends in the data. The\nprior information we implement is in the form of differential equations.\nResiduals are the differences between the left-hand and right-hand sides of\ncorresponding differential equations; PINNs minimize these residuals to\neffectively solve the differential equation and take advantage of prior\nknowledge. In this way, the solution and parameters are embedded into the loss\nfunction and optimized, allowing both the weights of the neural network and the\nmodel parameters to be found simultaneously, solving both the forward and\ninverse problems in the process. In this paper, we will create PINNs with\nresiduals of varying complexity, beginning with linear and quadratic models and\nthen expanding to fit models for the heat equation and other complex\ndifferential equations. We will mainly use Python as the computing language,\nusing the PyTorch library to aid us in our research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u5e76\u65e8\u5728\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6765\u89e3\u51b3\u590d\u6742\u7684\u5fae\u5206\u65b9\u7a0b\u7684\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\u3002PINN\u901a\u8fc7\u5c06\u5fae\u5206\u65b9\u7a0b\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u5d4c\u5165\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6b8b\u5dee\u6765\u540c\u65f6\u4f18\u5316\u7f51\u7edc\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\uff0c\u5373\u4f7f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u4f20\u7edf\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u89e3\u51b3\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u53ca\u5176\u53c2\u6570\u4f18\u5316\uff08\u5373\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\uff09\u65f6\u53ef\u80fd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u758f\u6216\u9700\u8981\u6cdb\u5316\u5230\u8bad\u7ec3\u96c6\u8fb9\u754c\u4e4b\u5916\u65f6\u3002\u5f15\u5165\u7269\u7406\u5148\u9a8c\u4fe1\u606f\uff08\u5982\u5fae\u5206\u65b9\u7a0b\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u6838\u5fc3\u65b9\u6cd5\u662f\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u3002\u5177\u4f53\u64cd\u4f5c\u662f\u5c06\u5fae\u5206\u65b9\u7a0b\u5f62\u5f0f\u7684\u5148\u9a8c\u5206\u6790\u4fe1\u606f\u6ce8\u5165\u5230\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5fae\u5206\u65b9\u7a0b\u7684\u6b8b\u5dee\uff08\u5de6\u53f3\u4fa7\u4e4b\u5dee\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u3001\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u4f18\u5316\u548c\u6a21\u578b\u53c2\u6570\u53d1\u73b0\u7684\u540c\u6b65\u8fdb\u884c\u3002\u672c\u6587\u5c06\u4ece\u7ebf\u6027\u3001\u4e8c\u6b21\u6a21\u578b\u5f00\u59cb\uff0c\u9010\u6b65\u6269\u5c55\u5230\u70ed\u65b9\u7a0b\u53ca\u5176\u4ed6\u590d\u6742\u5fae\u5206\u65b9\u7a0b\u7684PINN\u6784\u5efa\u4e0e\u5e94\u7528\u3002\u4e3b\u8981\u4f7f\u7528Python\u8bed\u8a00\u548cPyTorch\u5e93\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u4e0d\u540c\u590d\u6742\u5ea6\u7684PINN\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u8bc1\u660ePINN\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u5fae\u5206\u65b9\u7a0b\u76f8\u5173\u7684\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\u3002\u9884\u671f\u7ed3\u679c\u5305\u62ecPINN\u5728\u7a00\u758f\u6570\u636e\u4e0b\u7684\u9ad8\u6548\u6027\u3001\u907f\u514d\u8fc7\u62df\u5408\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u80fd\u591f\u901a\u8fc7\u5916\u63a8\u9002\u5e94\u6570\u636e\u4e2d\u7684\u66f4\u5927\u8d8b\u52bf\uff0c\u5e76\u540c\u65f6\u627e\u5230\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548c\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u800c\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\uff08\u5fae\u5206\u65b9\u7a0b\uff09\u6765\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u758f\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u9879\u7814\u7a76\u6709\u671b\u5c55\u793aPINN\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.19748", "pdf": "https://arxiv.org/pdf/2507.19748", "abs": "https://arxiv.org/abs/2507.19748", "authors": ["Yifan Hao", "Fangning Chao", "Yaqian Hao", "Zhaojun Cui", "Huan Bai", "Haiyu Zhang", "Yankai Liu", "Chao Deng", "Junlan Feng"], "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mathematical reasoning is a cornerstone of artificial general intelligence\nand a primary benchmark for evaluating the capabilities of Large Language\nModels (LLMs). While state-of-the-art models show promise, they often falter\nwhen faced with complex problems that demand deep conceptual understanding and\nintricate, multi-step deliberation. To address this challenge, we introduce\nJT-Math-8B, a series of open-source models comprising base, instruct, and\nthinking versions, built upon a systematic, multi-stage optimization framework.\nOur pre-training corpus is a high-quality, 210B-token dataset curated through a\ndedicated data pipeline that uses model-based validation to ensure quality and\ndiversity. The Instruct Model is optimized for direct, concise answers through\nSupervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using a Long\nChain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage\nRL curriculum that progressively increases task difficulty and context length\nup to 32K tokens. JT-Math-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's\nO1-mini and GPT-4o , and demonstrating superior performance on\ncompetition-level mathematics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86JT-Math-8B\uff0c\u4e00\u4e2a\u901a\u8fc7\u591a\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u65b0\u9896\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bfe\u7a0b\u8bad\u7ec3\u7684\u5f00\u6e90\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u9700\u8981\u6df1\u5ea6\u7406\u89e3\u548c\u591a\u6b65\u63a8\u5bfc\u7684\u590d\u6742\u6570\u5b66\u95ee\u9898\u65f6\u5e38\u6709\u4e0d\u8db3\uff0c\u800c\u6570\u5b66\u63a8\u7406\u662f\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u57fa\u77f3\u548c\u8bc4\u4f30LLMs\u80fd\u529b\u7684\u5173\u952e\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86JT-Math-8B\u7cfb\u5217\u5f00\u6e90\u6a21\u578b\uff08\u5305\u62ec\u57fa\u7840\u3001\u6307\u4ee4\u548c\u601d\u7ef4\u7248\u672c\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76842100\u4ebftoken\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u6307\u4ee4\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8eGRPO\u7684RL\u4f18\u5316\u4ee5\u63d0\u4f9b\u7b80\u6d01\u7b54\u6848\u3002\u601d\u7ef4\u6a21\u578b\u5219\u91c7\u7528\u957f\u601d\u7ef4\u94fe\uff08Long CoT\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408SFT\u548c\u65b0\u9896\u7684\u591a\u9636\u6bb5RL\u8bfe\u7a0b\uff0c\u9010\u6b65\u589e\u52a0\u4efb\u52a1\u96be\u5ea6\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u81f332K token\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002", "result": "JT-Math-8B\u5728\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86OpenAI\u7684O1-mini\u548cGPT-4o\u7b49\u77e5\u540d\u6a21\u578b\uff0c\u5e76\u5728\u7ade\u8d5b\u7ea7\u6570\u5b66\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "JT-Math-8B\u901a\u8fc7\u5176\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u6570\u636e\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u6570\u5b66\u63a8\u7406\u9886\u57df\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.20524", "pdf": "https://arxiv.org/pdf/2507.20524", "abs": "https://arxiv.org/abs/2507.20524", "authors": ["Zhang Liu", "Lianfen Huang", "Zhibin Gao", "Xianbin Wang", "Dusit Niyato", "Xuemin", "Shen"], "title": "A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach for UAV-Assisted Vehicular Networks with Delayed CSI Feedback", "categories": ["cs.NI"], "comment": "13 pages, 11 figures, transactions paper", "summary": "Low altitude uncrewed aerial vehicles (UAVs) are expected to facilitate the\ndevelopment of aerial-ground integrated intelligent transportation systems and\nunlocking the potential of the emerging low-altitude economy. However, several\ncritical challenges persist, including the dynamic optimization of network\nresources and UAV trajectories, limited UAV endurance, and imperfect channel\nstate information (CSI). In this paper, we offer new insights into low-altitude\neconomy networking by exploring intelligent UAV-assisted vehicle-to-everything\ncommunication strategies aligned with UAV energy efficiency. Particularly, we\nformulate an optimization problem of joint channel allocation, power control,\nand flight altitude adjustment in UAV-assisted vehicular networks. Taking CSI\nfeedback delay into account, our objective is to maximize the vehicle-to-UAV\ncommunication sum rate while satisfying the UAV's long-term energy constraint.\nTo this end, we first leverage Lyapunov optimization to decompose the original\nlong-term problem into a series of per-slot deterministic subproblems. We then\npropose a diffusion-based deep deterministic policy gradient (D3PG) algorithm,\nwhich innovatively integrates diffusion models to determine optimal channel\nallocation, power control, and flight altitude adjustment decisions. Through\nextensive simulations using real-world vehicle mobility traces, we demonstrate\nthe superior performance of the proposed D3PG algorithm compared to existing\nbenchmark solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08D3PG\uff09\u7684\u7b97\u6cd5\uff0c\u4ee5\u4f18\u5316\u65e0\u4eba\u673a\u8f85\u52a9\u8f66\u8054\u7f51\u4e2d\u7684\u4fe1\u9053\u5206\u914d\u3001\u529f\u7387\u63a7\u5236\u548c\u98de\u884c\u9ad8\u5ea6\uff0c\u65e8\u5728\u6700\u5927\u5316\u8f66-\u65e0\u4eba\u673a\u901a\u4fe1\u603b\u901f\u7387\u5e76\u6ee1\u8db3\u65e0\u4eba\u673a\u957f\u671f\u80fd\u91cf\u7ea6\u675f\uff0c\u540c\u65f6\u8003\u8651\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u53cd\u9988\u5ef6\u8fdf\u3002", "motivation": "\u4f4e\u7a7a\u65e0\u4eba\u673a\u6709\u671b\u4fc3\u8fdb\u7a7a\u5730\u4e00\u4f53\u5316\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u548c\u4f4e\u7a7a\u7ecf\u6d4e\u53d1\u5c55\uff0c\u4f46\u9762\u4e34\u7f51\u7edc\u8d44\u6e90\u548c\u8f68\u8ff9\u52a8\u6001\u4f18\u5316\u3001\u65e0\u4eba\u673a\u7eed\u822a\u6709\u9650\u53ca\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0d\u5b8c\u5584\u7b49\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u7b26\u5408\u65e0\u4eba\u673a\u80fd\u6548\u7684\u667a\u80fd\u65e0\u4eba\u673a\u8f85\u52a9\u8f66\u8054\u7f51\u901a\u4fe1\u7b56\u7565\u3002", "method": "\u7814\u7a76\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u8054\u5408\u4fe1\u9053\u5206\u914d\u3001\u529f\u7387\u63a7\u5236\u548c\u98de\u884c\u9ad8\u5ea6\u8c03\u6574\u7684\u4f18\u5316\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5728\u8003\u8651CSI\u53cd\u9988\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u5927\u5316\u8f66-\u65e0\u4eba\u673a\u901a\u4fe1\u603b\u901f\u7387\u5e76\u6ee1\u8db3\u65e0\u4eba\u673a\u957f\u671f\u80fd\u91cf\u7ea6\u675f\u3002\u9996\u5148\uff0c\u5229\u7528Lyapunov\u4f18\u5316\u5c06\u957f\u671f\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u9010\u65f6\u9699\u7684\u786e\u5b9a\u6027\u5b50\u95ee\u9898\uff1b\u7136\u540e\uff0c\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08diffusion model\uff09\u7684\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08D3PG\uff09\u7b97\u6cd5\u6765\u786e\u5b9a\u6700\u4f18\u51b3\u7b56\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u8f66\u8f86\u79fb\u52a8\u8f68\u8ff9\u8fdb\u884c\u5927\u91cf\u4eff\u771f\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684D3PG\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684D3PG\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u8f85\u52a9\u8f66\u8054\u7f51\u4e2d\u7684\u8d44\u6e90\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u901f\u7387\u6700\u5927\u5316\u548c\u80fd\u91cf\u7ea6\u675f\u7684\u5e73\u8861\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19749", "pdf": "https://arxiv.org/pdf/2507.19749", "abs": "https://arxiv.org/abs/2507.19749", "authors": ["Lin Ren", "Guohui Xiao", "Guilin Qi", "Yishuai Geng", "Haohan Xue"], "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)", "categories": ["cs.AI"], "comment": "Accepted for publication at the 22nd International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2025). The code is\n  available at https://github.com/HomuraT/ASPBench", "summary": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonic\nreasoning. Recently, large language models (LLMs) have demonstrated promising\ncapabilities in logical reasoning. Despite this potential, current evaluations\nof LLM capabilities in ASP are often limited. Existing works normally employ\noverly simplified ASP programs, do not support negation, disjunction, or\nmultiple answer sets. Furthermore, there is a lack of benchmarks that introduce\ntasks specifically designed for ASP solving. To bridge this gap, we introduce\nASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:\nASP entailment, answer set verification, and answer set computation. Our\nextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,\nincluding \\emph{deepseek-r1}, \\emph{o4-mini}, and\n\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two\nsimpler tasks, they struggle with answer set computation, which is the core of\nASP solving. These findings offer insights into the current limitations of LLMs\nin ASP solving. This highlights the need for new approaches that integrate\nsymbolic reasoning capabilities more effectively. The code and dataset are\navailable at https://github.com/HomuraT/ASPBench.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\u4e86ASPBench\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793aLLMs\u5728ASP\u7684\u6838\u5fc3\u4efb\u52a1\u2014\u2014\u7b54\u6848\u96c6\u8ba1\u7b97\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u903b\u8f91\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u5bf9LLM\u5728\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u80fd\u529b\u4e0a\u7684\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\u3002\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u4f7f\u7528\u8fc7\u4e8e\u7b80\u5316\u7684ASP\u7a0b\u5e8f\uff0c\u7f3a\u4e4f\u5bf9\u5426\u5b9a\u3001\u6790\u53d6\u6216\u591a\u91cd\u7b54\u6848\u96c6\u7684\u652f\u6301\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e13\u95e8\u4e3aASP\u6c42\u89e3\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002", "method": "\u4e3a\u5f25\u8865\u73b0\u6709\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aASPBench\u7684\u7efc\u5408ASP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7684ASP\u4efb\u52a1\uff1aASP\u8574\u6db5\u3001\u7b54\u6848\u96c6\u9a8c\u8bc1\u548c\u7b54\u6848\u96c6\u8ba1\u7b97\u3002\u5e76\u4f7f\u752814\u4e2a\u6700\u5148\u8fdb\u7684LLM\u5728\u6b64\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "\u5bf9LLM\u5728ASPBench\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u867d\u7136\u5b83\u4eec\u5728ASP\u8574\u6db5\u548c\u7b54\u6848\u96c6\u9a8c\u8bc1\u8fd9\u4e24\u4e2a\u76f8\u5bf9\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u4f5c\u4e3aASP\u6c42\u89e3\u6838\u5fc3\u7684\u7b54\u6848\u96c6\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u6323\u624e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLM\u5728ASP\u6c42\u89e3\u65b9\u9762\u7684\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u5730\u6574\u5408\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.19682", "pdf": "https://arxiv.org/pdf/2507.19682", "abs": "https://arxiv.org/abs/2507.19682", "authors": ["Matthew Drexler", "Benjamin Risk", "James J Lah", "Suprateek Kundu", "Deqiang Qiu"], "title": "DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages, 10 figures", "summary": "Conventional multimodal data integration methods provide a comprehensive\nassessment of the shared or unique structure within each individual data type\nbut suffer from several limitations such as the inability to handle\nhigh-dimensional data and identify nonlinear structures. In this paper, we\nintroduce DeepJIVE, a deep-learning approach to performing Joint and Individual\nVariance Explained (JIVE). We perform mathematical derivation and experimental\nvalidations using both synthetic and real-world 1D, 2D, and 3D datasets.\nDifferent strategies of achieving the identity and orthogonality constraints\nfor DeepJIVE were explored, resulting in three viable loss functions. We found\nthat DeepJIVE can successfully uncover joint and individual variations of\nmultimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) also identified biologically plausible\ncovariation patterns between the amyloid positron emission tomography (PET) and\nmagnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a\nuseful tool for multimodal data analysis.", "AI": {"tldr": "DeepJIVE\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u7ed3\u6784\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6210\u529f\u63ed\u793a\u8054\u5408\u4e0e\u4e2a\u4f53\u53d8\u5f02\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u548c\u8bc6\u522b\u975e\u7ebf\u6027\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faDeepJIVE\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8054\u5408\u4e0e\u4e2a\u4f53\u65b9\u5dee\u89e3\u91ca\uff08JIVE\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u6570\u5b66\u63a8\u5bfc\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u7684\u4e00\u7ef4\u3001\u4e8c\u7ef4\u3001\u4e09\u7ef4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u540c\u65f6\uff0c\u63a2\u7d22\u4e86\u5b9e\u73b0DeepJIVE\u6052\u7b49\u548c\u6b63\u4ea4\u7ea6\u675f\u7684\u4e0d\u540c\u7b56\u7565\uff0c\u5e76\u5bfc\u51fa\u4e86\u4e09\u79cd\u53ef\u884c\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "DeepJIVE\u6210\u529f\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u8054\u5408\u548c\u4e2a\u4f53\u53d8\u5f02\u3002\u5c06\u5176\u5e94\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u795e\u7ecf\u5f71\u50cf\u5b66\u8ba1\u5212\uff08ADNI\uff09\u6570\u636e\u96c6\uff0c\u8fd8\u8bc6\u522b\u51fa\u4e86\u6dc0\u7c89\u6837\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf\uff08PET\uff09\u548c\u78c1\u5171\u632f\uff08MR\uff09\u56fe\u50cf\u4e4b\u95f4\u5177\u6709\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u534f\u53d8\u6a21\u5f0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DeepJIVE\u53ef\u4ee5\u6210\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u6709\u7528\u5de5\u5177\u3002"}}
{"id": "2507.19523", "pdf": "https://arxiv.org/pdf/2507.19523", "abs": "https://arxiv.org/abs/2507.19523", "authors": ["Xingyu Su", "Xiner Li", "Yuchao Lin", "Ziqian Xie", "Degui Zhi", "Shuiwang Ji"], "title": "Language Models for Controllable DNA Sequence Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We consider controllable DNA sequence design, where sequences are generated\nby conditioning on specific biological properties. While language models (LMs)\nsuch as GPT and BERT have achieved remarkable success in natural language\ngeneration, their application to DNA sequence generation remains largely\nunderexplored. In this work, we introduce ATGC-Gen, an Automated Transformer\nGenerator for Controllable Generation, which leverages cross-modal encoding to\nintegrate diverse biological signals. ATGC-Gen is instantiated with both\ndecoder-only and encoder-only transformer architectures, allowing flexible\ntraining and generation under either autoregressive or masked recovery\nobjectives. We evaluate ATGC-Gen on representative tasks including promoter and\nenhancer sequence design, and further introduce a new dataset based on ChIP-Seq\nexperiments for modeling protein binding specificity. Our experiments\ndemonstrate that ATGC-Gen can generate fluent, diverse, and biologically\nrelevant sequences aligned with the desired properties. Compared to prior\nmethods, our model achieves notable improvements in controllability and\nfunctional relevance, highlighting the potential of language models in\nadvancing programmable genomic design. The source code is released at\n(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).", "AI": {"tldr": "\u9488\u5bf9\u53ef\u63a7DNA\u5e8f\u5217\u8bbe\u8ba1\uff0c\u672c\u6587\u63d0\u51faATGC-Gen\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u751f\u6210\u6a21\u578b\uff0c\u5229\u7528\u4ea4\u53c9\u6a21\u6001\u7f16\u7801\u6574\u5408\u751f\u7269\u4fe1\u53f7\uff0c\u80fd\u751f\u6210\u7b26\u5408\u7279\u5b9a\u751f\u7269\u5b66\u7279\u6027\u3001\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684DNA\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9886\u57df\u6210\u679c\u663e\u8457\uff0c\u4f46\u5728DNA\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e14\u7f3a\u4e4f\u80fd\u6839\u636e\u7279\u5b9a\u751f\u7269\u5b66\u7279\u6027\u751f\u6210\u53ef\u63a7DNA\u5e8f\u5217\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faATGC-Gen\u6a21\u578b\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u53ef\u63a7\u751f\u6210\u5668\uff0c\u91c7\u7528\u4ea4\u53c9\u6a21\u6001\u7f16\u7801\u6574\u5408\u751f\u7269\u4fe1\u53f7\u3002\u6a21\u578b\u652f\u6301\u4ec5\u89e3\u7801\u5668\u548c\u4ec5\u7f16\u7801\u5668\u4e24\u79cdTransformer\u67b6\u6784\uff0c\u5e76\u53ef\u7075\u6d3b\u9009\u62e9\u81ea\u56de\u5f52\u6216\u63a9\u7801\u6062\u590d\u8bad\u7ec3\u76ee\u6807\u3002\u5728\u542f\u52a8\u5b50\u3001\u589e\u5f3a\u5b50\u5e8f\u5217\u8bbe\u8ba1\u53ca\u57fa\u4e8eChIP-Seq\u7684\u86cb\u767d\u8d28\u7ed3\u5408\u7279\u5f02\u6027\u5efa\u6a21\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eATGC-Gen\u80fd\u751f\u6210\u6d41\u7545\u3001\u591a\u6837\u4e14\u5177\u6709\u751f\u7269\u5b66\u76f8\u5173\u6027\u7684\u5e8f\u5217\uff0c\u5e76\u4e0e\u6240\u9700\u7279\u6027\u4e00\u81f4\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6a21\u578b\u5728\u53ef\u63a7\u6027\u548c\u529f\u80fd\u76f8\u5173\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8fdb\u53ef\u7f16\u7a0b\u57fa\u56e0\u7ec4\u8bbe\u8ba1\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u53ef\u63a7DNA\u5e8f\u5217\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.19756", "pdf": "https://arxiv.org/pdf/2507.19756", "abs": "https://arxiv.org/abs/2507.19756", "authors": ["Rebecca M. M. Hicke", "Brian Haggard", "Mia Ferrante", "Rayhan Khanna", "David Mimno"], "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs", "categories": ["cs.CL"], "comment": null, "summary": "In addition to its more widely studied political activities, the American\nEvangelical movement has a well-developed but less externally visible cultural\nand literary side. Christian Fiction, however, has been little studied, and\nwhat scholarly attention there is has focused on the explosively popular Left\nBehind series. In this work, we use computational tools to provide both a broad\ntopical overview of Christian Fiction as a genre and a more directed\nexploration of how its authors depict divine acts. Working with human\nannotators we first developed definitions and a codebook for \"acts of God.\" We\nthen adapted those instructions designed for human annotators for use by a\nrecent, lightweight LM with the assistance of a much larger model. The\nlaptop-scale LM is capable of matching human annotations, even when the task is\nsubtle and challenging. Using these annotations, we show that significant and\nmeaningful differences exist between the Left Behind books and Christian\nFiction more broadly and between books by male and female authors.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8ba1\u7b97\u5de5\u5177\u5206\u6790\u57fa\u7763\u6559\u5c0f\u8bf4\uff0c\u63ed\u793a\u4e86\u8be5\u6587\u5b66\u4f53\u88c1\u7684\u4e3b\u9898\u6982\u89c8\u53ca\u5176\u5bf9\u201c\u4e0a\u5e1d\u4e4b\u884c\u201d\u7684\u63cf\u7ed8\uff0c\u5e76\u53d1\u73b0\u300a\u672b\u65e5\u8ff7\u8e2a\u300b\u7cfb\u5217\u4e0e\u66f4\u5e7f\u6cdb\u7684\u57fa\u7763\u6559\u5c0f\u8bf4\u4ee5\u53ca\u4e0d\u540c\u6027\u522b\u4f5c\u8005\u4f5c\u54c1\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7f8e\u56fd\u798f\u97f3\u6d3e\u7684\u6587\u5316\u548c\u6587\u5b66\u65b9\u9762\uff0c\u7279\u522b\u662f\u57fa\u7763\u6559\u5c0f\u8bf4\uff0c\u7814\u7a76\u4e0d\u8db3\u3002\u73b0\u6709\u5b66\u672f\u5173\u6ce8\u4e3b\u8981\u96c6\u4e2d\u5728\u70ed\u95e8\u7684\u300a\u672b\u65e5\u8ff7\u8e2a\u300b\u7cfb\u5217\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u57fa\u7763\u6559\u5c0f\u8bf4\u7684\u5e7f\u6cdb\u5206\u6790\u548c\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u9996\u5148\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u5408\u4f5c\uff0c\u5236\u5b9a\u4e86\u201c\u4e0a\u5e1d\u4e4b\u884c\u201d\u7684\u5b9a\u4e49\u548c\u7f16\u7801\u624b\u518c\u3002\u5176\u6b21\uff0c\u501f\u52a9\u5927\u578b\u6a21\u578b\uff0c\u5c06\u4eba\u5de5\u6807\u6ce8\u6307\u5357\u9002\u914d\u4e8e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u3002\u6700\u540e\uff0c\u4f7f\u7528\u8be5\u6a21\u578b\u5bf9\u57fa\u7763\u6559\u5c0f\u8bf4\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4f5c\u8005\u5982\u4f55\u63cf\u7ed8\u795e\u5723\u884c\u4e3a\u3002", "result": "\u5f00\u53d1\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u5339\u914d\u4eba\u7c7b\u6807\u6ce8\uff0c\u5373\u4f7f\u9762\u5bf9\u5fae\u5999\u548c\u590d\u6742\u7684\u4efb\u52a1\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0c\u300a\u672b\u65e5\u8ff7\u8e2a\u300b\u7cfb\u5217\u4e0e\u66f4\u5e7f\u6cdb\u7684\u57fa\u7763\u6559\u5c0f\u8bf4\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6\u7537\u6027\u548c\u5973\u6027\u4f5c\u8005\u7684\u4f5c\u54c1\u4e4b\u95f4\u4e5f\u5b58\u5728\u6709\u610f\u4e49\u7684\u533a\u522b\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u8fd0\u7528\u8ba1\u7b97\u65b9\u6cd5\u5bf9\u57fa\u7763\u6559\u5c0f\u8bf4\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u8d85\u8d8a\u4e86\u4ee5\u5f80\u5bf9\u7279\u5b9a\u7cfb\u5217\u4f5c\u54c1\u7684\u72ed\u7a84\u5173\u6ce8\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8be5\u6587\u4f53\u5185\u90e8\u7684\u591a\u6837\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7LM\u5728\u590d\u6742\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.20806", "pdf": "https://arxiv.org/pdf/2507.20806", "abs": "https://arxiv.org/abs/2507.20806", "authors": ["Yunming Xiao", "Peizhi Liu", "Ruijie Yu", "Chenkai Weng", "Matteo Varvello", "Aleksandar Kuzmanovic"], "title": "Collusion Resistant DNS With Private Information Retrieval", "categories": ["cs.NI", "cs.CR"], "comment": null, "summary": "There has been a growing interest in Internet user privacy, demonstrated by\nthe popularity of privacy-preserving products such as Telegram and Brave, and\nthe widespread adoption of HTTPS. The Domain Name System (DNS) is a key\ncomponent of Internet-based communication and its privacy has been neglected\nfor years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing\nthe issue of in-path middleboxes. Further progress has been made with\nproxy-based solutions such as Oblivious DoH (ODoH), which separate a user's\nidentity from their DNS queries. However, these solutions rely on non-collusion\nassumptions between DNS resolvers and proxies -- an assumption difficult to\nguarantee in practice. To address this, we explore integrating single-server\nPrivate Information Retrieval (PIR) into DNS to enable encrypted query\nprocessing without relying on trust assumptions. However, applying PIR to DNS\nis challenging due to its hierarchical nature -- particularly, interactions\nwith recursive resolvers can still leak information. Navigating performance and\nprivacy trade-offs, we propose PDNS, a DNS extension leveraging single-server\nPIR to strengthen privacy guarantees. We have implemented a prototype of PDNS\nand compared its performance against state-of-the-art solutions via\ntrace-driven experiments. The results show that PDNS achieves acceptable\nperformance (2x faster than DoH over Tor with similar privacy guarantees) and\nstrong privacy guarantees today, mainly at the cost of its scalability, which\nspecialized hardware for PIR can address in the near future.", "AI": {"tldr": "\u73b0\u6709DNS\u9690\u79c1\u65b9\u6848\u4f9d\u8d56\u4fe1\u4efb\u5047\u8bbe\u3002\u672c\u6587\u63d0\u51faPDNS\uff0c\u5229\u7528\u5355\u670d\u52a1\u5668PIR\u5b9e\u73b0\u65e0\u4fe1\u4efb\u5047\u8bbe\u7684DNS\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5728\u6027\u80fd\u548c\u9690\u79c1\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u4e92\u8054\u7f51\u7528\u6237\u9690\u79c1\u65e5\u76ca\u53d7\u5173\u6ce8\uff0c\u4f46\u4f5c\u4e3a\u5173\u952e\u7ec4\u4ef6\u7684DNS\u9690\u79c1\u6027\u957f\u671f\u88ab\u5ffd\u89c6\u3002\u73b0\u6709DNS\u9690\u79c1\u65b9\u6848\uff08\u5982DoH\u3001ODoH\uff09\u4f9d\u8d56\u4e8eDNS\u89e3\u6790\u5668\u548c\u4ee3\u7406\u4e4b\u95f4\u4e0d\u4e32\u901a\u7684\u5047\u8bbe\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u4fdd\u8bc1\uff0c\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4fe1\u4efb\u5047\u8bbe\u7684DNS\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5e76\u8bbe\u8ba1\u4e86PDNS\uff0c\u4e00\u4e2a\u5229\u7528\u5355\u670d\u52a1\u5668\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\uff08PIR\uff09\u7684DNS\u6269\u5c55\u65b9\u6848\uff0c\u65e8\u5728\u5b9e\u73b0\u52a0\u5bc6\u67e5\u8be2\u5904\u7406\uff0c\u4ece\u800c\u907f\u514d\u4fe1\u4efb\u5047\u8bbe\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406DNS\u7684\u5c42\u7ea7\u7ed3\u6784\u6311\u6218\u65f6\uff0c\u5728\u6027\u80fd\u548c\u9690\u79c1\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u7814\u7a76\u4eba\u5458\u5b9e\u73b0\u4e86PDNS\u539f\u578b\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6d41\u91cf\u8ddf\u8e2a\u7684\u5b9e\u9a8c\uff0c\u5c06\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6848\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "PDNS\u5728\u63d0\u4f9b\u5f3a\u5927\u9690\u79c1\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a5\u53d7\u7684\u6027\u80fd\uff0c\u6bd4\u63d0\u4f9b\u76f8\u4f3c\u9690\u79c1\u4fdd\u8bc1\u7684DoH over Tor\u5feb2\u500d\u3002\u76ee\u524d\u4e3b\u8981\u7684\u4ee3\u4ef7\u662f\u5176\u53ef\u4f38\u7f29\u6027\uff0c\u4f46\u8be5\u95ee\u9898\u6709\u671b\u5728\u672a\u6765\u901a\u8fc7\u4e13\u7528PIR\u786c\u4ef6\u89e3\u51b3\u3002", "conclusion": "PDNS\u901a\u8fc7\u6574\u5408\u5355\u670d\u52a1\u5668PIR\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4fe1\u4efb\u5047\u8bbe\u7684\u5f3a\u5927DNS\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002\u5176\u5728\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u969c\u65b9\u9762\u7684\u8868\u73b0\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765DNS\u9690\u79c1\u9886\u57df\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\uff0c\u5c3d\u7ba1\u5f53\u524d\u9762\u4e34\u53ef\u4f38\u7f29\u6027\u6311\u6218\uff0c\u4f46\u786c\u4ef6\u53d1\u5c55\u6709\u671b\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.19788", "pdf": "https://arxiv.org/pdf/2507.19788", "abs": "https://arxiv.org/abs/2507.19788", "authors": ["Rifny Rachman", "Josh Tingey", "Richard Allmendinger", "Pradyumn Shukla", "Wei Pan"], "title": "Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation", "categories": ["cs.AI"], "comment": null, "summary": "This study develops a generalised multi-objective, multi-echelon supply chain\noptimisation model with non-stationary markets based on a Markov decision\nprocess, incorporating economic, environmental, and social considerations. The\nmodel is evaluated using a multi-objective reinforcement learning (RL) method,\nbenchmarked against an originally single-objective RL algorithm modified with\nweighted sum using predefined weights, and a multi-objective evolutionary\nalgorithm (MOEA)-based approach. We conduct experiments on varying network\ncomplexities, mimicking typical real-world challenges using a customisable\nsimulator. The model determines production and delivery quantities across\nsupply chain routes to achieve near-optimal trade-offs between competing\nobjectives, approximating Pareto front sets. The results demonstrate that the\nprimary approach provides the most balanced trade-off between optimality,\ndiversity, and density, further enhanced with a shared experience buffer that\nallows knowledge transfer among policies. In complex settings, it achieves up\nto 75\\% higher hypervolume than the MOEA-based method and generates solutions\nthat are approximately eleven times denser, signifying better robustness, than\nthose produced by the modified single-objective RL method. Moreover, it ensures\nstable production and inventory levels while minimising demand loss.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5e7f\u4e49\u591a\u76ee\u6807\u3001\u591a\u5c42\u7ea7\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60(RL)\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u76ee\u6807\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\uff0c\u5176\u6700\u4f18\u6027\u3001\u591a\u6837\u6027\u548c\u5bc6\u5ea6\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u80fd\u7a33\u5b9a\u751f\u4ea7\u548c\u5e93\u5b58\u3002", "motivation": "\u5728\u975e\u5e73\u7a33\u5e02\u573a\u73af\u5883\u4e0b\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u8003\u8651\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u7b49\u591a\u91cd\u76ee\u6807\uff0c\u5e76\u5b9e\u73b0\u751f\u4ea7\u548c\u4ea4\u4ed8\u6570\u91cf\u8fd1\u6700\u4f18\u6743\u8861\u7684\u901a\u7528\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\u3002", "method": "1. \u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6784\u5efa\u5e7f\u4e49\u591a\u76ee\u6807\u3001\u591a\u5c42\u7ea7\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u7eb3\u5165\u7ecf\u6d4e\u3001\u73af\u5883\u3001\u793e\u4f1a\u8003\u91cf\u3002\n2. \u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60(RL)\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\u3002\n3. \u4e0e\u4fee\u6539\u540e\u7684\u5355\u76ee\u6807RL\u7b97\u6cd5\uff08\u52a0\u6743\u6c42\u548c\uff09\u4ee5\u53ca\u57fa\u4e8e\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5(MOEA)\u7684\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\n4. \u4f7f\u7528\u53ef\u5b9a\u5236\u6a21\u62df\u5668\u5728\u4e0d\u540c\u7f51\u7edc\u590d\u6742\u6027\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002\n5. \u901a\u8fc7\u5171\u4eab\u7ecf\u9a8c\u7f13\u51b2\u533a\u589e\u5f3a\u77e5\u8bc6\u8f6c\u79fb\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "1. \u8be5\u65b9\u6cd5\u5728\u6700\u4f18\u6027\u3001\u591a\u6837\u6027\u548c\u5bc6\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u5e73\u8861\u7684\u6743\u8861\u3002\n2. \u5728\u590d\u6742\u8bbe\u7f6e\u4e0b\uff0c\u5176\u8d85\u4f53\u79ef\u6bd4\u57fa\u4e8eMOEA\u7684\u65b9\u6cd5\u9ad8\u51fa75%\u3002\n3. \u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u5bc6\u5ea6\u6bd4\u4fee\u6539\u540e\u7684\u5355\u76ee\u6807RL\u65b9\u6cd5\u9ad8\u51fa\u7ea611\u500d\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\n4. \u6709\u6548\u786e\u4fdd\u4e86\u751f\u4ea7\u548c\u5e93\u5b58\u6c34\u5e73\u7684\u7a33\u5b9a\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u9700\u6c42\u635f\u5931\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u5e02\u573a\u4e2d\u7684\u591a\u76ee\u6807\u3001\u591a\u5c42\u7ea7\u4f9b\u5e94\u94fe\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\uff0c\u5176\u5728\u5b9e\u73b0\u591a\u76ee\u6807\u6743\u8861\u3001\u5e15\u7d2f\u6258\u524d\u6cbf\u8fd1\u4f3c\u4ee5\u53ca\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e3a\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19691", "pdf": "https://arxiv.org/pdf/2507.19691", "abs": "https://arxiv.org/abs/2507.19691", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate perception and scene understanding in complex urban environments is\na critical challenge for ensuring safe and efficient autonomous navigation. In\nthis paper, we present Co-Win, a novel bird's eye view (BEV) perception\nframework that integrates point cloud encoding with efficient parallel\nwindow-based feature extraction to address the multi-modality inherent in\nenvironmental understanding. Our method employs a hierarchical architecture\ncomprising a specialized encoder, a window-based backbone, and a query-based\ndecoder head to effectively capture diverse spatial features and object\nrelationships. Unlike prior approaches that treat perception as a simple\nregression task, our framework incorporates a variational approach with\nmask-based instance segmentation, enabling fine-grained scene decomposition and\nunderstanding. The Co-Win architecture processes point cloud data through\nprogressive feature extraction stages, ensuring that predicted masks are both\ndata-consistent and contextually relevant. Furthermore, our method produces\ninterpretable and diverse instance predictions, enabling enhanced downstream\ndecision-making and planning in autonomous driving systems.", "AI": {"tldr": "Co-Win\u662f\u4e00\u4e2a\u65b0\u9896\u7684BEV\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u7f16\u7801\u548c\u5e76\u884c\u7a97\u53e3\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u4e2d\u590d\u6742\u57ce\u5e02\u73af\u5883\u7684\u7cbe\u786e\u611f\u77e5\u548c\u7ec6\u7c92\u5ea6\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u4e3a\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u9ad8\u6548\uff0c\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u611f\u77e5\u548c\u573a\u666f\u7406\u89e3\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faCo-Win\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u6846\u67b6\u3002\u5b83\u6574\u5408\u4e86\u70b9\u4e91\u7f16\u7801\u4e0e\u9ad8\u6548\u5e76\u884c\u7a97\u53e3\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u5904\u7406\u73af\u5883\u591a\u6a21\u6001\u6027\u3002Co-Win\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u4e13\u7528\u7f16\u7801\u5668\u3001\u57fa\u4e8e\u7a97\u53e3\u7684\u4e3b\u5e72\u7f51\u7edc\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u89e3\u7801\u5668\u5934\u90e8\u3002\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u4e0d\u540c\uff0cCo-Win\u5f15\u5165\u4e86\u53d8\u5206\u65b9\u6cd5\u548c\u57fa\u4e8e\u63a9\u7801\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u573a\u666f\u5206\u89e3\u548c\u7406\u89e3\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u5904\u7406\u70b9\u4e91\u6570\u636e\u3002", "result": "Co-Win\u80fd\u591f\u6709\u6548\u6355\u83b7\u591a\u6837\u7a7a\u95f4\u7279\u5f81\u548c\u7269\u4f53\u5173\u7cfb\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u63a9\u7801\u65e2\u6570\u636e\u4e00\u81f4\u53c8\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u3002\u5b83\u80fd\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u591a\u6837\u7684\u5b9e\u4f8b\u9884\u6d4b\uff0c\u5e76\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u573a\u666f\u5206\u89e3\u548c\u7406\u89e3\u3002", "conclusion": "Co-Win\u6781\u5927\u5730\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u4e0b\u6e38\u51b3\u7b56\u548c\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2507.19524", "pdf": "https://arxiv.org/pdf/2507.19524", "abs": "https://arxiv.org/abs/2507.19524", "authors": ["Ugo Lomoio", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Kolmogorov Arnold Network Autoencoder in Medicine", "categories": ["cs.LG"], "comment": null, "summary": "Deep learning neural networks architectures such Multi Layer Perceptrons\n(MLP) and Convolutional blocks still play a crucial role in nowadays research\nadvancements. From a topological point of view, these architecture may be\nrepresented as graphs in which we learn the functions related to the nodes\nwhile fixed edges convey the information from the input to the output. A recent\nwork introduced a new architecture called Kolmogorov Arnold Networks (KAN) that\nreports how putting learnable activation functions on the edges of the neural\nnetwork leads to better performances in multiple scenarios. Multiple studies\nare focusing on optimizing the KAN architecture by adding important features\nsuch as dropout regularization, Autoencoders (AE), model benchmarking and last,\nbut not least, the KAN Convolutional Network (KCN) that introduced matrix\nconvolution with KANs learning. This study aims to benchmark multiple versions\nof vanilla AEs (such as Linear, Convolutional and Variational) against their\nKolmogorov-Arnold counterparts that have same or less number of parameters.\nUsing cardiological signals as model input, a total of five different classic\nAE tasks were studied: reconstruction, generation, denoising, inpainting and\nanomaly detection. The proposed experiments uses a medical dataset\n\\textit{AbnormalHeartbeat} that contains audio signals obtained from the\nstethoscope.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u5bf9\u6bd4\u57fa\u4e8eKolmogorov-Arnold Networks (KAN) \u7684\u81ea\u7f16\u7801\u5668\u4e0e\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u5728\u5fc3\u8840\u7ba1\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6d89\u53ca\u91cd\u5efa\u3001\u751f\u6210\u3001\u53bb\u566a\u3001\u4fee\u590d\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u3002", "motivation": "Kolmogorov Arnold Networks (KAN) \u901a\u8fc7\u5728\u795e\u7ecf\u7f51\u7edc\u8fb9\u4e0a\u5f15\u5165\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u5f53\u524d\u7814\u7a76\u6b63\u81f4\u529b\u4e8e\u4f18\u5316KAN\u67b6\u6784\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u81ea\u7f16\u7801\u5668\u7b49\u9886\u57df\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u8bc4\u4f30KAN\u67b6\u6784\u7684\u81ea\u7f16\u7801\u5668\u5728\u5fc3\u810f\u4fe1\u53f7\u5904\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e0e\u53c2\u6570\u91cf\u76f8\u540c\u6216\u66f4\u5c11\u7684\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u672c\u7814\u7a76\u5c06\u591a\u79cd\u7ecf\u5178\u81ea\u7f16\u7801\u5668\uff08\u5982\u7ebf\u6027\u3001\u5377\u79ef\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\u4e0e\u5176\u5bf9\u5e94\u7684Kolmogorov-Arnold\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u6bd4\u3002\u8f93\u5165\u6570\u636e\u4e3a\u5fc3\u810f\u75c5\u5b66\u4fe1\u53f7\uff08\u6765\u81ea\u542c\u8bca\u5668\u7684\u97f3\u9891\u4fe1\u53f7\uff09\u3002\u5b9e\u9a8c\u5c06\u5728\u533b\u5b66\u6570\u636e\u96c6`AbnormalHeartbeat`\u4e0a\u8fdb\u884c\uff0c\u6db5\u76d6\u4e94\u79cd\u81ea\u7f16\u7801\u5668\u4efb\u52a1\uff1a\u91cd\u5efa\u3001\u751f\u6210\u3001\u53bb\u566a\u3001\u56fe\u50cf\u4fee\u590d\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u5f97\u51fa\u7814\u7a76\u7ed3\u8bba\u3002"}}
{"id": "2507.19766", "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUloRL\uff0c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6bb5\u89e3\u7801\u548c\u52a8\u6001\u63a9\u853d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5904\u7406\u8d85\u957f\u8f93\u51fa\u5e8f\u5217\u65f6\u7684\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u80fd\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f20\u7edfRL\u6846\u67b6\u5728\u5904\u7406\u8d85\u957f\u8f93\u51fa\u65f6\u9762\u4e34\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5982\u957f\u5c3e\u5e8f\u5217\u5206\u5e03\u548c\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3002", "method": "\u63d0\u51faUltra-Long Output Reinforcement Learning (UloRL)\u65b9\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u5c06\u8d85\u957f\u8f93\u51fa\u89e3\u7801\u5212\u5206\u4e3a\u77ed\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u7f13\u89e3\u957f\u5c3e\u6837\u672c\u5bfc\u81f4\u7684\u5ef6\u8fdf\u30022) \u5f15\u5165\u52a8\u6001\u63a9\u853d\u5df2\u638c\u63e1\u6b63\u5411Token (MPTs) \u4ee5\u9632\u6b62\u71b5\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728Qwen3-30B-A3B\u6a21\u578b\u4e0a\uff0c\u5206\u6bb5\u56de\u6edaRL\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.06\u500d\u3002\u4f7f\u7528128k-token\u8f93\u51fa\u7684RL\u8bad\u7ec3\u4f7f\u6a21\u578b\u5728AIME2025\u4e0a\u7684\u6027\u80fd\u4ece70.9%\u63d0\u5347\u81f385.1%\uff0c\u5728BeyondAIME\u4e0a\u4ece50.7%\u63d0\u5347\u81f361.9%\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86Qwen3-235B-A22B\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u5347LLM\u8d85\u957f\u5e8f\u5217\u751f\u6210\u53ca\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u5de8\u5927\u6f5c\u529b\u3002\u4f5c\u8005\u5c06\u53d1\u5e03\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u4f9b\u793e\u533a\u8fdb\u4e00\u6b65\u4f7f\u7528\u3002"}}
{"id": "2507.20871", "pdf": "https://arxiv.org/pdf/2507.20871", "abs": "https://arxiv.org/abs/2507.20871", "authors": ["Wenxuan Ye", "Xueli An", "Junfan Wang", "Xueqiang Yan", "Georg Carle"], "title": "\\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted to ICC 2025", "summary": "Native AI support is a key objective in the evolution of 6G networks, with\nFederated Learning (FL) emerging as a promising paradigm. FL allows\ndecentralized clients to collaboratively train an AI model without directly\nsharing their data, preserving privacy. Clients train local models on private\ndata and share model updates, which a central server aggregates to refine the\nglobal model and redistribute it for the next iteration. However, client data\nheterogeneity slows convergence and reduces model accuracy, and frequent client\nparticipation imposes communication and computational burdens. To address these\nchallenges, we propose \\textit{FedABC}, an innovative client selection\nalgorithm designed to take a long-term view in managing data heterogeneity and\noptimizing client participation. Inspired by attention mechanisms,\n\\textit{FedABC} prioritizes informative clients by evaluating both model\nsimilarity and each model's unique contributions to the global model. Moreover,\nconsidering the evolving demands of the global model, we formulate an\noptimization problem to guide \\textit{FedABC} throughout the training process.\nFollowing the ``later-is-better\" principle, \\textit{FedABC} adaptively adjusts\nthe client selection threshold, encouraging greater participation in later\ntraining stages. Extensive simulations on CIFAR-10 demonstrate that\n\\textit{FedABC} significantly outperforms existing approaches in model accuracy\nand client participation efficiency, achieving comparable performance with 32\\%\nfewer clients than the classical FL algorithm \\textit{FedAvg}, and 3.5\\% higher\naccuracy with 2\\% fewer clients than the state-of-the-art. This work marks a\nstep toward deploying FL in heterogeneous, resource-constrained environments,\nthereby supporting native AI capabilities in 6G networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFedABC\uff0c\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f18\u5316\u95ee\u9898\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u548c\u5ba2\u6237\u7aef\u53c2\u4e0e\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u5ba2\u6237\u7aef\u6548\u7387\uff0c\u4ee5\u652f\u63016G\u7f51\u7edc\u4e2d\u7684\u672c\u5730AI\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u57286G\u7f51\u7edc\u4e2d\u652f\u6301\u672c\u5730AI\u9762\u4e34\u6311\u6218\uff1a\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u6536\u655b\u6162\u3001\u6a21\u578b\u51c6\u786e\u6027\u964d\u4f4e\uff1b\u9891\u7e41\u7684\u5ba2\u6237\u7aef\u53c2\u4e0e\u9020\u6210\u901a\u4fe1\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faFedABC\u5ba2\u6237\u7aef\u9009\u62e9\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u53d7\u6ce8\u610f\u529b\u673a\u5236\u542f\u53d1\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u76f8\u4f3c\u6027\u548c\u5176\u5bf9\u5168\u5c40\u6a21\u578b\u7684\u72ec\u7279\u8d21\u732e\u6765\u9009\u62e9\u6709\u4fe1\u606f\u91cf\u7684\u5ba2\u6237\u7aef\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u6784\u5efa\u4f18\u5316\u95ee\u9898\u6307\u5bfc\u8bad\u7ec3\uff0c\u5e76\u9075\u5faa\u201c\u540e\u671f\u66f4\u597d\u201d\u539f\u5219\u81ea\u9002\u5e94\u8c03\u6574\u9009\u62e9\u9608\u503c\uff0c\u9f13\u52b1\u540e\u671f\u66f4\u591a\u5ba2\u6237\u7aef\u53c2\u4e0e\u3002", "result": "\u5728CIFAR-10\u4e0a\u7684\u4eff\u771f\u663e\u793a\uff0cFedABC\u5728\u6a21\u578b\u51c6\u786e\u6027\u548c\u5ba2\u6237\u7aef\u53c2\u4e0e\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4e0e\u7ecf\u5178FedAvg\u76f8\u6bd4\uff0c\u5ba2\u6237\u7aef\u6570\u91cf\u51cf\u5c1132%\u65f6\u6027\u80fd\u76f8\u5f53\uff1b\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5ba2\u6237\u7aef\u51cf\u5c112%\u7684\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad83.5%\u3002", "conclusion": "\u672c\u5de5\u4f5c\u662f\u63a8\u52a8\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4ece\u800c\u6709\u6548\u652f\u63016G\u7f51\u7edc\u4e2d\u7684\u672c\u5730AI\u80fd\u529b\u3002"}}
{"id": "2507.19882", "pdf": "https://arxiv.org/pdf/2507.19882", "abs": "https://arxiv.org/abs/2507.19882", "authors": ["Xinshu Li", "Ruoyu Wang", "Erdun Gao", "Mingming Gong", "Lina Yao"], "title": "Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation", "categories": ["cs.AI"], "comment": null, "summary": "Prompt learning has garnered attention for its efficiency over traditional\nmodel training and fine-tuning. However, existing methods, constrained by\ninadequate theoretical foundations, encounter difficulties in achieving\ncausally invariant prompts, ultimately falling short of capturing robust\nfeatures that generalize effectively across categories. To address these\nchallenges, we introduce the $\\textit{\\textbf{DiCap}}$ model, a theoretically\ngrounded $\\textbf{Di}$ffusion-based $\\textbf{C}$ounterf$\\textbf{a}$ctual\n$\\textbf{p}$rompt learning framework, which leverages a diffusion process to\niteratively sample gradients from the marginal and conditional distributions of\nthe causal model, guiding the generation of counterfactuals that satisfy the\nminimal sufficiency criterion. Grounded in rigorous theoretical derivations,\nthis approach guarantees the identifiability of counterfactual outcomes while\nimposing strict bounds on estimation errors. We further employ a contrastive\nlearning framework that leverages the generated counterfactuals, thereby\nenabling the refined extraction of prompts that are precisely aligned with the\ncausal features of the data. Extensive experimental results demonstrate that\nour method performs excellently across tasks such as image classification,\nimage-text retrieval, and visual question answering, with particularly strong\nadvantages in unseen categories.", "AI": {"tldr": "DiCap\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u514b\u670d\u4e86\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u56e0\u679c\u4e0d\u53d8\u6027\u548c\u8de8\u7c7b\u522b\u6cdb\u5316\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u8db3\u591f\u7684\u7406\u8bba\u57fa\u7840\uff0c\u96be\u4ee5\u5b9e\u73b0\u56e0\u679c\u4e0d\u53d8\u7684\u63d0\u793a\uff0c\u4ece\u800c\u672a\u80fd\u6355\u83b7\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u7c7b\u522b\u7684\u9c81\u68d2\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86DiCap\u6a21\u578b\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u5b66\u4e60\u6846\u67b6\u3002\u5b83\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4ece\u56e0\u679c\u6a21\u578b\u7684\u8fb9\u9645\u548c\u6761\u4ef6\u5206\u5e03\u4e2d\u8fed\u4ee3\u91c7\u6837\u68af\u5ea6\uff0c\u751f\u6210\u6ee1\u8db3\u6700\u5c0f\u5145\u5206\u6027\u51c6\u5219\u7684\u53cd\u4e8b\u5b9e\u3002\u8be5\u65b9\u6cd5\u6709\u4e25\u683c\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u786e\u4fdd\u53cd\u4e8b\u5b9e\u7ed3\u679c\u7684\u53ef\u8bc6\u522b\u6027\u5e76\u9650\u5236\u4f30\u8ba1\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u7cbe\u786e\u63d0\u53d6\u4e0e\u6570\u636e\u56e0\u679c\u7279\u5f81\u5bf9\u9f50\u7684\u63d0\u793a\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u6587\u68c0\u7d22\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u672a\u89c1\u7c7b\u522b\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "DiCap\u901a\u8fc7\u5f15\u5165\u7406\u8bba\u57fa\u7840\u548c\u6269\u6563-\u53cd\u4e8b\u5b9e-\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u56e0\u679c\u4e0d\u53d8\u6027\u548c\u8de8\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b0\u9896\u6570\u636e\u65b9\u9762\u5177\u6709\u5f3a\u5927\u4f18\u52bf\u3002"}}
{"id": "2507.19705", "pdf": "https://arxiv.org/pdf/2507.19705", "abs": "https://arxiv.org/abs/2507.19705", "authors": ["Asmae Lamsaf", "Lucia Cascone", "Hugo Proen\u00e7a", "Jo\u00e3o Neves"], "title": "Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute", "categories": ["cs.CV"], "comment": null, "summary": "Bias analysis for synthetic face detection is bound to become a critical\ntopic in the coming years. Although many detection models have been developed\nand several datasets have been released to reliably identify synthetic content,\none crucial aspect has been largely overlooked: these models and training\ndatasets can be biased, leading to failures in detection for certain\ndemographic groups and raising significant social, legal, and ethical issues.\nIn this work, we introduce an evaluation framework to contribute to the\nanalysis of bias of synthetic face detectors with respect to several facial\nattributes. This framework exploits synthetic data generation, with evenly\ndistributed attribute labels, for mitigating any skew in the data that could\notherwise influence the outcomes of bias analysis. We build on the proposed\nframework to provide an extensive case study of the bias level of five\nstate-of-the-art detectors in synthetic datasets with 25 controlled facial\nattributes. While the results confirm that, in general, synthetic face\ndetectors are biased towards the presence/absence of specific facial\nattributes, our study also sheds light on the origins of the observed bias\nthrough the analysis of the correlations with the balancing of facial\nattributes in the training sets of the detectors, and the analysis of detectors\nactivation maps in image pairs with controlled attribute modifications.", "AI": {"tldr": "\u5206\u6790\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u53ca\u5176\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u5b58\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u5bf9\u7279\u5b9a\u4eba\u7fa4\u7684\u68c0\u6d4b\u5931\u8d25\uff0c\u4ece\u800c\u5f15\u53d1\u793e\u4f1a\u3001\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\uff0c\u4f46\u8fd9\u4e00\u70b9\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u751f\u6210\u5747\u5300\u5206\u5e03\u5c5e\u6027\u6807\u7b7e\u7684\u5408\u6210\u6570\u636e\u6765\u5206\u6790\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u3002\u901a\u8fc7\u5bf9\u4e94\u79cdSOTA\u68c0\u6d4b\u5668\u572825\u79cd\u53d7\u63a7\u9762\u90e8\u5c5e\u6027\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u7ed3\u5408\u8bad\u7ec3\u96c6\u5c5e\u6027\u5e73\u8861\u5206\u6790\u548c\u6fc0\u6d3b\u56fe\u5206\u6790\u6765\u63ed\u793a\u504f\u89c1\u6765\u6e90\u3002", "result": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u666e\u904d\u5bf9\u7279\u5b9a\u9762\u90e8\u5c5e\u6027\u7684\u5b58\u5728/\u7f3a\u5931\u5b58\u5728\u504f\u89c1\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u89c2\u6d4b\u5230\u7684\u504f\u89c1\u6765\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u4eba\u8138\u5c5e\u6027\u7684\u4e0d\u5e73\u8861\u4ee5\u53ca\u68c0\u6d4b\u5668\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u662f\u4e00\u4e2a\u666e\u904d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5176\u6839\u6e90\u5728\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5c5e\u6027\u7684\u4e0d\u5e73\u8861\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u516c\u5e73\u3001\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.19525", "pdf": "https://arxiv.org/pdf/2507.19525", "abs": "https://arxiv.org/abs/2507.19525", "authors": ["Chenchen Zhao", "Zhengyuan Shi", "Xiangyu Wen", "Chengjie Liu", "Yi Liu", "Yunhao Zhou", "Yuxiang Zhao", "Hefei Feng", "Yinan Zhu", "Gwok-Waa Wan", "Xin Cheng", "Weiyu Chen", "Yongqi Fu", "Chujie Chen", "Chenhao Xue", "Guangyu Sun", "Ying Wang", "Yibo Lin", "Jun Yang", "Ning Xu", "Xi Wang", "Qiang Xu"], "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 1 figure, 5 tables. To appear in ICCAD 2025", "summary": "The emergence of multimodal large language models (MLLMs) presents promising\nopportunities for automation and enhancement in Electronic Design Automation\n(EDA). However, comprehensively evaluating these models in circuit design\nremains challenging due to the narrow scope of existing benchmarks. To bridge\nthis gap, we introduce MMCircuitEval, the first multimodal benchmark\nspecifically designed to assess MLLM performance comprehensively across diverse\nEDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer\n(QA) pairs spanning digital and analog circuits across critical EDA stages -\nranging from general knowledge and specifications to front-end and back-end\ndesign. Derived from textbooks, technical question banks, datasheets, and\nreal-world documentation, each QA pair undergoes rigorous expert review for\naccuracy and relevance. Our benchmark uniquely categorizes questions by design\nstage, circuit type, tested abilities (knowledge, comprehension, reasoning,\ncomputation), and difficulty level, enabling detailed analysis of model\ncapabilities and limitations. Extensive evaluations reveal significant\nperformance gaps among existing LLMs, particularly in back-end design and\ncomplex computations, highlighting the critical need for targeted training\ndatasets and modeling approaches. MMCircuitEval provides a foundational\nresource for advancing MLLMs in EDA, facilitating their integration into\nreal-world circuit design workflows. Our benchmark is available at\nhttps://github.com/cure-lab/MMCircuitEval.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u72ed\u7a84\u7684\u95ee\u9898\uff0c\u672c\u6587\u63a8\u51fa\u4e86MMCircuitEval\uff0c\u9996\u4e2a\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4e2d\u6027\u80fd\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u540e\u7aef\u8bbe\u8ba1\u548c\u590d\u6742\u8ba1\u7b97\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7535\u8def\u8bbe\u8ba1\u9886\u57df\u7684\u8bc4\u4f30\u57fa\u51c6\u8303\u56f4\u8fc7\u4e8e\u72ed\u7a84\uff0c\u96be\u4ee5\u5168\u9762\u8861\u91cf\u5176\u5728\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165MMCircuitEval\uff0c\u4e00\u4e2a\u5305\u542b3614\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u6db5\u76d6\u6570\u5b57\u548c\u6a21\u62df\u7535\u8def\u7684\u5404\u9879EDA\u9636\u6bb5\u3002\u8fd9\u4e9b\u95ee\u9898\u6765\u81ea\u6559\u6750\u3001\u6280\u672f\u9898\u5e93\u548c\u5b9e\u9645\u6587\u6863\uff0c\u7ecf\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\uff0c\u5e76\u6309\u8bbe\u8ba1\u9636\u6bb5\u3001\u7535\u8def\u7c7b\u578b\u3001\u6d4b\u8bd5\u80fd\u529b\u548c\u96be\u5ea6\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5bf9\u73b0\u6709LLMs\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u540e\u7aef\u8bbe\u8ba1\u548c\u590d\u6742\u8ba1\u7b97\u65b9\u9762\uff0c\u8fd9\u7a81\u663e\u4e86\u5bf9\u76ee\u6807\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5efa\u6a21\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "conclusion": "MMCircuitEval\u4e3a\u63a8\u8fdbMLLMs\u5728EDA\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5176\u6574\u5408\u5230\u771f\u5b9e\u7684\u7535\u8def\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2507.19786", "pdf": "https://arxiv.org/pdf/2507.19786", "abs": "https://arxiv.org/abs/2507.19786", "authors": ["Tianxiang Chen", "Zhentao Tan", "Xiaofan Bo", "Yue Wu", "Tao Gong", "Qi Chu", "Jieping Ye", "Nenghai Yu"], "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale", "categories": ["cs.CL"], "comment": null, "summary": "Effectively handling long contexts is challenging for Large Language Models\n(LLMs) due to the rarity of long texts, high computational demands, and\nsubstantial forgetting of short-context abilities. Recent approaches have\nattempted to construct long contexts for instruction tuning, but these methods\noften require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present\nlong-context LLMs remains significant. In this paper, we introduce Flora, an\neffortless (human/LLM-free) long-context construction strategy. Flora can\nmarkedly enhance the long-context performance of LLMs by arbitrarily assembling\nshort instructions based on categories and instructing LLMs to generate\nresponses based on long-context meta-instructions. This enables Flora to\nproduce contexts of arbitrary length and scale with rich diversity, while only\nslightly compromising short-context performance. Experiments on\nLlama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three\nlong-context benchmarks while maintaining strong performances in short-context\ntasks. Our data-construction code is available at\n\\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.", "AI": {"tldr": "Flora\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5/LLM\u5e72\u9884\u7684\u957f\u6587\u672c\u6784\u5efa\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347LLM\u957f\u6587\u672c\u80fd\u529b\u5e76\u4fdd\u6301\u77ed\u6587\u672c\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u957f\u6587\u672c\u7a00\u7f3a\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u4ee5\u53ca\u77ed\u671f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u663e\u8457\u9057\u5fd8\u3002\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u6784\u5efa\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff08\u9700LLM\u6216\u4eba\u5de5\u5e72\u9884\uff09\uff0c\u4e14\u5728\u957f\u5ea6\u548c\u591a\u6837\u6027\u4e0a\u53d7\u9650\uff0c\u540c\u65f6\u5bfc\u81f4LLMs\u77ed\u671f\u4e0a\u4e0b\u6587\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51faFlora\u7b56\u7565\uff0c\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6216LLM\u5e72\u9884\u7684\u957f\u4e0a\u4e0b\u6587\u6784\u5efa\u65b9\u6cd5\u3002Flora\u901a\u8fc7\u4efb\u610f\u7ec4\u5408\u57fa\u4e8e\u7c7b\u522b\u7684\u77ed\u6307\u4ee4\uff0c\u5e76\u6307\u793aLLMs\u57fa\u4e8e\u957f\u4e0a\u4e0b\u6587\u5143\u6307\u4ee4\u751f\u6210\u54cd\u5e94\uff0c\u4ece\u800c\u751f\u6210\u4efb\u610f\u957f\u5ea6\u3001\u89c4\u6a21\u548c\u4e30\u5bcc\u591a\u6837\u6027\u7684\u957f\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u5728Llama3-8B-Instruct\u548cQwQ-32B\u6a21\u578b\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793a\u7ecfFlora\u589e\u5f3a\u7684LLMs\u5728\u4e09\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "Flora\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u6784\u5efa\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLMs\u5904\u7406\u957f\u6587\u672c\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4ec5\u8f7b\u5fae\u727a\u7272\u77ed\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5f53\u524dLLMs\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e0a\u7684\u5173\u952e\u75db\u70b9\u3002"}}
{"id": "2507.20971", "pdf": "https://arxiv.org/pdf/2507.20971", "abs": "https://arxiv.org/abs/2507.20971", "authors": ["Cl\u00e1udio Modesto", "Jo\u00e3o Borges", "Cleverson Nahum", "Lucas Matni", "Cristiano Bonato Both", "Kleber Cardoso", "Glauco Gon\u00e7alves", "Ilan Correa", "Silvia Lins", "Andrey Silva", "Aldebaro Klautau"], "title": "Towards a Robust Transport Network With Self-adaptive Network Digital Twin", "categories": ["cs.NI"], "comment": "19 pages, 10 figures, and 6 tables", "summary": "The ability of the network digital twin (NDT) to remain aware of changes in\nits physical counterpart, known as the physical twin (PTwin), is a fundamental\ncondition to enable timely synchronization, also referred to as twinning. In\nthis way, considering a transport network, a key requirement is to handle\nunexpected traffic variability and dynamically adapt to maintain optimal\nperformance in the associated virtual model, known as the virtual twin (VTwin).\nIn this context, we propose a self-adaptive implementation of a novel NDT\narchitecture designed to provide accurate delay predictions, even under\nfluctuating traffic conditions. This architecture addresses an essential\nchallenge, underexplored in the literature: improving the resilience of\ndata-driven NDT platforms against traffic variability and improving\nsynchronization between the VTwin and its physical counterpart. Therefore, the\ncontributions of this article rely on NDT lifecycle by focusing on the\noperational phase, where telemetry modules are used to monitor incoming\ntraffic, and concept drift detection techniques guide retraining decisions\naimed at updating and redeploying the VTwin when necessary. We validate our\narchitecture with a network management use case, across various emulated\nnetwork topologies, and diverse traffic patterns to demonstrate its\neffectiveness in preserving acceptable performance and predicting per-flow\ndelay under unexpected traffic variation. The results in all tested topologies,\nusing the normalized mean square error as the evaluation metric, demonstrate\nthat our proposed architecture, after a traffic concept drift, achieves a\nperformance improvement in prediction of at least 56.7% compared to a\nconfiguration without NDT synchronization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\uff08NDT\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u548c\u518d\u8bad\u7ec3\uff0c\u5728\u6d41\u91cf\u6ce2\u52a8\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u65f6\u5ef6\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5145\u5206\u89e3\u51b3\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\uff08NDT\uff09\u5728\u6d41\u91cf\u591a\u53d8\u73af\u5883\u4e0b\u4fdd\u6301\u4e0e\u5176\u7269\u7406\u5bf9\u5e94\u7269\uff08PTwin\uff09\u540c\u6b65\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5982\u4f55\u5e94\u5bf9\u610f\u5916\u6d41\u91cf\u53d8\u5316\uff0c\u4ee5\u7ef4\u6301\u865a\u62df\u6a21\u578b\uff08VTwin\uff09\u7684\u6700\u4f73\u6027\u80fd\u3002\u63d0\u9ad8\u6570\u636e\u9a71\u52a8\u578bNDT\u5e73\u53f0\u62b5\u5fa1\u6d41\u91cf\u6ce2\u52a8\u548c\u6539\u5584VTwin\u4e0ePTwin\u4e4b\u95f4\u540c\u6b65\u7684\u97e7\u6027\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\u67b6\u6784\u3002\u8be5\u67b6\u6784\u5229\u7528\u9065\u6d4b\u6a21\u5757\u76d1\u63a7\u5165\u7ad9\u6d41\u91cf\uff0c\u5e76\u91c7\u7528\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u6280\u672f\u6765\u6307\u5bfc\u5728\u5fc5\u8981\u65f6\u66f4\u65b0\u548c\u91cd\u65b0\u90e8\u7f72VTwin\u7684\u518d\u8bad\u7ec3\u51b3\u7b56\u3002\u901a\u8fc7\u7f51\u7edc\u7ba1\u7406\u7528\u4f8b\uff0c\u5728\u5404\u79cd\u4eff\u771f\u7f51\u7edc\u62d3\u6251\u548c\u4e0d\u540c\u6d41\u91cf\u6a21\u5f0f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u62d3\u6251\u4e2d\uff0c\u4ee5\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u672c\u6587\u63d0\u51fa\u7684\u67b6\u6784\u5728\u6d41\u91cf\u6982\u5ff5\u6f02\u79fb\u540e\uff0c\u9884\u6d4b\u6027\u80fd\u6bd4\u6ca1\u6709NDT\u540c\u6b65\u7684\u914d\u7f6e\u81f3\u5c11\u63d0\u9ad8\u4e8656.7%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\u67b6\u6784\u80fd\u591f\u6709\u6548\u5730\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u5e76\u5728\u610f\u5916\u6d41\u91cf\u53d8\u5316\u4e0b\u9884\u6d4b\u6bcf\u6d41\u65f6\u5ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u9a71\u52a8\u578bNDT\u5e73\u53f0\u5728\u52a8\u6001\u6d41\u91cf\u73af\u5883\u4e0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u97e7\u6027\u3002"}}
{"id": "2507.19960", "pdf": "https://arxiv.org/pdf/2507.19960", "abs": "https://arxiv.org/abs/2507.19960", "authors": ["Olivia Guest"], "title": "What Does 'Human-Centred AI' Mean?", "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "comment": null, "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes.", "AI": {"tldr": "\u8be5\u6587\u5c06AI\u5b9a\u4e49\u4e3a\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u7684\u5f71\u54cd\u7c7b\u578b\u3002\u4f5c\u8005\u5f3a\u8c03\uff0c\u7406\u89e3AI\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5185\u5728\u8054\u7cfb\u5bf9\u4e8e\u771f\u6b63\u4ee5\u4eba\u4e3a\u672c\u7684AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684AI\u89c6\u89d2\uff0c\u5373AI\u662f\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5176\u52a8\u673a\u5728\u4e8e\u7ea0\u6b63\u5bf9AI\u7684\u8bef\u89e3\uff0c\u5f3a\u8c03AI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u5ffd\u89c6\u8fd9\u79cd\u5173\u8054\u4f1a\u963b\u788d\u4ee5\u4eba\u4e3a\u672c\u7684AI\u53d1\u5c55\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5217\u4e3e\u5177\u4f53\u4f8b\u5b50\uff08\u5982\u7b97\u76d8\u5bf9\u5fc3\u7b97\u3001\u95f9\u949f\u5bf9\u53eb\u9192\u5458\u7b49\uff09\u6765\u5bf9\u6bd4\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4efb\u52a1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4f5c\u8005\u8fd0\u7528\u65b0\u9896\u7684\u5b9a\u4e49\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u793e\u4f1a\u6280\u672f\u5173\u7cfb\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc4\u4f30\u5176\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u5c06AI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u7684\u5f71\u54cd\u5206\u4e3a\u4e09\u7c7b\uff1a\u6709\u5bb3\u7684\u201c\u53d6\u4ee3\u201d\uff08displacement\uff09\u3001\u6709\u76ca\u7684\u201c\u589e\u5f3a\u201d\uff08enhancement\uff09\u548c\u4e2d\u6027\u7684\u201c\u66ff\u6362\u201d\uff08replacement\uff09\u3002\u6587\u7ae0\u6307\u51fa\uff0c\u6240\u6709AI\u90fd\u4e0d\u53ef\u907f\u514d\u5730\u6d89\u53ca\u4eba\u7c7b\u8ba4\u77e5\u3002", "conclusion": "\u6df7\u6dc6AI\u4e0e\u8ba4\u77e5\u7684\u5173\u7cfb\u4f1a\u5bfc\u81f4\u5bf9AI\u7684\u8bef\u8bfb\u548c\u6279\u5224\u6027\u601d\u8003\u7684\u7f3a\u5931\uff0c\u4ece\u800c\u9650\u5236\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u3002\u4f5c\u8005\u5f3a\u8c03\uff0c\u8981\u201c\u53bb\u795e\u79d8\u5316\u201dAI\uff0c\u5fc5\u987b\u6b63\u89c6\u201c\u4eba\u673a\u534f\u4f5c\u201d\u4e2d\u7684\u4eba\u7c7b\u89d2\u8272\u3002"}}
{"id": "2507.19730", "pdf": "https://arxiv.org/pdf/2507.19730", "abs": "https://arxiv.org/abs/2507.19730", "authors": ["Liyang Wang", "Shiqian Wu", "Shun Fang", "Qile Zhu", "Jiaxin Wu", "Sos Again"], "title": "Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Moving target detection is a challenging computer vision task aimed at\ngenerating accurate segmentation maps in diverse in-the-wild color videos\ncaptured by static cameras. If backgrounds and targets can be simultaneously\nextracted and recombined, such synthetic data can significantly enrich\nannotated in-the-wild datasets and enhance the generalization ability of deep\nmodels. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for\ncolor image processing. However, in color video processing, Quaternion Singular\nValue Decomposition (QSVD) incurs high computational costs, and rank-1\nquaternion matrix fails to yield rank-1 color channels. In this paper, we\nreduce the computational complexity of QSVD to o(1) by utilizing a quaternion\nRiemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)\nframework, which achieves a balance in simultaneously segmenting targets and\nrecovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by\nintroducing the Color Rank-1 Batch (CR1B) method to further process and obtain\nthe ideal low-rank background across color channels. Experiments demonstrate\nour uQRPCA+ achieves State Of The Art (SOTA) performance on moving target\ndetection and background recovery tasks compared to existing open-source\nmethods. Our implementation is publicly available on GitHub at\nhttps://github.com/Ruchtech/uQRPCA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3auQRPCA\u53ca\u5176\u589e\u5f3a\u7248uQRPCA+\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u56db\u5143\u6570\u5947\u5f02\u503c\u5206\u89e3\uff08QSVD\uff09\u5e76\u5f15\u5165\u989c\u8272\u79e91\u6279\u5904\u7406\uff08CR1B\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f69\u8272\u89c6\u9891\u4e2d\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u6062\u590d\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u5728\u9759\u6001\u6444\u50cf\u673a\u6355\u83b7\u7684\u590d\u6742\u5f69\u8272\u89c6\u9891\u4e2d\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u73b0\u6709\u57fa\u4e8e\u56db\u5143\u6570\u7684\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08QRPCA\uff09\u5728\u5904\u7406\u5f69\u8272\u89c6\u9891\u65f6\u9762\u4e34\u4e24\u5927\u95ee\u9898\uff1a\u56db\u5143\u6570\u5947\u5f02\u503c\u5206\u89e3\uff08QSVD\uff09\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u79e91\u56db\u5143\u6570\u77e9\u9635\u65e0\u6cd5\u751f\u6210\u79e91\u5f69\u8272\u901a\u9053\u3002\u540c\u65f6\u63d0\u53d6\u80cc\u666f\u548c\u76ee\u6807\u4ee5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5bf9\u4e8e\u4e30\u5bcc\u6807\u6ce8\u6570\u636e\u96c6\u548c\u589e\u5f3a\u6df1\u5ea6\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u6280\u672f\u969c\u788d\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u8fdb\u884c\uff1a1. \u5229\u7528\u56db\u5143\u6570\u9ece\u66fc\u6d41\u5f62\u5c06QSVD\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u81f3o(1)\u30022. \u63d0\u51fa\u4e86\u901a\u7528QRPCA (uQRPCA) \u6846\u67b6\uff0c\u65e8\u5728\u540c\u65f6\u5e73\u8861\u5f69\u8272\u89c6\u9891\u4e2d\u7684\u76ee\u6807\u5206\u5272\u548c\u80cc\u666f\u6062\u590d\u30023. \u8fdb\u4e00\u6b65\u5f15\u5165\u989c\u8272\u79e91\u6279\u5904\u7406 (CR1B) \u65b9\u6cd5\uff0c\u5c06uQRPCA\u6269\u5c55\u4e3auQRPCA+\uff0c\u4ee5\u5904\u7406\u5e76\u83b7\u5f97\u8de8\u989c\u8272\u901a\u9053\u7684\u7406\u60f3\u4f4e\u79e9\u80cc\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u5f00\u6e90\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f5c\u8005\u63d0\u51fa\u7684uQRPCA+\u5728\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u6062\u590d\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u901a\u8fc7\u4f18\u5316QSVD\u8ba1\u7b97\u548c\u63d0\u51fauQRPCA/uQRPCA+\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f69\u8272\u89c6\u9891\u4e2d\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u6062\u590d\u7684\u96be\u9898\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6848\uff0c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u548c\u63d0\u5347\u6df1\u5ea6\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.19526", "pdf": "https://arxiv.org/pdf/2507.19526", "abs": "https://arxiv.org/abs/2507.19526", "authors": ["Jianyuan Bo", "Hao Wu", "Yuan Fang"], "title": "Quantizing Text-attributed Graphs for Semantic-Structural Integration", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at KDD'2025", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation for\nmodeling complex relationships across diverse domains. With the rise of large\nlanguage models (LLMs), there is growing interest in leveraging their\ncapabilities for graph learning. However, current approaches face significant\nchallenges in embedding structural information into LLM-compatible formats,\nrequiring either computationally expensive alignment mechanisms or manual graph\nverbalization techniques that often lose critical structural details. Moreover,\nthese methods typically require labeled data from source domains for effective\ntransfer learning, significantly constraining their adaptability. We propose\nSTAG, a novel self-supervised framework that directly quantizes graph\nstructural information into discrete tokens using a frozen codebook. Unlike\ntraditional quantization approaches, our method employs soft assignment and KL\ndivergence guided quantization to address the unique challenges of graph data,\nwhich lacks natural tokenization structures. Our framework enables both\nLLM-based and traditional learning approaches, supporting true zero-shot\ntransfer learning without requiring labeled data even in the source domain.\nExtensive experiments demonstrate state-of-the-art performance across multiple\nnode classification benchmarks while maintaining compatibility with different\nLLM architectures, offering an elegant solution to bridging graph learning with\nLLMs.", "AI": {"tldr": "STAG\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u56fe\u7ed3\u6784\u4e3a\u79bb\u6563token\uff0c\u5b9e\u73b0\u56fe\u5b66\u4e60\u4e0eLLM\u7684\u65e0\u7f1d\u7ed3\u5408\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5e76\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u5d4c\u5165\u5230LLM\u517c\u5bb9\u683c\u5f0f\u65f6\u9762\u4e34\u6311\u6218\uff1a\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4fe1\u606f\u4e22\u5931\u3001\u4e14\u9700\u8981\u6e90\u57df\u7684\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSTAG\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u7801\u672c\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u76f4\u63a5\u91cf\u5316\u4e3a\u79bb\u6563token\u3002\u91c7\u7528\u8f6f\u5206\u914d\u548cKL\u6563\u5ea6\u5f15\u5bfc\u91cf\u5316\uff0c\u4ee5\u89e3\u51b3\u56fe\u6570\u636e\u7f3a\u4e4f\u81ea\u7136token\u5316\u7ed3\u6784\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u652f\u6301LLM\u548c\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "STAG\u5728\u591a\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4e0d\u540cLLM\u67b6\u6784\u517c\u5bb9\u3002", "conclusion": "STAG\u4e3a\u5f25\u5408\u56fe\u5b66\u4e60\u548cLLMs\u4e4b\u95f4\u7684\u9e3f\u6c9f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f18\u96c5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u56fe\u6570\u636e\u5904\u7406\u3002"}}
{"id": "2507.19823", "pdf": "https://arxiv.org/pdf/2507.19823", "abs": "https://arxiv.org/abs/2507.19823", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.", "AI": {"tldr": "HCAttention\u662f\u4e00\u79cd\u5f02\u6784\u6ce8\u610f\u529b\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u952e\u91cf\u5316\u3001\u503c\u5378\u8f7d\u548c\u52a8\u6001KV\u9010\u51fa\uff0c\u663e\u8457\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65f6\u7684KV\u7f13\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u5728\u6781\u7aef\u5185\u5b58\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u5904\u7406400\u4e07tokens\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u65f6\uff0c\u5176\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u9700\u8981\u5de8\u5927\u7684\u5185\u5b58\uff0c\u6784\u6210\u663e\u8457\u6311\u6218\u3002\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5728\u5185\u5b58\u51cf\u5c11\u8d85\u8fc785%\u65f6\u4f1a\u8868\u73b0\u51fa\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u5229\u7528GPU-CPU\u534f\u540c\u8fdb\u884c\u8fd1\u4f3c\u6ce8\u610f\u529b\u7684\u7b56\u7565\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86HCAttention\uff0c\u4e00\u4e2a\u5f02\u6784\u6ce8\u610f\u529b\u8ba1\u7b97\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u952e\u91cf\u5316\u3001\u503c\u5378\u8f7d\u548c\u52a8\u6001KV\u9010\u51fa\u6280\u672f\uff0c\u4ee5\u5728\u6781\u7aef\u5185\u5b58\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002HCAttention\u517c\u5bb9\u73b0\u6709Transformer\u67b6\u6784\uff0c\u4e14\u65e0\u9700\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728LongBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHCAttention\u5728\u5c06KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u7f29\u5c0f\u5230\u539f\u59cb\u5927\u5c0f\u768425%\u65f6\uff0c\u4ecd\u80fd\u4fdd\u6301\u5168\u6ce8\u610f\u529b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u5373\u4f7f\u5728\u4ec5\u4f7f\u752812.5%\u7f13\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u8868\u73b0\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u5e76\u5728LLM KV\u7f13\u5b58\u538b\u7f29\u9886\u57df\u8fbe\u5230\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002HCAttention\u9996\u6b21\u6210\u529f\u5c06Llama-3-8B\u6a21\u578b\u6269\u5c55\u5230\u5728\u5355\u5f2080GB\u5185\u5b58\u7684A100 GPU\u4e0a\u5904\u7406400\u4e07\u4e2atokens\u3002", "conclusion": "HCAttention\u6210\u529f\u5730\u89e3\u51b3\u4e86LLM\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684KV\u7f13\u5b58\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u5728\u5927\u5e45\u538b\u7f29\u5185\u5b58\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u5e76\u5728KV\u7f13\u5b58\u538b\u7f29\u9886\u57df\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002"}}
{"id": "2507.19550", "pdf": "https://arxiv.org/pdf/2507.19550", "abs": "https://arxiv.org/abs/2507.19550", "authors": ["Awid Vaziry", "Sandro Rodriguez Garzon", "Axel K\u00fcpper"], "title": "Towards Multi-Agent Economies: Enhancing the A2A Protocol with Ledger-Anchored Identities and x402 Micropayments for AI Agents", "categories": ["cs.MA", "cs.NI"], "comment": null, "summary": "This research article presents a novel architecture to empower multi-agent\neconomies by addressing two critical limitations of the emerging Agent2Agent\n(A2A) communication protocol: decentralized agent discoverability and\nagent-to-agent micropayments. By integrating distributed ledger technology\n(DLT), this architecture enables tamper-proof, on-chain publishing of\nAgentCards as smart contracts, providing secure and verifiable agent\nidentities. The architecture further extends A2A with the x402 open standard,\nfacilitating blockchain-agnostic, HTTP-based micropayments via the HTTP 402\nstatus code. This enables autonomous agents to seamlessly discover,\nauthenticate, and compensate each other across organizational boundaries. This\nwork further presents a comprehensive technical implementation and evaluation,\ndemonstrating the feasibility of DLT-based agent discovery and micropayments.\nThe proposed approach lays the groundwork for secure, scalable, and\neconomically viable multi-agent ecosystems, advancing the field of agentic AI\ntoward trusted, autonomous economic interactions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408DLT\u548cx402\u6807\u51c6\uff0c\u89e3\u51b3Agent2Agent (A2A) \u534f\u8bae\u4e2d\u4ee3\u7406\u53d1\u73b0\u548c\u5fae\u652f\u4ed8\u7684\u9650\u5236\uff0c\u8d4b\u80fd\u5b89\u5168\u3001\u53ef\u4fe1\u7684\u591a\u4ee3\u7406\u7ecf\u6d4e\u4e92\u52a8\u3002", "motivation": "\u89e3\u51b3\u65b0\u5174Agent2Agent (A2A) \u901a\u4fe1\u534f\u8bae\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u53bb\u4e2d\u5fc3\u5316\u4ee3\u7406\u7684\u53ef\u53d1\u73b0\u6027\uff08decentralized agent discoverability\uff09\u548c\u4ee3\u7406\u95f4\u7684\u5fae\u652f\u4ed8\uff08agent-to-agent micropayments\uff09\u95ee\u9898\uff0c\u4ee5\u8d4b\u80fd\u591a\u4ee3\u7406\u7ecf\u6d4e\u4f53\u3002", "method": "1. \u6574\u5408\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f (DLT)\uff0c\u5c06AgentCards\u4f5c\u4e3a\u667a\u80fd\u5408\u7ea6\u53d1\u5e03\u5230\u94fe\u4e0a\uff0c\u5b9e\u73b0\u9632\u7be1\u6539\u3001\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7406\u8eab\u4efd\u30022. \u6269\u5c55A2A\u534f\u8bae\uff0c\u5f15\u5165x402\u5f00\u653e\u6807\u51c6\uff0c\u901a\u8fc7HTTP 402\u72b6\u6001\u7801\u5b9e\u73b0\u57fa\u4e8eHTTP\u7684\u3001\u4e0e\u533a\u5757\u94fe\u65e0\u5173\u7684\u5fae\u652f\u4ed8\u529f\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u67b6\u6784\u4f7f\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u8de8\u7ec4\u7ec7\u8fb9\u754c\u65e0\u7f1d\u5730\u53d1\u73b0\u3001\u8ba4\u8bc1\u5e76\u76f8\u4e92\u652f\u4ed8\u3002\u901a\u8fc7\u5168\u9762\u7684\u6280\u672f\u5b9e\u73b0\u548c\u8bc4\u4f30\uff0c\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8eDLT\u7684\u4ee3\u7406\u53d1\u73b0\u548c\u5fae\u652f\u4ed8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u5b89\u5168\u3001\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u53ef\u884c\u7684\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4ee3\u7406AI\u9886\u57df\u5411\u53ef\u4fe1\u8d56\u3001\u81ea\u4e3b\u7684\u7ecf\u6d4e\u4e92\u52a8\u53d1\u5c55\u3002"}}
{"id": "2507.19973", "pdf": "https://arxiv.org/pdf/2507.19973", "abs": "https://arxiv.org/abs/2507.19973", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u5fae\u8c03\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u81ea\u52a8\u4eceMRI/CT\u62a5\u544a\u4e2d\u63d0\u53d6\u80f0\u817a\u56ca\u6027\u75c5\u53d8\uff08PCL\uff09\u7279\u5f81\u5e76\u8fdb\u884c\u98ce\u9669\u5206\u7c7b\uff0c\u5176\u6027\u80fd\u4e0eGPT-4o\u76f8\u5f53\u3002", "motivation": "\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u624b\u52a8\u63d0\u53d6\u80f0\u817a\u56ca\u6027\u75c5\u53d8\uff08PCL\uff09\u7279\u5f81\u8017\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7814\u7a76\u7684\u5f00\u5c55\uff0c\u4ece\u800c\u963b\u788d\u4e86PCL\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6536\u96c6\u4e866000\u4efd\u63cf\u8ff0PCL\u7684\u8179\u90e8MRI/CT\u62a5\u544a\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u4f7f\u7528GPT-4o\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u751f\u6210PCL\u7279\u5f81\u548c\u4e3b\u80f0\u7ba1\u7279\u5f81\u7684\u6807\u7b7e\u3002\u4e24\u4e2a\u5f00\u6e90LLM\uff08LLaMA\u548cDeepSeek\uff09\u5728GPT-4o\u751f\u6210\u7684CoT\u6570\u636e\u4e0a\u4f7f\u7528QLoRA\u8fdb\u884c\u4e86\u5fae\u8c03\u3002\u6839\u636e\u673a\u6784\u6307\u5357\u548c2017 ACR\u767d\u76ae\u4e66\uff0c\u5c06\u63d0\u53d6\u7684\u7279\u5f81\u6620\u5c04\u5230\u98ce\u9669\u7c7b\u522b\u3002\u6a21\u578b\u5728285\u4efd\u4eba\u5de5\u6807\u6ce8\u7684\u62a5\u544a\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u7531\u4e09\u540d\u653e\u5c04\u79d1\u533b\u751f\u72ec\u7acb\u5ba1\u67e5\u4e86100\u4e2a\u6848\u4f8b\u7684\u8f93\u51fa\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u3001\u98ce\u9669\u5206\u7c7b\u7684\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u4ee5\u53ca\u653e\u5c04\u79d1\u533b\u751f\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff08Fleiss' Kappa\uff09\u3002", "result": "\u601d\u7ef4\u94fe\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86LLaMA\uff08\u4ece80%\u523097%\uff09\u548cDeepSeek\uff08\u4ece79%\u523098%\uff09\u7684\u7279\u5f81\u63d0\u53d6\u51c6\u786e\u7387\uff0c\u8fbe\u5230\u4e86\u4e0eGPT-4o\uff0897%\uff09\u76f8\u5f53\u7684\u6c34\u5e73\u3002\u98ce\u9669\u5206\u7c7b\u7684F1\u5206\u6570\u4e5f\u6709\u6240\u63d0\u9ad8\uff08LLaMA\uff1a0.95\uff1bDeepSeek\uff1a0.94\uff09\uff0c\u4e0eGPT-4o\uff080.97\uff09\u975e\u5e38\u63a5\u8fd1\uff0c\u4e14\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u5dee\u5f02\u3002\u653e\u5c04\u79d1\u533b\u751f\u95f4\u7684\u4e00\u81f4\u6027\u5f88\u9ad8\uff08Fleiss' Kappa = 0.888\uff09\uff0c\u52a0\u5165DeepSeek-FT-CoT\uff08Fleiss' Kappa = 0.893\uff09\u6216GPT-CoT\uff08Fleiss' Kappa = 0.897\uff09\u540e\uff0c\u4e00\u81f4\u6027\u6ca1\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u6a21\u578b\u8fbe\u5230\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u4e00\u81f4\u7684\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u601d\u7ef4\u94fe\u76d1\u7763\u5fae\u8c03\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u5927\u89c4\u6a21PCL\u7814\u7a76\u63d0\u4f9b\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u9ad8\u6548\u7684\u8868\u578b\u63d0\u53d6\u80fd\u529b\uff0c\u5176\u6027\u80fd\u53ef\u4e0eGPT-4o\u5ab2\u7f8e\u3002"}}
{"id": "2507.19738", "pdf": "https://arxiv.org/pdf/2507.19738", "abs": "https://arxiv.org/abs/2507.19738", "authors": ["Jinsu Yoo", "Sooyoung Jeon", "Zanming Huang", "Tai-Yu Pan", "Wei-Lun Chao"], "title": "Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective", "categories": ["cs.CV"], "comment": null, "summary": "We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to\nimprove stereo matching accuracy by injecting precise LiDAR depth into the\ninitial disparity map. We find that the effectiveness of LiDAR guidance\ndrastically degrades when the LiDAR points become sparse (e.g., a few hundred\npoints per frame), and we offer a novel explanation from a signal processing\nperspective. This insight leads to a surprisingly simple solution that enables\nLiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity\nmap with interpolation. Interestingly, we find that pre-filling is also\neffective when injecting LiDAR depth into image features via early fusion, but\nfor a fundamentally different reason, necessitating a distinct pre-filling\napproach. By combining both solutions, the proposed Guided RAFT-Stereo\n(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under\nsparse LiDAR conditions across various datasets. We hope this study inspires\nmore effective LiDAR-guided stereo methods.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6fc0\u5149\u96f7\u8fbe\u7a00\u758f\u65f6RAFT-Stereo\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u63d2\u503c\u9884\u586b\u5145\u521d\u59cb\u89c6\u5dee\u56fe\u548c\u56fe\u50cf\u7279\u5f81\u6765\u89e3\u51b3\uff0c\u5f62\u6210GRAFT-Stereo\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u4e0b\u7684\u7acb\u4f53\u5339\u914d\u7cbe\u5ea6\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6ce8\u5165\u7cbe\u786e\u7684\u6fc0\u5149\u96f7\u8fbe\u6df1\u5ea6\u4fe1\u606f\u6765\u63d0\u9ad8RAFT-Stereo\u7acb\u4f53\u5339\u914d\u7684\u7cbe\u5ea6\uff0c\u5e76\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\u70b9\u7a00\u758f\u65f6\u5f15\u5bfc\u6548\u679c\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u89e3\u91ca\u4e86\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u5bfc\u81f4\u5f15\u5bfc\u6548\u679c\u4e0b\u964d\u7684\u539f\u56e0\uff1b\u63d0\u51fa\u901a\u8fc7\u63d2\u503c\u9884\u586b\u5145\u7a00\u758f\u521d\u59cb\u89c6\u5dee\u56fe\uff1b\u8fdb\u4e00\u6b65\u53d1\u73b0\u9884\u586b\u5145\u5bf9\u901a\u8fc7\u65e9\u671f\u878d\u5408\u5c06\u6fc0\u5149\u96f7\u8fbe\u6df1\u5ea6\u6ce8\u5165\u56fe\u50cf\u7279\u5f81\u4e5f\u6709\u6548\uff0c\u4f46\u9700\u4e0d\u540c\u65b9\u6cd5\uff1b\u6700\u7ec8\u7ed3\u5408\u4e24\u79cd\u9884\u586b\u5145\u65b9\u6848\uff0c\u63d0\u51faGuided RAFT-Stereo (GRAFT-Stereo)\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684GRAFT-Stereo\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u6761\u4ef6\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6fc0\u5149\u96f7\u8fbe\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u5f15\u5bfc\u7684\u7acb\u4f53\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u89c1\u89e3\u548c\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u542f\u53d1\u66f4\u591a\u9ad8\u6548\u7684\u6fc0\u5149\u96f7\u8fbe\u5f15\u5bfc\u7acb\u4f53\u65b9\u6cd5\u3002"}}
{"id": "2507.19527", "pdf": "https://arxiv.org/pdf/2507.19527", "abs": "https://arxiv.org/abs/2507.19527", "authors": ["Yihan Wang", "Jianing Zhao"], "title": "Research on the application of graph data structure and graph neural network in node classification/clustering tasks", "categories": ["cs.LG"], "comment": null, "summary": "Graph-structured data are pervasive across domains including social networks,\nbiological networks, and knowledge graphs. Due to their non-Euclidean nature,\nsuch data pose significant challenges to conventional machine learning methods.\nThis study investigates graph data structures, classical graph algorithms, and\nGraph Neural Networks (GNNs), providing comprehensive theoretical analysis and\ncomparative evaluation. Through comparative experiments, we quantitatively\nassess performance differences between traditional algorithms and GNNs in node\nclassification and clustering tasks. Results show GNNs achieve substantial\naccuracy improvements of 43% to 70% over traditional methods. We further\nexplore integration strategies between classical algorithms and GNN\narchitectures, providing theoretical guidance for advancing graph\nrepresentation learning research.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fe\u7b97\u6cd5\u80fd\u663e\u8457\u63d0\u9ad843%\u523070%\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u4e8c\u8005\u7684\u96c6\u6210\u7b56\u7565\u3002", "motivation": "\u56fe\u7ed3\u6784\u6570\u636e\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u975e\u6b27\u51e0\u91cc\u5f97\u7279\u6027\u5bf9\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6784\u6210\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u56fe\u6570\u636e\u7ed3\u6784\u3001\u7ecf\u5178\u56fe\u7b97\u6cd5\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff0c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7406\u8bba\u5206\u6790\u548c\u6bd4\u8f83\u8bc4\u4f30\u3002\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u91cf\u5316\u8bc4\u4f30\u4e86\u4f20\u7edf\u7b97\u6cd5\u4e0eGNNs\u5728\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u7ecf\u5178\u7b97\u6cd5\u4e0eGNN\u67b6\u6784\u7684\u96c6\u6210\u7b56\u7565\u3002", "result": "GNNs\u5728\u51c6\u786e\u6027\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u8fbe\u523043%\u81f370%\u3002\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u7ecf\u5178\u7b97\u6cd5\u4e0eGNN\u67b6\u6784\u7684\u96c6\u6210\u7b56\u7565\u3002", "conclusion": "GNNs\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u96c6\u6210\u7b56\u7565\u65b9\u5411\u3002"}}
{"id": "2507.19867", "pdf": "https://arxiv.org/pdf/2507.19867", "abs": "https://arxiv.org/abs/2507.19867", "authors": ["Anshul Chavda", "M Jagadeesh", "Chintalapalli Raja Kullayappa", "B Jayaprakash", "Medchalimi Sruthi", "Pushpak Bhattacharyya"], "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments", "categories": ["cs.CL"], "comment": null, "summary": "In-car conversational AI is becoming increasingly critical as autonomous\nvehicles and smart assistants gain widespread adoption. Yet, existing datasets\nfail to capture the spontaneous disfluencies such as hesitations, false starts,\nrepetitions, and self-corrections that characterize real driver-AI dialogs. To\naddress this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn\ndialogs across seven automotive domains, generated using a two-stage,\nprompt-driven pipeline that dynamically integrates disfluencies during\nsynthesis. We show that DiscoDrive is effective both as a training resource,\nenabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on\nthe MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4\nimprovements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1\nimprovements of 1.35 to 3.48), and as a data augmentation resource in\nlow-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,\nMETEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from\nDiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness\n(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more\ncontext-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and\nserves as a versatile corpus for both training and augmenting conversational\nAI, enabling robust handling of real-world, disfluent in-car interactions.", "AI": {"tldr": "\u5f15\u5165\u4e86DiscoDrive\uff0c\u4e00\u4e2a\u5408\u6210\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u8f66\u8f7d\u5bf9\u8bddAI\u4e2d\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u81ea\u7136\u53e3\u8bed\u4e0d\u6d41\u7545\u73b0\u8c61\u7684\u95ee\u9898\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u8f66\u8f7d\u5bf9\u8bddAI\u6570\u636e\u96c6\u672a\u80fd\u6355\u83b7\u771f\u5b9e\u9a7e\u9a76\u5458-AI\u5bf9\u8bdd\u4e2d\u5e38\u89c1\u7684\u53e3\u8bed\u4e0d\u6d41\u7545\u73b0\u8c61\uff08\u5982\u72b9\u8c6b\u3001\u53e3\u8bef\u3001\u91cd\u590d\u3001\u81ea\u6211\u4fee\u6b63\uff09\uff0c\u8fd9\u963b\u788d\u4e86AI\u5728\u5b9e\u9645\u8f66\u8f7d\u4ea4\u4e92\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86DiscoDrive\uff0c\u4e00\u4e2a\u5305\u542b3500\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u5408\u6210\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u4e03\u4e2a\u6c7d\u8f66\u9886\u57df\u3002\u8be5\u8bed\u6599\u5e93\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u3001\u63d0\u793a\u9a71\u52a8\u7684\u7ba1\u9053\u751f\u6210\uff0c\u5728\u5408\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6574\u5408\u4e86\u53e3\u8bed\u4e0d\u6d41\u7545\u73b0\u8c61\u3002", "result": "1. \u4f5c\u4e3a\u8bad\u7ec3\u8d44\u6e90\uff1a\u4f7fDialoGPT-Medium\u548cT5-Base\u5728MultiWOZ 2.2\u548cSchema-Guided Dialogue (SGD) \u76f8\u5173\u6d4b\u8bd5\u96c6\u4e0a\u5339\u914d\u6216\u8d85\u8d8aKVRET\u8bad\u7ec3\u7684\u6a21\u578b\uff0cBLEU-4\u3001METEOR\u3001ROUGE-L\u548cBERTScore F1\u7b49\u591a\u9879\u6307\u6807\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\n2. \u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u8d44\u6e90\uff1a\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u4e0e10%\u7684KVRET\u6570\u636e\u7ed3\u5408\u65f6\uff0c\u80fd\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002\n3. \u4eba\u5de5\u8bc4\u4f30\uff1aDiscoDrive\u7684\u5bf9\u8bdd\u5728\u81ea\u7136\u5ea6\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u5f97\u5206\u9ad8\u4e8eKVRET\u7684\u4eba\u5de5\u6536\u96c6\u5bf9\u8bdd\uff0c\u5e76\u4e14\u6bd4\u5176\u4ed6\u9886\u5148\u7684\u4e8b\u540e\u5904\u7406\u65b9\u6cd5\uff08\u5982LARD\uff09\u66f4\u5177\u4e0a\u4e0b\u6587\u9002\u5b9c\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6e05\u6670\u5ea6\u3002", "conclusion": "DiscoDrive\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u8bed\u6599\u5e93\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u589e\u5f3a\u5bf9\u8bddAI\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u7a33\u5065\u5730\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e26\u6709\u53e3\u8bed\u4e0d\u6d41\u7545\u73b0\u8c61\u7684\u8f66\u8f7d\u4ea4\u4e92\u3002"}}
{"id": "2507.19667", "pdf": "https://arxiv.org/pdf/2507.19667", "abs": "https://arxiv.org/abs/2507.19667", "authors": ["Niklas Carlsson", "Derek Eager"], "title": "Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies", "categories": ["cs.DC", "cs.NI", "cs.PF", "68M20, 68M01", "C.4; C.5.5"], "comment": "20 pages", "summary": "Cloud computing enables the dynamic provisioning of server resources. To\nexploit this opportunity, a policy is needed for dynamically allocating (and\ndeallocating) servers in response to the current load conditions. In this paper\nwe describe several simple policies for dynamic server allocation and develop\nanalytic models for their analysis. We also design semi-Markov decision models\nthat enable determination of the performance achieved with optimal policies,\nallowing us to quantify the performance gap between simple, easily implemented\npolicies, and optimal policies. Finally, we apply our models to study the\npotential performance benefits of state-dependent routing in multi-site systems\nwhen using dynamic server allocation at each site. Insights from our results\nare valuable to service providers wanting to balance cloud service costs and\ndelays.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e91\u8ba1\u7b97\u4e2d\u52a8\u6001\u670d\u52a1\u5668\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u548c\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u6a21\u578b\u91cf\u5316\u4e86\u7b80\u5355\u7b56\u7565\u4e0e\u6700\u4f18\u7b56\u7565\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u7814\u7a76\u4e86\u591a\u7ad9\u70b9\u7cfb\u7edf\u4e2d\u72b6\u6001\u4f9d\u8d56\u8def\u7531\u7684\u6f5c\u5728\u6536\u76ca\u3002", "motivation": "\u4e91\u8ba1\u7b97\u5141\u8bb8\u52a8\u6001\u5206\u914d\u670d\u52a1\u5668\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u7b56\u7565\u6765\u6839\u636e\u5f53\u524d\u8d1f\u8f7d\u52a8\u6001\u5206\u914d\uff08\u548c\u91ca\u653e\uff09\u670d\u52a1\u5668\uff0c\u4ee5\u5e2e\u52a9\u670d\u52a1\u63d0\u4f9b\u5546\u5e73\u8861\u4e91\u670d\u52a1\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "method": "\u63cf\u8ff0\u4e86\u51e0\u79cd\u7b80\u5355\u7684\u52a8\u6001\u670d\u52a1\u5668\u5206\u914d\u7b56\u7565\u5e76\u5f00\u53d1\u4e86\u5206\u6790\u6a21\u578b\u8fdb\u884c\u5206\u6790\uff1b\u8bbe\u8ba1\u4e86\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u6a21\u578b\u6765\u786e\u5b9a\u6700\u4f18\u7b56\u7565\u7684\u6027\u80fd\uff0c\u4ee5\u91cf\u5316\u7b80\u5355\u7b56\u7565\u4e0e\u6700\u4f18\u7b56\u7565\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1b\u5c06\u6a21\u578b\u5e94\u7528\u4e8e\u7814\u7a76\u591a\u7ad9\u70b9\u7cfb\u7edf\u4e2d\u72b6\u6001\u4f9d\u8d56\u8def\u7531\u5728\u52a8\u6001\u670d\u52a1\u5668\u5206\u914d\u4e0b\u7684\u6f5c\u5728\u6027\u80fd\u4f18\u52bf\u3002", "result": "\u91cf\u5316\u4e86\u7b80\u5355\u6613\u5b9e\u73b0\u7684\u7b56\u7565\u4e0e\u6700\u4f18\u7b56\u7565\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1b\u7814\u7a76\u4e86\u591a\u7ad9\u70b9\u7cfb\u7edf\u4e2d\u91c7\u7528\u72b6\u6001\u4f9d\u8d56\u8def\u7531\u65f6\u7684\u6f5c\u5728\u6027\u80fd\u4f18\u52bf\uff1b\u7814\u7a76\u7ed3\u679c\u4e3a\u670d\u52a1\u63d0\u4f9b\u5546\u5e73\u8861\u4e91\u670d\u52a1\u6210\u672c\u548c\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u5e0c\u671b\u5e73\u8861\u4e91\u670d\u52a1\u6210\u672c\u548c\u5ef6\u8fdf\u7684\u670d\u52a1\u63d0\u4f9b\u5546\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u8bc4\u4f30\u7b80\u5355\u7b56\u7565\u4e0e\u6700\u4f18\u7b56\u7565\u7684\u6027\u80fd\u5dee\u5f02\u4ee5\u53ca\u72b6\u6001\u4f9d\u8d56\u8def\u7531\u7684\u76ca\u5904\u65b9\u9762\u3002"}}
{"id": "2507.19974", "pdf": "https://arxiv.org/pdf/2507.19974", "abs": "https://arxiv.org/abs/2507.19974", "authors": ["Tongjie Li", "Jianhua Zhang", "Li Yu", "Yuxiang Zhang", "Yunlong Cai", "Fan Xu", "Guangyi Liu"], "title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks.", "AI": {"tldr": "\u4e3a\u5e94\u5bf96G\u7f51\u7edc\u4e25\u82db\u7684\u8d44\u6e90\u5206\u914d\u9700\u6c42\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053\uff08DTC\uff09\u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u73af\u5883\u611f\u77e5\u9884\u6d4b\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u5b9e\u65f6\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u541e\u5410\u91cf\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u7684\u5168\u606f\u901a\u4fe1\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u5de5\u4e1a\u7269\u8054\u7f51\u7b49\u65b0\u5174\u5e94\u7528\u5bf9\u8d44\u6e90\u5206\u914d\u7684\u7075\u6d3b\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u4e25\u683c\u8981\u6c42\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u7edf\u8ba1\u5efa\u6a21\u65b9\u6cd5\u5728\u52a8\u6001\u7279\u5b9a\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u83b7\u53d6\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u5bfc\u9891\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053\uff08DTC\uff09\u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4e2d\uff0cDTC\u901a\u8fc7\u73af\u5883\u611f\u77e5\u9884\u6d4b\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\uff0c\u968f\u540e\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u5229\u7528\u9884\u6d4b\u7684CSI\u8fdb\u884c\u53ca\u65f6\u9ad8\u6548\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u5de5\u4e1a\u8f66\u95f4\u6570\u5b57\u526f\u672c\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u5bfc\u9891\u7684\u7406\u60f3CSI\u65b9\u6848\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe11.5%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u9a8c\u8bc1\u4e86\u5176\u5728\u672a\u67656G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u3001\u73af\u5883\u611f\u77e5\u901a\u4fe1\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19754", "pdf": "https://arxiv.org/pdf/2507.19754", "abs": "https://arxiv.org/abs/2507.19754", "authors": ["Seunghun Lee", "Jiwan Seo", "Minwoo Choi", "Kiljoon Han", "Jaehoon Jeong", "Zane Durante", "Ehsan Adeli", "Sang Hyun Park", "Sunghoon Im"], "title": "Latest Object Memory Management for Temporally Consistent Video Instance Segmentation", "categories": ["cs.CV"], "comment": "ICCV 2025. Code: https://github.com/Seung-Hun-Lee/LOMM", "summary": "In this paper, we present Latest Object Memory Management (LOMM) for\ntemporally consistent video instance segmentation that significantly improves\nlong-term instance tracking. At the core of our method is Latest Object Memory\n(LOM), which robustly tracks and continuously updates the latest states of\nobjects by explicitly modeling their presence in each frame. This enables\nconsistent tracking and accurate identity management across frames, enhancing\nboth performance and reliability through the VIS process. Moreover, we\nintroduce Decoupled Object Association (DOA), a strategy that separately\nhandles newly appearing and already existing objects. By leveraging our memory\nsystem, DOA accurately assigns object indices, improving matching accuracy and\nensuring stable identity consistency, even in dynamic scenes where objects\nfrequently appear and disappear. Extensive experiments and ablation studies\ndemonstrate the superiority of our method over traditional approaches, setting\na new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of\n54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.\nProject page: https://seung-hun-lee.github.io/projects/LOMM/", "AI": {"tldr": "\u63d0\u51faLOMM\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u7ba1\u7406\u548c\u5bf9\u8c61\u5173\u8054\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u7684\u957f\u671f\u8ddf\u8e2a\u80fd\u529b\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u4e2d\u957f\u671f\u5b9e\u4f8b\u8ddf\u8e2a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u6838\u5fc3\u662f\u201c\u6700\u65b0\u5bf9\u8c61\u5185\u5b58\uff08LOM\uff09\u201d\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5bf9\u8c61\u5728\u6bcf\u5e27\u4e2d\u7684\u5b58\u5728\u6765\u9c81\u68d2\u8ddf\u8e2a\u5e76\u6301\u7eed\u66f4\u65b0\u5bf9\u8c61\u6700\u65b0\u72b6\u6001\u3002\u6b64\u5916\uff0c\u5f15\u5165\u201c\u89e3\u8026\u5bf9\u8c61\u5173\u8054\uff08DOA\uff09\u201d\u7b56\u7565\uff0c\u5355\u72ec\u5904\u7406\u65b0\u51fa\u73b0\u548c\u5df2\u5b58\u5728\u5bf9\u8c61\uff0c\u5229\u7528\u5185\u5b58\u7cfb\u7edf\u7cbe\u786e\u5206\u914d\u5bf9\u8c61\u7d22\u5f15\uff0c\u786e\u4fdd\u52a8\u6001\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5728VIS\u9886\u57df\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002LOMM\u5728\u6311\u6218\u6027\u7684YouTube-VIS 2022\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8654.0\u7684\u6700\u65b0AP\u5206\u6570\u3002", "conclusion": "LOMM\u901a\u8fc7LOM\u548cDOA\u7b49\u521b\u65b0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u957f\u671f\u8ddf\u8e2a\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3aVIS\u9886\u57df\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002"}}
{"id": "2507.19529", "pdf": "https://arxiv.org/pdf/2507.19529", "abs": "https://arxiv.org/abs/2507.19529", "authors": ["Obumneme Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As green hydrogen emerges as a major component of global decarbonisation,\nOman has positioned itself strategically through national auctions and\ninternational partnerships. Following two successful green hydrogen project\nrounds, the country launched its third auction (R3) in the Duqm region. While\nthis area exhibits relative geospatial homogeneity, it is still vulnerable to\nenvironmental fluctuations that pose inherent risks to productivity. Despite\ngrowing global investment in green hydrogen, operational data remains scarce,\nwith major projects like Saudi Arabia's NEOM facility not expected to commence\nproduction until 2026, and Oman's ACME Duqm project scheduled for 2028. This\nabsence of historical maintenance and performance data from large-scale\nhydrogen facilities in desert environments creates a major knowledge gap for\naccurate risk assessment for infrastructure planning and auction decisions.\nGiven this data void, environmental conditions emerge as accessible and\nreliable proxy for predicting infrastructure maintenance pressures, because\nharsh desert conditions such as dust storms, extreme temperatures, and humidity\nfluctuations are well-documented drivers of equipment degradation in renewable\nenergy systems. To address this challenge, this paper proposes an Artificial\nIntelligence decision support system that leverages publicly available\nmeteorological data to develop a predictive Maintenance Pressure Index (MPI),\nwhich predicts risk levels and future maintenance demands on hydrogen\ninfrastructure. This tool strengthens regulatory foresight and operational\ndecision-making by enabling temporal benchmarking to assess and validate\nperformance claims over time. It can be used to incorporate temporal risk\nintelligence into auction evaluation criteria despite the absence of historical\noperational benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5229\u7528\u516c\u5f00\u6c14\u8c61\u6570\u636e\u9884\u6d4b\u7eff\u6c22\u57fa\u7840\u8bbe\u65bd\u7684\u7ef4\u62a4\u538b\u529b\u6307\u6570\uff08MPI\uff09\uff0c\u4ee5\u5f25\u8865\u6c99\u6f20\u73af\u5883\u4e0b\u7f3a\u4e4f\u8fd0\u8425\u6570\u636e\u7684\u98ce\u9669\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u5927\u89c4\u6a21\u7eff\u6c22\u8bbe\u65bd\u5728\u6c99\u6f20\u73af\u5883\u4e0b\u7f3a\u4e4f\u5386\u53f2\u8fd0\u8425\u548c\u7ef4\u62a4\u6570\u636e\uff0c\u5bfc\u81f4\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u62cd\u5356\u51b3\u7b56\u9762\u4e34\u5de8\u5927\u7684\u98ce\u9669\u8bc4\u4f30\u77e5\u8bc6\u7a7a\u767d\u3002\u9274\u4e8e\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5229\u7528\u53ef\u83b7\u53d6\u73af\u5883\u6570\u636e\u9884\u6d4b\u7ef4\u62a4\u538b\u529b\u7684\u5de5\u5177\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u516c\u5f00\u53ef\u7528\u7684\u6c14\u8c61\u6570\u636e\uff0c\u5f00\u53d1\u5e76\u9884\u6d4b\u4e00\u4e2a\u201c\u7ef4\u62a4\u538b\u529b\u6307\u6570\u201d\uff08MPI\uff09\uff0c\u65e8\u5728\u8bc4\u4f30\u672a\u6765\u7eff\u6c22\u57fa\u7840\u8bbe\u65bd\u7684\u98ce\u9669\u548c\u7ef4\u62a4\u9700\u6c42\u3002", "result": "\u8be5MPI\u80fd\u591f\u9884\u6d4b\u7eff\u6c22\u57fa\u7840\u8bbe\u65bd\u7684\u98ce\u9669\u6c34\u5e73\u548c\u672a\u6765\u7684\u7ef4\u62a4\u9700\u6c42\u3002\u8be5\u5de5\u5177\u80fd\u589e\u5f3a\u76d1\u7ba1\u9884\u89c1\u6027\u548c\u8fd0\u8425\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u548c\u9a8c\u8bc1\u9879\u76ee\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u5386\u53f2\u8fd0\u8425\u57fa\u51c6\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5c06\u65f6\u95f4\u98ce\u9669\u667a\u80fd\u7eb3\u5165\u62cd\u5356\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684MPI\u5de5\u5177\uff0c\u901a\u8fc7\u5229\u7528\u73af\u5883\u6570\u636e\u4f5c\u4e3a\u53ef\u9760\u4ee3\u7406\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6c99\u6f20\u5730\u533a\u7eff\u6c22\u9879\u76ee\u89c4\u5212\u548c\u8bc4\u4f30\u4e2d\u56e0\u7f3a\u4e4f\u5386\u53f2\u8fd0\u8425\u6570\u636e\u800c\u5bfc\u81f4\u7684\u98ce\u9669\u8bc4\u4f30\u6311\u6218\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u76d1\u7ba1\u80fd\u529b\u3002"}}
{"id": "2507.19869", "pdf": "https://arxiv.org/pdf/2507.19869", "abs": "https://arxiv.org/abs/2507.19869", "authors": ["Danil Fokin", "Monika P\u0142u\u017cyczka", "Grigory Golovin"], "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment", "categories": ["cs.CL"], "comment": null, "summary": "We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing\nthe receptive vocabulary size of both native and non-native Polish speakers.\nBased on Item Response Theory and Computerized Adaptive Testing, PVST\ndynamically adjusts to each test-taker's proficiency level, ensuring high\naccuracy while keeping the test duration short. To validate the test, a pilot\nstudy was conducted with 1.475 participants. Native Polish speakers\ndemonstrated significantly larger vocabularies compared to non-native speakers.\nFor native speakers, vocabulary size showed a strong positive correlation with\nage. The PVST is available online at myvocab.info/pl.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPVST\u7684\u65b0\u578b\u6ce2\u5170\u8bed\u8bcd\u6c47\u91cf\u6d4b\u8bd5\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u548c\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5\uff0c\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u8bc4\u4f30\u6ce2\u5170\u8bed\u6bcd\u8bed\u53ca\u975e\u6bcd\u8bed\u5b66\u4e60\u8005\u7684\u8bcd\u6c47\u91cf\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65b0\u9896\u3001\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6ce2\u5170\u8bed\u6bcd\u8bed\u8005\u548c\u975e\u6bcd\u8bed\u5b66\u4e60\u8005\u7684\u63a5\u6536\u6027\u8bcd\u6c47\u91cf\u3002", "method": "\u8be5\u6d4b\u8bd5\u5de5\u5177\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u548c\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5\uff08CAT\uff09\u8bbe\u8ba1\uff0c\u80fd\u591f\u6839\u636e\u6d4b\u8bd5\u8005\u7684\u719f\u7ec3\u7a0b\u5ea6\u52a8\u6001\u8c03\u6574\u9898\u76ee\u3002\u901a\u8fc7\u4e00\u9879\u5305\u542b1475\u540d\u53c2\u4e0e\u8005\u7684\u521d\u6b65\u7814\u7a76\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u6ce2\u5170\u8bed\u6bcd\u8bed\u8005\u7684\u8bcd\u6c47\u91cf\u663e\u8457\u5927\u4e8e\u975e\u6bcd\u8bed\u8005\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6bcd\u8bed\u8005\u800c\u8a00\uff0c\u8bcd\u6c47\u91cf\u4e0e\u5e74\u9f84\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6b63\u76f8\u5173\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86PVST\u4f5c\u4e3a\u4e00\u79cd\u8bc4\u4f30\u6ce2\u5170\u8bed\u8bcd\u6c47\u91cf\u7684\u6709\u6548\u5de5\u5177\u3002\u8be5\u5de5\u5177\u5df2\u5728\u7ebf\u53ef\u7528\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u6ce2\u5170\u8bed\u6bcd\u8bed\u548c\u975e\u6bcd\u8bed\u5b66\u4e60\u8005\u7684\u8bcd\u6c47\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u8bcd\u6c47\u91cf\u589e\u957f\u6a21\u5f0f\u3002"}}
{"id": "2507.19712", "pdf": "https://arxiv.org/pdf/2507.19712", "abs": "https://arxiv.org/abs/2507.19712", "authors": ["Ngoc Hung Nguyen", "Nguyen Van Thieu", "Quang-Trung Luu", "Anh Tuan Nguyen", "Senura Wanasekara", "Nguyen Cong Luong", "Fatemeh Kavehmadavani", "Van-Dinh Nguyen"], "title": "Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning", "categories": ["cs.DC", "cs.AI", "cs.GT", "cs.LG", "cs.NI"], "comment": "15 pages, 13 figures", "summary": "In this paper, we explore mission assignment and task offloading in an Open\nRadio Access Network (Open RAN)-based intelligent transportation system (ITS),\nwhere autonomous vehicles leverage mobile edge computing for efficient\nprocessing. Existing studies often overlook the intricate interdependencies\nbetween missions and the costs associated with offloading tasks to edge\nservers, leading to suboptimal decision-making. To bridge this gap, we\nintroduce Oranits, a novel system model that explicitly accounts for mission\ndependencies and offloading costs while optimizing performance through vehicle\ncooperation. To achieve this, we propose a twofold optimization approach.\nFirst, we develop a metaheuristic-based evolutionary computing algorithm,\nnamely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline\nfor one-slot optimization. Second, we design an enhanced reward-based deep\nreinforcement learning (DRL) framework, referred to as the Multi-agent Double\nDeep Q-Network (MA-DDQN), that integrates both multi-agent coordination and\nmulti-action selection mechanisms, significantly reducing mission assignment\ntime and improving adaptability over baseline methods. Extensive simulations\nreveal that CGG-ARO improves the number of completed missions and overall\nbenefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN\nachieves even greater improvements of 11.0% in terms of mission completions and\n12.5% in terms of the overall benefit. These results highlight the\neffectiveness of Oranits in enabling faster, more adaptive, and more efficient\ntask processing in dynamic ITS environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOranits\u7cfb\u7edf\uff0c\u65e8\u5728Open RAN\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u548c\u5378\u8f7d\u95ee\u9898\uff0c\u901a\u8fc7\u8003\u8651\u4efb\u52a1\u4f9d\u8d56\u548c\u5378\u8f7d\u6210\u672c\uff0c\u5e76\u91c7\u7528CGG-ARO\u548cMA-DDQN\u4e24\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u6570\u548c\u7cfb\u7edf\u6536\u76ca\u3002", "motivation": "\u73b0\u6709\u5173\u4e8eOpen RAN\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u4efb\u52a1\u5206\u914d\u548c\u5378\u8f7d\u7684\u7814\u7a76\uff0c\u5e38\u5ffd\u7565\u4efb\u52a1\u95f4\u590d\u6742\u4f9d\u8d56\u6027\u53ca\u5c06\u4efb\u52a1\u5378\u8f7d\u5230\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u6210\u672c\uff0c\u5bfc\u81f4\u51b3\u7b56\u6b21\u4f18\u3002", "method": "\u63d0\u51faOranits\u7cfb\u7edf\u6a21\u578b\uff0c\u660e\u786e\u8003\u8651\u4efb\u52a1\u4f9d\u8d56\u548c\u5378\u8f7d\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u8f66\u8f86\u534f\u4f5c\u4f18\u5316\u6027\u80fd\u3002\u91c7\u7528\u53cc\u91cd\u4f18\u5316\u65b9\u6cd5\uff1a1. CGG-ARO\uff1a\u4e00\u79cd\u57fa\u4e8e\u5143\u542f\u53d1\u5f0f\u8fdb\u5316\u8ba1\u7b97\u7b97\u6cd5\uff0c\u4f5c\u4e3a\u5355\u65f6\u9699\u4f18\u5316\u7684\u57fa\u7ebf\u30022. MA-DDQN\uff1a\u4e00\u79cd\u589e\u5f3a\u578b\u57fa\u4e8e\u5956\u52b1\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u591a\u52a8\u4f5c\u9009\u62e9\u673a\u5236\uff0c\u65e8\u5728\u51cf\u5c11\u4efb\u52a1\u5206\u914d\u65f6\u95f4\u5e76\u63d0\u9ad8\u9002\u5e94\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cCGG-ARO\u4f7f\u5b8c\u6210\u4efb\u52a1\u6570\u548c\u603b\u4f53\u6536\u76ca\u5206\u522b\u63d0\u9ad8\u7ea67.1%\u548c7.7%\u3002MA-DDQN\u5219\u53d6\u5f97\u66f4\u5927\u63d0\u5347\uff0c\u5b8c\u6210\u4efb\u52a1\u6570\u63d0\u9ad811.0%\uff0c\u603b\u4f53\u6536\u76ca\u63d0\u9ad812.5%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660eOranits\u7cfb\u7edf\u5728\u52a8\u6001ITS\u73af\u5883\u4e2d\uff0c\u80fd\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u5177\u9002\u5e94\u6027\u3001\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.20000", "pdf": "https://arxiv.org/pdf/2507.20000", "abs": "https://arxiv.org/abs/2507.20000", "authors": ["Renaud Fabre", "Daniel Egret", "Patrice Bellot"], "title": "Matching Game Preferences Through Dialogical Large Language Models: A Perspective", "categories": ["cs.AI", "cs.DL"], "comment": "28 pages, 1 figure. Published in Applied Sciences", "summary": "This perspective paper explores the future potential of \"conversational\nintelligence\" by examining how Large Language Models (LLMs) could be combined\nwith GRAPHYP's network system to better understand human conversations and\npreferences. Using recent research and case studies, we propose a conceptual\nframework that could make AI rea-soning transparent and traceable, allowing\nhumans to see and understand how AI reaches its conclusions. We present the\nconceptual perspective of \"Matching Game Preferences through Dialogical Large\nLanguage Models (D-LLMs),\" a proposed system that would allow multiple users to\nshare their different preferences through structured conversations. This\napproach envisions personalizing LLMs by embedding individual user preferences\ndirectly into how the model makes decisions. The proposed D-LLM framework would\nrequire three main components: (1) reasoning processes that could analyze\ndifferent search experiences and guide performance, (2) classification systems\nthat would identify user preference patterns, and (3) dialogue approaches that\ncould help humans resolve conflicting information. This perspective framework\naims to create an interpretable AI system where users could examine,\nunderstand, and combine the different human preferences that influence AI\nresponses, detected through GRAPHYP's search experience networks. The goal of\nthis perspective is to envision AI systems that would not only provide answers\nbut also show users how those answers were reached, making artificial\nintelligence more transparent and trustworthy for human decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548cGRAPHYP\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u6574\u5408\u7528\u6237\u504f\u597d\uff0c\u521b\u5efa\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u4e2a\u6027\u5316AI\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u201c\u5bf9\u8bdd\u667a\u80fd\u201d\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u4e2a\u6027\u5316\uff0c\u96be\u4ee5\u8ba9\u7528\u6237\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408LLMs\u548cGRAPHYP\u6765\u589e\u5f3aAI\u7684\u201c\u5bf9\u8bdd\u667a\u80fd\u201d\u548c\u4fe1\u4efb\u5ea6\uff0c\u4f7f\u4eba\u7c7b\u80fd\u7406\u89e3AI\u7684\u51b3\u7b56\u673a\u5236\u3002", "method": "\u63d0\u51fa\u201c\u5bf9\u8bdd\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08D-LLMs\uff09\u201d\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06LLMs\u4e0eGRAPHYP\u7f51\u7edc\u7cfb\u7edf\u7ed3\u5408\u3002D-LLM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) \u5206\u6790\u641c\u7d22\u7ecf\u9a8c\u548c\u5f15\u5bfc\u6027\u80fd\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b2) \u8bc6\u522b\u7528\u6237\u504f\u597d\u6a21\u5f0f\u7684\u5206\u7c7b\u7cfb\u7edf\uff1b3) \u5e2e\u52a9\u4eba\u7c7b\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u7684\u5bf9\u8bdd\u65b9\u6cd5\u3002", "result": "\u6982\u5ff5\u4e0a\uff0cD-LLMs\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u6574\u5408\u591a\u7528\u6237\u504f\u597d\uff0c\u5c06\u4e2a\u4f53\u504f\u597d\u76f4\u63a5\u5d4c\u5165AI\u51b3\u7b56\u4e2d\u3002\u8be5\u6846\u67b6\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7GRAPHYP\u7f51\u7edc\u5ba1\u67e5\u3001\u7406\u89e3\u5e76\u6574\u5408\u5f71\u54cdAI\u54cd\u5e94\u7684\u4e0d\u540c\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u8be5\u89c6\u89d2\u65e8\u5728\u6784\u5efa\u4e0d\u4ec5\u63d0\u4f9b\u7b54\u6848\uff0c\u8fd8\u80fd\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b\u7684AI\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u66f4\u597d\u5730\u652f\u6301\u4eba\u7c7b\u51b3\u7b56\u3002"}}
{"id": "2507.19770", "pdf": "https://arxiv.org/pdf/2507.19770", "abs": "https://arxiv.org/abs/2507.19770", "authors": ["Jiaxin Liu", "Qichao Ying", "Zhenxing Qian", "Sheng Li", "Runqi Zhang", "Jian Liu", "Xinpeng Zhang"], "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration", "categories": ["cs.CV"], "comment": null, "summary": "The widespread use of face retouching on social media platforms raises\nconcerns about the authenticity of face images. While existing methods focus on\ndetecting face retouching, how to accurately recover the original faces from\nthe retouched ones has yet to be answered. This paper introduces Face\nRetouching Restoration (FRR), a novel computer vision task aimed at restoring\noriginal faces from their retouched counterparts. FRR differs from traditional\nimage restoration tasks by addressing the complex retouching operations with\nvarious types and degrees, which focuses more on the restoration of the\nlow-frequency information of the faces. To tackle this challenge, we propose\nMoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert\nisolation strategy, the MoFRR uses sparse activation of specialized experts\nhandling distinct retouching types and the engagement of a shared expert\ndealing with universal retouching traces. Each specialized expert follows a\ndual-branch structure with a DDIM-based low-frequency branch guided by an\nIterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based\nHigh-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a\nnewly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the\neffectiveness of MoFRR for FRR.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u9762\u90e8\u4fee\u56fe\u6062\u590d\uff08FRR\uff09\uff0c\u65e8\u5728\u8fd8\u539f\u4fee\u56fe\u540e\u7684\u539f\u59cb\u9762\u90e8\u3002\u4e3a\u6b64\uff0c\u63d0\u51faMoFRR\u6a21\u578b\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e13\u95e8\u548c\u5171\u4eab\u4e13\u5bb6\u5904\u7406\u4e0d\u540c\u4fee\u56fe\u7c7b\u578b\uff0c\u5e76\u5728\u65b0\u5efa\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9762\u90e8\u4fee\u56fe\u5f15\u53d1\u4e86\u5bf9\u56fe\u50cf\u771f\u5b9e\u6027\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u68c0\u6d4b\u4fee\u56fe\uff0c\u4f46\u5982\u4f55\u51c6\u786e\u5730\u4ece\u4fee\u56fe\u56fe\u50cf\u4e2d\u6062\u590d\u539f\u59cb\u9762\u90e8\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5f15\u5165\u9762\u90e8\u4fee\u56fe\u6062\u590d\uff08FRR\uff09\u8fd9\u4e00\u65b0\u578b\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u63d0\u51faMoFRR\uff08Mixture of Diffusion Models for FRR\uff09\u6a21\u578b\u3002MoFRR\u501f\u9274DeepSeek\u7684\u4e13\u5bb6\u9694\u79bb\u7b56\u7565\uff0c\u91c7\u7528\u7a00\u758f\u6fc0\u6d3b\u7684\u4e13\u4e1a\u4e13\u5bb6\u6765\u5904\u7406\u4e0d\u540c\u7684\u4fee\u56fe\u7c7b\u578b\uff0c\u5e76\u7ed3\u5408\u4e00\u4e2a\u5904\u7406\u901a\u7528\u4fee\u56fe\u75d5\u8ff9\u7684\u5171\u4eab\u4e13\u5bb6\u3002\u6bcf\u4e2a\u4e13\u4e1a\u4e13\u5bb6\u90fd\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff1a\u4e00\u4e2a\u7531\u8fed\u4ee3\u5931\u771f\u8bc4\u4f30\u6a21\u5757\uff08IDEM\uff09\u6307\u5bfc\u7684\u57fa\u4e8eDDIM\u7684\u4f4e\u9891\u5206\u652f\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u7ec6\u8282\u7ec6\u5316\u7684\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684HFCAM\u9ad8\u9891\u5206\u652f\u3002", "result": "\u5728\u65b0\u5efa\u7684\u9762\u90e8\u4fee\u56fe\u6570\u636e\u96c6RetouchingFFHQ++\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eMoFRR\u5728\u9762\u90e8\u4fee\u56fe\u6062\u590d\u4efb\u52a1\uff08FRR\uff09\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5f15\u5165\u5e76\u5b9a\u4e49\u4e86\u9762\u90e8\u4fee\u56fe\u6062\u590d\uff08FRR\uff09\u8fd9\u4e00\u65b0\u578b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684MoFRR\u6a21\u578b\uff0c\u5176\u5728\u4e13\u95e8\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\uff0c\u4e3a\u4ece\u4fee\u56fe\u56fe\u50cf\u4e2d\u6062\u590d\u539f\u59cb\u9762\u90e8\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19530", "pdf": "https://arxiv.org/pdf/2507.19530", "abs": "https://arxiv.org/abs/2507.19530", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)\nwhere hemodynamic instability can\n  rapidly progress to cardiovascular collapse. Current machine\n  learning (ML) approaches suffer from three limitations: lack of\n  external validation, absence of uncertainty quantification, and\n  inadequate data leakage prevention. This study presents the\n  first comprehensive framework with novel algorithmic leakage\n  prevention, uncertainty quantification, and cross-institutional\n  validation for electronic health records (EHRs) based BP pre dictions. Our\nmethodology implemented systematic data leakage\n  prevention, uncertainty quantification through quantile regres sion, and\nexternal validation between the MIMIC-III and eICU\n  databases. An ensemble framework combines Gradient Boosting,\n  Random Forest, and XGBoost with 74 features across five\n  physiological domains. Internal validation achieved a clinically\n  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03\n  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI\n  standards. External validation showed 30% degradation with\n  critical limitations in patients with hypotensive. Uncertainty\n  quantification generated valid prediction intervals (80.3% SBP\n  and 79.9% DBP coverage), enabling risk-stratified protocols\n  with narrow intervals (< 15 mmHg) for standard monitoring\n  and wide intervals (> 30 mmHg) for manual verification. This\n  framework provides realistic deployment expectations for cross institutional\nAI-assisted BP monitoring in critical care settings.\n  The source code is publicly available at https://github.com/\n  mdbasit897/clinical-bp-prediction-ehr.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u8840\u538b\u9884\u6d4b\u7684\u5168\u9762\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u65b0\u9896\u7684\u6570\u636e\u6cc4\u9732\u9884\u9632\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8de8\u673a\u6784\u9a8c\u8bc1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u63d0\u4f9b\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u8840\u538b\u76d1\u6d4b\u3002", "motivation": "\u8840\u538b\u76d1\u6d4b\u5728\u91cd\u75c7\u76d1\u62a4\u5ba4\uff08ICU\uff09\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5f53\u524d\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728EHR\u8840\u538b\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u5916\u90e8\u9a8c\u8bc1\u7f3a\u5931\u3001\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u6cc4\u9732\u9884\u9632\u4e0d\u8db3\u4e09\u5927\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5176\u4e34\u5e8a\u5e94\u7528\u53d7\u9650\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u7cfb\u7edf\u6027\u6570\u636e\u6cc4\u9732\u9884\u9632\u3001\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4ee5\u53ca\u5728MIMIC-III\u548ceICU\u6570\u636e\u5e93\u4e4b\u95f4\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5305\u542b\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5229\u7528\u6765\u81ea\u4e94\u4e2a\u751f\u7406\u9886\u57df\u768474\u4e2a\u7279\u5f81\u3002", "result": "\u5185\u90e8\u9a8c\u8bc1\u663e\u793a\u51fa\u4e34\u5e8a\u53ef\u63a5\u53d7\u7684\u6027\u80fd\uff08\u6536\u7f29\u538b\uff1aR^2=0.86\uff0cRMSE=6.03 mmHg\uff1b\u8212\u5f20\u538b\uff1aR^2=0.49\uff0cRMSE=7.13 mmHg\uff09\uff0c\u7b26\u5408AAMI\u6807\u51c6\u3002\u5916\u90e8\u9a8c\u8bc1\u8868\u73b0\u51fa30%\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u4f4e\u8840\u538b\u60a3\u8005\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u751f\u6210\u4e86\u6709\u6548\u7684\u9884\u6d4b\u533a\u95f4\uff08\u6536\u7f29\u538b\u8986\u76d6\u7387\u4e3a80.3%\uff0c\u8212\u5f20\u538b\u4e3a79.9%\uff09\uff0c\u652f\u6301\u98ce\u9669\u5206\u5c42\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u4e2dAI\u8f85\u52a9\u8840\u538b\u76d1\u6d4b\u7684\u8de8\u673a\u6784\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5207\u5408\u5b9e\u9645\u7684\u9884\u671f\uff0c\u5e76\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2507.19885", "pdf": "https://arxiv.org/pdf/2507.19885", "abs": "https://arxiv.org/abs/2507.19885", "authors": ["Cesar Augusto Madid Truyts", "Amanda Gomes Rabelo", "Gabriel Mesquita de Souza", "Daniel Scaldaferri Lages", "Adriano Jose Pereira", "Uri Adrian Prync Flato", "Eduardo Pontes dos Reis", "Joaquim Edson Vieira", "Paulo Sergio Panse Silveira", "Edson Amaro Junior"], "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam", "categories": ["cs.CL"], "comment": null, "summary": "Artificial intelligence (AI) has shown the potential to revolutionize\nhealthcare by improving diagnostic accuracy, optimizing workflows, and\npersonalizing treatment plans. Large Language Models (LLMs) and Multimodal\nLarge Language Models (MLLMs) have achieved notable advancements in natural\nlanguage processing and medical applications. However, the evaluation of these\nmodels has focused predominantly on the English language, leading to potential\nbiases in their performance across different languages.\n  This study investigates the capability of six LLMs (GPT-4.0 Turbo,\nLLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and\nCommand R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,\nand Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese\nfrom the medical residency entrance exam of the Hospital das Cl\\'inicas da\nFaculdade de Medicina da Universidade de S\\~ao Paulo (HCFMUSP) - the largest\nhealth complex in South America. The performance of the models was benchmarked\nagainst human candidates, analyzing accuracy, processing time, and coherence of\nthe generated explanations.\n  The results show that while some models, particularly Claude-3.5-Sonnet and\nClaude-3-Opus, achieved accuracy levels comparable to human candidates,\nperformance gaps persist, particularly in multimodal questions requiring image\ninterpretation. Furthermore, the study highlights language disparities,\nemphasizing the need for further fine-tuning and data set augmentation for\nnon-English medical AI applications.\n  Our findings reinforce the importance of evaluating generative AI in various\nlinguistic and clinical settings to ensure a fair and reliable deployment in\nhealthcare. Future research should explore improved training methodologies,\nimproved multimodal reasoning, and real-world clinical integration of AI-driven\nmedical assistance.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\u533b\u7597\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u90e8\u5206\u6a21\u578b\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u5728\u591a\u6a21\u6001\u548c\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\u4ecd\u6709\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u53ef\u80fd\u5bfc\u81f4\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6027\u80fd\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u975e\u82f1\u8bed\u533b\u7597\u8bed\u5883\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u9009\u53d6\u4e86\u516d\u4e2aLLM\u548c\u56db\u4e2aMLLM\uff0c\u4f7f\u7528\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u5723\u4fdd\u7f57\u5927\u5b66\u533b\u5b66\u9662\u9644\u5c5e\u533b\u9662\uff08HCFMUSP\uff09\u533b\u5b66\u4f4f\u9662\u533b\u5e08\u5165\u5b66\u8003\u8bd5\u95ee\u9898\u8fdb\u884c\u6d4b\u8bd5\u3002\u5c06\u6a21\u578b\u8868\u73b0\u4e0e\u4eba\u7c7b\u8003\u751f\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5206\u6790\u4e86\u51c6\u786e\u6027\u3001\u5904\u7406\u65f6\u95f4\u548c\u89e3\u91ca\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u90e8\u5206\u6a21\u578b\uff08\u7279\u522b\u662fClaude-3.5-Sonnet\u548cClaude-3-Opus\uff09\u7684\u51c6\u786e\u7387\u53ef\u4e0e\u4eba\u7c7b\u8003\u751f\u5ab2\u7f8e\u3002\u4f46\u5728\u9700\u8981\u56fe\u50cf\u89e3\u8bfb\u7684\u591a\u6a21\u6001\u95ee\u9898\u4e0a\u4ecd\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002\u7814\u7a76\u8fd8\u7a81\u51fa\u4e86\u8bed\u8a00\u5dee\u5f02\uff0c\u5f3a\u8c03\u975e\u82f1\u8bed\u533b\u7597AI\u5e94\u7528\u9700\u8fdb\u4e00\u6b65\u5fae\u8c03\u548c\u6570\u636e\u589e\u5f3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u4e34\u5e8a\u73af\u5883\u4e0b\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u533b\u7597\u9886\u57df\u516c\u5e73\u53ef\u9760\u7684\u90e8\u7f72\u3002\u672a\u6765\u7814\u7a76\u5e94\u63a2\u7d22\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u3001\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u4ee5\u53caAI\u9a71\u52a8\u533b\u7597\u8f85\u52a9\u7684\u5b9e\u9645\u4e34\u5e8a\u6574\u5408\u3002"}}
{"id": "2507.20399", "pdf": "https://arxiv.org/pdf/2507.20399", "abs": "https://arxiv.org/abs/2507.20399", "authors": ["Rajat Bhattacharjya", "Arnab Sarkar", "Ish Kool", "Sabur Baidya", "Nikil Dutt"], "title": "ACCESS-AV: Adaptive Communication-Computation Codesign for Sustainable Autonomous Vehicle Localization in Smart Factories", "categories": ["eess.SY", "cs.AR", "cs.NI", "cs.RO", "cs.SY", "eess.SP"], "comment": "28 pages, 9 figures", "summary": "Autonomous Delivery Vehicles (ADVs) are increasingly used for transporting\ngoods in 5G network-enabled smart factories, with the compute-intensive\nlocalization module presenting a significant opportunity for optimization. We\npropose ACCESS-AV, an energy-efficient Vehicle-to-Infrastructure (V2I)\nlocalization framework that leverages existing 5G infrastructure in smart\nfactory environments. By opportunistically accessing the periodically broadcast\n5G Synchronization Signal Blocks (SSBs) for localization, ACCESS-AV obviates\nthe need for dedicated Roadside Units (RSUs) or additional onboard sensors to\nachieve energy efficiency as well as cost reduction. We implement an\nAngle-of-Arrival (AoA)-based estimation method using the Multiple Signal\nClassification (MUSIC) algorithm, optimized for resource-constrained ADV\nplatforms through an adaptive communication-computation strategy that\ndynamically balances energy consumption with localization accuracy based on\nenvironmental conditions such as Signal-to-Noise Ratio (SNR) and vehicle\nvelocity. Experimental results demonstrate that ACCESS-AV achieves an average\nenergy reduction of 43.09% compared to non-adaptive systems employing AoA\nalgorithms such as vanilla MUSIC, ESPRIT, and Root-MUSIC. It maintains sub-30\ncm localization accuracy while also delivering substantial reductions in\ninfrastructure and operational costs, establishing its viability for\nsustainable smart factory environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faACCESS-AV\uff0c\u4e00\u4e2a\u5229\u7528\u73b0\u67095G\u57fa\u7840\u8bbe\u65bd\u7684V2I\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u4f1a\u6027\u5730\u4f7f\u75285G\u540c\u6b65\u4fe1\u53f7\u5757\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u667a\u80fd\u5de5\u5382\u4e2d\u7684\u9ad8\u80fd\u6548\u3001\u4f4e\u6210\u672c\u5b9a\u4f4d\uff0c\u5e76\u4fdd\u6301\u4e9a30\u5398\u7c73\u7684\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08ADVs\uff09\u57285G\u667a\u80fd\u5de5\u5382\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u8ba1\u7b97\u5bc6\u96c6\u578b\u5b9a\u4f4d\u6a21\u5757\u5b58\u5728\u5de8\u5927\u7684\u4f18\u5316\u7a7a\u95f4\u3002\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6848\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u4e13\u7528\u8def\u8fb9\u5355\u5143\u6216\u8f66\u8f7d\u4f20\u611f\u5668\uff0c\u5bfc\u81f4\u80fd\u8017\u548c\u6210\u672c\u589e\u52a0\u3002", "method": "ACCESS-AV\u6846\u67b6\u5229\u7528\u667a\u80fd\u5de5\u5382\u4e2d\u73b0\u6709\u76845G\u57fa\u7840\u8bbe\u65bd\uff0c\u673a\u4f1a\u6027\u5730\u8bbf\u95ee\u5468\u671f\u6027\u5e7f\u64ad\u76845G\u540c\u6b65\u4fe1\u53f7\u5757\uff08SSBs\uff09\u8fdb\u884c\u5b9a\u4f4d\uff0c\u4ece\u800c\u65e0\u9700\u4e13\u7528\u8def\u8fb9\u5355\u5143\u6216\u989d\u5916\u8f66\u8f7d\u4f20\u611f\u5668\u3002\u8be5\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5230\u8fbe\u89d2\uff08AoA\uff09\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5177\u4f53\u4f7f\u7528\u591a\u91cd\u4fe1\u53f7\u5206\u7c7b\uff08MUSIC\uff09\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u4fe1-\u8ba1\u7b97\u7b56\u7565\u8fdb\u884c\u4f18\u5316\uff0c\u6839\u636e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u548c\u8f66\u8f86\u901f\u5ea6\u7b49\u73af\u5883\u6761\u4ef6\u52a8\u6001\u5e73\u8861\u80fd\u8017\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u975e\u81ea\u9002\u5e94\u7cfb\u7edf\uff08\u5982\u9999\u8349MUSIC\u3001ESPRIT\u548cRoot-MUSIC\uff09\u76f8\u6bd4\uff0cACCESS-AV\u5e73\u5747\u80fd\u8017\u964d\u4f4e\u4e8643.09%\u3002\u5b83\u4fdd\u6301\u4e86\u4e9a30\u5398\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u57fa\u7840\u8bbe\u65bd\u548c\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "ACCESS-AV\u6846\u67b6\u901a\u8fc7\u5229\u7528\u73b0\u67095G\u57fa\u7840\u8bbe\u65bd\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4e3a\u667a\u80fd\u5de5\u5382\u73af\u5883\u4e2d\u7684ADVs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u53ef\u6301\u7eed\u667a\u80fd\u5de5\u5382\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.20010", "pdf": "https://arxiv.org/pdf/2507.20010", "abs": "https://arxiv.org/abs/2507.20010", "authors": ["M\u00fcge Fidan", "Esra Erdem"], "title": "Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems", "categories": ["cs.AI", "cs.GT", "cs.LO"], "comment": null, "summary": "The Stable Roommates problems are characterized by the preferences of agents\nover other agents as roommates. A solution is a partition of the agents into\npairs that are acceptable to each other (i.e., they are in the preference lists\nof each other), and the matching is stable (i.e., there do not exist any two\nagents who prefer each other to their roommates, and thus block the matching).\nMotivated by real-world applications, and considering that stable roommates\nproblems do not always have solutions, we continue our studies to compute\n\"good-enough\" matchings. In addition to the agents' habits and habitual\npreferences, we consider their networks of preferred friends, and introduce a\nmethod to generate personalized solutions to stable roommates problems. We\nillustrate the usefulness of our method with examples and empirical\nevaluations.", "AI": {"tldr": "\u7814\u7a76\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u4e60\u60ef\u548c\u793e\u4ea4\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5339\u914d\u65b9\u6cd5\uff0c\u65e8\u5728\u5bfb\u627e\u201c\u8db3\u591f\u597d\u201d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6709\u5e94\u7528\u9700\u6c42\uff0c\u4f46\u5e76\u975e\u603b\u5b58\u5728\u7a33\u5b9a\u89e3\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u8ba1\u7b97\u201c\u8db3\u591f\u597d\u201d\u7684\u5339\u914d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8003\u8651\u4e86\u4ee3\u7406\u4eba\u7684\u4e60\u60ef\u3001\u4e60\u60ef\u6027\u504f\u597d\u53ca\u5176\u670b\u53cb\u7f51\u7edc\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ee3\u7406\u4eba\u7684\u4e60\u60ef\u3001\u4e60\u60ef\u6027\u504f\u597d\u4ee5\u53ca\u5176\u504f\u597d\u670b\u53cb\u7f51\u7edc\u7eb3\u5165\u8003\u8651\uff0c\u4ee5\u751f\u6210\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5b9e\u7528\u6027/\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u751f\u6210\u4e2a\u6027\u5316\u4e14\u201c\u8db3\u591f\u597d\u201d\u7684\u5339\u914d\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5b8c\u7f8e\u7a33\u5b9a\u89e3\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u66f4\u4e30\u5bcc\u7684\u4ee3\u7406\u4eba\u504f\u597d\u4fe1\u606f\u6765\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
