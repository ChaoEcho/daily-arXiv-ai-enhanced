{"id": "2505.00138", "pdf": "https://arxiv.org/pdf/2505.00138", "abs": "https://arxiv.org/abs/2505.00138", "authors": ["Martin Haenggi"], "title": "Q Cells in Wireless Networks", "categories": ["cs.NI", "cs.IT", "math.IT", "math.PR"], "comment": null, "summary": "For a given set of transmitters such as cellular base stations or WiFi access\npoints, is it possible to analytically characterize the set of locations that\nare \"covered\" in the sense that users at these locations experience a certain\nminimum quality of service? In this paper, we affirmatively answer this\nquestion, by providing explicit simple outer bounds and estimates for the\ncoverage manifold. The key geometric elements of our analytical method are the\nQ cells, defined as the intersections of a small number of disks. The Q cell of\na transmitter is an outer bound to the service region of the transmitter, and,\nin turn, the union of Q cells is an outer bound to the coverage manifold. In\ninfinite networks, connections to the meta distribution of the\nsignal-to-interference ratio allow for a scaling of the Q cells to obtain\naccurate estimates of the coverage manifold.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u201cQ\u5355\u5143\u201d\uff08\u5c11\u91cf\u5706\u76d8\u7684\u4ea4\u96c6\uff09\u4f5c\u4e3a\u4e00\u79cd\u89e3\u6790\u65b9\u6cd5\uff0c\u6765\u754c\u5b9a\u548c\u4f30\u8ba1\u7531\u4e00\u7ec4\u65e0\u7ebf\u53d1\u5c04\u5668\u63d0\u4f9b\u6700\u4f4e\u670d\u52a1\u8d28\u91cf\u4fdd\u969c\u7684\u8986\u76d6\u533a\u57df\u3002", "motivation": "\u786e\u5b9a\u662f\u5426\u6709\u53ef\u80fd\u89e3\u6790\u5730\u523b\u753b\u4e00\u7ec4\u53d1\u5c04\u5668\uff08\u5982\u8702\u7a9d\u57fa\u7ad9\u6216WiFi\u63a5\u5165\u70b9\uff09\u80fd\u591f\u4fdd\u8bc1\u7528\u6237\u6700\u4f4e\u670d\u52a1\u8d28\u91cf\u7684\u8986\u76d6\u4f4d\u7f6e\u96c6\u5408\u3002", "method": "\u5f15\u5165\u201cQ\u5355\u5143\u201d\uff08Q cells\uff09\u4f5c\u4e3a\u5173\u952e\u51e0\u4f55\u5143\u7d20\uff0c\u5373\u5c11\u91cf\u5706\u76d8\u7684\u4ea4\u96c6\u3002\u5355\u4e2a\u53d1\u5c04\u5668\u7684Q\u5355\u5143\u662f\u5176\u670d\u52a1\u533a\u57df\u7684\u5916\u8fb9\u754c\uff0c\u6240\u6709Q\u5355\u5143\u7684\u5e76\u96c6\u6784\u6210\u4e86\u6574\u4e2a\u8986\u76d6\u533a\u57df\u7684\u5916\u8fb9\u754c\u3002\u5728\u65e0\u9650\u7f51\u7edc\u4e2d\uff0c\u5229\u7528\u4fe1\u53f7\u5e72\u6270\u6bd4\uff08SINR\uff09\u7684\u5143\u5206\u5e03\uff08meta distribution\uff09\u6765\u7f29\u653eQ\u5355\u5143\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u4f30\u8ba1\u8986\u76d6\u533a\u57df\u3002", "result": "\u8bba\u6587\u80af\u5b9a\u5730\u56de\u7b54\u4e86\u7814\u7a76\u52a8\u673a\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u8986\u76d6\u533a\u57df\u7684\u663e\u5f0f\u3001\u7b80\u5355\u7684\u5916\u8fb9\u754c\u548c\u4f30\u8ba1\u65b9\u6cd5\u3002\u8bc1\u660e\u4e86Q\u5355\u5143\u662f\u6709\u6548\u7684\u754c\u5b9a\u5de5\u5177\uff0c\u5e76\u4e14\u5728\u65e0\u9650\u7f51\u7edc\u6a21\u578b\u4e2d\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u5143\u5206\u5e03\u4fe1\u606f\u8fdb\u884c\u7cbe\u786e\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8eQ\u5355\u5143\u7684\u89e3\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u5f81\u548c\u4f30\u8ba1\u65e0\u7ebf\u53d1\u5c04\u5668\u7f51\u7edc\u7684\u8986\u76d6\u533a\u57df\u3002"}}
{"id": "2505.00321", "pdf": "https://arxiv.org/pdf/2505.00321", "abs": "https://arxiv.org/abs/2505.00321", "authors": ["Zixin Wang", "Yuanming Shi", "Yong Zhou", "Jingyang Zhu", "Khaled. B. Letaief"], "title": "Edge Large AI Models: Revolutionizing 6G Networks", "categories": ["cs.NI", "cs.LG", "eess.SP"], "comment": null, "summary": "Large artificial intelligence models (LAMs) possess human-like abilities to\nsolve a wide range of real-world problems, exemplifying the potential of\nexperts in various domains and modalities. By leveraging the communication and\ncomputation capabilities of geographically dispersed edge devices, edge LAM\nemerges as an enabling technology to empower the delivery of various real-time\nintelligent services in 6G. Unlike traditional edge artificial intelligence\n(AI) that primarily supports a single task using small models, edge LAM is\nfeatured by the need of the decomposition and distributed deployment of large\nmodels, and the ability to support highly generalized and diverse tasks.\nHowever, due to limited communication, computation, and storage resources over\nwireless networks, the vast number of trainable neurons and the substantial\ncommunication overhead pose a formidable hurdle to the practical deployment of\nedge LAMs. In this paper, we investigate the opportunities and challenges of\nedge LAMs from the perspectives of model decomposition and resource management.\nSpecifically, we propose collaborative fine-tuning and full-parameter training\nframeworks, alongside a microservice-assisted inference architecture, to\nenhance the deployment of edge LAM over wireless networks. Additionally, we\ninvestigate the application of edge LAM in air-interface designs, focusing on\nchannel prediction and beamforming. These innovative frameworks and\napplications offer valuable insights and solutions for advancing 6G technology.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u76846G\u8fb9\u7f18\u7f51\u7edc\u4e2d\u90e8\u7f72\u5927\u578bAI\u6a21\u578b\uff08LAM\uff09\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u534f\u540c\u8bad\u7ec3\u548c\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u5728\u7a7a\u53e3\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5927\u578bAI\u6a21\u578b\uff08LAM\uff09\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u6a21\u578b\u5c3a\u5bf8\u548c\u9ad8\u6602\u7684\u901a\u4fe1\u5f00\u9500\u7ed9\u5728\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\uff08\u5c24\u5176\u4e3a\u652f\u63016G\u5b9e\u65f6\u667a\u80fd\u670d\u52a1\uff09\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\uff0c\u4f20\u7edf\u8fb9\u7f18AI\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u7814\u7a76\u4e86\u6a21\u578b\u5206\u89e3\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\u3002\u63d0\u51fa\u4e86\u534f\u540c\u5fae\u8c03\u3001\u5168\u53c2\u6570\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5fae\u670d\u52a1\u8f85\u52a9\u7684\u63a8\u7406\u67b6\u6784\u6765\u4fc3\u8fdb\u8fb9\u7f18LAM\u5728\u65e0\u7ebf\u7f51\u7edc\u4e0a\u7684\u90e8\u7f72\u3002\u540c\u65f6\uff0c\u63a2\u8ba8\u4e86\u8fb9\u7f18LAM\u5728\u4fe1\u9053\u9884\u6d4b\u548c\u6ce2\u675f\u8d4b\u5f62\u7b49\u7a7a\u53e3\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u8bc6\u522b\u4e86\u8fb9\u7f18LAM\u90e8\u7f72\u9762\u4e34\u7684\u4e3b\u8981\u969c\u788d\uff08\u901a\u4fe1\u3001\u8ba1\u7b97\u3001\u5b58\u50a8\u8d44\u6e90\u9650\u5236\uff0c\u6a21\u578b\u53c2\u6570\u591a\uff0c\u901a\u4fe1\u5f00\u9500\u5927\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u6846\u67b6\uff08\u534f\u540c\u5fae\u8c03\u3001\u5168\u53c2\u6570\u8bad\u7ec3\u3001\u5fae\u670d\u52a1\u63a8\u7406\uff09\u548c\u5177\u4f53\u5e94\u7528\uff08\u4fe1\u9053\u9884\u6d4b\u3001\u6ce2\u675f\u8d4b\u5f62\uff09\u4f5c\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fb9\u7f18LAM\u662f\u8d4b\u80fd6G\u5404\u9879\u670d\u52a1\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u90e8\u7f72\u9700\u8981\u514b\u670d\u8d44\u6e90\u548c\u6a21\u578b\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u5e94\u7528\u4e3a\u57286G\u4e2d\u6210\u529f\u90e8\u7f72\u548c\u5229\u7528\u8fb9\u7f18LAM\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00447", "pdf": "https://arxiv.org/pdf/2505.00447", "abs": "https://arxiv.org/abs/2505.00447", "authors": ["Govind Rajendran", "Samar Agnihotri"], "title": "Deterministic Scheduling over Wi-Fi 6 using Target Wake Time: An Experimental Approach", "categories": ["cs.NI"], "comment": "16 pages, 13 figures, and 7 tables", "summary": "Wi-Fi networks traditionally use Distributed Coordination Function (DCF) that\nemploys CSMA/CA along with the binary backoff mechanism for channel access.\nThis causes unavoidable contention overheads and does not provide performance\nguarantees. In this work, we outline some issues that occur with the\nprobabilistic channel access in highly congested scenarios and how those can be\nmitigated using deterministic scheduling. Towards this, we propose to use\nTarget Wake Time (TWT) - a feature introduced in Wi-Fi 6 as a power-saving\nmechanism, to improve the performance of Wi-Fi. To gain insights into the\nworkings of the TWT over commercially available off-the-shelf components and to\nanalyze the factors that affect its performance, we carry out various\nexperiments with it over our Wi-Fi 6 testbed. Using these insights and\nanalysis, we formulate and solve an optimization problem to synthesize\ndeterministic schedules and obtain the optimal values of various system\nparameters. Lastly, we configure our testbed with these optimal parameter\nvalues and show that the TWT based deterministic scheduling consistently\nresults in better performance of the TWT-capable clients and overall system\nperformance compared to traditional CSMA/CA based scheduling.", "AI": {"tldr": "\u5229\u7528Wi-Fi 6\u7684TWT\u529f\u80fd\u5b9e\u73b0\u786e\u5b9a\u6027\u8c03\u5ea6\uff0c\u4ee5\u6539\u5584\u62e5\u585e\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfWi-Fi\u7684CSMA/CA\u673a\u5236\u5728\u62e5\u585e\u573a\u666f\u4e0b\u5b58\u5728\u4e89\u7528\u5f00\u9500\u5927\u3001\u65e0\u6cd5\u63d0\u4f9b\u6027\u80fd\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728Wi-Fi 6\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884cTWT\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\uff1b\u6784\u5efa\u5e76\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u4ee5\u83b7\u5f97\u786e\u5b9a\u6027\u8c03\u5ea6\u7684\u6700\u4f18\u53c2\u6570\uff1b\u5728\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u914d\u7f6e\u6700\u4f18\u53c2\u6570\u5e76\u4e0e\u4f20\u7edfCSMA/CA\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8eTWT\u7684\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u652f\u6301TWT\u7684\u5ba2\u6237\u7aef\u548c\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7684CSMA/CA\u8c03\u5ea6\u3002", "conclusion": "\u4f7f\u7528Wi-Fi 6\u7684TWT\u529f\u80fd\u8fdb\u884c\u786e\u5b9a\u6027\u8c03\u5ea6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347Wi-Fi\u7f51\u7edc\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u62e5\u585e\u73af\u5883\u4e0b\u3002"}}
{"id": "2505.00605", "pdf": "https://arxiv.org/pdf/2505.00605", "abs": "https://arxiv.org/abs/2505.00605", "authors": ["Sotiris Chatzimiltis", "Mohammad Shojafar", "Mahdi Boloursaz Mashhadi", "Rahim Tafazolli"], "title": "Surviving the Storm: The Impacts of Open RAN Disaggregation on Latency and Resilience", "categories": ["cs.NI"], "comment": "13 pages, 10 figures", "summary": "The development of Open Radio Access Networks (Open RAN), with their\ndisaggregated architectures and virtualization of network functions, has\nbrought considerable flexibility and cost savings to mobile networks. However,\nthese architectural advancements introduce additional latency during the\ninitial attachment procedure of User Equipment (UE), increasing the risk of\nsignaling storms. This paper investigates the latency impact due to\ndisaggregation of the Base-band Unit (BBU) into the Central Unit (CU) and\nDistributed Unit (DU). Specifically, we model the delays induced due to\ndisaggregation on UE attachment, analyzing the performance under varying load\nconditions, and sensitivity to processing times. We demonstrate that while both\nmonolithic and Open RAN architectures experience performance degradation under\nhigh-load conditions, Open RAN's added overheads can increase its\nsusceptibility to congestion and signaling storms. However, Open RAN's inherent\nflexibility, enabled by disaggregation and virtualization, allows efficient\ndeployment of resources, faster service deployment, and adaptive congestion\ncontrol mechanisms to mitigate these risks and enhance overall system\nresilience. Thereby, we quantify resilience by introducing a new utility\nfunction and propose a novel adaptation mechanism to reinforce Open RAN's\nrobustness against signaling storms. Our results show that the proposed\nadaptive mechanism significantly enhances resilience, achieving improvements of\nup to 286% over fixed configurations, with resilience scores approaching 0.96\nunder optimal conditions. While simulation results show that Open RAN\ndisaggregation increases attachment latency and susceptibility to signaling\ncongestion, they also highlight that its architectural flexibility can mitigate\nthese effects, improving resilience under high-load conditions.", "AI": {"tldr": "\u5206\u6790\u4e86 Open RAN \u89e3\u8026\u67b6\u6784\u5bf9 UE \u63a5\u5165\u65f6\u5ef6\u548c\u4fe1\u4ee4\u98ce\u66b4\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u97e7\u6027\u3002", "motivation": "Open RAN \u7684\u89e3\u8026\u67b6\u6784\u5e26\u6765\u4e86\u7075\u6d3b\u6027\u4f46\u4e5f\u5f15\u5165\u4e86\u989d\u5916\u7684\u521d\u59cb\u63a5\u5165\u65f6\u5ef6\uff0c\u589e\u52a0\u4e86\u4fe1\u4ee4\u98ce\u66b4\u7684\u98ce\u9669\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u8fd9\u79cd\u65f6\u5ef6\u5f71\u54cd\uff0c\u5e76\u63a2\u7a76\u5982\u4f55\u5229\u7528 Open RAN \u7684\u7075\u6d3b\u6027\u6765\u7f13\u89e3\u98ce\u9669\u3002", "method": "\u5bf9\u57fa\u5e26\u5355\u5143 (BBU) \u89e3\u8026\u4e3a\u4e2d\u592e\u5355\u5143 (CU) \u548c\u5206\u5e03\u5355\u5143 (DU) \u9020\u6210\u7684 UE \u63a5\u5165\u65f6\u5ef6\u8fdb\u884c\u5efa\u6a21\uff1b\u5206\u6790\u4e0d\u540c\u8d1f\u8f7d\u548c\u5904\u7406\u65f6\u95f4\u4e0b\u7684\u6027\u80fd\uff1b\u5f15\u5165\u65b0\u7684\u6548\u7528\u51fd\u6570\u6765\u91cf\u5316\u97e7\u6027\uff1b\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u673a\u5236\u6765\u589e\u5f3a Open RAN \u5bf9\u6297\u4fe1\u4ee4\u98ce\u66b4\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cOpen RAN \u89e3\u8026\u589e\u52a0\u4e86\u63a5\u5165\u65f6\u5ef6\u548c\u5bf9\u4fe1\u4ee4\u62e5\u585e\u7684\u654f\u611f\u6027\u3002\u4f46\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u673a\u5236\u80fd\u663e\u8457\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\uff0c\u76f8\u6bd4\u56fa\u5b9a\u914d\u7f6e\u63d0\u5347\u9ad8\u8fbe 286%\uff0c\u6700\u4f18\u6761\u4ef6\u4e0b\u97e7\u6027\u8bc4\u5206\u63a5\u8fd1 0.96\u3002", "conclusion": "\u867d\u7136 Open RAN \u89e3\u8026\u589e\u52a0\u4e86\u65f6\u5ef6\u548c\u62e5\u585e\u98ce\u9669\uff0c\u4f46\u5176\u67b6\u6784\u7684\u7075\u6d3b\u6027\u5141\u8bb8\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u7b49\u624b\u6bb5\u6709\u6548\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u7684\u6574\u4f53\u97e7\u6027\u3002"}}
{"id": "2505.00001", "pdf": "https://arxiv.org/pdf/2505.00001", "abs": "https://arxiv.org/abs/2505.00001", "authors": ["Shaun Baek", "Shaun Esua-Mensah", "Cyrus Tsui", "Sejan Vigneswaralingam", "Abdullah Alali", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are primarily trained on high-resource natural\nlanguages, limiting their effectiveness in low-resource settings and in tasks\nrequiring deep logical reasoning. This research introduces Rosetta-PL, a\nbenchmark designed to evaluate LLMs' logical reasoning and generalization\ncapabilities in a controlled environment. We construct Rosetta-PL by\ntranslating a dataset of logical propositions from Lean into a custom logical\nlanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Our\nexperiments analyze the impact of the size of the dataset and the translation\nmethodology on the performance of the model. Our results indicate that\npreserving logical relationships in the translation process significantly\nboosts precision, with accuracy plateauing beyond roughly 20,000 training\nsamples. These insights provide valuable guidelines for optimizing LLM training\nin formal reasoning tasks and improving performance in various low-resource\nlanguage applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a Rosetta-PL \u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06 Lean \u903b\u8f91\u547d\u9898\u7ffb\u8bd1\u6210\u81ea\u5b9a\u4e49\u903b\u8f91\u8bed\u8a00\u6765\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u903b\u8f91\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u9ad8\u8d44\u6e90\u81ea\u7136\u8bed\u8a00\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u548c\u9700\u8981\u6df1\u5ea6\u903b\u8f91\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u6784\u5efa\u4e86 Rosetta-PL \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06 Lean \u7684\u903b\u8f91\u547d\u9898\u6570\u636e\u96c6\u7ffb\u8bd1\u6210\u81ea\u5b9a\u4e49\u903b\u8f91\u8bed\u8a00\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982 GPT-4o\uff09\uff0c\u5206\u6790\u6570\u636e\u96c6\u5927\u5c0f\u548c\u7ffb\u8bd1\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u903b\u8f91\u5173\u7cfb\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7cbe\u786e\u5ea6\uff1b\u5f53\u8bad\u7ec3\u6837\u672c\u91cf\u8d85\u8fc7\u7ea6 20,000 \u6761\u540e\uff0c\u51c6\u786e\u7387\u8d8b\u4e8e\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u4ee5\u53ca\u63d0\u5347\u5728\u5404\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2505.00044", "pdf": "https://arxiv.org/pdf/2505.00044", "abs": "https://arxiv.org/abs/2505.00044", "authors": ["Richard Schmit"], "title": "Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors", "categories": ["cs.CV", "math.OC"], "comment": null, "summary": "Detecting small objects remains a significant challenge in single-shot object\ndetectors due to the inherent trade-off between spatial resolution and semantic\nrichness in convolutional feature maps. To address this issue, we propose a\nnovel framework that enables small object representations to \"borrow\"\ndiscriminative features from larger, semantically richer instances within the\nsame class. Our architecture introduces three key components: the Feature\nMatching Block (FMB) to identify semantically similar descriptors across\nlayers, the Feature Representing Block (FRB) to generate enhanced shallow\nfeatures through weighted aggregation, and the Feature Fusion Block (FFB) to\nrefine feature maps by integrating original, borrowed, and context information.\nBuilt upon the SSD framework, our method improves the descriptive capacity of\nshallow layers while maintaining real-time detection performance. Experimental\nresults demonstrate that our approach significantly boosts small object\ndetection accuracy over baseline methods, offering a promising direction for\nrobust object detection in complex visual environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5c0f\u76ee\u6807\u5b9e\u4f8b\u201c\u501f\u7528\u201d\u540c\u7c7b\u5927\u76ee\u6807\u7684\u7279\u5f81\u6765\u63d0\u5347\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5355\u9636\u6bb5\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u68c0\u6d4b\u5c0f\u76ee\u6807\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5377\u79ef\u7279\u5f81\u56fe\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u8bed\u4e49\u4e30\u5bcc\u5ea6\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u6743\u8861\u3002", "method": "\u5728SSD\u6846\u67b6\u57fa\u7840\u4e0a\u5f15\u5165\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7279\u5f81\u5339\u914d\u5757\uff08FMB\uff09\u7528\u4e8e\u8de8\u5c42\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u7684\u63cf\u8ff0\u7b26\uff1b\u7279\u5f81\u8868\u793a\u5757\uff08FRB\uff09\u901a\u8fc7\u52a0\u6743\u805a\u5408\u751f\u6210\u589e\u5f3a\u7684\u6d45\u5c42\u7279\u5f81\uff1b\u7279\u5f81\u878d\u5408\u5757\uff08FFB\uff09\u901a\u8fc7\u6574\u5408\u539f\u59cb\u3001\u501f\u7528\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u4f18\u5316\u7279\u5f81\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4fdd\u6301\u4e86\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u76ee\u6807\u68c0\u6d4b\uff08\u7279\u522b\u662f\u5c0f\u76ee\u6807\u68c0\u6d4b\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.00018", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "AI": {"tldr": "\u8be5\u7acb\u573a\u6587\u4ef6\u56de\u987e\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6700\u65b0\u5b9e\u8bc1\u7814\u7a76\uff0c\u6307\u51fa\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\u201d\u7684\u65b0\u6982\u5ff5\u67b6\u6784\u6765\u6574\u5408\u4e0d\u540c\u65b9\u6cd5\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u89c2\u5bdf\u5230\u5f53\u524d\u4eba\u673a\u534f\u4f5c\u9886\u57df\u7684\u5b9e\u8bc1\u7814\u7a76\u867d\u7136\u4f17\u591a\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7cfb\u7edf\u5730\u6574\u5408\u8fd9\u4e9b\u4e0d\u540c\u7684\u6210\u679c\uff0c\u7279\u522b\u662f\u5728\u5e94\u5bf9\u5f00\u653e\u5f0f\u3001\u590d\u6742\u4efb\u52a1\u65f6\u3002", "method": "\u6279\u5224\u6027\u5730\u8c03\u7814\u4e86\u4eba\u673a\u534f\u4f5c\u9886\u57df\u7684\u6700\u65b0\u5b9e\u8bc1\u8fdb\u5c55\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\u67b6\u6784\uff08\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff09\uff0c\u8be5\u67b6\u6784\u6574\u5408\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u53cd\u9988\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\uff1b\u5c06\u73b0\u6709\u7684\u5404\u79cd\u8d21\u732e\uff08\u5305\u62ec\u7b26\u53f7AI\u3001\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u3001\u6df7\u5408\u7ec4\u7ec7\u5b9e\u8df5\u7b49\uff09\u6620\u5c04\u5230\u6b64\u6846\u67b6\u4e0a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u5ff5\u67b6\u6784\uff08\u5206\u5c42\u63a2\u7d22-\u5229\u7528\u7f51\u7edc\uff09\uff0c\u8be5\u67b6\u6784\u6709\u52a9\u4e8e\u68b3\u7406\u73b0\u6709\u7814\u7a76\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u4fee\u8ba2\uff0c\u5e76\u542f\u53d1\u878d\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u8303\u5f0f\u7684\u65b0\u5de5\u4f5c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u66f4\u6df1\u5165\u5730\u7406\u89e3\u548c\u8bbe\u8ba1\u4eba\u673a\u5171\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u4e0d\u540c\u6280\u672f\u65b9\u6cd5\u7684\u878d\u5408\uff0c\u5e76\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u7684\u672a\u6765\u5171\u540c\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.00101", "pdf": "https://arxiv.org/pdf/2505.00101", "abs": "https://arxiv.org/abs/2505.00101", "authors": ["Barak Gahtan", "Sanketh Vedula", "Gil Samuelly Leichtag", "Einat Kodesh", "Alex M. Bronstein"], "title": "From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Understanding physiological responses during running is critical for\nperformance optimization, tailored training prescriptions, and athlete health\nmanagement. We introduce a comprehensive framework -- what we believe to be the\nfirst capable of predicting instantaneous oxygen consumption (VO$_{2}$)\ntrajectories exclusively from consumer-grade wearable data. Our approach\nemploys two complementary physiological models: (1) accurate modeling of heart\nrate (HR) dynamics via a physiologically constrained ordinary differential\nequation (ODE) and neural Kalman filter, trained on over 3 million HR\nobservations, achieving 1-second interval predictions with mean absolute errors\nas low as 2.81\\,bpm (correlation 0.87); and (2) leveraging the principles of\nprecise HR modeling, a novel VO$_{2}$ prediction architecture requiring only\nthe initial second of VO$_{2}$ data for calibration, enabling robust,\nsequence-to-sequence metabolic demand estimation. Despite relying solely on\nsmartwatch and chest-strap data, our method achieves mean absolute percentage\nerrors of approximately 13\\%, effectively capturing rapid physiological\ntransitions and steady-state conditions across diverse running intensities. Our\nsynchronized dataset, complemented by blood lactate measurements, further lays\nthe foundation for future noninvasive metabolic zone identification. By\nembedding physiological constraints within modern machine learning, this\nframework democratizes advanced metabolic monitoring, bridging laboratory-grade\naccuracy and everyday accessibility, thus empowering both elite athletes and\nrecreational fitness enthusiasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6d88\u8d39\u7ea7\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\uff08\u5fc3\u7387\uff09\u548c\u751f\u7406\u6a21\u578b\u6765\u5b9e\u65f6\u9884\u6d4b\u8dd1\u6b65\u8fc7\u7a0b\u4e2d\u77ac\u65f6\u6444\u6c27\u91cf\uff08VO2\uff09\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u7406\u89e3\u8dd1\u6b65\u65f6\u7684\u751f\u7406\u53cd\u5e94\uff08\u5982VO2\uff09\u5bf9\u4f18\u5316\u8868\u73b0\u3001\u5b9a\u5236\u8bad\u7ec3\u548c\u5065\u5eb7\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5c40\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u901a\u8fc7\u666e\u901a\u53ef\u7a7f\u6234\u8bbe\u5907\u8fdb\u884c\u5b9e\u65f6VO2\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u578b\uff1a1) \u4f7f\u7528\u751f\u7406\u7ea6\u675f\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u548c\u795e\u7ecf\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7cbe\u786e\u5efa\u6a21\u5fc3\u7387\uff08HR\uff09\u52a8\u6001\uff0c\u57fa\u4e8e\u8d85\u8fc7300\u4e07\u6b21HR\u89c2\u6d4b\u6570\u636e\u8bad\u7ec3\uff1b2) \u5229\u7528\u7cbe\u786e\u7684HR\u5efa\u6a21\u539f\u7406\uff0c\u6784\u5efa\u4e00\u79cd\u65b0\u7684VO2\u9884\u6d4b\u67b6\u6784\uff0c\u4ec5\u9700\u521d\u59cb1\u79d2\u7684VO2\u6570\u636e\u8fdb\u884c\u6821\u51c6\uff0c\u5373\u53ef\u6839\u636e\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\uff08\u667a\u80fd\u624b\u8868\u548c\u5fc3\u7387\u80f8\u5e26\uff09\u8fdb\u884c\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u4ee3\u8c22\u9700\u6c42\u4f30\u8ba1\u3002", "result": "\u5fc3\u7387\u6a21\u578b\u57281\u79d2\u95f4\u9694\u9884\u6d4b\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u81f32.81 bpm\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08\u76f8\u5173\u60270.87\uff09\u3002VO2\u9884\u6d4b\u6a21\u578b\u4ec5\u4f9d\u8d56\u6d88\u8d39\u7ea7\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u7ea6\u4e3a13%\uff0c\u80fd\u6709\u6548\u6355\u6349\u4e0d\u540c\u8dd1\u6b65\u5f3a\u5ea6\u4e0b\u7684\u5feb\u901f\u751f\u7406\u8f6c\u53d8\u548c\u7a33\u6001\u6761\u4ef6\u3002\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u8840\u4e73\u9178\u6d4b\u91cf\u7684\u540c\u6b65\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u751f\u7406\u7ea6\u675f\u5d4c\u5165\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u6d88\u8d39\u7ea7\u53ef\u7a7f\u6234\u8bbe\u5907\u9884\u6d4bVO2\u8f68\u8ff9\uff0c\u4f7f\u5148\u8fdb\u7684\u4ee3\u8c22\u76d1\u6d4b\u66f4\u52a0\u666e\u53ca\uff0c\u5f25\u5408\u4e86\u5b9e\u9a8c\u5ba4\u7cbe\u5ea6\u4e0e\u65e5\u5e38\u53ef\u53ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u60e0\u53ca\u7cbe\u82f1\u8fd0\u52a8\u5458\u548c\u5065\u8eab\u7231\u597d\u8005\u3002"}}
{"id": "2505.00472", "pdf": "https://arxiv.org/pdf/2505.00472", "abs": "https://arxiv.org/abs/2505.00472", "authors": ["Alaa Saleh", "Sasu Tarkoma", "Praveen Kumar Donta", "Naser Hossein Motlagh", "Schahram Dustdar", "Susanna Pirttikangas", "Lauri Lov\u00e9n"], "title": "UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "comment": null, "summary": "Agentic AI, with its autonomous and proactive decision-making, has\ntransformed smart environments. By integrating Generative AI (GenAI) and\nmulti-agent systems, modern AI frameworks can dynamically adapt to user\npreferences, optimize data management, and improve resource allocation. This\npaper introduces UserCentrix, an agentic memory-augmented AI framework designed\nto enhance smart spaces through dynamic, context-aware decision-making. This\nframework integrates personalized Large Language Model (LLM) agents that\nleverage user preferences and LLM memory management to deliver proactive and\nadaptive assistance. Furthermore, it incorporates a hybrid hierarchical control\nsystem, balancing centralized and distributed processing to optimize real-time\nresponsiveness while maintaining global situational awareness. UserCentrix\nachieves resource-efficient AI interactions by embedding memory-augmented\nreasoning, cooperative agent negotiation, and adaptive orchestration\nstrategies. Our key contributions include (i) a self-organizing framework with\nproactive scaling based on task urgency, (ii) a Value of Information\n(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM\nagent, and (iv) an intelligent multi-agent coordination system for seamless\nenvironment adaptation. Experimental results across various models confirm the\neffectiveness of our approach in enhancing response accuracy, system\nefficiency, and computational resource management in real-world application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUserCentrix\u7684\u667a\u80fd\u8bb0\u5fc6\u589e\u5f3a\u578bAI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4ee3\u7406\u548c\u6df7\u5408\u63a7\u5236\u7cfb\u7edf\u6765\u4f18\u5316\u667a\u80fd\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u7684\u667a\u80fd\u73af\u5883\u9700\u8981\u66f4\u52a8\u6001\u3001\u66f4\u9002\u5e94\u7528\u6237\u504f\u597d\u3001\u66f4\u9ad8\u6548\u7684AI\u7cfb\u7edf\u6765\u7ba1\u7406\u6570\u636e\u548c\u8d44\u6e90\u3002", "method": "\u5f00\u53d1\u4e86UserCentrix\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\uff1a1) \u4e2a\u6027\u5316\u3001\u5177\u6709\u8bb0\u5fc6\u80fd\u529b\u7684LLM\u4ee3\u7406\uff1b2) \u7ed3\u5408\u4e2d\u5fc3\u5316\u548c\u5206\u5e03\u5f0f\u5904\u7406\u7684\u6df7\u5408\u5206\u5c42\u63a7\u5236\u7cfb\u7edf\uff1b3) \u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u7684\u51b3\u7b56\u8fc7\u7a0b\uff1b4) \u81ea\u6211\u7ec4\u7ec7\u7684\u4efb\u52a1\u7d27\u6025\u6027\u4e3b\u52a8\u6269\u5c55\uff1b5) \u5143\u63a8\u7406\u4e2a\u4ebaLLM\u4ee3\u7406\uff1b6) \u667a\u80fd\u591a\u4ee3\u7406\u534f\u8c03\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cUserCentrix\u5728\u63d0\u9ad8\u54cd\u5e94\u51c6\u786e\u6027\u3001\u7cfb\u7edf\u6548\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "UserCentrix\u6846\u67b6\u901a\u8fc7\u5176\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u7279\u6027\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u667a\u80fd\u7a7a\u95f4\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2505.00002", "pdf": "https://arxiv.org/pdf/2505.00002", "abs": "https://arxiv.org/abs/2505.00002", "authors": ["Vincent C. M\u00fcller"], "title": "Symbol grounding in computational systems: A paradox of intentions", "categories": ["cs.CL"], "comment": null, "summary": "The paper presents a paradoxical feature of computational systems that\nsuggests that computationalism cannot explain symbol grounding. If the mind is\na digital computer, as computationalism claims, then it can be computing either\nover meaningful symbols or over meaningless symbols. If it is computing over\nmeaningful symbols its functioning presupposes the existence of meaningful\nsymbols in the system, i.e. it implies semantic nativism. If the mind is\ncomputing over meaningless symbols, no intentional cognitive processes are\navailable prior to symbol grounding. In this case, no symbol grounding could\ntake place since any grounding presupposes intentional cognitive processes. So,\nwhether computing in the mind is over meaningless or over meaningful symbols,\ncomputationalism implies semantic nativism.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u8ba1\u7b97\u4e3b\u4e49\u5728\u89e3\u91ca\u7b26\u53f7\u63a5\u5730\u65f6\u5b58\u5728\u6096\u8bba\uff0c\u65e0\u8bba\u5fc3\u667a\u8ba1\u7b97\u662f\u6709\u610f\u4e49\u7b26\u53f7\u8fd8\u662f\u65e0\u610f\u4e49\u7b26\u53f7\uff0c\u6700\u7ec8\u90fd\u4f1a\u5bfc\u5411\u8bed\u4e49\u5929\u751f\u8bba\u3002", "motivation": "\u6311\u6218\u8ba1\u7b97\u4e3b\u4e49\uff08\u8ba4\u4e3a\u5fc3\u667a\u662f\u6570\u5b57\u8ba1\u7b97\u673a\u7684\u89c2\u70b9\uff09\uff0c\u8bba\u8bc1\u5176\u65e0\u6cd5\u89e3\u91ca\u7b26\u53f7\u63a5\u5730\uff08\u7b26\u53f7\u5982\u4f55\u83b7\u5f97\u610f\u4e49\uff09\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u903b\u8f91\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e00\u4e2a\u4e24\u96be\u63a8\u7406\uff1a\u5206\u6790\u5fc3\u667a\u8ba1\u7b97\u662f\u57fa\u4e8e\u201c\u6709\u610f\u4e49\u7b26\u53f7\u201d\u8fd8\u662f\u201c\u65e0\u610f\u4e49\u7b26\u53f7\u201d\u8fd9\u4e24\u79cd\u53ef\u80fd\u6027\u53ca\u5176\u5bf9\u7b26\u53f7\u63a5\u5730\u7684\u5f71\u54cd\u3002", "result": "\u5982\u679c\u5fc3\u667a\u57fa\u4e8e\u6709\u610f\u4e49\u7b26\u53f7\u8fdb\u884c\u8ba1\u7b97\uff0c\u5219\u5176\u529f\u80fd\u9884\u8bbe\u4e86\u610f\u4e49\u7684\u5b58\u5728\uff0c\u5373\u5bfc\u5411\u8bed\u4e49\u5929\u751f\u8bba\u3002\u5982\u679c\u5fc3\u667a\u57fa\u4e8e\u65e0\u610f\u4e49\u7b26\u53f7\u8ba1\u7b97\uff0c\u5219\u5728\u7b26\u53f7\u63a5\u5730\u4e4b\u524d\u4e0d\u5b58\u5728\u610f\u5411\u6027\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u800c\u4efb\u4f55\u63a5\u5730\u8fc7\u7a0b\u90fd\u9700\u8981\u610f\u5411\u6027\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u56e0\u6b64\u7b26\u53f7\u63a5\u5730\u65e0\u6cd5\u53d1\u751f\u3002", "conclusion": "\u65e0\u8bba\u8ba1\u7b97\u4e3b\u4e49\u5047\u8bbe\u5fc3\u667a\u8ba1\u7b97\u662f\u57fa\u4e8e\u6709\u610f\u4e49\u7b26\u53f7\u8fd8\u662f\u65e0\u610f\u4e49\u7b26\u53f7\uff0c\u5b83\u90fd\u8574\u542b\u7740\u8bed\u4e49\u5929\u751f\u8bba\uff0c\u65e0\u6cd5\u5728\u975e\u5929\u751f\u8bba\u7684\u7acb\u573a\u4e0a\u6210\u529f\u89e3\u91ca\u7b26\u53f7\u63a5\u5730\u3002"}}
{"id": "2505.00134", "pdf": "https://arxiv.org/pdf/2505.00134", "abs": "https://arxiv.org/abs/2505.00134", "authors": ["Vasudev Sharma", "Ahmed Alagha", "Abdelhakim Khellaf", "Vincent Quoc-Huy Trinh", "Mahdi S. Hosseini"], "title": "Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have gained significant attention in\ncomputational pathology due to their multimodal learning capabilities that\nenhance big-data analytics of giga-pixel whole slide image (WSI). However,\ntheir sensitivity to large-scale clinical data, task formulations, and prompt\ndesign remains an open question, particularly in terms of diagnostic accuracy.\nIn this paper, we present a systematic investigation and analysis of three\nstate of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and\nCONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each\nin giga-pixel form, across distinct tissue types. Through a structured ablative\nstudy on cancer invasiveness and dysplasia status, we develop a comprehensive\nprompt engineering framework that systematically varies domain specificity,\nanatomical precision, instructional framing, and output constraints. Our\nfindings demonstrate that prompt engineering significantly impacts model\nperformance, with the CONCH model achieving the highest accuracy when provided\nwith precise anatomical references. Additionally, we identify the critical\nimportance of anatomical context in histopathological image analysis, as\nperformance consistently degraded when reducing anatomical precision. We also\nshow that model complexity alone does not guarantee superior performance, as\neffective domain alignment and domain-specific training are critical. These\nresults establish foundational guidelines for prompt engineering in\ncomputational pathology and highlight the potential of VLMs to enhance\ndiagnostic accuracy when properly instructed with domain-appropriate prompts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63d0\u793a\u5de5\u7a0b\u5bf9\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u578b\u6d88\u5316\u9053\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u8bca\u65ad\u764c\u75c7\u4fb5\u88ad\u6027\u548c\u4e0d\u5178\u578b\u589e\u751f\u72b6\u6001\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u5bf9\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u3001\u4efb\u52a1\u5236\u5b9a\u548c\u63d0\u793a\u8bbe\u8ba1\u7684\u654f\u611f\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bca\u65ad\u51c6\u786e\u6027\u65b9\u9762\uff0c\u4ecd\u662f\u672a\u89e3\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u5728\u4e00\u4e2a\u5305\u542b3507\u5f20\u5343\u5146\u50cf\u7d20WSI\u7684\u5185\u90e8\u6d88\u5316\u9053\u75c5\u7406\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u7684VLM\uff08Quilt-Net, Quilt-LLAVA, CONCH\uff09\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u6d88\u878d\u7814\u7a76\uff0c\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u8c03\u6574\u9886\u57df\u7279\u5f02\u6027\u3001\u89e3\u5256\u7cbe\u786e\u6027\u3001\u6307\u4ee4\u6846\u67b6\u548c\u8f93\u51fa\u7ea6\u675f\u3002", "result": "\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002CONCH\u6a21\u578b\u5728\u83b7\u5f97\u7cbe\u786e\u89e3\u5256\u53c2\u8003\u65f6\u51c6\u786e\u7387\u6700\u9ad8\u3002\u964d\u4f4e\u89e3\u5256\u7cbe\u786e\u6027\u4f1a\u5bfc\u81f4\u6027\u80fd\u6301\u7eed\u4e0b\u964d\u3002\u6a21\u578b\u590d\u6742\u6027\u672c\u8eab\u5e76\u4e0d\u4fdd\u8bc1\u9ad8\u6027\u80fd\uff0c\u9886\u57df\u5bf9\u9f50\u548c\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6070\u5f53\u7684\u3001\u5305\u542b\u7cbe\u786e\u89e3\u5256\u80cc\u666f\u7684\u9886\u57df\u63d0\u793a\u80fd\u591f\u663e\u8457\u63d0\u5347VLM\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\u5efa\u7acb\u4e86\u57fa\u7840\u6307\u5357\u3002"}}
{"id": "2505.00173", "pdf": "https://arxiv.org/pdf/2505.00173", "abs": "https://arxiv.org/abs/2505.00173", "authors": ["Isabelle Bloch", "Enzo Bonnot", "Pietro Gori", "Giammarco La Barbera", "Sabine Sarnacki"], "title": "First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images", "categories": ["cs.AI", "cs.LO", "math.LO"], "comment": "Accepted for presentation at the FUZZ-IEEE 2025 conference", "summary": "This article deals with the description and recognition of fiber bundles, in\nparticular nerves, in medical images, based on the anatomical description of\nthe fiber trajectories. To this end, we propose a logical formalization of this\nanatomical knowledge. The intrinsically imprecise description of nerves, as\nfound in anatomical textbooks, leads us to propose fuzzy semantics combined\nwith first-order logic. We define a language representing spatial entities,\nrelations between these entities and quantifiers. A formula in this language is\nthen a formalization of the natural language description. The semantics are\ngiven by fuzzy representations in a concrete domain and satisfaction degrees of\nrelations. Based on this formalization, a spatial reasoning algorithm is\nproposed for segmentation and recognition of nerves from anatomical and\ndiffusion magnetic resonance images, which is illustrated on pelvic nerves in\npediatric imaging, enabling surgeons to plan surgery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u548c\u5f62\u5f0f\u5316\u89e3\u5256\u5b66\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u533b\u5b66\u56fe\u50cf\uff08\u7279\u522b\u662f\u5f25\u6563\u78c1\u5171\u632f\u56fe\u50cf\uff09\u4e2d\u7684\u795e\u7ecf\u7ea4\u7ef4\u675f\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u89e3\u5256\u5b66\u63cf\u8ff0\u4e2d\u56fa\u6709\u7684\u4e0d\u7cbe\u786e\u6027\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u51c6\u786e\u63cf\u8ff0\u548c\u8bc6\u522b\u795e\u7ecf\u7ea4\u7ef4\u675f\uff0c\u8f85\u52a9\u5916\u79d1\u533b\u751f\u8fdb\u884c\u624b\u672f\u89c4\u5212\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u6a21\u7cca\u8bed\u4e49\u7684\u4e00\u9636\u903b\u8f91\u6765\u5f62\u5f0f\u5316\u63cf\u8ff0\u795e\u7ecf\u7ea4\u7ef4\u8f68\u8ff9\u7684\u89e3\u5256\u5b66\u77e5\u8bc6\uff0c\u5b9a\u4e49\u4e86\u4e00\u79cd\u5305\u542b\u7a7a\u95f4\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u91cf\u8bcd\u7684\u8bed\u8a00\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u7528\u4e8e\u5206\u5272\u548c\u8bc6\u522b\u795e\u7ecf\u7684\u7a7a\u95f4\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u513f\u7ae5\u9aa8\u76c6\u795e\u7ecf\u7684\u5f25\u6563\u78c1\u5171\u632f\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u795e\u7ecf\u7684\u5206\u5272\u4e0e\u8bc6\u522b\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u89e3\u5256\u5b66\u77e5\u8bc6\u6765\u8bc6\u522b\u548c\u5206\u5272\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u795e\u7ecf\u7ed3\u6784\uff0c\u4e3a\u624b\u672f\u89c4\u5212\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.00131", "pdf": "https://arxiv.org/pdf/2505.00131", "abs": "https://arxiv.org/abs/2505.00131", "authors": ["Dalton Durant", "Renato Zanetti"], "title": "Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis\nDensity (EnGM-PHD) filter is presented for multi-target filtering applications.\nThe EnGM-PHD filter combines the Gaussian-mixture-based techniques of the\nGaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the\nparticle-based techniques of the Sequential Monte Carlo Probability Hypothesis\nDensity (SMC-PHD) filter. It achieves this by obtaining particles from the\nposterior intensity function, propagating them through the system dynamics, and\nthen using Kernel Density Estimation (KDE) techniques to approximate the\nGaussian mixture of the prior intensity function. This approach guarantees\nconvergence to the true intensity function in the limit of the number of\ncomponents. Moreover, in the special case of a single target with no births,\ndeaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces\nto the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented\nexperiment, the results indicate that the EnGM-PHD filter achieves better\nmulti-target filtering performance than both the GM-PHD and SMC-PHD filters\nwhile using the same number of components or particles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u96c6\u5408\u9ad8\u65af\u6df7\u5408\u6982\u7387\u5047\u8bbe\u5bc6\u5ea6(EnGM-PHD)\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u591a\u76ee\u6807\u6ee4\u6ce2\uff0c\u7ed3\u5408\u4e86GM-PHD\u548cSMC-PHD\u7684\u4f18\u70b9\u3002", "motivation": "\u65e8\u5728\u7ed3\u5408GM-PHD\u6ee4\u6ce2\u5668\u7684\u9ad8\u65af\u6df7\u5408\u6280\u672f\u548cSMC-PHD\u6ee4\u6ce2\u5668\u7684\u7c92\u5b50\u6280\u672f\uff0c\u5f00\u53d1\u4e00\u79cd\u6027\u80fd\u66f4\u4f18\u7684\u591a\u76ee\u6807\u6ee4\u6ce2\u5668\u3002", "method": "\u63d0\u51faEnGM-PHD\u6ee4\u6ce2\u5668\uff1a\u4ece\u540e\u9a8c\u5f3a\u5ea6\u51fd\u6570\u83b7\u53d6\u7c92\u5b50\uff0c\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u4f20\u64ad\u7c92\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1(KDE)\u6280\u672f\u6765\u8fd1\u4f3c\u5148\u9a8c\u5f3a\u5ea6\u51fd\u6570\u7684\u9ad8\u65af\u6df7\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5206\u91cf\u6216\u7c92\u5b50\u6570\u91cf\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\uff0cEnGM-PHD\u6ee4\u6ce2\u5668\u6bd4GM-PHD\u548cSMC-PHD\u6ee4\u6ce2\u5668\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u76ee\u6807\u6ee4\u6ce2\u6027\u80fd\u3002", "conclusion": "EnGM-PHD\u6ee4\u6ce2\u5668\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u76ee\u6807\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u7684GM-PHD\u548cSMC-PHD\u6ee4\u6ce2\u5668\uff0c\u5e76\u4fdd\u8bc1\u4e86\u5206\u91cf\u6570\u8d8b\u4e8e\u65e0\u7a77\u65f6\u6536\u655b\u5230\u771f\u5b9e\u5f3a\u5ea6\u51fd\u6570\u3002"}}
{"id": "2505.00618", "pdf": "https://arxiv.org/pdf/2505.00618", "abs": "https://arxiv.org/abs/2505.00618", "authors": ["Gurjot Singh", "Alim Dhanani", "Diogo Barradas"], "title": "RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "Network attackers have increasingly resorted to proxy chains, VPNs, and\nanonymity networks to conceal their activities. To tackle this issue, past\nresearch has explored the applicability of traffic correlation techniques to\nperform attack attribution, i.e., to identify an attacker's true network\nlocation. However, current traffic correlation approaches rely on\nwell-provisioned and centralized systems that ingest flows from multiple\nnetwork probes to compute correlation scores. Unfortunately, this makes\ncorrelation efforts scale poorly for large high-speed networks.\n  In this paper, we propose RevealNet, a decentralized framework for attack\nattribution that orchestrates a fleet of P4-programmable switches to perform\ntraffic correlation. RevealNet builds on a set of correlation primitives\ninspired by prior work on computing and comparing flow sketches -- compact\nsummaries of flows' key characteristics -- to enable efficient, distributed,\nin-network traffic correlation. Our evaluation suggests that RevealNet achieves\ncomparable accuracy to centralized attack attribution systems while\nsignificantly reducing both the computational complexity and bandwidth\noverheads imposed by correlation tasks.", "AI": {"tldr": "\u63d0\u51faRevealNet\uff0c\u4e00\u4e2a\u4f7f\u7528P4\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u8fdb\u884c\u6d41\u91cf\u5173\u8054\u7684\u53bb\u4e2d\u5fc3\u5316\u653b\u51fb\u6eaf\u6e90\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u653b\u51fb\u8005\u5229\u7528\u4ee3\u7406\u3001VPN\u7b49\u9690\u85cf\u8eab\u4efd\uff0c\u73b0\u6709\u7684\u4e2d\u5fc3\u5316\u6d41\u91cf\u5173\u8054\u6eaf\u6e90\u65b9\u6cd5\u5728\u5927\u578b\u9ad8\u901f\u7f51\u7edc\u4e2d\u6269\u5c55\u6027\u5dee\u3002", "method": "\u91c7\u7528\u540d\u4e3aRevealNet\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u534f\u8c03P4\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u7fa4\u7ec4\u3002\u5229\u7528\u57fa\u4e8e\u6d41\u8349\u56fe\uff08flow sketches\uff09\u7684\u5173\u8054\u539f\u8bed\uff0c\u5728\u7f51\u7edc\u5185\u90e8\u9ad8\u6548\u3001\u5206\u5e03\u5f0f\u5730\u6267\u884c\u6d41\u91cf\u5173\u8054\u3002", "result": "RevealNet\u5728\u51c6\u786e\u6027\u4e0a\u4e0e\u4e2d\u5fc3\u5316\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5e26\u5bbd\u5f00\u9500\u3002", "conclusion": "RevealNet\u4e3a\u7f51\u7edc\u653b\u51fb\u6eaf\u6e90\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u4e2d\u5fc3\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.00003", "pdf": "https://arxiv.org/pdf/2505.00003", "abs": "https://arxiv.org/abs/2505.00003", "authors": ["Zizhou Liu", "Ziwei Gong", "Lin Ai", "Zheng Hui", "Run Chen", "Colin Wayne Leach", "Michelle R. Greene", "Julia Hirschberg"], "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Psychological insights have long shaped pivotal NLP breakthroughs, including\nthe cognitive underpinnings of attention mechanisms, formative reinforcement\nlearning, and Theory of Mind-inspired social modeling. As Large Language Models\n(LLMs) continue to grow in scale and complexity, there is a rising consensus\nthat psychology is essential for capturing human-like cognition, behavior, and\ninteraction. This paper reviews how psychological theories can inform and\nenhance stages of LLM development, including data, pre-training, post-training,\nand evaluation\\&application. Our survey integrates insights from cognitive,\ndevelopmental, behavioral, social, personality psychology, and\npsycholinguistics. Our analysis highlights current trends and gaps in how\npsychological theories are applied. By examining both cross-domain connections\nand points of tension, we aim to bridge disciplinary divides and promote more\nthoughtful integration of psychology into future NLP research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u5fc3\u7406\u5b66\u7406\u8bba\u5982\u4f55\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f00\u53d1\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u6570\u636e\u3001\u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u5e94\u7528\uff09\uff0c\u4ee5\u63d0\u5347\u5176\u7c7b\u4eba\u8ba4\u77e5\u3001\u884c\u4e3a\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u9274\u4e8e\u5fc3\u7406\u5b66\u89c1\u89e3\u66fe\u63a8\u52a8\u5173\u952e\u7684NLP\u7a81\u7834\uff0c\u5e76\u4e14\u968f\u7740LLM\u65e5\u76ca\u590d\u6742\uff0c\u7814\u7a76\u8005\u8ba4\u4e3a\u5fc3\u7406\u5b66\u5bf9\u4e8e\u5b9e\u73b0LLM\u7684\u7c7b\u4eba\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u6574\u5408\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u53d1\u5c55\u5fc3\u7406\u5b66\u3001\u884c\u4e3a\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u5fc3\u7406\u5b66\u3001\u4eba\u683c\u5fc3\u7406\u5b66\u548c\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u89c1\u89e3\uff0c\u5206\u6790\u5b83\u4eec\u5728LLM\u5f00\u53d1\u4e0d\u540c\u9636\u6bb5\u7684\u5e94\u7528\u73b0\u72b6\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u5fc3\u7406\u5b66\u7406\u8bba\u5728LLM\u4e2d\u5e94\u7528\u7684\u8d8b\u52bf\u548c\u4e0d\u8db3\u4e4b\u5904\uff0c\u5e76\u8bc6\u522b\u4e86\u5fc3\u7406\u5b66\u4e0eNLP\u4e4b\u95f4\u7684\u8de8\u9886\u57df\u8054\u7cfb\u548c\u6f5c\u5728\u51b2\u7a81\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4fc3\u8fdb\u5fc3\u7406\u5b66\u4e0eNLP\u7684\u4ea4\u53c9\u878d\u5408\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u66f4\u6df1\u5165\u3001\u66f4\u5ba1\u614e\u5730\u5c06\u5fc3\u7406\u5b66\u7406\u8bba\u6574\u5408\u5230LLM\u7684\u5f00\u53d1\u4e2d\u3002"}}
{"id": "2505.00135", "pdf": "https://arxiv.org/pdf/2505.00135", "abs": "https://arxiv.org/abs/2505.00135", "authors": ["Michal Geyer", "Omer Tov", "Linyi Jin", "Richard Tucker", "Inbar Mosseri", "Tali Dekel", "Noah Snavely"], "title": "Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The rising popularity of immersive visual experiences has increased interest\nin stereoscopic 3D video generation. Despite significant advances in video\nsynthesis, creating 3D videos remains challenging due to the relative scarcity\nof 3D video data. We propose a simple approach for transforming a text-to-video\ngenerator into a video-to-stereo generator. Given an input video, our framework\nautomatically produces the video frames from a shifted viewpoint, enabling a\ncompelling 3D effect. Prior and concurrent approaches for this task typically\noperate in multiple phases, first estimating video disparity or depth, then\nwarping the video accordingly to produce a second view, and finally inpainting\nthe disoccluded regions. This approach inherently fails when the scene involves\nspecular surfaces or transparent objects. In such cases, single-layer disparity\nestimation is insufficient, resulting in artifacts and incorrect pixel shifts\nduring warping. Our work bypasses these restrictions by directly synthesizing\nthe new viewpoint, avoiding any intermediate steps. This is achieved by\nleveraging a pre-trained video model's priors on geometry, object materials,\noptics, and semantics, without relying on external geometry models or manually\ndisentangling geometry from the synthesis process. We demonstrate the\nadvantages of our approach in complex, real-world scenarios featuring diverse\nobject materials and compositions. See videos on\nhttps://video-eye2eye.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u8f6c\u6362\u4e3a\u89c6\u9891\u5230\u7acb\u4f53\u89c6\u9891\u751f\u6210\u5668\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u5408\u6210\u65b0\u7684\u89c6\u70b9\u4ee5\u521b\u5efa3D\u6548\u679c\uff0c\u65e0\u9700\u4f30\u8ba1\u6df1\u5ea6\u6216\u8fdb\u884c\u56fe\u50cf\u626d\u66f2\u3002", "motivation": "\u7acb\u4f533D\u89c6\u9891\u751f\u6210\u56e0\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\uff08\u4f30\u8ba1\u89c6\u5dee/\u6df1\u5ea6->\u626d\u66f2->\u4fee\u590d\uff09\u5728\u5904\u7406\u955c\u9762\u6216\u900f\u660e\u7269\u4f53\u65f6\u7684\u5c40\u9650\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff08\u51e0\u4f55\u3001\u6750\u8d28\u3001\u5149\u5b66\u3001\u8bed\u4e49\uff09\uff0c\u76f4\u63a5\u4ece\u8f93\u5165\u89c6\u9891\u5408\u6210\u4e00\u4e2a\u504f\u79fb\u89c6\u70b9\u7684\u89c6\u9891\u5e27\uff0c\u4ece\u800c\u5c062D\u89c6\u9891\u8f6c\u6362\u4e3a\u7acb\u4f53\u89c6\u9891\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u7684\u51e0\u4f55\u4f30\u8ba1\u548c\u626d\u66f2\u6b65\u9aa4\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u5177\u6709\u4ee4\u4eba\u4fe1\u670d\u76843D\u6548\u679c\u7684\u7acb\u4f53\u89c6\u9891\uff0c\u5e76\u5728\u5305\u542b\u591a\u6837\u5316\u6750\u8d28\u548c\u590d\u6742\u6210\u5206\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u4f18\u52bf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u591a\u9636\u6bb5\u65b9\u6cd5\u4e2d\u56e0\u89c6\u5dee\u4f30\u8ba1\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u4f2a\u5f71\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u5408\u6210\u65b0\u89c6\u70b9\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u89c6\u9891\u8f6c\u7acb\u4f53\u89c6\u9891\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u5149\u5b66\u73b0\u8c61\uff08\u5982\u955c\u9762\u53cd\u5c04\u548c\u900f\u660e\u5ea6\uff09\u65f6\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2505.00174", "pdf": "https://arxiv.org/pdf/2505.00174", "abs": "https://arxiv.org/abs/2505.00174", "authors": ["Ilan Strauss", "Isobel Moure", "Tim O'Reilly", "Sruly Rosenblat"], "title": "Real-World Gaps in AI Governance Research", "categories": ["cs.AI"], "comment": null, "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.", "AI": {"tldr": "\u5bf9\u6bd4\u9876\u5c16AI\u516c\u53f8\u4e0e\u5927\u5b66\u5728\u751f\u6210\u5f0fAI\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\uff0c\u53d1\u73b0\u516c\u53f8\u66f4\u5173\u6ce8\u90e8\u7f72\u524d\uff08\u5bf9\u9f50\u3001\u6d4b\u8bd5\uff09\uff0c\u800c\u90e8\u7f72\u9636\u6bb5\uff08\u504f\u89c1\u3001\u7279\u5b9a\u98ce\u9669\u9886\u57df\uff09\u7684\u7814\u7a76\u5173\u6ce8\u5ea6\u4e0b\u964d\uff0c\u5b58\u5728\u77e5\u8bc6\u9e3f\u6c9f\u3002", "motivation": "\u4e86\u89e3\u5e76\u6bd4\u8f83\u9886\u5148AI\u516c\u53f8\u548c\u9876\u5c16\u5927\u5b66\u5728\u751f\u6210\u5f0fAI\u5b89\u5168\u4e0e\u53ef\u9760\u6027\u65b9\u9762\u7684\u7814\u7a76\u91cd\u70b9\u548c\u6f5c\u5728\u5dee\u8ddd\u3002", "method": "\u5206\u6790\u4e862020\u5e741\u6708\u81f32025\u5e743\u6708\u671f\u95f4\uff0c\u4ece9439\u7bc7\u751f\u6210\u5f0fAI\u8bba\u6587\u4e2d\u7b5b\u9009\u51fa\u76841178\u7bc7\u5b89\u5168\u4e0e\u53ef\u9760\u6027\u8bba\u6587\uff0c\u5bf9\u6bd4\u4e86\u4e94\u5bb6\u4e3b\u8981AI\u516c\u53f8\uff08Anthropic, Google DeepMind, Meta, Microsoft, OpenAI\uff09\u548c\u516d\u6240\u9876\u5c16AI\u5927\u5b66\uff08CMU, MIT, NYU, Stanford, UC Berkeley, UW\uff09\u7684\u7814\u7a76\u4ea7\u51fa\u3002", "result": "\u4f01\u4e1aAI\u7814\u7a76\u8d8a\u6765\u8d8a\u96c6\u4e2d\u5728\u6a21\u578b\u90e8\u7f72\u524d\u7684\u9886\u57df\uff08\u5982\u6a21\u578b\u5bf9\u9f50\u3001\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\uff09\uff0c\u800c\u5bf9\u90e8\u7f72\u9636\u6bb5\u95ee\u9898\uff08\u5982\u6a21\u578b\u504f\u89c1\uff09\u7684\u5173\u6ce8\u6709\u6240\u51cf\u5f31\u3002\u5728\u9ad8\u98ce\u9669\u90e8\u7f72\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u3001\u865a\u5047\u4fe1\u606f\u3001\u8bf1\u5bfc\u6027\u4e0e\u6210\u763e\u6027\u7279\u5f81\u3001\u5e7b\u89c9\u3001\u7248\u6743\uff09\u5b58\u5728\u663e\u8457\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u4f01\u4e1a\u7814\u7a76\u7684\u65e5\u76ca\u96c6\u4e2d\u53ef\u80fd\u5bfc\u81f4\u5bf9\u5df2\u90e8\u7f72AI\u7cfb\u7edf\u884c\u4e3a\u7684\u8ba4\u77e5\u4e0d\u8db3\u3002\u5efa\u8bae\u6269\u5927\u5916\u90e8\u7814\u7a76\u4eba\u5458\u5bf9\u90e8\u7f72\u6570\u636e\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5bf9\u5e02\u573a\u4e0a\u7684AI\u884c\u4e3a\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u89c2\u5bdf\u3002"}}
{"id": "2505.00136", "pdf": "https://arxiv.org/pdf/2505.00136", "abs": "https://arxiv.org/abs/2505.00136", "authors": ["Maksim Helmann", "Alexander Strack", "Dirk Pfl\u00fcger"], "title": "GPRat: Gaussian Process Regression with Asynchronous Tasks", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "13 pages, 7 figures", "summary": "Python is the de-facto language for software development in artificial\nintelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow,\nrely on parallelization built into their BLAS backends to achieve speedup on\nCPUs. However, only applying parallelization in a low-level backend can lead to\nperformance and scaling degradation. In this work, we present a novel way of\nbinding task-based C++ code built on the asynchronous runtime model HPX to a\nhigh-level Python API using pybind11. We develop a parallel Gaussian process\n(GP) li- brary as an application. The resulting Python library GPRat combines\nthe ease of use of commonly available GP libraries with the performance and\nscalability of asynchronous runtime systems. We evaluate the per- formance on a\nmass-spring-damper system, a standard benchmark from control theory, for\nvarying numbers of regressors (features). The results show almost no binding\noverhead when binding the asynchronous HPX code using pybind11. Compared to\nGPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD\nEPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction\nspeedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number\nof features from eight to 128, we observe speedups of 29.62 and 21.19,\nrespectively. These results showcase the potential of using asynchronous tasks\nwithin Python-based AI applications.", "AI": {"tldr": "\u901a\u8fc7pybind11\u5c06\u57fa\u4e8eHPX\u5f02\u6b65\u8fd0\u884c\u65f6\u7684C++\u4ee3\u7801\u7ed1\u5b9a\u5230Python\uff0c\u5f00\u53d1\u4e86\u5e76\u884c\u9ad8\u65af\u8fc7\u7a0b\u5e93GPRat\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6838CPU\u4e0a\u4f18\u4e8eGPyTorch\u548cGPflow\u7684\u8bad\u7ec3\u548c\u9884\u6d4b\u6027\u80fd\u53ca\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524dPython AI\u5e93\u4f9d\u8d56\u5e95\u5c42BLAS\u5e76\u884c\u5316\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u548c\u6269\u5c55\u6027\u74f6\u9888\u3002", "method": "\u4f7f\u7528pybind11\u5c06\u57fa\u4e8eC++\u5f02\u6b65\u8fd0\u884c\u65f6\u6a21\u578bHPX\u7684\u4efb\u52a1\u578b\u4ee3\u7801\u7ed1\u5b9a\u5230Python API\uff0c\u5e76\u4ee5\u6b64\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e76\u884c\u9ad8\u65af\u8fc7\u7a0b\u5e93GPRat\u3002", "result": "\u7ed1\u5b9a\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002GPRat\u5728\u591a\u6838CPU\uff08\u6700\u591a64\u6838\uff09\u8bad\u7ec3\u65f6\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6269\u5c55\u6027\u3002\u9884\u6d4b\u901f\u5ea6\u4e0a\uff0cGPRat\u6bd4GPyTorch\u5feb7.63\u500d\uff0c\u6bd4GPflow\u5feb25.25\u500d\uff0c\u5e76\u4e14\u968f\u7740\u7279\u5f81\u6570\u91cf\u589e\u52a0\uff0c\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002", "conclusion": "\u5728Python AI\u5e94\u7528\u4e2d\u4f7f\u7528\u5f02\u6b65\u4efb\u52a1\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2505.00004", "pdf": "https://arxiv.org/pdf/2505.00004", "abs": "https://arxiv.org/abs/2505.00004", "authors": ["Danilo S. Carvalho", "Yingji Zhang", "Harriet Unsworth", "Andr\u00e9 Freitas"], "title": "LangVAE and LangSpace: Building and Probing for Language Model VAEs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present LangVAE, a novel framework for modular construction of variational\nautoencoders (VAEs) on top of pre-trained large language models (LLMs). Such\nlanguage model VAEs can encode the knowledge of their pre-trained components\ninto more compact and semantically disentangled representations. The\nrepresentations obtained in this way can be analysed with the LangVAE companion\nframework: LangSpace, which implements a collection of probing methods, such as\nvector traversal and interpolation, disentanglement measures, and cluster\nvisualisations. LangVAE and LangSpace offer a flexible, efficient and scalable\nway of building and analysing textual representations, with simple integration\nfor models available on the HuggingFace Hub. Additionally, we conducted a set\nof experiments with different encoder and decoder combinations, as well as\nannotated inputs, revealing a wide range of interactions across architectural\nfamilies and sizes w.r.t. generalisation and disentanglement. Our findings\ndemonstrate a promising framework for systematising the experimentation and\nunderstanding of textual representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86 LangVAE \u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u4e4b\u4e0a\u6784\u5efa\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE)\uff0c\u4ee5\u53ca LangSpace \u6846\u67b6\u7528\u4e8e\u5206\u6790\u8fd9\u4e9b VAE \u4ea7\u751f\u7684\u8868\u793a\u3002", "motivation": "\u65e8\u5728\u5c06\u9884\u8bad\u7ec3 LLM \u7684\u77e5\u8bc6\u7f16\u7801\u6210\u66f4\u7d27\u51d1\u3001\u8bed\u4e49\u89e3\u8026\u7684\u8868\u793a\uff0c\u5e76\u7cfb\u7edf\u5316\u5730\u5b9e\u9a8c\u548c\u7406\u89e3\u6587\u672c\u8868\u793a\u3002", "method": "\u6784\u5efa LangVAE\uff08\u57fa\u4e8e LLM \u7684 VAE\uff09\u548c LangSpace\uff08\u5305\u542b\u5411\u91cf\u904d\u5386\u3001\u63d2\u503c\u3001\u89e3\u8026\u5ea6\u91cf\u548c\u805a\u7c7b\u53ef\u89c6\u5316\u7b49\u63a2\u6d4b\u65b9\u6cd5\uff09\u3002\u8fdb\u884c\u4e86\u4e0d\u540c\u7f16\u7801\u5668/\u89e3\u7801\u5668\u7ec4\u5408\u548c\u6807\u6ce8\u8f93\u5165\u7684\u5b9e\u9a8c\u3002", "result": "LangVAE \u548c LangSpace \u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u6765\u6784\u5efa\u548c\u5206\u6790\u6587\u672c\u8868\u793a\uff0c\u6613\u4e8e\u4e0e HuggingFace Hub \u96c6\u6210\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u548c\u5c3a\u5bf8\u5728\u6cdb\u5316\u548c\u89e3\u8026\u65b9\u9762\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7cfb\u7edf\u5316\u5b9e\u9a8c\u548c\u7406\u89e3\u6587\u672c\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.00150", "pdf": "https://arxiv.org/pdf/2505.00150", "abs": "https://arxiv.org/abs/2505.00150", "authors": ["Minh-Hao Van", "Xintao Wu"], "title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid evolution of social media has provided enhanced communication\nchannels for individuals to create online content, enabling them to express\ntheir thoughts and opinions. Multimodal memes, often utilized for playful or\nhumorous expressions with visual and textual elements, are sometimes misused to\ndisseminate hate speech against individuals or groups. While the detection of\nhateful memes is well-researched, developing effective methods to transform\nhateful content in memes remains a significant challenge. Leveraging the\npowerful generation and reasoning capabilities of Vision-Language Models\n(VLMs), we address the tasks of detecting and mitigating hateful content. This\npaper presents two key contributions: first, a definition-guided prompting\ntechnique for detecting hateful memes, and second, a unified framework for\nmitigating hateful content in memes, named UnHateMeme, which works by replacing\nhateful textual and/or visual components. With our definition-guided prompts,\nVLMs achieve impressive performance on hateful memes detection task.\nFurthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a\nstrong capability to convert hateful memes into non-hateful forms that meet\nhuman-level criteria for hate speech and maintain multimodal coherence between\nimage and text. Through empirical experiments, we show the effectiveness of\nstate-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the\nproposed tasks, providing a comprehensive analysis of their respective\nstrengths and limitations for these tasks. This paper aims to shed light on\nimportant applications of VLMs for ensuring safe and respectful online\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u68c0\u6d4b\u548c\u6d88\u9664\uff08\u51cf\u8f7b\uff09\u7f51\u7edc\u8868\u60c5\u5305\uff08meme\uff09\u4e2d\u7684\u4ec7\u6068\u5185\u5bb9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u4e49\u5f15\u5bfc\u7684\u63d0\u793a\u65b9\u6cd5\u7528\u4e8e\u68c0\u6d4b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3a UnHateMeme \u7684\u7edf\u4e00\u6846\u67b6\u7528\u4e8e\u6d88\u9664\u4ec7\u6068\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u591a\u6a21\u6001\u8868\u60c5\u5305\u5e38\u88ab\u6ee5\u7528\u4ee5\u4f20\u64ad\u4ec7\u6068\u8a00\u8bba\uff0c\u867d\u7136\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5982\u4f55\u6709\u6548\u8f6c\u5316\u6216\u6d88\u9664\u8fd9\u4e9b\u4ec7\u6068\u5185\u5bb9\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5b9a\u4e49\u5f15\u5bfc\u7684\u63d0\u793a\u6280\u672f\u201d\u6765\u68c0\u6d4b\u4ec7\u6068\u8868\u60c5\u5305\u3002\u5176\u6b21\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a UnHateMeme \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u66ff\u6362\u4ec7\u6068\u6587\u672c\u548c/\u6216\u89c6\u89c9\u5143\u7d20\u6765\u6d88\u9664\u8868\u60c5\u5305\u4e2d\u7684\u4ec7\u6068\u5185\u5bb9\u3002", "result": "\u4f7f\u7528\u5b9a\u4e49\u5f15\u5bfc\u7684\u63d0\u793a\uff0cVLM \u5728\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002UnHateMeme \u6846\u67b6\u7ed3\u5408 VLM\uff0c\u80fd\u591f\u6709\u6548\u5730\u5c06\u4ec7\u6068\u8868\u60c5\u5305\u8f6c\u5316\u4e3a\u975e\u4ec7\u6068\u5f62\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u591a\u6a21\u6001\u8fde\u8d2f\u6027\uff0c\u5e76\u7b26\u5408\u4eba\u7c7b\u5bf9\u4ec7\u6068\u8a00\u8bba\u7684\u5224\u65ad\u6807\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86 LLaVA\u3001Gemini \u548c GPT-4o \u7b49\u5148\u8fdb\u9884\u8bad\u7ec3 VLM \u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u662f\u68c0\u6d4b\u548c\u6d88\u9664\u4ec7\u6068\u8868\u60c5\u5305\u7684\u6709\u6548\u5de5\u5177\uff0c\u5176\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u521b\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u5c0a\u91cd\u7684\u5728\u7ebf\u73af\u5883\u3002"}}
{"id": "2505.00204", "pdf": "https://arxiv.org/pdf/2505.00204", "abs": "https://arxiv.org/abs/2505.00204", "authors": ["Sumit Verma", "Pritam Prasun", "Arpit Jaiswal", "Pritish Kumar"], "title": "RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset", "categories": ["cs.AI"], "comment": null, "summary": "As AI systems become embedded in real-world applications, ensuring they meet\nethical standards is crucial. While existing AI ethics frameworks emphasize\nfairness, transparency, and accountability, they often lack actionable\nevaluation methods. This paper introduces a systematic approach using the\nResponsible AI Labs (RAIL) framework, which includes eight measurable\ndimensions to assess the normative behavior of large language models (LLMs). We\napply this framework to Anthropic's \"Values in the Wild\" dataset, containing\nover 308,000 anonymized conversations with Claude and more than 3,000 annotated\nvalue expressions. Our study maps these values to RAIL dimensions, computes\nsynthetic scores, and provides insights into the ethical behavior of LLMs in\nreal-world use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528RAIL\u6846\u67b6\u7cfb\u7edf\u6027\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709AI\u4f26\u7406\u6846\u67b6\u867d\u5f3a\u8c03\u516c\u5e73\u3001\u900f\u660e\u548c\u95ee\u8d23\uff0c\u4f46\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cfAI\u7cfb\u7edf\uff08\u7279\u522b\u662fLLM\uff09\u662f\u5426\u7b26\u5408\u4f26\u7406\u6807\u51c6\u3002", "method": "\u91c7\u7528\u8d1f\u8d23\u4efbAI\u5b9e\u9a8c\u5ba4\uff08RAIL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u516b\u4e2a\u53ef\u8861\u91cf\u7ef4\u5ea6\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8eAnthropic\u7684\u201cValues in the Wild\u201d\u6570\u636e\u96c6\uff08\u5305\u542bClaude\u7684\u533f\u540d\u5bf9\u8bdd\u548c\u6807\u6ce8\u7684\u4ef7\u503c\u8868\u8fbe\uff09\uff0c\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u4ef7\u503c\u6620\u5c04\u5230RAIL\u7ef4\u5ea6\uff0c\u5e76\u8ba1\u7b97\u7efc\u5408\u5f97\u5206\u3002", "result": "\u7814\u7a76\u6210\u529f\u5c06\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u4e2d\u7684\u4ef7\u503c\u8868\u8fbe\u6620\u5c04\u5230RAIL\u6846\u67b6\u7684\u7ef4\u5ea6\uff0c\u8ba1\u7b97\u4e86\u7efc\u5408\u5f97\u5206\uff0c\u5e76\u4e3a\u7406\u89e3LLM\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u4f26\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "RAIL\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u4e14\u53ef\u8861\u91cf\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bc4\u4f30LLM\u7684\u89c4\u8303\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u73b0\u6709\u4f26\u7406\u8bc4\u4f30\u65b9\u6cd5\u5728\u53ef\u64cd\u4f5c\u6027\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.00162", "pdf": "https://arxiv.org/pdf/2505.00162", "abs": "https://arxiv.org/abs/2505.00162", "authors": ["Nuojin Cheng", "Alireza Doostan", "Stephen Becker"], "title": "Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Efficient optimization remains a fundamental challenge across numerous\nscientific and engineering domains, especially when objective function and\ngradient evaluations are computationally expensive. While zeroth-order\noptimization methods offer effective approaches when gradients are\ninaccessible, their practical performance can be limited by the high cost\nassociated with function queries. This work introduces the bi-fidelity\nstochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order\noptimization method designed to reduce this computational burden. BF-SSD\nleverages a bi-fidelity framework, constructing a surrogate model from a\ncombination of computationally inexpensive low-fidelity (LF) and accurate\nhigh-fidelity (HF) function evaluations. This surrogate model facilitates an\nefficient backtracking line search for step size selection, for which we\nprovide theoretical convergence guarantees under standard assumptions. We\nperform a comprehensive empirical evaluation of BF-SSD across four distinct\nproblems: a synthetic optimization benchmark, dual-form kernel ridge\nregression, black-box adversarial attacks on machine learning models, and\ntransformer-based black-box language model fine-tuning. Numerical results\ndemonstrate that BF-SSD consistently achieves superior optimization performance\nwhile requiring significantly fewer HF function evaluations compared to\nrelevant baseline methods. This study highlights the efficacy of integrating\nbi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as\na promising and computationally efficient approach for tackling large-scale,\nhigh-dimensional problems encountered in various real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a BF-SSD \u7684\u65b0\u578b\u96f6\u9636\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u4fdd\u771f\u5ea6\u548c\u9ad8\u4fdd\u771f\u5ea6\u51fd\u6570\u8bc4\u4f30\u6765\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6602\u8d35\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u51fd\u6570\u67e5\u8be2\u6210\u672c\u3002", "motivation": "\u5728\u76ee\u6807\u51fd\u6570\u548c\u68af\u5ea6\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u573a\u666f\u4e0b\uff0c\u4f18\u5316\u662f\u4e00\u4e2a\u6311\u6218\u3002\u867d\u7136\u96f6\u9636\u65b9\u6cd5\u5728\u68af\u5ea6\u4e0d\u53ef\u7528\u65f6\u6709\u6548\uff0c\u4f46\u5176\u51fd\u6570\u67e5\u8be2\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u5f15\u5165\u53cc\u4fdd\u771f\u5ea6\u968f\u673a\u5b50\u7a7a\u95f4\u4e0b\u964d (BF-SSD) \u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4f4e\u6210\u672c\u7684\u4f4e\u4fdd\u771f\u5ea6 (LF) \u548c\u51c6\u786e\u7684\u9ad8\u4fdd\u771f\u5ea6 (HF) \u51fd\u6570\u8bc4\u4f30\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u6b64\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u7684\u56de\u6eaf\u7ebf\u641c\u7d22\u6765\u786e\u5b9a\u6b65\u957f\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u95ee\u9898\uff08\u7efc\u5408\u57fa\u51c6\u3001\u6838\u5cad\u56de\u5f52\u3001\u5bf9\u6297\u653b\u51fb\u3001\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cBF-SSD \u5728\u663e\u8457\u51cf\u5c11\u9ad8\u4fdd\u771f\u5ea6\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u7684\u540c\u65f6\uff0c\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "BF-SSD \u901a\u8fc7\u5c06\u53cc\u4fdd\u771f\u5ea6\u7b56\u7565\u6574\u5408\u5230\u96f6\u9636\u4f18\u5316\u4e2d\uff0c\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u7684\u8ba1\u7b97\u6602\u8d35\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.00006", "pdf": "https://arxiv.org/pdf/2505.00006", "abs": "https://arxiv.org/abs/2505.00006", "authors": ["Hayden Helm", "Tianyi Chen", "Harvey McGuinness", "Paige Lee", "Brandon Duderstadt", "Carey E. Priebe"], "title": "Toward a digital twin of U.S. Congress", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u7f8e\u56fd\u56fd\u4f1a\u8bae\u5458\u865a\u62df\u6a21\u578b\u53ef\u4f5c\u4e3a\u6570\u5b57\u5b6a\u751f\uff0c\u5176\u751f\u6210\u7684\u63a8\u6587\u53ef\u9884\u6d4b\u6295\u7968\u884c\u4e3a\u3002", "motivation": "\u9a8c\u8bc1\u57fa\u4e8e\u5927\u91cf\u63a8\u6587\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u6ee1\u8db3\u56fd\u4f1a\u8bae\u5458\u6570\u5b57\u5b6a\u751f\u7684\u5b9a\u4e49\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u653f\u6cbb\u5206\u6790\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u6536\u96c6\u6bcf\u65e5\u66f4\u65b0\u7684\u7f8e\u56fd\u56fd\u4f1a\u8bae\u5458\u63a8\u6587\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u9488\u5bf9\u7279\u5b9a\u8bae\u5458\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u751f\u6210\u6a21\u62df\u63a8\u6587\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u63a8\u6587\u9884\u6d4b\u8bae\u5458\u7684\u6295\u7968\u884c\u4e3a\u548c\u8de8\u515a\u6d3e\u503e\u5411\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u63a8\u6587\u4e0e\u8bae\u5458\u771f\u5b9e\u53d1\u5e03\u7684\u63a8\u6587\u96be\u4ee5\u533a\u5206\uff0c\u4e14\u80fd\u6709\u6548\u9884\u6d4b\u8bae\u5458\u7684\u6295\u7968\u884c\u4e3a\u548c\u91cf\u5316\u5176\u8de8\u515a\u6d3e\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u57fa\u4e8e\u63a8\u6587\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6784\u5efa\u6709\u6548\u7684\u56fd\u4f1a\u8bae\u5458\u6570\u5b57\u5b6a\u751f\uff0c\u6709\u52a9\u4e8e\u5229\u76ca\u76f8\u5173\u8005\u5206\u914d\u8d44\u6e90\u5e76\u53ef\u80fd\u5f71\u54cd\u7acb\u6cd5\u52a8\u6001\uff0c\u4f46\u8be5\u5206\u6790\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2505.00156", "pdf": "https://arxiv.org/pdf/2505.00156", "abs": "https://arxiv.org/abs/2505.00156", "authors": ["Jannik L\u00fcbberstedt", "Esteban Rivera", "Nico Uhlemann", "Markus Lienkamp"], "title": "V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision Language Models (LVLMs) have shown strong capabilities in\nunderstanding and analyzing visual scenes across various domains. However, in\nthe context of autonomous driving, their limited comprehension of 3D\nenvironments restricts their effectiveness in achieving a complete and safe\nunderstanding of dynamic surroundings. To address this, we introduce V3LMA, a\nnovel approach that enhances 3D scene understanding by integrating Large\nLanguage Models (LLMs) with LVLMs. V3LMA leverages textual descriptions\ngenerated from object detections and video inputs, significantly boosting\nperformance without requiring fine-tuning. Through a dedicated preprocessing\npipeline that extracts 3D object data, our method improves situational\nawareness and decision-making in complex traffic scenarios, achieving a score\nof 0.56 on the LingoQA benchmark. We further explore different fusion\nstrategies and token combinations with the goal of advancing the interpretation\nof traffic scenes, ultimately enabling safer autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa V3LMA \u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM)\uff0c\u5229\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u89c6\u9891\u751f\u6210\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684 3D \u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u7406\u89e3 3D \u73af\u5883\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u5bf9\u52a8\u6001\u73af\u5883\u5168\u9762\u3001\u5b89\u5168\u7406\u89e3\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165 V3LMA \u65b9\u6cd5\uff1a1) \u5c06 LLM \u4e0e LVLM \u96c6\u6210\uff1b2) \u5229\u7528\u4ece\u76ee\u6807\u68c0\u6d4b\u548c\u89c6\u9891\u8f93\u5165\u751f\u6210\u7684\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u8f93\u5165\uff1b3) \u901a\u8fc7\u4e13\u95e8\u7684\u9884\u5904\u7406\u6d41\u7a0b\u63d0\u53d6 3D \u5bf9\u8c61\u6570\u636e\uff1b4) \u63a2\u7d22\u4e0d\u540c\u7684\u878d\u5408\u7b56\u7565\u548c token \u7ec4\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u6539\u5584\u4e86\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u6001\u52bf\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5728 LingoQA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86 0.56 \u5206\u3002", "conclusion": "\u5c06 LLM \u4e0e LVLM \u76f8\u7ed3\u5408\uff0c\u5229\u7528\u6587\u672c\u63cf\u8ff0\u589e\u5f3a 3D \u573a\u666f\u7406\u89e3\uff0c\u662f\u63d0\u5347\u4ea4\u901a\u573a\u666f\u89e3\u8bfb\u80fd\u529b\u3001\u5b9e\u73b0\u66f4\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.00278", "pdf": "https://arxiv.org/pdf/2505.00278", "abs": "https://arxiv.org/abs/2505.00278", "authors": ["Lo Pang-Yun Ting", "Yu-Hao Chiang", "Yi-Tung Tsai", "Hsu-Chao Lai", "Kun-Ta Chuang"], "title": "DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing", "categories": ["cs.AI"], "comment": null, "summary": "In the semiconductor industry, integrated circuit (IC) processes play a vital\nrole, as the rising complexity and market expectations necessitate improvements\nin yield. Identifying IC defects and assigning IC testing tasks to the right\nengineers improves efficiency and reduces losses. While current studies\nemphasize fault localization or defect classification, they overlook the\nintegration of defect characteristics, historical failures, and the insights\nfrom engineer expertise, which restrains their effectiveness in improving IC\nhandling. To leverage AI for these challenges, we propose DeCo, an innovative\napproach for optimizing task assignment in IC testing. DeCo constructs a novel\ndefect-aware graph from IC testing reports, capturing co-failure relationships\nto enhance defect differentiation, even with scarce defect data. Additionally,\nit formulates defect-aware representations for engineers and tasks, reinforced\nby local and global structure modeling on the defect-aware graph. Finally, a\ncontrasting-based assignment mechanism pairs testing tasks with QA engineers by\nconsidering their skill level and current workload, thus promoting an equitable\nand efficient job dispatch. Experiments on a real-world dataset demonstrate\nthat DeCo achieves the highest task-handling success rates in different\nscenarios, exceeding 80\\%, while also maintaining balanced workloads on both\nscarce or expanded defect data. Moreover, case studies reveal that DeCo can\nassign tasks to potentially capable engineers, even for their unfamiliar\ndefects, highlighting its potential as an AI-driven solution for the real-world\nIC failure analysis and task handling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DeCo \u7684 AI \u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7f3a\u9677\u611f\u77e5\u56fe\u548c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u4efb\u52a1\u5206\u914d\u673a\u5236\uff0c\u4f18\u5316\u96c6\u6210\u7535\u8def\uff08IC\uff09\u6d4b\u8bd5\u4efb\u52a1\u5206\u914d\u7ed9\u5de5\u7a0b\u5e08\uff0c\u4ee5\u63d0\u9ad8\u5904\u7406\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7684 IC \u7f3a\u9677\u7814\u7a76\u4fa7\u91cd\u4e8e\u6545\u969c\u5b9a\u4f4d\u6216\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u7f3a\u9677\u7279\u5f81\u3001\u5386\u53f2\u6545\u969c\u548c\u5de5\u7a0b\u5e08\u7ecf\u9a8c\u7684\u6574\u5408\uff0c\u5bfc\u81f4\u5728\u63d0\u9ad8 IC \u5904\u7406\u6548\u7387\u65b9\u9762\u6548\u679c\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u6574\u5408\u8fd9\u4e9b\u56e0\u7d20\u4ee5\u4f18\u5316\u4efb\u52a1\u5206\u914d\u7684\u65b9\u6cd5\u3002", "method": "1. \u4ece IC \u6d4b\u8bd5\u62a5\u544a\u6784\u5efa\u65b0\u9896\u7684\u7f3a\u9677\u611f\u77e5\u56fe\uff0c\u6355\u6349\u5171\u540c\u5931\u6548\u5173\u7cfb\u4ee5\u533a\u5206\u7f3a\u9677\u3002 2. \u5229\u7528\u56fe\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u4e3a\u5de5\u7a0b\u5e08\u548c\u4efb\u52a1\u751f\u6210\u7f3a\u9677\u611f\u77e5\u8868\u793a\u3002 3. \u91c7\u7528\u57fa\u4e8e\u5bf9\u6bd4\u7684\u5206\u914d\u673a\u5236\uff0c\u6839\u636e\u5de5\u7a0b\u5e08\u7684\u6280\u80fd\u6c34\u5e73\u548c\u5f53\u524d\u5de5\u4f5c\u91cf\u5c06\u6d4b\u8bd5\u4efb\u52a1\u5206\u914d\u7ed9 QA \u5de5\u7a0b\u5e08\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeCo \u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u4efb\u52a1\u5904\u7406\u6210\u529f\u7387\uff08\u8d85\u8fc7 80%\uff09\uff0c\u5e76\u4e14\u65e0\u8bba\u7f3a\u9677\u6570\u636e\u7a00\u758f\u8fd8\u662f\u5145\u8db3\uff0c\u90fd\u80fd\u4fdd\u6301\u5de5\u7a0b\u5e08\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5747\u8861\u3002\u6848\u4f8b\u7814\u7a76\u8fd8\u663e\u793a\uff0cDeCo \u80fd\u591f\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u6709\u6f5c\u529b\u5904\u7406\u8be5\u4efb\u52a1\u7684\u5de5\u7a0b\u5e08\uff0c\u5373\u4f7f\u662f\u4ed6\u4eec\u4e0d\u719f\u6089\u7684\u7f3a\u9677\u7c7b\u578b\u3002", "conclusion": "DeCo \u662f\u4e00\u4e2a\u6709\u6548\u7684 AI \u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u7528\u4e8e\u5b9e\u9645\u7684 IC \u5931\u6548\u5206\u6790\u548c\u4efb\u52a1\u5904\u7406\u573a\u666f\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u5206\u914d\u7684\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u5e76\u5e73\u8861\u5de5\u7a0b\u5e08\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002"}}
{"id": "2505.00169", "pdf": "https://arxiv.org/pdf/2505.00169", "abs": "https://arxiv.org/abs/2505.00169", "authors": ["Filipp Nikitin", "Ian Dunn", "David Ryan Koes", "Olexandr Isayev"], "title": "GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep generative models have shown significant promise in generating valid 3D\nmolecular structures, with the GEOM-Drugs dataset serving as a key benchmark.\nHowever, current evaluation protocols suffer from critical flaws, including\nincorrect valency definitions, bugs in bond order calculations, and reliance on\nforce fields inconsistent with the reference data. In this work, we revisit\nGEOM-Drugs and propose a corrected evaluation framework: we identify and fix\nissues in data preprocessing, construct chemically accurate valency tables, and\nintroduce a GFN2-xTB-based geometry and energy benchmark. We retrain and\nre-evaluate several leading models under this framework, providing updated\nperformance metrics and practical recommendations for future benchmarking. Our\nresults underscore the need for chemically rigorous evaluation practices in 3D\nmolecular generation. Our recommended evaluation methods and GEOM-Drugs\nprocessing scripts are available at\nhttps://github.com/isayevlab/geom-drugs-3dgen-evaluation.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u73b0\u67093D\u5206\u5b50\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4fee\u6b63\u540e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u751f\u62103D\u5206\u5b50\u7ed3\u6784\u7684GEOM-Drugs\u57fa\u51c6\u6d4b\u8bd5\u534f\u8bae\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5305\u62ec\u9519\u8bef\u7684\u4ef7\u6001\u5b9a\u4e49\u3001\u952e\u7ea7\u8ba1\u7b97\u9519\u8bef\u4ee5\u53ca\u4f7f\u7528\u7684\u529b\u573a\u4e0e\u53c2\u8003\u6570\u636e\u4e0d\u4e00\u81f4\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6GEOM-Drugs\u6570\u636e\u96c6\uff0c\u4fee\u6b63\u6570\u636e\u9884\u5904\u7406\u4e2d\u7684\u95ee\u9898\uff0c\u6784\u5efa\u5316\u5b66\u4e0a\u51c6\u786e\u7684\u4ef7\u6001\u8868\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eGFN2-xTB\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u80fd\u91cf\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f7f\u7528\u6b64\u65b0\u6846\u67b6\u91cd\u65b0\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86\u51e0\u4e2a\u4e3b\u6d41\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u66f4\u65b0\u540e\u7684\u6a21\u578b\u6027\u80fd\u6307\u6807\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u51fa\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u57283D\u5206\u5b50\u751f\u6210\u9886\u57df\u91c7\u7528\u5316\u5b66\u4e0a\u4e25\u8c28\u7684\u8bc4\u4f30\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\u3002"}}
