<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.CV](#cs.CV) [Total: 52]
- [cs.AI](#cs.AI) [Total: 56]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.NI](#cs.NI) [Total: 15]
- [cs.DC](#cs.DC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本研究针对葡萄牙语半自动化事实核查（SAFC）系统缺乏外部证据数据集的问题，开发了一种利用大型语言模型（LLM）和搜索引擎API丰富现有新闻语料库（Fake.Br, COVID19.BR, MuMiN-PT）的方法，并引入了数据验证框架。


<details>
  <summary>Details</summary>
Motivation: 假信息迅速传播超出了人工核查能力，急需半自动化事实核查系统。然而，在葡萄牙语环境中，缺乏整合外部证据的公开数据集，现有资源多依赖文本内部特征，这阻碍了健壮的自动事实核查（AFC）系统的发展。

Method: 开发并应用了一套方法论，旨在利用外部证据丰富葡萄牙语新闻语料库。该方法模拟用户验证过程：使用LLM（Gemini 1.5 Flash）提取文本主张，并利用搜索引擎API（Google Search API, Google FactCheck Claims Search API）检索相关外部证据。此外，还引入了包含近重复检测的数据验证和预处理框架，以提升语料库质量。

Result: 成功开发并应用了一套利用外部证据丰富葡萄牙语新闻语料库的方法论，有效填补了该领域数据集的空白。同时，构建了提升语料库质量的数据验证与预处理框架。

Conclusion: 该研究通过开发并应用创新的方法，有效解决了葡萄牙语环境中缺乏整合外部证据的事实核查数据集的挑战，为未来更鲁棒的半自动化事实核查系统奠定了基础。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [2] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 本文提出一种基于检索增强生成（RAG）的动态提示策略，以提升大语言模型（LLM）在少样本生物医学命名实体识别（NER）中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在少样本生物医学命名实体识别（NER）中存在的性能挑战，尽管LLM在此领域展现出潜力。

Method: 提出一种结合检索增强生成（RAG）的动态提示策略。该方法根据输入文本动态选择带注释的上下文学习示例并更新提示。同时，作者还实施并优化了静态和动态提示工程技术，并在五个生物医学NER数据集上进行了评估，其中检索方法包括TF-IDF和SBERT。

Result: 结构化静态提示使GPT-4的平均F1分数相对基本静态提示提高了12%，GPT-3.5和LLaMA 3-70B提高了11%。动态提示进一步提升了性能，其中TF-IDF和SBERT检索方法在5样本和10样本设置下分别使平均F1分数提高了7.3%和5.6%。

Conclusion: 研究结果强调了通过RAG实现的上下文自适应提示在生物医学NER中的实用性。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [3] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 本文提出CarbonScaling框架，量化大模型精度与碳足迹关系，指出碳排放随模型规模呈幂律增长，并提供优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律仅关注大语言模型(LLM)的精度与参数、数据、计算量的关系，但忽略了LLM规模指数级增长所带来的碳排放问题。

Method: 本文提出CarbonScaling分析框架，将操作和隐含碳排放纳入LLM训练的神经缩放定律。通过整合神经缩放、GPU硬件演进、并行优化和碳排放估算模型，CarbonScaling定量连接模型精度与碳足迹。

Result: 研究表明，精度与碳排放之间存在幂律关系，但实际低效率显著增加了缩放因子。硬件技术缩放可降低中小模型碳排放，但对超大型LLM由于通信开销和GPU未充分利用而收益递减。训练优化，特别是激进的关键批次大小缩放，有助于缓解低效率。

Conclusion: CarbonScaling框架为训练更可持续和碳效率更高的大语言模型提供了关键见解。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [4] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本研究系统探讨了多语言LLM中分词对效率和模型质量的影响。通过分析和优化印度语分词，提出了一种新的数据组成算法，显著降低了token-to-word比率，并提升了模型性能和推理速度，强调了分词在高效多语言LLM开发中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下LLM的分词开发被相对忽视，现有分词器存在高token-to-word比率、上下文利用率低和推理慢等问题。

Method: 通过系统研究词汇量大小、预分词规则和训练语料组成对分词效率和模型质量的影响。在印度语脚本上进行广泛实验，并在此基础上提出了一种新的数据组成算法，以平衡多语言数据进行分词器训练。

Result: 预分词策略显著提升了模型性能；数据组成算法使平均token-to-word比率比传统方法降低约6%；本研究的分词器在平均token-to-word比率上比现有最先进的多语言印度语模型提升超过40%；这些改进带来了模型性能和推理速度的可衡量增益。

Conclusion: 分词与模型架构和训练目标一样，是构建高效、可扩展多语言LLM的关键因素。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [5] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLM）生成的高维文本嵌入导致下游任务效率低下和成本高昂的问题，本文提出了AEALT（AutoEncoder-Augmented Learning with Text）框架。该框架通过有监督的增强型自编码器将降维直接集成到LLM工作流中，学习低维、任务相关的潜在因子，并在多种任务和真实世界数据集中取得了显著优于原始嵌入和传统降维方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）生成的文本嵌入维度高，导致下游任务的效率低下和计算成本高昂。

Method: 提出AutoEncoder-Augmented Learning with Text (AEALT) 框架。该框架是一个有监督的、因子增强的降维方法，直接整合到预训练LLM工作流中。具体而言，它首先从文本中提取嵌入，然后通过一个有监督的增强型自编码器来学习低维、任务相关的潜在因子，从而建模复杂嵌入的非线性结构。

Result: AEALT在分类、异常检测和预测任务上，使用多个真实世界公共数据集进行广泛实验，结果显示其性能优于依赖原始嵌入的传统深度学习方法。数值结果表明，AEALT相对于原始嵌入和几种标准的降维方法取得了显著的性能提升。

Conclusion: AEALT是一个有效的文本嵌入降维方法，能够显著提高下游任务的效率和性能，且具有广泛的适用性，通过学习低维、任务相关的潜在因子，有效解决了高维嵌入带来的计算和效率问题。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [6] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本研究关注大型语言模型（LLMs）在教学中适应性引导学生的能力，提出GuideEval基准来评估，并发现现有模型在这方面表现不佳。文章进一步引入了一种行为引导的微调策略以提升性能，倡导以学习者为中心的对话式评估范式。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大型语言模型（LLMs）的苏格拉底式提问能力，但忽视了根据学习者认知状态进行适应性引导的关键维度。本研究旨在探讨LLMs是否能像专家导师一样，根据学习者的理解动态调整教学策略。

Method: 提出GuideEval基准，该基准基于真实的教育对话，通过三阶段行为框架（感知学习者状态、编排教学策略、启发反思）评估教学引导能力。此外，引入了一种利用行为提示教学对话的行为引导微调策略。

Result: 经验发现，现有大型语言模型在学习者困惑或需要纠正时，往往无法提供有效的适应性支架式引导。而新引入的行为引导微调策略显著提升了模型的引导性能。

Conclusion: 研究将评估重心从孤立的内容评估转向以学习者为中心的交互，倡导采用更具对话性的范式来评估苏格拉底式大型语言模型，并证明通过行为引导微调可以显著提升其适应性引导能力。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [7] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 本文提出一种利用大型语言模型自动生成高质量“遗忘数据集”的方法，用于实现模型知识的遗忘，其效果优于现有合成数据并媲美专家标注数据。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）常包含敏感、有害或受版权保护的知识，需要事后遗忘。现有遗忘流程的主要瓶颈在于构建有效的“遗忘数据集”，以指导模型移除特定知识领域。

Method: 作者提出一种可扩展的自动化方法，利用语言模型本身生成高质量的遗忘数据集。该方法通过结构化提示管道合成“教科书式”数据，仅需提供一个领域名称作为输入。

Result: 在生物安全、网络安全和《哈利·波特》小说的遗忘实验中，作者的合成数据集持续优于基线合成替代品，并与专家策划的数据集效果相当。消融研究表明，多步骤生成管道显著提高了数据多样性，从而提升了遗忘效用。

Conclusion: 合成数据集为各种新兴领域提供了一种有前景的、实用且可扩展的知识遗忘途径，无需人工干预。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [8] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 现有深度研究代理评估基准存在公平性与透明度问题。本文提出BrowseComp-Plus，一个基于固定语料库的基准，能实现受控实验，有效区分不同系统的性能，并支持对深度研究代理和检索方法进行解耦分析。


<details>
  <summary>Details</summary>
Motivation: 现有对集成LLM与搜索工具的深度研究代理的评估基准（如BrowseComp）依赖于动态、不透明的实时网络搜索API，导致评估缺乏公平性和可复现性，难以分离检索器的贡献，无法为底层LLM的能力提供受控实验的洞察。

Method: 引入BrowseComp-Plus，这是一个从BrowseComp派生出的新基准，采用固定且精心策划的语料库。每个查询都包含人工验证的支持文档和挖掘出的具有挑战性的负样本，从而实现受控实验。

Result: BrowseComp-Plus基准能有效区分不同深度研究系统的性能。例如，Search-R1模型（结合BM25检索器）准确率3.86%，而GPT-5准确率达55.9%。将GPT-5与Qwen3-Embedding-8B检索器集成后，其准确率进一步提升至70.1%，同时搜索调用次数更少。

Conclusion: BrowseComp-Plus基准能够对深度研究代理和检索方法进行全面且解耦的评估分析，从而深入洞察检索有效性、引用准确性以及深度研究系统中的上下文工程。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [9] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 研究发现，在不依赖合并列表的情况下进行BPE推理（尤其是非定向压缩）对语言模型性能影响微乎其微，为更简单、更注重隐私的标记化方案铺平道路。


<details>
  <summary>Details</summary>
Motivation: 标准BPE标记化方案的合并列表可能暴露语言模型训练数据，构成潜在的信息提取攻击面。因此，研究不依赖合并列表的BPE推理算法对下游任务性能的影响，以探索更安全的标记化方案。

Method: 本文探讨了两类不同于BPE训练编码过程的BPE推理方案：a) 有针对性地偏离合并列表（如随机合并顺序、删除/截断合并列表）；b) 不依赖合并列表但专注于文本压缩（贪婪或精确）的非定向BPE推理算法。通过在问答、机器翻译和开放式生成等多种语言建模任务上进行实验，评估这些方案对模型性能的影响。

Result: 实验结果显示，有针对性地偏离合并列表会导致语言模型性能显著下降。然而，不依赖合并列表的非定向推理算法对下游性能的影响极小，且通常远低于预期。

Conclusion: 研究结果表明，开发更简单、可能更注重隐私的标记化方案是可行的，这些方案在不严重损害模型性能的前提下，能够规避合并列表带来的潜在风险。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [10] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 研究发现，所有测试的大型语言模型在生成个人档案时，都显著存在刻板印象偏见和偏差偏见，揭示了其输出内容的潜在危害。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）应用广泛，但其局限性和潜在风险令人担忧。本研究旨在调查LLMs可能表现出的两种偏见：刻板印象偏见（将特定特质与特定人口群体关联）和偏差偏见（LLM生成内容的人口分布与真实世界分布的差异）。

Method: 通过要求四种先进的LLMs生成个人档案，研究人员检查了每个人口群体与政治立场、宗教和性取向等属性之间的关联。

Result: 实验结果显示，所有受检验的LLMs都对多个群体表现出显著的刻板印象偏见和偏差偏见。

Conclusion: 本研究结果揭示了LLMs在推断用户属性时出现的偏见，并阐明了LLM生成内容潜在的危害。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [11] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: 研究了LLM在数字资源匮乏的卡努里语上的翻译能力，发现并行句是最有效的资源，而语法资源单独使用效果不佳，且LLM在准确性上优于流利性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有LLM能利用上下文学习进行语言翻译，但对于像卡努里语这种拥有大量使用者却数字资源稀缺的语言，其翻译表现以及领域特异性任务对LLM翻译质量的影响尚不明确。

Method: 本研究为评估LLM的翻译能力，设计了两个数据集（一个专注于健康和人道主义术语，另一个包含通用术语）。通过提供不同组合的语言资源（语法、词典和并行句），测量LLM的翻译有效性。评估标准包括自动指标以及母语使用者对译文流利度和准确性的评估，并与母语使用者和人类语言学家的翻译表现进行比较。

Result: 研究结果表明，并行句仍然是LLM翻译最有效的数据源，在人类评估和自动指标中均优于其他方法。虽然整合语法能改善零样本翻译，但单独的语法资源并非有效的独立数据源。人类评估揭示LLM在翻译准确性（意义）方面表现优于流利性（语法）。

Conclusion: 研究发现LLM翻译评估应采用多维度方法，而非仅限于简单的准确性指标。此外，单独的语法信息在缺乏并行句的情况下，不足以提供有效领域特定翻译所需的充分上下文。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [12] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 研究发现，语言模型的思维过程（chain-of-thought）中的偏见与最终输出的偏见相关性不高，意味着有偏见的输出不一定源于有偏见的“思考”。


<details>
  <summary>Details</summary>
Motivation: 语言模型性能卓越，但其固有的性别、种族、社会经济地位等偏见阻碍了其广泛部署。本研究旨在探讨链式思考（chain-of-thought）提示对模型公平性的影响。

Method: 对5个流行的大型语言模型进行实验，使用公平性指标量化其思维过程和最终输出中的11种不同偏见。

Result: 实验结果表明，思维步骤中的偏见与输出偏见相关性不高（多数情况下相关性小于0.6，p值小于0.001）。

Conclusion: 与人类不同，测试模型即使有偏见决策，其内部思维过程不一定也存在偏见。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [13] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 本研究提出一个统计框架，用于识别和量化大型语言模型（LLM）作为评判者时的“自偏见”和“家族偏见”，实证发现GPT-4o和Claude 3.5 Sonnet等模型存在这些偏见，并提供了缓解偏见的建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）可用作评判者，但它们可能对其自身输出给予过高评价（自偏见），从而扭曲真实的模型性能评估。以往研究常混淆模型质量差异与偏见，或错误假设LLM和人类评判分布相同。因此，需要一个明确的方法来识别和量化这种自偏见，以获得更准确的评估。

Method: 本研究提出了一个统计框架，明确形式化了识别和估计自偏见的假设。该方法通过建模LLM评判器对其自身完成度评分分布与对其他模型完成度评分分布之间的差异来量化偏见，同时会考虑独立第三方（如人类）评判提供的底层真实完成度质量。研究在一个包含超过5000个提示-完成对的大型数据集上进行了实证分析，该数据集结合了人类专家标注和来自九个不同LLM评判器的判断。

Result: 实证分析发现，某些模型（如GPT-4o和Claude 3.5 Sonnet）系统性地给自己的输出分配更高的分数，表现出自偏见。此外，这些模型还表现出“家族偏见”，即系统性地给同一家族其他模型产生的输出分配更高的评分。

Conclusion: 研究结果揭示了使用LLM作为评判器时潜在的陷阱，并为在解释自动化评估时减轻偏见提供了实用指导。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [14] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出并评估了一个基于大型语言模型（LLMs）的可扩展框架，用于自动化标注大规模口述历史档案（特指日本裔美国人囚禁口述历史）中的语义和情感信息。


<details>
  <summary>Details</summary>
Motivation: 口述历史是重要且宝贵的记录，但由于其非结构化格式、情感复杂性和高昂的标注成本，大规模分析面临挑战，这限制了对其内容的有效访问和理解。

Method: 研究采用多阶段方法，结合专家标注、提示工程设计和LLM（ChatGPT、Llama、Qwen）评估。首先，标注了558句话用于构建高质量数据集，并评估了零样本、少样本和RAG（检索增强生成）策略。随后，利用最佳提示配置，对来自1002次访谈的92,191句话进行了大规模自动化标注。

Result: 在语义分类中，ChatGPT表现最佳（F1分数88.71%），Llama和Qwen紧随其后。在情感分析中，Llama略优于Qwen和ChatGPT，但所有模型均表现出可比较的结果。研究表明，在良好设计的提示引导下，LLMs能有效地对大规模口述历史集合进行语义和情感标注。

Conclusion: 本研究提供了一个可重用的标注流程和实际指导，用于在文化敏感的档案分析中应用LLMs。通过将档案伦理与可扩展的自然语言处理技术相结合，为数字人文和集体记忆保存中负责任地使用人工智能奠定了基础。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [15] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 该论文提出了大型语言模型（LLM）的多轮越狱概念，并构建了首个针对此场景的基准测试集MTJ-Bench，以揭示其潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 现有LLM越狱研究仅关注单轮交互，而先进LLM支持多轮对话。多轮越狱构成更严重威胁，因为它允许用户持续追问越狱细节，或导致LLM在初始越狱后对无关问题也持续提供不安全响应。

Method: 作为探索多轮越狱的第一步，作者构建了一个多轮越狱基准测试集（MTJ-Bench），用于评估一系列开源和闭源模型在该设置下的表现。

Result: 研究揭示了LLM在多轮对话中的新型安全漏洞，并提供了新的见解。

Conclusion: 通过揭示这一新漏洞，旨在呼吁社区共同努力，构建更安全的LLM，并为更深入地理解越狱机制铺平道路。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [16] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: 提出SEVADE，一个新颖的自演化多智能体分析框架，采用解耦评估，以实现抗幻觉的讽刺检测，并在基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在讽刺检测中存在单视角分析、静态推理路径及易受幻觉影响的问题，损害了其准确性和可靠性。

Method: 提出SEVADE框架，其核心是动态智能体推理引擎（DARE），利用一组基于语言学理论的专业智能体进行文本多方面解构并生成结构化推理链。随后，独立的轻量级理由裁决器（RA）仅基于该推理链进行最终分类，通过解耦推理与判断来降低幻觉风险。

Result: 在四个基准数据集上的实验表明，SEVADE达到了最先进的性能，平均准确率提高了6.75%，Macro-F1分数提高了6.29%。

Conclusion: SEVADE框架通过创新的多智能体和解耦评估方法，有效解决了现有LLM在讽刺检测中易受幻觉影响的局限性，显著提升了任务的准确性和可靠性。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [17] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 本文提出一个用于生成教学性语法反馈的标注框架和数据集，并评估了使用大语言模型生成此类反馈的不同方法，旨在改进现有AWE系统对语言学习的支持。


<details>
  <summary>Details</summary>
Motivation: 现有自动化写作评估（AWE）系统倾向于直接修正语法错误，而缺乏对学习者有益的解释和间接提示，这不利于语言学习者理解错误原因和泛化语法规则。

Method: 1. 引入一个标注框架，用于建模错误类型和其可泛化性，并提出一种错误类型分类法以识别学习者的知识缺陷。2. 基于此框架收集了一个包含已标注学习者错误及相应人工反馈（直接修正或提示）的数据集。3. 利用该数据集，评估了基于大语言模型（LLMs）的关键词引导、无关键词和模板引导等反馈生成方法。4. 由人类教师对各系统输出的相关性、事实性和可理解性进行评估。

Result: 论文报告了所开发数据集的构建过程以及所研究的各类反馈生成系统的比较性能。

Conclusion: 本研究通过提出创新框架、构建专业数据集并评估基于LLMs的反馈生成方法，为开发更有效地支持语言学习的自动化写作评估系统提供了重要基础和见解。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [18] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本文介绍了一个为梅泰文（Meitei Mayek）书写的曼尼普尔语（Manipuri）开发的文本到语音（TTS）系统，利用Tacotron 2和HiFi-GAN实现了清晰自然的语音合成。


<details>
  <summary>Details</summary>
Motivation: 为支持具有声调音系的资源稀缺语言环境，开发一个能够实现语音保存和技术包容的曼尼普尔语TTS系统。

Method: 利用Tacotron 2和HiFi-GAN构建了一个神经TTS架构，该架构适应声调音系和资源稀缺环境。具体方法包括开发梅泰文到ARPAbet的音素映射，并整理了一个单说话人数据集。

Result: 成功展示了清晰自然的语音合成效果，并通过主观和客观指标进行了验证。

Conclusion: 该系统为曼尼普尔语的语言保存和技术包容奠定了基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [19] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 本文提出一种基于标签相似度的自动对齐方法，旨在高效整合多源命名实体识别（NER）数据集，以克服数据标注成本高昂的问题，并在资源稀缺领域提升NER性能。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别（NER）对大规模高质量标注数据集的依赖性强，但其构建成本高昂且耗时。现有数据集合并方法缺乏可解释性和可扩展性，无法有效整合多源NER数据。

Method: 提出一种基于标签相似度的自动标签对齐方法。该方法结合经验相似度和语义相似度，采用贪婪配对合并策略来统一不同数据集的标签空间。

Result: 实验结果表明，该方法能够有效合并NER数据集，并在低资源金融领域显著提升了NER性能。

Conclusion: 本研究提供了一种高效、可解释且可扩展的解决方案，用于整合多源NER语料库。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [20] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP是一个能够对用户设备上异构个人数据进行复杂查询并提供答案的系统，其独特之处在于递归分解问题、构建操作树，并结合轻量级语言模型。


<details>
  <summary>Details</summary>
Motivation: 用户设备上存在海量异构个人数据，难以进行涉及筛选、连接和聚合的复杂查询。本研究旨在开发一个系统，使用户能够便捷地对这些数据进行复杂提问，并确保答案的透明度和可信赖性。

Method: 本研究提出了ReQAP系统。该系统通过递归分解复杂问题，并逐步构建一个可执行的操作符树。在问题解释和各个操作符的实现中，ReQAP明智地利用了经过微调的轻量级语言模型。系统还提供了答案计算过程的详细追踪功能。

Result: ReQAP系统能够成功处理涉及筛选、连接和聚合的复杂用户问题，并跨越异构数据源提供准确答案。其演示展示了丰富的功能性，并且能够清晰地追踪答案的计算路径和数据来源，这对于用户理解和信任系统至关重要。

Conclusion: ReQAP系统有效地解决了在异构个人数据上进行复杂查询的难题，通过创新的问题分解和语言模型应用，不仅提供了强大的查询能力，还通过结果追溯增强了透明度和用户信任，使其成为个人数据管理的重要工具。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [21] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: 提出SBS框架，通过将响应质量分数纳入训练，显著提升大型语言模型在角色化对话生成中的保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能力不断提升，但由于现有对话数据多样性有限，在对话中有效整合角色（persona）保真度仍然具有挑战性。

Method: 提出SBS（Score-Before-Speaking）框架，将响应学习及其相对质量判断统一于单个步骤。核心创新是在训练中让对话模型将增强的响应与质量分数关联，并在推理时利用该知识。使用名词替换进行数据增强，并以语义相似度分数作为响应质量代理。

Result: SBS框架在百万和十亿参数模型上均优于现有方法，并提升了模型捕捉角色一致对话的能力。实验证明，在训练时将分数纳入输入提示优于传统训练设置。

Conclusion: SBS框架通过创新的分数条件训练，有效解决了现有数据多样性不足导致的角色整合挑战，显著提升了大型语言模型在角色化对话生成中的表现和一致性。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [22] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect通过分析情感分布稳定性来检测LLM生成文本，在性能和鲁棒性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM检测方法通用性差且易受攻击，且观察到LLM输出的情感模式比人类文本更一致。

Method: 提出SentiDetect，一个模型无关的框架，通过分析情感分布稳定性的差异来检测LLM生成文本。定义了情感分布一致性和情感分布保持两个互补指标来量化这种稳定性。

Result: 在多种数据集和先进LLM上，SentiDetect的F1分数比SOTA基线提升显著（如Gemini-1.5-Pro提升超16%，GPT-4-0613提升超11%），并对重述、对抗性攻击和文本长度变化表现出更强的鲁棒性。

Conclusion: SentiDetect利用LLM和人类文本情感模式的固有差异，提供了一种有效且鲁棒的LLM生成文本检测方案，在挑战性场景下表现优越。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [23] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 本文提出一个两阶段框架，结合模型集成和指令微调大语言模型，解决了古兰经问答中的低资源挑战，并在相关任务中取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 古兰经问答面临独特挑战，源于古典阿拉伯语的语言复杂性和宗教文本的语义丰富性。

Method: 提出一个两阶段框架：篇章检索阶段，集成微调的阿拉伯语语言模型；答案抽取阶段，采用指令微调的大语言模型结合少样本提示，以克服小数据集微调的局限性。

Result: 在Quran QA 2023共享任务中取得了最先进结果：检索的MAP@10为0.3128，MRR@10为0.5763；抽取的pAP@10为0.669，显著优于现有方法。

Conclusion: 结果表明，结合模型集成和指令微调语言模型能有效解决专业领域中低资源问答的挑战。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [24] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出一种渐进式训练方法，通过利用预训练模型，实现了高性能1比特大语言模型量化，避免了从头训练的高昂成本和精度损失。


<details>
  <summary>Details</summary>
Motivation: 现有1比特大语言模型量化方法通常从头训练，未能充分利用预训练模型，导致训练成本高昂且精度显著下降。全精度与1比特表示之间存在巨大鸿沟，使得直接适应困难。

Method: 提出了一种前向和后向一致的渐进式训练方法，将浮点权重平滑转换为二值化权重。同时，引入了二值感知初始化和双尺度补偿，以降低渐进训练难度并提升性能。

Result: 在不同规模的大语言模型上，实验结果表明该方法优于现有方法。研究结果显示，可以利用预训练模型实现高性能1比特大语言模型，无需从头进行昂贵的训练。

Conclusion: 本文提出的方法有效解决了1比特大语言模型从头训练成本高昂和精度下降的问题，证明了通过渐进式训练可以利用预训练模型实现高性能1比特大语言模型。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [25] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ是一种新型抽象摘要方法，通过将文档集合压缩为语义嵌入空间中的一个平均向量，并使用生成语言模型进行解码来生成摘要。该方法旨在解决LLM的上下文长度限制和可控性问题，并在可扩展性和语义控制方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLM）的摘要方法存在上下文长度限制、难以解释和控制，以及在大规模语料库上效率不高的问题。

Method: Vec2Summ将摘要任务视为语义压缩。它通过在语义嵌入空间中用单个平均向量表示文档集合来捕捉语料库的核心含义。然后，利用生成式语言模型对该平均向量进行嵌入反演，以重构流畅的自然语言摘要。为提高重建质量和引入主题多样性，该方法还引入了随机性，即从以平均向量为中心的 Gaussian 分布中进行采样。此方法避免了上下文长度限制，并以高效的 $O(d + d^2)$ 参数实现。

Result: Vec2Summ能为主题集中、顺序不变的语料库生成连贯的摘要。在主题覆盖和效率方面，其性能与直接LLM摘要方法相当，但细粒度细节略有不足。

Conclusion: Vec2Summ在优先考虑可扩展性、语义控制和语料库级抽象的应用场景中具有重要潜力。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [26] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: 引入了SEADialogues，这是一个以东南亚文化为背景的多语言对话数据集，旨在弥补现有对话数据集中文化细微差别的不足。


<details>
  <summary>Details</summary>
Motivation: 大多数现有闲聊数据集忽略了自然人类对话中固有的文化细微差别。

Method: 开发并发布了SEADialogues数据集，其特点是包含来自六个东南亚国家的八种语言的对话。为增强文化相关性，每个对话都包含人物角色属性和两个反映当地日常生活的文化主题。

Result: 成功构建并发布了一个多轮、文化接地气的东南亚多语言对话数据集。

Conclusion: 该数据集旨在推动文化感知和以人为中心的大型语言模型（包括会话式对话代理）的研究。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [27] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 为解决现有偏见评估基准在印度语境下的局限性，本研究引入了多语言多文化基准BharatBBQ，评估发现印度语言模型中存在持续且常被放大的偏见，强调了本土化评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型偏见评估基准（如BBQ）主要聚焦西方语境，无法有效评估印度语境中的偏见，限制了其在印度AI系统中的公平性保障作用。

Method: 研究引入了BharatBBQ，一个针对印地语、英语、马拉地语、孟加拉语、泰米尔语、泰卢固语、奥里亚语和阿萨姆语八种语言、包含13个社会类别（含3个交叉类别）的文化适应性偏见评估基准。该数据集包含49,108个原始示例，通过翻译和验证扩展到392,864个示例。研究评估了五种多语言语言模型家族在零样本和少样本设置下的偏见和刻板印象偏见分数。

Result: 研究发现，不同语言和不同社会类别中均存在持续的偏见。与英语相比，印度语言中的偏见常常被放大。

Conclusion: 研究结果证明了构建具有语言和文化基础的偏见评估基准的必要性，以确保AI系统的公平性并减少有害刻板印象的强化。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [28] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: 本文提出一种名为LessIsMore的无训练稀疏注意力机制，用于加速大型推理模型，在不牺牲准确性的前提下显著提升推理速度和效率，并优于现有稀疏注意力方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽表现强大，但在测试时因过多的令牌生成而产生巨大的计算开销，尤其是在处理短输入提示时。现有稀疏注意力机制能降低延迟和内存使用，但常因长生成推理中的累积误差导致显著的准确性下降，且通常需要高令牌保留率或昂贵的再训练。

Method: 引入LessIsMore，一种针对推理任务的无训练稀疏注意力机制。该方法利用全局注意力模式而非传统的头部特定局部优化。LessIsMore通过结合局部注意力头和最新的上下文信息来聚合令牌选择，实现跨头统一的令牌排序，以用于后续解码层，从而提高泛化能力和效率。

Result: LessIsMore在多样化的推理任务和基准测试中，保持甚至在某些情况下提高了准确性，并相较于全注意力实现了平均1.1倍的解码加速。此外，LessIsMore在不损失准确性的前提下关注的令牌数量减少2倍，相较于现有稀疏注意力方法实现了1.13倍的端到端加速。

Conclusion: LessIsMore提供了一种高效且无需训练的稀疏注意力解决方案，有效解决了大型推理模型的计算开销问题。它通过统一的跨头令牌选择，在显著提高推理速度和效率的同时，保持或提升了准确性，表现优于现有方法。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [29] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 本研究通过创建WinoIdentity基准和提出Coreference Confidence Disparity指标，深入评估了大型语言模型（LLMs）的交叉偏见，发现模型在处理多重弱势群体时存在显著置信度差异，并暗示LLMs的性能可能更多源于记忆而非逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被广泛应用于招聘和招生等关键资源受限领域。然而，人工智能系统可能反映并加剧社会偏见，引发了在重要社会情境中使用时对基于身份的伤害的担忧。现有工作主要关注单轴偏见评估，本研究旨在扩展至交叉偏见，以识别多轴歧视交织所产生的独特劣势模式。

Method: 研究扩展了单轴公平性评估，以考察交叉偏见。为此，创建了一个名为WinoIdentity的新基准，通过将WinoBias数据集与10个属性（包括年龄、国籍、种族）的25个人口统计学标记与二元性别交叉结合，生成了245,700个提示，用于评估50种不同的偏见模式。研究关注因代表性不足导致的遗漏伤害，并通过不确定性视角调查偏见，并提出了一种名为“指代置信度差异”（Coreference Confidence Disparity）的群体（不）公平性指标，该指标衡量模型对某些交叉身份的置信度是否高于或低于其他身份。评估了五种近期发布的LLMs。

Result: 研究发现，在体型、性取向和社会经济地位等各种人口统计学属性上，置信度差异高达40%。模型在反刻板印象设置下，对双重弱势身份表现出最大的不确定性。令人惊讶的是，即使对于主导或特权标记，指代置信度也会下降，这表明LLMs近期令人印象深刻的性能更可能归因于记忆而非逻辑推理。

Conclusion: 研究发现的置信度差异是价值对齐和有效性方面的独立缺陷，这两种缺陷可能结合起来导致社会危害。LLMs在处理交叉身份时存在显著偏见，其性能可能主要依赖于记忆，而非真正的逻辑推理能力，这在关键社会应用中构成风险。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [30] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 本文从哲学角度审视自动语音识别（ASR）系统中的偏见，认为其对非标准语种的误识不仅是技术缺陷，更是一种不尊重，加剧了对边缘语言群体的历史不公。文章提出了ASR偏见的独特伦理维度，并主张解决偏见需超越技术范畴，应尊重语言多样性。


<details>
  <summary>Details</summary>
Motivation: ASR系统应用广泛但公平性研究不足；系统对特定语音变体的误识不仅是技术局限，更是对边缘语言群体的不尊重，加剧了历史不公；现有技术公平性指标未能捕捉ASR偏见的独特伦理维度；ASR开发中存在语言意识形态问题。

Method: 通过哲学分析视角，区分了中性分类与有害歧视；识别了语音技术的三个独特伦理维度（时间负担、对话中断、语音与身份联系）；分析了ASR开发中语言标准化与多元化之间的张力。

Result: ASR系统对非标准方言的持续误识可将中性分类转变为有害歧视；揭示了ASR偏见的“时间税收”、对话中断、语音与个人/文化认同联系等独特伦理维度；这些因素导致的不对称权力关系未被现有技术公平性指标捕获；当前的ASR方法常固化并强化有问题的语言意识形态。

Conclusion: 解决ASR偏见需超越技术干预，要求承认和接纳多样化的语音变体作为合法的表达形式；这种哲学重构为开发尊重语言多样性和说话者自主权的ASR系统提供了新途径。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [31] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: 提出SafeGrad方法，通过梯度手术解决微调大型语言模型（LLMs）时，恶意数据导致的LLM安全对齐失效问题，在高有害数据比例下仍能保持领先的安全性且不影响任务性能。


<details>
  <summary>Details</summary>
Motivation: 微调即服务（Fine-tuning-as-a-Service）引入了关键漏洞，少量恶意数据即可损害LLMs的安全对齐。现有解决方案在恶意数据比例增加时防御能力急剧下降，其失败根源在于用户任务更新与安全目标之间的梯度冲突。

Method: 提出SafeGrad方法，利用梯度手术解决冲突。当检测到冲突时，将用户任务梯度投影到对齐梯度的正交平面上，以消除有害成分，使模型在学习用户任务的同时不牺牲安全性。此外，采用KL散度对齐损失，学习基础模型丰富的分布式安全特征，以增强鲁棒性和数据效率。

Result: SafeGrad在各种LLM和数据集上均提供了最先进（SOTA）的防御能力。即使在有害数据比例很高的情况下，它也能保持强大的安全性，同时不损害任务的准确性（保真度）。

Conclusion: SafeGrad通过有效解决梯度冲突，为LLM在恶意数据存在下的安全微调提供了一种高效且鲁棒的解决方案，实现了性能与安全性的平衡，显著优于现有方法。

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [32] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: 本文针对全能语言模型（OLLMs）安全评估基准的缺失，提出了首个综合性并行基准Omni-SafetyBench和定制化评估指标，揭示了现有OLLMs在复杂多模态输入下存在显著的安全和一致性漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着整合视觉、听觉和文本处理的OLLMs的兴起，需要对其进行强大的安全评估以缓解有害输出。然而，目前缺乏专门针对OLLMs的基准，现有LLM基准无法评估音视频联合输入下的安全性能或跨模态安全一致性。

Method: 研究者引入了Omni-SafetyBench，这是首个全面的OLLM安全评估并行基准，包含24种模态组合和变体，共972个样本，包括专门的音视频有害案例。同时，提出了定制化的评估指标：基于条件攻击成功率（C-ASR）和拒绝率（C-RR）的Safety-score，以及用于衡量跨模态一致性的Cross-Modal Safety Consistency Score（CMSC-score）。

Result: 对6个开源和4个闭源OLLMs的评估揭示了关键漏洞：1) 没有模型在整体安全性和一致性上都表现出色，仅3个模型两项指标均超过0.6；2) 面对复杂输入（特别是音视频联合输入），安全防御显著减弱；3) 普遍存在严重弱点，某些模型在特定模态上得分低至0.14。

Conclusion: 本研究所提出的基准和指标凸显了加强OLLM安全性的紧迫需求，并为未来的改进工作奠定了基础。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [33] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 该研究提出PHG-DIF框架，通过识别并去除用户历史点击行为中的噪声，更准确地捕捉用户兴趣，从而生成高质量的个性化新闻标题，并发布了新的基准数据集DT-PENS。


<details>
  <summary>Details</summary>
Motivation: 现有个性化标题生成方法未能有效处理用户历史点击流中与个性化无关的点击噪声，导致生成的标题可能偏离用户真实兴趣，从而影响生成质量。

Method: 本文提出了PHG-DIF（通过隐式反馈去噪虚假兴趣的个性化标题生成）框架。该框架首先采用双阶段过滤技术，利用短停留时间和异常点击爆发来有效去除点击流噪声；接着，利用多级时间融合技术动态建模用户不断演变和多方面的兴趣。此外，还发布了包含停留时间注释的新基准数据集DT-PENS。

Result: PHG-DIF框架显著减轻了点击噪声的不利影响，并显著提升了标题生成质量，在DT-PENS数据集上取得了最先进（SOTA）的结果。

Conclusion: 通过深入分析点击噪声的负面影响，并提出有效的去噪和兴趣建模方法，PHG-DIF框架成功解决了个性化标题生成中的关键问题，极大地提升了生成质量，为未来的研究提供了新的思路和资源。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [34] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 针对企业数据管道中因语义漂移导致的数据可复用性与治理问题，本文提出了一种从多语言脚本中自动提取细粒度模式血缘的框架。研究引入了新的评估指标和基准，并实验证明，通过优化提示，开源小模型可实现与大型专有模型相当的性能，为部署模式感知智能体提供了经济高效的途径。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中复杂的跨语言转换常常导致原始元数据与下游数据之间的“语义漂移”，这损害了数据的可复用性、治理能力，并降低了RAG和text-to-SQL等服务的实用性。

Method: 本文提出了一种自动化框架，用于从多语言企业管道脚本中提取细粒度模式血缘，该方法识别并标准化了源模式、源表、转换逻辑和聚合操作等四个关键组件。为评估血缘质量，研究引入了Schema Lineage Composite Evaluation (SLiCE) 指标，并构建了一个包含1700个真实工业脚本手动标注血缘的新基准。实验使用了1.3B到32B的12种语言模型（包括SLMs和GPT-4o/GPT-4.1等LLMs）。

Result: 研究结果表明，模式血缘提取的性能随模型规模和提示技术复杂度的提高而提升。特别是，一个32B的开源模型，通过单一推理跟踪，在标准提示下能达到与GPT系列模型相媲美的性能。

Conclusion: 本研究的发现为在实际应用中部署模式感知智能体提供了一种可扩展且经济高效的方法。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [35] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: DySK-Attn是一种新颖框架，通过结合动态知识图谱和稀疏知识注意力机制，使大型语言模型能高效集成实时知识，在时间敏感问答任务中表现优于RAG和模型编辑，同时提高计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的知识是静态且易过时，重新训练成本高昂，现有知识编辑技术效率低且可能引入副作用。研究旨在解决LLMs知识更新的挑战。

Method: 提出DySK-Attn框架，将LLM与可即时更新的动态知识图谱（KG）结合。核心是稀疏知识注意力机制，实现粗粒度到细粒度的搜索，高效识别KG中少量相关事实，避免对整个知识库进行密集注意力的高计算成本，并减轻无关信息的干扰。

Result: 在时间敏感的问答任务中进行广泛实验，DySK-Attn在更新知识的事实准确性和计算效率方面，显著优于包括标准检索增强生成（RAG）和模型编辑技术在内的强基线模型。

Conclusion: DySK-Attn框架提供了一种可扩展且有效的解决方案，使LLMs能够与不断变化的世界保持同步。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [36] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: LLM在时序预测中面临挑战（异质性、模态鸿沟）。本文提出TALON框架，通过异质性建模和语义对齐来增强LLM时序预测能力，并在多个真实世界基准测试中取得了显著优于SOTA方法的性能。


<details>
  <summary>Details</summary>
Motivation: 将LLM直接应用于时序预测面临挑战：时序模式的固有异质性，以及连续数值信号与离散语言表示之间的模态鸿沟。

Method: 提出统一框架TALON，通过以下方式增强LLM时序预测：1. 设计异质时序编码器，划分多元时序并进行局部建模。2. 引入语义对齐模块，将时序特征与LLM兼容表示对齐，消除手动提示。

Result: 在七个真实世界基准测试中，TALON表现优异，平均MSE相较于现有SOTA方法提升高达11%。

Conclusion: 将模式感知和语义感知设计融入LLM时序预测中能有效提升性能。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [37] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 现有预训练语言模型在社交媒体谣言检测中表现不佳。本文提出Post Engagement Prediction (PEP)持续预训练策略，通过建模传播结构中的帖子互动关系来提升模型性能，并发布大规模社交语料和训练出SoLM模型，实验证明PEP显著提升了谣言检测效果。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLMs）在通用NLP任务中表现出色，但在社交媒体应用（如谣言检测）中表现不佳。这归因于预训练语料与社交文本不匹配、对社交符号处理不足以及预训练任务未能有效捕捉传播结构中隐含的用户互动。

Method: 本文提出Post Engagement Prediction (PEP)持续预训练策略，使PLMs预测帖子间的“根、分支、父”关系，以捕捉传播结构中隐含的立场和情感互动。同时，本文整理并发布了大规模Twitter语料库（TwitterCorpus）以及两个包含传播结构的无标签对话数据集（UTwitter和UWeibo）。利用这些资源和PEP策略，训练了针对Twitter的PLM模型SoLM。

Result: 广泛实验表明，PEP策略显著提升了通用和社交媒体PLMs的谣言检测性能，包括在小样本场景下。PEP使基线模型准确率提升1.0-3.7%，并在多个数据集上超越了当前最先进的方法。SoLM模型即使不使用高级模块，也取得了有竞争力的结果，突显了该策略在学习判别性帖子互动特征方面的有效性。

Conclusion: PEP持续预训练策略能有效将传播结构信息融入PLMs，显著提升其在社交媒体谣言检测任务上的性能，并使得模型能超越现有SOTA，验证了建模帖子互动特征的重要性。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [38] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 深度学习模型在词汇重音预测上表现优异（最高92%准确率），通过可解释性分析发现其主要依赖重读元音的频谱特性（特别是共振峰），证明了深度学习从自然语料中获取分布式重音线索的能力，并拓宽了传统语音学研究。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在语音处理领域取得成功，但其作为“黑箱”模型，其决策依据及其解释性仍是未解之谜。本研究旨在解决此问题，尤其是在词汇重音预测背景下。

Method: 自动构建了包含英语双音节词的朗读及自然语音数据集。训练卷积神经网络（CNN）模型，从双音节词的频谱图预测重音位置。采用层级相关性传播（LRP）和特征特异性相关性分析等可解释性技术来揭示模型的决策依据。

Result: CNN模型在测试集上实现了高达92%的重音位置预测准确率。可解释性分析表明，模型预测主要受重读音节与非重读音节信息的影响，特别是重读元音的频谱特性。具体地，模型的最佳表现主要由重读元音的第一和第二共振峰决定，音高和第三共振峰亦有贡献。

Conclusion: 研究结果表明深度学习能够从自然语料中有效捕获分布式重音线索，为传统基于高度受控刺激的语音学研究提供了新的视角和扩展。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [39] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 本文提出一种结合提示调优（APT）和记忆演示模板（MDT）的方法，旨在解决少样本持续学习命名实体识别（FS-CLNER）任务中面临的旧知识蒸馏受阻和新类别泛化困难等问题。


<details>
  <summary>Details</summary>
Motivation: 在少样本持续学习命名实体识别（FS-CLNER）任务中，新类别实体稀缺导致模型难以泛化，且旧类别实体信息不足会阻碍知识蒸馏，造成“少样本蒸馏困境”，从而影响模型的持续学习能力。

Method: 通过提示调优范式和记忆演示模板策略解决挑战。具体包括：1) 设计可扩展的锚词导向提示调优（APT）范式，以弥合预训练与微调的差距，增强少样本性能。2) 引入记忆演示模板（MDT）到每个训练实例中，提供旧任务回放样本，以避免蒸馏困境并促进上下文学习。

Result: 实验结果表明，该方法在少样本持续学习命名实体识别（FS-CLNER）任务上取得了有竞争力的性能。

Conclusion: 所提出的结合APT和MDT的方法有效解决了少样本持续学习命名实体识别中的挑战，提升了模型在数据稀缺场景下的泛化能力和旧知识蒸馏效率。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [40] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 扩展二维动态发音模型DYNARTmo，整合三维腭穹顶表示以估计舌腭接触区域，并生成类似电腭图的可视化，适用于言语科学教育和治疗。


<details>
  <summary>Details</summary>
Motivation: 增强现有二维发音模型（DYNARTmo）的功能，通过引入三维腭穹顶表示，以更准确地估计和可视化舌腭接触区域，克服二维模型在捕捉横向接触方面的局限性。

Method: 通过整合内部三维腭穹顶表示来扩展二维动态发音模型DYNARTmo；实现了半椭圆和基于余弦的两种腭穹顶几何形状来建模横向曲率；分析计算横向接触点并生成类似电腭图的可视化。

Result: 增强模型能够从正中矢状舌轮廓估计舌腭接触面积，并生成类似电腭图的可视化；该模型支持矢状、声门和腭三种同步视图，用于静态和动态发音显示。

Conclusion: 该增强模型因其提供多角度同步视图及静态/动态显示，且能生成类似电腭图的可视化，故适用于言语科学教育和言语治疗领域。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [41] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 本研究提出显式和隐式两种方法，通过整合语境副语言信息，显著提升了语音大模型（Speech-LLMs）的同理心推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语音语言模型（Speech-LLMs）在同理心推理方面存在局限性，主要是因为缺乏整合语境内容和副语言线索的训练数据集。

Method: 提出了两种将语境副语言信息融入模型训练的方法：1) 显式方法：直接向LLM提供副语言元数据（如情感标注）；2) 隐式方法：利用类别和维度情感标注以及语音转录，自动生成新的训练问答对。此外，还通过验证其与分类指标的相关性，支持了LLM评判器的可靠性。

Result: 在人工标注的QA基准测试中，隐式方法将性能（LLM判断）提升了38.41%；当与显式方法结合时，性能达到46.02%，显示出在语境副语言理解方面的有效性。同时，LLM评判器与分类指标的相关性也得到了验证。

Conclusion: 本研究提出的显式和隐式方法，通过有效整合语境副语言信息，成功提升了大型语音语言模型在同理心推理方面的性能，克服了现有模型的关键局限性。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [42] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA是一个自适应问答框架，结合大语言模型和心理测量学方法，显著减少心理健康筛查所需的提问数量，提高效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为可扩展的互动式心理健康评估提供了新机遇，但其过多的提问给用户带来负担，且对于跨诊断症状的实际筛查效率低下。

Method: 引入MAQuA框架，通过结合对语言响应的多输出建模、项目反应理论（IRT）和因子分析，在每次提问时选择跨多个维度信息量最大的问题，以优化诊断信息，从而提高准确性并减少响应负担。

Result: 在一项新数据集上的实证结果表明，与随机排序相比，MAQuA将评估问题数量减少了50-87%（例如，抑郁症评分稳定所需问题减少71%，饮食失调评分减少85%）。MAQuA在内化（抑郁、焦虑）和外化（物质使用、饮食失调）领域均表现出稳健性能，早期停止策略进一步减少了患者时间和负担。

Conclusion: MAQuA是一个强大且高效的工具，适用于可扩展、精细且交互式的心理健康筛查，推动了大语言模型代理与实际临床工作流程的整合。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [43] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 本研究评估了14个大型语言模型在27个电车难题场景中的道德推理能力，揭示了其在不同道德哲学框架下的表现差异和对人类共识的对齐情况，并呼吁将道德推理作为LLM对齐的核心标准。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）日益参与伦理敏感决策，理解它们的道德推理过程变得至关重要。

Method: 研究对14个领先LLM（包括推理增强型和通用型）进行了全面的实证评估。使用了27个电车难题场景，并结合了功利主义、道义论、利他主义等10种道德哲学框架。通过因子式提示协议，共收集了3,780个二元决策和自然语言解释，并分析了决策的果断性、解释答案的一致性、公共道德对齐度以及对伦理无关线索的敏感性。

Result: 研究发现不同伦理框架和模型类型之间存在显著差异。推理增强模型表现出更高的决策果断性和结构化解释，但并非总能与人类共识更好地对齐。在利他主义、公平和美德伦理框架下出现了“甜蜜区”，模型能实现高干预率、低解释冲突和与人类判断的最小偏离。然而，在强调亲缘关系、合法性或自身利益的框架下，模型表现出分歧，常产生伦理上具争议的结果。这些模式表明道德提示不仅是行为修正器，也是揭示潜在对齐哲学的诊断工具。

Conclusion: 道德推理应成为LLM对齐的主要维度。研究呼吁建立标准化基准，不仅评估LLM做出什么决策，还要评估它们如何以及为何做出这些决策。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [44] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: 针对AEC领域NER中标准模型表现受限且传统预训练成本高昂的问题，本文提出ARCE，利用LLM生成简单解释语料（Cote）预训练RoBERTa。ARCE在AEC数据集上取得了SOTA，并发现简单解释比复杂推理更有效。


<details>
  <summary>Details</summary>
Motivation: 建筑、工程、施工（AEC）领域的命名实体识别（NER）面临挑战，标准预训练模型因领域差异表现受限，而大型领域语料的进一步预训练成本高昂。尽管大语言模型（LLMs）在知识生成方面前景广阔，但如何有效生成知识以增强小型高效模型仍是一个未解决的问题。

Method: 本文提出ARCE（augmented RoBERTa with contextualized elucidations）方法。该方法首先利用一个LLM生成一个由简单、直接解释组成的语料库（Cote），然后使用此Cote语料库对RoBERTa模型进行增量预训练，最后在下游任务上进行微调。

Result: ARCE在基准AEC数据集上取得了77.20%的Macro-F1分数，建立了新的最先进性能。研究结果还表明，对于该任务，基于简单解释的知识比基于复杂角色的推理更有效。

Conclusion: ARCE是一种有效的新方法，通过利用LLM生成的简单、解释性知识对小型模型进行增量预训练，显著提升了AEC领域命名实体识别的性能，并达到了新的SOTA。这证明了简单、解释性知识在特定任务中出人意料的有效性。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [45] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了CCFQA基准，用于评估多模态大语言模型（MLLMs）在多语言和跨模态（语音-文本）环境下的事实性。研究发现现有MLLMs表现不佳，并提出了一种高效的少样本迁移学习策略以提升其多语言语音问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态大语言模型可靠性的基准主要集中在文本或视觉模态，且以英语为主，未能有效评估模型在处理多语言输入（特别是语音）时的幻觉消除能力和事实性，导致评估存在空白。

Method: 本文提出了一种新颖的“跨语言跨模态事实性基准”（CCFQA），该基准包含8种语言的并行语音-文本事实性问题，旨在系统评估MLLMs的跨语言和跨模态事实性能力。此外，本文还提出了一种少样本迁移学习策略，用于将大语言模型（LLMs）的英语问答能力有效地迁移到多语言语音问答（SQA）任务中。

Result: 实验结果表明，当前的多模态大语言模型在CCFQA基准上仍面临重大挑战。所提出的少样本迁移学习策略能够有效地将英语大语言模型的问答能力迁移到多语言语音问答任务中，仅通过5次样本训练，便能达到与GPT-4o-mini-Audio相当的性能。

Conclusion: CCFQA基准的发布将作为一项基础研究资源，旨在促进开发具有更强大、更可靠语音理解能力的多模态大语言模型。同时，所提出的少样本迁移学习策略为提升模型在多语言语音问答任务上的表现提供了有效途径。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [46] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches是一个新型医疗问答基准数据集，专为评估大型语言模型（LLMs）的复杂推理能力而设计。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在医疗领域的复杂推理能力评估不足，需要一个能够评估其多步推理和结构化检索增强生成（RAG）能力的数据集，以促进开发更可信赖和临床可靠的LLMs。

Method: 通过半自动化流程，将医学来源的明确决策路径转换为真实的患者病例、相关问题和答案来生成数据集。

Result: 创建了HealthBranches数据集，包含4,063个病例研究，涵盖17个医疗主题。每个数据点都基于临床验证的推理链，支持开放式和多项选择题格式，并独特地包含每个问答的完整推理路径。

Conclusion: HealthBranches为开发更值得信赖、可解释和临床可靠的LLMs奠定了基础，同时也是一个有价值的教育资源。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [47] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: LLMs在模糊化问题上的鲁棒性尚未充分测试。本研究提出ObfusQA框架，用于评估LLMs在不同模糊化程度下的性能，发现LLMs易失败或产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在事实问答中取得了显著进展，但目前缺乏对其在面对模糊化问题时的鲁棒性进行系统性评估的研究。

Method: 本文提出了一种新颖的模糊化技术ObfusQAte，并基于此引入了ObfusQA框架。该框架具有多层模糊化级别，从命名实体间接性、干扰物间接性和上下文过载三个维度评估LLMs的能力。

Result: 研究发现，当LLMs面对这些日益细微的模糊化变体时，它们倾向于失败或生成幻觉响应。

Conclusion: ObfusQA提供了一个评估LLM鲁棒性和适应性的全面基准，揭示了LLMs在处理模糊化问题时的局限性。为促进相关研究，ObfusQAte已公开发布。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [48] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 通过与能进行西英语码转换的聊天机器人进行地图任务实验，发现用户更喜欢可预测的码转换，并能感知到不当的码转换行为，揭示了多语言技术在双语研究中的潜力与风险。


<details>
  <summary>Details</summary>
Motivation: 大多数人是多语言使用者并会进行码转换，但码转换语言的特征尚未被完全理解。研究旨在探究使用聊天机器人进行码转换语言实验的可行性，以及参与者是否能感知到话语和语法模式的变化。

Method: 开发了一个能够与人类参与者进行西班牙语和英语码转换地图任务的聊天机器人。在两个实验中，机器人被设定为按照不同的策略进行码转换，以观察实验的可行性以及参与者对码转换模式的敏感度。

Result: 参与者普遍喜欢与行为可预测的机器人进行码转换。当码转换随机或不符合语法（例如，产生未经证实的混合语言名词短语如“la fork”）时，参与者的任务享受度和完成度都会降低。

Conclusion: 研究结果强调了部署不成熟的多语言语言技术的潜在缺点，同时也说明了此类技术在进行双语语言使用研究方面的巨大前景。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [49] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: 本文提出TurnGuide方法，通过回合级文本指导解决全双工语音语言模型（FD-SLMs）在实时对话中面临的对话能力下降问题，显著提升了其会话表现。


<details>
  <summary>Details</summary>
Motivation: 全双工语音语言模型（FD-SLMs）旨在实现自然、实时的语音交互，但在处理长语音序列和受限于高质量口语数据时，其对话能力常会下降。此外，现有的文本引导语音生成方案在整合到双声道音频流时，存在时序和长度问题，影响了精确的时间对齐和自然交互。

Method: 本文提出了一种名为TurnGuide的新型规划启发式方法。该方法模仿人类的会话规划过程，动态地将助手语音分割成对话回合，并在语音输出之前生成回合级的文本指导，从而有效地解决了文本引导中存在的插入时序和长度挑战。

Result: 广泛的实验表明，TurnGuide方法显著提升了端到端FD-SLMs的对话能力。该方法使模型能够生成语义上有意义且连贯的语音，同时保持自然的会话流畅性。

Conclusion: TurnGuide通过其创新的会话规划和回合级文本指导，有效解决了FD-SLMs在实时语音交互中的关键挑战，为实现更自然、高效和类人的人机语音对话提供了有效途径。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [50] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 该研究通过构建大规模多语言文化视觉问答数据集（CulturalGround）并训练开源多模态大模型（CulturalPangea），有效解决了现有模型在文化实体理解和低资源语言上的不足，显著提升了模型的文化敏感性，同时保持了通用能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在高资源环境下表现优秀，但在理解长尾文化实体和处理低资源语言时常出现误解或性能不佳，存在文化鸿沟。

Method: 提出一种以数据为中心的方法，将MLLMs直接与文化知识结合。利用维基数据（Wikidata）构建了包含2200万个高质量、文化丰富VQA对的多语言合成数据集CulturalGround，涵盖42个国家和39种语言。在此数据集上训练了开源MLLM CulturalPangea，并结合标准多语言指令微调数据以保持通用能力。

Result: CulturalPangea在多个以文化为中心的多语言多模态基准测试中，在开源模型中取得了最先进的性能，平均超越现有模型5.0，且未降低在主流视觉-语言任务上的表现。

Conclusion: 研究结果表明，这种有针对性的、基于文化知识的方法能够显著缩小MLLMs中的文化鸿沟，为构建全球包容的多模态系统提供了实用路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [51] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: 本文提出ReLoc，一个统一的局部搜索框架，通过逐步代码修订和专门的奖励模型，显著提升了LLMs在代码生成任务中的效率和性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成中面临效率和可扩展性挑战。现有的基于构建的树搜索方法存在树规模大、代币消耗高的问题，而基于改进的方法则受限于奖励信号信息不足和搜索策略效率低下。

Method: 提出ReLoc框架，通过四个核心组件（初始代码草拟、邻域代码生成、候选评估、当前代码更新）实现分步代码修订。该框架可实例化为如爬山法或遗传算法等多种局部搜索算法。此外，开发了基于修订距离的专门奖励模型，以提供细粒度偏好指导局部搜索。

Result: 实验结果表明，ReLoc在多种代码生成任务中表现优越，显著超越了基于构建的树搜索方法和当前最先进的基于改进的代码生成方法。

Conclusion: ReLoc作为一个统一的局部搜索框架，通过有效的代码修订策略和精细的奖励指导，成功解决了LLMs在代码生成中的效率和性能问题，实现了显著的性能提升。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [52] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 本研究通过相对输入长度分析，揭示大型语言模型（LLMs）在长文本中的位置偏差。发现“中间丢失”（LiM）效应在输入小于上下文窗口50%时最强，超过则初位偏见减弱，近位偏见稳定，表现为信息越靠近结尾性能越好的距离偏见。推理中的位置偏见主要源于检索。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLM在长输入中位置偏见（如LiM效应）的研究结果存在不一致，对其强度和表现条件存在疑问，亟需深入探究。

Method: 采用相对于模型上下文窗口的“相对输入长度”而非“绝对输入长度”，对LLM进行了全面分析。

Result: ['LiM效应在输入长度占模型上下文窗口50%以内时最为显著。', '当输入长度超过50%时，初位偏见显著减弱，近位偏见保持稳定，LiM效应消失，转变为一种“基于距离”的偏见：相关信息越接近输入末尾，模型性能越好。', '成功的信息检索是LLM进行推理的先决条件，且推理中的位置偏见很大程度上继承自检索过程。']

Conclusion: 本研究的发现对长上下文任务、未来LLM基准测试设计以及长输入评估方法具有重要启示。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [53] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文介绍ALOPE，一个自适应层优化框架，通过层级自适应改进大型语言模型（LLM）在机器翻译质量评估（QE）中的回归预测性能，尤其针对跨语言和低资源语言挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在自然语言处理任务中表现出色，但在机器翻译质量评估（QE）这一跨语言任务上仍面临挑战。现有LLM-based QE系统受限于LLM的预训练（侧重因果语言建模而非回归任务）和低资源语言的数据分布，导致性能不足。

Method: 本文提出了ALOPE框架，一个自适应层优化框架，旨在通过层级自适应重构Transformer表示，以优化LLM在回归任务（如QE）中的预测性能。具体方法包括：将低秩适配器（LoRA）与回归任务头结合，利用选定的预训练Transformer层来改进跨语言对齐。此外，ALOPE引入了动态加权策略（自适应组合多层表示）和多头回归策略（聚合多头回归损失）。

Result: ALOPE框架在各种现有基于LLM的QE方法上均显示出改进。经验证据表明，LLM中间的Transformer层提供的上下文表示更符合QE任务的跨语言特性。

Conclusion: ALOPE框架通过层级优化有效提升了大型语言模型在机器翻译质量评估任务上的表现，尤其在跨语言场景中。研究揭示LLM的中间层对跨语言QE任务具有更强的适应性。相关模型和代码已公开，以促进后续研究和现有LLM-based MT框架的QE能力扩展。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [54] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 通过拓扑数据分析，本研究发现GPT-2中的偏见集中在特定的注意力头中，为模型去偏见提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型偏见检测方法无法识别模型内部导致偏见的具体部分。

Method: 本研究采用拓扑数据分析，用于识别GPT-2模型中对StereoSet数据集中身份群体误表示做出贡献的注意力头。

Result: 研究发现，针对特定类别（如性别、职业）的偏见集中在充当“热点”的注意力头中；提出的度量标准还能识别特定偏见类别中某个群体的偏见源。

Conclusion: 该方法成功定位了大型语言模型内部的偏见源，为未来进一步消除这些模型中的偏见提供了潜在途径。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [55] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: 本文提出ThemeClouds，一个利用大型语言模型(LLMs)从对话文本生成主题式、参与者加权的词云工具，旨在克服传统词云在定性访谈分析中的局限性，并能识别出更具可操作性的见解。


<details>
  <summary>Details</summary>
Motivation: 传统基于频率的词云在分析对话型定性访谈时存在局限性，如突出填充词、忽略意译、分割语义相关概念，导致研究人员难以快速获取可解释性的早期分析概览。

Method: 引入开源工具ThemeClouds，该工具利用LLM识别对话文本中的概念级主题，并基于提及每个主题的独特参与者数量进行加权（而非原始词频）生成词云。研究人员可自定义提示和可视化参数。

Result: 通过对一项用户研究的访谈数据分析，ThemeClouds比传统的频率词云和主题模型基线（如LDA、BERTopic）能发现更多可操作的设备相关问题。

Conclusion: ThemeClouds提供了一种有效且可解释的LLM辅助方法，改进了定性数据分析工作流，提升了研究人员的洞察能力，并为交互式分析（如条件对比“差异词云”）提供了可能性。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [56] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本报告系统调查了强化学习与可验证奖励（RLVR）中大型语言模型（LLM）的探索能力，通过分析探索空间、熵-性能交换和性能优化方法，旨在为推进RLVR系统提供一个基础框架。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在提升LLM推理能力方面已取得经验性成功，但LLM在RLVR中的探索行为所依赖的基本机制尚未得到充分探索。

Method: 本研究对RLVR中的探索能力进行了系统性调查，涵盖三个主要方面：1) 探索空间塑造，通过开发量化指标来刻画LLM的能力边界；2) 熵-性能交换分析，跨训练阶段、个体实例和token级模式进行；3) RL性能优化，研究如何有效将探索收益转化为可衡量的改进。

Result: 本报告呈现了对RLVR中LLM探索能力的系统性调查，并通过整合先前已识别的见解与新的经验证据，展示了对探索机制的深入理解。

Conclusion: 本工作旨在通过其系统性调查和对探索机制的深入分析，为推进未来的RLVR系统发展提供一个基础框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [57] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 该论文提出了印度保释预测系统（IBPS），一个基于AI的框架，旨在通过预测保释结果和生成法律依据来辅助印度的保释决策。为解决印度保释系统的主观性、延误和不一致问题，研究团队构建了一个大规模保释判决数据集，并微调大型语言模型。实验结果表明，结合法规知识的模型在准确性和解释质量上表现优异，有助于提高司法效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 印度保释决策存在严重的主观性、延误和不一致问题，导致超过75%的监狱人口为审前羁押人员（其中多数来自社会经济弱势群体），这不仅侵犯人权，也加剧了司法系统积压。

Method: 研究团队构建并发布了一个包含150,430份印度高等法院保释判决的大规模数据集，并进行了年龄、健康、犯罪历史、案件类别、羁押时长、法规和司法推理等结构化标注。在此基础上，通过参数高效技术微调大型语言模型，并结合检索增强生成（RAG）等方法，在有无法规上下文等多种配置下评估了模型的性能。

Result: 结果表明，通过法规知识微调的模型显著优于基线模型，取得了高准确度和优秀的解释质量，并能很好地泛化到由法律专家独立标注的测试集。

Conclusion: IBPS为印度司法系统提供了一个透明、可扩展且可复现的解决方案，能够支持数据驱动的法律援助，减少保释延误，并促进程序公平性。

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [58] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: LLM在事件检测中存在触发词理解和过度解读问题，ICL难以有效纠正。本文提出KeyCP++，一种关键词中心化的CoT提示方法，通过“提议-判断”机制促进LLM学习检测规则，显著提升单样本事件检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的上下文学习（ICL）范式在各种自然语言处理任务中取得了成功，但在事件检测中却面临挑战。这是因为LLM缺乏对事件触发词的准确理解并容易过度解读，而单独使用上下文示例无法有效纠正这些问题。

Method: 针对最具挑战性的单样本事件检测设置，本文提出KeyCP++，一种关键词中心化的思维链（Chain-of-Thought）提示方法。KeyCP++通过自动标注演示示例中输入文本与检测结果之间的逻辑鸿沟来弥补传统ICL的不足。具体而言，它构建了一个触发词判别提示模板，将示例触发词（即关键词）作为锚点融入提示，让LLM提出候选触发词并逐一判断，这种“提议-判断”的推理过程有助于LLM减轻对关键词的过度依赖，并促进检测规则的学习。

Result: 广泛的实验证明了KeyCP++方法的有效性，并在单样本事件检测中取得了显著的进展。

Conclusion: KeyCP++通过创新的关键词中心化思维链提示方法，有效解决了LLM在单样本事件检测中对触发词理解不足和过度解读的问题，显著提升了检测性能，表明其在应对LLM事件检测挑战方面的潜力。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [59] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: 引入InterChart，一个诊断基准，用于评估视觉-语言模型（VLMs）在多个相关图表间推理的能力，揭示了现有模型在多图表整合方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用（如科学报告、金融分析、公共政策仪表盘）需要跨多个相关图表进行推理，而现有基准主要关注孤立的单一图表，未能充分评估VLMs在复杂多图表环境下的推理能力。

Method: InterChart包含基于2-3个主题或结构相关图表的各种问题类型，包括实体推断、趋势关联、数值估计和抽象多步推理。该基准分为三个难度递增的级别：个体图表的事实推理、合成对齐图表集间的整合分析，以及视觉复杂真实世界图表对的语义推断。

Result: 对最先进的开源和闭源VLMs的评估显示，随着图表复杂性增加，模型准确率持续急剧下降。研究发现，当将多实体图表分解为更简单的视觉单元时，模型表现更好，这突显了它们在跨图表整合方面的困难。

Conclusion: InterChart通过揭示VLMs在复杂、多视觉环境中存在的系统性局限，提供了一个严谨的框架，以推动多模态推理能力的发展。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: 本文提出Med-GRIM模型和DermaGraph数据集，通过稠密编码和图检索实现低成本、高精度的医学VQA，性能媲美大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型在医学等复杂、领域特定应用中缺乏细节精度，且依赖计算量大的微调，难以满足需求。

Method: 提出BIND模型，通过受对比预训练启发的稠密、基于查询的编码来优化联合嵌入空间。基于BIND，开发Med-GRIM医学VQA模型，利用图检索和提示工程整合领域知识，采用低计算量、模块化工作流和小型语言模型。此外，引入了用于零样本多模态医学应用的DermaGraph Graph-RAG数据集。

Result: Med-GRIM在计算成本大大降低的情况下，达到了与大型语言模型相当的性能，并确保了响应的准确性和鲁棒性。DermaGraph数据集支持可扩展的零样本多模态医学研究。

Conclusion: 通过创新的表示模型、高效的检索机制和专门的医学数据集，Med-GRIM为医学VQA提供了一个计算效率高且性能强大的解决方案，有效解决了领域特定精度和计算成本的挑战。

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [61] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker是一个基于DiT的统一框架，用于生成说话风格可控的肖像动画，解决了现有方法在动态风格（如头部运动）控制上的不足及计算效率问题，并在口型同步和风格可控性方面表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的肖像动画方法主要关注口型同步或静态情感转换，常忽略头部运动等动态风格；同时，多数方法采用双U-Net架构，虽保持身份一致性但计算开销大。

Method: 提出DiTalker，一个统一的基于DiT的肖像动画框架。设计了样式-情感编码模块，分离提取身份特定样式（如头部姿势和运动）和身份无关情感特征；引入了音频-样式融合模块，通过并行交叉注意力层解耦音频和说话风格。此外，采用了两项优化的约束，以提升口型同步和保持身份及背景细节。

Result: 大量实验证明DiTalker在口型同步和说话风格可控性方面表现出优越性。

Conclusion: DiTalker成功提供了一个高效且高度可控的肖像动画解决方案，有效解决了现有技术在动态风格生成和计算效率上的局限性，为高质量说话人像视频合成提供了新途径。

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [62] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: 本研究开发了多模态检测框架BigTokDetect及其专家标注数据集BigTok，旨在识别TikTok上宣传肌肉变形症（大肌症）的有害内容，并证明多模态融合能显著提升检测性能，为有害内容审核设立新基准。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台难以有效检测推广肌肉变形症的有害内容，特别是针对青少年男性的“大肌症”信息。这类内容常以合法健身形式出现，通过复杂的视觉、编码语言和激励信息组合规避传统文本检测系统，现有检测方法不足以应对多模态挑战。

Method: 开发了临床知情的BigTokDetect检测框架，用于识别TikTok上的大肌症内容。构建了首个专家标注的多模态数据集BigTok，包含2200多个TikTok视频，由临床心理学家和精神病学家在身体形象、营养、锻炼、补充剂和男子气概等五个主要类别下进行标注。通过领域特定微调，全面评估了最先进的视觉语言模型。

Result: 在主要类别分类上实现了0.829的准确率，在子类别检测上实现了0.690的准确率。消融研究表明，多模态融合比纯文本方法性能提升5-10%，其中视频特征提供了最具区分性的信号。

Conclusion: 研究结果为多模态有害内容检测建立了新的基准。该研究提供了计算工具和方法框架，可用于专业心理健康领域的可扩展内容审核。

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [63] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: 针对结肠息肉分割中数据稀缺和域漂移问题，本文提出了频率先验引导匹配（FPGM）框架。FPGM利用息肉边缘一致的频率特征进行数据增强，显著提升了模型在跨域和零样本场景下的泛化能力，并在多项公开数据集上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 结肠癌的早期诊断需要精确的自动化息肉分割，但现有模型面临标注数据有限和域漂移导致性能显著下降的挑战。半监督学习虽能减少标注需求，但其通用增强方式忽略了息肉特有的结构属性，导致在新成像中心和设备上泛化能力差，因此亟需一种更鲁棒的分割方法。

Method: 核心发现是息肉边缘在不同数据集中具有高度一致的频率特征。FPGM是一个两阶段增强框架：首先，从已标注息肉的边缘区域学习域不变的频率先验；然后，对未标注图像执行频谱扰动，使其幅度谱与学习到的先验对齐，同时保留相位信息以维持结构完整性，从而标准化域特有的纹理变化，促使模型学习通用解剖结构。

Result: FPGM在六个公共数据集上进行了验证，超越了十种现有方法，建立了新的技术水平。在数据稀缺场景下，其展现出卓越的零样本泛化能力，Dice分数绝对增益超过10%。

Conclusion: FPGM显著增强了息肉分割的跨域鲁棒性，为在有限监督下临床部署的息肉分割提供了强大解决方案。

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [64] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: 大型多模态模型（LMMs）通过“视觉反思”（验证视觉模型预测）可提高识别准确率，即使在ImageNet上。其内部机制是将视觉特征转化为紧凑文本概念供常识推理。研究还发现无需训练的连接器也能提升性能，表明视觉反思是实现稳健可解释视觉识别的有效策略。


<details>
  <summary>Details</summary>
Motivation: 探究大型多模态模型（LMMs）中“视觉反思”的可解释性，并利用LMMs的能力提升视觉识别准确率，克服其在视觉任务上通常不如专用模型的局限性。

Method: 1. 提示LMMs验证专用视觉模型的预测。2. 分析视觉-语言连接器将视觉特征映射为文本概念的内部行为，并研究语言模型如何利用常识进行推理。3. 实验用少量文本token替换大量视觉token对LMMs（如LLaVA）行为的影响。4. 引入并测试无需训练的连接器在细粒度识别任务中的效能。

Result: 1. LMMs通过验证专门视觉模型的预测，能提高识别准确率，包括在ImageNet等基准上。2. 视觉-语言连接器能将视觉特征转化为显式文本概念，使语言模型能通过常识推理判断预测的合理性。3. LMMs可能主要依赖少量精炼的文本表示，而非原始视觉特征。4. 无需训练的连接器可在不进行大量特征对齐训练的情况下，提升LMM在细粒度识别任务中的性能。

Conclusion: 这些发现提供了视觉-语言模型可解释性的新见解，并指出“视觉反思”是实现稳健且可解释视觉识别的一种有前景的策略。

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [65] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 提出一种结合3D CNN和Transformer的混合框架，用于视频行为识别，解决了传统方法的局限性，并实现了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 视频行为识别在多个领域至关重要。传统3D CNN难以捕捉长程依赖，而Transformer计算成本高，亟需一种兼顾效率与性能的解决方案。

Method: 开发了一种混合框架，结合3D CNN和Transformer架构。3D CNN模块负责提取低级时空特征，Transformer模块捕捉长程时间依赖，并通过融合机制整合两种表示。

Result: 该模型在基准数据集上表现优异，超越了传统3D CNN和独立Transformer，实现了更高的识别精度且复杂度可控。消融研究进一步证实了两个模块的互补优势。

Conclusion: 该混合框架为视频行为识别提供了一种有效且可扩展的解决方案。

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [66] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: 本文提出RMT-PPAD，一个基于Transformer的实时多任务模型，用于自动驾驶中的目标检测、可行驶区域分割和车道线分割，并在BDD100K数据集上取得了最先进的性能和实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要高精度和实时的全景感知能力。现有的多任务感知模型面临任务间负迁移、分割任务解码器手动设计复杂以及车道线标签训练测试不一致等挑战，影响了模型的效率、泛化性和公平评估。

Method: RMT-PPAD模型采用基于Transformer的架构。核心创新包括：1) 引入轻量级门控适配器模块，自适应融合共享和任务特定特征，有效缓解负迁移；2) 设计自适应分割解码器，在训练阶段自动学习多尺度特征权重，避免手动设计；3) 识别并解决车道线分割中训练和测试标签的不一致性，以实现更公平的评估。

Result: 在BDD100K数据集上，RMT-PPAD实现了SOTA性能：目标检测mAP50 84.9%，Recall 95.4%；可行驶区域分割mIoU 92.6%；车道线分割IoU 56.8%，精度84.7%。模型推理速度达到32.6 FPS。此外，在真实场景中的评估表明RMT-PPAD能持续提供稳定的性能。

Conclusion: RMT-PPAD是一个高效且鲁棒的多任务感知模型，能够满足自动驾驶系统对精度和实时性的双重需求。它通过创新性的模块设计，有效解决了多任务学习中的挑战，并在基准测试和实际应用中展现出卓越的性能。

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [67] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）存在物体幻觉问题，现有评估基准POPE效果不佳。本文提出HOPE基准，通过生成高度误导性的干扰项（包括图像特定和描述类干扰），更严格地评估LVLMs的抗幻觉能力，并证明其能显著暴露LVLMs的幻觉漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs取得了显著进展，但它们仍受困于物体幻觉问题，即生成与图像内容不符的物体。最常用的POPE评估基准因其采样策略过于简单、忽略图像特定信息且限制干扰项，在评估LVLMs的物体幻觉方面效果逐渐减弱。

Method: 本文提出HOPE（Hallucination searching-based Object Probing Evaluation）基准，旨在生成最具误导性的干扰项（不存在的物体或不正确的图像描述），以更严格地评估LVLMs的抗幻觉能力。
1. **内容感知幻觉搜索**：利用CLIP近似LVLMs的预测行为，选择预测可能性最高的负面对象作为干扰项，以探索图像特定信息。
2. **基于描述的幻觉搜索**：通过将真实对象与虚假描述配对来构建高度误导性的干扰项，以扩大幻觉评估的范围。

Result: 实验结果表明，HOPE在各种先进的LVLMs上导致至少9%到23%的精度下降，显著优于POPE在暴露幻觉漏洞方面的表现。

Conclusion: HOPE基准通过引入更具挑战性和误导性的干扰项，能更严格、有效地评估LVLMs的物体幻觉问题，显著暴露了当前最先进LVLMs的幻觉脆弱性。

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [68] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 本文提出了一个名为MobilTelesco的智能手机天文摄影数据集，旨在解决现有目标检测数据集缺乏信号稀疏性的问题，并在此数据集上评估了多种检测模型，揭示了在特征不足条件下的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型主要基于日常物体数据集（如ImageNet, COCO, PASCAL VOC）训练，这些数据集缺乏非商业领域（如天文摄影）中存在的信号稀疏性。

Method: 构建了一个基于智能手机的天文摄影数据集MobilTelesco，该数据集提供稀疏的夜空图像。在该数据集上对多个目标检测模型进行了基准测试。

Result: 基准测试结果揭示了在特征稀疏（信号不足）条件下，现有目标检测模型所面临的挑战。

Conclusion: 当前目标检测模型在处理具有信号稀疏性的非商业领域数据时存在局限性，MobilTelesco数据集有助于凸显这些挑战并促进相关研究。

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [69] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 针对扩散模型在复杂多人物图像擦除中的不足，本文提出MILD框架，通过构建新数据集、多层分解生成和引入人体形态/空间引导机制，显著提升了人体擦除性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂多人物（涉及人体-人体遮挡、人体-物体缠绕及背景干扰）人体擦除任务中表现不佳。主要原因在于：1) 数据集缺乏对密集遮挡、伪装背景和多样互动的覆盖；2) 缺乏有效的空间解耦能力，无法干净地分离前景实例并恢复背景。

Method: 1. 构建了一个高质量的多人物人体擦除数据集，包含多样姿态和复杂背景。
2. 提出了多层扩散（Multi-Layer Diffusion, MILD）策略，将图像生成分解为语义上独立的人体实例和背景路径。
3. 引入了人体形态引导（Human Morphology Guidance），结合姿态、解析和空间关系以增强以人为中心的理解。
4. 提出了空间调制注意力（Spatially-Modulated Attention），以更好地引导注意力流。

Result: 在挑战性人体擦除基准测试中，MILD方法显著优于现有最先进（SOTA）方法。

Conclusion: 本文通过创新性地构建数据集、提出MILD多层分解策略并引入特定引导机制，成功解决了复杂多人物场景下的人体擦除难题，并取得了领先的性能，推动了该领域的进展。

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [70] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本研究提出一种仅利用多视角RGB图像进行3D语义场景图估计的方法，通过语义掩码、邻居信息整合和统计先验来克服噪声，性能优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D语义场景图估计依赖于3D真值标注。本研究旨在解决在缺乏3D真值的情况下，仅利用多视角RGB图像进行此任务的挑战，特别是要克服预测深度图导致的噪声几何以及多视角图像特征中的背景噪声。

Method: 核心是利用准确的语义和空间信息以及邻居关系来丰富节点和边缘特征。具体方法包括：获取语义掩码以引导特征聚合并过滤背景；设计新方法整合邻居节点信息以增强鲁棒性；利用训练统计数据计算的显式统计先验，基于一跳邻居关系精炼节点和边缘预测。

Result: 实验证明，该方法在仅使用多视角图像作为初始输入的情况下，性能优于当前同类方法。

Conclusion: 本研究成功地提出了一种在无3D真值标注的情况下，仅从多视角RGB图像估计3D语义场景图的有效方法，通过创新的特征增强和噪声过滤策略，显著提升了估计的准确性和鲁棒性。

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [71] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator是一种新颖的实用控制机制，允许单个AI模型根据预定义条件动态调整性能，从而实现分层访问并提高资源效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练和模型适应性成本高昂，且难以用单一模型满足不同应用的多样化性能需求，传统多模型版本解决方案效率低下、难以维护。

Method: 提出NNObfuscator，一种实用控制机制，使单个AI模型能够根据预定义条件动态调整其性能，实现实时适应和分层访问（例如，免费用户与高级用户）。

Result: 在图像分类、语义分割和文本到图像生成等任务上，使用ResNet、DeepLab、VGG16、FCN和Stable Diffusion等模型进行的实验表明，NNObfuscator成功提高了模型的适应性，使单个训练模型无需大量修改即可处理广泛的任务。

Conclusion: NNObfuscator为AI模型的部署提供了一种有效且可持续的解决方案，通过允许单个模型动态调整性能，优化了资源分配，减少了不必要的计算，并支持了可持续的业务模型。

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [72] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 本研究通过构建一个年龄多样化的深度伪造数据集，有效缓解了现有数据集中年龄特异性偏差问题，提升了深度伪造检测模型的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术发展，其检测面临挑战。现有检测模型普遍存在数据集中的人口统计学偏差，特别是年龄特异性偏差，这导致检测公平性不足。

Method: 研究构建了一个年龄多样化的深度伪造数据集。该数据集整合了Celeb-DF、FaceForensics++和UTKFace等现有数据集，并通过生成合成数据来填补年龄分布的空白。使用XceptionNet、EfficientNet和LipForensics三种深度伪造检测模型，通过AUC、pAUC和EER等指标评估了该数据集的有效性和泛化能力。

Result: 实验结果表明，在年龄多样化数据集上训练的模型，在不同年龄组间表现出更公平的性能，且整体准确性更高，在不同数据集上的泛化能力也更强。

Conclusion: 本研究提供了一个可复现的、具备公平性意识的深度伪造数据集和模型管道，为未来更公平的深度伪造检测研究奠定了基础。

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [73] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出StaticEmbodiedBench，一个基于静态场景表示的即插即用基准，旨在解决现有具身智能评估成本高、碎片化和难以扩展的问题，并建立了首个统一的静态排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前的具身智能评估基准依赖于昂贵、碎片化且难以扩展的交互式模拟或真实世界设置，导致对高效评估的需求。

Method: 引入了StaticEmbodiedBench，一个利用静态场景表示进行统一评估的即插即用基准。它涵盖42个多样化场景和8个核心维度，通过简单界面实现可扩展和全面的评估。

Result: 使用StaticEmbodiedBench评估了19个视觉-语言模型（VLMs）和11个视觉-语言-动作模型（VLAs），建立了首个具身智能的统一静态排行榜。此外，还发布了基准的200个样本子集，以加速具身智能的发展。

Conclusion: StaticEmbodiedBench通过提供一个高效、可扩展且统一的评估框架，利用静态场景表示，显著推进了具身智能的评估和发展。

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [74] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor是一个首创的协同代理框架，通过多级负反馈迭代视觉优化，整合个性化服装设计、购物推荐和虚拟试穿，显著提升了智能时尚系统性能。


<details>
  <summary>Details</summary>
Motivation: 尽管智能代理在问题解决方面取得了显著进展，但个性化时尚造型的解决方案仍未充分探索，而这对于提升购物体验具有巨大潜力。

Method: StyleTailor框架通过一个迭代视觉优化范式实现，该范式由多级负反馈驱动，以实现自适应和精确的用户对齐。它包含两个核心代理：负责个性化服装选择的Designer和用于虚拟试穿的Consultant。它们的输出通过分层视觉-语言模型反馈（涵盖单品、完整套装和试穿效果）逐步优化。反例被聚合成负面提示，形成闭环机制以提高推荐质量。同时，引入了一个综合评估套件来评估性能。

Result: StyleTailor在提供个性化设计和推荐方面表现出卓越性能，超越了没有负反馈的强大基线，并为智能时尚系统建立了新的基准。

Conclusion: StyleTailor成功地将个性化服装设计、购物推荐和虚拟试穿整合到一个统一的框架中，并利用多级负反馈机制显著提升了推荐质量和用户对齐度，为智能时尚系统领域树立了新标杆。

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [75] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [76] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 多模态特权知识蒸馏（MMPKD）利用训练时独有的辅助模态，提升单模态视觉模型零样本ROI定位能力，但其跨域泛化性有限。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中部署深度学习模型常需利用多模态数据以实现稳健决策，但并非所有模态在推理时都可用，这给单模态模型带来了挑战。

Method: 作者提出了多模态特权知识蒸馏（MMPKD）训练策略，该策略利用仅在训练时可用的额外模态来指导单模态视觉模型。具体而言，他们使用基于文本的教师模型处理胸部X光片（MIMIC-CXR），以及基于表格元数据的教师模型处理乳腺X光片（CBIS-DDSM），将知识蒸馏到视觉Transformer学生模型中。

Result: MMPKD能够提升所生成注意力图在输入图像中定位感兴趣区域（ROI）的零样本能力。然而，这种效果并未跨领域泛化，这与先前的研究结果相悖。

Conclusion: MMPKD是一种利用特权知识增强单模态视觉模型可解释性和定位能力的有效策略。然而，其在不同临床领域间的泛化性需要重新评估，因为它并未像先前假设的那样普遍泛化。

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [77] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: 针对对话中的多模态情感识别，本文提出一种基于CLIP图像编码器的新型视觉情感引导锚定（VEGA）机制，通过构建情感视觉锚点引导特征对齐，并在IEMOCAP和MELD数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 对话中的多模态情感识别仍具挑战，现有模型虽通过高级融合策略提升了性能，但普遍缺乏心理学意义上的先验知识来有效指导多模态特征的对齐。

Method: 提出一种新颖的视觉情感引导锚定（VEGA）机制，该机制利用CLIP的图像编码器，而非文本编码器，基于面部示例构建情感特异性视觉锚点。这些锚点借鉴认知理论，旨在将单模态和多模态特征引导至感知上和心理上对齐的表示空间。为增强鲁棒性，还引入了随机锚点采样策略，并将VEGA机制整合到采用自蒸馏的双分支架构中。

Result: 所提出的VEGA增强模型在IEMOCAP和MELD两个基准数据集上均取得了最先进（SOTA）的性能。

Conclusion: VEGA机制通过引入具有心理学意义的视觉先验（情感视觉锚点），有效解决了多模态情感识别中的特征对齐问题，显著提升了模型性能，表明将认知理论融入模型设计是有效途径。

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [78] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 提出一种新颖框架，将脑连接组与临床报告在共享跨模态空间中对齐，通过将脑子网络视为数据令牌，实现脑疾病的早期诊断并发现临床有意义的关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究在整合客观脑成像数据与主观文本报告时面临挑战，难以有效连接两者，阻碍了多模态信息在脑疾病诊断中的应用。

Method: 开发了一种新颖框架，在共享跨模态潜在空间中对齐脑连接组与临床报告。核心创新在于将脑子网络而非原始图像块视为成像数据令牌，与临床报告中的词令牌对齐，以识别系统级关联。

Result: 将该方法应用于ADNI数据集的轻度认知障碍(MCI)诊断，不仅取得了最先进的预测性能，还成功识别出具有临床意义的连接组-文本对。

Conclusion: 该方法为阿尔茨海默病早期机制提供了新见解，并支持开发具有临床应用价值的多模态生物标志物。

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [79] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 该论文提出了Surformer v1，一个基于Transformer的多模态模型，结合结构化触觉特征和PCA降维视觉嵌入进行表面材料识别。实验表明，Surformer v1在保持高准确率的同时，实现了极高的推理效率，在多模态融合任务中表现出优越的平衡性。


<details>
  <summary>Details</summary>
Motivation: 表面材料识别是机器人感知和物理交互的关键组成部分，特别是在融合触觉和视觉感官输入时。因此，需要开发高效准确的模型来解决这一问题。

Method: 1. 提出Surformer v1，一个基于Transformer的架构，用于表面分类，输入为结构化触觉特征和ResNet-50提取的PCA降维视觉嵌入。2. 模型整合了模态特定编码器和跨模态注意力层，以实现视觉和触觉之间的丰富交互。3. 实验分为两部分：a) 纯触觉表面分类：评估了多种机器学习模型，并实现了一个专门用于触觉特征的仅编码器Transformer模型。b) 多模态融合：结合视觉和触觉输入，比较了Surformer v1（基于特征）和多模态CNN（基于原始图像）在分类准确率和计算效率方面的影响。

Result: 1. 纯触觉分类实验中，仅编码器的Transformer模型取得了最高的准确率和显著更快的推理时间。2. 多模态融合实验中，Surformer v1实现了99.4%的准确率，推理时间为0.77毫秒。3. 相比之下，多模态CNN虽然准确率略高，但需要显著更长的推理时间。

Conclusion: Surformer v1在表面材料识别方面，在准确性、效率和计算成本之间取得了引人注目的平衡，具有很高的实用潜力。

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [80] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出ImpliHateVid数据集和两阶段对比学习框架，用于视频中的隐式仇恨言论检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于文本和图像的仇恨言论检测，而视频领域的，特别是隐式仇恨言论检测，尚未得到充分探索。

Method: 1. 构建了大规模视频数据集ImpliHateVid（2009个视频）用于隐式仇恨言论检测。2. 提出了一个新颖的两阶段对比学习框架：第一阶段训练模态特定编码器，第二阶段训练交叉编码器以优化多模态表示。3. 整合了情感、情绪和字幕特征以增强检测。

Result: 所提出的多模态对比学习方法在ImpliHateVid和HateMM数据集上均表现出有效性，证明了其在视频仇恨内容检测中的效果及数据集的重要性。

Conclusion: 本研究通过引入专用视频数据集和创新的多模态对比学习框架，有效地推进了视频中隐式仇恨言论的检测能力，填补了现有研究的空白。

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [81] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: 本文提出ContextGuard-LVLM框架，基于LVLM和多阶段语境推理机制，通过强化/对抗学习和增强数据集，旨在解决数字新闻中图文内容的细粒度跨模态语境一致性问题。实验证明，该模型在复杂逻辑推理和细致语境理解方面显著优于现有基线模型，并能有效检测细微的语境不一致。


<details>
  <summary>Details</summary>
Motivation: 数字新闻媒体的普及，使得验证内容真实性，特别是图文信息间的一致性变得至关重要。传统方法在处理细粒度跨模态语境一致性（FCCC）问题上存在不足，FCCC要求对视觉叙事、情感基调和背景信息与文本进行更深层次的对齐，而非仅仅实体匹配。

Method: 本文提出了ContextGuard-LVLM框架，该框架基于先进的视觉-语言大模型（LVLMs），并整合了多阶段语境推理机制。模型通过强化学习或对抗学习范式进行独特增强，使其能够检测到零样本基线难以察觉的细微语境错位。此外，研究者扩展并增强了三个现有数据集，增加了“语境情感”、“视觉叙事主题”和“场景-事件逻辑连贯性”等新的细粒度语境标注，并引入了全面的CTXT（Contextual Coherence）实体类型。

Result: 广泛的实验表明，ContextGuard-LVLM在几乎所有细粒度一致性任务上都持续优于最先进的零样本LVLM基线模型（InstructBLIP和LLaVA 1.5），在复杂逻辑推理和细致语境理解方面显示出显著改进。此外，模型对细微扰动表现出卓越的鲁棒性，并在具挑战性的样本上与人类专家判断具有更高的一致性。

Conclusion: ContextGuard-LVLM能够有效识别复杂的语境脱节形式，证实了其在内容真实性验证中辨别精细语境分离的有效性。

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [82] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: 本文提出VL-MedGuide框架，利用视觉-语言大模型（LVLMs）实现皮肤病的智能可解释辅助诊断，在Derm7pt数据集上达到SOTA性能，并提供清晰可靠的解释。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断面临挑战，主要由于皮肤镜图像特征复杂多样，且现有纯视觉诊断模型缺乏可解释性。

Method: 引入VL-MedGuide框架，该框架利用视觉-语言大模型（LVLMs）的多模态理解和推理能力。它包含两个阶段：多模态概念感知模块（通过提示工程识别并描述皮肤病学相关视觉特征）和可解释疾病推理模块（通过思维链提示整合概念与原始视觉信息，提供精确诊断和透明理由）。

Result: 在Derm7pt数据集上，VL-MedGuide在疾病诊断（83.55% BACC, 80.12% F1）和概念检测（76.10% BACC, 67.45% F1）方面均达到最先进水平。此外，人工评估证实其生成的解释具有高清晰度、完整性和可信度。

Conclusion: VL-MedGuide通过提供可操作、可解释的见解，弥合了人工智能性能与临床实用性之间的差距，为皮肤科实践提供了有价值的辅助诊断工具。

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [83] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [84] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 本文提出一种基于动态系数分解的神经渲染框架，以解决NeRF在建模复杂高光和反射时存在的模糊和优化问题，从而生成更清晰逼真的视图。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF在渲染具有复杂镜面反射和高光的场景时面临挑战，表现为光照与材质属性的耦合导致反射模糊，以及依赖物理逆渲染可能出现优化不稳定。

Method: 本研究提出一个基于动态系数分解的神经渲染框架。它将复杂外观分解为：1. 编码固有材质属性的共享静态神经基；2. 由系数网络根据视角和光照生成的动态系数。然后，一个动态辐射积分器将这些分量结合以合成最终辐射。

Result: 在多个挑战性基准测试中，实验结果表明该方法与现有技术相比，能够生成更清晰、更真实的镜面高光。

Conclusion: 所提出的分解范式为神经场景表示中复杂外观建模提供了一个灵活有效的方向。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [85] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: 微表情识别中，现有基于关键帧的方法受关键帧索引误差影响大。本文提出CausalNet框架，通过处理完整微表情序列并引入CMPLM和CAB模块，实现了在关键帧索引存在误差情况下的鲁棒识别，并在准确关键帧下超越了SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键帧的微表情识别方法过分依赖精确的关键帧索引，忽视了关键帧获取难度及客观存在的索引误差，这限制了其在实际应用中的推广。

Method: 本文提出CausalNet框架以实现鲁棒的微表情识别。具体方法包括：1. 以完整的微表情序列作为输入，以增强鲁棒性。2. 引入因果运动位置学习模块（CMPLM），帮助模型定位与动作单元（AUs）相关的肌肉运动区域，从而减少对冗余区域的关注。3. 设计因果注意力块（CAB），以深入学习微表情中肌肉收缩与放松运动之间的因果关系。

Result: 实验结果表明，在流行的微表情基准测试上，CausalNet在不同程度的关键帧索引噪声下均实现了鲁棒的微表情识别。同时，在使用提供的带标注关键帧时，CausalNet在多个标准微表情基准测试上超越了现有最先进（SOTA）方法。

Conclusion: CausalNet框架有效解决了微表情识别中关键帧索引误差导致的鲁棒性问题，并在保持高准确性的同时提升了实用性。它能够在存在关键帧索引噪声时进行鲁棒识别，同时在准确关键帧下达到或超越SOTA水平。

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [86] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 针对自回归（AR）图像模型水印的鲁棒性问题，本文提出两种基于视觉令牌聚类的新型水印方法，有效提升了在图像扰动下的检测率，同时保持图像质量并实现快速验证。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式水印技术已应用于潜在扩散模型（LDMs）并显示出高鲁棒性，但尚未在自回归（AR）图像模型中进行探索，这促使作者研究AR模型的生成式水印。

Method: 研究首先探索了借鉴大型语言模型（LLMs）的令牌级水印方案，但发现其在图像扰动下检测率显著下降。为此，作者提出了两种基于视觉令牌聚类的新型水印方法：一是依赖聚类查找表的无训练方法；二是通过微调VAE编码器直接从受扰动图像预测令牌聚类的方法。

Result: 实验结果表明，聚类级水印显著提高了对图像扰动和再生成攻击的鲁棒性，同时保持了图像质量。聚类分类进一步提升了水印检测率，优于现有基线方法。此外，所提出的方法验证运行时效快，与轻量级后置水印方法相当。

Conclusion: 所提出的基于视觉令牌聚类的水印方法成功解决了自回归（AR）图像模型水印在鲁棒性方面的挑战，特别是在应对常见图像扰动和再生成攻击方面表现优异，同时兼顾了图像质量和验证效率。

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [87] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: 本研究提出利用线稿进行预训练，以获取更紧凑、泛化能力更强的视觉表示，并证明其在提升模型性能、数据效率和促进知识蒸馏方面的有效性，支持“结构优先”的视觉学习范式。


<details>
  <summary>Details</summary>
Motivation: 尽管计算机视觉取得了显著进展，但现代识别系统仍受限于对丰富、冗余视觉输入的依赖。与此相反，人类能够轻松理解稀疏、极简的表示（如线稿），这表明结构而非外观是高效视觉理解的基础。现有系统未能充分利用这种“结构优先”的学习方式。

Method: 研究提出将线稿作为一种“结构优先”的预训练模式，旨在诱导生成更紧凑和可泛化的视觉表示。此外，该预训练方法还可以通过“学习绘画”扩展到无监督设置。

Result: 经过线稿预训练的模型在分类、检测和分割任务中表现出更强的形状偏好、更集中的注意力和更高的数据效率。这些模型还具有更低的内在维度，能用更少的主成分捕获表示方差。此外，线稿预训练能产生更可压缩的表示，从而更好地蒸馏到轻量级学生模型中，且蒸馏出的学生模型性能优于由彩色监督教师蒸馏的模型。

Conclusion: 研究结果支持“结构优先”的视觉学习能够提升视觉系统的效率、泛化能力和人类对齐的归纳偏置。通过线稿预训练，为构建更鲁棒和适应性更强的视觉系统提供了一个简单而强大的策略。

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [88] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出MMFformer，一个多模态网络，通过分析社交媒体音视频信息，实现了更准确的抑郁症早期检测，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症早期诊断至关重要但传统方法主观且困难。社交网络数据提供新途径，但其多模态信息的准确提取和有效融合是关键挑战。

Method: 本文引入MMFformer，一个多模态抑郁症检测网络。它利用带残差连接的Transformer从视频中捕获空间特征，并使用Transformer编码器处理音频中的重要时间动态。此外，通过后期和中间融合策略融合提取的特征，以发现模态间关联，从而识别抑郁症时空模式。

Result: MMFformer在D-Vlog和LMVD两个大型抑郁症数据集上进行了评估，F1-Score分别提高了13.92%和7.74%，性能显著优于现有最先进的方法。

Conclusion: MMFformer通过有效整合社交媒体的多模态信息，显著提升了抑郁症的早期检测精度，为克服传统诊断局限和数据融合挑战提供了有效的解决方案。

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [89] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: 提出了一种高效快速的CGH合成管线框架，利用点云和MRI数据，通过非凸傅里叶光学优化生成全息图，并通过中值滤波显著改善了重建性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效快速的CGH合成管线框架，以调制用户定义波形，并利用初始点云和MRI数据作为输入。

Method: 该方法提出一个管线框架，将初始点云和MRI数据重建为体三维对象。这些对象随后输入到非凸傅里叶光学优化算法（包括交替投影、SGD和拟牛顿法）中，用于生成相位型全息图(POH)和复数全息图(CH)。通过MSE、RMSE和PSNR指标评估这些算法的重建性能，并与HoloNet深度学习CGH进行比较。在优化过程中，应用2D中值滤波以消除伪影和散斑噪声。

Result: 所提出的方法能够生成POH和CH，并通过MSE、RMSE和PSNR指标评估了重建性能。结果显示，在优化过程中使用2D中值滤波可以有效去除伪影和散斑噪声，从而显著提升性能指标。

Conclusion: 本研究提出了一个高效快速的CGH合成管线框架，能够利用点云和MRI数据生成高质量全息图，并通过集成2D中值滤波进一步提升了重建效果。

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [90] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 利用真实视频的运动先验，对可变形3D场景进行4D重演，修正生成模型错误，实现物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像/图像到视频生成模型在生成4D内容时，缺乏物理真实性和运动动力学，难以捕捉可变形物体的运动细节。作者提出利用真实世界视频的运动先验，以实现物理一致的4D内容生成，并纠正合成运动中的伪影。

Method: 提出了Restage4D，一个几何保持的视频条件4D重演流水线。该方法通过“视频回溯”训练策略，利用共享运动表示来连接真实基础视频和合成驱动视频，并引入了“遮挡感知刚性损失”和“去遮挡回溯机制”，以提高复杂运动下的结构和几何一致性。

Result: 在DAVIS和PointOdyssey数据集上进行了验证，结果表明Restage4D显著提升了几何一致性、运动质量和3D跟踪性能。

Conclusion: 该方法不仅在新的运动下保持了可变形结构，还自动纠正了生成模型引入的错误，揭示了视频先验在4D重演任务中的巨大潜力。

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [91] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 研究提出FoundBioNet，一个基于基础深度学习模型的方法，通过多参数MRI无创准确预测胶质瘤IDH突变状态。


<details>
  <summary>Details</summary>
Motivation: 传统胶质瘤异柠檬酸脱氢酶（IDH）突变检测方法具有侵入性，且难以捕捉肿瘤空间异质性；现有深度学习模型常受限于标注数据稀缺，泛化能力不足。因此，需要开发一种准确、非侵入性且泛化能力强的IDH突变检测方法。

Method: 本研究提出FoundBioNet（Foundation-based Biomarker Network），一个基于SWIN-UNETR架构的基础深度学习模型，利用多参数MRI无创预测IDH突变状态。该模型整合了两个关键模块：肿瘤感知特征编码（TAFE）用于提取多尺度、以肿瘤为中心的特征，以及跨模态差异（CMD）用于突出与IDH突变相关的细微T2-FLAIR错配信号。

Result: 该模型在来自六个公共数据集的1705名胶质瘤患者的多元多中心队列上进行了训练和验证。在EGD、TCGA、Ivy GAP、RHUH和UPenn的独立测试集上，模型分别取得了90.58%、88.08%、65.41%和80.31%的AUC，并持续优于基线方法（p <= 0.05）。消融研究证实TAFE和CMD模块对提高预测准确性至关重要。

Conclusion: FoundBioNet通过整合大规模预训练和任务特定微调，实现了胶质瘤特征描述的泛化能力，显著提高了诊断准确性和可解释性，有望为患者提供更个性化的护理。

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [92] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 研究人员创建了一个名为VOccl3D的视频基准数据集，包含逼真的3D人体姿态和形状遮挡场景，旨在弥补现有数据集在真实性上的不足，并通过该数据集显著提升了人体姿态估计和人体检测在遮挡情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态和形状（HPS）估计方法在复杂姿态或显著遮挡的场景下表现不佳。此外，尽管有研究关注遮挡下的3D HPS，但现有数据集的遮挡通常不真实或不充分（例如，使用随机补丁或剪贴画覆盖），无法有效反映真实世界的挑战。

Method: 研究引入并构建了一个名为VOccl3D的视频基准数据集，该数据集包含3D人体姿态和形状标注，通过先进的计算机图形渲染技术生成，融合了多样化的真实世界遮挡场景、服装纹理和人体动作。此外，研究人员在该数据集上微调了现有的HPS方法（CLIFF和BEDLAM-CLIFF），并微调了YOLO11目标检测器以增强遮挡下的人体检测性能，从而构建了一个鲁棒的端到端HPS估计系统。

Result: 在VOccl3D数据集和多个公共数据集上，经过微调的HPS方法（CLIFF和BEDLAM-CLIFF）展现出显著的定性和定量改进，并优于其他SOTA方法。通过微调YOLO11，有效提升了遮挡下的人体检测性能，最终形成了一个在遮挡条件下更为鲁棒的端到端HPS估计系统。

Conclusion: VOccl3D数据集是一个宝贵的资源，为未来研究旨在处理遮挡问题的方法提供了更真实的基准测试平台，是现有遮挡数据集的有效替代方案。

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [93] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG通过像素级理解和时间定位，解决多模态大语言模型（MLLMs）在交通事故分析中细粒度不足的问题，并构建了新的交通事故理解数据集。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在交通事故理解方面主要侧重粗粒度分析，难以处理细粒度视觉细节或局部场景组件，限制了其在复杂事故场景中的应用。

Method: 本文提出SafePLUG框架，赋予MLLMs像素级理解和时间定位能力，以实现全面的交通事故分析。SafePLUG支持区域感知问答、像素级分割和时间事件识别。此外，还构建了一个包含详细像素级标注和时间事件边界的新数据集。

Result: 实验结果表明，SafePLUG在多项任务上表现出色，包括基于区域的问答、像素级分割、时间事件定位和事故事件理解。

Conclusion: SafePLUG的能力为复杂交通场景的细粒度理解奠定了基础，有望提升驾驶安全并增强智能交通系统中的态势感知能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [94] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一个基于物理的可微分超声渲染器，能将MRI数据转化为逼真的B模式超声图像，旨在弥合术前规划与术中引导之间的差距，并支持下游优化应用。


<details>
  <summary>Details</summary>
Motivation: 术中超声成像虽提供实时引导，但其解释受限于噪声、伪影以及与高分辨率术前MRI/CT扫描对齐不佳的问题，导致术前规划与术中引导之间存在脱节。

Method: DiffUS首先通过机器学习将MRI 3D扫描转换为声阻抗体积；接着，利用射线追踪和耦合反射-透射方程模拟超声波传播，并将其表述为捕获多重内部反射的稀疏线性系统；最后，通过深度解析回波提取重建B模式图像，并融入散斑噪声、深度依赖性退化等真实伪影。整个系统在PyTorch中实现为可微分张量操作，支持基于梯度的优化。

Result: 在ReMIND数据集上的评估证明，DiffUS能够从脑部MRI数据生成解剖学上准确的超声图像。

Conclusion: DiffUS成功地提供了一种基于物理且可微分的方法，通过合成逼真的超声图像来解决术中超声解释的挑战，并为切片到体积配准和体积重建等下游应用提供了潜力。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [95] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 本文提出一种医用清晰边缘检测器，采用新颖的自顶向下逆向细化架构，能为2D和3D医学图像提供高精度、毫米级器官边界定位，显著优于现有方法，并提升下游医学任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中精确的器官边界定位对分割、配准、手术规划和放疗至关重要。尽管深度卷积网络在通用边缘检测上表现出色，但其输出缺乏精确的定位，这对于需要毫米级精度的医学应用而言是一个严重缺陷。

Method: 该方法基于对卷积网络边缘输出的系统分析，提出一种医用清晰边缘检测器。它采用新颖的自顶向下逆向细化架构，通过逆向细化路径逐步上采样并融合高级语义特征与细粒度低级信息，生成高分辨率、定位精确的器官边界。为处理各向异性体素，该设计结合2D切片细化与轻量级3D上下文聚合，以保持计算效率。

Result: 在多个CT和MRI器官数据集上的评估显示，与基线卷积网络检测器和当代医学边缘/轮廓方法相比，该方法在严格标准（边界F-measure、豪斯多夫距离）下显著改善了边界定位。将该清晰边缘图整合到下游流程中，能一致性地提升器官分割（更高的Dice分数、更低的边界误差）、更精确的图像配准以及改善器官界面附近病变的描绘。

Conclusion: 所提出的方法能够生成具有临床价值的清晰器官边缘，显著增强了常见的医学成像任务。

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [96] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 针对皮肤镜图像中黑色素瘤分割的挑战，本文提出一种基于ResNet的双分辨率网络，通过融合边界感知连接、通道注意力、伪影抑制和多任务学习，实现了高精度、边界清晰的肿瘤分割，为自动化诊断提供支持。


<details>
  <summary>Details</summary>
Motivation: 自动皮肤癌筛查和临床决策急需皮肤镜图像中黑色素肿瘤的精确分割。该任务面临纹理色彩细微变化、频繁伪影（如毛发、尺子、气泡）干扰及对高精度边界定位的严格要求。

Method: 设计ResNet启发的双分辨率架构：包含一个全分辨率流以保留精细边界信息，和一个池化流以聚合多尺度上下文特征。通过边界感知残差连接和通道注意力模块紧密耦合双流。引入轻量级伪影抑制模块以应对图像伪影。采用多任务训练目标，结合Dice Tversky分割损失、显式边界损失和对比正则器以提高特征稳定性。

Result: 所提出的方法无需繁重后处理或复杂预训练，即可生成像素级精确的分割掩膜。在公共皮肤镜基准测试中，与标准编码器-解码器基线相比，显著提升了边界依从性和临床相关分割指标。

Conclusion: 本文方法为自动化黑色素瘤评估系统提供了一个实用且高效的构建模块。

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [97] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 本文提出一种弱监督框架，利用少量稀疏标注，结合可微分随机游走模型和CNN，实现准确的皮下血管分割，显著减少标注工作量并保持拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 皮下血管的精确分割面临挑战：真实标注数据稀缺且昂贵；临床图像中血管对比度低、噪声大且跨患者和模态表现不一。

Method: 开发了一种新颖的弱监督训练框架。该框架利用廉价的稀疏标注（如中心线、点或短划线）并将其通过一个可微分随机游走标签传播模型扩展为密集的概率监督，模型整合了图像驱动的血管性特征和管状连续性先验。传播结果包括像素级命中概率和校准的不确定性估计，用于不确定性加权损失以避免对模糊区域的过拟合。标签传播器与基于CNN的分割预测器联合学习，无需显式边缘监督即可发现血管边缘和连续性约束。此外，引入了拓扑感知正则化器，以促进中心线连通性并惩罚虚假分支。

Result: 在临床皮下成像数据集上的实验表明，该方法始终优于基于稀疏标签的朴素训练和传统的密集伪标签方法。它能生成更完整的血管图，并为后续决策提供更好校准的不确定性。

Conclusion: 该方法大幅减少了标注负担，同时保留了临床相关的血管拓扑结构，提升了临床可用性。

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [98] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: 该论文提出SAGE-reID，一种成本效益高、无需源数据的多源域适应方法，用于行人重识别。它通过训练低秩适配器(LoRA)和门控网络动态融合专家知识，在保持计算效率的同时，优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人员重识别多源域适应(MSDA)方法在将模型适应新目标环境时，通常需要巨大的训练参数或在适应过程中访问源域数据，导致计算成本和内存消耗显著增加。

Method: 本文提出了Source-free Adaptive Gated Experts (SAGE-reID)方法。该方法首先通过无源域适应(UDA)训练独立的源域特定低秩适配器(LoRA)。随后，引入一个轻量级门控网络来动态分配LoRA专家的最优融合权重，以实现有效的跨域知识迁移。该方法保持骨干网络参数不变，LoRA专家参数量极小（小于骨干的2%）。

Result: 在Market-1501、DukeMTMC-reID和MSMT17三个挑战性基准上的广泛实验表明，SAGE-reID在性能上超越了现有最先进的方法，同时计算效率高，内存消耗和过拟合风险显著降低。

Conclusion: SAGE-reID是一种高效且无需源数据的人员重识别多源域适应新方法。通过创新性地利用低秩适配器和动态门控网络，它解决了传统MSDA方法的计算成本问题，并在多个基准测试中展示了卓越的性能和计算效率。

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [99] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: 本研究利用先进的3D表面分析和混合机器学习框架，显著提高了制造零件几何偏差的预测精度，超越了传统方法，并揭示了制造参数与偏差间的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管现代制造业技术先进，但在复杂几何零件的生产中，准确预测几何偏差和保持尺寸精度仍是主要挑战。

Method: 采用高分辨率3D扫描仪获取237个零件的多角度表面数据，并进行数据处理（对齐、降噪、合并）；开发混合机器学习框架，结合卷积神经网络（CNN）进行特征提取和梯度提升决策树（GBDT）进行预测建模。

Result: 预测精度达到0.012毫米（95%置信水平），比传统统计过程控制方法提高73%；同时揭示了制造参数与几何偏差之间的隐藏关联。

Conclusion: 该方法在精密制造的自动化质量控制、预测性维护和设计优化方面具有显著潜力，并为未来的预测建模研究提供了坚实的数据基础。

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [100] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: 本文提出注意力引导图像描述（AGIC）模型，通过放大视觉特征区域和混合解码策略，在图像描述任务上实现了更优或相当的性能，且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 尽管图像描述领域取得了显著进展，但生成准确和描述性强的文本仍然是一个长期存在的挑战。

Method: 提出了注意力引导图像描述（AGIC）模型，该模型通过在特征空间直接放大显著视觉区域来指导描述生成。此外，引入了一种结合确定性和概率采样的混合解码策略，以平衡描述的流畅性和多样性。

Result: AGIC模型在Flickr8k和Flickr30k数据集上与多个现有最先进模型持平或超越，同时实现了更快的推理速度。该模型在多项评估指标上均表现出强大的性能。

Conclusion: AGIC提供了一种可扩展且可解释的图像描述解决方案。

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [101] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [102] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: 针对长视频理解中关键帧检索的多模态对齐与时序语义捕获不足，VSI提出一种融合字幕、时间戳和场景边界的双流多模态关键帧搜索方法，显著提升了关键帧定位和长视频问答的准确率。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对MLLMs构成挑战，现有关键帧检索方法因多模态对齐弱和未能捕捉复杂时序语义信息，在处理长视频时效果受限。

Method: 提出Visual-Subtitle Integration (VSI) 方法，通过整合字幕、时间戳和场景边界，采用视频搜索流和字幕匹配流的双流搜索机制，捕获视觉与文本信息，并利用双流交互提升关键帧搜索精度。

Result: VSI在LongVideoBench的文本相关子集上关键帧定位准确率达40.00%，在长视频问答任务上准确率达68.48%，分别超越基线20.35%和15.79%。同时，VSI在中长视频问答任务中达到SOTA水平。

Conclusion: VSI有效解决了长视频理解中的关键帧检索难题，通过其创新的多模态搜索策略，展现了卓越的鲁棒性和泛化能力，显著提升了视频问答性能。

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [103] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出一种新型噪声抑制特征金字塔网络（NS-FPN），通过频域分析和噪声抑制，解决了现有红外小目标检测方法误报率高的问题，显著降低误报并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测与分割（IRSTDS）任务面临目标模糊、背景杂乱的挑战。现有基于CNN的方法侧重特征增强，但未有效抑制噪声，导致误报率较高。

Method: 本文从频域分析入手，提出NS-FPN，将低频引导特征净化（LFP）模块和螺旋感知特征采样（SFS）模块集成到FPN结构中。LFP模块通过净化高频分量抑制噪声，SFS模块则利用螺旋采样融合目标相关特征。该网络轻量且可插拔。

Result: 在公共IRSTDS数据集上进行的广泛实验表明，所提方法显著降低了误报率。

Conclusion: NS-FPN在IRSTDS任务上取得了卓越性能，通过有效抑制噪声，成功减少了误报，提升了红外小目标检测与分割的准确性。

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [104] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 主流多模态大语言模型（MLLMs）在视觉理解中缺乏对视觉投影器输出的直接视觉监督，阻碍了视觉嵌入的精细对齐。本文提出BASIC方法，利用LLM内部的精炼视觉嵌入直接指导视觉投影器，显著提升了MLLM在多项基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 主流MLLMs依赖视觉投影器连接视觉编码器和大语言模型，但现有方法在对齐视觉嵌入时，仅将其作为文本输出的上下文线索，缺乏直接的视觉监督。这种不足阻碍了视觉嵌入的精细对齐和潜在性能提升。

Method: 本文提出BASIC方法。该方法基于对LLM浅层视觉嵌入细化过程的分析，利用LLM内部的细化视觉嵌入作为监督，直接指导视觉投影器生成初始视觉嵌入。具体指导从两个方面进行：一是通过减小初始嵌入和监督嵌入在语义空间中的角度来优化嵌入方向；二是通过最小化两种视觉嵌入的logit分布差异来改善语义匹配。

Result: BASIC方法在无需额外监督模型或人工标注的情况下，显著提高了MLLMs在广泛基准测试上的性能。

Conclusion: 本文引入的直接视觉监督方法（BASIC）有效提升了MLLM的性能，并成功解决了现有方法中视觉嵌入精细对齐不足的问题，证明了直接视觉监督的有效性。

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [105] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 对基于深度学习的中文书法生成方法进行全面的综述，并探讨其挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 中文字体生成是热门课题，但提升生成质量仍是难题。本研究旨在系统梳理现有技术进展与瓶颈。

Method: 采用综述方法，首先阐述研究背景与综述方法，回顾深度学习架构、字体表示等基础；然后根据所需参考样本数量将现有方法分为多样本生成和少样本生成两类，并详细总结各类代表性方法及其优缺点。

Result: 提供了对现有深度学习中文书法生成方法的全面分类、总结与分析，并识别出当前面临的挑战。

Conclusion: 总结了该领域的挑战与未来发展方向，旨在为研究人员提供有价值的启示。

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [106] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: 本文针对短视频（SV）情感分析（VEA）的挑战，构建了大规模情感数据集eMotions，并提出了端到端音视频融合网络AV-CANet，以有效分析SV中的情感。


<details>
  <summary>Details</summary>
Motivation: 短视频已成为获取和分享信息的重要途径，但其多模态复杂性给视频分析带来了新挑战，特别是在情感分析方面。目前短视频情感数据稀缺，且现有方法难以处理短视频内容多样性、语义鸿沟以及音视频表达不一致导致的局部偏差和信息鸿沟。

Method: 1. 构建了大规模短视频情感数据集eMotions（包含27,996个视频），采用多阶段标注流程和人员分配优化，并提供类别均衡和测试导向的变体以确保数据质量和多样性。
2. 提出了AV-CANet，一个端到端的音视频融合网络，利用视频Transformer捕获语义相关表示。
3. 设计了局部-全局融合模块（Local-Global Fusion Module）以逐步捕捉音视频特征之间的关联。
4. 构建了EP-CE Loss损失函数，通过三极惩罚全局引导优化。

Result: 在三个eMotions相关数据集和四个公共VEA数据集上进行了广泛实验，结果证明了所提出的AV-CANet的有效性。此外，消融研究也验证了方法中关键组件的重要性，并为未来研究提供了广泛见解。

Conclusion: 本研究通过构建大规模高质量的短视频情感数据集和提出创新的AV-CANet音视频融合网络，成功解决了短视频情感分析中的关键挑战，为该领域未来的发展奠定了基础。

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [107] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 针对无训练伪装目标分割（COS）中现有方法生成粗糙语义掩码且难以处理多实例的局限性，本文提出IAPF框架。该框架是首个将任务通用提示转换为细粒度实例掩码的无训练COS流程，通过多模态大语言模型（MLLM）、Grounding DINO和自洽投票策略，显著超越了现有最先进的无训练COS方法。


<details>
  <summary>Details</summary>
Motivation: 伪装目标分割（COS）因目标与周围环境的高度视觉相似性而极具挑战。尽管基于训练的方法表现良好，但其性能随标注稀疏性增加而迅速下降。为解决此限，近期无训练COS方法（利用SAM）兴起，但这些方法通常仅生成语义级视觉提示，导致SAM输出粗糙语义掩码，且无法有效处理多个离散伪装实例的情况。这是当前亟待解决的关键问题。

Method: 本文提出一个名为IAPF（Instance-Aware Prompting Framework）的简单而强大的无训练COS框架，其目标是将任务通用提示显式转换为细粒度实例掩码。IAPF包含三个步骤：1. **文本提示生成器**：利用任务通用查询提示多模态大语言模型（MLLM），生成图像特定的前景和背景标签。2. **实例掩码生成器**：结合Grounding DINO生成精确的实例级边界框提示，并采用提出的“单前景多背景提示策略”在每个框内采样区域受限的点提示，以促使SAM生成候选实例掩码。3. **自洽实例掩码投票**：通过识别在多个候选实例掩码中最具一致性的一个，来选择最终的COS预测。

Result: 在标准COS基准测试上进行的广泛评估表明，所提出的IAPF显著超越了现有最先进的无训练COS方法。

Conclusion: IAPF成功克服了现有无训练COS方法在处理多实例和生成细粒度掩码方面的局限性，通过创新的实例感知提示策略，实现了伪装目标分割性能的显著提升，为该领域提供了新的有效解决方案。

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [108] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文引入了MultiRef-bench评估框架和MultiRef数据集，旨在解决现有图像生成模型无法有效处理多视觉参考的问题。实验结果显示，即使是当前最先进的模型也难以整合多源视觉灵感，为未来开发更类人、更灵活的创意工具指明了方向。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成框架主要依赖单一输入源（如文本或单个参考图像），与人类设计师从多个视觉参考中汲取灵感的自然方式不符。现有技术在整合多源视觉信息以进行可控图像生成方面存在不足。

Method: 引入了MultiRef-bench，一个包含990个合成样本和1000个真实世界样本的严格评估框架。开发了数据引擎RefBlend用于生成合成样本，并在此基础上构建了包含38k高质量图像的MultiRef数据集。对三种交错图像-文本模型（OmniGen、ACE、Show-o）和六种代理框架（如ChatDiT、LLM + SD）进行了实验评估。

Result: 实验结果表明，即使是当前最先进的系统也难以有效处理多参考条件生成。表现最佳的模型OmniGen，在合成样本上平均准确率仅为66.6%，在真实世界样本上为79.0%，与“黄金答案”相比仍有显著差距。

Conclusion: 这些发现揭示了当前生成模型在整合多源视觉灵感能力方面的显著不足，为开发更灵活、更像人类的创意工具指明了有价值的方向。本研究发布的公开数据集也将促进该领域的进一步研究和发展。

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [109] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 行人重识别面临多模态泛化挑战。本文提出MMReID-Bench，首个多任务多模态行人重识别基准，旨在全面评估并利用MLLMs的潜力，结果表明MLLMs能力显著但对某些模态（如热成像）仍有局限。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别模型缺乏多模态能力，导致泛化性差。现有多模态大语言模型（MLLMs）应用仅限于特征提取或字幕生成，未能充分发挥其推理、指令遵循和跨模态理解能力。

Method: 引入MMReID-Bench，这是首个专为行人重识别设计的多任务多模态基准，包含20,710个多模态查询和图库图像，涵盖10种不同行人重识别任务。通过该基准进行全面实验以评估MLLMs的性能。

Result: 实验证明MLLMs在提供有效和通用行人重识别方面表现出卓越能力。然而，它们在处理少数模态（特别是热成像和红外数据）时仍存在局限。

Conclusion: 希望MMReID-Bench能促进社区开发出更鲁棒、更通用的多模态行人重识别基础模型。

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [110] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: 针对多轮对话场景中的交互式图像生成和编辑，本文提出了一种新颖的多智能体系统Talk2Image，通过多智能体协作和反馈机制，显著提升了生成的可控性、连贯性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成系统主要关注单轮任务，难以应对迭代、多轮的创意任务，即使是基于对话的系统也常因单智能体顺序范式导致意图漂移和编辑不连贯。

Method: 提出多智能体系统Talk2Image，该方法整合了三个关键组件：从对话历史中解析用户意图、通过专业化智能体进行任务分解和协作执行、以及基于多视图评估机制的反馈驱动式优化。

Result: 实验结果表明，Talk2Image在迭代图像生成和编辑任务中，其可控性、连贯性和用户满意度均优于现有基线系统。

Conclusion: Talk2Image通过实现与用户意图的逐步对齐和图像的持续编辑，有效解决了多轮对话场景中图像生成和编辑的局限性。

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [111] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: 受LLM中RL成功的启发，本文提出AR-GRPO，一种将在线RL训练集成到自回归图像生成模型的方法，通过GRPO和多维奖励函数显著提升了生成图像的质量和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 受强化学习在优化大型语言模型（LLMs）方面成功的启发，探索将在线强化学习应用于自回归（AR）图像生成模型。

Method: 提出AR-GRPO框架，通过改编Group Relative Policy Optimization (GRPO)算法，并设计评估感知质量、真实感和语义保真度等多维度奖励函数，来优化自回归模型的输出。

Result: 在类条件和文本条件图像生成任务上，与标准AR基线相比，AR-GRPO显著提升了生成图像的质量和人类偏好，并在各种评估指标上显示出一致的改进。

Conclusion: 强化学习优化的方法对于自回归图像生成是可行的，为可控和高质量图像合成开辟了新途径。

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文提出一个CUDA加速的计算框架，利用高效内存管理和CFR算法，解决了复杂纸牌游戏Pasur的纳什均衡问题，并通过大规模自博弈评估牌组价值。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏规则复杂且博弈树庞大，给计算其近纳什均衡带来了独特挑战，现有方法难以高效处理。

Method: 开发了一个CUDA加速的计算框架，强调高效内存管理。采用反事实后悔最小化（CFR）算法来求解近纳什均衡。通过PyTorch CUDA张量处理规则复杂度。将博弈树分解为“实际游戏状态”和“前几轮继承得分”两部分，通过展开过程构建完整博弈树以减少内存开销。使用逐轮向后训练策略，从最终轮开始递归传播平均效用。在计算出近纳什均衡策略后，训练一个树状模型预测这些策略，并通过大规模自博弈（例如每场对决模拟10,000次，并使用GPU并行加速）评估每副牌的公平价值。

Result: 成功构建了平均节点数超过$10^9$的完整游戏树，并计算出了Pasur的近纳什均衡策略。实现了在GPU加速下高效的策略预测和牌组价值评估。

Conclusion: 该框架提供了一种高效解决复杂不完全信息游戏（如Pasur）纳什均衡的方案。类似框架可扩展应用于其他行动树自然分解为多轮的强化学习算法，例如回合制策略游戏或金融市场中的序列交易决策。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [113] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个AI框架，旨在通过自动化观察分析、新颖性评估和理论模拟，在材料研究中促进意外发现，弥补了自动化实验在探索性研究方面的不足。


<details>
  <summary>Details</summary>
Motivation: 科学史充满了意外发现，但现代自动化实验室虽然擅长加速假设检验，却可能因效率优化而忽视这些重要的、计划外的发现，从而导致科学探索的局限性。

Method: 引入SciLink，一个开源、多智能体AI框架。该框架采用混合AI策略，结合专业机器学习模型进行实验数据定量分析和大型语言模型进行高级推理。它能够自主将材料表征的原始数据转化为可证伪的科学主张，并根据已发表文献对其新颖性进行量化评分，从而实现实验观察、新颖性评估和理论模拟之间的直接自动化连接。

Result: 展示了SciLink在多种研究场景中的多功能性，包括处理原子分辨率和高光谱数据、整合实时人类专家指导，以及通过提出有针对性的后续实验来完成研究闭环。

Conclusion: SciLink通过系统分析和语境化所有观察结果，为AI驱动的材料研究提供了一个实用框架，不仅提高了效率，还积极培养了有利于意外发现的环境，从而弥合了自动化实验与开放式科学探索之间的鸿沟。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [114] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文提出IRL-VLA，一种结合逆强化学习（IRL）奖励世界模型的闭环视觉-语言-动作（VLA）强化学习框架，旨在克服现有自动驾驶VLA模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型多采用开环模仿学习，导致性能次优且受限；同时，闭环训练依赖高保真传感器仿真，存在域差距和计算效率低的问题。

Method: IRL-VLA框架分为三阶段：1. 通过模仿学习预训练VLA策略。2. 构建轻量级逆强化学习奖励世界模型，实现高效闭环奖励计算。3. 利用奖励世界模型引导的PPO强化学习，优化驾驶行为，平衡安全性、舒适性和交通效率。

Result: 该方法在NAVSIM v2端到端驾驶基准测试中取得了最先进（SOTA）性能，并在CVPR2025自动驾驶挑战赛中获得亚军。

Conclusion: 所提出的IRL-VLA框架有望加速闭环自动驾驶领域VLA研究的进展。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [115] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在物体计数方面存在显著缺陷，尤其在复杂场景中。本文引入了CountQA基准来评估这一能力，并发现现有顶级MLLMs在此任务上的表现远低于预期。


<details>
  <summary>Details</summary>
Motivation: MLLMs虽然擅长理解视觉场景，但在物体计数这一基本认知技能上严重不足，这限制了其在现实应用中的可靠性。现有评估基准未能有效测试模型在复杂、真实场景下的计数能力，存在评估空白。

Method: 引入了CountQA，一个包含1500多对问答的新挑战性基准，其特点是使用具有高物体密度、杂乱和遮挡的真实世界图像。通过在CountQA基准上评估15个主流MLLMs来揭示它们的计数弱点。

Result: 评估结果显示，即使是性能最佳的MLLM在CountQA上的准确率也仅为42.9%，且随着物体数量的增加，其性能显著下降。

Conclusion: CountQA提供了一个专门的基准，用于诊断和纠正MLLMs在数值理解和空间感知方面的核心弱点。它旨在推动新一代MLLMs的发展，使其不仅在描述上流畅，而且在数值上更加准确可靠。数据集和代码将开源以促进后续研究。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [116] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文旨在阐明形式概念分析（FCA）对变异性分析的关键特性及其应用，以弥补其数学基础与实际应用间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 形式概念分析（FCA）在知识表示和变异性分析方面潜力巨大，但由于其深奥的数学性质，如何有效利用其特性进行变异性相关任务并不直观。

Method: 本文通过收集对变异性分析至关重要的FCA特性，并解释如何利用这些特性来解读所生成概念结构中的多样化变异性信息。

Result: 抽象中未提供具体的研究结果，但论文的成果在于汇集并阐述了FCA中对变异性分析至关重要的属性及其解读变异性信息的方法。

Conclusion: 通过系统地阐述FCA的特定属性及其在变异性分析中的应用，论文为理解和利用FCA进行变异性相关任务提供了清晰的指导，从而弥补了理论与实践之间的差距。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [117] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 该论文提出了一种零样本蜂窝轨迹地图匹配（CTMM）方法，通过像素级轨迹校准、结合高斯混合模型（GMM）的变分自编码器（VAE）进行知识共享、时空感知模块处理定位误差以及约束路径查找算法，显著提高了在未探索区域的匹配精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有蜂窝轨迹地图匹配（CTMM）方法依赖于基于ID和区域特定的数据，导致其在未探索区域适应性差。为了实现高精度的零样本CTMM，需要提取区域自适应特征，并考虑序列信息和位置不确定性以缓解蜂窝数据中的定位误差。

Method: 本文提出了一种像素级轨迹校准助手，利用可迁移的地理空间知识校准像素化轨迹，并指导道路网络层面的路径查找。具体方法包括：1) 将高斯混合模型整合到VAE中，通过软聚类识别场景自适应专家，以增强类似区域的知识共享；2) 设计一个时空感知模块以捕捉序列特征和位置不确定性，从而推断近似用户位置，缓解高定位误差；3) 采用约束路径查找算法重建道路ID序列，确保道路网络的拓扑有效性，并优化最短可行路径。

Result: 广泛的实验表明，与现有方法相比，所提出的模型在零样本CTMM方面的性能提高了16.8%。

Conclusion: 该研究成功解决了零样本CTMM的挑战，通过创新的像素级校准和多模块集成，实现了在不进行额外训练的情况下，在未探索区域的高精度轨迹匹配，展现出卓越的泛化能力和鲁棒性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [118] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 本文提出一种利用概率电路和“规则上下文”的方法，大幅减少知识图谱补全中基于规则方法的规则数量，同时保持或提升性能，并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 基于规则的知识图谱补全方法虽然可解释，但为达到竞争力能，通常需要大量规则，这反而会因为规则集过大而损害其可解释性。

Method: 从训练数据中发现“规则上下文”（协同工作的规则子集），并利用在这些规则上下文上学习到的概率分布（即概率电路）进行推理。该框架基于概率逻辑，不要求独立性假设，且其可处理的推理过程能提供近似下界和精确的查询概率。

Result: 该方法在所用规则数量上实现了70-96%的削减，在使用同等最少规则时，性能超越基线达31倍。即使将本文的最少规则集与基线的完整规则集相比，也能保持91%的峰值基线性能。在8个标准基准数据集上的实证研究证实了其有效性，仅使用AnyBURL（当前基于规则知识图谱补全的SOTA）标准推理方法所需规则的一小部分，即达到了有竞争力的性能。

Conclusion: 该研究成功地减少了基于规则的知识图谱补全所需的规则数量，同时保持了高性能和可解释性，并为泛化到学习规则集上的概率推理提供了进一步的启示。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [119] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种新的可微分规则学习方法，通过支持更复杂的规则结构（如分支和循环）来解决现有链式规则的局限性，从而在知识图谱任务上实现了卓越的性能，并能提取可解释的规则。


<details>
  <summary>Details</summary>
Motivation: 现有可微分归纳逻辑编程（ILP）技术在知识图谱链接预测和节点分类中，普遍假设链式规则结构，这限制了其性能和可解释性。

Method: 本文提出GLIDR，一种可微分规则学习方法，利用可微分消息传递推理算法来建模具有更丰富语法的逻辑规则（包括分支和循环）。其规则搜索空间通过最大自由变量数进行参数化，并能从模型权重中提取显式逻辑规则。

Result: GLIDR在知识图谱补全任务上显著优于现有规则学习方法，并能与嵌入方法竞争。从GLIDR提取的规则保持了显著的预测性能，且GLIDR对训练数据噪声高度鲁棒。此外，GLIDR可与深度神经网络结合，实现端到端优化。

Conclusion: GLIDR通过支持更具表现力的规则结构，显著提升了知识图谱规则学习的性能和鲁棒性，并能提供可解释的规则。其与深度学习的结合扩展了其在多种数据模态上的应用潜力。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [120] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文提出ParBalans，一个利用求解器和算法层面并行化的Balans扩展，旨在加速求解混合整数规划(MIP)问题，并在性能上与商业求解器Gurobi竞争。


<details>
  <summary>Details</summary>
Motivation: 混合整数规划(MIP)问题计算资源需求大，并行化是加速求解和提高可扩展性的关键策略。已有的Balans求解器虽具有模块化架构，理论上支持并行探索，但其并行化潜力尚未被充分研究和利用。

Method: 引入ParBalans，它是Balans的扩展，通过利用求解器层面（solver-level）和算法层面（algorithmic-level）的双重并行化来提升在复杂MIP实例上的性能。

Result: 实验结果表明，ParBalans在解决挑战性MIP实例时表现出有竞争力的性能，特别是在难度优化基准上，其表现可与最先进的商业求解器Gurobi相媲美。

Conclusion: ParBalans成功地将并行化能力融入Balans，显著提高了其MIP求解效率和应对复杂问题的能力，证明了其在实际应用中与顶级商业求解器相当的有效性。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [121] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 本文提出一种结合GDPO和Stackelberg博弈的自组织无人机网络框架，旨在解决敏感应用中无人机网络面临的动态移动性和暴露风险，以实现可靠连接和隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用（如城市监控、应急响应、安全感知）中的需求增长，确保可靠连接和隐蔽通信变得至关重要，但动态移动性和暴露风险带来了严峻挑战。

Method: 提出一个自组织无人机网络框架。该框架结合了基于图扩散的策略优化（GDPO）和基于Stackelberg博弈（SG）的激励机制。GDPO利用生成式AI动态生成稀疏但连接良好的拓扑结构，以适应节点分布和地面用户（GU）需求；SG激励机制则引导自利无人机选择支持合作并增强隐蔽通信的中继行为和邻居链路。

Result: 通过大量实验验证了所提框架在模型收敛性、拓扑生成质量和隐蔽通信性能增强方面的有效性。

Conclusion: 该框架能够有效提升无人机网络在动态且高风险环境中的可靠连接和隐蔽通信能力。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [122] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 本文通过为现代CPU设计和实现优化的1/2比特微核，并将其集成到PyTorch-TPP框架中，显著提升了超低比特LLM模型的推理效率，最高达到现有SOTA运行时的2.2倍和16比特模型的7倍。


<details>
  <summary>Details</summary>
Motivation: 尽管超低比特LLM模型为资源受限环境下的推理提供了潜力，但现有最先进的推理运行时（如bitnet.cpp）在部署这些模型时的计算效率尚未得到充分优化。

Method: 研究采用自下而上的方法，首先设计并实现了针对现代CPU优化的1比特和2比特微核，以实现峰值计算效率；然后将这些微核集成到PyTorch-TPP这一LLM推理框架中。

Result: 通过2比特模型进行端到端推理，性能比当前最先进的运行时bitnet.cpp提高了高达2.2倍，并且与16比特模型推理相比，速度提升高达7倍。

Conclusion: 本研究的优化运行时提升了AI PC和边缘设备上LLM推理的水平，为超低比特LLM模型的有效部署奠定了基础。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [123] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 引入一个模块化提示框架，基于近侧发展区（ZPD）理论，使大型语言模型（LLMs）在用户中心任务中实现更安全、更自适应的行为，无需微调即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决LLMs在动态、用户中心任务中安全和自适应应用的问题，使其行为能够根据用户状态进行调整，同时无需进行微调或外部协调，并提供可解释、目标对齐的行为。

Method: 提出一个模块化提示框架，该框架结合了自然语言边界提示和一个编码有模糊脚手架逻辑和自适应规则的控制模式，其理论基础是人类学习理论中的近侧发展区（ZPD）。

Result: 在模拟智能辅导场景中，该框架显著提升了多个模型的脚手架质量、自适应性和教学一致性，表现优于标准提示基线。评估采用基于评分标准的LLM评分器。此外，该框架在游戏程序内容生成等其他交互密集型领域也显示出潜力。

Conclusion: 该框架提供了一种可重用的方法论，用于在不确定或演变的环境中构建可解释、目标对齐的LLM行为，设计上注重安全部署，并具有超越教育领域的广泛应用前景。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [124] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 该研究提出一个基于自然语言和强化学习的框架，自动化体数据视点选择，以提高探索效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索体数据至关重要，但选择最佳视点对非专业用户而言极具挑战性。

Method: 提出一个利用自然语言交互的框架，通过编码体数据块并结合CLIP Score提供语义信息。该框架采用强化学习，利用语义线索高效搜索和识别用户意图的视点，并用CLIP Score评估选定视点。

Result: 该方法自动化了视点选择，提升了体数据导航效率，并增强了复杂科学现象的可解释性。

Conclusion: 该方法显著改善了体数据导航的效率和复杂科学现象的解释性，降低了用户探索难度。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [125] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 该综述提出一种语言中心化的遥感图像解译新范式，借鉴人类认知全局工作空间理论，将大语言模型（LLMs）作为认知中心枢纽，旨在克服传统视觉中心模型的局限性，并探讨了相关技术挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 主流的遥感图像解译范式以视觉中心模型为主，但在处理多模态推理、语义抽象和交互式决策方面存在固有限制。尽管大语言模型已应用于遥感，但现有研究缺乏解释语言认知作用的统一理论框架。

Method: 倡导从视觉中心向语言中心的遥感解译范式转变。提出一个以LLMs为认知中心枢纽的语言中心框架，该框架整合感知、任务、知识和行动空间，受人类认知全局工作空间理论启发。文章探讨了LLMs作为核心认知组件的潜力，总结了统一多模态表示、知识关联、推理与决策等技术挑战，并构建了全局工作空间驱动的解译机制来应对这些挑战。

Result: 该工作提出了一个统一的语言中心化遥感解译框架，总结了该领域面临的核心技术挑战，并回顾了语言中心化解决方案如何应对这些挑战。它为未来的遥感解译系统提供了概念基础。

Conclusion: 本研究旨在为下一代遥感解译系统提供概念基础，并为认知驱动的智能地理空间分析建立路线图。提出了四个未来研究方向：多模态数据自适应对齐、动态知识约束下的任务理解、可信推理和自主交互。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [126] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 合作多智能体强化学习中，引入MACA方法通过多级优势函数和注意力机制解决信用分配难题，有效识别和评估不同协作层级的贡献。


<details>
  <summary>Details</summary>
Motivation: 在合作多智能体强化学习中，准确评估每个智能体对共享奖励的贡献（信用分配）是一个关键挑战，尤其当协作类型多样且奖励归因于不同或重叠的智能体子集时。

Method: 提出MACA（Multi-level Advantage Credit Assignment）方法。该方法通过形式化信用分配层级（协作智能体数量），引入多级优势函数进行明确的反事实推理，以推断不同层级的贡献。MACA整合了对个体、联合和关联行为的优势函数，并利用注意力机制识别关联智能体关系。

Result: 在具有挑战性的Starcraft v1和v2任务上的综合实验表明，MACA表现优异。

Conclusion: MACA在复杂信用分配场景中表现出卓越的有效性，提升了合作多智能体学习的性能。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [127] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 本文提出一种隐蔽语义通信框架，服务器向用户传输图像语义信息，并部署干扰器对抗窃听者。针对服务器无法感知干扰器功率的问题，提出了一种基于优先采样双延迟深度确定性策略梯度（DDPG）算法，用于联合优化语义信息传输和功率分配，以最大化用户隐私和传输质量。仿真结果显示该算法在隐私和传输质量方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在隐蔽语义通信中，服务器向用户传输图像语义信息时面临窃听威胁。为确保传输的隐蔽性和用户隐私，同时克服服务器无法感知友好干扰器功率的挑战，亟需一种优化策略来协同最大化隐私和语义信息传输质量。

Method: 提出了一种“优先采样辅助双延迟深度确定性策略梯度（prioritised sampling assisted twin delayed DDPG）”算法。该算法在服务器与干扰器无通信的情况下，联合决定每个时隙的语义信息传输内容和相应的发射功率。相较于标准强化学习方法，该算法通过引入额外的Q网络，避免了局部最优动作选择和Q值估计偏差。

Result: 仿真结果表明，与传统强化学习方法相比，所提出的算法能够将用户隐私提高高达77.8%，并将语义信息传输质量提高高达14.3%。

Conclusion: 所提出的基于优先采样双延迟深度确定性策略梯度的算法，有效解决了服务器无法感知干扰器功率下的隐蔽语义通信问题，显著提升了用户隐私和语义信息传输质量。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [128] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文针对现有MLLM评估基准的局限性，提出了MDK12-Bench，一个大规模多学科基准，并设计了动态评估框架。研究发现当前MLLMs在多方面存在不足，并为模型改进和AI教育提供了方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）对推进通用人工智能（AGI）至关重要。然而，现有用于衡量MLLMs智能的基准存在规模有限、覆盖范围狭窄和知识结构化不足等问题，仅能提供静态和非差异化的评估，无法全面捕捉MLLMs的真实智能水平。

Method: 引入了MDK12-Bench，一个基于K-12真实考试构建的大规模多学科基准，包含14.1万个实例和6225个知识点（组织在六层分类法中）。该基准涵盖五种题型，并具有难度和年份标注，能够从难度、跨年份、上下文和知识驱动推理四个维度全面评估MLLMs。此外，提出了一种新颖的动态评估框架，引入不熟悉的视觉、文本和问题形式变化，以挑战模型的泛化能力并减轻数据污染。还评估了知识点参考增强生成（KP-RAG）以探讨知识在解决问题中的作用。

Result: 关键发现揭示了当前MLLMs在多个方面存在的局限性。

Conclusion: 研究结果为增强模型的鲁棒性、可解释性以及促进人工智能辅助教育提供了指导。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [129] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 本文提出MP-Bench，一个用于恶劣天气事件预测的大规模时序多模态数据集，并在此基础上开发了MMLM（气象多模态大模型），旨在解决AI驱动的恶劣天气预警中存在的样本稀缺、数据与文本对齐不佳以及现有模型无法处理高维气象数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的恶劣天气预报系统严重依赖人工专家判读，导致主观性强和操作负担重。尽管AI气象站作为新趋势正在兴起，但其发展面临三大核心挑战：1) 恶劣天气事件样本稀缺；2) 高维气象数据与文本预警的对齐不完善；3) 现有多模态语言模型无法处理高维气象数据，且难以充分捕捉时间序列、垂直气压层和空间维度间的复杂依赖关系。

Method: 为解决上述挑战，研究者引入了MP-Bench数据集，这是首个用于恶劣天气事件预测的大规模时序多模态数据集，包含421,363对原始多年气象数据及其对应的文本描述，覆盖中国广泛的恶劣天气情景。基于此数据集，开发了气象多模态大模型（MMLM），该模型能直接处理4D气象输入，并针对4D气象数据流的独特特性，集成了三个即插即用的自适应融合模块，以实现跨时间序列、垂直气压层和空间维度的动态特征提取和整合。

Result: 在MP-Bench数据集上进行的广泛实验表明，MMLM在多项任务中表现出色，突显了其在恶劣天气理解方面的有效性。

Conclusion: MMLM的成功开发和验证标志着实现自动化、AI驱动的天气预报系统的关键一步。研究者将公开发布源代码和数据集。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [130] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了下推奖励机（pdRMs），作为奖励机（RMs）的扩展，能够识别和奖励确定性上下文无关语言所表示的更复杂、更具时间扩展性的行为，并进行了理论分析与实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机（RMs）只能处理正则语言表示的行为，对于更复杂的时间扩展行为，需要更具表达能力的奖励函数机制。

Method: 引入基于确定性下推自动机的下推奖励机（pdRMs）。提出了两种基于pdRM的策略变体：一种可访问整个栈，另一种仅访问栈顶k个符号。提出了一种检查两种策略何时能达到相同最优预期奖励的程序。

Result: 理论上，建立了pdRMs的表达能力和学习问题的空间复杂度。实验上，展示了代理如何通过pdRMs训练以执行确定性上下文无关语言表示的任务。

Conclusion: pdRMs显著增强了奖励函数的表达能力，使得强化学习代理能够学习并执行比正则语言更复杂的、时间上扩展的行为，从而克服了现有奖励机的局限性。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [131] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一种新的分布式引导局部搜索(DGLS)框架，解决了DCOP局部搜索算法易陷于局部最优的问题，并在多种基准测试中表现出显著优越性。


<details>
  <summary>Details</summary>
Motivation: 分布式约束优化问题(DCOP)的局部搜索算法常收敛于较差的局部最优。现有方法GDBA效果不佳，原因在于过激的约束违反条件、无限制的惩罚累积和不协调的惩罚更新。

Method: 提出分布式引导局部搜索(DGLS)框架。该方法引入自适应违反条件以选择性惩罚高成本约束、惩罚蒸发机制以控制惩罚量级、以及同步机制以协调惩罚更新。理论上证明惩罚值有界且智能体在DGLS中进行势博弈。

Result: DGLS在各种标准基准测试中，相较于现有最先进基线表现出显著优越性。尤其在结构化问题上，DGLS在任一时间结果上均显著优于Damped Max-sum（优势幅度达3.77%-66.3%），在一般值问题上表现也具竞争力。

Conclusion: DGLS通过改进引导局部搜索机制有效解决了DCOP中局部搜索的缺陷，提供了一种性能优越且理论基础稳固的新方法。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [132] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF是一个概念驱动的检索增强数学形式化框架，通过检索数学概念的正式定义，显著提高了LLM驱动的自动形式化准确性，解决了幻觉和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器（ITPs）需要大量人工形式化工作和专业知识。自动化形式化面临模型幻觉（如未定义谓词、符号误用）和自然语言描述中模糊或缺失前提导致的语义鸿沟两大挑战。

Method: 提出了CRAMF框架，通过检索核心数学概念的正式定义，为LLM驱动的自动形式化提供上下文基础。为解决检索增强生成（RAG）在此领域的挑战，构建了Mathlib4中的概念-定义知识库，并针对概念多态性提出了上下文查询增强。此外，设计了一种带有重排功能的双通道混合检索策略。

Result: CRAMF能无缝集成到基于LLM的自动形式化工具中，在miniF2F、ProofNet和AdvancedMath基准测试中，持续提高翻译准确性，最高相对改进达62.1%，平均相对改进达29.9%。

Conclusion: CRAMF框架通过有效地处理幻觉和语义鸿沟，显著提升了LLM驱动的数学形式化能力，为自动化形式化提供了一条有前景的路径。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [133] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: Transformer模型在亚田块级作物产量预测中表现优异，并利用其自解释性（如Attention Rollout）提供可靠的特征和模态归因，解决了多模态学习模型可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在农业等领域中具有广泛应用潜力，但由于数据异构性，构建复杂模型后其可解释性常被忽视。本研究旨在解决多模态学习网络的可解释性问题，特别是在作物产量预测任务中。

Method: 利用Transformer模型的内在可解释性来解释多模态学习网络，专注于亚田块级作物产量预测。使用包含多光谱卫星、天气时间序列、地形高程图和土壤属性四种模态的大型数据集。基于自注意力机制，采用Attention Rollout (AR) 和 Generic Attention (GA) 估计特征归因，并与Shapley Value Sampling (SVS) 进行评估。此外，提出Weighted Modality Activation (WMA) 方法评估模态归因，并与SVS进行比较。解释结果结合作物物候期信息进行。

Result: Transformer模型表现优于卷积和循环网络，在亚田块和田块级别R2分数分别高出0.10和0.04。Attention Rollout (AR) 相比GA和SVS值，提供了更稳健可靠的时间归因，并通过定性和定量评估得到证实。模态归因在不同方法间显示出不同的模式。

Conclusion: 本研究成功将Transformer模型应用于多模态作物产量预测任务，实现了卓越的预测性能，并利用其内在可解释性提供了可靠的特征和模态归因。Attention Rollout方法在时间归因方面表现出色，为理解农业中多模态数据的影响提供了宝贵见解，有助于解决多模态学习中可解释性缺失的问题。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [134] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 本文警告反对LLMs模拟人类心理并取代心理学研究中人类参与者的观点。


<details>
  <summary>Details</summary>
Motivation: 鉴于有研究提出大型语言模型（LLMs）可能模拟人类心理并取代心理学研究中的人类参与者，本文旨在反驳此观点，并强调其潜在风险。

Method: 研究结合概念论证和实证证据。实证部分通过展示LLMs（包括为心理学响应微调的CENTAUR模型）对细微措辞变化的响应与人类存在显著差异，以及不同LLMs对新奇条目响应的高度不一致性来支持其论点。

Result: 结果表明，LLMs对语言细微变化的敏感度与人类不同步，并且它们对新奇问题的响应缺乏可靠性与一致性。这有力地证明了LLMs无法模拟人类心理。

Conclusion: LLMs不能模拟人类心理。心理学研究人员应将LLMs视为有用但根本上不可靠的工具，在每次新应用中都必须与人类响应进行验证。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [135] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文提出了DatasetResearch基准，用于评估AI代理发现和合成数据集的能力，揭示了当前AI在按需数据发现方面的巨大差距，并为未来自主数据发现系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的进步，AI开发的瓶颈已从计算能力转向数据可用性。大量有价值的数据集分散隐藏，亟需解决AI代理能否超越传统搜索，系统性地发现满足用户需求的任意数据集，实现真正的按需数据管理。

Method: 研究引入了DatasetResearch，这是首个全面评估AI代理从208个知识密集型和推理密集型真实世界需求中发现和合成数据集能力的基准。采用三维评估框架进行评估。

Result: 评估结果显示，即使是先进的深度研究系统在DatasetResearch-pro子集上得分也仅为22%，暴露了当前能力与完美数据集发现之间的巨大差距。分析揭示了根本性差异：搜索代理擅长通过检索广度处理知识任务，而合成代理通过结构化生成主导推理挑战；但两者在现有分布之外的“边缘案例”上都遭遇灾难性失败。

Conclusion: 本研究为数据集发现代理建立了首个严格的基线，并指明了实现AI系统能够在数字宇宙中找到任何数据集的途径。所提供的基准和全面分析为下一代自我改进型AI系统奠定了基础。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [136] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: 本文提出了MASteer，一个基于表示工程的端到端框架，通过自动化样本生成和自适应策略，有效且高效地修复大型语言模型（LLMs）的信任度问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在持续的信任度问题。现有修复方法（如SFT、RLHF）成本高、耗时长，提示工程缺乏鲁棒性。表示工程虽有潜力，但当前方法依赖手动样本和固定策略，限制了自动化和适应性。

Method: 本文提出了MASteer，首个基于表示工程的LLM信任度端到端修复框架。MASteer包含两大核心组件：AutoTester（一个多智能体系统，用于生成多样化、高质量的引导样本）和AutoRepairer（构建自适应引导策略，在推理时通过锚向量实现自动、上下文感知的策略选择）。

Result: 实验结果表明，MASteer在标准和定制的信任度任务上均优于基线方法，在LLaMA-3.1-8B-Chat上将指标提高了15.36%，在Qwen-3-8B-Chat上提高了4.21%，同时保持了模型通用能力。它还展现出强大的鲁棒性和泛化能力。

Conclusion: MASteer为LLMs的信任度修复提供了一种可扩展、高效的解决方案，具有显著的实用价值，并展示出强大的鲁棒性和泛化能力。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [137] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个模块化分布式机器学习推理框架，通过对子计算进行策略性加密验证（“分片”），避免了全模型电路化的成本和僵化，从而实现可扩展且目标式的信任最小化。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式零知识机器学习范式中，全模型电路化成本高昂且缺乏灵活性。

Method: DSperse通过对策略性选择的子计算（“分片”）进行目标式验证，以避免全模型电路化。它支持灵活的证明边界，并可通过审计、复制或经济激励来强制执行全局一致性，将零知识证明定位到最有价值的组件。

Result: 作者使用多种证明系统对DSperse进行了评估，并报告了在分片和未分片配置下的内存使用、运行时和电路行为的实证结果。

Conclusion: DSperse通过允许证明边界与模型的逻辑结构灵活对齐，支持可扩展、目标式的验证策略，适用于多样化的部署需求，并实现了实用形式的信任最小化。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [138] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 本研究提出了一个基于主动推理的框架，模拟并展示了具身智能体（模拟生物神经网络）在决策过程中的学习能力，为可解释AI提供生物学基础方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI的快速发展，理解自主智能体目的性行为的基础对开发安全高效的系统至关重要。尽管人工神经网络占据主导地位，但基于生物的系统（如活体生物神经元网络）在能效和数据效率方面有前景，并有望提供更可解释和生物学上合理的模型。

Method: 本研究提出一个植根于主动推理（一种通用行为理论）的框架，用于模拟具身智能体的决策过程。利用受实验启发的生成模型，在模拟游戏环境中模拟决策过程，以模仿生物神经元实验设置。

Result: 研究结果表明这些智能体展现出学习能力，并为记忆学习和预测规划在智能决策中的作用提供了见解。

Conclusion: 这项工作为可解释人工智能领域做出了贡献，提供了一种生物学基础且可扩展的方法来理解智能体目的性行为。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [139] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文探索了隐式命中集(IHS)框架中命中集优化的替代算法，发现基于伪布尔(PB)推理的精确计算能与传统IP求解器媲美，并能提供正确性证明，解决了IP可能存在的数值不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 隐式命中集(IHS)方法中，命中集(HS)优化器通常采用整数规划(IP)实现。然而，IP求解器可能因数值不稳定性而导致计算正确性问题。因此，有必要探索并评估更可靠、能确保正确性的HS优化替代技术。

Method: 研究探索了基于伪布尔(PB)推理和随机局部搜索的命中集(HS)优化算法。通过在伪布尔(0-1 IP)优化背景下进行广泛评估，分析了这些替代方法的实际可行性。

Result: 商业IP求解器在HS计算中仍最有效，但可能存在数值不稳定性导致的正确性问题。基于伪布尔(PB)推理的精确HS计算能够与数值精确的IP求解器竞争。此外，PB推理能为IHS计算提供正确性证明。

Conclusion: 尽管商业IP求解器效率较高，但基于伪布尔(PB)推理的命中集(HS)计算提供了一种有竞争力的替代方案，它不仅能在性能上与IP求解器匹敌，还能通过提供正确性证明来解决其潜在的数值不稳定性问题，从而提高了IHS计算的可靠性。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [140] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: 提出了MultiMedEdit，首个用于评估多模态医疗知识编辑的基准，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑（KE）研究在多模态医疗场景中关注不足；与纯文本设置不同，医疗领域中的KE需要结合视觉推理来支持安全和可解释的临床决策。

Method: 提出MultiMedEdit，一个专门用于评估临床多模态任务中知识编辑的基准；该框架涵盖理解和推理任务，并定义了可靠性、通用性和局部性三维指标；在单次编辑和终身编辑设置下进行了广泛实验，并进行了效率分析。

Result: 结果表明，当前方法在泛化能力和长尾推理方面存在困难，尤其是在复杂的临床工作流中；效率分析揭示了不同知识编辑范式在实际部署中的权衡。

Conclusion: MultiMedEdit不仅揭示了当前方法的局限性，也为未来开发临床稳健的知识编辑技术奠定了坚实基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [141] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst是一个分层多智能体系统，通过结合规划与验证执行，显著提升了生物信息学分析的自动化水平，超越了现有大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析的复杂性导致数据生成与科学洞察之间存在巨大鸿沟。尽管大型语言模型（LLMs）在科学推理方面展现潜力，但它们在需要迭代计算、工具集成和严格验证的真实分析工作流中仍存在根本性限制。

Method: 引入K-Dense Analyst，一个分层多智能体系统，通过双循环架构实现自主生物信息学分析。该系统利用专业代理将复杂目标分解为可执行、可验证的任务，并在安全计算环境中结合规划与验证执行。

Result: 在BixBench基准测试中，K-Dense Analyst实现了29.2%的准确率，比表现最佳的语言模型（GPT-5）高出6.3个百分点（提升近27%）。值得注意的是，K-Dense Analyst使用直接表现仅为18.3%的Gemini 2.5 Pro达到了这一性能，表明其架构创新极大地超越了底层模型的基线性能。

Conclusion: 自主科学推理不仅需要更强的大型语言模型，更需要专门构建的系统来弥合高级科学目标与低级计算执行之间的差距。这项研究是向实现完全自主计算生物学家迈出的重要一步，有望加速生命科学领域的发现。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [142] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 本研究针对大型语言模型（LLMs）在内容审核中处理复杂道德推理和偏见的局限性，开发了一个实验框架和统一基准数据集。通过QLoRA微调Phi-4模型，提出了SafePhi，其在审核性能上显著优于现有基准模型，并强调了引入多样化数据和人工干预以提升模型鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益融入日常生活，对更安全可靠的审核需求日益增长。尽管大型语言模型（LLMs）能力强大，但其在处理需要细致道德推理（如检测隐性仇恨、冒犯性语言和性别偏见）时仍存在不足，主要原因在于问题的上下文依赖性和主观性，以及训练数据可能无意中强化社会偏见。因此，探索和解决LLMs在内容审核中的这些局限性变得至关重要。

Method: 研究开发了一个基于SOTA模型的实验框架，用于评估人类情感和冒犯行为。该框架引入了一个包含49个独特类别的统一基准数据集，涵盖了人类情感、冒犯性及仇恨文本以及性别和种族偏见。此外，研究提出了SafePhi，一个通过QLoRA技术对Phi-4模型进行微调的版本，使其能适应多样化的道德语境。

Result: SafePhi在内容审核任务中取得了显著的性能提升，Macro F1分数为0.89，优于基准审核模型OpenAI Moderator（0.77）和Llama Guard（0.74）。研究还揭示了LLM审核器持续表现不佳的关键领域。

Conclusion: 为了提高模型在内容审核中的鲁棒性和可解释性，迫切需要结合更多异构和具有代表性的数据，并引入人工干预（human-in-the-loop）机制。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [143] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 本研究提出一个反馈驱动的决策支持系统，通过增量再训练和实时数据反馈，使学生表现预测模型能持续自我改进，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有教育机器学习模型大多是静态的，无法适应新数据（如干预后的学生表现）的出现，这限制了其在学业干预中提供及时准确预测的能力。

Method: 本研究提出了一个反馈驱动的决策支持系统（DSS），采用闭环架构实现模型持续优化。该系统整合了基于LightGBM的回归器和增量再训练机制，允许教育工作者输入更新的学生成绩，从而自动触发模型更新。此外，平台通过基于Flask的网页界面实现实时交互，并整合SHAP以增强模型可解释性。

Result: 实验结果显示，通过增量再训练，系统的RMSE降低了10.7%，并且对受干预学生的预测分数显示出持续的向上调整，验证了其预测准确性的提升。

Conclusion: 该方法将静态预测器转变为自我改进的系统，推动了教育分析向以人为本、数据驱动和响应式AI的方向发展。该框架设计易于集成到学习管理系统（LMS）和机构仪表板中。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [144] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出一个使用大型语言模型（LLM）代理的新颖框架，用于对多维企业数据进行摘要，解决了传统方法在处理复杂业务报告方面的不足，并显著提升了摘要质量。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型缺乏对层次结构和上下文敏感变化的推理能力，而这些在业务报告任务中至关重要。

Method: 引入了一个多代理管道，利用LLM代理进行数据切片、差异检测、上下文构建和摘要生成，以提取、分析和汇总多维数据。

Result: 该框架优于传统方法，实现了83%的数据忠实度，对显著变化的覆盖率更高，决策关键洞察的相关性评分达到4.4/5。在涉及细微权衡的类别中，性能提升尤为显著，超越了竞争方法。

Conclusion: 所提出的基于LLM代理的框架显著提升了多维企业数据摘要的忠实度、相关性和洞察质量，特别适用于传统方法难以处理的复杂业务场景。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [145] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: 提出首个记忆引导AI智能体EndoAgent，用于内窥镜图像诊断，通过双记忆设计和工具集成，在复杂临床任务中表现出色，并引入新基准EndoAgentBench。


<details>
  <summary>Details</summary>
Motivation: 现有内窥镜AI诊断方法缺乏任务间协调性，难以处理复杂临床工作流中的多步骤过程；同时，AI智能体在内窥镜领域的潜力未被充分探索。

Method: 提出EndoAgent，首个用于视觉-决策内窥镜分析的记忆引导智能体。它采用双记忆设计（短期行动跟踪和长期经验学习）以确保决策连贯性并增强推理能力，并集成一套专家设计的工具。同时，构建了EndoAgentBench基准数据集用于评估。

Result: 广泛实验表明，EndoAgent持续优于现有通用和医学多模态模型，展现出强大的灵活性和推理能力。

Conclusion: EndoAgent通过引入记忆引导智能体范式，有效解决了内窥镜AI诊断中现有方法的局限性，显著提升了复杂临床工作流中的决策能力和分析表现。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [146] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 本文将大语言模型（LLM）形式化为概率图灵机，首次从理论上证明了LLM幻觉的不可避免性，并提出了两种“逃逸路径”：通过将RAG建模为预言机实现计算跳跃，以及通过神经网络博弈论实现持续学习。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的幻觉现象是其可靠部署的核心障碍，亟需从理论上解释并寻找解决方案。

Method: 1. 将LLM形式化为概率图灵机，通过构建“计算必要性层次结构”和“学习器泵引理”来分析幻觉。
2. 提出两种“逃逸路径”：a) 将检索增强生成（RAG）建模为预言机，通过“计算跳跃”证明其有效性。b) 将持续学习形式化为“内化预言机”机制，并通过新颖的神经网络博弈论框架实现。

Result: 1. 首次证明了LLM幻觉在对角化、不可计算性及信息论边界上的必然性。
2. 首次为RAG的有效性提供了形式化理论，证明其能够绝对消除幻觉。
3. 提出了通过神经网络博弈论实现持续学习的理论框架。

Conclusion: 该研究不仅深入剖析了LLM幻觉的理论根源与不可避免性，更为克服这些限制提供了创新的理论框架和可行的“逃逸路径”，特别是在RAG和持续学习方面，为LLM的可靠应用奠定了理论基础。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [147] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 针对领域特定LLM基准构建效率问题，本文提出基于全面性-紧凑性原则的Comp-Comp框架，并创建了XUBench基准。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定LLM基准主要依赖缩放定律，但语料库和问答集设计对领域LLM精度和召回率的影响未被充分探索。本文旨在解决这一空白，并质疑缩放定律在特定领域基准构建中的最优性。

Method: 提出Comp-Comp迭代基准测试框架，该框架基于“全面性-紧凑性”原则。其中，全面性确保领域语义召回，紧凑性提升精度，两者共同指导语料库和问答集的构建。

Result: 通过在一个著名大学的案例研究，成功创建了XUBench，一个大规模、全面的封闭领域基准。

Conclusion: Comp-Comp框架虽以学术领域为例，但其设计具有可扩展性，能为跨领域的基准构建提供有价值的见解。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [148] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: 本文提出Pentest-R1框架，旨在解决当前大型语言模型（LLM）在自动化渗透测试中面临的局限。该框架采用两阶段强化学习（离线数据学习基础逻辑，在线CTF环境自适应微调）来优化LLM的推理能力，并在AutoPenBench和Cybench基准测试中取得了显著成果，为开源LLM设定了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在自动化渗透测试中存在显著局限，如错误处理能力差、推理效率低下以及难以自主执行复杂的端到端任务，这阻碍了网络安全能力的提升。

Method: 引入Pentest-R1框架，采用两阶段强化学习管道：1) 离线强化学习：利用包含500多个真实世界多步骤演练的数据集，为LLM灌输基础攻击逻辑。2) 在线强化学习：在交互式夺旗（CTF）环境中对LLM进行微调，使其直接从环境反馈中学习，发展鲁棒的错误自纠正和自适应策略。

Result: 在AutoPenBench基准上，Pentest-R1的成功率为24.2%，超越了大多数现有先进模型，仅次于Gemini 2.5 Flash。在Cybench基准的非指导任务中，成功率达到15.0%，为开源LLM树立了新的SOTA，并与顶级的专有模型表现相当。消融研究证实，两个训练阶段的协同作用对其成功至关重要。

Conclusion: Pentest-R1框架通过其创新的两阶段强化学习方法，有效解决了LLM在自动化渗透测试中的现有挑战，显著提升了性能，并在开源LLM领域设立了新的最先进水平，证明了结合式训练策略的关键作用。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [149] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: Invert4TVG框架通过引入三个反演任务（动词补全、动作识别、视频描述）并结合强化学习，解决了现有TVG方法过度优化IoU而牺牲语义理解的问题，显著提升了视频定位的准确性和动作理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前的时间视频定位（TVG）方法在优化高时间交并比（IoU）时，往往过度拟合该指标，从而损害了视频和查询中的语义动作理解，而这对于鲁棒的TVG至关重要。

Method: 提出Invert4TVG框架，该方法利用现有TVG标注，引入三个反演任务：1) 动词补全，从视频片段预测查询中被遮蔽的动作动词；2) 动作识别，识别查询描述的动作；3) 视频描述，生成明确嵌入查询相关动作的视频片段描述。这些任务通过强化学习框架和精心设计的奖励函数与TVG集成，以平衡定位和语义的优化。

Result: 实验表明，本方法优于现有最先进的方法，在Charades-STA数据集上，3B模型在R1@0.7指标上比Time-R1提升了7.1%。

Conclusion: 通过反演TVG以从视频片段中推导出查询相关的动作，本方法加强了语义理解，显著提高了定位准确性的上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [150] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 本文提出并评估了一个利用生成式AI（GAI）开发大型政府组织战略计划的模块化模型，重点评估了BERTopic和非负矩阵分解（NMF）在从政府报告中提取战略计划愿景要素主题方面的能力。研究发现，这些技术能生成与现有愿景要素高度相似的主题，其中BERTopic表现最佳。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式AI（GAI）和大型语言模型（LLMs）在自动化专业服务方面的突破，本研究旨在探索如何利用GAI辅助大型政府组织开发战略计划，以应对价值数十亿美元的行业需求并帮助联邦政府克服重要的监管要求。

Method: 研究提出了一个利用GAI开发战略计划的模块化模型，并具体评估了BERTopic和非负矩阵分解（NMF）这两种主题建模技术。通过使用大量政府问责局（GAO）报告训练模型，然后将生成的主题与已发布战略计划的愿景要素进行相似性评分并比较结果。

Result: 研究结果显示，这些技术能够生成与100%被评估的战略计划愿景要素相似的主题。其中，BERTopic在此应用中表现最佳，其超过一半的相关主题达到了“中等”或“强”相关性。

Conclusion: 利用GAI开发战略计划的能力对一个价值数十亿美元的产业具有重大影响，并有助于联邦政府克服关键的监管要求。未来的工作将专注于将本研究中验证的概念投入实际运用，并探索所提出模型中其余模块的可行性。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [151] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本综述全面审视了自我演化AI代理技术，提出了一个统一的概念框架，并系统回顾了相关策略、领域特定方法以及评估、安全与伦理考量，旨在促进更具适应性和终身学习能力的AI代理系统开发。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理系统部署后配置固定，难以适应动态环境。为解决这一局限，研究正探索代理演化技术以实现自我演化代理。本综述旨在为研究人员和从业者提供对这些技术的系统性理解。

Method: 本综述首先提出了一个包含系统输入、代理系统、环境和优化器的统一概念框架，以抽象自我演化代理的反馈循环。基于此框架，系统回顾了针对代理系统不同组件的自我演化技术，并调查了生物医学、编程和金融等领域特定的演化策略。此外，还讨论了自我演化代理系统的评估、安全和伦理考量。

Result: 本综述通过提出统一的概念框架，并系统性地回顾现有技术、领域特定策略以及评估、安全和伦理考量，为自我演化AI代理系统提供了全面的分类和理解，揭示了该领域的发展现状和关键要素。

Conclusion: 本综述旨在为研究人员和从业者提供对自我演化AI代理的系统性理解，为开发更具适应性、自主性和终身学习能力的代理系统奠定基础。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [152] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文提出一个将大型语言模型（LLMs）与多智能体决策算法结合的系统框架，通过高级提示工程、记忆架构、多模态处理和微调来增强多智能体协作能力，并在经典博弈论设置中进行评估。


<details>
  <summary>Details</summary>
Motivation: 语言是推理和协作的基础工具，在多智能体互动中至关重要。建立共同语言能促进智能体间清晰沟通、理解和协调，因此，本研究旨在利用大型语言模型（LLMs）提升多智能体决策与策略制定能力。

Method: 提出一个系统性的多智能体大型语言模型（LLMs）设计框架，通过集成多智能体决策算法来扩展LLMs的能力。关键集成实践包括：高级提示工程技术、有效的记忆架构开发、多模态信息处理以及通过微调算法实现的对齐策略。这些设计选择将通过在具有显著社会困境和博弈论考虑的经典游戏设置中进行广泛的消融研究来评估。

Result: 抽象中未提供具体的研究结果，仅阐述了评估方法：通过在具有社会困境和博弈论考虑的经典游戏设置中进行广泛的消融研究来评估所提出的设计选择。

Conclusion: 抽象中未直接提供明确的结论。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [153] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出一种纯代理策略，无需固定流程，将自然语言问题描述翻译为约束模型，成功解决所有CP-Bench基准测试问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题描述转换为形式化约束模型是约束编程中的一个根本性挑战，需要深厚的领域和建模专业知识。现有自动化方法采用固定工作流，在大量基准问题上失败。

Method: 采用纯代理策略，无固定流程。开发了一个基于ReAct（Reason and Act）原则的通用Python编码代理，利用持久的IPython内核进行有状态的代码执行和迭代开发。领域专业知识通过精心设计的项目提示注入，而非嵌入代理架构。代理结合提示知识与文件操作和代码执行工具，动态测试、调试和验证解决方案。

Result: 该架构仅用几百行代码实现，成功解决了CP-Bench约束编程基准测试集中的全部101个问题。

Conclusion: 研究结果表明，约束建模任务需要结合通用编码工具和提示中编码的领域专业知识，而非依赖专门的代理架构或预定义的工作流。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [154] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 本文提出首个评估工具，使通用LLM无需微调即可玩《外交》游戏，并深入分析其战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究受限于《外交》游戏的高复杂性和信息密度，需要顶尖或微调过的LLM，且比赛结果变异性大，导致该领域研究受阻。

Method: 开发了一个评估工具，通过数据驱动迭代优化了文本游戏状态表示，使未微调的24B模型能可靠完成比赛；开发了假设检验和统计分析工具；引入了关键状态分析协议。

Result: 实验发现，较大模型表现最佳，但较小模型也能充分进行游戏；完成了说服、侵略性游戏风格及不同模型表现的案例研究。

Conclusion: 该评估工具通过消除微调需求，使LLM战略推理能力的评估民主化，并提供了关于这些能力如何在通用LLM中自然涌现的见解。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [155] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 提出MCPToolBench++，一个大规模、多领域的AI Agent工具使用基准，旨在解决评估LLMs调用MCP工具时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估LLM和AI Agent的MCP工具使用能力存在多项问题：缺乏全面的数据集或基准；MCP工具调用结果格式多样化；实际工具调用成功率不稳定；以及LLM上下文窗口限制可用工具数量。因此，需要一个专业的评估基准来克服这些挑战。

Method: 研究提出了MCPToolBench++，这是一个大规模、多领域的AI Agent工具使用基准。该基准建立在来自40多个类别的4000多个MCP服务器之上，数据集包含单步和多步工具调用。研究人员使用此基准评估了SOTA LLMs的Agent能力。

Result: 成功构建了MCPToolBench++基准，并利用其评估了当前最先进（SOTA）的具备Agent能力的LLM。研究报告了这些评估结果。

Conclusion: MCPToolBench++为解决LLM调用MCP工具的评估挑战提供了一个全面且大规模的解决方案，有助于推动AI Agent工具使用能力的研究和发展。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [156] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: 本文提出了一种名为分层高斯混合框架（HGMF）的概率剪枝方法，旨在解决大型语言模型在从大规模、分层结构的工具库中进行工具选择时面临的准确性和效率挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在大型、分层工具库中选择工具时，由于上下文窗口有限和无关选项的干扰，导致选择准确率低和计算成本高。

Method: HGMF将用户查询和工具描述映射到统一的语义空间。它采用两阶段分层过程：首先使用高斯混合模型（GMM）聚类服务器并根据查询似然进行过滤；然后对选定服务器关联的工具重复此GMM聚类和过滤过程，以生成紧凑、高相关性的候选集供LLM最终选择。

Result: 在公开数据集上的实验表明，HGMF显著提高了工具选择准确率，并降低了推理延迟。

Conclusion: HGMF框架具有可扩展性和有效性，适用于处理大规模工具库的工具选择问题。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [157] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 当前RL方法无法从无到有地培养LLM的推理能力。本文提出了ThinkTuning，一种基于GRPO的互动式训练方法，通过教师模型的反馈指导学生模型，显著提升了其推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，强化学习（RL）虽能驱动LLM的自我提升，但它并不能真正灌输新的推理能力，而只是激发了基础模型中已有的行为。因此，研究动机在于如何训练那些不具备思考行为的模型，使其从一开始就发展出这种能力。

Method: 本文提出了ThinkTuning，这是一种基于广义策略优化（GRPO）的互动式训练方法。该方法通过一个教师模型向学生模型的推理过程（rollouts）提供指导和反馈来增强其训练。其灵感来源于课堂教学实践：教师提出问题，让学生尝试回答，然后提供纠正性反馈，以引导学生找到正确解决方案。

Result: 研究发现，通过同等大小的教师模型提供隐式监督（反馈），可以有效提升学生模型的推理能力。具体而言，该方法在各类基准测试中比零样本基线平均提升了3.85%；在MATH-500、AIME和GPQA-Diamond数据集上，分别比香草GRPO基线提升了2.08%、2.23%和3.99%。

Conclusion: 通过教师模型提供互动式、隐式的反馈监督，可以有效地训练大型语言模型发展出新的推理能力，即使这些能力在基础模型中并不存在，从而显著提升模型的性能。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [158] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 本文提出将传统家禽福利评估升级为数据驱动的智能监测系统。研究应用多模态AI，发现特征级融合策略最优，并引入DTS、DRI评估工具及模块化部署框架，旨在实现结合生产力与伦理的精准主动式动物福利管理。


<details>
  <summary>Details</summary>
Motivation: 现有家禽福利评估方法依赖人工观察和单一传感器数据，存在主观性、劳动密集和无法全面捕捉现代农场蛋鸡福利复杂性的局限性。未来家禽生产亟需向数据驱动的智能监测生态系统转型。

Method: 采用多模态人工智能（AI）整合视觉、听觉、环境和生理数据流进行福利监测。研究对比不同数据融合策略，发现中间（特征级）融合策略在真实家禽环境下表现最佳。为解决传感器脆弱性、部署成本、行为定义不一致及跨农场泛化性差等障碍，引入了两种新型评估工具：领域迁移分数（DTS）衡量模型跨农场适应性，数据可靠性指数（DRI）评估传感器数据质量。此外，还提出了一个模块化、情境感知的部署框架。

Result: 研究表明，中间（特征级）数据融合策略在实际家禽养殖条件下，实现了鲁棒性与性能的最佳平衡，并展现出优于早期或后期融合方法的可扩展性。成功引入了DTS和DRI两种新型评估工具以解决模型泛化性和数据质量评估难题。提出了模块化、情境感知的部署框架，实现了多模态感知的可伸缩和实用集成。

Conclusion: 本工作为家禽福利监测从被动、单模态方法向结合生产力与伦理科学动物护理的主动、精准驱动福利系统转型奠定了基础。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [159] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文提出了SkillNav，一个模块化框架，通过引入结构化、基于技能的推理来增强视觉-语言导航（VLN）代理的泛化能力，并在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言导航（VLN）方法在泛化到未知场景时面临挑战，尤其是在需要复杂空间和时间推理的情况下，尽管已有大规模预训练和数据增强的进展。

Method: 本文提出了SkillNav框架，它将导航分解为一系列可解释的原子技能（如垂直移动、区域识别、停止和暂停），每种技能由一个专门的代理处理。此外，引入了一个新颖的零样本视觉-语言模型（VLM）路由器，该路由器根据子目标、视觉观察和历史动作动态选择最合适的代理。

Result: SkillNav在R2R基准测试上取得了新的最先进性能，并在包含新指令风格和未见环境的GSA-R2R基准测试中展示了强大的泛化能力。

Conclusion: SkillNav通过引入结构化、基于技能的推理，有效提升了Transformer-based VLN代理在复杂3D环境中的导航能力和对未知场景的泛化性。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [160] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: 提出DiMuST模型，通过解耦表示学习解决现有POI推荐中时空表示错位问题，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有POI推荐模型独立建模时空转换，导致时空节点表示错位，融合时引入冗余信息，增加模型不确定性并降低可解释性。

Method: 提出DiMuST，一个基于多路时空转换图的社交增强POI推荐模型。其核心是新型解耦变分多路图自动编码器（DAE），通过多路时空图策略解耦共享和私有分布，再利用专家乘积（PoE）机制融合共享特征，并通过对比约束去噪私有特征。

Result: DiMuST模型有效捕获POI时空转换表示并保留其内在关联。在两个数据集上的实验显示，DiMuST在多项指标上显著优于现有方法。

Conclusion: DiMuST通过解耦表示学习有效解决了POI推荐中时空表示错位问题，显著提升了模型性能和可解释性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [161] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 提出一种多智能体框架，通过分解隐私推理任务，有效减少大型语言模型（LLMs）在处理多源信息时的隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 在交互式环境中，LLMs处理包含私有和公共信息的复杂内容时，难以有效解决上下文隐私问题，容易导致信息泄露。

Method: 引入一个多智能体框架，将隐私推理分解为专业子任务（如提取、分类），以减少单个智能体的信息负载，并实现迭代验证。同时，通过对信息流拓扑进行系统消融研究，分析隐私错误的产生和传播机制。

Result: 在ConfAIde和PrivacyLens基准测试上，最佳多智能体配置显著降低了私有信息泄露（使用GPT-4o时分别降低18%和19%），同时保持了公共内容的准确性，表现优于单智能体基线。

Conclusion: 在基于LLMs的多智能体系统中，原则性的信息流设计对于解决上下文隐私问题具有巨大潜力。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [162] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: 本文提出了EMPATHIA多智能体框架，旨在通过整合文化、情感和伦理因素，提升AI在难民安置决策中的人性化与尊严保留，并在实践中取得了可解释的成果。


<details>
  <summary>Details</summary>
Motivation: 现有AI在难民融合中仅关注就业等狭隘目标，未能捕捉文化、情感和伦理等关键维度。研究动机是解决“机器参与改变生活的决策时如何保留人类尊严”这一核心问题。

Method: 引入EMPATHIA框架，基于Kegan的建构发展理论，包含SEED（初始安置）、RISE（早期独立）和THRIVE（持续结果）三个模块。SEED采用选择器-验证器架构，由情感、文化和伦理三个专业智能体进行透明和可解释的决策。

Result: 在联合国卡库马数据集上对6,359名难民进行实施，取得了87.4%的验证收敛率，能在五个接收国提供可解释的评估。EMPATHIA通过加权整合文化、情感和伦理因素，平衡了竞争性价值体系，并支持了实践者与AI的协作。

Conclusion: EMPATHIA通过增强而非取代人类专业知识，提供了一个通用的AI驱动分配任务框架，可在需要协调多种价值观的场景中应用，有助于保留AI参与决策时的人类尊严。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [163] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 为解决人工智能伦理对齐问题，本研究提出Ethics2Vec方法，将智能体行为策略映射为可量化的向量表示，从而实现与人类价值观的比较和对齐评估。


<details>
  <summary>Details</summary>
Motivation: 智能体行为中隐含或显式的伦理价值难以被人类理解（即对齐问题），尤其因为人类伦理考量常涉及不可通约的价值。为了实现人类与人工价值观的对齐，需要定义一个可以进行度量的共同空间。

Method: 本研究提出Ethics2Vec方法，该方法将传统的“Anything2vec”范式扩展到伦理领域。它将自动智能体的决策制定或控制律策略映射为多元向量表示。该方法首先应用于二元决策情境，随后探讨了如何扩展到自动控制设置（如自动驾驶汽车的控制律向量化）。

Result: 通过将智能体行为策略映射为多元向量，该表示可用于比较和评估智能体与人类价值观的对齐程度。

Conclusion: 本论文提出了一种新颖的方法，通过将伦理考量向量化，为量化和评估AI伦理对齐提供了一个可行的途径，有望克服伦理价值不可通约的挑战，促进智能体行为与人类价值观的对齐。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [164] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 针对规划领域中Transformer模型（如PlanGPT）因对称性导致的泛化能力差问题，本文提出了一种结合对比学习目标和架构改进的对称感知训练方法，有效提升了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在自动化规划领域的应用受限，特别是像PlanGPT这样的模型，难以从简单问题泛化到复杂问题。这源于规划任务的对称性，即变量命名任意性导致等价表示的组合爆炸，纯粹的Transformer模型难以高效学习。

Method: 提出了一种新颖的对比学习目标，旨在使Transformer模型具有对称感知能力，弥补其归纳偏置的不足。结合架构改进，使Transformer能高效地用于规划生成或启发式预测。

Result: 在多个规划领域的结果表明，所提出的对称感知训练方法有效且高效地解决了PlanGPT的局限性。

Conclusion: 通过引入对称感知训练（结合对比学习和架构改进），Transformer模型可以克服在自动化规划中的固有局限性，实现有效且高效的性能表现。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [165] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 针对鲁棒马尔可夫决策过程（RMDPs）中存在多个最坏情况最优策略的问题，本文提出了一种名为最优鲁棒最佳努力（ORBE）策略的新型策略选择标准。ORBE策略不仅最大化最坏情况预期收益，还兼顾非对抗性概率下的最大预期收益，提供了一种原则性的策略选择方法。


<details>
  <summary>Details</summary>
Motivation: 在鲁棒马尔可夫决策过程（RMDPs）中，即使在s-矩形不确定性下，也可能存在多个最优鲁棒策略。这些策略在最坏情况下的预期收益相同，但在非对抗性（非完全最坏情况）的转移概率下，其预期收益可能不同，导致策略选择的困难。

Method: 本文受博弈论中“支配”和“最佳努力”概念的启发，提出了一种新的策略选择准则——最优鲁棒最佳努力（ORBE）策略。该方法不仅要求策略最大化最坏情况下的预期收益，还额外要求策略在不同（非完全对抗性）转移概率下实现最大预期收益。在此基础上，提出了一个与标准鲁棒值迭代相比开销较小的算法来计算ORBE策略。

Result: 研究证明了ORBE策略始终存在，并刻画了其结构。开发了一种计算ORBE策略的算法，该算法相对于标准鲁棒值迭代的计算开销较小。数值实验结果表明所提方法是可行的。

Conclusion: ORBE策略为鲁棒马尔可夫决策过程中的多个最优鲁棒策略提供了一种有原则的决策辅助机制（或“决胜局”策略），能够更好地平衡最坏情况与非最坏情况下的预期收益。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [166] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出一个知识图谱，通过AI辅助的情境预识别，为急救人员提供创新的知识管理和智能治疗建议，以应对紧急情况下的救治需求。


<details>
  <summary>Details</summary>
Motivation: 全球急救需求日益增长，急救人员在时间紧迫的紧急情况下，难以充分运用自身知识并提供个性化、优化的医疗救助，亟需即时协助和治疗建议。

Method: 文章提出并构建了一个知识图谱作为核心知识表示，旨在通过创新的知识管理，并结合人工智能的情境预识别能力，为急救人员提供智能治疗建议。

Result: 该知识图谱能够现场计算、评估和处理相关知识，从而为急救人员提供智能治疗建议，以期在紧急情况下优化治疗方案。

Conclusion: 所提出的知识图谱通过提供即时、智能的治疗建议，有效支持急救人员，有助于提升紧急救援的质量和效率。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [167] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [168] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 该研究提出自组织生存流形（SOSM）理论，将生存建模为生物状态空间中内在的几何性质，而非监督学习任务，通过测地线曲率最小化使预后与几何流稳定性对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的生存建模依赖于预设的标签和固定协变量，是一种监督学习任务。本研究拒绝这一前提，旨在提出一种将生存视为生物状态空间中曲率和流的几何结果，即一种内在 emergent 属性的新范式。

Method: 开发了自组织生存流形（SOSM）理论，其中生存相关动态源于受内部生物约束塑造的潜在流形上的低曲率测地线流。引入了基于测地线曲率最小化的生存能量函数，并推导了目标函数的离散和连续公式。该框架将生存建模与热力学效率、熵流、Ricci曲率和最优传输等物理定律联系起来。

Result: 研究表明，所引入的生存能量函数能够诱导结构，使预后与几何流的稳定性对齐。在生物学合理条件下，理论证明了生存对齐轨迹的出现和收敛性。健康、疾病、衰老和死亡被重新定义为流形结构中的几何相变。

Conclusion: 该理论为生存建模提供了一个通用的、无标签的基础，将生存视为形式的属性而非外部标注，从而成功连接了机器学习、生物物理学和生命几何学，提供了一种基于物理定律的全新生存分析视角。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [169] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 本文提出一个两阶段学习框架，结合无监督异常检测和半监督支持向量机，有效应对现代供应链欺诈检测中数据稀疏和类别不平衡的挑战，并在真实数据集上取得高F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代供应链欺诈检测面临全球网络复杂性、标注数据稀缺、类别不平衡以及传统检测方法监督受限的挑战，导致其在实际应用中效果不佳。

Method: 提出一个两阶段学习框架：第一阶段，使用Isolation Forest算法进行无监督异常检测，识别潜在欺诈并减少分析数据量；第二阶段，使用自训练支持向量机(SVM)结合有标签和高置信度伪标签样本进行半监督学习，以精炼预测。

Result: 在DataCo智能供应链数据集上评估，该方法实现了0.817的F1分数，同时将误报率控制在3.0%以下。

Conclusion: 研究结果表明，结合无监督预过滤和半监督精炼的方法，在真实世界约束下，对于供应链欺诈检测是有效且高效的，但仍需注意概念漂移并与深度学习方法进行比较。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [170] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 针对药物-药物相互作用（DDI）预测中罕见类别的严重不平衡问题，本文提出一个结合GFlowNet和VGAE的框架，通过生成合成样本来提高预测性能和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 药物-药物相互作用（DDI）预测存在严重的类别不平衡，导致现有模型在罕见但关键的相互作用上表现不佳；现有方法常将其视为二元问题，忽视类别特异性，加剧了对常见相互作用的偏倚。

Method: 提出一个结合生成流网络（Generative Flow Networks, GFlowNet）与变分图自编码器（Variational Graph Autoencoders, VGAE）的框架，用于为罕见DDI类别生成合成样本。

Result: 该方法提升了对不同相互作用类型的预测性能。

Conclusion: 通过生成合成样本，该框架改善了模型平衡性，生成了有效且新颖的DDI对，从而确保了更好的临床可靠性。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [171] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出HGMN，一种融合了角色和邻接特征的超图神经网络，通过捕捉高阶关系和缓解过平滑问题，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）主要关注邻接关系，忽略了对节点表示至关重要的丰富角色特征。现有捕获角色特征的方法大多是无监督的，在下游任务中表现不佳。

Method: 提出HGMN（带有状态空间模型的超图神经网络），通过超图构建技术（基于节点度数和邻域级别）建模高阶关系，并通过可学习的Mamba Transformer机制结合角色与邻接表示。模型包含超图卷积层以捕获复杂依赖，并引入残差网络以缓解深度GNN中的过平滑问题。

Result: 在新建数据集和四个基准数据集上的广泛实验表明，HGMN优于现有最先进的GNN方法，在节点分类任务中取得了显著的性能提升。

Conclusion: HGMN通过有效嵌入角色特征和邻接信息，提供了更丰富的节点表示，使其成为各种基于图的学习应用的通用而强大的工具。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [172] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 本文首次揭示并分析了图数据上向量量化（VQ）中的码本崩溃问题，并提出了RGVQ框架，通过整合图拓扑和特征相似性，结合Gumbel-Softmax和结构感知对比正则化，有效提升了码本利用率和图令牌表示性能。


<details>
  <summary>Details</summary>
Motivation: 向量量化（VQ）在图数据离散表示中很有前景，但图领域中的“码本崩溃”问题未被充分探索，严重限制了图令牌的表达能力和泛化性。现有缓解策略在图数据上仍无效，需要深入理解其原因并提出新的解决方案。

Method: 本文首先进行实证研究，证明码本崩溃在图VQ中普遍存在。接着，通过理论分析，识别出早期分配不平衡（源于图特征和结构模式的冗余）和确定性VQ中自我强化优化循环是导致崩溃的关键因素。为解决这些问题，提出了RGVQ框架，该框架：1. 通过Gumbel-Softmax重参数化引入软分配，确保所有码字接收梯度更新，增强码本利用率。2. 引入结构感知对比正则化，惩罚相似节点对之间的令牌共分配，促进令牌多样性。

Result: RGVQ显著提高了码本利用率，并在多个下游任务中持续提升了最先进图VQ骨干模型的性能，从而实现了更具表达性和可迁移性的图令牌表示。

Conclusion: RGVQ成功解决了图数据中VQ的码本崩溃问题，通过整合图拓扑和特征相似性作为显式正则化信号，并通过软分配和结构感知对比学习，有效提高了图令牌的表达能力和泛化性，为图数据的离散表示学习提供了新的有效框架。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [173] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 为解决神经影像计算机辅助诊断（CAD）系统中数据异质性和亚型混淆问题，本文提出一种新型联邦学习框架，通过动态导航和元集成模块显著提高了诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经影像CAD系统面临挑战：小样本研究复现性低，而大规模数据集因多重疾病亚型被单一标签标注而引入混淆异质性。因此，需要有效处理数据异质性和亚型混淆，以提升诊断系统的可靠性和复现性。

Method: 提出一种新型联邦学习框架，专为神经影像CAD系统设计。该框架包含：1) 动态导航模块，根据潜在亚型表示将样本路由至最适合的本地模型；2) 元集成模块，将异构本地模型的预测结果整合成统一的诊断输出。

Result: 在包含超过1300名抑郁症患者和1100名健康对照的fMRI数据集上进行评估。结果显示，与传统方法相比，该框架在诊断准确性和鲁棒性方面有显著提升，平均准确率达到74.06%。消融研究进一步证实了动态导航和元集成模块对性能提升的关键作用。

Conclusion: 该框架通过有效解决数据异质性和亚型混淆问题，提升了神经影像CAD系统的可靠性和可复现性，在个性化医疗和神经精神科临床决策中展现出巨大的应用潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [174] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 该研究提出了一个框架，将生成式AI与跨学科知识（如植物科学、仿生学、材料工程）结合，利用BioinspiredLLM、RAG等AI工具，用于设计湿度响应的生物启发材料。通过实验验证，成功制造出LLM设计的新型花粉基粘合剂，证明了AI在推动实际材料设计和实现人机协作方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在知识检索和创意构思方面表现出色，但其在材料科学等高度多学科的实验科学领域的应用仍受到限制。

Method: 本研究提出了一个整合生成式AI与跨学科文献（如植物科学、仿生学、材料工程）的框架，旨在提取见解并设计材料实验。研究聚焦于花粉基材料和棕榈叶等湿度响应系统。采用了一系列AI工具，包括微调模型（BioinspiredLLM）、检索增强生成（RAG）、Agentic系统和分层采样策略，以提取结构-性能关系。通过结构化推理协议生成和评估数百个假设。

Result: 研究成功提取了结构-性能关系并将其转化为新型生物启发材料。通过实验室验证，LLM生成的实验程序、材料设计和力学预测得以实现，最终成功制造出一种具有可调形态和可测量剪切强度的新型花粉基粘合剂。

Conclusion: 本工作为未来的植物源粘合剂设计奠定了基础。它展示了AI辅助构思如何有效推动现实世界的材料设计，并实现人机之间的有效协作。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [175] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 研究提出通过预训练数据过滤双用途内容，可显著提高开源AI模型对恶意篡改的抵抗力，但仍需多层防御。


<details>
  <summary>Details</summary>
Motivation: 开源AI系统虽有优势，但易受篡改攻击，现有微调等后训练安全方法对对抗性微调抵抗力弱，缺乏稳健的风险管理科学。

Method: 引入多阶段数据过滤管道，从训练数据中筛选出双用途（如生物威胁）相关文本，并用此过滤数据从头预训练多个6.9B参数模型，以评估其抗篡改能力。

Result: 过滤后的模型对对抗性微调攻击展现出显著抵抗力（高达10,000步和300M生物威胁相关文本），远超现有后训练基线，且未影响无关能力。但发现模型在上下文中获得危险信息时仍能利用。

Conclusion: 预训练数据筛选是开源AI系统有前景的防御层，有助于构建更耐篡改的模型；但鉴于模型仍能利用外部上下文信息，仍需采用深度防御策略。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [176] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 本文提出数据分布相位的概念，分析了扩散模型去噪过程中存在的相变现象，并据此建议在不同阶段使用局部或全局神经网络来优化计算效率，从而降低扩散模型的计算成本。


<details>
  <summary>Details</summary>
Motivation: 普通扩散模型在处理具有局部空间结构的数据时，学习全局分数函数，导致计算成本高昂。

Method: 引入“数据分布相位”新视角，定义通过局部操作可相互连接的分布属于同一相位。证明了逆向去噪过程包含早期平凡阶段、晚期数据阶段以及中间的快速相变阶段（此时局部去噪器失效）。通过基于条件互信息的信息论界限和数值实验来诊断这些相变。

Result: 研究发现去噪过程存在相变点，并提出一种更高效的扩散模型架构：在远离相变点时，可使用小型局部神经网络计算分数函数；仅在相变窄时间间隔内需要全局神经网络。

Conclusion: 本研究为数据分布相位、生成式人工智能以及受物理概念启发设计神经网络提供了新方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [177] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出一种通用的语言模型缩放定律，适用于稠密和稀疏模型，旨在解决现有特定架构缩放定律的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型规模和训练成本呈指数级增长，现有缩放定律多为特定架构（稠密或稀疏），难以统一预测模型大小和优化资源分配。

Method: 重新审视现有缩放定律，并提出一种广义的缩放定律，以提供适用于稠密和稀疏大型语言模型的统一框架。

Result: 对所提出的广义缩放定律与现有缩放定律进行了评估和比较，以证明其有效性。

Conclusion: 通过提出广义缩放定律，为稠密和稀疏大型语言模型提供了一个统一的预测和资源分配框架，有效克服了现有方法架构特异性的局限性。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [178] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE是一种处理标签噪声的新型目标函数，通过对抗训练实现，性能优于现有方法且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，训练数据中普遍存在固有或对抗性引入的标签噪声，这会影响模型的学习效果。

Method: 引入ANTIDOTE，一种基于信息散度邻域松弛的新型目标函数，用于噪声标签学习。通过凸对偶性，将其重新表述为一种对抗训练方法。

Result: ANTIDOTE能自适应地降低噪声标签样本的影响，表现出“遗忘”行为。在多种标签噪声类型（对称、非对称、人工标注、真实世界）下，其性能优于现有主流方法，且时间复杂度接近标准交叉熵损失。

Conclusion: ANTIDOTE为有噪声标签的学习提供了一种有效且计算高效的解决方案，在实际应用中表现出色，能够应对固有的或对抗性引入的标签噪声问题。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [179] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态机器学习方法，通过整合电子健康记录中的诊断代码历史和实验室测量数据，实现胰腺导管腺癌（PDAC）的早期检测。


<details>
  <summary>Details</summary>
Motivation: 胰腺导管腺癌（PDAC）是最致命的癌症之一，由于缺乏特异性症状和可靠的生物标志物，早期发现仍然是一个重大的临床挑战。

Method: 该方法结合了多种技术：使用神经受控微分方程建模不规则的实验室时间序列，利用预训练语言模型和循环网络学习诊断代码轨迹表示，并采用交叉注意力机制捕捉两种模态之间的相互作用。该方法整合了纵向诊断代码历史和常规收集的实验室测量数据。

Result: 在近4700名患者的真实世界数据集上，该方法在AUC方面比现有最新方法提高了6.5%至15.5%。此外，该模型识别出与PDAC风险升高相关的诊断代码和实验室指标，包括已知的和新的生物标志物。

Conclusion: 该多模态方法能够显著提高胰腺导管腺癌的早期检测能力，并识别出潜在的风险生物标志物，为临床早期干预提供了新途径。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [180] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 针对大语言模型生成合成数据与真实数据结合的统计有效性问题，本文提出一种基于广义矩方法的新型估计器，并发现合成数据与真实数据的矩残差交互可显著提升参数估计效果。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的情况下，计算社会科学和人类受试者研究越来越多地利用大语言模型（LLMs）生成合成样本。但如何将这些合成数据与真实数据有效结合，并确保统计结论的有效性，是一个待解决的挑战。

Method: 引入了一种基于广义矩方法（Generalized Method of Moments, GMM）的新型估计器。该估计器无需超参数，并具有强大的理论保证。

Result: 1. 发现合成数据与真实数据的矩残差之间的交互作用，可以提升目标参数的估计精度。
2. 在计算社会科学应用中的多种回归任务上，经验验证了该估计器具有出色的有限样本性能，并带来了显著的经验增益。

Conclusion: 本文提出的基于GMM的估计器，为结合大语言模型生成的合成数据与真实数据以得出统计有效结论提供了一种鲁棒且高效的解决方案，并通过数据间意想不到的交互作用提升了参数估计性能。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [181] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 针对非平稳时间序列数据中的异常检测挑战，本文提出了两种新型自适应阈值框架SCS和MACS，通过在线学习和分段原理显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在制造业、IT和基础设施监控中日益普遍，但其非平稳特性导致传统静态阈值在面对状态变化、概念漂移或多尺度变化时失效，亟需适应性更强的异常检测方法。

Method: 引入并实证评估了两种新型自适应阈值框架：分段置信序列（Segmented Confidence Sequences, SCS）和多尺度自适应置信分段（Multi-Scale Adaptive Confidence Segments, MACS）。这两种方法均利用统计在线学习和分段原理实现局部、上下文敏感的自适应，即使在数据分布演变的情况下也能保持误报率的保证。

Result: 在晶圆制造基准数据集上的实验表明，与传统的百分位和滚动分位数方法相比，SCS和MACS在F1分数上取得了显著提升。

Conclusion: 研究表明，鲁棒且基于统计原理的自适应阈值能够实现对各类真实世界异常的可靠、可解释和及时检测。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [182] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 本文提出了一种最优的基于索引阈值的调度策略，以最小化多模态远程推理系统中因网络资源受限导致的数据不新鲜造成的机器学习模型推理误差，并在广泛条件下证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多模态远程推理系统中，实时推理依赖于从远程传感器收集的特征。由于传感器数据动态变化，特征的新鲜度至关重要。然而，有限的网络资源使得及时传输所有模态的特征变得不可行，这会导致机器学习模型产生推理误差。

Method: 研究了一个双模态调度问题，旨在最小化机器学习模型的推理误差，该误差被表达为两种模态AoI（信息新鲜度）的惩罚函数。为此，开发了一种基于索引的阈值策略并证明了其最优性。具体而言，调度器在当前模态的索引函数超过某个阈值时切换模态。该策略适用于非单调、非可加的一般AoI函数以及异构传输时间。

Result: 数值结果显示，与不考虑AoI推理误差函数的轮询和均匀随机策略相比，所提出的策略能够将推理误差降低高达55%。

Conclusion: 本研究结果揭示了如何通过优化面向任务的AoI函数来提高远程推理的准确性，为解决远程推理中的数据新鲜度与准确性权衡问题提供了有效方法。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [183] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: 本文通过解决通用序列图（USM）编码中的种子偏差，提升了其对符号序列的生物射频分形编码能力，揭示了USM作为一种高效、收敛的数值过程，可实现数值定位与序列身份的完全一致。


<details>
  <summary>Details</summary>
Motivation: 鉴于基于Transformer的语言模型（如ChatGPT）的兴起，人们对能以多尺度和多嵌入维度对符号序列进行数值表示的编码程序重新产生了兴趣。研究动机在于需要一种能独特保留单个符号上下文信息的机制，以便通过神经网络等非线性模型进行建模。

Method: 本文基于通用序列图（USM）进行研究，USM是一种迭代函数，通过前后向混沌游戏表示（CGR）将符号序列双射编码到数值空间。本报告的核心方法是解决影响USM迭代过程的种子偏差问题，以提高其编码准确性。

Result: 解决了USM迭代过程中的种子偏差问题，获得了两项主要结果：1）实现了数值定位与序列身份的完全一致；2）揭示了USM作为一种高效的数值过程，可收敛到稳态序列嵌入解的本质。这些结果在基因组序列上进行了演示，并验证了其对任意基数字母表的通用性。

Conclusion: 本研究通过解决种子偏差，显著提升了通用序列图（USM）的编码能力和准确性。USM现在能够将数值定位与序列身份完美匹配，并被证明是一种高效、收敛的符号序列嵌入方法，为神经网络模型处理符号序列提供了可靠的上下文信息表示。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [184] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: 提出TabularARGN，一种新的神经网络架构，用于生成高质量、保护隐私的合成表格数据。


<details>
  <summary>Details</summary>
Motivation: 敏感数据共享和分析需要合成数据，但传统匿名化技术在隐私保护方面不足。

Method: 引入TabularARGN，一个基于离散化的自回归神经网络架构，专门用于生成高质量的合成表格数据，同时保持计算效率。

Result: TabularARGN在统计相似性、机器学习效用和检测鲁棒性方面表现出竞争力；通过深入的成员推理攻击评估，显示出良好的鲁棒性和有效的隐私-效用平衡。

Conclusion: TabularARGN是一种有效的合成表格数据生成方法，能够实现高质量数据和强大的隐私保护之间的良好平衡。

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [185] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 本文提出CORAL框架，通过双智能体涌现通信范式学习可迁移的上下文表示，显著提升强化学习智能体的上下文学习和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）智能体通常难以在不更新参数的情况下泛化到新任务和新环境，因其学习到的表示和策略过度拟合了训练环境。

Method: 将上下文强化学习（ICRL）问题建模为双智能体涌现通信问题。CORAL框架解耦了潜在表示学习与控制，包括一个预训练为世界模型的“信息智能体”（IA）和一个基于IA提供的通信上下文学习任务的“控制智能体”（CA）。通信协议由新颖的因果影响损失塑造。

Result: 实验表明，该方法显著提高了CA的样本效率，并在IA的帮助下，在完全未见的稀疏奖励环境中成功实现了零样本适应。

Conclusion: 验证了学习可迁移通信表示的有效性，提升了RL智能体的泛化能力。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [186] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 本文将Kolmogorov-Arnold Networks (KANs) 集成到GNNs中，构建了KGAT、KSGC、KAPPNP模型，提升了节点分类精度。同时，通过多教师知识融合框架，将KAN-GNNs的知识蒸馏到图无关的KAN学生模型中，实现了高效、无图的推理。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）性能强大，但依赖图连接性，限制了可扩展性和效率。Kolmogorov-Arnold Networks（KANs）具有强大的非线性表达能力和高效推理能力，研究旨在结合二者克服GNN的局限性。

Method: 1. 将KANs集成到GAT、SGC和APPNP三种流行GNN架构中，形成KGAT、KSGC和KAPPNP模型。2. 采用多教师知识融合框架，将来自多个基于KAN的GNNs的知识蒸馏到一个图无关的KAN学生模型中。

Result: 1. 提出的模型（KGAT、KSGC、KAPPNP）在基准数据集上提高了节点分类精度。2. 知识融合方法显著提升了学生模型的性能。

Conclusion: 研究结果突出了KANs增强GNN表达能力以及实现高效、无图推理的潜力。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [187] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 提出一种新颖的基于离散余弦变换的激活水印（DCT-AW）方法，专门用于保护科尔莫哥洛夫-阿诺德网络（KAN）的知识产权。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型知识产权保护日益重要，现有水印方法无法适应新兴的科尔莫哥洛夫-阿诺德网络（KAN）独特的架构（可学习激活函数），而KAN在复杂关系建模中潜力巨大。

Method: 提出DCT-AW方法，利用KAN的可学习激活函数，通过离散余弦变换（DCT）扰动激活输出以嵌入水印，确保方法与任务兼容并实现任务独立性。

Result: 实验结果显示，DCT-AW对模型性能影响极小，且对多种水印移除攻击（如微调、剪枝、剪枝后重训练）具有卓越的鲁棒性。

Conclusion: DCT-AW为KAN模型提供了一种有效且鲁棒的知识产权保护机制，克服了现有水印方法在KAN上应用面临的挑战。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [188] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [189] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: 现有物种分布模型常忽略生物互作，且处理不完整数据能力不足。本文提出CISO，一个深度学习模型，能有效整合不完整物种观测数据和环境变量进行分布预测，并在多个数据集上显示出优越性能。


<details>
  <summary>Details</summary>
Motivation: 物种分布模型（SDMs）在预测物种地理分布时，通常忽略物种间生物互作的影响。少数考虑生物互作的方法，存在假设对称关系和要求完整共现数据的局限，与实际观测数据稀疏和不完整的情况不符。

Method: 提出CISO（Conditioned on Incomplete Species Observations），一种基于深度学习的物种分布建模方法。CISO能根据灵活数量的不完整物种观测数据结合环境变量进行预测。该方法在sPlotOpen（植物）、SatBird（鸟类）和SatButterfly（蝴蝶）三个数据集上进行了验证。

Result: 研究结果表明，引入部分生物信息能提升在空间分离测试集上的预测性能。CISO在预测剩余物种分布时，优于以相同数据集内部分物种为条件的替代方法。此外，结合多数据集的观测数据可进一步提升性能。

Conclusion: CISO是一个有潜力的生态工具，能有效整合不完整的生物信息，并识别不同分类群物种间潜在的相互作用。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [190] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [191] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: Fed MobiLLM提出一种服务器辅助的联邦侧调优范式，显著降低移动设备上LLM微调的计算、内存和通信开销，并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 在异构移动设备上协作微调大型语言模型（LLMs）具有巨大潜力，但现有联邦LLM微调方法给移动硬件带来过高的计算和内存负担，且同步聚合协议效率低下，难以应对设备异构性。

Method: 本文提出Fed MobiLLM，核心是服务器辅助的联邦侧调优范式。移动设备仅对冻结的LLM骨干进行轻量级前向传播，并上传选定的中间激活。服务器独立训练一个共享的侧网络，从而消除客户端的反向传播，并支持异步更新。为解决模型异构性，引入自适应层级特征对齐方法。

Result: Fed MobiLLM在保持鲁棒微调性能的同时，实现了极低的设备内存占用，计算开销减少至少95.2%，通信成本降低93.2%，收敛速度比现有方法快5.1倍。

Conclusion: Fed MobiLLM有效解决了在异构移动设备上联邦LLM微调的系统挑战，验证了其在实际LLM自适应场景中的高效性与实用性。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [192] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: 本研究提出PANAMA算法，一种基于MARL的多智能体路径规划方法，用于优化数字孪生生态系统中的数据处理与共享，在准确性、速度和可扩展性上超越现有基准，旨在促进DT、无线网络和AI自动化的协同。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和自动化系统规模的扩大，高效的数据共享框架和鲁棒算法变得至关重要。研究动机在于探索数字孪生（DT）生态系统中下一代网络数据处理的关键作用，特别关注应用和网络提供商（AP/NP）之间的动态关系，以支持具身AI的自主任务执行。

Method: 引入了一种名为PANAMA的新型算法，该算法基于网络感知多智能体强化学习（MARL）的多智能体路径规划（MAPF），并采用优先级非对称。其核心是结合集中式训练与分布式执行（CTDE）框架和异步actor-learner架构，以加速训练并实现具身AI的自主任务执行。

Result: PANAMA在路径规划性能方面，相比现有基准，在准确性、速度和可扩展性上均表现出卓越的性能。通过模拟，研究强调了优化的数据共享策略对于可扩展自动化系统的重要性，确保了在复杂真实环境中的韧性。

Conclusion: PANAMA成功地弥合了网络感知决策与鲁棒多智能体协调之间的差距，显著推进了数字孪生、无线网络和AI驱动自动化之间的协同作用。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [193] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: Zero-Direction Probing (ZDP)是一个纯理论框架，旨在通过分析Transformer激活的零方向来检测模型漂移，无需任务标签或输出评估。


<details>
  <summary>Details</summary>
Motivation: 在缺乏任务标签或输出评估的情况下，有效检测深度学习模型（特别是Transformer）的表征漂移是一个重要且具有挑战性的问题。

Method: 提出纯理论框架Zero-Direction Probing (ZDP)，在A1-A6假设下，通过分析Transformer层激活的右/左零空间及其Fisher几何结构，证明了一系列定理。基于这些理论，进一步推导了频谱零泄漏（SNL）度量。

Result: 成功证明了方差泄漏定理、Fisher零空间守恒、低秩更新的秩泄漏界限以及在线零空间跟踪器的对数遗憾保证。同时，提出了频谱零泄漏（SNL）度量，并给出了非渐近尾部界、集中不等式以及在高斯零模型下的先验漂移阈值。

Conclusion: 监控Transformer层激活的右/左零空间及其Fisher几何结构，能够为模型表征变化（即模型漂移）的检测提供具体且可检验的理论保证。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [194] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文提出PROPS，一种多阶段隐私保护自对齐框架，旨在解决LLM对齐中人类反馈的隐私问题，通过保护偏好标签隐私，在提升模型效用的同时，显著优于现有隐私保护方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的对齐过程依赖人类反馈，这引发了关于标签者个人偏好可能泄露其隐私（价值观、信仰、人格特质）的担忧。现有方法如差分隐私随机梯度下降（DP-SGD）虽然提供严格隐私保证，但可能过度隐私化，并损害模型效用。

Method: 本文提出了PROPS（PROgressively Private Self-alignment），这是一个多阶段的隐私保护对齐框架。在该框架中，前一阶段经过隐私保护对齐的模型可以作为标签者，为后续阶段的对齐补充训练数据。

Result: PROPS提供了理论保证，并通过多模型和数据集进行了全面验证。在相同的隐私预算下，PROPS对齐方法相比DP-SGD可实现高达3倍的胜率提升，相比基于随机响应（RR）的对齐方法可实现2.5倍的胜率提升。

Conclusion: PROPS在LLM对齐中，成功实现了偏好级别的隐私保护，并在保持高隐私预算的同时，显著提升了模型的实用性或对齐效果，表现优于现有的隐私保护方法。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [195] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: 提出MA-NTAE，一种非线性Tucker自编码器，通过模式感知和递归操作有效处理高维高阶张量，在压缩和聚类任务中优于现有方法，且计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中，高维高阶张量处理面临挑战：基于MLP的自编码器受维度灾难影响，导致模型大、计算高、优化难；现有张量网络虽减轻计算负担但难以捕捉非线性关系。

Method: 引入模式感知非线性Tucker自编码器（MA-NTAE）。它将经典Tucker分解推广到非线性框架，采用“Pick-and-Unfold”策略，通过递归的展开-编码-折叠操作实现灵活的逐模式编码，有效整合张量结构先验。其计算复杂度随张量阶数线性增长，随模式维度成比例增长。

Result: MA-NTAE在压缩和聚类任务中表现优于标准自编码器和现有张量网络。对于更高阶、更高维度的张量，其性能优势更为显著。

Conclusion: MA-NTAE为高维高阶张量的自监督学习提供了一种有效且高效的解决方案，尤其在处理大型张量时表现出色。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [196] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: 针对多模态情感识别(MER)中缺失模态的挑战，特别是传统方法在处理“难样本”时的局限性，本文提出了一种名为HARDY-MER的“难度感知动态课程学习”框架。该框架通过多视角难度评估和基于检索的动态课程学习策略，有效提升了模型在缺失模态情景下对困难样本的处理能力，并在基准数据集上取得了超越现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别（MER）中处理缺失模态的传统方法（如模态重建）未能充分考虑不同样本重建难度的差异，导致模型在处理“难样本”时性能受限。

Method: 本文提出了HARDY-MER框架，包含两个关键阶段：1. **难度估计**：引入多视角难度评估机制，通过直接难度（模态重建误差）和间接难度（跨模态互信息）量化样本的重建难度。2. **动态学习**：引入基于检索的动态课程学习策略，通过检索具有相似语义信息的样本，并平衡对简单样本和困难样本的学习侧重，动态调整训练课程，从而强调对困难样本的学习。

Result: 在基准数据集上的广泛实验表明，HARDY-MER在缺失模态场景下持续优于现有方法。

Conclusion: HARDY-MER通过其新颖的难度感知与动态课程学习机制，成功克服了传统方法在缺失模态情景下处理难样本的局限性，显著提升了多模态情感识别的性能。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [197] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 针对O2O RL中数据增强存在的离线-在线数据分布差距问题，本文提出无分类器扩散生成（CFDG）方法，通过优化数据生成质量和对齐方式，显著提升了现有O2O RL算法的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有O2O RL数据增强方法生成的离线数据与在线数据之间存在分布差异，这限制了在线微调策略的整体性能。

Method: 本文提出Classifier-Free Diffusion Generation (CFDG)数据增强方法。CFDG利用无分类器指导扩散技术，在不引入额外训练开销的情况下，显著提升了不同分布离线和在线数据的生成质量。此外，它采用重加权方法使更多生成数据与在线数据对齐，旨在提升性能并保持智能体稳定性。

Result: 实验结果表明，CFDG在数据生成质量和性能上均优于直接回放或使用标准扩散模型生成数据。通过将CFDG与主流O2O RL算法（如IQL、PEX和APL）集成，在D4RL基准（如MuJoCo和AntMaze）上实现了平均15%的经验性能提升。

Conclusion: CFDG是一种通用且有效的O2O RL数据增强范式，能够显著弥补离线与在线数据间的分布差距，并能与现有O2O RL算法结合，提升其在线微调性能和稳定性。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [198] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 该研究提出一种开源方法，通过预训练、SFT和RL将LLM适配于互联网上数据稀缺的小众编程语言Q，并在新构建的Q评估数据集上，其最佳模型表现显著优于现有前沿大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能力不断提升，但它们在互联网上数据不足的专业领域（如小众编程语言Q）表现不佳，难以应用于利基编程语言和私有领域，这一挑战尚未得到有效解决。

Method: 研究引入了一个新的Q语言Leetcode风格评估数据集，并基于Qwen-2.5系列模型（涵盖1.5B到32B五种参数规模），进行了预训练、监督微调和强化学习，训练了一系列推理和非推理模型。同时，对主要前沿模型在该数据集上进行了基准测试。

Result: 团队的最佳模型在Q基准测试中取得了59%的pass@1准确率，比表现最佳的前沿模型Claude Opus-4高出29.5%。此外，所有训练的模型（包括最小的1.5B模型）在该任务上都优于GPT-4.1。

Conclusion: 该工作成功开发并提供了一种将LLM适应于小众编程语言Q的综合性、开源方法（包括模型、代码和数据），证明了通过特定领域的适应性训练，LLMs在数据稀缺任务上的性能可显著超越通用模型。所提出的方法具有广泛适用性，可推广到其他类似任务。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [199] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 本研究将神经网络中隐藏恶意行为的检测问题框架为对抗性游戏，其中红队训练模型，蓝队尝试识别受损模型，发现对抗性攻击方法在CNNs上表现出色，并指出LLMs的审计需特定提示。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏先验知识和潜在的对抗性混淆，检测神经网络中的隐藏行为是一个重大挑战。本研究旨在解决如何在信息有限的情况下识别被恶意行为感染的模型。

Method: 研究采用对抗性游戏框架：红队训练两个相似模型（一个良性，一个包含隐藏有害行为），蓝队尝试识别受损模型。实验在CNNs上进行，蓝队策略包括高斯噪声分析、模型差异分析、集成梯度和对抗性攻击，并考虑红队提供的不同程度提示。对LLMs，则探讨了如何将CNNs的发现迁移，并强调了有效审计需要关于非期望分布的提示。

Result: 对于CNNs，基于对抗性攻击的方法在有提示的情况下实现了高准确率（100%正确预测），而其他技术表现各异。对于LLMs，研究发现直接迁移CNNs的方法有限，有效的LLM审计方法需要关于非期望分布的提示，然后可用于黑盒和开源方法进一步探测模型以揭示其未对齐行为。

Conclusion: 对抗性攻击结合提示在检测CNNs隐藏行为方面显示出巨大潜力。对于LLMs，有效的审计需要关于非期望行为的明确提示。本研究开源了审计游戏，旨在促进更好的审计方法设计。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [200] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 动态稀疏化方法（如GMP和SET）能有效提升多任务强化学习（MTRL）中代理的适应性，缓解塑性损失，并改善整体性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中存在塑性损失（适应能力随训练下降）的挑战，在需要高表示灵活性的多任务强化学习（MTRL）中，这一问题尤为关键。

Method: 系统性地探究了稀疏化方法（特别是渐进幅度剪枝GMP和稀疏演化训练SET）如何增强塑性并改善MTRL代理的性能。研究在不同MTRL架构和标准化基准上进行评估，并与密集基线及其他塑性增强方法进行比较。

Result: 结果表明GMP和SET有效缓解了塑性退化的关键指标，如神经元休眠和表征崩溃。这些塑性提升通常与多任务性能的增强相关，稀疏代理常优于密集代理并与显式塑性干预方法效果相当。

Conclusion: 研究揭示了塑性、网络稀疏性和MTRL设计之间的相互作用，强调动态稀疏化是开发更具适应性MTRL系统的强大但具有情境敏感性的工具。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [201] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文回顾了共形预测器（Conformal Predictors）在可信AI中的潜力，探讨其如何超越传统用途，解决泛化风险和AI治理等问题，并展示其在校准预测和偏差识别与缓解中的应用。


<details>
  <summary>Details</summary>
Motivation: 共形预测器因其提供具有保证置信水平的集合预测而成为机器学习中不确定性量化的主流方法。其可靠性使其对构建可信AI至关重要。本文旨在深入探讨共形预测器如何超越其边际有效性，为可信AI的更广泛挑战（如泛化风险和AI治理）做出贡献。

Method: 本文通过文献综述（review）的方式进行，并辅以实验和实例来展示共形预测器的应用潜力。

Result: 研究结果展示了共形预测器作为一种校准良好的预测器的能力，以及其在识别和缓解机器学习模型中偏差方面的有效性。

Conclusion: 共形预测器在促进可信AI发展方面具有显著潜力，不仅限于提供边际有效性，还能有效解决泛化风险和AI治理等复杂问题，并支持偏差的识别与缓解。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [202] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 该论文提出了QuiZSF，一个将检索增强生成（RAG）引入时间序列预训练模型（TSPMs）的轻量级框架，旨在通过动态整合外部知识，提升零样本时间序列预测（ZSF）在数据稀缺场景下的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 零样本时间序列预测（ZSF）在数据稀缺场景中至关重要，但传统模型难以应对。现有时间序列预训练模型（TSPMs）虽然在ZSF中表现良好，但缺乏动态整合外部知识的能力。因此，研究旨在将检索增强生成（RAG）与TSPMs结合，以弥补这一缺陷并增强ZSF能力。

Method: 本文提出了QuiZSF框架，该框架结合了高效检索、表示学习和模型自适应：
1.  **ChronoRAG Base (CRB)**：构建分层树状结构，用于可伸缩的时间序列存储和领域感知检索。
2.  **Multi-grained Series Interaction Learner (MSIL)**：用于提取细粒度和粗粒度关系特征。
3.  **Model Cooperation Coherer (MCC)**：开发一个双分支模块，将检索到的知识与基于非LLM和基于LLM的两种TSPMs进行对齐。

Result: QuiZSF在分别使用基于非LLM和基于LLM的TSPMs作为基础模型时，在75%和87.5%的预测设置中排名第一，超越了当代基线，同时在内存和推理时间方面保持了高效率。

Conclusion: QuiZSF通过有效整合RAG与TSPMs，显著提升了零样本时间序列预测的性能和效率，成功解决了现有模型在动态整合外部知识方面的不足，为数据稀缺场景下的时间序列预测提供了强大的解决方案。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [203] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 本文提出Cls-unbias模型，通过引入类别不等式损失和类别加权优化，有效缓解了医学诊断中存在的类别特征偏差和类别不平衡问题，从而提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学诊断可能因偏差而失败，特别是模型可能依赖于仅与部分类别强相关的特征（即类别特征偏差），导致性能偏差和对其他类别的泛化能力差。研究旨在训练一个能够同时缓解类别不平衡和类别特征偏差的无偏模型。

Method: ['提出一种类别不等式损失（class-wise inequality loss），旨在促进正类和负类样本的分类损失贡献相等。', '提出优化一个类别组分布鲁棒优化目标（class-wise group distributionally robust optimization objective），这是一个类别加权的训练目标，通过上调表现不佳的类别，以增强在类别不平衡情况下不等式损失的有效性。']

Result: ['通过合成数据集和真实世界数据集，经验性地证明了类别特征偏差会对模型性能产生负面影响。', '所提出的方法有效缓解了类别特征偏差和类别不平衡，从而提高了模型的泛化能力。']

Conclusion: 本研究提出的Cls-unbias模型能够有效解决医疗诊断中的类别特征偏差和类别不平衡问题，显著提升模型的泛化能力。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [204] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: AMFT提出一种新颖的单阶段LLM微调算法，通过元梯度自适应权重控制器动态平衡SFT和RL，实现SOTA性能和卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM微调通常采用SFT后RL的两阶段流程，存在灾难性遗忘和模仿与探索次优权衡的问题。现有单阶段方法缺乏动态平衡SFT和RL的原则性机制。

Method: 将SFT和RL重新定义为互补的隐式和显式奖励信号。提出Adaptive Meta Fine-Tuning (AMFT) 算法，通过一个元梯度自适应权重控制器，将SFT-RL平衡作为可学习参数，动态优化以最大化长期任务性能。该方法通过策略熵进行正则化，自主发现有效训练课程。

Result: AMFT在数学推理、抽象视觉推理和视觉语言导航等基准测试中持续达到最先进水平（SOTA），并在分布外（OOD）任务上展现出卓越的泛化能力。消融研究和训练动态分析证实，元学习控制器对于AMFT的稳定性、样本效率和性能至关重要。

Conclusion: AMFT为LLM对齐提供了一种更具原则性和更有效的新范式。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [205] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [206] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 针对遥感多模态学习模型解释性不足的问题，本研究提出一种基于多任务学习的新方法，将某些模态作为辅助预测目标，从而在保持性能的同时提升模型可解释性并应对数据稀缺。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的多模态学习网络能利用数据多样性提升模型性能，但其复杂性牺牲了模型可解释性。本研究旨在探索如何通过多任务学习利用模态来内在地解释模型行为。

Method: 本研究提出一种多任务学习方法，将特定模态作为除主要任务之外的额外预测目标（辅助任务），而非额外的输入。该方法依赖于卫星输入数据本身的丰富信息内容。

Result: 1. 部署优势：在模型部署时，额外模态（作为目标）无需收集，利于应对数据稀缺。2. 性能：模型性能与多模态基线相当，在某些情况下表现更优。3. 解释性：可以通过分析模型在辅助任务上的行为来解释主任务的预测错误。该方法已在分割、分类和回归三类数据集上验证了效率。

Conclusion: 所提出的多任务学习方法有效地利用模态作为辅助目标，为遥感模型提供了内在解释能力。它在数据稀缺、性能保持和错误解释方面均具有优势，并在多种任务上展现了高效性。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [207] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 本文提出一个基于结构保持降阶有限元模型的实时数字孪生框架，通过条件注意力机制和有限元外微分几何（FEEC）确保数值稳定性和守恒量精确性，实现复杂几何下的实时推理和传感器数据校准。


<details>
  <summary>Details</summary>
Motivation: 研究旨在构建能够进行闭环推理、与传感器数据校准并保证数值稳定性和守恒量精确性的实时数字孪生。

Method: 该方法基于结构保持降阶有限元模型，并以潜在变量Z为条件。它利用条件注意力机制在有限元外微分几何（FEEC）框架内学习降阶有限元基和非线性守恒律。该框架以非侵入式方式与传统有限元机制接口，支持对参数变量的实时校准，并处理复杂几何。

Result: 该方法在复杂几何和稀疏数据（25次LES模拟）下实现了精确预测，包括捕获湍流过渡。它实现了约0.1秒的实时推理，相对于LES有3.1x10^8倍的速度提升。基准测试包括对流扩散、激波流体动力学、静电学以及复杂的电池热失控问题。

Conclusion: 该框架成功构建了实时、精确且物理守恒的数字孪生，即使在数据稀疏的情况下也能保持数值适定性和守恒量的精确性，并在计算效率上取得显著提升，适用于复杂工程问题。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [208] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: 为解决电池设计验证耗时耗力问题，本文提出Discovery Learning (DL) 科学机器学习范式，通过整合多种学习技术，无需原型制作即可快速准确预测电池寿命，相较于工业实践大幅节省时间和能源。


<details>
  <summary>Details</summary>
Motivation: 电池等复杂物理系统的新设计验证速度对于加速技术创新至关重要。然而，电池研发受到评估新设计候选方案所需的高昂时间与能源成本（尤其在原型制作和寿命测试阶段）的严重制约。尽管数据驱动的电池寿命预测有所进展，但现有方法依赖目标设计的标签数据，且需在原型制作后才能进行可靠预测，远不能满足电池设计所需的高效率反馈。

Method: 本文引入了Discovery Learning (DL) 方法，这是一种科学机器学习范式。它借鉴教育心理学中的学习理论，将主动学习、物理引导学习和零样本学习整合到类人推理循环中。DL能够从历史电池设计中学习，并主动减少对原型制作的需求，从而无需额外数据标记即可实现对未观察到的材料-设计组合的快速寿命评估。

Result: 为验证DL，研究使用了123个工业级大型锂离子软包电池，涵盖8种材料-设计组合和多种循环协议。仅使用小容量圆柱形电池的公共数据集进行训练，DL在预测未知设备变异下的平均循环寿命时，实现了7.2%的测试误差。与工业实践相比，这带来了98%的时间节省和95%的能源节省。

Conclusion: 这项工作强调了从历史设计中发现洞察力，以指导和加速下一代电池技术开发的潜力。DL代表了高效数据驱动建模的关键进展，有助于实现机器学习在加速科学发现和工程创新方面的承诺。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [209] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 提出UniMove，一个统一模型，用于多城市人体移动预测，通过通用空间表示和异构模式建模，显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 人体移动预测对城市规划等至关重要，但面临内在随机性、非均匀时间间隔、复杂模式以及不同城市结构导致的异构性挑战，且现有方案通常需为每座城市单独训练模型。

Method: 提出UniMove统一模型，旨在解决跨城市通用空间表示和异构移动模式建模。采用轨迹-位置双塔架构：位置塔用于通用空间编码，轨迹塔用于序列移动建模。同时，设计MoE Transformer块以自适应处理不同移动模式。

Result: 通过在多个城市数据集上的广泛实验，UniMove通过多城市数据联合训练和相互增强，显著提高了移动预测精度超过10.2%，验证了其作为统一模型的有效性。

Conclusion: UniMove是实现人体移动领域统一基础模型的重要进展。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [210] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 首次为Tsetlin机器（TM）建立了特征选择（FS）的综合基线，评估了多种FS技术（包括新提出的TM内部评分器），发现TM内部评分器表现良好且高效。


<details>
  <summary>Details</summary>
Motivation: 特征选择对提高模型可解释性、降低复杂性和增强准确性至关重要。Tsetlin机器虽提供可解释的基于子句的学习，但缺乏成熟的特征重要性评估工具。

Method: 适配并评估了一系列FS技术用于Tsetlin机器，包括经典过滤方法、嵌入式方法、源自神经网络的事后解释方法（如SHAP和LIME），以及基于TM子句权重和Tsetlin自动机状态的新型嵌入式评分器。在12个数据集上通过Remove and Retrain (ROAR) 和 Remove and Debias (ROAD) 等评估协议进行基准测试。

Result: TM内部评分器表现出竞争力，并能利用子句的可解释性揭示交互特征模式。更简单的TM专用评分器能以显著降低的计算成本实现相似的精度保留。

Conclusion: 本研究为Tsetlin机器中的特征选择建立了首个全面的基线，为开发专门的TM可解释性技术铺平了道路。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [211] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 本文研究多专家人机协作分类中的实例特定专家子集选择问题，提出一种利用保形预测集的贪婪算法，旨在提升决策支持系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重于单专家人机协作，但实际应用中可能存在多位人类专家。研究动机在于探索如何从多专家池中为特定实例选择最相关的专家子集，以期通过人机协作进一步提升分类性能。

Method: 本文提出一种基于保形预测集的贪婪算法，用于识别并选择针对特定实例的最优专家子集。该算法利用保形预测集来指导专家选择过程，以融合所选专家的预测进行分类。

Result: 基于CIFAR-10H和ImageNet-16H数据集的真实专家预测仿真研究表明，所提出的贪婪算法能够识别出接近最优的专家子集，并且与朴素的人类子集选择方法相比，显著提高了多专家协作的分类性能。

Conclusion: 结合保形预测集与贪婪算法进行实例特定的专家子集选择，能够有效优化多专家人机协作决策支持系统的性能，为提升复杂分类任务中的人机智能协同提供了有效途径。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [212] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 本文提出TLCCSP框架，通过整合时滞交叉相关序列显著提高时间序列预测精度，并利用对比学习实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测模型常忽略序列间重要的时滞交叉相关性，导致预测精度受限，而准确预测对决策和风险管理至关重要。

Method: 提出时滞交叉相关序列预测框架（TLCCSP），该框架采用序列移位动态时间规整（SSDTW）算法捕获时滞相关性，并利用基于对比学习的编码器高效近似SSDTW距离，以提升预测性能。

Result: 在气象数据集中，SSDTW使MSE降低16.01%，对比学习编码器（CLE）进一步降低17.88%。在股票数据集中，SSDTW使MSE降低9.95%，CLE降低6.13%。在房地产数据集中，SSDTW和CLE分别使MSE降低21.29%和8.62%。此外，对比学习方法将SSDTW的计算时间减少了约99%。

Conclusion: TLCCSP框架通过有效整合时滞交叉相关性显著提高了时间序列预测准确性，且对比学习的应用大幅提升了计算效率和可扩展性，使其适用于多种时间序列预测任务的实时应用。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [213] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 在自动驾驶中，从大规模离线数据学习鲁棒驾驶策略面临挑战。研究表明，行为克隆（BC）效果不佳，但通过应用最先进的离线强化学习（CQL），能从相同数据中学习到显著更鲁棒的策略，并在大规模评估中大幅优于BC基线。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中从大规模真实世界数据集学习鲁棒驾驶策略是核心挑战，因为在线数据收集既不安全也不切实际。传统的行为克隆（BC）方法训练出的策略脆弱，在实际执行中容易出现错误累积。

Method: 本研究首先开发了一系列复杂的行为克隆（BC）基线，包括基于Transformer的模型。随后，将最先进的离线强化学习算法——保守Q学习（CQL）应用于相同的数据和架构，并设计了精细的奖励函数，使CQL智能体能够学习保守价值函数以恢复错误并避免分布外状态。

Result: 尽管复杂的BC模型实现了低模仿损失，但在长周期模拟中仍然失败。通过应用CQL，在Waymo开放运动数据集的1,000个未见场景中，最终的CQL智能体比最强的BC基线成功率提高了3.2倍，碰撞率降低了7.4倍。

Conclusion: 研究证明，离线强化学习方法对于从静态专家数据中学习鲁棒、长周期的驾驶策略至关重要。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [214] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 提出一种名为IGND-MoE的阶段感知混合专家模型，用于克服神经退行性疾病进展建模中数据稀疏性和机制复杂性的挑战，揭示了不同阶段的主导病理机制。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病的长期进展模型面临两大挑战：1) 纵向数据稀疏且获取不规律；2) 传统模型假定病理机制在疾病进展中固定不变，无法捕捉跨脑区和疾病阶段的复杂相互作用。

Method: 提出一个 novel 阶段感知混合专家 (MoE) 框架，通过时间依赖的专家权重建模不同病理机制在不同疾病阶段的主导作用。数据层面，使用迭代双重优化方法从不规则快照估计观测时间，构建队列级进展轨迹。模型层面，引入非均匀图神经网络扩散模型 (IGND) 增强空间组件，允许扩散率随节点状态和时间变化；并引入局部神经反应模块捕捉标准过程之外的复杂动力学。最终形成 IGND-MoE 模型。

Result: IGND-MoE 模型能动态整合组件，理解阶段特异性病理机制如何导致进展。阶段性权重提供了新的临床洞察，与现有文献一致，表明图相关过程在早期阶段影响更大，而其他未知物理过程在后期阶段占主导地位。

Conclusion: IGND-MoE 模型提供了一种原则性方法来理解阶段特异性病理机制对神经退行性疾病进展的贡献，并揭示了与现有文献相符的临床洞察。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [215] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: 本文提出了OTAKNet，一种创新的在线学习自适应卡尔曼滤波方法，它利用最优传输解决现实世界中噪声统计漂移导致的性能下降问题，实现了无需真实标签或重新训练的在线自适应，并在有限训练数据下表现卓越。


<details>
  <summary>Details</summary>
Motivation: 基于学习的滤波方法在非线性动力系统中表现良好，但在实际部署中，环境因素（如风速变化或电磁干扰）会导致未观测的噪声统计漂移，严重降低现有学习方法的性能。

Method: 本文提出了OTAKNet，这是首个解决学习自适应卡尔曼滤波中噪声统计漂移的在线方案。与现有离线微调方法不同，OTAKNet通过一步预测测量似然将状态估计与漂移关联起来，并利用最优传输（OT）进行处理。这利用了OT的几何感知成本和稳定梯度，实现了无需真实标签或重新训练的完全在线自适应。

Result: OTAKNet在合成数据和真实世界NCLT数据集上都展示了出色的性能，尤其是在训练数据有限的情况下。它优于经典的基于模型的自适应卡尔曼滤波和离线学习滤波方法。

Conclusion: OTAKNet成功解决了学习自适应卡尔曼滤波中噪声统计漂移的挑战，提供了一种无需真实标签、无需重新训练的在线自适应解决方案，在实际应用中，尤其是在数据受限的环境下，具有显著优势。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [216] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: LLM知识蒸馏技术存在从教师模型向学生模型传递成员和记忆隐私风险的问题，且风险程度因蒸馏方法和组件而异。


<details>
  <summary>Details</summary>
Motivation: 为降低大型语言模型（LLM）的计算需求，知识蒸馏（KD）被广泛应用。然而，当教师模型基于私有数据训练时，学生模型可能继承其隐私。本研究旨在系统地评估LLM知识蒸馏中固有的成员和记忆隐私风险。

Method: 本研究系统地分析并调查了六种LLM知识蒸馏技术中的成员和记忆隐私风险。实验在包含七项NLP任务的指令微调设置下进行，使用了GPT-2、LLAMA-2和OPT三种教师模型家族以及不同大小的学生模型。研究还系统分析了KD目标函数、学生训练数据和NLP任务等关键组件如何影响隐私风险，并表征了逐块隐私风险。

Result: 所有现有LLM知识蒸馏方法都会将成员和记忆隐私风险从教师模型传递给学生模型。隐私风险的程度因不同的KD技术而异。记忆隐私和成员隐私风险之间存在显著差异。此外，逐块隐私风险的差异也很大。

Conclusion: LLM知识蒸馏中普遍存在成员和记忆隐私泄露风险，这些风险的程度受蒸馏技术和关键组件的影响，且记忆和成员隐私风险之间存在显著差异。这强调了在LLM知识蒸馏实践中，隐私保护是需要重点关注和进一步研究的关键问题。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [217] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 本文提出一种基于$IA^3$和电路定位的“先遗忘后学习”策略，用于LLMs的精确知识编辑，有效更新冲突事实，减轻灾难性遗忘，并实现软遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在动态知识更新方面面临挑战，特别是当新信息与既有事实冲突时，会导致模型难以采纳新事实，并严重遗忘不相关知识。

Method: 本文提出一种新颖的“先遗忘后学习”两阶段知识编辑策略，利用参数高效微调（PEFT）技术$IA^3$。该方法关键在于先进行电路定位，识别并针对编码冲突事实的特定内部组件。

Result: 在Phi-3-mini-4k-instruct上，该方法对新事实达到近乎完美的准确率（98.50%），并有效抑制了原有冲突事实（96.00%的遗忘率）。它表现出前所未有的知识定位能力（72.00%的F_control准确率），显著减轻了灾难性遗忘。定性分析揭示了“软遗忘”机制，即原有知识被抑制但仍处于潜在状态且可条件性访问。

Conclusion: 这些发现代表了在紧凑型LLMs中实现精确、局部化和安全知识管理方面的重要进展。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [218] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 提出一个结合Transformer和Autoencoder的混合框架及信任分数方法，用于在线概念漂移检测，在航空乘客数据集上实现了比现有方法更早、更灵敏的漂移检测。


<details>
  <summary>Details</summary>
Motivation: 应用机器学习中，概念漂移会显著降低模型性能。现有检测方法通常反应迟缓，对早期漂移不敏感，无法满足在线检测需求。

Method: 本研究提出一个结合Transformer和Autoencoder的混合框架，用于建模复杂时间动态和在线漂移检测。核心是构建了一个“信任分数”方法，整合了统计漂移指标（PSI, JSD）、重建误差（Transformer-AE误差）、预测不确定性、规则违规和分类器误差趋势等信号。

Result: 在注入合成漂移的航空乘客时间序列数据集上，该模型整体表现和在不同检测阈值下，相比基线方法都能更早、更灵敏地检测到概念漂移，并提供了更佳的建模能力，减少了错误率和逻辑违规。

Conclusion: 本研究开发了一个鲁棒的框架，能可靠地实时监控概念漂移，为应用机器学习提供了高效的漂移检测解决方案。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [219] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [220] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 针对fMRI数据中脑功能连接动态的建模挑战，本文提出了BrainATCL框架，通过自适应时间窗和GINE-Mamba2骨干网络捕获长程时空依赖，并结合生物学属性，在连接预测和年龄估计任务中展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 功能磁共振成像（fMRI）信号在不同脑区之间存在高度结构化的瞬时同步与去同步现象，这些功能连接动态可能与行为和神经精神疾病相关。然而，传统的图神经网络（GNNs）在捕捉动态fMRI数据中的长程时间依赖性方面存在困难。

Method: 本文提出了BrainATCL，一个无监督、非参数的自适应时间脑连接学习框架。该方法根据新增边的速率动态调整每个快照的回溯窗口，并使用GINE-Mamba2作为骨干网络来编码图序列，以学习动态功能连接的时空表示。为增强空间建模能力，模型还融入了脑结构和功能相关的边属性（如左右半球身份和子网络成员）。研究使用了来自人类连接组计划的1,000名参与者的静息态fMRI数据。

Result: BrainATCL在功能连接预测和年龄估计两项任务上均表现出卓越的性能和强大的泛化能力，包括在跨会话预测场景中也表现良好。

Conclusion: BrainATCL框架有效解决了动态fMRI数据中长程时间依赖性建模的难题，通过创新的自适应时间窗和结合生物学属性的方法，在功能连接预测和年龄估计方面取得了显著成果，为深入理解大脑瞬时神经状态和网络重构提供了新的工具。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [221] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 本文提出一种新的多示例学习（MIL）机器学习方法，旨在提高假设检验中参数预测的精度和判别力，并系统性地减少模型误差。该方法通过理论分析和在大型强子对撞机（LHC）数据上对标准模型有效场论（SMEFT）威尔逊系数的约束应用，展示了其在特定条件下提取理论最大费舍尔信息的能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在某些假设检验问题中难以提供精确预测，且判别力不足，尤其在面对极具挑战性的场景时。因此，需要开发一种能提升预测精度、增加模型判别力并系统性减少误差的新方法。

Method: 本文提出一种基于多示例学习（MIL）的机器学习方法。研究者从数学角度阐述了MIL相比单示例学习的优越预测能力，并通过分析MIL模型随实例数量变化的缩放行为来支持理论主张。作为具体应用，该方法被用于利用大型强子对撞机（LHC）亚原子粒子碰撞事件的运动学信息，约束标准模型有效场论（SMEFT）的威尔逊系数。

Result: 所提出的MIL方法能够为假设检验问题中的参数提供更精确的预测，并显著提高ML模型的判别力，即使在传统分类器难以有效预测的挑战性场景下。该方法能系统性地减少ML模型的预测误差。研究显示，在特定情况下，有可能从数据集中提取理论最大费舍尔信息。在SMEFT威尔逊系数约束应用中取得了成功。

Conclusion: 所提出的基于MIL的机器学习方法在假设检验问题中表现出卓越的预测精度和判别力，能够有效降低模型误差，并在物理学应用中成功约束了SMEFT参数，展现了其提取数据集最大潜在信息的能力。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [222] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: 本文提出LOGIC框架，利用大型语言模型（LLM）为图神经网络（GNN）在文本属性图上的预测生成忠实且可解释的自然语言解释和简洁的解释子图。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）在处理文本属性图时表现强大，但其本身缺乏可解释性。现有解释方法在生成可解释、细粒度的解释时，尤其当节点属性包含丰富的自然语言时，表现不佳。

Method: 引入LOGIC，一个轻量级、后验的框架。它将GNN节点嵌入投影到LLM嵌入空间，并构建混合提示（将软提示与图结构中的文本输入交织），使LLM能够理解GNN的内部表示，从而生成自然语言解释和简洁的解释子图。

Result: 在四个真实世界的文本属性图数据集上进行实验表明，LOGIC在忠实度和稀疏性之间取得了良好的权衡，并显著提升了以人为中心的指标，如洞察力。

Conclusion: LOGIC通过将GNN内部表示与人类推理对齐，为基于LLM的图学习可解释性设定了新方向。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [223] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 本文提出一种基于时空图神经网络的性能预测算法，用于解决分布式后端系统多级服务调用结构中的性能波动预测难题。


<details>
  <summary>Details</summary>
Motivation: 预测具有多级服务调用结构的分布式后端系统的性能波动，这是一个挑战。

Method: 将系统状态抽象为图序列，整合服务节点运行时特征和调用关系。模型通过图卷积网络提取服务拓扑的高阶依赖信息，并利用门控循环网络捕获性能指标的时间动态演变，同时引入时间编码机制增强对非平稳时间序列的表示能力。模型采用端到端训练。

Result: 在大型公共集群数据集上，该模型在MAE、RMSE和R2等关键指标上优于现有代表性方法，并在不同负载强度和结构复杂度下保持强大的鲁棒性。

Conclusion: 所提出的模型在后端服务性能管理任务中具有实际应用潜力。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [224] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE将在线强化学习中的实时标量反馈转化为偏好数据，通过处理反馈不一致性并聚合用户反馈，显著提升了奖励模型的学习效率和泛化能力，甚至超越了专家设计的稠密奖励。


<details>
  <summary>Details</summary>
Motivation: 在强化学习任务目标难以通过稠密奖励函数明确时，人类反馈至关重要。现有方法依赖离线轨迹比较，不适用于在线学习。尽管实时标量反馈被用于在线场景，但其噪声和不一致性限制了学习奖励模型的准确性和泛化能力。

Method: 提出Pref-GUIDE框架，将实时标量反馈转换为基于偏好的数据，以改进奖励模型学习，支持持续策略训练。具体包括：Pref-GUIDE Individual通过在短期窗口内比较智能体行为并过滤模糊反馈来缓解时间不一致性；Pref-GUIDE Voting通过聚合用户群体的奖励模型来形成共识偏好，进一步增强鲁棒性。

Result: 在三个具有挑战性的环境中，Pref-GUIDE显著优于基于标量反馈的基线方法。其中，Pref-GUIDE Voting变体甚至超越了专家设计的稠密奖励。

Conclusion: Pref-GUIDE通过将标量反馈重新构建为结构化偏好数据并结合群体反馈，为在线强化学习中有效利用人类输入提供了一种可扩展且有原则的方法。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [225] [Iris RESTful Server and IrisTileSource: An Iris implementation for existing OpenSeaDragon viewers](https://arxiv.org/abs/2508.06615)
*Ryan Erik Landvater,Navin Kathawa,Mustafa Yousif MD,Ulysses Balis MD*

Main category: cs.NI

TL;DR: 该研究开发了一个名为Iris RESTful Server的低开销HTTP服务器，用于高效、安全地流式传输Iris文件扩展（IFE）格式的全玻片图像（WSI），解决了现有静态HTTP服务器无法直接流式传输高分辨率图像子区域的问题，并兼容DICOMweb和OpenSeaDragon。


<details>
  <summary>Details</summary>
Motivation: Iris文件扩展（IFE）是一种低开销、高性能的WSI格式，但传统的静态HTTP文件服务器无法原生流式传输其高分辨率图像的子区域。当前的WSI查看器系统多将WSI文件转换为DZI格式以兼容简单的静态HTTP服务器，这是一种局限。因此，需要一个能直接、高效、安全地流式传输IFE文件的解决方案。

Method: 开发了Iris RESTful Server，一个基于C++、使用Boost Beast HTTP和Asio网络库构建的低开销HTTP服务器。该服务器具有与DICOMweb WADO-RS API原生兼容的RESTful API。此外，还开发并合并了一个新的OpenSeaDragon TileSource，使其兼容Iris RESTful API，并设计为安全的跨域资源共享微服务。

Result: 测试表明，单个服务器实例在私有网络中每秒可处理超过5000个瓦片请求，中位数延迟为21毫秒。新的OpenSeaDragon TileSource使WSI查看器系统能够简单、即时地将DZI图像替换为IFE图像。

Conclusion: Iris RESTful Server成功解决了IFE文件的流式传输限制，提供了高性能和高安全性，并实现了与现有OpenSeaDragon生态系统的无缝集成。它通过消除DZI转换的需要，简化了图像管理，并改善了病理学家的图像渲染体验，从而加速了WSI网络查看器的开发。

Abstract: The Iris File Extension (IFE) is a low overhead performance-oriented whole
slide image (WSI) file format designed to improve the image rendering
experience for pathologists and simplify image management for system
administrators. However, static hypertext transfer protocol (HTTP) file servers
cannot natively stream subregions of high-resolution image files, such as the
IFE. The majority of contemporary WSI viewer systems are designed as
browser-based web applications and leverage OpenSeaDragon as the tile-based
rendering framework. These systems convert WSI files to Deep Zoom Images (DZI)
for compatibility with simple static HTTP file servers. In order to address
this limitation, we have developed the Iris RESTful Server, a low-overhead HTTP
server with a RESTful API that is natively compatible with the DICOMweb WADO-RS
API. Written in C++ with Boost Beast HTTP and Asio networking libraries atop
the public IFE libraries, the server offers both security and high performance.
Testing shows that a single instance can handle over 5000 tile requests per
second with a median latency of 21 ms on a private network. We also developed
and merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful
API, into the next OpenSeaDragon release, enabling simple and immediate drop-in
replacement of DZI images within WSI viewer stacks. Designed as a secure
cross-origin resource sharing microservice, this architecture includes detailed
deployment instructions for new or existing WSI workflows, and the public
examples.restful.irisdigitialpathology.org subdomain is provided as a
development tool to accelerate WSI web viewer development.

</details>


### [226] [Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach](https://arxiv.org/abs/2508.06616)
*Md Arafat Habib,Medhat Elsayed,Yigit Ozcan,Pedro Enrique Iturria-Rivera,Majid Bavand,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 本文提出了一种分层式意图驱动网络（IDN）架构，将生成式AI（GenAI）整合到意图处理、验证和执行的各个阶段，并通过基于Mamba的案例研究，证明其在6G异构网络中优于传统IDN的性能提升。


<details>
  <summary>Details</summary>
Motivation: 6G移动网络日益复杂，需要高级自动化管理。意图驱动网络（IDN）能实现高层意图到策略的转换，而大型语言模型（LLMs）可增强其智能自动化能力。鉴于现有GenAI在IDN中多仅用于意图处理，亟需探索其在IDN全流程的整合应用。

Method: 本文首先对基于LLM的IDN架构进行了全面综述。在此基础上，提出了一种分层学习使能的IDN架构，该架构将生成式AI（GenAI）整合到意图处理、意图验证和意图执行三个关键阶段。通过一个基于最新GenAI架构Mamba的案例研究，演示了所提架构的有效性。

Result: 案例研究表明，所提出的GenAI驱动分层IDN架构通过智能自动化显著提升了网络性能，并超越了传统的意图驱动网络架构。

Conclusion: 结合GenAI的分层IDN架构能够有效增强6G网络的管理，实现更智能、更高效的自动化，并在性能上优于现有IDN方法。

Abstract: With the emergence of 6G, mobile networks are becoming increasingly
heterogeneous and dynamic, necessitating advanced automation for efficient
management. Intent-Driven Networks (IDNs) address this by translating
high-level intents into optimization policies. Large Language Models (LLMs) can
enhance this process by understanding complex human instructions to enable
adaptive, intelligent automation. Given the rapid advancements in Generative AI
(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated
Radio Access Network (RAN) environments is both timely and critical. This
article provides such a survey, along with a case study on a hierarchical
learning-enabled IDN architecture that integrates GenAI across three key
stages: intent processing, intent validation, and intent execution. Unlike most
existing approaches that apply GenAI in the form of LLMs for intent processing
only, we propose a hierarchical framework that introduces GenAI across all
three stages of IDN. To demonstrate the effectiveness of the proposed IDN
management architecture, we present a case study based on the latest GenAI
architecture named Mamba. The case study shows how the proposed GenAI-driven
architecture enhances network performance through intelligent automation,
surpassing the performance of the conventional IDN architectures.

</details>


### [227] [THz/RF Multi-Hop Routing Throughput: Performance, Optimization, and Application](https://arxiv.org/abs/2508.06975)
*Zhengying Lou,Baha Eddine Youcef Belmekki,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 本文提出首个基于随机几何（SG）的太赫兹（THz）系统路由分析框架，并开发了分步优化方法以最大化吞吐量，其性能优于现有方法并接近理论上限。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信虽具高吞吐潜力，但面临严重的路径损耗问题，其有效性受质疑。因此，需要建立有效的分析框架和路由策略来克服这一挑战。

Method: 研究方法包括：1) 建立首个基于随机几何（SG）的太赫兹系统路由分析框架；2) 开发分步优化方法以最大化吞吐量，涵盖功率分配、中继选择和跳数设计；3) 在SG框架下推导出吞吐量和覆盖概率的解析表达式。

Result: 研究结果显示：1) 所提出的分步优化路由策略性能优于现有基于SG的方法，并接近理想上限；2) 对太赫兹和射频路由的吞吐量及覆盖性能进行了比较；3) 证明了所提出的分析框架和路由策略在系统参数设计和无人机网络中的应用价值。

Conclusion: 所提出的基于随机几何的分析框架和分步优化路由策略能有效提升太赫兹系统的吞吐量和覆盖性能，克服路径损耗问题，并为太赫兹系统设计及无人机网络等应用提供了有价值的工具和见解。

Abstract: Terahertz (THz) communication offers a promising solution for high-throughput
wireless systems. However, the severe path loss of THz signals raises concerns
about its effectiveness compared to radio frequency (RF) communication. In this
article, we establish the first stochastic geometry (SG)-based analytical
framework for routing in THz systems. We develop a stepwise optimization
approach to maximize throughput, including power allocation, relay selection,
and number of hops design. Analytical expressions for throughput and coverage
probability are derived under the SG framework, enabling low complexity and
scalable performance evaluation. Numerical results show that the proposed
stepwise-optimal routing strategies not only outperform existing SG-based
methods but also approach the ideal upper bound. Moreover, we compare the
throughput and coverage performance of THz and RF routing and demonstrate the
applications of the proposed analytical framework and routing strategies in
system parameter design and unmanned aerial vehicle networks.

</details>


### [228] [Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization](https://arxiv.org/abs/2508.07001)
*Myeung Suk Oh,Zhiyao Zhang,FNU Hairi,Alvaro Velasquez,Jia Liu*

Main category: cs.NI

TL;DR: 提出了一种基于完全去中心化多智能体强化学习的随机接入（RA）MAC协议，通过设备间共识信息交换和仅交换局部奖励的方式，显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 智能网络中随机接入MAC设计面临高冲突和公平性挑战。现有集中训练、去中心化执行（CTDE）的多智能体强化学习（MARL）方法因集中训练依赖和信息收集开销大，不适用于实际部署。

Method: 采用完全去中心化的MARL架构，策略学习通过设备间的共识信息交换实现，而非依赖集中任务。算法基于Actor-Critic (AC) 网络设计，仅交换局部奖励以最小化通信开销。此外，提供了全局收敛的理论证明。

Result: 数值实验表明，所提出的MARL算法与现有基线相比，能够显著提升随机接入网络的性能。

Conclusion: 本研究提出的去中心化MARL方法，通过创新的信息交换机制和理论收敛性保证，有效解决了随机接入MAC设计中的关键挑战，并为实际应用提供了更可行的解决方案。

Abstract: With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.

</details>


### [229] [ProtoScan: Measuring censorship in IPv6](https://arxiv.org/abs/2508.07194)
*Jack Wampler,Hammas Bin Tanveer,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 研究发现，IPv6审查与IPv4存在显著差异，且通常不如IPv4全面或可靠，为规避审查提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 互联网审查影响数十亿人，但现有测量多集中于IPv4。鉴于IPv6用户不断增长（谷歌数据显示超过三分之一用户拥有原生IPv6访问），理解IPv6对审查的影响至关重要。

Method: 对IPv4和IPv6上常见的审查协议（包括HTTP、DNS和TLS）进行了全面的全球审查测量，并比较了结果。

Result: 研究发现，各国在审查IPv6流量方面存在多项差异，包括对IPv6资源的审查方式以及在IPv6网络上部署的黑名单或技术。多数审查者具备一定的IPv6封锁能力，但与IPv4审查系统相比，其全面性和可靠性较低。

Conclusion: IPv6为审查规避研究提供了新的探索领域，可能带来规避审查的新方法。随着更多用户获得IPv6地址和网络，将需要利用IPv6技术和基础设施来绕过审查的工具。

Abstract: Internet censorship continues to impact billions of people worldwide, and
measurement of it remains an important focus of research. However, most
Internet censorship measurements have focused solely on the IPv4 Internet
infrastructure. Yet, more clients and servers are available over IPv6:
According to Google, over a third of their users now have native IPv6 access.
Given the slow-but-steady rate of IPv6 adoption, it is important to understand
its impact on censorship. In this paper, we measure and analyze how censorship
differs over IPv6 compared to the well-studied IPv4 censorship systems in use
today. We perform a comprehensive global study of censorship across an array of
commonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and
IPv6, and compare the results. We find that there are several differences in
how countries censor IPv6 traffic, both in terms of IPv6 resources, and in
where and what blocklists or technologies are deployed on IPv6 networks. Many
of these differences are not all-or-nothing: we find that most censors have
some capacity to block in IPv6, but are less comprehensive or less reliable
compared to their IPv4 censorship systems. Our results suggest that IPv6 offers
new areas for censorship circumvention researchers to explore, providing
potentially new ways to evade censors. As more users gain access to IPv6
addresses and networks, there will be a need for tools that take advantage of
IPv6 techniques and infrastructure to bypass censorship.

</details>


### [230] [Mind the IP Gap: Measuring the impact of IPv6 on DNS censorship](https://arxiv.org/abs/2508.07197)
*Ian Martiny,Hammas Bin Tanveer,Jack Wampler,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 研究发现全球IPv6 DNS审查普遍存在，但其策略不如IPv4审查有效和一致，为规避审查提供了新机会。


<details>
  <summary>Details</summary>
Motivation: 以往的审查研究主要集中在IPv4上，但随着IPv6的普及，其信息控制系统对IPv6流量的有效性尚不明确，因此需要对IPv6互联网上的DNS审查进行首次全球性测量。

Method: 利用一种新技术发现支持IPv6的开放解析器（及其对应IPv4地址），向全球DNS解析器发送超过2000万次A和AAAA DNS请求，测量解析器、网络和国家层面的屏蔽率，并分析被屏蔽域名的特性。

Result: 观察到几乎所有审查者都支持IPv6屏蔽，但其策略与IPv4不一致，且效果普遍不如其IPv4审查基础设施。许多审查者对IPv6审查的支持不到位。

Conclusion: 审查者可能需要投入额外资源以使IPv6审查达到与IPv4相同的水平。同时，这为审查规避研究人员提供了利用这些差异来规避检测和屏蔽的新机会。

Abstract: Internet censorship impacts large segments of the Internet, but so far, prior
work has focused almost exclusively on performing measurements using IPv4. As
the Internet grows, and more users connect, IPv6 is increasingly supported and
available to users and servers alike. But despite this steady growth, it
remains unclear if the information control systems that implement censorship
(firewalls, deep packet inspection, DNS injection, etc) are as effective with
IPv6 traffic as they are with IPv4. In this paper, we perform the first global
measurement of DNS censorship on the IPv6 Internet. Leveraging a recent
technique that allows us to discover IPv6-capable open resolvers (along with
their corresponding IPv4 address), we send over 20 million A and AAAA DNS
requests to DNS resolvers worldwide, and measure the rate at which they block,
at the resolver, network, and country level as well examine the characteristics
of blocked domains. We observe that while nearly all censors support blocking
IPv6, their policies are inconsistent with and frequently less effective than
their IPv4 censorship infrastructure. Our results suggest that supporting IPv6
censorship is not all-or-nothing: many censors support it, but poorly. As a
result, these censors may have to expend additional resources to bring IPv6
censorship up to parity with IPv4. In the meantime, this affords censorship
circumvention researchers a new opportunity to exploit these differences to
evade detection and blocking.

</details>


### [231] [The Search for Relevance: A Context-Aware Paradigm Shift in Semantic and Task-Oriented V2X Communications](https://arxiv.org/abs/2508.07394)
*Luca Lusvarghi,Javier Gozalvez,Baldomero Coll-Perales,Mohammad Irfan Khan,Miguel Sepulcre,Seyhan Ucar,Onur Altintas*

Main category: cs.NI

TL;DR: 本文提出了一种语义与任务导向的V2X通信范式，通过仅传输相关信息，将通信效率提升一倍，以应对6G时代的扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统侧重数据可靠及时传输，但6G驱动的社会面临扩展性挑战，需要新的通信范式，筛选传输内容。车联网（V2X）环境天然适合开发这种通信范式，因为CAVs是原生语义设备且情境信息丰富。

Method: 本文设想了一种联合语义与任务导向的通信范式，其中互联自动驾驶车辆（CAVs）仅传输对接收方有意义且基于通信上下文必要的信息。利用V2X领域的情境信息来估计信息对接收方的相关性。通过数值结果定量评估其潜在益处。

Result: 数值结果表明，通过专注于传输对目标接收方最相关的信息，语义与任务导向的V2X通信可以实现通信效率的两倍提升。

Conclusion: 语义与任务导向的V2X通信能显著提高V2X网络的扩展性，为未来6G通信系统提供新的解决方案，通过优化信息传输来应对高并发挑战。

Abstract: The design of communication systems has traditionally focused on the reliable
and timely delivery of data. However, the scalability challenges faced by the
evolution to a 6G-driven society demand new communication paradigms that
carefully curate the content being transmitted. This paper envisions a joint
semantic and task-oriented communication paradigm where Connected and
Autonomous Vehicles (CAVs) transmit only the information necessary to convey
the desired meaning that is relevant to the intended receivers based on the
communication context. The V2X domain offers a unique environment for the
development of the envisioned semantic and task-oriented communications
paradigm, as CAVs are native semantic devices, and the V2X domain is rich in
contextual information. This contextual information can be leveraged to
estimate the relevance that information may have for the intended receivers. We
illustrate and quantitatively evaluate the potential benefits of semantic and
task-oriented V2X communications. Numerical results show that by focusing on
the transmission of the most relevant information for the intended receivers,
semantic and task-oriented V2X communications can achieve a two-fold
improvement in communication efficiency, which will significantly benefit the
scalability of V2X networks.

</details>


### [232] [Unveiling IPv6 Scanning Dynamics: A Longitudinal Study Using Large Scale Proactive and Passive IPv6 Telescopes](https://arxiv.org/abs/2508.07506)
*Hammas Bin Tanveer,Wai Sun Chan,Ricky K. P. Mok,Sebastian Kappes,Philipp Richter,Oliver Gasser,John Ronan,Arthur Berger,kc Claffy*

Main category: cs.NI

TL;DR: 通过部署大型IPv6主动望远镜，该研究收集并分析了大规模IPv6扫描流量的来源、特征和策略。


<details>
  <summary>Details</summary>
Motivation: 开发和整合主动技术以吸引IPv6扫描流量，进而对其进行分析。

Method: 在一个生产ISP网络中部署了有史以来最大的IPv6主动望远镜。

Result: 在10个月内，从1.9k自治系统收集了超过6亿个未请求数据包。该研究表征了未请求流量的来源，评估了网络堆栈中五个主要特征的有效性，并推断了扫描仪目标地址的来源及其策略。

Conclusion: 成功吸引并分析了大规模IPv6扫描流量，揭示了扫描源的特点及扫描策略，为深入理解和防御IPv6扫描行为提供了宝贵洞察。

Abstract: We introduce new tools and vantage points to develop and integrate proactive
techniques to attract IPv6 scan traffic, thus enabling its analysis. By
deploying the largest-ever IPv6 proactive telescope in a production ISP
network, we collected over 600M packets of unsolicited traffic from 1.9k
Autonomous Systems in 10 months. We characterized the sources of unsolicited
traffic, evaluated the effectiveness of five major features across the network
stack, and inferred scanners' sources of target addresses and their strategies.

</details>


### [233] [Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach](https://arxiv.org/abs/2508.07578)
*Yu Gou,Tong Zhang,Jun Liu,Tingting Yang,Shanshan Song,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 本文针对非完美、能量受限水声传感器网络（IC-UASNs）中的公平有效通信与鲁棒性问题，提出一种基于分布式多智能体强化学习的半协作功率分配方法（SECOPA），以在应对节点故障和时变信道的同时，优化个体与全局性能。


<details>
  <summary>Details</summary>
Motivation: 在非完美、能量受限水声传感器网络中，存在意外节点故障和时变声学信道，且节点需满足QoS要求。然而，实现个体QoS可能干扰并发通信，导致个体QoS与全局公平有效通信难以权衡。传统的完全协作方法过度依赖其他节点的理性，在非完美条件下难以有效解决此问题。

Method: 本文提出一种半协作功率分配方法（SECOPA），该方法基于分布式多智能体强化学习（MARL）。该方法使每个智能节点能够独立决定传输功率，从而同时优化个体和全局性能。此外，开发了高级训练算法，在非完美环境中训练出鲁棒模型，以适应时变声学信道并处理意外节点故障。

Result: 数值结果验证了所提出SECOPA方法的有效性。

Conclusion: 所提出的SECOPA方法能够在非完美、能量受限水声传感器网络中实现公平有效的通信和鲁棒性，成功应对时变信道和意外节点故障，并有效平衡个体与全局性能目标。

Abstract: This paper investigates the fair-effective communication and robustness in
imperfect and energy-constrained underwater acoustic sensor networks
(IC-UASNs). Specifically, we investigate the impact of unexpected node
malfunctions on the network performance under the time-varying acoustic
channels. Each node is expected to satisfy Quality of Service (QoS)
requirements. However, achieving individual QoS requirements may interfere with
other concurrent communications. Underwater nodes rely excessively on the
rationality of other underwater nodes when guided by fully cooperative
approaches, making it difficult to seek a trade-off between individual QoS and
global fair-effective communications under imperfect conditions. Therefore,
this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that
achieves fair-effective communication and robustness in IC-UASNs. The approach
is distributed multi-agent reinforcement learning (MARL)-based, and the
objectives are twofold. On the one hand, each intelligent node individually
decides the transmission power to simultaneously optimize individual and global
performance. On the other hand, advanced training algorithms are developed to
provide imperfect environments for training robust models that can adapt to the
time-varying acoustic channels and handle unexpected node failures in the
network. Numerical results are presented to validate our proposed approach.

</details>


### [234] [Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL](https://arxiv.org/abs/2508.07604)
*Maryam Abbasalizadeh,Sashank Narain*

Main category: cs.NI

TL;DR: 本研究提出了一种基于深度强化学习（DRL）的框架，用于在动态、易受干扰的综合接入和回传（IAB）网络中进行联合链路调度和资源切片，显著提升了调度精度和吞吐量。


<details>
  <summary>Details</summary>
Motivation: IAB对密集5G及未来部署至关重要，尤其是在光纤回传不可行的毫米波频段。当前挑战在于如何有效管理动态且易受干扰的IAB网络中的链路调度与资源分配。

Method: 提出了一种新颖的深度强化学习（DRL）框架。该方法结合了贪婪的双深度Q网络（DDQN）调度器（用于根据流量和拓扑激活接入和回传链路）和一个多智能体DDQN分配器（用于跨网络切片的带宽和天线分配）。此去中心化方法能满足严格的天线约束并支持异构链路的并发调度。

Result: 在96种动态拓扑上的评估显示，该框架的调度准确率达到99.84%，吞吐量比基线提高了20.90%。

Conclusion: 该框架运行高效且适应性强，非常适合需要快速链路调度和自主回传协调的动态、资源受限的部署环境。

Abstract: Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond
deployments, especially in mmWave bands where fiber backhaul is infeasible. We
propose a novel Deep Reinforcement Learning (DRL) framework for joint link
scheduling and resource slicing in dynamic, interference-prone IAB networks.
Our method integrates a greedy Double Deep Q-Network (DDQN) scheduler to
activate access and backhaul links based on traffic and topology, with a
multi-agent DDQN allocator for bandwidth and antenna assignment across network
slices. This decentralized approach respects strict antenna constraints and
supports concurrent scheduling across heterogeneous links. Evaluations across
96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent
throughput improvement over baselines. The framework's efficient operation and
adaptability make it suitable for dynamic and resource-constrained deployments,
where fast link scheduling and autonomous backhaul coordination are vital.

</details>


### [235] [Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks](https://arxiv.org/abs/2508.07679)
*Tong Zhang,Yu Gou,Jun Liu,Shanshan Song,Tingting Yang,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 本文提出ICRL-JSA，一种基于深度多智能体强化学习的算法，通过联合链路调度和功率分配，解决水下无线传感器网络（UWSNs）中能量受限和节点故障导致的公平、高效、可靠通信问题。


<details>
  <summary>Details</summary>
Motivation: 水下无线传感器网络（UWSNs）面临严重的能量供应限制和意想不到的节点故障，导致通信难以实现公平、高效和可靠（FER）。

Method: 本文将FER通信问题建模为FERCOP优化问题，并提出ICRL-JSA来解决。ICRL-JSA是一种基于深度多智能体强化学习（MARL）的优化器，通过联合链路调度和功率分配实现自动化学习。它将深度Q网络融入到水下环境中，并引入一种先进的训练机制，以应对复杂的声学信道、有限能量供应和节点故障。

Result: 仿真结果表明，本文提出的ICRL-JSA方案及其先进的训练机制，相比于各种基准算法，在性能上表现出优越性。

Conclusion: ICRL-JSA能够有效实现能量受限且存在节点故障的水下无线传感器网络中的公平、高效和可靠通信，其性能优于现有方法。

Abstract: Underwater wireless sensor networks (UWSNs) stand as promising technologies
facilitating diverse underwater applications. However, the major design issues
of the considered system are the severely limited energy supply and unexpected
node malfunctions. This paper aims to provide fair, efficient, and reliable
(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).
Therefore, we formulate a FER-communication optimization problem (FERCOP) and
propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep
multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through
joint link scheduling and power allocation, which automatically learns
scheduling algorithms without human intervention. However, conventional RL
methods are unable to address the challenges posed by underwater environments
and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs
and propose an advanced training mechanism to deal with complex acoustic
channels, limited energy supplies, and unexpected node malfunctions. Simulation
results demonstrate the superiority of the proposed ICRL-JSA scheme with an
advanced training mechanism compared to various benchmark algorithms.

</details>


### [236] [An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study](https://arxiv.org/abs/2508.07778)
*Farhad Rezazadeh,Raymond Zhao,Jiongyu Dai,Amir Ashtari Gargari,Hatim Chergui,Lingjia Liu*

Main category: cs.NI

TL;DR: 本文提出RA-MAT模型，一种结合回声状态网络（ESN）和掩码自编码的Transformer，旨在高效处理6G O-RAN产生的高维、非平稳时序数据，并实现优异的KPI预测性能。


<details>
  <summary>Details</summary>
Motivation: 下一代开放无线接入网络（O-RAN）持续产生超高维、非平稳的时序数据（包括KPI和IQ样本），传统Transformer架构难以有效处理，面临严苛的延迟、能效和可扩展性挑战。

Method: 引入一种名为RA-MAT的储层增强掩码自编码Transformer。该模型结合回声状态网络（ESN）计算和掩码自编码，通过固定、随机初始化的ESN将时间片快速投影为动态嵌入，将Transformer的二次自注意力瓶颈转化为轻量级线性操作。这些嵌入驱动一个逐片掩码自编码器重构30%的随机掩码片，促使编码器从无标签数据中捕获局部动态和长程结构。自监督预训练后，RA-MAT通过浅层任务头进行微调，同时冻结储层和大部分Transformer层，以低开销适应O-RAN KPI预测等多样下游任务。

Result: 在全面的O-RAN KPI案例研究中，RA-MAT在多个连续和离散KPI上均实现了低于0.06的均方误差（MSE）。

Conclusion: 这项工作使RA-MAT成为未来6G网络中实现实时、基础级分析的实用途径。

Abstract: Next-generation open radio access networks (O-RAN) continuously stream tens
of key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)
samples, yielding ultra-high-dimensional, non-stationary time series that
overwhelm conventional transformer architectures. We introduce a
reservoir-augmented masked autoencoding transformer (RA-MAT). This time series
foundation model employs echo state network (ESN) computing with masked
autoencoding to satisfy the stringent latency, energy efficiency, and
scalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN
rapidly projects each temporal patch into a rich dynamical embedding without
backpropagation through time, converting the quadratic self-attention
bottleneck into a lightweight linear operation. These embeddings drive a
patch-wise masked autoencoder that reconstructs 30% randomly masked patches,
compelling the encoder to capture both local dynamics and long-range structure
from unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned
with a shallow task head while keeping the reservoir and most transformer
layers frozen, enabling low-footprint adaptation to diverse downstream tasks
such as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT
achieved sub-0.06 mean squared error (MSE) on several continuous and discrete
KPIs. This work positions RA-MAT as a practical pathway toward real-time,
foundation-level analytics in future 6G networks.

</details>


### [237] [Scalable and Energy-Efficient Predictive Data Collection in Wireless Sensor Networks with Constructive Interference](https://arxiv.org/abs/2508.07882)
*Conor Muldoon*

Main category: cs.NI

TL;DR: STAIR是一个利用建设性干扰的新型无线传感器网络（WSN）框架，通过次模优化算法选择性激活节点，以最小化数据预测误差，并在实际测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 新兴的建设性干扰技术为无线传感器网络提供了低能耗、低延迟的数据收集潜力。需要一个可扩展、弹性且在资源受限下高效利用此技术的数据收集框架，并能有效估计未监测区域的值。

Method: 1. 提出STAIR（Spatio-Temporal Activation for Intelligent Relaying）框架，利用建设性干扰实现多节点同步数据中继。2. 通过粗粒度拓扑信息泛洪网络子集，在分配时隙中中继传感器读数。3. 采用具有质量保证的次模优化算法，确定近乎最优的传感器激活位置和时间。4. 优化目标是最小化多元线性回归模型（用于估计未选择位置和时间的值）的均方预测误差总和。

Result: STAIR框架在真实世界测试平台部署中得到了广泛验证，证明其在利用建设性干扰实现数据收集和值估计方面的有效性、可伸缩性和弹性。

Conclusion: STAIR框架成功地为利用建设性干扰的新型无线传感器网络提供了一个可扩展、弹性且资源高效的数据收集和值估计解决方案，并通过实际部署得到了验证。

Abstract: A new class of Wireless Sensor Network has emerged whereby multiple nodes
transmit data simultaneously, exploiting constructive interference to enable
data collection frameworks with low energy usage and latency. This paper
presents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a
scalable, resilient framework for Wireless Sensor Networks that leverages
constructive interference and operates effectively under stringent resource
constraints. Using constructive interference requires all nodes to transmit the
same packet at the same time, thus, only one source node can send data per time
slot. STAIR uses coarse-grained topology information to flood a selected subset
of the network, relaying sensor readings from individual nodes during their
allocated time slots. A submodular optimisation algorithm with proven quality
bounds determines near-optimal sensor activation locations and times, aiming to
minimise the sum of mean squared prediction errors from a multiple multivariate
linear regression model, which is used to estimate values at unselected
locations and times. This framework has been extensively validated on a
real-world testbed deployment.

</details>


### [238] [Adaptive Multiple Access and Service Placement for Generative Diffusion Models](https://arxiv.org/abs/2508.07978)
*Hamidreza Mazandarani,Mohammad Farhoudi,Masoud Shokrnezhad,Tarik Taleb*

Main category: cs.NI

TL;DR: 本文提出一个基于深度强化学习的统一优化框架LEARN-GDM，旨在解决生成扩散模型在移动边缘网络中部署时面临的资源密集和实时性挑战，通过动态块划分和多路访问控制，显著提升了可扩展性和低延迟性能。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型(GDMs)在生成式人工智能(GenAI)中展现出强大的表达和控制能力，但其迭代和资源密集型推理过程使其难以在实时和移动环境中部署。

Method: 本文提出了一个统一优化框架，联合处理移动边缘网络中GDM的服务放置和多路访问控制。具体而言，引入了基于深度强化学习(DRL)的算法LEARN-GDM，该算法能动态地将去噪块分配到异构边缘节点，考虑潜在传输成本并自适应减少推理步骤。此外，结合了贪婪多路访问方案和基于D3QL(Double and Dueling Deep Q-Learning)的服务放置策略。

Result: 仿真结果表明，与传统的单一和固定链长放置策略相比，所提出的框架在可扩展性和延迟弹性方面表现出卓越的性能。

Conclusion: 该工作通过为GDM服务编排提供自适应解决方案，提升了边缘赋能GenAI的现有技术水平，并为未来向语义网络和分布式环境中的协同推理扩展奠定了基础。

Abstract: Generative Diffusion Models (GDMs) have emerged as key components of
Generative Artificial Intelligence (GenAI), offering unparalleled
expressiveness and controllability for complex data generation tasks. However,
their deployment in real-time and mobile environments remains challenging due
to the iterative and resource-intensive nature of the inference process.
Addressing these challenges, this paper introduces a unified optimization
framework that jointly tackles service placement and multiple access control
for GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement
Learning-based algorithm that dynamically partitions denoising blocks across
heterogeneous edge nodes, while accounting for latent transmission costs and
enabling adaptive reduction of inference steps. Our approach integrates a
greedy multiple access scheme with a Double and Dueling Deep Q-Learning
(D3QL)-based service placement, allowing for scalable, adaptable, and
resource-efficient operation under stringent quality of service requirements.
Simulations demonstrate the superior performance of the proposed framework in
terms of scalability and latency resilience compared to conventional monolithic
and fixed chain-length placement strategies. This work advances the state of
the art in edge-enabled GenAI by offering an adaptable solution for GDM
services orchestration, paving the way for future extensions toward semantic
networking and co-inference across distributed environments.

</details>


### [239] [Industrial Viewpoints on RAN Technologies for 6G](https://arxiv.org/abs/2508.08225)
*Mansoor Shafi,Erik G. Larsson,Xingqin Lin,Dorin Panaitopol,Stefan Parkvall,Flavien Ronteix-Jacquet,Antti Toskala*

Main category: cs.NI

TL;DR: 本文展望6G标准化和部署，重点分析其技术组成和性能要求，特别是无线接入技术。


<details>
  <summary>Details</summary>
Motivation: 6G标准化即将启动，文章旨在探讨其技术组成和性能要求，以应对未来商业部署。

Method: 论文通过强调和探讨6G无线接入技术（如MIMO、AI、波形、编码、信号星座和与非地面网络的集成），并结合对实施和部署方面的考量，提出预测性观点。

Result: 本文提出了关于6G技术的推测性预测和观点，这些观点基于对未来实施和部署的考量，而非现有研究的正式结果。

Conclusion: 本文的观点旨在为研究人员和行业从业者提供指导。

Abstract: 6G standardization is to start imminently, with commercial deployments
expected before 2030. Its technical components and performance requirements are
the focus of this article. Our emphasis is on the 6G radio access, especially
MIMO, AI, waveforms, coding, signal constellations and integration with
non-terrestrial networks. Whilst standardization has not yet formally started,
the scope of the 6G study items has been defined. Our predictions in this paper
are speculative as there are no results of the study yet, but our views are
guided by implementation and deployment aspects. We expect that the views here
will guide researchers and industry practitioners.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [240] [Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure](https://arxiv.org/abs/2508.07744)
*Ingo Friese,Jochen Klaffer,Mandy Galkow-Schneider,Sergiy Melnyk,Qiuheng Zhou,Hans Dieter Schotten*

Main category: cs.DC

TL;DR: 论文探讨了在6G网络架构中，引入一个OTT（Over-The-Top）代理来管理资源分配的作用。该代理通过抽象基础设施复杂性，在分体式计算场景中实现跨多提供商的无缝资源访问和简化服务部署，并提供了概念验证实现细节。


<details>
  <summary>Details</summary>
Motivation: 6G网络将引入分体式计算和动态处理节点等创新服务。为应对多样化处理节点的无缝资源访问、统一接口、多运营商协作、基于位置和服务的性能保证以及简化服务部署的需求，需要一个能够抽象复杂基础设施的解决方案。

Method: 本文提出并探讨了引入一个over-the-top (OTT) 代理作为资源分配的有效方法。具体分析了该代理在两种分体式计算场景中的作用，并通过详细讨论一个概念验证（Proof-of-Concept）实现，展示了代理的实际架构框架。

Result: 通过抽象各种基础设施的复杂性，所提出的代理被证明是一种多功能解决方案。该代理不仅适用于云环境，而且可扩展应用于网络及更广泛的领域。

Conclusion: OTT代理是未来6G网络中实现统一资源访问、简化服务部署、保证性能以及支持多提供商协作的关键。其广泛适用性和通过概念验证所证实的实际可行性，使其成为应对复杂基础设施挑战的有效工具。

Abstract: 6G network architectures will usher in a wave of innovative services and
capabilities, introducing concepts like split computing and dynamic processing
nodes. This implicates a paradigm where accessing resources seamlessly aligns
with diverse processing node characteristics, ensuring a uniform interface. In
this landscape, the identity of the operator becomes inconsequential, paving
the way for a collaborative ecosystem where multiple providers contribute to a
shared pool of resources. At the core of this vision is the guarantee of
specific performance parameters, precisely tailored to the location and service
requirements. A consistent layer, as the abstraction of the complexities of
different infrastructure providers, is needed to simplify service deployment.
One promising approach is the introduction of an over-the-top broker for
resource allocation, which streamlines the integration of these services into
the network and cloud infrastructure of the future. This paper explores the
role of the broker in two split computing scenarios. By abstracting the
complexities of various infrastructures, the broker proves to be a versatile
solution applicable not only to cloud environments but also to networks and
beyond. Additionally, a detailed discussion of a proof-of-concept
implementation provides insights into the broker's actual architectural
framework.

</details>


### [241] [Performance Evaluation of Brokerless Messaging Libraries](https://arxiv.org/abs/2508.07934)
*Lorenzo La Corte,Syed Aftab Rashid,Andrei-Marian Dan*

Main category: cs.DC

TL;DR: 本文通过设计开源基准测试套件，系统评估了ZeroMQ、NanoMsg和NNG等无代理消息系统的性能，旨在为实际应用提供选型指导。


<details>
  <summary>Details</summary>
Motivation: 现有消息系统研究多集中于有代理系统，而无代理系统在提供更高性能、避免单点故障方面具有优势，但其性能评估研究匮乏。

Method: 首先对无代理消息系统候选者进行定性分析筛选；随后设计并实现一个开源基准测试套件，用于系统性评估ZeroMQ、NanoMsg和NNG在不同指标和负载条件下的性能。

Result: 评估了ZeroMQ、NanoMsg和NNG等库在不同度量标准和工作负载条件下的性能，并揭示了它们的局限性。

Conclusion: 本研究的分析结果能帮助实践者根据自身需求选择最合适的无代理消息系统库。

Abstract: Messaging systems are essential for efficiently transferring large volumes of
data, ensuring rapid response times and high-throughput communication. The
state-of-the-art on messaging systems mainly focuses on the performance
evaluation of brokered messaging systems, which use an intermediate broker to
guarantee reliability and quality of service. However, over the past decade,
brokerless messaging systems have emerged, eliminating the single point of
failure and trading off reliability guarantees for higher performance. Still,
the state-of-the-art on evaluating the performance of brokerless systems is
scarce. In this work, we solely focus on brokerless messaging systems. First,
we perform a qualitative analysis of several possible candidates, to find the
most promising ones. We then design and implement an extensive open-source
benchmarking suite to systematically and fairly evaluate the performance of the
chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).
We evaluate these libraries considering different metrics and workload
conditions, and provide useful insights into their limitations. Our analysis
enables practitioners to select the most suitable library for their
requirements.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [242] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 本研究利用迁移学习和数据增强，通过预训练CNN模型（特别是EfficientNet-B3）实现了对急性淋巴细胞白血病血涂片图像的高精度分类，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 从外周血涂片图像中准确分类急性淋巴细胞白血病（ALL）对于早期诊断和有效治疗计划至关重要。

Method: 研究采用预训练卷积神经网络（CNNs）进行迁移学习，并对不平衡数据集进行广泛数据增强以创建平衡训练集。评估了包括ResNet50、ResNet101和EfficientNet变体（B0、B1、B3）在内的多个模型。

Result: EfficientNet-B3表现最佳，F1-score达到94.30%，准确率为92.02%，AUC为94.79%，优于C-NMC挑战赛中先前报告的方法。

Conclusion: 结合数据增强和先进的迁移学习模型（特别是EfficientNet-B3），能有效开发准确且鲁棒的血液恶性肿瘤诊断工具。

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [243] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: 针对冠状动脉CTCA分析耗时且现有自动化标注方法存在不足的问题，本文提出一种结合解剖知识与规则拓扑约束的轻量级方法，实现了冠状动脉的自动化标注，并在基准数据集上达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病诊断中CTCA分析手动操作耗时费力。现有自动化标注方法（传统知识型和深度学习型）各有局限，前者未能充分利用数据驱动洞察，后者计算资源需求高且常忽略临床知识。

Method: 提出一种轻量级方法，该方法将解剖学知识与基于规则的拓扑约束相结合，以实现有效的冠状动脉标注。

Result: 该方法在基准数据集上取得了最先进的性能。

Conclusion: 该方法为冠状动脉的自动化标注提供了一个有前景的替代方案。

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [244] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: 提出一种集成深度学习框架，结合可解释人工智能（XAI）和临床决策规则，实现了脑肿瘤的高精度、可解释分类。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤磁共振成像（MRI）的准确且可解释分类对有效的诊断和治疗规划至关重要。

Method: 本研究提出一个基于集成深度学习的框架，结合MobileNetV2和DenseNet121卷积神经网络，采用软投票策略分类胶质瘤、脑膜瘤和垂体腺瘤。框架集成了Grad-CAM++模块用于类特异性显著性可视化，并加入了符号化临床决策规则叠加（CDRO）以将预测映射到放射学启发式规则。模型在Figshare数据集上进行训练和评估，并邀请放射科医生进行人本可解释性评估。

Result: 集成分类器性能优于单个CNN，实现了91.7%的准确率、91.9%的精确率、91.7%的召回率和91.6%的F1分数。Grad-CAM++可视化显示模型注意力与专家标注肿瘤区域高度对齐（Dice系数高达0.88，IoU得分高达0.78）。临床规则激活进一步验证了模型预测。放射科医生对解释有用性（平均4.4）和热图区域对应性（平均4.0）给出了较高的李克特评分。

Conclusion: 所提出的方法为自动脑肿瘤分类提供了一个鲁棒、可解释且通用性强的解决方案，有助于将深度学习集成到临床神经诊断中。

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>
