<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.CV](#cs.CV) [Total: 56]
- [cs.AI](#cs.AI) [Total: 56]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.NI](#cs.NI) [Total: 15]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 研究Transformer中稳定的注意力头，通过校准文本近似其输出，并发现GPT2-Small中响应高级上下文属性的神经元。


<details>
  <summary>Details</summary>
Motivation: 分析Transformer语言模型中注意力模式分散且对内容依赖弱的注意力头，并探索如何解释它们对文本高级上下文属性的贡献。

Method: 研究注意力模式分散且对内容依赖弱的注意力头，提出其softmax分母在固定token分布下稳定。通过“校准文本”采样softmax分母，结合GPT2-Small第一层多个稳定头的输出，并用周围文本的线性摘要近似其组合输出。

Result: 该近似方法仅凭模型权重和一份校准文本，即可发现GPT2-Small第一层中数百个响应周围文本高级上下文属性的神经元，甚至包括未在校准文本上激活的神经元。

Conclusion: 通过分析特定稳定注意力头并近似其行为，开发出一种新方法，用于解释和发现Transformer模型中对高级上下文属性敏感的神经元。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出Graph-$S^3$框架，通过合成逐步监督训练LLM检索器，解决文本图问答中的图检索挑战，显著提高准确性和F1分数，尤其在多跳推理任务中。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常以文本图形式存在，将其整合到大型语言模型（LLM）中对实现复杂图问答前景广阔。然而，LLM文本图问答系统的关键挑战在于图检索，即如何从大型图中检索出足够信息量且紧凑的内容以适应LLM上下文。现有检索器因依赖浅层嵌入相似性或交互式检索策略但需高昂标注和训练成本，导致性能不佳。

Method: 提出Graph-$S^3$框架，一个智能体文本图推理框架，采用基于LLM的检索器，并通过合成逐步监督进行训练。该方法不基于最终答案奖励（可能导致稀疏和不稳定的训练信号），而是通过离线提取的黄金子图密切评估检索器的每一步。主要技术包括：1) 用于提取黄金子图以生成奖励的数据合成管道；2) 用于学习基于合成奖励的交互式图探索策略的两阶段训练方案。

Result: 在三个通用数据集上与七个强基线进行广泛实验比较，该方法在准确性上平均提高8.1%，在F1分数上平均提高9.7%。在更复杂的多跳推理任务中，优势更为显著。

Conclusion: Graph-$S^3$通过引入合成逐步监督的LLM检索器，有效解决了文本图问答中的关键图检索挑战，显著提升了问答系统的性能，尤其在多跳推理方面表现优异。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）在执行主观日常任务时所展现的隐性价值观，通常与人类及其他LLM不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能助手（由LLM驱动）前景广阔，但对于它们在完成主观日常任务（如推荐、计算）时所展现的隐性价值观（如环保主义、慈善、多样性）知之甚少。因此，本研究旨在探究LLM在多大程度上展现这些价值观，以及它们与人类的比较情况。

Method: 通过审计六个流行的LLM完成30项日常任务的表现来回答研究问题。研究团队将LLM之间以及LLM与来自美国的100名人类众包工作者的表现进行比较。

Result: 研究发现，LLM在所展现的隐性价值观方面，经常与人类不一致，也与其他LLM之间存在差异。

Conclusion: LLM在执行主观日常任务时，其内在的价值观展现出显著的异质性，并且与人类价值观存在明显差异，这表明在AI助手实际应用中需关注其价值观对用户的影响。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR是一种用于从新兴语言语料中归纳词素的贪婪算法，通过形式与意义的互信息加权选择词素，并在生成数据集和人类语言数据上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 从并行语料（包含话语和意义）中归纳新兴语言的词素。

Method: 引入CSAR算法，一个贪婪算法，其步骤为：1) 基于形式和意义之间的互信息对词素进行加权；2) 选择权重最高的词素对；3) 将其从语料中移除；4) 重复此过程（即计数、选择、消融、重复）。

Result: CSAR在程序生成数据集上得到了有效验证，并与相关任务的基线进行了比较。在人类语言数据上表现出合理的预测能力。此外，还量化分析了一些新兴语言的同义性和多义性等语言特征。

Conclusion: CSAR算法能够有效从新兴语言中归纳词素，并在邻近领域做出合理预测，同时为分析新兴语言的语言特征提供了工具。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: 本文提出了Omni-Embed-Nemotron，一个统一的多模态检索嵌入模型，旨在处理复杂多样的真实世界信息需求，支持文本、图像、音频和视频的跨模态及联合模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索系统在RAG应用中，难以处理真实世界文档（如PDF、幻灯片、视频）中视觉和语义丰富的复杂内容，因其依赖于干净、结构化的输入。

Method: 借鉴ColPali通过图像表示保留文档布局的思路，并受Qwen2.5-Omni等多模态模型的启发，将检索能力扩展至文本、图像、音频和视频。该模型使用单一架构实现跨模态（如文本-视频）和联合模态（如文本-视频+音频）检索。

Result: 论文描述了Omni-Embed-Nemotron的架构、训练设置及评估结果，并展示了其在文本、图像和视频检索中的有效性。

Conclusion: Omni-Embed-Nemotron通过统一的多模态嵌入模型，成功处理了真实世界中日益复杂的信息需求，并在文本、图像和视频检索中表现出有效性。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本文通过基于信号博弈的突现通信环境和超参数优化，利用XferBench生成了与人类语言高度相似的突现语言。研究揭示了熵对突现语言迁移学习性能的预测力，并总结了生成更真实突现语言的超参数规律。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一种突现通信环境，以生成在统计学上与人类语言更相似的突现语言，并理解影响这种相似性的关键因素。

Method: 设计了一个基于信号博弈的突现通信环境。使用超参数优化，并以XferBench作为目标函数。XferBench通过测量突现语言对人类语言深度迁移学习的适用性来量化其统计相似性。此外，还分析了熵与突现语言迁移学习性能的关系。

Result: 成功生成了在与人类语言相似度方面达到SOTA水平的突现语言。证明了熵对突现语言迁移学习性能的预测能力。同时，也印证了先前关于突现通信系统熵最小化特性的研究结果。

Conclusion: 识别并报告了哪些超参数能够产生更真实（即对人类语言迁移效果更好）的突现语言的泛化规律。熵对突现语言的迁移学习表现具有预测作用。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: 引入SEER基准测试，评估大型语言模型在识别文本中表达情感的具体短语（情感证据）方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务仅为整个句子分配标签，缺乏粒度。为了更好地支持同理心对话和临床支持等应用，需要识别情感是如何表达的，而不仅仅是识别情感本身。现有方法未能充分探索情感证据检测。

Method: 引入SEER基准测试，包含两个任务：单句内情感证据识别和短篇（五句）情感证据识别。该基准包含1200个真实世界句子上的情感和情感证据新注释。评估了14个开源大型语言模型。

Result: 部分大型语言模型在单句输入上接近人类平均表现，但在较长文本段落中，它们的准确性显著下降。错误分析揭示了主要的失败模式，包括过度依赖情感关键词和在中性文本中出现假阳性。

Conclusion: 大型语言模型在跨度级情感证据检测方面仍面临挑战，尤其是在处理较长上下文时。未来的研究应关注克服模型对关键词的过度依赖，并提高在复杂语境中识别情感表达的准确性。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个大规模阿拉伯语数据集，旨在区分人类与LLM生成的文本，涵盖多流派和方言，包含40多万样本。基准测试显示，微调BERT模型优于LLM模型，但跨流派泛化能力仍有挑战，尤其在新闻领域。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模、全面的阿拉伯语数据集来区分人类与LLM生成的文本，这对于应对虚假信息、学术不端和网络威胁等风险至关重要。

Method: 引入ALHD数据集，包含新闻、社交媒体、评论三种流派，覆盖标准阿拉伯语和方言，拥有超过40万由三个领先LLM和多个人类来源生成的平衡样本。提供严格预处理、丰富标注和标准化分割。对传统分类器、BERT模型和LLM（零样本、少样本）进行基准测试。

Result: 微调的BERT模型表现出竞争力，且优于基于LLM的模型。然而，模型在跨流派泛化时面临挑战，特别是在处理新闻文章时，由于LLM生成的文本与人类文本风格相似，导致泛化能力下降。

Conclusion: ALHD为阿拉伯语LLM检测研究奠定了基础，有助于减轻虚假信息、学术不端和网络威胁的风险。未来的研究应专注于提高模型在跨流派，特别是在新闻领域，的泛化能力。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出TS-Reasoner，通过对齐时间序列基础模型（TSFMs）的潜在表示与大型语言模型（LLMs）的文本输入，实现高效的时间序列理解与推理，并在性能和数据效率上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理对各领域决策至关重要。现有TSFMs擅长预测但缺乏高级推理能力，而LLMs虽具推理潜力却不擅长数值理解。将这两种模型有效整合以应对推理任务，并解决两者模态对齐的挑战。

Method: 提出TS-Reasoner模型，旨在对齐TSFMs的潜在表示与LLMs的文本输入。具体方法包括：1) 创建多样化的时间序列及其文本描述的合成对，用于对齐训练。2) 开发一个两阶段训练范式，先进行对齐预训练，再进行指令微调。3) 利用并冻结一个预训练TSFM，而非训练LLM直接接收时间序列输入。

Result: TS-Reasoner在多个基准测试中表现卓越，不仅超越了广泛的LLMs、视觉语言模型（VLMs）和时间序列LLMs，还在数据效率上显著领先，例如仅使用不到一半的训练数据。

Conclusion: TS-Reasoner成功地将TSFMs的数值理解能力与LLMs的推理能力结合，有效解决了时间序列推理问题，并在模型性能和数据效率方面均展示出显著优势，为时间序列分析提供了新范式。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 现有RAG在专业领域推理时输出泛泛且缺乏特定洞察，本文提出一个“同伴感知对比推理层”叠加在RAG上，使其能检索可比案例并生成更具体的洞察，在文本生成指标上优于基线RAG。


<details>
  <summary>Details</summary>
Motivation: 现有RAG-LLM系统在专业领域（如金融）进行推理时，虽然能捕获相关信息，但无法检索可比案例或相关问题，导致输出结果过于通用，缺乏领域特定的细致洞察（例如，金融领域中生成通用风险而非公司特定风险）。

Method: 提出并实现了一个“同伴感知对比推理层（peer-aware comparative inference layer）”，将其叠加在RAG之上，以实现基于可比案例的推理。

Result: 该对比方法在文本生成指标（如ROUGE和BERTScore）上优于基线RAG，并且在与人类生成的股票研究和风险分析进行比较时表现更佳。

Conclusion: 通过引入同伴感知对比推理层，可以有效增强RAG在专业领域中的推理能力，使其能够生成更具上下文特异性和洞察力的结果，从而克服了传统RAG输出泛化的局限性。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [11] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文提出共识图（ConGrs），一种基于DAG的数据结构，通过综合语言模型多次采样的长篇响应中的知识信号，显著提高了事实准确性、弃权能力和推理任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效综合语言模型多次采样产生的长篇响应中丰富的知识信号（epistemic signals）。

Method: 引入共识图（ConGrs），一种灵活的DAG数据结构，用于表示采样LM响应中的共享信息和语义变异。通过生物信息学的轻量级词汇序列比对算法，辅以辅助LM判断器构建ConGrs。此外，设计了任务相关的解码方法，从ConGr结构中合成单一的最终响应。

Result: 实验表明，从ConGrs合成的响应在两项传记生成任务中将事实准确性比平均响应提高了高达31%，并将对LM判断器的依赖减少了80%以上。在三项基于拒绝的任务中，无法回答查询的弃权率提高了高达56%。在MATH和AIME推理任务中，比自验证和多数投票基线提高了高达6个准确度点。

Conclusion: ConGrs提供了一种灵活的方法，能够捕获LM响应中的变异，并利用响应变异提供的知识信号来合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [12] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 研究发现，通过在指令微调数据中引入扰动（如删除停用词、打乱词序），可以提升大型语言模型对噪音指令的鲁棒性，甚至在某些情况下改善下游性能。


<details>
  <summary>Details</summary>
Motivation: 指令微调对提升大型语言模型（LLMs）的任务解决能力至关重要，但LLMs对指令措辞的微小变化非常敏感。

Method: 探索在指令微调数据中引入扰动（如删除停用词、打乱词序）是否能增强LLMs对噪音指令的抵抗力。通过评估LLMs在广泛使用的基准测试（MMLU, BBH, GSM8K）的原始及扰动版本上的性能，并进一步评估学习动态和模型行为变化。

Result: 研究结果表明，在某些情况下，对扰动指令进行微调可以改善LLMs的下游任务性能。

Conclusion: 在指令微调中纳入扰动指令非常重要，这能使LLMs对嘈杂的用户输入更具弹性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [13] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ是一个三元组结构化方法，通过将患者反馈转化为知识图谱中的结构化三元组，显著提高了大型语言模型在多轮交互式医学问答中的诊断准确性，解决了现有LLM在处理非结构化对话日志时可靠性下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在静态和单轮医学问答基准测试中表现出色，但这种设置与实际临床咨询所需的迭代信息收集过程不符。MEDIQ框架将诊断重构为互动对话，但当LLMs被迫在缺乏明确关联的对话日志中进行推理时，其可靠性急剧下降。

Method: 引入TriMediQ框架，它通过以下方式解决问题：1. 将患者的回答总结为三元组并整合到知识图谱（KG）中，以实现多跳推理。2. 使用一个冻结的三元组生成器，通过精心设计的提示词提取临床相关三元组，确保事实一致性。3. 包含一个可训练的投影模块（由图编码器和投影器组成），用于从KG中捕获关系信息以增强专家推理。TriMediQ分两步操作：(i) 在LLM所有权重冻结的情况下微调投影模块；(ii) 在推理过程中使用微调后的模块指导多跳推理。

Result: TriMediQ在两个交互式问答基准测试上进行了评估，在iMedQA数据集上，相比五个基线模型，其准确性最高提升了10.4%。

Conclusion: 将患者回答转换为结构化的基于三元组的图，能够在多轮设置中实现更准确的临床推理，为部署基于LLM的医疗助手提供了解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [14] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: LLM在CSS文本分类中易使分析师忽视概念化步骤，导致下游估计偏差。模拟显示该偏差无法单靠提高LLM准确性或事后校正解决。强调概念化在LLM时代仍是首要问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在计算社会科学(CSS)文本分类中广泛应用，但其前的概念化和其后的统计推断步骤常被忽视。研究旨在指出LLM可能诱导分析师跳过概念化，从而导致下游估计产生偏差。

Method: 采用模拟方法来论证概念化错误如何产生偏差，并评估通过提高LLM准确性或事后偏差校正方法对该偏差的修正效果。

Result: 模拟结果表明，由概念化错误引起的偏差无法仅通过提高LLM的准确性或使用事后偏差校正方法来有效纠正。

Conclusion: 在LLM时代，概念化对于计算社会科学分析师来说仍是一个至关重要的问题。研究提供了获取低成本、无偏、低方差下游估计的实用建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [15] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在处理跨文化价值冲突时表现不佳，倾向于偏爱北欧和日耳曼欧洲价值观，且其多元主义是肤浅的。这表明当前的对齐策略未能充分考虑多元的世界观。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在决策中日益重要，但其在处理合法文化价值系统之间显性冲突的能力仍未被充分研究。现有基准主要关注文化知识、价值预测或单轴偏见诊断，缺乏评估LLMs如何裁决多种文化价值直接冲突的场景。

Method: 引入CCD-Bench基准，用于评估LLMs在跨文化价值冲突下的决策能力。该基准包含2182个涵盖七个领域的开放式困境，每个困境配有十个匿名响应选项，对应十个GLOBE文化集群。采用分层拉丁方设计以减轻排序效应。共评估了17个非推理LLM。

Result: 模型过度偏好北欧（平均20.2%）和日耳曼欧洲（12.4%）的选项，而东欧和中东及北非的选项则代表性不足（5.6%至5.8%）。尽管87.9%的理由引用了多个GLOBE维度，但这种多元主义是肤浅的，模型主要重组“未来导向”和“绩效导向”，很少基于“武断性”或“性别平等主义”做出选择（均低于3%）。排序效应可以忽略不计，模型聚类更多是基于开发者谱系而非地理区域。

Conclusion: 当前LLM的对齐流程可能促进了一种以共识为导向的世界观，未能充分处理需要权力谈判、基于权利的推理或性别意识分析的场景。CCD-Bench将评估从孤立的偏见检测转向多元决策，强调需要实质性地融合不同世界观的对齐策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [16] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [17] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了作者在EvalLLM 2025挑战赛（法语生物医学命名实体识别和健康事件提取，少样本设置）中的参与。研究提出了结合大型语言模型（LLMs）、指令和合成数据的三种NER方法，以及用于事件提取的上下文学习策略。GPT-4.1表现最佳，强调了在低资源场景中精心设计的提示的重要性。


<details>
  <summary>Details</summary>
Motivation: 参与EvalLLM 2025挑战赛，旨在解决法语生物医学命名实体识别（NER）和健康事件提取在少样本（few-shot）设置下的挑战。

Method: 对于NER，提出了三种方法：1) 使用GPT-4.1进行上下文学习（ICL），包含10个自动选择的示例和注释指南摘要；2) 通用NER系统GLiNER，在合成语料库上微调并经LLM后处理验证；3) 开源LLM LLaMA-3.1-8B-Instruct，在同一合成语料库上微调。事件提取则采用与GPT-4.1相同的ICL策略，并重用指南摘要。

Result: GPT-4.1在所有方法中表现最好。在NER任务中，GPT-4.1达到了61.53%的宏F1分数；在事件提取任务中，GPT-4.1达到了15.02%的宏F1分数。

Conclusion: 研究表明，GPT-4.1在这些任务中表现出色，并且在极低资源场景下，精心设计的提示（prompt）对于最大化模型性能至关重要。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [18] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: 一种名为Deco-G的解码框架，通过将格式遵循与任务解决解耦，显著提升了大型语言模型在复杂指令下的性能，并保证了格式合规性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂指令时，难以同时遵循推理指令和严格的格式要求，因为这会产生相互竞争的目标，导致模型性能下降。

Method: 引入Deco-G解码框架，明确解耦格式遵循与任务解决。该框架使用单独的可处理概率模型（TPM）处理格式合规性，而大型语言模型仅处理任务指令。在每个解码步骤中，Deco-G结合LLM的下一个词元概率和TPM计算的格式合规性似然来形成输出概率。为实现实用性和可扩展性，该方法引入了指令感知蒸馏、灵活的Trie构建算法和HMM状态剪枝。

Result: Deco-G在数学推理、LLM-as-a-judge和事件论元提取等多种具有不同格式要求的任务中，相比常规提示实践取得了1.0%至6.0%的相对性能提升，并保证了格式合规性。

Conclusion: Deco-G通过将格式 adherence 与任务解决解耦，有效地解决了大型语言模型在复杂指令下遵循所有指令的难题，显著提高了模型性能并确保了格式合规性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [19] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 研究发现，在复杂关系推理任务中（如从文本中提取图结构），LLMs的记忆漂移和遗忘发生得比现有简单基准测试所显示的更早，即使是专门的推理模型也如此，这揭示了其抽象结构化知识能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估基准（如“大海捞针”任务）过于简化，无法准确反映LLM在信息密集场景中的性能，也未能有效刻画其有效上下文长度和遗忘倾向。

Method: 本文提出应通过更复杂的推理任务来评估LLM，这些任务要求模型从可能嘈杂的自然语言内容中归纳出结构化关系知识（例如图），而非简单的下一词预测。这些任务需要模型从长上下文和无关信息中推断出分散的文本线索。

Result: LLM在处理关系推理任务时，其记忆漂移和上下文遗忘在比现有基准测试建议的更短的有效长度下就开始出现。即使是像OpenAI o1这样专门用于推理的模型，在这种设置下也容易过早出现记忆漂移。

Conclusion: 这些发现揭示了LLM从非结构化输入中抽象结构化知识的显著局限性，并强调需要进行架构调整以改进长程推理能力。本文还根据研究结果为复杂推理任务中LLM的最佳使用提供了建议。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [20] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 本文提出了一种基于掩码语言建模的音节级无监督语音识别（UASR）框架，克服了现有方法对G2P的依赖和训练不稳定性，在LibriSpeech上显著降低了CER，并有效泛化到普通话。


<details>
  <summary>Details</summary>
Motivation: 将ASR扩展到低资源语言和非并行数据的多模态学习是关键挑战。现有基于音素的UASR方法依赖昂贵的G2P资源，且因训练不稳定难以泛化到音素边界模糊的语言。

Method: 引入一种基于掩码语言建模的音节级UASR框架，以避免对G2P的需求和基于GAN方法的训练不稳定性。

Result: 在LibriSpeech上实现了高达40%的字符错误率（CER）相对降低；有效泛化到对现有方法而言特别困难的普通话。

Conclusion: 该音节级UASR框架成功解决了现有UASR方法的资源依赖和稳定性问题，在多语言（包括英语和普通话）上均表现出优异的性能和泛化能力。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [21] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文介绍了UniDoc-Bench，这是首个大规模、真实的MM-RAG基准测试，基于7万份PDF页面构建，涵盖多种查询类型和RAG范式。实验表明，多模态文本-图像融合RAG系统优于单模态和联合检索，揭示了当前多模态嵌入的不足，并为更稳健的MM-RAG管道开发提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有MM-RAG评估方法零散且简化，未能捕获文档中心的真实世界多模态用例，这阻碍了大型语言模型（LLMs）和智能体在实际知识库中的有效应用。

Method: 研究人员构建了UniDoc-Bench，一个基于7万份真实世界PDF页面（涵盖8个领域）的大规模基准。通过管道从文本、表格和图中提取并链接证据，生成1600个涵盖事实检索、比较、摘要和逻辑推理的多模态QA对。20%的QA对经过多人标注和专家裁决验证，确保可靠性。该基准在一个统一协议下，通过标准化候选池、提示和评估指标，支持对纯文本、纯图像、多模态文本-图像融合以及多模态联合检索四种范式进行公平比较。

Result: 实验结果显示，多模态文本-图像融合RAG系统持续优于单模态和基于联合多模态嵌入的检索方法。这表明单独的文本或图像不足以应对任务，且当前的多模态嵌入仍不完善。此外，分析揭示了视觉上下文何时以及如何补充文本证据，发现了系统性故障模式，并为开发更强大的MM-RAG管道提供了可操作的指导。

Conclusion: 对于文档中心的MM-RAG任务，多模态文本-图像融合是目前最有效的方法，优于单模态和联合多模态嵌入方案。当前的多模态嵌入能力仍有待提高。UniDoc-Bench作为新的基准，结合深入分析，为未来开发更鲁棒的MM-RAG系统提供了关键洞察和发展方向。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [22] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究提出一种基于QLoRA的微调框架，用于罗马乌尔都语-英语混合代码中的冒犯性语言检测。通过将数据集翻译成英语，利用英语LLM，Meta LLaMA 3 8B达到了91.45%的F1分数，证实了QLoRA在低资源语言任务中微调LLM的有效性。


<details>
  <summary>Details</summary>
Motivation: 在罗马乌尔都语等混合代码语言中，贬义词的使用给自然语言处理系统带来了挑战，主要原因在于语法不明确、拼写不一致以及缺乏标注数据。

Method: 本研究提出一个基于QLoRA的微调框架，用于改进罗马乌尔都语-英语文本中的冒犯性语言检测。数据集通过Google Translate翻译成英语，以利用现有的英语LLM。研究对Meta LLaMA 3 8B、Mistral 7B v0.1、LLaMA 2 7B、ModernBERT和RoBERTa等多个Transformer和大型语言模型使用QLoRA进行微调，并在一个手动标注的罗马乌尔都语冒犯性与非冒犯性内容数据集上进行训练和评估。

Result: 在所有测试模型中，Meta LLaMA 3 8B取得了最高的F1分数91.45，紧随其后的是Mistral 7B，F1分数为89.66，两者均超越了传统的Transformer基线模型。

Conclusion: 研究结果表明QLoRA在低资源环境（如混合代码冒犯性语言检测）中微调高性能模型的有效性，并确认了LLM在该任务中的潜力。该工作为罗马乌尔都语审核提供了一种可扩展的方法，并为未来基于LLM的多语言冒犯性检测系统奠定了基础。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [23] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 本文提出MedReflect框架，通过模拟医生反思式思维，使大型语言模型无需外部辅助或大量标注即可高效解决医学问题，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学问题解决复杂，现有大型语言模型（LLMs）方法（如检索增强、推理数据集训练）存在检索开销大、标注成本高、性能受限等缺点，且过度依赖外部辅助。

Method: MedReflect框架通过生成单通道反思链，包括初始假设生成、自我提问、自我回答和决策完善，实现LLM的自我验证和自我反思，释放其在医学问题解决中的潜在能力，不依赖外部检索或大量标注。

Result: MedReflect仅用2000个随机采样的训练示例进行轻量级微调，即可实现成本高效的医学数据集构建，并在多个医学基准测试中取得显著的绝对准确性提升，同时大幅降低标注需求。

Conclusion: LLMs能够通过自我反思和自我改进来解决专业的医学问题，从而减少对外部监督和大量特定任务微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [24] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出TreePrompt，一种新的少样本提示示例选择方法，它通过学习大型语言模型（LLM）的偏好，在树结构中识别高质量且上下文相关的示例，以改进机器翻译。


<details>
  <summary>Details</summary>
Motivation: 现有少样本提示的示例选择方法主要关注查询与示例的相似性，但未能充分考虑示例本身的质量，这限制了大型语言模型（LLMs）在机器翻译中的潜在性能。

Method: 本文提出了TreePrompt，一种新颖的示例选择方法。它在一个树形框架内，通过学习LLM的偏好来识别高质量、上下文相关的示例。为探索相似性和质量的平衡，将TreePrompt与K-Nearest Neighbors (K-NN)和Adaptive Few-Shot Prompting (AFSP)相结合，并在英-波斯语（MIZAN）和英-德语（WMT19）两种语言对上进行了评估。

Result: 在英-波斯语（MIZAN）和英-德语（WMT19）两个语言对上的评估显示，将TreePrompt与AFSP或随机选择方法结合使用，能有效提升机器翻译性能。

Conclusion: TreePrompt通过引入对示例质量和上下文相关性的考虑，为大型语言模型在少样本机器翻译中的示例选择提供了一种有效策略，尤其在与AFSP结合时能带来显著的性能改进。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [25] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 开发了一种多语言、粒度感知的帕金森病语音检测方法，发现音素级分析表现最佳（AUROC 93.78%），且关键信息特征与临床协议相符。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者语音障碍普遍，但现有语音检测系统常忽略特定语音元素的诊断价值，仅分析完整语句。

Method: 构建自动化管道，从录音中提取时间对齐的音素、音节和单词特征。采用双向LSTM与多头注意力模型，在意大利语、西班牙语和英语数据集上比较不同粒度分析的诊断性能。

Result: 音素级分析表现最佳，AUROC为93.78% ± 2.34%，准确率为92.17% ± 2.43%。注意力分析显示，最具信息量的语音特征（如音素级的持续元音、音节级的交替运动音节）与现有临床协议一致。

Conclusion: 粒度感知方法，特别是音素级分析，能显著提升跨语言帕金森病语音检测能力，并通过注意力机制揭示了与临床实践相符的关键诊断特征。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [26] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究发现，在少样本提示中，不平衡的样本分布会导致多语言词义消歧（WSD）任务预测不准确，但在英文WSD中未出现此问题，表明多语言WSD对样本分布敏感。


<details>
  <summary>Details</summary>
Motivation: 探究少样本提示策略如何影响词义消歧（WSD）任务，特别是关注由不平衡样本分布引入的偏差。

Method: 采用GLOSSGPT提示方法，在英语、德语、西班牙语、法语和意大利语五种语言上测试其有效性，并评估了GPT-4o和LLaMA-3.1-70B模型。

Result: 不平衡的少样本示例会导致多语言（非英语）词义预测不准确，但该问题未在英语中出现。结果凸显了多语言WSD在少样本设置中对样本分布的高度敏感性。

Conclusion: 多语言WSD在少样本设置中对样本分布表现出高度敏感性，因此需要平衡且具有代表性的提示策略。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [27] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Rezwan的大规模AI辅助圣训语料库的开发，该语料库包含超过120万篇叙述，通过全自动化流程提取和结构化，利用LLMs进行多层富集。专家评估显示其在结构化任务上达到近人准确度，并在规模和质量上超越现有语料库，同时显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有数字宗教文本资源的处理效率和丰富度不足，传统方法需要大量专家人工劳动，耗时耗力。研究旨在利用AI开发一个大规模、多语言、语义丰富的圣训语料库，以克服这些局限性，为数字人文和伊斯兰研究提供高效、研究就绪的基础设施。

Method: 开发了一个全自动流水线，从Maktabat Ahl al-Bayt等数字存储库中提取内容。该流水线利用大型语言模型（LLMs）进行文本分割、链-文本分离、验证和多层富集。富集功能包括将每篇叙述机器翻译成十二种语言、智能加注、抽象总结、主题标签以及跨文本语义分析。

Result: 对1,213篇随机抽样叙述的专家评估显示，该系统在链-文本分离和总结等结构化任务上达到了近人准确度（9.33/10）。尽管在加注和语义相似度检测方面仍面临挑战，但与人工整理的Noor语料库相比，该方法（Najm）在规模和质量上均表现优越（平均总分8.46/10 vs 3.66/10）。成本分析证实了AI方法的经济可行性，仅用数月时间、以极小成本完成了需要超过229,000小时专家劳动的工作。

Conclusion: 该工作通过展示AI如何增强人类专业知识，引入了宗教文本处理的新范式，从而实现了对伊斯兰遗产的大规模、多语言和语义丰富的访问。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [28] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 大型语言模型(LLMs)能够生成和识别深层认知框架（如“严父”和“慈母”），并且这些框架在模型隐藏表示中对应特定的维度。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在社会政治语境中生成和识别深层认知框架的能力，并理解LLMs如何捕捉和表达有意义的人类概念。

Method: ['演示LLMs生成唤起特定框架文本的流畅性及其在零样本设置下识别这些框架的能力。', '受机制可解释性研究启发，调查“严父”和“慈母”等框架在模型隐藏表示中的位置。', '识别与这些框架存在强烈相关的奇异维度。']

Result: ['LLMs在生成唤起特定框架的文本方面表现出高度流畅性。', 'LLMs能够在零样本设置下识别这些框架。', '在模型的隐藏表示中，识别出与“严父”和“慈母”框架存在强烈相关的奇异维度。']

Conclusion: 研究结果有助于理解LLMs如何捕捉和表达有意义的人类概念。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [29] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 为解决大型推理模型（LRMs）的冗长问题，本文提出Step Pruner (SP)，一个强化学习框架，通过步长感知奖励和动态停止机制，在保持准确性的同时，显著减少了推理步骤和响应长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务上表现出色，但常因“过度思考”而过度冗长。现有基于强化学习（RL）的解决方案通过惩罚生成的令牌来促进简洁，但面临两个挑战：令牌减少不总意味着推理步骤减少，且模型可能通过丢弃推理步骤来规避令牌惩罚（作弊行为）。

Method: 本文引入Step Pruner (SP)，一个强化学习框架，旨在引导LRMs进行更高效的推理。SP使用以下机制：1. **步长感知奖励函数**：优先考虑正确性，惩罚冗余步骤，并对错误响应不给予奖励，以防止强化错误推理。2. **动态停止机制**：当任何输出步骤的长度超过预设上限时，停止更新，以防止模型通过合并步骤来作弊。

Result: SP在四个推理基准测试中取得了最先进的准确性，并显著减少了响应长度。例如，在AIME24数据集上，令牌使用量减少了69.7%。

Conclusion: Step Pruner (SP) 有效解决了大型推理模型的冗长问题，通过优化推理步骤实现了高效且准确的推理，显著提升了模型的实用性。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [30] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 该研究利用INCEpTION工具探索语篇修辞关系标注，并比较了人工标注与基于LLM的自动方法，在体育报道中发现DistilBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索语篇中修辞关系的标注方法，并比较人工标注与基于大语言模型的自动方法的有效性。

Method: 使用INCEpTION工具进行修辞关系标注。在体育报道（板球新闻）上，对比了人工标注与基于BERT、DistilBERT和逻辑回归模型的自动分类方法。

Result: DistilBERT模型在分类修辞关系方面取得了最高的准确率。

Conclusion: DistilBERT在高效语篇关系预测方面具有巨大潜力，该工作为语篇分析与基于Transformer的NLP交叉领域做出了贡献。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [31] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 孟加拉语政治立场检测面临数据稀缺。本文构建首个孟加拉语新闻立场数据集，并评估28个LLM，发现LLM在识别政府批评内容表现良好，但在中立内容和政府倾向内容上存在挑战。


<details>
  <summary>Details</summary>
Motivation: 南亚地区媒体偏见检测至关重要，但孟加拉语政治偏见研究缺乏标注数据集和计算研究。孟加拉语新闻立场检测需理解复杂的语言、文化和社会政治背景。

Method: 引入首个包含200篇孟加拉语政治新闻文章的基准数据集，标注为“政府倾向”、“政府批评”和“中立”立场。并对28个专有和开源大型语言模型（LLMs）进行诊断性评估。

Result: LLMs在检测“政府批评”内容上表现出色（F1高达0.83），但在识别“中立”文章上存在显著困难（F1低至0.00）。模型还倾向于过度预测“政府倾向”立场，常误解模糊叙述。

Conclusion: 该数据集及其诊断分析为孟加拉语媒体立场检测研究奠定基础，并为改进低资源语言中LLM的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [32] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个为波斯语心理治疗设计的框架，利用小型语言模型，强调隐私保护、文化适应性和结构化记忆，可在设备端部署。


<details>
  <summary>Details</summary>
Motivation: 在资源不足的语言（如波斯语）中，开发具有文化根基、治疗连贯性、支持多轮交互且注重隐私的心理治疗对话系统面临挑战。

Method: 该研究提出了PsychoLexTherapy框架，优化以实现设备端部署。开发分为三阶段：评估小型语言模型(SLM)的心理知识、设计和实现面向推理的PsychoLexTherapy框架、构建两个评估数据集（PsychoLexQuery和PsychoLexDialogue）并与多种基线（简单提示、多智能体辩论、结构化治疗推理路径）进行比较。

Result: 实验结果显示，PsychoLexTherapy在单轮评估（PsychoLexQuery）中优于所有基线，并在人类评估中获得最高排名。在多轮评估（PsychoLexDialogue）中，其长时记忆模块至关重要，完整框架在同理心、连贯性、文化契合度和个性化方面获得了最高评价，而简单的历史拼接会导致不连贯和信息丢失。

Conclusion: PsychoLexTherapy为波斯语心理治疗模拟建立了一个实用、保护隐私且文化契合的基础，并贡献了新的数据集、可复现的评估流程以及关于治疗推理中结构化记忆的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [33] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型分析了410万条患者评论，以量化医生个性特质和患者主观评价，揭示了系统性模式并识别出四种医生原型。


<details>
  <summary>Details</summary>
Motivation: 理解患者如何看待医生对于增进信任、沟通和满意度至关重要。

Method: 开发了一个基于大型语言模型（LLM）的管道，用于从410万条患者评论中推断医生的大五人格特质和五种以患者为导向的主观判断。通过多模型比较和人工专家基准测试进行验证。

Result: LLM评估与人工评估高度一致（相关系数0.72-0.89），并与患者满意度呈正相关（r=0.41-0.81）。男性医生在所有特质上得分更高，临床能力感知差异最大；儿科和精神病学中同理心相关特质更突出；所有特质均正向预测总体满意度。识别出四种医生原型，包括“全面优秀型”和“表现不佳型”。

Conclusion: 通过患者叙述自动提取特质，可提供可解释、经过验证的指标，用于大规模理解医患关系，对医疗质量衡量、偏见检测和劳动力发展具有重要意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [34] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 本研究提出了一个多智能体模拟框架，用于在长周期交互和动态压力下评估大型语言模型（LLM）的欺骗行为。实验发现LLM的欺骗行为具有模型依赖性，随压力增加而增强，并持续侵蚀信任，揭示了隐瞒、模棱两可和伪造等欺骗策略。


<details>
  <summary>Details</summary>
Motivation: 欺骗是人类交流中普遍存在的现象，也日益成为大型语言模型（LLM）的一个新兴问题。现有研究多局限于单轮提示，未能捕捉到欺骗策略通常展开的长周期交互，因此需要更全面的评估框架。

Method: 引入了一个多智能体模拟框架，包含：1. 执行者（Performer）智能体：负责完成任务；2. 监督者（Supervisor）智能体：评估进展、提供反馈并维护信任状态；3. 独立欺骗审计员：审查完整的交互轨迹以识别欺骗行为的发生时间和方式。该框架用于在相互依存的任务序列和动态情境压力下探测和评估LLM的欺骗行为。

Result: 对11个前沿模型（包括闭源和开源系统）进行广泛实验后发现：欺骗行为具有模型依赖性，随事件压力的增加而增强，并持续侵蚀监督者的信任。定性分析进一步揭示了隐瞒、模棱两可和伪造等不同的欺骗策略。

Conclusion: 研究结果证实，欺骗是LLM在长周期交互中一种新兴的风险，并为未来在真实世界、信任敏感情境中评估LLM提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [35] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 针对社交媒体上COVID-19文本命名实体识别面临的非正式性、数据稀缺和领域知识需求等挑战，本文提出一种新颖的实体知识增强方法，有效提升了全监督和少样本设置下的NER性能，并可推广到一般生物医学领域。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情引发社交媒体上大量讨论，识别疫情相关命名实体对于理解讨论内容至关重要。然而，该领域NER研究有限，主要原因是社交媒体文本非正式、标注数据稀缺不足以训练鲁棒模型，且NER需大量领域特定知识。

Method: 提出一种新颖的实体知识增强方法，旨在解决COVID-19命名实体识别面临的数据和知识挑战。该方法也可应用于一般生物医学命名实体识别，包括非正式和正式文本格式。

Result: 在COVID-19推文数据集和PubMed数据集上的实验表明，所提出的实体知识增强方法在全监督和少样本设置下均显著提升了命名实体识别（NER）性能。

Conclusion: 本研究的实体知识增强方法成功解决了COVID-19社交媒体文本NER的挑战，有效利用知识弥补了数据不足，提高了识别性能，并展现了在生物医学领域中的泛化潜力。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [36] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 本文提出AgriGPT-VL套件，一个为农业领域定制的多模态框架，包含迄今最大的视觉语言语料库、专用模型和全面的评估基准，并在农业任务上实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型发展迅速，但农业应用受限于领域定制模型、精选视觉语言语料库和严格评估方法的稀缺性。

Method: 1. 构建Agri-3M-VL，一个通过可扩展多智能体数据生成器策展的农业视觉语言语料库（包含1M图像-字幕对、2M图像-VQA对、50K专家级VQA实例和15K GRPO强化学习样本）。2. 开发AgriGPT-VL，一个农业专用视觉语言模型，通过文本接地、多模态浅层/深层对齐和GRPO优化的渐进式课程进行训练。3. 建立AgriBench-VL-4K，一个包含开放式和图像接地问题的评估套件，采用多指标评估和LLM-as-a-judge框架。

Result: AgriGPT-VL在AgriBench-VL-4K上显著优于领先的通用视觉语言模型，在LLM-as-a-judge评估中获得更高的成对胜率。同时，它在纯文本AgriBench-13K上保持竞争力，语言能力没有明显下降。消融研究进一步证实了对齐和GRPO优化阶段带来的一致收益。

Conclusion: AgriGPT-VL套件有效解决了农业领域多模态AI的现有挑战，展示了在农业任务上的卓越性能，并通过开源所有资源支持可复现研究和在资源匮乏农业环境中的部署。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [37] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 本研究利用模型激活预测大型语言模型输出的正确性，并评估外部上下文的有效性，以提高模型的可信度并防止错误信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常以高置信度生成错误信息，且难以判断何时需要外部上下文及其有效性，因此信任度是一个主要问题。

Method: 研究采用可解释性方法，仅从模型激活中预测输出正确性，并探究模型内部是否存在关于外部上下文（包括正确、不正确和无关上下文）有效性的信号。通过训练一个基于第一个输出token中间层激活的简单分类器，以及引入区分上下文类型的指标进行实验。

Result: 一个简单分类器能以约75%的准确率预测输出正确性，实现早期审计。基于模型内部的度量标准在区分正确与不正确上下文方面显著优于提示基线方法，有效防止了污染上下文引入的不准确性。

Conclusion: 研究结果为理解大型语言模型的底层决策过程提供了新视角，有助于提升模型的可信度和上下文利用效率。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [38] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本研究首次系统探讨泰语文本端点（EOT）检测，发现小型微调模型能实现低延迟、适用于设备端的EOT决策。


<details>
  <summary>Details</summary>
Motivation: 流畅的语音交互需要可靠、低延迟的用户话语结束检测。传统音频静音端点检测器会引入数百毫秒延迟，并在犹豫或特定语言现象下失效。

Method: 研究比较了紧凑型LLM的零样本/少样本提示与轻量级Transformer的监督微调。利用YODAS语料库的转录字幕和泰语特有语言线索（如句末助词），将EOT问题表述为词元边界上的二元决策。

Result: 研究报告了明显的准确性-延迟权衡，并提出了可实施的方案。结果建立了泰语EOT检测的基线，并证明小型微调模型能提供适用于设备上的近乎即时EOT决策。

Conclusion: 本工作为泰语EOT检测建立了基线，并表明小型微调模型能够提供近乎即时且适用于设备端代理的EOT决策。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [39] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 本文研究了在黑盒且昂贵的LLM文本分类决策中，如何通过引入反事实来识别关键贡献词，并提出了“决策改变率”框架，实验证明反事实有助于此。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本分类任务中表现出色，但需要解释其决策。由于LLM是黑盒模型且调用成本高，因此有必要寻找有效的解释方法。

Method: 研究了将反事实（counterfactuals）纳入LLM推理中，如何影响LLM识别对其分类决策贡献最大的关键词的能力。为此，引入了一个名为“决策改变率”（decision changing rate）的框架，以量化分类中关键词的重要性。

Result: 实验结果表明，使用反事实（counterfactuals）可能是有帮助的。

Conclusion: 引入反事实可以有效帮助LLM识别文本分类中的关键贡献词，为LLM决策解释提供了一条有前景的途径。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [40] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文评估了小型语言模型（SLM）在急诊科（ED）决策支持中的潜力。结果显示，通用领域SLM在ED基准测试中表现优于医学微调模型，表明ED应用可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在急诊科面临硬件限制、运营成本和隐私等实际部署挑战。小型语言模型（SLM）因其高效性能和推理能力，能提供及时准确的信息合成，因此在急诊科的快节奏、高风险环境中具有巨大潜力，以改进临床决策和工作流程效率。

Method: 本文设计了一个综合基准，旨在识别适合急诊科决策支持的SLM，考量其医学专业知识和通用问题解决能力。评估侧重于通用领域和医学语料库混合训练的SLM。使用的基准数据集包括MedMCQA、MedQA-4Options和PubMedQA，其中医学摘要数据集模拟急诊科医生日常任务。

Result: 实验结果揭示，通用领域SLM在各项急诊科基准测试中的表现，出人意料地优于经过医学微调的同类模型。

Conclusion: 这表明对于急诊科应用而言，模型可能不需要进行专门的医学微调。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [41] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究了使用思维链（CoT）推理技术实现大语言模型（LLMs）可控多元化的方法，发现强化学习与可验证奖励（RLVR）表现最佳且训练效率高。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）的价值取向相对单一，限制了其理解细致人类视角的应用。因此，需要使LLMs具备可控多元化能力，即采纳特定视角并生成与其一致的输出。

Method: 研究探索了多种思维链（CoT）推理技术：CoT提示、基于人类创作CoT的微调、基于合成解释的微调以及可验证奖励强化学习（RLVR）。通过Value Kaleidoscope和OpinionQA数据集进行评估，并分析了生成CoT轨迹的忠实性和安全性。

Result: 在所研究的方法中，RLVR始终优于其他方法，并展现出强大的训练样本效率。此外，还对CoT轨迹的忠实性和安全性进行了分析。

Conclusion: RLVR是一种有效且高效率的方法，能够利用思维链推理技术，帮助大语言模型实现可控的多元化能力。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [42] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 本文研究扩散语言模型的数据效率来源，发现随机输入令牌掩码是关键因素，并指出随机正则化普遍提升了多epoch训练中的数据效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在有限数据下表现出卓越的数据效率，但其背后的机制尚不清楚，本研究旨在揭示这些机制。

Method: 通过进行广泛的消融实验，以解构数据效率的来源。

Result: 研究结果显示，输入令牌的随机掩码是数据效率的主要贡献者。此外，MLP dropout和权重衰减也能获得类似的数据效率提升。

Conclusion: 随机正则化在多epoch训练中能普遍增强数据效率。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [43] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 本文提出PoLi-RL，一个新颖的点到列表强化学习框架，通过两阶段课程和创新的并行切片排序奖励机制，成功将强化学习应用于条件语义文本相似度（C-STS）任务，克服了传统RL应用的挑战，并为交叉编码器架构取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 条件语义文本相似度（C-STS）旨在解决传统STS的歧义性。然而，现有方法主要局限于判别模型，未能有效整合大型语言模型（LLMs）和强化学习（RL）的最新突破。RL特别适合C-STS任务，因为它能直接优化不可微的Spearman排序指标。但简单应用列表式RL因奖励信号过于复杂和粗糙而效果不佳。

Method: 引入PoLi-RL（Point-to-List Reinforcement Learning）框架。该方法采用两阶段课程：首先通过简单的逐点奖励（pointwise rewards）训练模型建立基础评分能力；然后过渡到结合逐点、成对和列表式目标的混合奖励，以精炼模型区分细微语义差别的能力。关键创新是提出了并行切片排序奖励（Parallel Slice Ranking Reward, PSRR）机制，该机制在并行切片中计算排序奖励，每个切片由来自不同样本的相同索引补全组成，从而为每个单独补全提供精确且差异化的学习信号，实现细粒度信用分配和有效优化。

Result: 在官方C-STS基准测试中，PoLi-RL取得了48.18的Spearman相关系数，为交叉编码器（cross-encoder）架构建立了新的SOTA。

Conclusion: 本研究首次成功将强化学习应用于C-STS任务，引入了一种强大且精确的范式，用于训练大型语言模型处理复杂的、基于排序的条件判断任务。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [44] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco框架通过代码驱动增强，自动化合成高质量、可验证且多样化的指令-CoT推理数据。该框架能有效提升LLMs在数学推理任务上的表现，并具备良好的泛化能力，为构建无需人工干预的自主推理系统提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）解决复杂任务时推理能力至关重要，但实现可靠和可扩展的推理仍具挑战。现有CoT方法存在生成不可控、质量不足和推理路径多样性有限等问题。利用代码增强CoT的方法常局限于预定义数学问题，限制了可扩展性和泛化性。

Method: 提出Caco（Code-Assisted Chain-of-ThOught）框架，通过代码驱动增强，自动化合成高质量、可验证、多样化的指令-CoT推理数据。具体步骤包括：首先，在统一代码格式的现有数学和编程解决方案上微调一个基于代码的CoT生成器；然后，扩展数据生成以获得大量多样化的推理轨迹；接着，通过代码执行和基于规则的过滤实现自动验证，确保逻辑正确性和结构多样性；最后，将过滤后的输出逆向工程为自然语言指令和语言CoT，以丰富任务适应性。这是一个闭环过程，实现了完全自动化、可扩展且保证可执行性的推理数据合成。

Result: 在自建的Caco-1.3M数据集上的实验表明，Caco训练的模型在数学推理基准测试中取得了强大的竞争力，优于现有的强基线模型。进一步分析显示，Caco的代码锚定验证和指令多样性有助于在未见任务中实现卓越的泛化能力。

Conclusion: 本研究为构建无需人工干预的自维持、可信赖的推理系统建立了新的范式。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [45] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究从概念映射、隐喻-字面知识库和句法敏感性三个角度评估了大型语言模型（LLMs）的隐喻处理能力，发现LLMs在隐喻理解上存在局限，依赖训练数据而非语境线索，并对句法不规范更敏感。


<details>
  <summary>Details</summary>
Motivation: 隐喻分析受语境和外部因素影响，是一个复杂的语言现象。尽管大型语言模型（LLMs）在知识整合、语境推理和创造性生成方面表现出色，但其隐喻理解机制仍未得到充分探索。

Method: 本研究从三个方面考察了LLMs的隐喻处理能力：1) **概念映射**：通过嵌入空间投影评估LLMs如何映射目标域概念；2) **隐喻-字面知识库**：分析隐喻词及其字面对应词，以识别固有的隐喻知识；3) **句法敏感性**：评估隐喻句法结构如何影响LLMs的性能。

Result: 研究结果显示，LLMs会产生15%-25%的概念不相关解释，其表现依赖于训练数据中的隐喻指示而非语境线索，并且LLMs对句法不规范的敏感度高于对结构理解的敏感度。

Conclusion: 这些发现突显了LLMs在隐喻分析方面的局限性，并呼吁开发更强大的计算方法来解决这一问题。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [46] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 发布了斯里兰卡多领域（议会、法律、政府、新闻等）的开放式、多语言（僧伽罗语、泰米尔语、英语）机器可读文档数据集。


<details>
  <summary>Details</summary>
Motivation: 旨在支持计算语言学、法律分析、社会政治研究以及多语言自然语言处理等领域的研究。

Method: 通过建立数据收集管道，收集并整理议会记录、法律判决、政府出版物、新闻和旅游统计数据，每日更新并托管在GitHub和Hugging Face上。

Result: 截至v20251005，该数据集包含215,670份文档（60.3 GB），涵盖13个数据集，提供僧伽罗语、泰米尔语和英语三种语言。

Conclusion: 这些数据集为计算语言学和相关研究领域提供了宝贵资源，并在发布时考虑了许可和伦理因素。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [47] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 大型语言模型存在文化偏见，该项目旨在通过开发通用方法准备文化相关数据集并对Gemma 2模型进行后训练，以提高其在代表性不足语言上的表现，并提供文化遗产保护的方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英文文本和文化训练，导致它们在其他语言和文化背景下表现不佳，未能普惠所有文化。

Method: 开发一种可推广的方法来准备文化相关数据集，并对Gemma 2模型进行后训练。

Result: 旨在提高Gemma 2在一种代表性不足语言上的性能。

Conclusion: 展示了如何将生成式AI的潜力应用于不同国家，以解锁其力量并保护文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [48] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出自推测解码（SSD），一种无损推理加速方法，利用扩散大语言模型（dLLMs）自身作为草稿生成器和验证器，实现了高达3.46倍的加速，同时保持与逐步解码相同的输出质量。


<details>
  <summary>Details</summary>
Motivation: 当前的dLLM并行解码方法与逐步解码存在偏差，可能导致性能下降，从而限制了其实际部署。

Method: SSD通过引入自草稿机制，使dLLM在一次前向传播中生成多个位置的预测，并通过分层验证树进行验证。该方法利用dLLM固有的并行预测能力，避免了传统推测解码所需的辅助模块和额外的草稿模型。

Result: 实验结果表明，在LLaDA和Dream等开源模型上，SSD实现了高达3.46倍的推理速度提升，并且输出与逐步解码完全一致。

Conclusion: SSD通过无损加速解决了dLLM并行解码的性能退化问题，显著提高了其推理效率和实用性。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [49] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: 提出LTPO框架，在测试时通过优化LLM的潜在“思维”向量并利用内在置信度奖励，显著提升LLM在复杂和分布外推理任务上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的潜在推理能力（将中间思想表示为向量）在面对具有挑战性的、分布外（OOD）任务时表现脆弱，而这些场景最需要鲁棒的推理。

Method: 引入Latent Thought Policy Optimization (LTPO)，一个参数无关的框架，在测试时优化每个问题实例的中间潜在“思维”向量。它采用在线策略梯度方法，并由从冻结LLM自身输出分布计算出的内在、基于置信度的奖励信号指导，无需外部监督或文本生成。

Result: 在五个推理基准测试中，LTPO在标准任务上匹配或超越了强基线，并在其他方法失败时展现出卓越的鲁棒性。尤其在极具挑战性的AIME基准上，当现有潜在推理基线准确率接近于零时，LTPO取得了显著的性能提升。

Conclusion: LTPO提供了一种独特且无需模型参数更新的能力，能有效增强LLM在复杂和OOD推理任务上的鲁棒性和性能。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [50] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出CALM框架，通过专家提示和轻量级修改，使大型推理模型（LRMs）在优化建模任务中进行自适应修正。在此基础上，开发了4B参数的STORM模型，其在五项优化建模基准上达到了68.9%的最新平均准确率，与671B模型表现相当，证明了动态提示数据合成的有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法未能充分利用现代LRMs的先进推理能力进行优化建模，直接在传统数据集上微调效果有限。

Method: 提出CALM（Corrective Adaptation with Lightweight Modification）框架。通过专家干预识别LRM推理缺陷并提供简洁的纠正提示（修改少于2.6%的token），生成高质量数据进行监督微调（SFT），随后通过强化学习（RL）进一步改进模型。基于此框架构建了STORM模型。

Result: 基于CALM开发的4B参数LRM STORM在五项流行的优化建模基准测试中达到了68.9%的最新平均准确率，与一个671B的LRM模型表现相当。

Conclusion: 动态、基于提示的数据合成方法既保留又增强了现代LRMs固有的推理模式，为在具有挑战性的优化建模任务中实现专家级性能提供了更有效和可扩展的途径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [51] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出Reward-Enhanced Policy Optimization (REPO)框架，通过结合多源异构奖励信号，显著提升大语言模型在在线旅行社（OTA）中进行说服性议价谈判的性能、对话质量和合规性，并展现出新兴能力。


<details>
  <summary>Details</summary>
Motivation: 在在线旅行社中，LLM作为商务发展（BD）代理进行价格谈判，需平衡旅行者支付能力与酒店盈利，这对于预订和伙伴关系至关重要。然而，现有LLM后训练方法（如SFT）难以遵循标准操作规程（SOP）、处理多轮说服、解释口语输入以及遵守安全限制（如无过度承诺、无幻觉），常出现脚本过拟合、说服风格不足和业务约束执行失败等问题。

Method: 本文提出Reward-Enhanced Policy Optimization (REPO)，一个基于强化学习的后训练框架，旨在将LLM与异构奖励对齐。该框架整合了：1) 偏好训练的奖励模型（RM）用于密集的人类对齐；2) 奖励评判器（RJ）用于高层次的说服行为和SOP合规性；3) 程序化奖励函数（RF）用于对数字、格式和安全限制进行确定性检查。通过一种直接的增强机制，将RM与RJ和RF信号相结合，以遏制奖励作弊并提高谈判质量。

Result: 在生产环境式评估中（包括真实对话和不良案例对话），REPO框架表现优异：平均对话评分提升至4.63分（比基线高1.20，比DPO高0.83，比GRPO高0.33）；至少包含一个优秀响应的对话占比增至66.67%（比GRPO高23.34个百分点）；不良案例修复率达93.33%（其中75.56%为干净修复），全面超越SFT、DPO、PPO和GRPO等现有方法。此外，还观察到LLM涌现出主动同理心、局部推理和策略校准等超越黄金标注的新兴能力。

Conclusion: REPO框架有效解决了LLM在复杂议价场景中面临的挑战，通过结合多源异构奖励，显著提升了LLM作为BD代理的谈判性能、对话质量和合规性。其卓越的评估结果以及展现出的新兴能力，为在线旅行社的商务发展提供了一个强大且可靠的解决方案。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [52] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在知识同质化风险，本文提出新方法衡量LLM输出的认知多样性。研究发现LLMs多样性普遍低于网页搜索，大模型多样性低，RAG有帮助，但存在非英语知识表示差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）倾向于生成词汇、语义和风格上同质化的文本，这带来了知识坍塌的风险，即同质化的LLMs可能导致可获取信息范围随时间缩小。现有关于同质化的研究在方法上存在局限性，未能考察跨时间及文化背景的趋势。

Method: 本文提出一种新方法来衡量LLM输出中真实世界主张的“认知多样性”（epistemic diversity）。通过一项广泛的实证研究，测试了27个LLM、155个涵盖12个国家的主题以及200种源自真实用户聊天的提示变体。

Result: 研究发现：1) 新模型倾向于生成更多样化的主张，但几乎所有模型在认知多样性方面均不如基本的网络搜索。2) 模型大小对认知多样性有负面影响。3) 检索增强生成（RAG）对认知多样性有积极影响，但其改进效果因文化背景而异。4) 与传统知识来源（维基百科）相比，LLM生成的国家特定主张更多地反映英语而非当地语言，凸显了认知表示的差距。

Conclusion: LLMs存在知识同质化问题，尽管新模型有所改善，但其认知多样性仍低于传统网页搜索。模型规模越大，多样性越差，而RAG有助于提升多样性但效果受文化语境影响。当前LLMs在非英语语境下存在显著的知识表示鸿沟。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [53] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出了一种混合语言思维链（Language-Mixed CoT）推理框架，并构建了大规模韩语推理数据集Yi-Sang，训练出SOTA的KO-REAson-35B模型，显著提升了韩语推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有前沿模型在思维链推理方面表现优异，但大多集中在英语领域，对语言特定推理（尤其是非英语语言）的研究不足，存在空白。

Method: 引入了**Language-Mixed CoT**，一种将英语作为推理锚点并与目标语言切换的推理方案。针对韩语，构建了**Yi-Sang**数据集（包含5.79M韩语提示和3.7M推理轨迹），并在六个模型家族（如Qwen2.5, Llama-3.1）上训练了9个模型。通过消融实验验证了方法的有效性，并发布了相关资源。

Result: 最佳模型**KO-REAson-35B**在韩语任务上达到最先进性能（SOTA），平均得分64.0 ± 25，在9个基准测试中5项排名第一。中小型模型也平均提升了+18.6分。消融实验表明Language-Mixed CoT比单语CoT更有效，并带来了跨语言和多模态性能提升。

Conclusion: Language-Mixed CoT结合高质量特定语言数据（如Yi-Sang），能有效提升语言特定推理能力，实现SOTA性能，并对跨语言和多模态推理产生积极影响，为该领域研究提供了新方向和资源。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [54] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文引入了LongTail-Swap (LT-Swap)基准，用于评估语言模型在低数据量下学习稀有词汇的能力，并发现模型在稀有词上的表现较差，且架构差异更为显著。


<details>
  <summary>Details</summary>
Motivation: 儿童能以少量数据学习新词，具有高数据效率。现有BabyLM挑战赛的指标集中在常见词上，未能充分评估语言模型在低数据情境下学习稀有词（长尾分布）的能力。

Method: 引入了LongTail-Swap (LT-Swap)基准，该基准专注于词汇分布的长尾部分。LT-Swap是一个特定于预训练语料库的测试集，包含可接受和不可接受的句子对，用于隔离稀有词的语义和句法用法。模型通过计算每对句子的平均对数概率进行零样本评估。构建了两个分别对应10M和100M BabyLM训练集的测试集，并评估了16个BabyLM排行榜上的模型。

Result: 语言模型在稀有词上的表现普遍较差。与常见词相比，不同语言模型架构在长尾词上的性能差异更为显著。

Conclusion: LT-Swap为理解哪些架构更擅长处理稀有词泛化提供了新见解。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [55] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 该研究引入了一种累积量展开框架，以量化大型语言模型如何内化高阶统计结构，揭示了其对上下文的依赖、训练过程中的学习演变以及对不同内容类型的独特处理机制。


<details>
  <summary>Details</summary>
Motivation: 量化并理解大型语言模型（LLMs）在下一词预测过程中如何内化高阶统计结构。

Method: 引入累积量展开框架，将每层logit分布的softmax熵视为对其“中心”分布的扰动，推导出可分离高阶相关性的闭合形式累积量可观测值。通过在GPT-2和Pythia模型上使用Pile-10K提示，经验性地跟踪这些累积量。

Result: 1. 结构化提示在层间展现出特征性的上升-平稳累积量曲线，而乱序提示保持平坦，表明累积量曲线依赖于有意义的上下文。2. 在训练过程中，所有累积量单调增加直至饱和，直观展示了模型从捕获方差到学习偏度、峰度和更高阶统计结构的过程。3. 数学提示与一般文本相比，显示出独特的累积量特征，量化了模型对数学和语言内容采用根本不同的处理机制。

Conclusion: 累积量分析被确立为一种轻量级、有数学依据的探针，用于探索高维神经网络中的特征学习动态。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [56] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE通过对令牌隐藏向量的分片进行路由，而非整个令牌，解决了传统MoE的瓶颈和负载不均问题，实现了更快的推理速度、更低的困惑度、更好的专家平衡和可解释的专业化。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 层的令牌级路由将整个语义频谱分配给每个专家，导致容量瓶颈、负载不平衡和专业化受限。

Method: 引入SliceMoE架构，将令牌的d维嵌入向量分成S个连续分片，每个分片由轻量级共享路由器预测top-k专家。专家独立处理分配的分片，然后重新组合输出。此外，提出了分片级容量损失、跨分片Dropout和高效融合批处理GEMM核。

Result: SliceMoE比密集基线推理速度快1.7倍，比参数匹配的令牌MoE困惑度降低12%至18%，专家平衡性更好，并实现了语法与语义子空间的可解释专业化。

Conclusion: SliceMoE通过细粒度分片路由解决了传统MoE的挑战，显著提升了Transformer模型的推理效率、性能和专家专业化能力。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [57] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 针对波斯语情感分析面临的数据和工具稀缺问题，本文提出了一种混合机器学习与深度学习方法，利用多语言BERT极性分数作为特征并结合决策树分类器，在Pars-ABSA数据集上达到93.34%的准确率。此外，还引入了波斯语同义词和实体词典进行文本增强。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析面临挑战，原因是标注数据集稀缺、预处理工具有限以及高质量嵌入和特征提取方法不足。

Method: 提出一种结合机器学习（ML）和深度学习（DL）技术的混合方法，用于波斯语基于方面的情感分析（ABSA）。具体地，利用多语言BERT的极性分数作为附加特征，并将其整合到决策树分类器中。此外，还引入了一个波斯语同义词和实体词典，通过同义词和命名实体替换支持文本增强。

Result: 在Pars-ABSA数据集上实现了93.34%的准确率，超越了现有基准。研究结果证明了混合建模和特征增强的有效性。

Conclusion: 混合建模和特征增强在推进波斯语等低资源语言的情感分析方面表现出有效性。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT是一个可微分框架，结合反应扩散模型、SoC干预和个性化数据，用于精确预测肿瘤在标准治疗下的轨迹，并在合成数据和真实胶质瘤数据上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 肿瘤学中，精确预测标准治疗下肿瘤轨迹是一个尚未满足的重大需求，对于优化治疗计划和预测疾病进展至关重要。传统反应扩散模型无法捕捉异质性治疗范式下的肿瘤动态，因此亟需能模拟SoC干预并考虑患者间变异性（基因组、人口统计学、治疗方案）的计算框架。

Method: 引入了Standard-of-Care Digital Twin (SoC-DT)框架，它是一个可微分框架，融合了反应扩散肿瘤生长模型、离散SoC干预（手术、化疗、放疗）以及基因组和人口统计学个性化，以预测治疗后的影像肿瘤结构。同时提出了隐式-显式指数时间差分求解器IMEX-SoC，确保SoC治疗情境下的稳定性、正性和可扩展性。

Result: SoC-DT在合成数据和真实世界胶质瘤数据上均进行了评估，结果显示其在预测肿瘤动态方面，持续优于经典的偏微分方程基线模型和纯数据驱动的神经网络模型。

Conclusion: SoC-DT通过将机制可解释性与现代可微分求解器相结合，为肿瘤学中患者特异性数字孪生奠定了原则性基础，实现了生物学上一致的肿瘤动态估计。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [59] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 本文提出了一个混合框架，结合分布式多GPU推理系统和交互式可视化平台，用于分析视频节目中的名人动态，提供多维度洞察以支持娱乐分析和内容策略。


<details>
  <summary>Details</summary>
Motivation: 在视频内容日益主导的时代，理解其结构和动态变得至关重要。

Method: 该研究提出了一个混合框架，结合了：1) 一个分布式多GPU推理系统，利用优化的ONNX模型、异构批量推理和高吞吐并行性，高效处理大量视频数据并生成带时间戳的出场记录；2) 一个交互式可视化平台，将这些记录转化为多样化的图表，包括出场频率图、时长分析、饼图、共现矩阵、网络图、堆叠面积图、季节性比较和热力图。

Result: 该系统能够提供视频内容的多维度洞察，揭示名人的突出程度、屏幕时间分布、时间动态、共现关系以及跨剧集和季节的强度模式。其交互性使用户能够动态探索数据，识别关键时刻并发现个体之间不断演变的关系。

Conclusion: 通过将分布式识别与结构化、视觉驱动的分析相结合，这项工作为娱乐分析、内容创作策略和观众参与度研究开辟了新的可能性。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [60] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 本研究评估了用于水下塑料检测的跨域鲁棒性模型，发现轻量级MobileNetV2性能最佳（F1 0.97），并分析了微调模型和零样本模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染构成严重环境威胁，因此可靠的自动化水下碎片检测至关重要。然而，在特定数据集上训练的视觉系统常因域偏移而在新图像上性能下降。

Method: 研究人员在标记的水下数据集上训练了卷积神经网络（MobileNetV2、ResNet-18、EfficientNet-B0）和视觉Transformer（DeiT-Tiny、ViT-B16），然后在一个平衡的跨域测试集上进行评估。测试集由来自不同来源的含塑料图像和训练域的负样本组成。同时评估了两个零样本模型：CLIP ViT-L14和Google的Gemini 2.0 Flash。

Result: 结果显示，轻量级MobileNetV2实现了最强的跨域性能（F1 0.97），超越了更大的模型。所有微调模型都达到了高精度（约99%），但在召回率上有所差异。零样本CLIP具有较高的敏感性（召回率约80%），但易产生误报（精度约56%），而Gemini则表现出相反的特征（精度约99%，召回率约81%）。错误分析表明，模型常将珊瑚纹理、悬浮颗粒和镜面眩光误识别为塑料。

Conclusion: 总体而言，紧凑型CNN通过有监督训练可以有效地实现跨域水下检测的泛化，而大型预训练视觉语言模型则提供了互补的优势。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [61] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个用于阿拉伯语图像字幕生成的框架，它结合了基于CLIP的视觉标签检索和多模态文本生成，实现了可解释且文化上一致的字幕。


<details>
  <summary>Details</summary>
Motivation: 现有端到端字幕生成方法可能缺乏可解释性及文化相关性。本研究旨在通过引入可解释的视觉概念来生成更具文化连贯性和上下文准确性的阿拉伯语字幕。

Method: VLCAP框架分两阶段：1. 使用mCLIP、AraCLIP和Jina V4三种多语言编码器进行CLIP-based视觉标签检索，利用包含21K通用领域标签的混合词汇表提取可解释的阿拉伯语视觉概念。2. 将检索到的top-k标签转换为流畅的阿拉伯语提示，并与原始图像一起输入Qwen-VL或Gemini Pro Vision等视觉语言模型进行字幕生成。共测试了六种编码器-解码器配置。

Result: mCLIP + Gemini Pro Vision配置在BLEU-1 (5.34%) 和余弦相似度 (60.01%) 上表现最佳。而AraCLIP + Qwen-VL配置在LLM-judge分数上获得最高分 (36.33%)。

Conclusion: VLCAP提供的可解释性管道能够生成文化上连贯且上下文准确的阿拉伯语图像字幕。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [62] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 在SpaceNet数据集上，对卷积神经网络（EfficientNet-B0）和Vision Transformer（ViT-Base）在不平衡和平衡数据分布下的性能和效率进行了对比。EfficientNet-B0在效率和精度上表现更优，尤其是在平衡数据下。


<details>
  <summary>Details</summary>
Motivation: 比较卷积神经网络（EfficientNet-B0）和Vision Transformer（ViT-Base）在遥感数据集SpaceNet上，在自然不平衡和平衡重采样两种标签分布下的性能差异和部署指标。

Method: 研究者在SpaceNet数据集上对比了EfficientNet-B0和ViT-Base模型。实验设置包括两种标签分布：一种是自然不平衡的五分类，另一种是每类700张图像的平衡重采样。所有模型采用相同的预处理（224x224，ImageNet归一化），轻量级数据增强，并在NVIDIA P100上训练40个epoch。评估指标包括准确率、macro-F1、平衡准确率、每类召回率以及部署指标（模型大小和延迟）。

Result: 在不平衡数据集上，EfficientNet-B0达到93%的测试准确率，macro-F1表现强劲且延迟更低；ViT-Base也达到93%，但参数量和运行时间更大。在平衡数据集上，两个模型表现均很出色；EfficientNet-B0达到99%，ViT-Base也具有竞争力，这表明数据平衡缩小了架构差距，但CNN在效率上仍保持优势。

Conclusion: 数据平衡可以缩小不同模型架构间的性能差距。尽管ViT-Base表现良好，但在SpaceNet数据集上，EfficientNet-B0在效率上更具优势，且在平衡数据下能达到更高的准确率。研究结果通过发布清单、日志和图像级预测支持可重复性。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [63] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本论文综述了近五年基于摄像头的AI感知系统在弱势道路使用者（VRU）安全领域的最新进展，涵盖检测、跟踪、轨迹预测和意图识别，并提出了数据、模型和部署方面的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 保护弱势道路使用者（如行人和骑自行车者）的安全是一个全球性挑战，传统基础设施措施在动态城市环境中往往不足。人工智能为主动和情境感知的VRU保护提供了新机遇，但现有综述主要侧重于检测，对其他关键的视觉任务覆盖有限。

Method: 本文对过去五年基于摄像头的AI感知系统在VRU安全方面的进展和新兴趋势进行了最新审查。系统地考察了四个核心任务：检测与分类、跟踪与重识别、轨迹预测以及意图识别与预测。

Result: 通过系统审查，明确了检测与分类、跟踪与重识别、轨迹预测以及意图识别与预测这四个任务是AI驱动VRU主动保护解决方案的支柱。同时，从数据、模型和部署的角度，提出了四个主要的开放挑战。

Conclusion: 本综述旨在为下一代感知系统的开发提供基础性参考，以增强VRU安全，将视觉AI的进步与实际部署考虑相结合。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [64] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 研究表明地球观测基础模型（EOFMs）的内部表示对传感器架构高度敏感，揭示了当前设计的缺陷并为未来发展提供了方向。


<details>
  <summary>Details</summary>
Motivation: 地球观测基础模型（EOFMs）在处理海量遥感数据和执行地球监测任务方面日益普及，其输出常被用作高维数据的嵌入。然而，大多数EOFMs仅在单一模态数据上训练，并跨不同模态应用，现有工作尚不清楚多样化的传感器架构如何影响EOFMs的内部表示。

Method: 本摘要未明确描述具体研究方法，但指出研究通过展示EOFMs表示空间对传感器架构的敏感性来达成结论。

Result: 研究结果表明，EOFMs的表示空间对传感器架构高度敏感。

Conclusion: 理解EOFMs表示空间对传感器架构的敏感性，为当前EOFMs设计的潜在缺陷提供了重要视角，并为模型开发者、用户以及整个遥感科学社区指明了未来的发展方向。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [65] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 该论文提出一种基于图像修复的扰动解释技术，旨在提高生态学视觉模型的透明度和信任度，通过生成逼真、无伪影的局部编辑来揭示模型预测的关键形态线索。


<details>
  <summary>Details</summary>
Motivation: 生态监测中视觉模型应用日益广泛，但其不透明的预测结果限制了用户信任和实际应用。

Method: 提出一种以图像修复为导向、基于扰动的解释技术，能生成逼真、掩码局部化并保留场景上下文的编辑。该方法通过替换或移除目标物体（如用冰/水替换海豹）和替换背景两种干预方式，在细调后的YOLOv9海豹检测器上进行了演示，并使用Segment-Anything-Model精炼掩码。解释的有效性通过扰动图像重新评分（翻转率、置信度下降）和专家审查评估。

Result: 该技术生成的解释能够定位诊断结构，避免了传统扰动方法常见的删除伪影，并揭示了模型预测的细粒度形态线索，保持了图像的分布内特征。

Conclusion: 该方法产生的解释提供了与领域相关的深刻见解，支持专家验证，并有助于在生态学领域更值得信赖地部署AI模型。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [66] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 本文全面综述了医学图像分割(MIS)方法，涵盖从传统技术到现代深度学习，探讨了新兴趋势，并指出了该领域面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割(MIS)作为医学图像分析的核心，对精确诊断、治疗规划和病情监测至关重要。因此，有必要对从传统到深度学习的各种MIS方法进行系统性回顾和总结。

Method: 本文采用系统性综述(survey)方法，分析了MIS方法，包括阈值分割、边缘检测、区域分割、聚类算法和模型基技术等传统方法，以及CNNs、FCNs、U-Net及其变体、注意力机制、半监督学习、GANs和Transformer等深度学习架构。此外，还探讨了混合架构、跨模态学习、联邦学习和主动学习等新兴趋势，并提供了腰椎分割的案例研究。

Result: 该综述全面概述了医学图像分割领域从传统到深度学习的各种方法、最新进展和新兴趋势，并识别了其所应对的挑战（如标记数据有限、计算复杂性、模型泛化能力）。同时，通过腰椎分割的案例研究提供了具体见解。

Conclusion: 尽管医学图像分割领域取得了显著进展，但仍面临关键挑战，包括数据集偏差、领域适应、深度学习模型的可解释性以及与实际临床工作流程的有效整合。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [67] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR是一种深度聚类框架，用于半导体晶圆缺陷模式识别，能够处理复杂、无标签、不平衡的数据，并对晶圆图的方向变化具有鲁棒性，性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中，早期检测晶圆缺陷对产品良率至关重要。然而，原始晶圆数据通常复杂、无标签、不平衡且单个晶圆可能包含多种缺陷，这使得在不完美数据条件下设计可靠的聚类方法至关重要。

Method: 引入了DECOR，一个具有方向鲁棒性的深度聚类框架。该框架将晶圆图中的复杂缺陷模式聚类成一致的簇，并明确考虑了晶圆图的方向变化，确保空间相似的缺陷无论旋转或对齐如何都能一致聚类。

Result: 在开源MixedWM38数据集上进行评估，DECOR无需手动调优即可发现簇。实验表明，DECOR的性能优于现有聚类基线方法。

Conclusion: DECOR为自动化视觉检测系统提供了一个可靠且可扩展的解决方案。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [68] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 本文提出一种基于LSTM和注意力机制的神经网络模型，解决不平衡人脸情感分类中的错误校正问题，尤其在小类别上显示出有效性。


<details>
  <summary>Details</summary>
Motivation: 解决不平衡样本中人脸图像多类别分类的错误校正问题，特别是处理某些情感类别显著多于其他类别的情况。

Method: 使用基于LSTM并结合注意力机制的神经网络模型，该机制专注于面部关键区域。实验通过在六个类别子集上训练，然后对训练阶段排除的第七个类别进行错误校正。

Result: 所有类别均可进行错误校正，但成功程度不同。关键在于，在校正某些类别时，小类别的关键质量指标有所提升，表明该方法在罕见事件检测等应用中具有潜力。

Conclusion: 所提出的方法可有效应用于面部表情分析系统以及需要在倾斜类别分布下实现稳定分类的任务。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [69] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型在动态图表生成（DCG）领域研究不足的问题，提出了首个DCG基准DCG-Bench和高质量数据集DCG-8K。通过两阶段训练和联合代码-视觉奖励方法，开发了专家MLLM Qwen2.5-VL-DCG-3B，该模型在DCG任务上显著优于现有开源模型，并与专有模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在静态图表生成和理解方面取得了显著进展，但其在动态图表生成（DCG）和理解方面的潜力仍未得到充分探索，存在研究空白。

Method: 引入了DCG-Bench，这是首个评估MLLM动态图表生成能力的基准，涵盖简单文本到图表、详细文本到图表和视频到图表三个维度。构建了高质量的DCG-8K数据集，包含指令-代码-视频三元组及问答对。在此基础上，提出了一种两阶段训练方案，并引入联合代码-视觉奖励（Joint-Code-Visual Reward）进行群体相对策略优化，以构建用于DCG任务的专家MLLM Qwen2.5-VL-DCG-3B。

Result: 基准测试结果表明现有MLLMs在视频到图表任务中存在不足。提出的Qwen2.5-VL-DCG-3B模型在三项任务上的平均性能比最佳开源MLLM提高了8.31%，且在仅3B参数的情况下，与专有模型表现相当，证明了所提训练方法的有效性。

Conclusion: 本文通过引入DCG-Bench、DCG-8K数据集和高效的训练方法，成功填补了MLLMs在动态图表生成领域的空白，显著提升了MLLMs在该任务上的表现，并提供了一个高性能、参数量相对较小的模型。相关代码和数据集将公开可用。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [70] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: 本文提出VoT，一种端到端Transformer模型，通过时空注意力直接预测单目视觉里程计的相机运动，无需传统手工组件或密集几何估计，实现了更高的性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有单目视觉里程计方法复杂、依赖相机标定和超参数调优，且在未知场景中表现不佳。尽管大型3D模型有所改进，但在处理长视频和提供高精度每帧估计方面仍有限制，而这些是视觉里程计所必需的。

Method: 引入VoT（Visual odometry Transformer），一个端到端的框架。它通过提取特征并利用时空注意力建模全局关系来处理单目帧序列，直接预测相机运动，不估计密集几何，仅依赖相机位姿进行监督。该框架模块化且灵活，可无缝集成各种预训练编码器作为特征提取器。

Result: 实验结果表明，VoT能有效扩展到更大数据集，受益于更强大的预训练骨干网络，能泛化到不同的相机运动和标定设置，且性能优于传统方法，运行速度快3倍以上。

Conclusion: VoT证明了单目视觉里程计可以被有效、端到端地解决，通过直接预测相机运动并消除对传统复杂组件或密集3D重建的需求，超越了现有方法的局限性。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [71] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出一种基于侧信息的推理时搜索算法，用于引导扩散模型解决逆问题，显著提高了图像重建质量，并优于现有梯度引导方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决逆问题时常忽略侧信息，导致在病态设置下重建质量受限。此外，现有梯度引导方法易受“奖励作弊”（reward-hacking）伪影影响，限制了重建准确性和可靠性。

Method: 提出一种新颖的推理时搜索算法，利用侧信息引导扩散模型的采样过程，以平衡探索和利用。该方法可无缝集成到现有扩散模型重建流程中，并作为梯度引导的替代方案。

Result: 在盒内修复、超分辨率以及多种去模糊任务（运动、高斯、非线性、盲去模糊）等多个逆问题上，所提方法持续提升了扩散模型图像重建的定性和定量性能。实验结果表明，该方法优于包括基于奖励梯度的引导算法在内的其他基线方法。

Conclusion: 该研究提供了一种利用侧信息有效提升扩散模型在各种逆问题中重建质量和可靠性的通用方法，成功克服了传统梯度引导的局限性，并展现出优越的性能。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [72] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 本文综述了现有声纳图像数据集，旨在解决公开数据稀缺的瓶颈，为水下声学数据分析提供全面的资源目录和研究路线图。


<details>
  <summary>Details</summary>
Motivation: 公开可用的、标注良好的声纳图像数据集稀缺，严重阻碍了水下探索、自主导航和生态系统监测领域中稳健机器学习模型的发展。

Method: 对现有声纳图像数据集进行了全面综述，梳理并分类了公开可用的多模态声纳数据集（包括SSS, FLS, SAS, MBES, DIDSON），并分析了它们在分类、检测、分割和三维重建等应用中的情况。工作综合了最新发布的数据集，并将其结果整理成主表和时间线。

Result: 绘制了跨多种声纳模态的公开数据集图谱，分析了它们在不同应用中的表现。研究结果被整合为一张主表和时间轴，清晰地比较了数据集的特征、大小和标注细节。

Conclusion: 该综述为水下声学数据分析领域的研究人员提供了一个清晰的入门和进阶指南，旨在解决数据集稀缺的瓶颈问题，推动水下探索技术的发展。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [73] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 本文提出一种结合无透镜相机和基于隐式神经表示算法的方法，无需专业硬件即可从多视角捕获显示器特性，以简化显示器校准任务。


<details>
  <summary>Details</summary>
Motivation: 显示器校准对内容创作者而言是基本但麻烦的任务，现有方法测量显示器多视角特性需专业设备和暗室，导致大多数用户难以实现。

Method: 共同设计了一种无透镜相机和一个基于隐式神经表示（INR）的算法，用于从不同视角捕获显示器特性。

Result: 该管线能够高效重建显示器发出的光场，覆盖46.6° x 37.6°的观看锥角。

Conclusion: 该新兴管线为实现轻松的显示器校准和特性描述奠定了初步基础，避免了对专业硬件的需求。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [74] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 本文介绍了一种名为“溯源网络”的新型神经网络模型，通过将预测结果直接关联到训练样本，实现端到端、基于训练数据的可解释性，从而提升模型透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型的“黑箱”问题，如不透明性、幻觉，并提供一种内嵌于模型架构的可解释性，以提高神经网络的透明度、鲁棒性和可信度，并能将功劳归因于数据贡献者。

Method: 引入溯源网络（provenance networks），一种新型神经网络模型。该模型在正常操作中，将每个预测直接链接到支持它的训练样本，将可解释性嵌入架构中。其工作原理类似于学习型KNN，通过特征空间中加权的具体实例来解释每个输出。

Result: 该方法促进了记忆与泛化之间权衡的系统性研究；能验证输入是否包含在训练集中；有助于检测错误标记或异常数据点；增强了对输入扰动的弹性；并支持识别对新数据点生成有贡献的相似输入。它提供了传统深度网络无法提供的模型行为洞察。

Conclusion: 溯源网络通过联合优化主任务和可解释性目标，解决了现代深度学习中的关键挑战，如模型不透明性、幻觉和数据贡献者归因，从而提高了神经网络的透明度、鲁棒性和可信度。尽管有计算成本和数据集规模限制，但它为现有可解释性技术提供了一种有益的补充方法。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [75] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 无监督异常检测(UAD)面临匹配噪声和单模态/多模态方法分离的挑战。本文提出统一代价过滤(UCF)，一个通用的后处理优化框架，通过可学习的注意力引导过滤模块，消除异常代价体积中的匹配噪声并突出细微异常。UCF显著提升了各种UAD方法在单模态和多模态场景下的性能，达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测(UAD)方法（无论是基于重建还是基于嵌入）在生成异常图时，通过图像或特征匹配来完成，但通常忽视了“匹配噪声”问题，这限制了它们的检测能力。此外，尽管多模态UAD（如RGB-3D、RGB-Text）日益发展，但单模态和多模态方法之间仍相对孤立，阻碍了对共同挑战的全面理解和知识迁移。

Method: 本文提出了统一代价过滤(UCF)，这是一个通用的后处理优化框架，旨在对任何UAD模型的异常代价体积进行细化。UCF的核心思想是从匹配视角统一单模态和多模态UAD。它首先通过将测试样本与来自相同或不同模态的正常样本进行匹配来构建代价体积，然后使用一个可学习的过滤模块，该模块结合了测试样本的多层注意力引导，以缓解匹配噪声并突出细微异常。

Result: 在22个不同的基准数据集上进行的综合实验表明，UCF能有效提升多种现有UAD方法的性能。它在单模态(RGB)和多模态(RGB-3D、RGB-Text)UAD场景中持续取得新的最先进(SOTA)结果，证明了其优越性。

Conclusion: 本文成功解决了现有UAD方法中匹配噪声的局限性以及单模态和多模态UAD方法之间的隔离问题。通过提出的UCF这一统一且通用的后处理优化框架，显著提高了各种设置下异常检测的准确性和鲁棒性，为该领域树立了新的基准。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [76] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 本文提出一个基于视觉语言模型（VLM）的框架，用于自动化评估和优化工业图纸（如P&ID）上的目标检测结果，以解决缺乏自动质量评估方法的问题。


<details>
  <summary>Details</summary>
Motivation: 工业图纸数字化对构建数字孪生和智能自动化至关重要，但精确的目标检测是核心挑战。现有目标检测算法虽有进步，却缺乏自动评估其输出质量的方法。

Method: 引入一个利用视觉语言模型（VLM）评估目标检测结果并指导其改进的框架。该方法利用VLM的多模态能力来识别缺失或不一致的检测。

Result: 实现了目标检测结果的自动化质量评估。

Conclusion: 通过VLM实现自动化质量评估和指导检测优化，从而提高了复杂工业图纸上的整体检测性能。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [77] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 本文引入SpatialViLT，一个通过整合深度图、3D坐标等空间特征的增强型视觉-语言模型，解决了VLM在3D空间推理上的挑战，并在VSR数据集上实现了最先进的（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在3D场景和复杂物体配置的空间推理方面仍面临挑战。

Method: 引入SpatialViLT，一个通过多任务学习框架整合深度图、3D坐标和边缘图等空间特征的增强型VLM。提出两种变体：SpatialViLT（关注完整物体区域）和MaskedSpatialViLT（关注蒙版物体区域）。SpatialEnsemble结合了这两种方法。

Result: 提出的模型在Visual Spatial Reasoning (VSR) 数据集上，在方向、拓扑和邻近关系等空间推理类别上表现出色，其中SpatialEnsemble实现了最先进的（SOTA）准确性。

Conclusion: 本工作在增强AI系统的空间智能方面迈出了重要一步，对高级多模态理解和现实世界应用具有关键意义。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [78] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本研究利用合成数据训练编码器-解码器网络（DAE和U-Net）对两相光切结构光照（OS-SI）图像中的伪影进行去噪，成功提高了图像清晰度。


<details>
  <summary>Details</summary>
Motivation: 两相OS-SI因采集时间缩短会产生残余伪影，传统去噪方法难以抑制。深度学习虽有潜力，但监督训练受限于缺乏干净的光切真实数据。

Method: 研究者通过将真实伪影场应用于合成图像来构建合成训练对，并使用这些数据训练了一个非对称去噪自编码器（DAE）和一个U-Net网络，随后在真实OS-SI图像上进行了评估。

Result: 两种网络都提升了图像清晰度，且各自在不同类型的伪影抑制上表现出色。

Conclusion: 这些结果表明，合成训练能够实现OS-SI图像的监督去噪，并突出了编码器-解码器网络在简化重建工作流程方面的潜力。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [79] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一种多模态框架，通过通路激活分数表示转录组学并与组织病理学对齐，在癌症空间转录组学数据集中显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有结合组织病理学和空间转录组学的方法多依赖少数高变基因，限制了预测范围，且忽略了组织表型背后的协同生物学程序。

Method: 提出PEaRL（Pathway Enhanced Representation Learning）框架。该框架通过ssGSEA计算通路激活分数来表示转录组学，并使用Transformer编码通路信号。然后，通过对比学习将这些通路信号与组织病理学特征对齐，以降低维度、提高可解释性并加强跨模态对应。

Result: 在三个癌症空间转录组学数据集（乳腺癌、皮肤癌、淋巴结癌）上，PEaRL在基因和通路水平的表达预测精度上均持续优于SOTA方法，皮尔逊相关系数分别最高提升58.9%和20.4%。

Conclusion: 将转录组学表示与生物通路相结合，能产生更符合生物学原理且更具可解释性的多模态模型，推动计算病理学超越基因水平的嵌入。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [80] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一个高效的多模态医学图像分析深度学习框架，通过新颖的视觉-语言和分层语义提示实现细粒度控制，在分割和预后预测任务上表现出色，并具有很强的泛化能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像深度学习模型存在泛化性差、缺乏预后能力、对医学语义理解不足以及“通用”方法过于简单的问题，阻碍了其在临床中的应用。

Method: DuPLUS引入了一个新颖的视觉-语言框架，利用分层语义提示实现对分析任务的细粒度控制。它采用了一个由独特的双提示机制驱动的、文本控制的分层架构，以实现对不同医学任务的扩展性。通过参数高效微调技术，实现对新任务和模态的快速适应。此外，该框架还可无缝集成电子健康记录（EHR）数据进行预后预测。

Result: 在分割任务上，DuPLUS能够泛化到三种成像模态、十个解剖学上不同的医学数据集（涵盖30多种器官和肿瘤类型），并在10个数据集中有8个超越了最先进的特定任务和通用模型。在头颈癌数据集上，DuPLUS通过集成EHR数据进行预后预测，获得了0.69的一致性指数（CI）。

Conclusion: DuPLUS通过其多模态分析、细粒度控制、强大的泛化能力、可扩展性以及快速适应性，被确立为一种多功能且具有临床相关性的医学图像分析解决方案。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [81] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出一种移动优化的两阶段深度学习框架，通过线程化并行YOLOv10检测和MobileSAM分割，实现野外动物的实时、高效检测与分割，并在濒危物种上表现出色。


<details>
  <summary>Details</summary>
Motivation: 野外环境中动物的实时检测与分割对野生动物保护至关重要，但受限于计算资源和物种伪装性，现有方法难以实现高效非侵入式监测。

Method: 提出一个移动优化的两阶段深度学习框架，整合线程化检测模型（TDM），并行执行基于YOLOv10的检测和基于MobileSAM的轻量级分割，通过线程化降低延迟以提高实时性能。同时，构建了一个包含4万张带注释图像的Houbara Bustard数据集。

Result: 在Houbara Bustard数据集上，模型mAP50达到0.9627，mAP75达到0.7731，mAP95达到0.7178，MobileSAM的mIoU为0.7421。YOLOv10每帧运行时间为43.7毫秒，证实了实时能力。

Conclusion: 该框架通过创新的线程化并行处理，有效解决了有限计算资源下野外动物实时检测和分割的挑战，实现了高精度和高效率，为野生动物监测提供了实用的解决方案。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [82] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: 本文提出了Platonic Transformer，通过引入柏拉图固体对称群参考系，在不增加计算成本和保持灵活性的前提下，赋予Transformer几何对称性归纳偏置，并在多领域取得竞争力。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer缺乏对几何对称性的归纳偏置，而现有等变方法常以牺牲Transformer的效率和灵活性为代价，设计复杂且计算密集。研究旨在解决这一效率与等变性之间的权衡。

Method: 引入Platonic Transformer，通过定义相对于柏拉图固体对称群参考系的注意力，实现一种原则性的权重共享方案。该注意力形式上等价于动态群卷积，模型学习自适应几何滤波器，并可实现高效的卷积变体。

Result: Platonic Transformer实现了对连续平移和柏拉图对称性的组合等变性，同时保留了标准Transformer的架构和计算成本。在计算机视觉（CIFAR-10）、3D点云（ScanObjectNN）和分子性质预测（QM9, OMol25）等多样化基准测试中，在不增加额外成本的情况下取得了有竞争力的性能。

Conclusion: Platonic Transformer提供了一种有效解决Transformer几何对称性不足的方法，它在保持效率和灵活性的同时，引入了强大的几何约束，并在多个领域展现出卓越的性能和普适性。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [83] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本文综述了面向语义分割的域泛化（DG）领域，分析了现有方法，并指出基础模型对DG产生了显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络取得了巨大进展，但其泛化到未知领域的能力仍是主要挑战。域泛化（DG）领域应运而生，旨在使模型能够泛化到多个不同的、未见的域，尤其对于语义分割等任务至关重要。

Method: 本文进行了一项综合性调查，对现有面向域泛化语义分割的方法进行了分类和综述，并识别出向基于基础模型的域泛化的范式转变。此外，还对所有方法进行了广泛的性能比较。

Result: 研究结果表明，基础模型对域泛化产生了显著影响。

Conclusion: 基础模型对域泛化具有重要影响。本调查旨在推动域泛化研究，并启发研究人员探索新的研究方向。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [84] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 本文提出一种基于Transformer的两阶段训练模型，实现内窥镜检查报告的自动生成，以减轻医生文档负担。


<details>
  <summary>Details</summary>
Motivation: 胃肠病医生在内窥镜检查（如EGD和结肠镜检查）中面临繁重的文档工作，导致临床工作流程效率低下和医生倦怠。

Method: 开发了一种新颖的自动化报告生成模型，该模型包含一个基于Transformer的视觉编码器和文本解码器，采用两阶段训练框架。第一阶段，在图像/文本标题对上预训练；第二阶段，在图像/报告对上进行微调以生成临床发现。

Result: 该方法有望简化文档流程，减轻医生工作量，并改善患者护理。

Conclusion: 所提出的自动化报告生成模型有效解决了内窥镜检查的文档负担，具有提高临床效率和患者护理的潜力。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [85] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一种基于扩散的无人机路径规划器，能将2D手绘草图转换为3D飞行路径，实现零样本模拟到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法，通过人类在深度图像上的2D手绘草图，为无人机生成3D飞行路径，从而实现直观的无人机导航。

Method: 提出SketchPlan，包含两部分：SketchAdapter将手绘草图映射到2D投影路径；DiffPath（扩散模型）根据2D投影和深度图推断3D轨迹。模型通过32k合成飞行路径（自动标注2D投影）和872条真实人工标注2D草图的3D飞行路径混合训练。

Result: SketchPlan在模拟和现实世界中均表现出色，实现零样本模拟到现实迁移。在真实无人机测试中，低/中度杂乱环境成功率为100%，高杂乱环境为40%，任务完成率比关键消融模型高20-60%。模块化设计和混合数据训练显著提升性能。

Conclusion: SketchPlan作为一种扩散模型，能有效解释2D手绘草图并生成3D无人机飞行路径，展现出强大的模拟到现实迁移能力，并在复杂真实环境中实现鲁棒导航，其模块化设计和混合数据训练方式是关键成功因素。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [86] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: AI视频会议系统存在实时换脸（木偶操控）的安全漏洞，现有检测方法无效。本文提出一种新颖的生物识别防御机制，通过分析传输的姿态-表情潜在信息中的身份线索，实时检测非法身份互换，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI驱动的视频会议系统通过传输紧凑的姿态-表情潜在信息以减少带宽，但攻击者可利用此潜在信息实时劫持受害者形象（木偶操控）。由于视频完全合成，现有深度伪造检测器无法有效识别，因此亟需一种新的安全防御方案。

Method: 本文提出一种不依赖重建RGB视频的生物识别泄露防御方法。该方法采用一个姿态-条件化、大间隔对比编码器，从传输的潜在信息中分离出持久的身份线索，并消除瞬态的姿态和表情。通过对解耦后的嵌入进行简单的余弦测试，即可实时标记非法身份互换。

Result: 实验结果表明，该方法在多个talking-head生成模型上持续优于现有木偶操控防御方案，能实时操作，并对分布外场景表现出强大的泛化能力。

Conclusion: 本研究成功开发并验证了首个无需重建RGB视频的生物识别泄露防御机制，有效解决了AI驱动视频会议系统中实时换脸的安全问题，并在性能、实时性和泛化能力上均优于现有技术。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [87] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出REVEL任务，旨在实现对自回归视频扩散模型进行流式、精细化的拖拽交互控制。针对潜在空间漂移和上下文干扰两大挑战，提出无训练方法DragStream，通过自适应分布自校正和空频选择性优化机制，显著提升用户对生成视频的操控性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型难以实现流式、精细化的输出控制，使其难以持续符合用户预期。具体挑战在于：1) 拖拽操作引起的扰动在潜在空间积累，导致严重的潜在分布漂移，进而中断拖拽过程；2) 流式拖拽易受上下文帧干扰，导致视觉上不自然的结果。

Method: 1. 提出了新任务REVEL (stReaming drag-oriEnted interactiVe vidEo manipuLation)，统一了拖拽式视频操作，支持用户自定义平移、变形和旋转等效果。
2. 提出了一种无训练的方法DragStream来解决REVEL任务，该方法包含：i) 一种自适应分布自校正策略，利用相邻帧的统计信息有效约束潜在嵌入的漂移；ii) 一种空频选择性优化机制，在充分利用上下文信息的同时，通过选择性传播视觉线索来减轻其干扰。

Result: DragStream方法可以无缝集成到现有自回归视频扩散模型中。广泛的实验有力地证明了DragStream的有效性。

Conclusion: 通过引入REVEL任务和无训练的DragStream方法，成功解决了自回归视频扩散模型在流式、精细化控制方面的挑战，有效抑制了潜在分布漂移和上下文干扰，从而实现了更自然、符合用户期望的交互式视频操作。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [88] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: 本文提出了GAS-MIL框架，一个灵活的集成方法，能够无缝整合多个基础模型的特征，在计算病理学任务中表现优异，并简化了模型部署。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的基础模型虽强大，但针对特定诊断任务的适配和基准测试耗时且资源密集，尤其考虑到模型的规模和多样性，这是一个亟待解决的挑战。

Method: 引入了Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) 框架。GAS-MIL是一个灵活的集成方法，能够无缝整合来自多个基础模型的特征，且无需手动特征选择或大量任务特定微调，以保留其互补优势。

Result: 在三个癌症数据集（前列腺癌PANDA、卵巢癌UBC-OCEAN和乳腺癌TCGA-BrCa）的分类任务中，GAS-MIL的表现持续优于或与单个基础模型及现有MIL方法持平，充分展示了其鲁棒性和泛化能力。

Conclusion: GAS-MIL通过实现异构基础模型的高效整合，简化了病理学中模型的部署，并为未来的多模态和精准肿瘤学应用奠定了可扩展的基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [89] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 本研究引入了无人机辅助纳洛酮递送模拟数据集（DANDSD），并提出了一个基于视频的实时态势感知（SA）评估框架，利用图嵌入和Transformer模型，评估非医疗专业旁观者在模拟阿片类药物过量紧急情况下的SA，以支持开发自适应无人机系统。


<details>
  <summary>Details</summary>
Motivation: 在阿片类药物过量紧急情况下，无人机快速递送纳洛酮为非医疗专业旁观者提供了挽救生命干预的机会。然而，在人机协作（HAT）中，旁观者的态势感知（SA）至关重要，但目前缺乏实时的SA评估方法。

Method: 研究首先创建了DANDSD数据集，该数据集在模拟阿片类药物过量场景中，捕获了大学生旁观者（无医疗培训）向模拟受害者施用鼻内纳洛酮时的人机协作数据。在此基础上，提出了一种基于视频的实时SA评估框架，该框架利用图嵌入和Transformer模型，整合了几何、运动学和交互图特征等视觉感知和理解线索来评估旁观者的SA。

Result: 该方法实现了高性能的SA预测，并展示了强大的时间分割准确性。在平均帧率（MoF）上，其性能比FINCH基线高出9%；在交并比（IoU）上，高出5%。

Conclusion: 这项工作为开发能够有效指导旁观者的自适应无人机系统提供了支持，从而有望改善紧急响应结果并挽救生命。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [90] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 本研究评估了Tesseract、EasyOCR、PaddleOCR和TrOCR四款开源OCR系统在食品包装图像上的性能，旨在提取配料表和营养成分。Tesseract在准确性方面表现最佳（CER最低，BLEU最高），EasyOCR在准确性和多语言支持之间取得了良好平衡，PaddleOCR覆盖率高但速度慢，TrOCR表现最弱。研究为包装OCR提供了基准。


<details>
  <summary>Details</summary>
Motivation: 食品包装上的OCR对合规性和营养监测至关重要，但由于多语言文本、密集布局、字体多样、眩光和曲面等挑战而难以实现。本研究旨在评估开源OCR系统提取食品包装上配料表和营养成分面板的能力。

Method: 评估了Tesseract、EasyOCR、PaddleOCR和TrOCR四款开源OCR系统。使用231种产品（1,628张图像）的数据集评估速度和覆盖率，并创建了113张图像（60种产品）的真值子集用于准确性评估。衡量指标包括字符错误率（CER）、词错误率（WER）、BLEU、ROUGE-L、F1、覆盖率和执行时间。

Result: 在真值子集上，Tesseract实现了最低的CER（0.912）和最高的BLEU（0.245）。EasyOCR在准确性和多语言支持之间取得了良好平衡。PaddleOCR实现了近乎完整的覆盖率，但由于GPU不兼容导致仅在CPU上运行，速度较慢。尽管有GPU加速，TrOCR仍产生了最差的结果。

Conclusion: 这些结果为包装特定OCR提供了基准，建立了基线，并指出了未来研究方向，如布局感知方法和文本定位。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [91] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle是一个轻量级模块，通过预测相关帧和所需帧数，解决了视觉语言模型（VLMs）在视频理解中处理输入帧数限制的问题，显著提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）的视频理解性能受限于可处理的输入帧数。当前的帧采样策略（如均匀或固定预算选择）效率低下，且未能适应信息密度和任务复杂性变化，导致信息丢失。

Method: 本文提出了FrameOracle，一个轻量级即插即用模块，用于预测与给定查询最相关的帧以及所需的帧数。FrameOracle采用四阶段课程训练，前三阶段依赖弱代理信号（如跨模态相似性），最后阶段利用新引入的FrameOracle-41K数据集（首个提供关键帧标注的大规模VideoQA集合）的强监督。

Result: 在五个VLM和六个基准测试中，FrameOracle将16帧输入平均减少到10.4帧，同时不损失准确性。从64帧候选中，它将输入平均减少到13.9帧，同时将准确性提高1.4%。该方法在可扩展视频理解中实现了最先进的效率-准确性权衡。

Conclusion: FrameOracle通过智能的帧选择策略，有效提高了视频理解任务的效率和准确性，解决了VLM在处理大量视频帧时的限制，为可扩展视频理解提供了新方法。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [92] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出了一种混合Co-FineTuning (CFT) 方法，通过整合有标签和无标签数据，以及利用来自目标游戏和多领域游戏的有标签样本，有效解决了视频游戏视觉bug检测中标签数据稀缺的问题，提高了检测性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 视频游戏中的视觉bug手动识别耗时耗力且成本高昂，需要专业知识。尽管监督视觉bug检测模型有前景，但由于视觉bug发生频率低，依赖大量标注数据集成为一个重大挑战。

Method: 提出一种混合Co-FineTuning (CFT) 方法。该方法整合了有标签和无标签数据，利用来自目标游戏和多领域游戏的有标签样本，并结合无标签数据来增强特征表示学习。此策略最大限度地利用所有可用数据，大幅减少对特定目标游戏标注样本的依赖。

Result: 实验结果表明，所提出的方法在游戏视觉bug检测方面具有鲁棒性，在多个游戏环境中表现优于传统基线。即使只用目标游戏50%的标注数据进行训练，CFT仍能保持有竞争力的性能。

Conclusion: CFT方法能有效解决视频游戏视觉bug检测中标签数据稀缺的问题，通过整合多种数据源，显著提高了检测的效率、可扩展性和适应性，并在有限标注数据下依然表现出色。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [93] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: 在无数据增强的小分辨率图像分类任务中，HRM模型表现不佳，不如简单CNN，但未来有改进潜力。


<details>
  <summary>Details</summary>
Motivation: 评估Hierarchical Reasoning Model (HRM) 在作为实用图像分类器时的表现，尤其是在不使用数据增强的原始条件下。

Method: 该研究采用包含两个Transformer风格模块 $(f_L,f_H)$ 的HRM，结合一步 (DEQ风格) 训练、深度监督、旋转位置嵌入和RMSNorm。模型在MNIST、CIFAR-10和CIFAR-100数据集上进行评估，条件是严格的“原始”设置：无数据增强、特定优化器（单epoch预热和余弦衰减）和标签平滑。

Result: HRM在MNIST上表现良好（约98%测试准确率），但对小型自然图像（CIFAR-10, CIFAR-100）过拟合且泛化能力差。在CIFAR-10上，HRM达到65.0%，远低于Conv-BN-ReLU基线的77.2%（且基线训练快30倍）；在CIFAR-100上，HRM测试准确率仅为29.7%（训练准确率91.5%），而CNN基线达到45.3%。分析表明，优化过程健康，但HRM缺乏针对图像的归纳偏置。

Conclusion: 在无数据增强的小分辨率图像分类场景下，当前版本的HRM模型不具备与简单卷积架构竞争的能力。但研究也指出，未来的模型修改可能使其性能大幅提升。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [94] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 该综述分析了自监督学习方法DINOv2的核心思想、性能表现、涌现特性、局限性、影响及未来方向。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）在视觉特征学习方面取得显著进展，特别是DINOv2已超越弱监督学习（WSL）方法，成为新的最先进技术。本研究旨在探究其成功的核心思想。

Method: 本综述通过以下方式进行：审视DINOv2的核心思想（多裁剪视图增强、平均教师自蒸馏）并追溯其发展；比较DINO和DINOv2与其他SSL和WSL方法在多种下游任务上的性能；突出其基于Transformer骨干学习特征的显著涌现特性。

Result: DINO/DINOv2学习到的特征（使用Transformer骨干）展现出显著的涌现特性；通过与其他SSL和WSL方法的性能比较，评估了其在各种下游任务上的表现。

Conclusion: 文章最后讨论了DINOv2的局限性、其影响以及未来的研究方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [95] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 本文提出Diffusion-Classifier Synergy (DCS)框架，通过扩散模型与FSCIL分类器的互助循环和多维度奖励机制，生成高质量数据以克服数据稀缺问题，显著提升FSCIL的性能。


<details>
  <summary>Details</summary>
Motivation: Few-Shot Class-Incremental Learning (FSCIL)面临数据稀缺和稳定性-可塑性困境。现有FSCIL方法因依赖有限数据集而泛化能力不足。扩散模型虽能用于数据增强，但直接应用存在语义错位和指导无效的问题。

Method: 引入Diffusion-Classifier Synergy (DCS)框架，建立扩散模型与FSCIL分类器之间的互助提升循环。DCS采用奖励对齐学习策略，利用分类器状态派生的动态多维奖励函数指导扩散模型。该奖励系统在特征层面（通过原型锚定最大均值差异和维度方差匹配确保语义一致性和多样性）和Logits层面（通过置信度校准和跨会话混淆感知机制增强探索性生成和类间可辨识性）协同运作。

Result: 在FSCIL基准测试中取得了最先进的性能，显著增强了知识保留能力和新类学习能力。

Conclusion: DCS通过扩散模型与分类器的协同进化以及精巧的多维奖励机制，有效解决了FSCIL中的数据稀缺和泛化挑战，成功平衡了模型学习新类的可塑性与保留旧知识的稳定性，达到了最先进的性能。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [96] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: 本文提出MonitorVLM，一个新型视觉-语言框架，通过领域特定数据集、子句过滤和行为放大模块，显著提升了高风险行业（如采矿）中不安全行为的自动化检测精度和召回率，取代了传统人工检查。


<details>
  <summary>Details</summary>
Motivation: 高风险行业（如采矿）的工业事故常由不安全行为引起。传统人工检查劳动密集、易出错且效率低下，无法满足大规模动态环境的需求，因此急需智能自动化安全监控解决方案。

Method: 本文提出了MonitorVLM视觉-语言框架，直接从监控视频流中检测安全违规行为。主要创新包括：(1) 构建了一个包含9,000个VQA样本的领域特定违规数据集；(2) 设计了子句过滤器(CF)模块，动态选择最相关子句以降低推理延迟；(3) 开发了行为放大器(BM)模块，增强工人区域以改进细粒度动作识别。系统还集成了一个轻量级基于网络的界面。

Result: 实验表明，MonitorVLM显著优于基线视觉-语言模型，在精度上提升22.01%，召回率提升34.22%，F1分数提升28.37%。其中，CF模块在保持准确性的前提下，将推理延迟降低了13.56%；BM模块使精度额外提高了3.45%，召回率提高了8.62%。

Conclusion: 该研究强调了多模态大模型在提升采矿及其他高风险领域职业安全监控方面的巨大潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [97] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 提出一种将引导分类与扩散模型相结合的新型混合模型，用于智能交通系统（ITS）中的事故检测，实现了97.32%的图像事故检测准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在处理复杂数据分布方面存在局限性；扩散模型有望显著改进智能交通系统中的事故检测。

Method: 开发了一种融合引导分类与扩散技术的新型混合模型。该模型将预训练ExceptionNet架构的输出作为扩散模型的输入，并以图像张量作为条件。模型包含多个条件模块，利用时间嵌入和图像协变量嵌入动态调整输入投影。采用云端部署以提高处理效率和可扩展性，并通过消融研究探讨了扩散模型的关键特性。

Result: 所提出的扩散模型在基于图像的事故检测中表现最佳，准确率达到97.32%。

Conclusion: 该混合扩散模型通过有效理解复杂数据分布，显著提高了智能交通系统中基于图像的事故检测性能，超越了传统分类方法的局限性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [98] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: RGB-T SOD模型SAMSOD通过单模态监督、梯度解冲突及解耦适配器，解决了模态收敛不平衡和高低激活梯度差异问题，显著提升了多模态显著目标检测的性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T显著目标检测方法（包括基于SAM微调的方案）忽视了双模态收敛的不平衡性以及高低激活之间显著的梯度差异，导致模型性能存在进一步提升的空间。

Method: 本文提出SAMSOD模型，主要方法包括：1) 利用单模态监督增强非主导模态的学习；2) 采用梯度解冲突机制，以减少冲突梯度对模型收敛的影响；3) 引入两个解耦适配器，分别掩盖高低激活神经元，通过增强背景学习来突出前景对象。

Result: 在RGB-T SOD基准数据集上的基础实验以及在涂鸦监督RGB-T SOD、全监督RGB-D SOD和全监督RGB-D轨道表面缺陷检测等任务上的泛化性实验，均证实了所提方法的有效性。

Conclusion: SAMSOD模型通过创新性地解决模态收敛不平衡和梯度差异问题，显著提升了RGB-T显著目标检测的性能和跨任务泛化能力，为多模态感知任务提供了有效解决方案。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [99] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文提出针对自动驾驶场景中小型目标指代表达理解（REC）的新数据集SOREC和新方法PIZA，显著提升了模型对小型目标的定位精度。


<details>
  <summary>Details</summary>
Motivation: 指代表达理解（REC）在定位目标物体方面取得进展，但对于自动驾驶等实际应用中极其微小目标的定位仍然面临巨大挑战。

Method: 引入了小型目标REC（SOREC）数据集，包含10万对自动驾驶场景中的小型目标指代表达和边界框；提出了渐进迭代缩放适配器（PIZA），一个参数高效微调的适配器模块，使模型能逐步放大并定位小型目标。

Result: 将PIZA应用于GroundingDINO模型，在SOREC数据集上展示了显著的准确性提升。

Conclusion: SOREC数据集和PIZA方法有效解决了小型目标REC的挑战，相关资源已公开。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [100] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 本文提出了一种名为Attention-WNet的新型深度学习方法，通过将注意力机制融入WNet模型，用于视网膜血管动脉-静脉分割，并在公开数据集上超越了现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管（动脉和静脉）分割是视网膜血管分析的先决条件，能为诊断多种视网膜眼病提供潜在洞察和生物标志物。视网膜血管的规律性和宽度变化可作为全身血管系统健康的指标，有助于识别中风和心肌梗死等血管疾病的高风险患者。

Method: 本文提出了一种新的深度学习方法，名为Attention-WNet，该方法基于将注意力机制整合到WNet深度学习模型中，以实现动脉-静脉分割。

Result: 所提出的Attention-WNet方法已在HRF和DRIVE等公开数据集上进行了测试，并超越了文献中其他现有最先进的模型。

Conclusion: Attention-WNet模型通过引入注意力机制，有效提升了视网膜动脉-静脉分割的性能，为视网膜血管分析和相关疾病诊断提供了更优的工具。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [101] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 通过对LAION-400M数据集进行大规模人口统计学标注，该研究揭示了视觉-语言模型偏见的来源，发现数据中存在有害关联，并证明数据共现可解释模型中的大部分性别偏见，从而首次建立了数据集构成与模型偏见的实证联系。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型普遍存在人口统计学偏见，但训练数据在产生这些偏见中的作用尚不明确，主要原因是缺乏大规模网络数据集（如LAION-400M）的人口统计学标注。

Method: 通过结合物体检测、多模态字幕和微调分类器的自动化标注流程，为LAION-400M数据集创建了以人为中心的标注，包括超过2.76亿个边界框、感知的性别和种族/民族标签以及自动生成的描述。

Result: 1. 揭示了数据中人口统计学的不平衡和有害关联，例如男性以及被认为是黑人或中东裔的个体与犯罪相关及负面内容过度关联。2. 表明CLIP和Stable Diffusion中60-70%的性别偏见可以由数据中的直接共现线性解释。

Conclusion: 该研究工作首次在大规模层面建立了数据集构成与下游模型偏见之间的经验联系。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [102] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 本研究比较通用型与卫星图像专用型预训练神经网络在里约热内卢贫民窟检测中的表现，以探究任务特异性或数据量对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法未充分利用最新预训练神经网络在非正式住区检测中的潜力。研究旨在探讨任务特异性（专用网络）与数据量（通用网络）对城市非正式住区检测性能的相对重要性。

Method: 对比两种预训练神经网络：1. 在大型多样化非特定图像数据集上预训练的通用网络。2. 在卫星图像上预训练的专用网络。应用于里约热内卢贫民窟的检测任务。

Result: 抽象中未提及具体研究结果。

Conclusion: 抽象中未提及具体结论。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [103] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 研究表明现有主动式Deepfake防御缺乏鲁棒性。提出了一种名为LoRA patching的新方法，可以绕过这些防御，甚至能嵌入可见警告，揭示了当前防御策略的脆弱性。


<details>
  <summary>Details</summary>
Motivation: Deepfakes带来显著的社会风险，促使人们开发主动式防御以防止图像篡改。然而，现有通过在面部图像中嵌入对抗性扰动的主动式防御通常缺乏鲁棒性和可靠性。

Method: 提出了一种名为低秩适应（LoRA）补丁的新方法，通过将即插即用LoRA补丁注入Deepfake生成器来绕过最先进的防御。该方法使用可学习的门控机制自适应控制LoRA补丁的效果并防止微调期间的梯度爆炸。同时引入了多模态特征对齐（MMFA）损失，鼓励对抗性输出的特征在语义层面与期望输出对齐。此外，还提出了防御性LoRA patching，可在输出中嵌入可见警告，以缓解新的安全漏洞。

Result: 仅使用1,000个面部图像示例和单次微调迭代，LoRA patching成功击败了多种主动式防御。

Conclusion: 这些结果揭示了当前Deepfake防御范式的关键弱点，并强调了开发更鲁棒的Deepfake防御策略的必要性。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [104] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 针对VPR在训练-测试域差距大的挑战，本文提出利用测试时的参考集（“地图”）对VPR模型进行简单微调（RSF），平均提升SOTA模型在Recall@1上约2.3%，并保持泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当VPR方法的测试环境与常规训练数据集显著不同时，现有方法表现不佳，存在明显的训练-测试域差距，导致在挑战性基准测试中难以取得良好性能。

Method: 提出Reference-Set-Finetuning (RSF) 方法。该方法利用测试时可用的参考集（即“地图”，其中包含目标域的图像和姿态信息）对VPR模型进行简单的微调，以适应目标域。

Result: RSF方法在挑战性数据集上将SOTA VPR模型的Recall@1平均提高了约2.3%。微调后的模型保持了泛化能力，并且RSF方法适用于各种不同的测试数据集。

Conclusion: 通过利用测试时的参考集进行简单的微调（RSF），可以有效弥合VPR任务中的训练-测试域差距，显著提升SOTA方法在复杂环境下的性能，同时保持模型的泛化能力。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [105] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: SAM能提升泛化能力但计算开销大。本文提出ARSAM，通过分解SAM梯度并自适应地复用其中的二阶梯度投影（PSF），显著加速SAM的优化过程，同时保持与SAM相当的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: Sharpness-Aware Minimization (SAM) 能够提高模型的泛化能力，但其计算成本是标准随机梯度下降 (SGD) 的两倍，因为它在每个优化步骤中需要两次梯度计算。

Method: ARSAM 方法通过以下步骤实现：1) 发现 SAM 的梯度可以分解为 SGD 梯度和一阶梯度投影的二阶梯度 (PSF)。2) 观察到 SGD 梯度和 PSF 在训练过程中动态演变，PSF 在实现平坦最小值中起着越来越重要的作用。3) ARSAM 旨在复用 PSF 并及时更新，以在保持模型泛化能力的同时加速计算。

Result: ARSAM 在多种网络架构上实现了与 SAM 相当的最新准确度。在 CIFAR-10/100 数据集上，ARSAM 的性能与 SAM 相当，同时提供了约 40% 的加速。此外，ARSAM 在人体姿态估计和模型量化等各种挑战性任务中加速了优化，且不牺牲性能，展示了其广泛的实用性。

Conclusion: ARSAM 成功地解决了 SAM 计算成本高的问题，通过梯度分解和自适应复用策略，在保持甚至匹配 SAM 优秀泛化能力和准确性的前提下，显著提高了优化速度，具有广泛的应用前景。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [106] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出CoPA框架，通过提示引导和多层概念聚合，提升了概念瓶颈模型在临床诊断中的概念捕获能力和预测性能，优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床诊断中需提高透明度。现有概念瓶颈模型（CBM）在概念捕获上存在局限：仅依赖最终层特征，忽略浅层和多尺度特征，且缺乏有效的概念编码指导，阻碍细粒度概念提取。

Method: 引入CoPA（Concept Prompting and Aggregating）框架：1. 使用概念感知嵌入生成器（CEG）从视觉编码器各层提取概念表示。2. 这些表示作为概念提示调优（CPT）的提示，引导模型放大关键概念视觉线索。3. 聚合各层视觉表示以与文本概念表示对齐。

Result: CoPA有效捕获并利用了图像中宝贵的概念信息，显著提升了概念和疾病预测的性能。在三个公开数据集上的大量实验结果表明，CoPA优于最先进的方法。

Conclusion: CoPA通过其多层概念捕获和提示引导聚合机制，成功解决了现有概念瓶颈模型在概念捕获能力方面的不足，显著提高了临床诊断中概念和疾病预测的准确性。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [107] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: 研究表明，ZFP压缩技术能以高压缩比（最高22.89:1）有效降低大规模3D医疗影像数据量，同时几乎不影响脑血管自动分割的性能（Dice系数保持高水平），从而促进协同研究。


<details>
  <summary>Details</summary>
Motivation: 3D医疗影像数据集日益庞大且复杂，给协同研究和数据可转移性带来了显著障碍。

Method: 本研究将ZFP压缩技术（包括容错模式和固定速率模式）应用于一个大型3D医疗数据集，该数据集包含血管分割的真实标注。通过严格比较压缩前后数据上自动脑血管分割的质量（使用Dice系数）来评估ZFP的性能。

Result: 研究发现，ZFP在容错模式下实现了高达22.89:1的数据压缩比，同时分割性能（平均Dice系数为0.87656）与未压缩基线（Dice约0.8774）保持高度一致。

Conclusion: ZFP是一种可行且强大的工具，能够使大规模医疗数据集的研究更加高效和易于访问，从而促进社区内部更广泛的协作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [108] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出一种结合CNN、Transformer和Mamba的混合分割架构，通过捕捉多尺度依赖和增强特征交互，在医学图像分割中实现SOTA性能和泛化能力，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医学图像分割中常是任务特异性，泛化能力差，且难以在临床应用中平衡模型复杂度和性能（准确性和效率）。

Method: 提出一种混合分割架构：
1.  三分支编码器：整合CNN、Transformer和基于Mamba的注意力融合（MAF）机制，以捕捉局部、全局和长程依赖。
2.  多尺度注意力CNN解码器：重建精细分割图并保持上下文一致性。
3.  协同注意力门：在编码和解码过程中增强特征选择和跨尺度通信。

Result: 在多个基准数据集上的广泛实验表明，该方法在准确性和泛化能力方面优于现有SOTA方法，同时计算复杂度相当。

Conclusion: 该架构通过有效平衡效率和效果，为多样化的医学影像任务提供了一个实用且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [109] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出一种基于YOLOv9深度学习算法，结合多边形标注，实现道路损坏和沙井的自动化检测，以解决传统人工监测的痛点。


<details>
  <summary>Details</summary>
Motivation: 城市安全和基础设施维护是智慧城市发展的关键。传统人工监测道路损坏耗时、成本高且易出错，急需高效自动化的解决方案。

Method: 采用YOLOv9算法，并使用多边形标注（而非传统边界框）对路面缺陷进行更精确的定位。构建了一个包含千余张图像的新数据集（主要收集自孟加拉国达卡），用于训练模型检测“损坏”、“未损坏”和“沙井”三类。

Result: 模型实现了78.1%的整体图像级准确率。YOLOv9在“损坏”（F1-score 86.7%）和“未损坏”（F1-score 89.2%）类别上表现出色，但在“沙井”检测上由于类别不平衡而面临挑战（F1-score 18.2%）。

Conclusion: 该方法为发展中国家监测城市基础设施提供了一个高效且可扩展的解决方案。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [110] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 本文提出一种名为Contrastive-SDE的方法，将时间依赖的对比学习与预训练的扩散模型相结合，用于无配对图像到图像翻译。该方法在多项指标上取得了与SOTA相当的性能，且收敛速度更快，无需标签监督。


<details>
  <summary>Details</summary>
Motivation: 无配对图像到图像翻译在缺乏对齐样本时学习映射具有挑战性。虽然基于分数的扩散模型在生成任务中表现出色，且对比学习在无监督下学习语义相似性有效，但如何将两者结合以在无配对场景中实现高保真、多样化且语义一致的生成是研究动机。

Method: 本文提出一种时间依赖的对比学习方法，通过SimCLR训练模型，将图像及其域不变特征视为正样本对，从而保留域不变特征并丢弃域特定特征。接着，训练好的对比模型被用来指导预训练随机微分方程（SDE）的推理过程，以完成I2I翻译任务。

Result: 在三个常见的无配对I2I任务中，使用四种指标进行评估，Contrastive-SDE在多项指标上取得了与现有最先进方法相当的结果。此外，该模型收敛速度显著加快，且无需标签监督或分类器训练。

Conclusion: 结合时间依赖对比学习与扩散模型，为无配对图像到图像翻译提供了一种高效且无需监督的替代方案，能够在保持高性能的同时，显著提升训练效率并降低对外部监督的依赖。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [111] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: 现有VLA基准（如LIBERO）评估结果虚高，模型表现可能基于死记硬背。本文提出LIBERO-PRO，通过引入扰动揭示模型缺乏泛化和真正理解能力，呼吁采用更鲁棒的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action (VLA) 模型评估基准LIBERO存在问题，导致性能估算虚高且阻碍公平比较。研究旨在揭示模型是否真正理解任务而非仅依赖死记硬背。

Method: 引入LIBERO-PRO，一个扩展的LIBERO基准。通过在操作对象、初始状态、任务指令和环境四个维度上系统地引入合理扰动，全面评估VLA模型的泛化能力和鲁棒性。

Result: 现有模型在标准LIBERO下准确率超过90%，但在LIBERO-PRO的泛化设置下，性能急剧下降至0.0%。这暴露了模型依赖于对训练集中动作序列和环境布局的死记硬背，而非真正的任务理解或环境感知。

Conclusion: 当前VLA模型评估方法存在严重缺陷，无法有效衡量模型的泛化和理解能力。呼吁社区放弃误导性方法，转而采用更鲁棒的评估标准。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [112] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: AI生成图像日益逼真，现有AI检测器难以识别，但人类仍可分辨。本文引入Mirage数据集来捕捉这种差异，并发现大型视觉语言模型（LVLMs）在检测有可见伪影的AI图像方面表现出色，但在缺乏此类线索时性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于先进的AI生成模型能产生难以被现有AI检测器识别的合成图像，而人类仍能分辨，这导致了检测器与人类判断之间的性能差异。特别是，现有最先进的检测方法在包含可见伪影的AI生成图像上表现不佳，促使研究更有效且可解释的检测方法。

Method: 1. 引入并构建了一个名为“Mirage”的策展数据集，该数据集包含多样化的、具有可见伪影的AI生成图像，现有最先进的检测方法在此数据集上 largely 失败。2. 调查了大型视觉语言模型（LVLMs）是否可以用于可解释的AI图像检测。3. 在Mirage数据集和现有基准数据集上进行了实验。

Result: 1. 大型视觉语言模型（LVLMs）在检测具有可见伪影的AI生成图像方面非常有效。2. 当图像缺乏此类伪影线索时，LVLMs的检测性能会下降。

Conclusion: 大型视觉语言模型（LVLMs）在检测带有明显伪影的AI生成图像方面表现出色，但在处理缺乏此类视觉线索的图像时，其性能会显著下降。这表明LVLMs在特定场景下具有作为人类判断替代品的潜力，但并非所有情况下的通用解决方案。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [113] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround是一种统一的视觉定位范式，通过动态选择Transformer中间层作为“掩码即提示”，克服了现有方法中误差累积和缺乏明确空间提示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位范式面临两大挑战：1) 依赖固定的最后一层隐藏层，导致误差逐层累积且无法中间校正；2) 使用`<SEG>`作为提示，隐式地将文本嵌入投射到视觉空间，缺乏明确的空间线索。

Method: 核心是Policy-Prompted Masking，包含两部分：1) **随机跳跃连接（SSC）**，一个强化学习策略，通过随机采样动态选择`<SEG>` token连接视觉模型（如SAM）的Transformer中间层；2) **掩码即提示（MasP）**，利用`<SEG>` token和图像token的相似性图作为软logit掩码，为SAM提供明确的空间提示以生成掩码。

Result: UGround首次从属性角度将视觉定位任务统一到单一框架中。它能处理从传统指代表达分割到推理分割、单目标到多目标、正向查询到错误前提（空目标）等多种复杂任务。

Conclusion: UGround通过其创新的动态层选择和明确空间提示机制，成功解决了现有视觉定位范式的关键限制，实现了一个统一、鲁棒且能够处理广泛定位任务的视觉定位框架。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: 鉴于现有LLM浏览器代理基准测试环境过于理想，本文提出WAREX框架，用于在真实世界不稳定性下评估代理可靠性，发现当前最先进代理鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 现有浏览器LLM代理的评估基准在受控环境中进行，忽略了真实世界网络不稳定性、网站攻击和动态修改等因素，导致无法充分评估代理的实际鲁棒性。

Method: 引入WAREX（Web Agent Reliability Evaluation on Existing Benchmarks）框架，旨在测量这些代理在现有基准测试（如WebArena、WebVoyager和REAL）中面临真实世界挑战时的可靠性。

Result: 将WAREX引入测试后，LLM代理的任务成功率显著下降。

Conclusion: 最先进的LLM代理在面对真实世界的网络不稳定性和网站动态变化时，表现出有限的鲁棒性。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [115] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 针对存在阻塞约束的混合流水车间调度问题（BHFS），本文旨在同时最小化完工时间（makespan）和总能耗。研究提出了一种多目标混合整数规划（MIP）模型、增强型epsilon-约束方法以及Refined Iterated Pareto Greedy (RIPG) 元启发式算法，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 全球经济面临非可再生能源稀缺、地缘政治冲突、能源价格上涨及气候变化等挑战，亟需开发更高效的能源解决方案。制造业作为主要能耗部门，迫切需要通过节能调度等方法快速降低能耗。

Method: 1. 将问题建模为新颖的多目标混合整数规划（MIP）模型。2. 提出了一种增强的epsilon-约束方法以寻找帕累托最优解。3. 开发了高效的多目标元启发式算法Refined Iterated Pareto Greedy (RIPG) 以处理大规模实例。4. 使用小、中、大规模实例对提出的方法进行基准测试，并与两种现有算法进行比较。

Result: 计算结果表明，本研究所提出的方法是有效的。

Conclusion: 本研究提出的多目标优化方法能有效解决混合流水车间阻塞调度问题，实现在最小化完工时间的同时降低能耗，为制造业的节能运营提供了有效方案。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [116] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）在识别自身生成文本方面普遍失败，并表现出对特定模型的偏见，尽管对自身存在有一定认知，但推理存在层级偏见。


<details>
  <summary>Details</summary>
Motivation: 自我识别对AI系统的元认知能力和安全性至关重要，但此前关于模型是否具备自我识别的解释存在矛盾，促使本研究进行系统评估。

Method: 引入系统评估框架，通过二元自我识别和精确模型预测两个任务，评估了10个主流LLM识别自身及其他模型生成文本的能力。此外，还首次评估了模型对自己和他人存在的认知及其选择背后的推理过程。

Result: 研究发现模型在自我识别上普遍失败，10个模型中仅有4个能识别自身，且表现很少高于随机猜测。模型表现出对预测GPT和Claude家族的强烈偏见。模型对自身及其他模型的存在有一定认知，但其推理揭示了层级偏见，倾向于将GPT、Claude（偶尔Gemini）视为顶级模型并将其与高质量文本关联。

Conclusion: 研究结果对AI安全具有重要启示，并为未来开发AI适当的自我意识指明了方向。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [117] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 为解决RAG在企业应用中因证据矛盾导致输出不可靠的问题，本文提出了ContraGen框架，这是一个专门针对企业文档的矛盾感知基准，用于系统评估文档内及跨文档一致性。


<details>
  <summary>Details</summary>
Motivation: RAG系统在企业应用中，检索到的信息矛盾会导致输出不一致或不可信，这在合规、治理和问责制至关重要的企业环境中尤其严重。现有矛盾检测基准仅限于句子层面，无法处理复杂企业文档的矛盾。

Method: 提出ContraGen框架，通过生成带有嵌入式矛盾的合成企业风格文档，系统评估文档内部和跨文档的一致性。该方法结合自动化矛盾挖掘与人工验证，并涵盖建模矛盾类型分类、受控创建自矛盾和成对矛盾、开发矛盾感知检索评估管道及引入人工监督。

Result: 成功开发了一个能够生成逼真企业文档、识别并分类业务流程中矛盾类型、受控创建不同矛盾形式、以及包含人工监督的矛盾感知评估框架（ContraGen）。

Conclusion: 本研究为在企业信息查询应用中构建更可信、更负责任的RAG系统奠定了基础，强调了矛盾检测和解决对于降低风险、确保合规的重要性。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [118] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 评估认知架构和生成式神经网络架构的理论具有挑战性。本文通过广泛的视角，对这两类架构及其系统进行了广泛的定性比较。


<details>
  <summary>Details</summary>
Motivation: 理论评估对于任何理论都至关重要，但对于基于认知架构和生成式神经网络架构的理论而言，评估尤其困难，本文旨在解决这一“双重挑战”。

Method: 采用广阔的理论评估视角，对面向“整体心智”的认知架构和生成式架构及其完整系统进行广泛但定性的比较。

Result: 通过比较，产出了对面向“整体心智”的认知架构和生成式架构及其完整系统的广泛定性分析。

Conclusion: 本文通过提供对认知和生成式架构的广泛定性比较，以应对这些架构中理论评估的挑战。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [119] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 本文提出一个基于LLMs将自然语言计划转换为Kripke结构和LTL进行模型检测的框架，用于评估计划与预期行为的对齐性。实验显示GPT-5分类性能优异（F1 96.3%）。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言计划与其预期行为之间的对齐性。

Method: 利用大型语言模型（LLMs）将自然语言计划转换为Kripke结构和线性时序逻辑（LTL），并进行模型检测。在PlanBench计划验证数据集的简化版本上，使用准确率、精确率、召回率和F1分数等指标进行系统评估。

Result: GPT-5取得了卓越的分类性能（F1分数为96.3%），并且几乎总能生成语法上完美的、可作为保证的正式表示。

Conclusion: 当前框架能生成语法完美的正式表示，但语义完美的正式模型合成仍是未来探索方向。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [120] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 本文引入PolicyGuardBench基准和PolicyGuard-4B模型，首次全面研究了网络智能体轨迹的策略合规性，并证明了轻量级、准确且可泛化的防护栏在小规模下是可行的。


<details>
  <summary>Details</summary>
Motivation: 自主网络智能体需遵循既定策略生成长轨迹，但现有工作鲜少研究这些轨迹是否符合策略，以及策略违规在不同领域和子领域中是否持续存在。

Method: 引入了PolicyGuardBench，一个包含约6万个示例的基准数据集，用于检测智能体轨迹中的策略违规。该数据集涵盖了多样化的策略生成、子领域内和跨子领域的违规配对，并包含完整轨迹和基于前缀的违规检测任务。在此基础上，训练了轻量级防护栏模型PolicyGuard-4B。

Result: PolicyGuard-4B模型在所有任务中均实现了强大的检测准确性，同时保持高效推理。该模型能够跨领域泛化，并在未见设置上保持高精度。

Conclusion: PolicyGuardBench和PolicyGuard-4B共同构建了一个研究网络智能体轨迹策略合规性的首个综合框架，并表明在小规模下实现准确且可泛化的防护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [121] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是一个非自回归的多模态模型，支持可变长度和并发的图文生成，在性能和效率上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 克服自回归模型在图文生成中僵化的因果顺序限制，实现可变长度和并发的多模态生成。

Method: 结合了用于离散文本token的基于插入的Edit Flow和用于图像潜在空间的Flow Matching，并采用优先内容而非语法的分层采样机制。

Result: 在1B到8B的模型规模上，OneFlow在生成和理解任务上均优于自回归基线，训练FLOPs减少高达50%。它超越了自回归和基于扩散的方法，并解锁了并发生成、迭代细化和类推理生成等新能力。

Conclusion: OneFlow是一款创新且高效的非自回归多模态模型，通过其独特的生成机制，在性能和新能力方面均表现出色，超越了传统方法。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [122] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文通过对在上下文权重预测任务上训练的Transformer模型进行理论分析和实验验证，探讨了测试时缩放（test-time scaling）提升大型语言模型推理能力（如生成长思维链）的条件。研究发现，增加测试时计算量可在固定误差下减少上下文长度，但若训练数据技能不足可能损害性能，且在多样化、相关且困难的任务集上训练能达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时缩放（Test-time scaling）已在大型语言模型（LLMs）中展现出强大的推理能力提升（例如生成更长的思维链），但关于训练数据中产生长思维链的条件以及何时这些长思维链能有效提高性能的机制尚不明确。

Method: 作者通过研究在用于线性回归的上下文权重预测任务上训练的Transformer模型，来分析测试时缩放的性能。方法包括理论分析，并通过在大型非线性Transformer架构上进行的实验来验证研究结果。

Result: ['在任何固定测试误差下，增加测试时计算量允许减少训练提示中的上下文示例数量（上下文长度）。', '如果解决下游任务所需的技能在训练数据中不够充分，增加测试时计算量反而会损害性能。', '通过特征协方差矩阵的最小特征值来表征任务难度，结果表明在多样化、相关且困难的任务集上进行训练，能使测试时缩放达到最佳性能。']

Conclusion: 测试时缩放可以有效提升LLMs的推理能力，但其效果取决于训练数据的特性。通过增加测试时计算量可以权衡减少上下文长度；然而，若训练数据未能充分涵盖相关技能，过多的测试时计算可能适得其反。为最大化测试时缩放的益处，关键在于在多样化、相关且具有挑战性的任务集上进行训练。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [123] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 本文提出一种名为跨模态偏好引导（CPS）的新方法，通过联合操纵视觉和文本信息，在黑盒设置下实现对视觉-语言模型（VLM）代理更有效的偏好操纵，且更难以被检测，凸显了对鲁棒防御的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 基于VLM的网络代理在内容推荐、产品排名等高风险选择任务中日益重要，但现有研究表明它们容易受到通过对抗性弹出窗口、图像扰动或内容调整进行的偏好操纵攻击。然而，现有工作要么假设强大的白盒访问权限和有限的单模态扰动，要么使用不切实际的设置，未能充分反映真实世界中攻击者的能力。

Method: 本文提出“跨模态偏好引导（Cross-Modal Preference Steering, CPS）”方法，首次证明联合利用视觉和文本通道能产生更强大的偏好操纵。CPS联合优化项目视觉和自然语言描述中难以察觉的修改，利用CLIP可迁移的图像扰动和RLHF诱导的语言偏差来引导代理决策。与假设梯度访问或网页控制的先前研究不同，CPS采用真实的黑盒威胁设置：非特权攻击者只能编辑自己的列表图片和文本元数据，对代理模型内部机制一无所知。

Result: 研究在GPT-4.1、Qwen-2.5VL和Pixtral-Large等最先进的专有及开源VLM代理上，通过电影选择和电子商务任务评估了CPS。结果显示，CPS比领先的基线方法更有效。例如，CPS在所有模型中始终优于基线，同时保持低70%的检测率，证明了其有效性和隐蔽性。

Conclusion: 这些发现强调，鉴于代理系统在社会中扮演着越来越重要的角色，迫切需要开发鲁棒的防御措施来对抗此类跨模态偏好操纵攻击。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [124] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: MITS通过基于点互信息（PMI）的评分函数和动态采样策略，为大型语言模型（LLMs）的树搜索推理提供了一种高效且准确的指导框架。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs树搜索推理方法（如Tree-of-Thought, MCTS）难以即时可靠地评估中间推理步骤质量，且广泛的路径探索计算成本高昂。

Method: 提出互信息树搜索（MITS）框架。该框架引入基于点互信息（PMI）的有效评分函数，实现推理路径的逐步评估和通过束搜索进行树扩展，避免昂贵的预估模拟。MITS还包含一个基于熵的动态采样策略，自适应地将计算资源分配给不确定的推理步骤。最终预测采用结合PMI分数和预测共识的加权投票方案。

Result: 通过在多样推理基准上的综合实验，MITS持续超越基线方法。

Conclusion: MITS为LLM推理建立了一个原则性强且高效的框架。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [125] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 指令微调的扩散大语言模型存在“<eos>”溢出漏洞，导致响应变短。本文将其归因于“<eos>”的双重作用，并提出了“彩虹填充”（Rainbow Padding），通过使用不同的填充令牌来显著提高模型长度鲁棒性和输出质量。


<details>
  <summary>Details</summary>
Motivation: 指令微调的扩散大语言模型（dLLMs）在分配序列长度增加时，响应反而变短，甚至退化为“<eos>”令牌流，即“<eos>”溢出。尽管在实践中已被注意到，但该问题尚未得到系统分析，阻碍了dLLMs的潜力。

Method: 研究追溯了“<eos>”溢出的根本原因，发现其在于“<eos>”作为终止符和填充符的双重作用。为解决此问题，提出了“彩虹填充”方法，用一个重复循环的不同填充令牌替换重复的“<eos>”占位符，以分散概率质量，打破“<eos>”的主导地位。并通过实验验证其效果，包括使用LoRA在少量数据上进行单epoch微调。

Result: 实验表明，“彩虹填充”显著改善了模型的长度鲁棒性和输出质量。只需七个填充令牌即可有效防止早期终止。此外，该方法可以高效集成到现有模型中，通过LoRA微调在少量数据上进行单epoch即可获得显著改进。

Conclusion: “彩虹填充”是一个简单而高效的解决方案，能够有效解决指令微调的扩散大语言模型中存在的“<eos>”溢出漏洞，显著提升其长度鲁棒性和输出质量，具有高度实用性。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [126] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 本文提出了一套针对多轮多智能体对话系统的目标导向评估框架，引入目标成功率（GSR）和失败根本原因（RCOF）分类法，并结合教师LLM实现可解释、数据高效的评估，成功应用于企业级系统并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话机器人评估方法多停留在轮次层面，难以有效评估用户整体目标（如信息需求或任务）是否达成，导致多轮对话质量评估具有挑战性。

Method: 提出面向目标的多智能体系统评估框架，包含：1) 目标成功率（GSR）来衡量目标达成百分比；2) 失败根本原因（RCOF）分类法来识别故障原因。该方法通过用户目标分割对话，并利用所有相关轮次评估成功。评估系统结合了教师LLM，由领域专家定义目标和质量标准，LLM通过“思考令牌”生成可解释的理由。

Result: 将该框架应用于企业级多智能体对话系统AIDA，观察到其GSR在六个月内从63%提升至79%。该框架通过详细的缺陷分类法提供了可操作的见解，能够诊断整体成功、识别关键失败模式并指导系统改进。

Conclusion: 该通用框架为多智能体对话系统提供了可解释、数据高效的评估能力，能有效诊断系统性能、识别故障模式并为系统改进提供有价值的洞察。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [127] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 本文提出H-DDx，一个分层评估框架，用于更准确地评估大型语言模型在鉴别诊断中的表现，纠正了传统扁平化指标低估模型性能的问题，并增强了结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型（LLMs）在鉴别诊断领域的评估主要依赖于扁平化指标（如Top-k准确率），这些指标无法区分临床相关的“近失”错误和诊断上相距甚远的错误，导致对模型性能的评估不准确。

Method: 引入了H-DDx分层评估框架。该框架利用检索和重排序流程将自由文本诊断映射到ICD-10编码，并应用分层指标来奖励与真实诊断密切相关的预测。

Result: 在基准测试22个主流模型后发现，传统扁平化指标低估了性能，H-DDx揭示了模型（特别是领域专用开源模型）的真实优势。此外，该框架通过揭示分层错误模式，增强了可解释性，表明LLMs即使未能给出精确诊断，也常能识别更广泛的临床背景。

Conclusion: H-DDx框架提供了一种更具临床相关性和可解释性的评估方法，用于评估LLMs在鉴别诊断中的能力，纠正了传统指标的不足，并揭示了模型理解更广泛临床背景的潜力。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [128] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 本文旨在通过增强多模态基础模型（MFMs）的推理能力和引入结构化、可控的生成框架，弥合MFMs与有效世界模型之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型（MFMs）缺乏反事实推理、动力学模拟、时空理解、生成结果控制及多方面推理等关键能力，使其无法作为有效的世界模型，这与人类的多感官世界理解能力存在显著差距。

Method: 1. 通过判别性任务和赋予结构化推理技能（如因果推理、反事实思维、时空推理），提升MFMs的推理能力。2. 探索MFMs在图像和视频模态上的生成能力，引入利用场景图、多模态条件和多模态对齐策略的新框架，实现结构化和可控生成。3. 将技术扩展至可控4D生成，以实现交互、可编辑和可变形的对象时空合成。

Result: 本文通过提出的方法，旨在实现增强MFMs的深度推理能力，包括因果、反事实和时空推理；并推动其在图像、视频和4D模态上实现结构化和可控的生成，以确保与高级语义和用户意图的一致性。

Conclusion: 本文提供了一条路径，通过全面提升多模态基础模型的推理能力和开发先进的结构化、可控生成技术，使其能够更接近世界模型，从而更好地理解和模拟动态物理过程，并生成符合复杂意图的视觉内容。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [129] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: 本文提出OptAgent框架，结合多智能体模拟和遗传算法，用于优化电商查询重写。通过LLM代理作为动态奖励信号，OptAgent能有效提升查询质量，超越现有基线。


<details>
  <summary>Details</summary>
Motivation: LLM系统在主观任务（如电商查询重写）中的评估面临挑战，因为缺乏单一正确答案且难以算法化判断用户意图。

Method: OptAgent框架结合多智能体模拟和遗传算法。它不依赖静态奖励模型或单一LLM评判，而是使用多个LLM代理（模拟购物顾客）作为动态奖励信号。这些代理的平均得分作为进化算法的适应度函数，迭代优化用户初始查询。

Result: OptAgent在1000个真实电商查询数据集上进行了评估，结果显示，相对于原始用户查询，平均提升了21.98%；相对于“最佳N个LLM重写”基线，平均提升了3.36%。

Conclusion: OptAgent通过创新的多智能体模拟和遗传算法组合，有效解决了电商查询重写的评估和优化难题，显著优于现有方法，为部署用户对齐的LLM系统提供了可靠方案。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [130] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 本文提出GuidedSampling算法，通过解耦推理过程的探索和生成阶段，有效解决了传统Repeated Sampling (RS)在生成多样性解决方案上的不足，显著提升了模型性能和生成候选方案的多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的Repeated Sampling (RS)算法虽然能提升模型性能，但在生成多样化解决方案时表现不佳，常常依赖相同方法，导致样本冗余。

Method: 提出GuidedSampling推理算法，将推理过程解耦为探索和生成阶段。探索阶段识别解决问题的多个概念，生成阶段应用特定概念提供最终解决方案。作者定义了其理论界限，并进行了经验性验证和比较。

Result: 实验结果表明，与RS相比，GuidedSampling在pass@50上平均提升基础模型性能约21.6%。使用GuidedSampling轨迹训练的模型在pass@5上平均提升约9.7%。此外，GuidedSampling将每个实例的平均概念数从1.67增加到3.03，生成了更多样化的候选方案。

Conclusion: GuidedSampling算法通过提高解决方案的多样性，显著改善了基础模型的性能，并在模型训练方面也显示出优越性，优于传统的Repeated Sampling方法。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [131] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 论文研究大型策略空间博弈，提出“隐藏博弈问题”，并通过组合后悔最小化技术实现了最优后悔界限，加速收敛到隐藏子博弈的均衡。


<details>
  <summary>Details</summary>
Motivation: 解决AI对齐和语言博弈中的挑战，特别是针对大型策略空间中玩家存在未知高回报策略子集的情况，旨在设计高效的后悔最小化算法来发现并利用这些隐藏结构。

Method: 开发了一种后悔最小化技术的组合方法。

Result: 实现了最优的外部后悔和交换后悔界限；确保了在隐藏子博弈中快速收敛到相关均衡；通过利用隐藏博弈结构提高了计算效率。

Conclusion: 所提出的后悔最小化方法能够有效发现和利用隐藏博弈结构，从而实现子博弈中的均衡并保持整体理性，具有计算效率优势。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [132] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小型语言模型 (SLMs) 在代理工作负载中表现出色，尤其在需要精确的结构化输出时，结合引导解码和验证器可显著降低成本、延迟和能耗，并常能超越大型语言模型 (LLMs)。


<details>
  <summary>Details</summary>
Motivation: 旨在探索小型语言模型在代理工作负载中的潜力，以解决传统大型语言模型在处理受限于模式和API的精确任务时可能存在的成本高、延迟大和能耗高的问题。

Method: 综合分析了多种开放和专有SLM，并将其与现代评估基准、服务堆栈和引导解码库结合。研究提出SLM默认、LLM备用系统，采用不确定性感知路由和验证器级联。提出了实际生产指标（如每次成功任务成本、模式有效性）。应用引导解码、严格JSON Schema输出和验证器优先的工具执行，并提供包括schema优先提示、类型安全函数注册和轻量级适配（LoRA/QLoRA）的设计模式。

Result: 引导解码、严格JSON Schema输出和验证器优先的工具执行显著缩小了SLM与LLM的能力差距。SLM在工具使用、函数调用和RAG任务上能够匹配或超越LLM，同时实现10-100倍的代币成本降低，并显著改善延迟和能耗。

Conclusion: SLMs通过恰当的工程设计（如引导解码和结构化输出），可以作为构建快速、廉价且可靠代理系统的首选，仅在开放域推理和长周期规划等少数情况下才需要LLM作为补充，从而提供了一个高效代理堆栈的实用蓝图。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [133] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: 现有LLM在系统算法设计中缺乏创造性，本文提出MetaMuse框架，通过自反思原则显著提升了缓存替换和在线装箱等关键系统问题的性能。


<details>
  <summary>Details</summary>
Motivation: 系统算法设计因解空间不连续而充满挑战，导致工程师常依赖通用启发式算法牺牲性能。研究发现现有LLM倾向于通用设计，难以实现所需的创造性突破。

Method: 引入MetaMuse框架，用于创意构思，基于三个自反思原则：(1) 在可衡量的性能空间而非抽象概念空间量化解决方案多样性和效用；(2) 通过外部刺激而非内部随机性引导构思；(3) 使用航路点推理而非自由形式思维链构建可执行方案。

Result: MetaMuse能为全球云提供商的两个关键问题生成高性能解决方案：缓存替换（缓存未命中率降低高达35.76%）和在线装箱（装箱使用率降低高达30.93%）。

Conclusion: MetaMuse有效解决了LLM在系统算法生成中创造性不足的问题，为复杂系统设计提供了显著优化的解决方案。

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [134] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 本文提出一种结合大语言模型（LLM）和可解释人工智能（XAI）的上下文推理方法，以提升物联网异常检测的准确性和可解释性，并在智能电网和医疗场景中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 关键物联网系统需快速发现异常以确保安全平稳运行，但传统检测方法在日益复杂的（如智能医疗、能源电网、工业自动化）、动态、高维、数据不完整或持续演进的系统中面临局限，亟需自适应、智能的检测系统。

Method: 提出一种基于LLM的上下文推理方法，辅以XAI代理来增强物联网环境中的异常检测。该方法利用注意力机制、避免逐时间步处理细节，并使用具有语义的内存缓冲区来发现隐藏模式和不一致性。通过在实际智能电网和医疗场景仿真中，将此LLM增强模型与传统模型进行对比测试，评估检测准确性、误报率、结果可读性和系统响应速度。

Result: 研究表明，新方法在检测准确性和解释性方面均显著优于大多数现有模型。

Conclusion: 该LLM增强的异常检测方法在物联网未来的异常检测任务中具有良好的应用前景和潜力。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [135] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 提出Spatial CAPTCHA，利用人类与MLLM在空间推理上的差异，有效对抗先进AI，人类表现远超MLLM。


<details>
  <summary>Details</summary>
Motivation: 现有CAPTCHA因多模态大语言模型（MLLMs）的进步而失效，无法有效防御自动化滥用。

Method: 提出Spatial CAPTCHA，利用人类和MLLM在空间推理上的根本差异。生成动态问题，要求几何推理、透视、遮挡处理和心理旋转。系统采用程序化生成管道，具备难度控制、自动验证和人工验证。

Result: 在Spatial-CAPTCHA-Bench上评估显示，人类表现远超10个SOTA MLLMs，最佳模型准确率仅31.0%。与Google reCAPTCHA对比，证实了其作为安全机制和AI空间推理诊断工具的有效性。

Conclusion: Spatial CAPTCHA是一种有效的人机验证框架，能抵御现代AI，并可作为评估AI空间推理能力的工具。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [136] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 针对MM-DiTs在生成罕见概念时的不足，本文提出一种无需额外训练的干预方法，通过在联合注意力块前扩大文本嵌入的表示域，有效提升模型生成罕见语义的能力，并泛化至多种文本到视觉任务。


<details>
  <summary>Details</summary>
Motivation: 多模态扩散Transformer (MM-DiTs) 在处理用户提出的新颖或罕见提示时表现不欠佳，因为这些概念在预训练数据中通常稀缺，未能留下足够强的印记。

Method: 提出一种简单而有效的干预措施，无需额外的训练步骤、数据、去噪时间优化或对外部模块的依赖。具体方法是，在MM-DiT的联合注意力块之前，通过增加文本token嵌入的方差来数学性地扩大其表示域。

Result: 该方法能使MM-DiT的输出中清晰地浮现罕见语义。此外，研究结果在文本到图像、文本到视频和文本驱动的图像编辑等多种文本到视觉任务中均表现出良好的泛化能力。

Conclusion: 该工作促使生成模型能够揭示用户意图的、原本隐藏但已准备好浮现的语义。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [137] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 我们提出了一个游戏化可解释AI (XAI) 系统，旨在帮助消费者在咖啡领域做出符合道德的决策，该系统结合了康德主义规则检查和功利主义多标准评分。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助具有道德意识的消费者在复杂的咖啡市场中做出明智、符合道德的决策，通过提供可解释的AI洞察来揭示产品背后的伦理影响和权衡。

Method: 该系统采用两个符号引擎：一个康德主义模块标记违反道德规则的行为（如童工、毁林风险），一个功利主义模块通过标准化属性（价格、碳足迹、水资源、透明度、农民收入等）的多标准聚合来评估选项。一个元解释器会突出康德主义与功利主义的（不）一致性，并在福利损失较小时，推荐道德上“干净”的近似平价选项。

Result: 开发了一个功能性的游戏化XAI系统，提供实时道德解释。同时发布了结构化配置（属性模式、认证图、权重、规则集）、用于可审计性的策略追踪以及一个交互式用户界面。

Conclusion: 该系统提供了一种新颖的方法，通过游戏化、可解释的AI，结合伦理规则和效用评估来指导咖啡领域的道德消费决策，并提供了透明和可审计的工具。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [138] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: 本文提出了QRLLM，一个针对LLM多轮对话中灾难性风险的认证框架。该框架通过建模对话概率分布并提供统计保证，揭示了前沿模型中高达70%的显著风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在对话中可能产生灾难性响应，对公共安全构成严重威胁。现有评估方法因依赖固定攻击提示、缺乏统计保证且无法扩展到广阔的多轮对话空间，未能充分揭示这些漏洞。

Method: QRLLM是一个新颖的、有原则的LLM多轮对话灾难性风险认证框架，能够以统计保证限制LLM产生灾难性响应的概率。该方法将多轮对话建模为查询序列的概率分布（通过查询图上的马尔可夫过程表示，图边编码语义相似性），并使用置信区间量化灾难性风险。定义了几种经济实用的分布：随机节点、图路径和带拒绝的自适应分布。

Result: 研究结果表明，这些分布能揭示前沿模型中显著的灾难性风险，其中最差模型的认证下限高达70%。

Conclusion: 前沿LLM迫切需要改进其安全训练策略。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [139] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 为解决基础模型（FMs）创造力评估框架分散且缺乏理论基础的问题，本文提出C^2-Eval，一个统一评估FMs收敛性和发散性创造力的基准，并基于U-O-S理论进行实验分析，揭示了FMs创造能力的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型（FMs）的崛起使得创造力成为衡量机器智能的关键维度，但现有评估框架碎片化，缺乏理论依据。

Method: 引入C^2-Eval基准，旨在统一评估FMs的创造力。该基准区分收敛性（如代码生成）和发散性（如故事创作）创造力，并采用源于社会科学理论的“有用性、原创性和惊喜度”（U-O-S）细粒度标准进行评估。

Result: 通过对领先的专有和开源模型进行广泛实验，分析了它们在创造力方面的权衡。结果揭示了当前FMs在实现创造性机器思维方面的优势与挑战。

Conclusion: C^2-Eval被证明是审查创意AI发展格局的有效工具，为理解FMs的创造能力提供了清晰视角。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [140] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 本文提出了一个代理框架（Zephyrus）及其代码环境（ZephyrusWorld），旨在弥合天气基础模型与大型语言模型（LLMs）之间的能力鸿沟，使LLM能够通过代码工具与天气数据交互，并在新基准ZephyrusBench上超越纯文本基线。


<details>
  <summary>Details</summary>
Motivation: 现有天气基础模型缺乏语言推理能力，而LLMs无法有效处理高维气象数据集，这限制了它们在交互式科学工作流中的实用性。

Method: 构建了一个代理框架，包含：1) ZephyrusWorld，一个基于Python代码的环境，提供WeatherBench 2数据集接口、自然语言地理查询、天气预报和气候模拟工具；2) Zephyrus，一个多轮LLM天气代理，通过对话反馈迭代分析数据；3) ZephyrusBench，一个新基准，包含从基本查询到高级预报、极端事件检测和反事实推理等多样化天气任务。

Result: Zephyrus代理在ZephyrusBench上表现优于纯文本基线，正确率最高提升了35个百分点。但在更困难的任务上，Zephyrus与纯文本基线表现相似。

Conclusion: 所提出的代理框架有效地增强了LLM在天气科学中的数据交互和推理能力，尽管在应对复杂任务时仍面临挑战，但也为未来的研究提供了有前景的方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [141] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 该综述首次全面分类了基于大型语言模型的数据科学智能体，分析了45个系统在数据科学生命周期各阶段的能力，并指出了当前系统的优势、局限性及未来发展挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步催生了一类新的AI智能体，能够自动化数据科学工作流程的多个阶段。本研究旨在首次对这些数据科学智能体进行全面、与生命周期对齐的分类，以系统性地分析和理解它们。

Method: 本研究采用综述方法，系统分析了45个数据科学智能体，并将其映射到端到端数据科学流程的六个阶段。此外，还根据推理规划风格、模态整合、工具编排深度、学习与对齐方法以及信任与安全机制等五个设计维度对每个智能体进行了标注。研究还对智能体的能力进行了批判性综合，强调了各阶段的优缺点，并回顾了新兴的基准和评估实践。

Result: 分析揭示了三个主要趋势：大多数系统侧重于探索性分析、可视化和建模，而忽视了业务理解、部署和监控；多模态推理和工具编排仍是未解决的挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 当前数据科学智能体在对齐稳定性、可解释性、治理和鲁棒性评估框架方面存在开放挑战。未来研究应着力开发更稳健、值得信赖、低延迟、透明且普及的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [142] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 为解决临床AI系统缺乏标准化事件记录的问题，本文提出MedLog协议，用于记录临床AI模型的使用情况，以实现持续监控、审计和改进。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统有syslog记录关键事件，但医疗保健领域快速增长的临床AI缺乏类似标准，导致难以衡量真实世界性能、检测不良事件或纠正偏差。

Method: 引入MedLog协议，在AI模型被调用时创建记录。每条记录包含九个核心字段（header, model, user, target, inputs, artifacts, outputs, outcomes, feedback），并支持基于风险的抽样、生命周期感知的保留策略和写回缓存。

Result: MedLog提供了一种结构化、一致的模型活动记录方式，能够促进新数据库和软件的开发，从而实现对医疗AI的持续监控、审计和迭代改进。

Conclusion: MedLog为医疗AI的持续监管、改进以及新型数字流行病学奠定了基础，解决了当前临床AI透明度和可见性不足的挑战。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [143] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 本研究引入了FaithCoT-Bench，一个用于检测大型语言模型(LLMs)链式思维(CoT)解释不忠实性的实例级基准，并评估了现有检测方法的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs的CoT解释在提高问题解决能力的同时，其对底层推理过程的忠实性受到质疑，尤其是在高风险应用中。现有研究多集中于机制级分析，缺乏实用方法来判断特定CoT轨迹是否忠实于模型的内部推理。

Method: 我们提出了FaithCoT-Bench，一个统一的实例级CoT不忠实性检测基准。该框架将不忠实性检测定义为判别决策问题，并提供了FINE-CoT，一个专家标注的数据集，包含来自四种LLM在四个领域生成的1000多条CoT轨迹，其中包括300多个带有细粒度原因和步骤级证据的不忠实实例。我们进一步系统评估了11种代表性检测方法，涵盖反事实、基于对数和LLM作为评判者的范式。

Result: 通过评估，我们获得了经验性见解，阐明了现有方法的优缺点。结果表明，在知识密集型领域和更先进的模型中，不忠实性检测面临更大的挑战。

Conclusion: FaithCoT-Bench建立了首个全面的实例级CoT忠实性基准，为未来研究LLMs中更可解释和可信赖的推理奠定了坚实基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [144] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 提出一种带有可变投票阈值的集成方法，允许大型语言模型（LLMs）在不确定时“弃权”，显著提升了答案的可信度，适用于高要求场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）缺乏量化其回答不确定性的便捷可靠方法，导致其在高风险应用中难以获得信任。

Method: 扩展了传统的集成（ensembling）方法，引入可变投票阈值。该方法允许集成模型在主导响应未达到预设阈值时“弃权”不提供答案，从而提高剩余答案的可信度。研究建立了理论框架，并在算术问题解决和临床笔记问答两个领域进行了实验验证。

Result: 在两个实验领域中，使用高度限制性的投票集成方法可以大幅提高答案的可信度，同时仅导致相对适度的回答率和准确性下降。

Conclusion: 该投票集成方法特别适用于需要高度确定性但不需要每个问题都获得自动化答案的应用场景，例如医疗保健和数据标注。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [145] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: 现有LLM评估成本高昂，且基于IRT的方法存在局限。本文提出LEGO-IRT框架，通过支持连续分数和利用结构知识，大幅提高了LLM评估的数据效率和准确性，并能更好地反映模型潜在能力。


<details>
  <summary>Details</summary>
Motivation: LLM的全面基准评估计算和财务成本高昂。现有的基于IRT的评估方法存在显著局限：仅限于二元指标，无法处理生成任务的连续分数；且仅在单一基准上运行，忽略了跨指标或基准的结构知识。

Method: 引入LEGO-IRT，一个统一且灵活的数据高效LLM评估框架。其设计原生支持二元和连续评估指标。同时，引入因子化架构以显式建模和利用结构知识，将模型能力估计分解为通用组件和结构特定组件。

Result: 通过在70个LLM和5个基准上的实验表明，LEGO-IRT仅使用3%的评估项即可实现稳定的能力估计。结合结构知识可将估计误差降低高达10%。此外，该框架估计的潜在能力可能与人类偏好更密切地对齐。

Conclusion: LEGO-IRT提供了一种数据高效、灵活且准确的LLM评估方法，克服了现有IRT方法在处理连续分数和利用结构知识方面的局限性，显著提升了LLM能力估计的稳定性和准确性。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [146] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 本研究揭示了大型语言模型（LLMs）内部存在明确的情感编码机制，它在模型早期形成、中期达到峰值，且具有可塑性和持久性，并提供了新的数据集和探测工具。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能模拟情商，但其内部情感机制仍未被充分探索。研究旨在理解情感是如何、在何处、以及持续多长时间被编码在其神经架构中。

Method: 构建了一个包含约40万条Reddit文本的大规模语料库，通过分类、重写和合成生成，平衡了七种基本情绪。利用轻量级“探针”技术，在不改变模型参数的情况下，读取Qwen3和LLaMA模型隐藏层中的信息。

Result: LLMs发展出令人惊讶的、定义明确的内部情感几何结构，其清晰度随模型规模增大而增强，并显著优于零样本提示。情感信号并非仅是最后一层现象，它在网络早期出现并于中期达到峰值。此外，LLMs的内部状态既可塑（受简单系统提示影响）又持久（初始情感基调在数百个后续token中仍可检测）。

Conclusion: 研究贡献了新的数据集、开源探测工具和LLMs内部情感图谱，为开发更透明、更一致的AI系统提供了关键见解。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [147] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 该研究提出了道德锚定系统（MAS），一个新颖的框架，用于实时检测、预测和缓解人工智能（AI）代理中的价值漂移，以确保AI行为与人类道德和意图保持一致。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为超级助手，其与人类价值观的对齐问题日益突出。特别是价值漂移风险，即AI可能因环境变化、学习动态或意外优化而偏离预设价值观，导致效率低下或道德违规，是当前亟需解决的关键问题。

Method: 研究提出了道德锚定系统（MAS），该系统结合了实时贝叶斯推断来监控价值状态，LSTM网络来预测漂移，以及一个以人为中心的治理层来进行自适应干预。MAS强调低延迟响应（<20 ms），并通过监督微调和人类反馈减少误报和警报疲劳。

Result: 在模拟中，MAS能够将价值漂移事件减少80%或更多，同时保持高检测准确率（85%）和低误报率（适应后为0.08）。对目标未对齐代理的严格实验验证了MAS的可扩展性和响应性。

Conclusion: MAS通过其预测性和自适应特性，提供了一种原创的价值对齐方法，优于静态对齐方法。其贡献包括MAS架构、强调速度和可用性的实证结果、跨领域适用性见解，以及开放源代码以供复制。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [148] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: 本文提出SPOGW，一种基于分数的偏好方法，通过群组比较和连续空间优化，自动化生成和优化LLM智能体工作流，并在多项基准测试中超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 设计LLM智能体工作流需要大量手动操作，限制了可扩展性和通用性。现有自动化方法受限于表示能力、适应性、可扩展性不足及依赖离散优化和两两比较范式。

Method: 引入名为SPOGW的基于分数的偏好方法，通过群组比较直接作用于基数奖励信号，在连续空间中实现高效稳定的优化。该方法结合了迭代离线GRPO (ioGRPO) 和优势掩蔽KL散度 (mKL)，通过强调策略响应的有利区域来调节训练更新。

Result: 在涵盖数学推理、编码和问答的五个基准数据集上，SPOGW的性能与当前最先进的方法持平或超越。

Conclusion: SPOGW为自动化生成和优化智能体工作流提供了一种可行且具有前瞻性的方法，有效克服了现有方法的局限性。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [149] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 本文提出DLLM，一个基于扩散模型的LLM框架，用于Web智能教育系统中噪声鲁棒的认知诊断，通过两阶段去噪扩散和多源表示对齐解决噪声和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的认知诊断方法在处理结构化数据和抵抗噪声方面存在困难，特别是在Web智能教育系统（WIES）的开放环境中，大量日志加剧了数据不平衡和噪声问题。

Method: DLLM框架首先根据响应正确性构建独立子图，并利用关系增强对齐模块缓解数据不平衡。接着，将子图表示与LLM衍生的语义增强表示进行融合和对齐。关键在于，在每次对齐前，引入了两阶段去噪扩散模块：首先进行无条件去噪以消除错误信息，然后进行图引导的条件去噪以消除误导信息。最终，将整合了语义知识和结构信息的噪声鲁棒表示输入到现有认知诊断模型中进行预测。

Result: 在三个公开的Web教育平台数据集上的实验结果表明，DLLM在不同噪声水平下均能实现最佳的预测性能。

Conclusion: DLLM在有效利用LLM语义知识的同时，成功实现了噪声鲁棒性。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [150] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 为解决UI图像到Web代码转换（WebUI-to-Code）任务中现有基准数据多样性和评估可靠性不足的问题，本文提出了WebRenderBench大规模基准和基于渲染页面的新型评估指标，并引入ALISA代理利用该指标进行强化学习，最终实现了最先进的（SOTA）生成性能。


<details>
  <summary>Details</summary>
Motivation: 自动化UI图像到Web代码的转换对前端开发和快速原型设计至关重要。尽管多模态大语言模型（MLLMs）使WebUI-to-Code日益可行，但现有基准在数据多样性和评估可靠性方面存在局限性。

Method: ['构建了WebRenderBench，一个包含2.25万个真实门户网站页面的大规模基准，提供了比以往基准更高的数据多样性、复杂性和真实性。', '提出了一种新颖的评估指标，通过测量最终渲染页面的布局和样式一致性来评估UI质量，相比传统方法，该指标更高效、客观和可靠。', '引入了自动化布局和样式检查代理（ALISA），它将上述新指标作为强化学习的奖励信号，用于增强对抓取到的非对称网页的训练。']

Result: 实验表明，ALISA显著提升了生成性能，并在多个指标上取得了最先进的（SOTA）结果。

Conclusion: WebRenderBench基准、提出的新型评估指标和ALISA代理共同解决了WebUI-to-Code领域的数据和评估挑战，通过更可靠的评估和训练机制，显著提高了代码生成质量，达到了最先进水平。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [151] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: 提出AutoMR框架，利用DAG自动搜索查询感知的元推理骨架，以提升大型语言模型（LLM）的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有元推理骨架多为手动设计，难以适应特定查询需求，也无法有效捕捉推理步骤间的复杂逻辑依赖。

Method: 将元推理骨架表示为有向无环图（DAG）以统一现有工作并建模逻辑依赖。提出AutoMR框架，受AutoML启发，自动搜索查询感知的元推理骨架。构建基于DAG的搜索空间，并设计动态骨架采样算法，在推理时根据推理上下文扩展骨架，实现高效的查询感知骨架搜索。

Result: 实验结果表明，AutoMR在广泛的基准数据集上，推理性能显著优于现有方法。

Conclusion: AutoMR通过自动化、查询感知且基于DAG的元推理骨架搜索，克服了手动设计的局限性，有效提升了LLM的推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [152] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 本文探究了“wait”标记前模型的潜在状态如何影响推理过程，识别出调控“wait”生成及多种推理模式（如回溯、复核）的关键特征。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，推理模型性能的关键在于其推理和自我纠正能力，其中“wait”标记常作为回溯等复杂推理行为的信号。然而，目前对模型为何以及如何进行这类推理的机制理解不足，这限制了我们对推理模型有效性的认识。本工作旨在探究“wait”标记前的模型潜在状态是否包含调节后续推理过程的相关信息。

Method: 通过在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层上训练crosscoders，并引入了一种在crosscoder设置中的潜在归因技术。随后，通过分析最大激活示例和进行因果干预实验来验证发现。

Result: 成功定位了一小部分与提升/抑制“wait”标记概率相关的特征。通过实验证实，这些被识别的特征确实与推理过程密切相关，并能产生多种不同的推理模式，包括从头开始、回忆先验知识、表达不确定性和仔细检查。

Conclusion: 模型在“wait”标记前的潜在状态中包含了调节后续推理过程的关键信息。识别出的特征揭示了模型进行复杂推理行为（如回溯、复核）的机制，加深了对推理模型有效性的理解。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [153] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: RLVR中，现有方法通过模仿专家轨迹提升LLM推理的有效性但忽视多样性。本文提出MENTOR框架，通过在关键决策点而非整个路径提供专家指导，实现有效且多样化的探索，从而显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）在增强大型语言模型（LLMs）的推理能力方面应用广泛，但其效果强烈依赖于基础模型的探索能力。高质量的探索需要兼具有效性和多样性。现有方法通过模仿专家轨迹提升了探索的有效性，但却忽视了多样性。

Method: 本文提出MENTOR (Mixed-policy Expert Navigation for Token-level Optimization of Reasoning) 框架。该方法的核心洞见是专家仅需在关键决策点而非整个推理路径提供指导。MENTOR在RLVR中利用这一洞见，在关键决策点提供专家指导，以实现有效且多样化的探索。

Result: 广泛的实验表明，MENTOR使模型能够捕捉专家策略的精髓，而非仅仅进行表面模仿，从而执行高质量的探索，并实现了卓越的整体性能。

Conclusion: MENTOR通过在关键决策点提供专家指导，有效解决了RLVR中LLMs高质量探索（兼顾有效性和多样性）的挑战，显著提升了模型的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [154] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 这篇综述探讨了多模态AI评估的演变，从简单识别转向复杂推理，并指出旧基准的局限性推动了这一转变。文章回顾了历史上的各类评估方法，探讨了当前为多模态大语言模型设计的先进基准，并展望了未来对抽象、创造性及社会智能的评估。结论是AI评估是一个不断演进、对抗性的过程，它重新定义了智能系统的目标。


<details>
  <summary>Details</summary>
Motivation: 评估领域正在经历范式转变，需要从模型“看到什么”的简单识别任务转向探究“为何”和“如何”理解的复杂推理基准。旧基准已趋于饱和，其高表现常掩盖如捷径学习、组合泛化失败等根本性缺陷，因此需要设计更先进、能诊断系统性问题的“认知考试”。

Method: 该论文采用综述方法，将多模态AI的评估演变描绘为一系列日益复杂的“认知考试”。它回顾了从ImageNet时代的“知识测试”，到GQA、VCR等旨在诊断系统缺陷的“应用逻辑与理解”考试，再到为多模态大语言模型（MLLMs）设计的MMBench、SEED-Bench、MMMU等“专家级整合”基准。此外，还探讨了评估抽象、创造性和社会智能的未来方向。

Result: 论文展现了AI评估的清晰演进路径，从早期简单识别任务发展到复杂推理任务，旨在诊断更深层次的缺陷。它揭示了针对多模态大语言模型（MLLMs）的当前基准日益侧重于评估其推理过程，并识别了捷径学习和组合泛化失败等系统性问题。

Conclusion: AI评估不仅仅是数据集的历史，而是一个持续的、对抗性的过程，通过设计更完善的测试来重新定义我们构建真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [155] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一个声明性语言，旨在通过提供统一规范，解决AI代理开发中的碎片化问题，促进跨框架的可移植性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发中框架碎片化的问题，提高AI代理在不同框架间的互操作性和可重用性，减少冗余开发工作，并促进开发工具和可移植性。

Method: 引入Agent Spec作为一种声明性语言和通用的统一规范，使其能作为AI代理及其工作流的交换格式，以实现跨不同AI框架的兼容性。

Result: Agent Spec使代理开发者获得更多可重用组件，框架和工具开发者受益于互操作性支持，研究人员能实现可重现和可比的结果，企业则能加速原型部署、提高生产力和可扩展性。

Conclusion: Agent Spec通过提供统一的声明性语言规范，显著提升了AI代理开发的互操作性、可重用性和效率，为整个AI代理生态系统带来了多方面益处。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [156] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 针对LLM在长环境下的增量地图构建中出现的结构不一致问题，本文提出一个LLM驱动的构建与修复框架，核心在于版本控制和边影响分数，显著提升了地图的正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理大规模环境时，其依赖上下文的查询能力不足以有效推断空间布局并回答用户查询，导致无法进行增量地图构建。因此，需要一种能够从逐步观察中构建完整拓扑图并处理结构不一致性的增量地图构建与修复方法。

Method: 论文提出了一个LLM驱动的地图构建与修复框架，核心方法包括：
1.  **版本控制（Version Control）**：记录图编辑和源观察的完整历史，支持细粒度回滚、冲突追踪和修复评估。
2.  **边影响分数（Edge Impact Score）**：根据结构可达性、路径使用和冲突传播来优先选择成本最低的修复方案。
3.  **精炼基准数据集**：为评估方法，创建了MANGO基准数据集的精炼版本，移除了非拓扑动作和固有结构冲突。

Result: 该方法显著提高了地图的正确性和鲁棒性，尤其在处理纠缠或链式不一致性情境中表现出色。

Conclusion: 研究结果强调了内省的、历史感知的修复机制对于维持LLM智能体连贯空间记忆的重要性。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [157] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: 本文提出了COSMO-RL，一个混合强化学习框架，通过多模态、多任务和多目标信号训练大型多模态推理模型（LMRMs），使其安全性和能力共同提升。实验证明，COSMO-R1模型在提高安全性的同时保持并改进了推理能力，增强了对多模态越狱的鲁棒性，并减少了不必要的拒绝。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型（LMRMs）在实际应用中，安全性和实用性同等重要。多模态设置下的安全性尤其具有挑战性，图像和文本组合可能绕过安全防护，而单一目标训练可能导致策略漂移，从而在良性输入上过度拒绝或在风险输入上不安全地服从。

Method: 研究提出了COSMO-RL，一个混合强化学习框架。该框架在多模态、多任务和多目标信号下训练面向推理的LMRMs，旨在使安全性和能力在一个稳定的管道中共同成长，而非在对齐过程中相互竞争。研究并发布了由此产生的模型COSMO-R1。

Result: 实验结果表明，COSMO-R1在提高安全性的同时，保持并经常改进了多模态推理和指令遵循能力，对多模态越狱表现出更强的鲁棒性，并减少了不必要的拒绝。此外，该框架在不同骨干网络之间也表现出一致的增益。消融实验支持了设计选择。

Conclusion: COSMO-RL框架提供了一条简单有效的路径，可以在大型多模态推理模型中同时推进安全性和通用能力的发展。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [158] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: AgentRL是一个用于LLM智能体可扩展多轮多任务强化学习训练的框架，通过创新的基础设施和算法设计，显著优于现有模型，并开源。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在构建通用智能体方面引起了广泛兴趣，但将强化学习（RL）应用于多轮、多任务LLM智能体的训练仍面临可扩展基础设施和稳定训练算法的挑战。

Method: 本文提出了AgentRL框架，用于可扩展的多轮、多任务智能体强化学习训练。基础设施方面，它包含一个完全异步的生成-训练流水线，一个统一的基于函数调用的API接口、容器化环境开发和集中式控制器。算法方面，提出了交叉策略采样以鼓励模型探索和任务优势归一化以稳定多任务训练。

Result: 实验表明，AgentRL在五个智能体任务上显著优于GPT-5、Claude-Sonnet-4、DeepSeek-R1及其他开源LLM智能体。其多任务训练结果可与所有任务特定模型的最佳结果相媲美。AgentRL已开源。

Conclusion: AgentRL框架及其提出的算法有效解决了LLM智能体在多轮、多任务RL训练中的可扩展性和稳定性挑战，实现了卓越的性能，并为通用智能体的进一步研究提供了坚实的基础。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [159] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 本文提出一个贝叶斯评估框架，用模型潜在成功概率的后验估计和可信区间取代Pass@k和平均准确率，以提供更稳定和可靠的LLM推理能力排名，尤其在样本有限的情况下。


<details>
  <summary>Details</summary>
Motivation: Pass@k和平均准确率在评估LLM推理能力时，尤其在试验次数有限和计算资源受限时，常导致不稳定和误导性的排名。

Method: 引入一个贝叶斯评估框架：将评估结果建模为分类变量（非仅0/1），并使用Dirichlet先验，得到后验均值和不确定性的封闭形式表达式。它用模型潜在成功概率的后验估计和可信区间取代了Pass@k和N次试验的平均准确率，并允许使用先验证据。

Result: 理论上，在均匀先验下，贝叶斯后验均值与平均准确率（Pass@1）是顺序等价的，这解释了其经验鲁棒性。实验证明，在模拟和真实基准测试中，贝叶斯/平均程序比Pass@k及其变体收敛更快，排名稳定性更高，能在更小的样本量下进行可靠比较。该框架能区分统计学上有意义的差异（非重叠可信区间）与噪音，并自然扩展到基于评分标准的评估。

Conclusion: 研究结果建议用这种基于后验的、计算高效的协议取代Pass@k进行LLM评估和排名，该协议统一了二元和非二元评估，并明确了不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [160] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出一种基于多智能体强化学习（RL）的可扩展框架，用于解决复杂商业环境中跨职能（如库存补货与产品推荐）的协调优化问题，显著提高了企业盈利能力。


<details>
  <summary>Details</summary>
Motivation: 在组织复杂性和规模日益增长的背景下，有效的跨职能协调对于提升企业整体盈利能力至关重要。人工智能，特别是强化学习（RL），为解决这一核心挑战提供了新的机会。

Method: 本文提出了一个统一的多智能体RL框架，旨在联合优化不同职能模块（例如库存补货和个性化产品推荐）。首先，开发了一个集成理论模型以捕获功能间的复杂相互作用并推导分析基准。基于此，设计了一种新颖的多时间尺度多智能体RL架构，该架构根据部门功能分解策略组件，并根据任务复杂性和响应性分配不同的学习速度。采用无模型的智能体设计以提高可扩展性，并通过多时间尺度更新增强收敛稳定性和适应性，并建立了算法的渐近收敛性。

Result: 理论分析揭示了产品和时间上的同步调整模式，强调了协调决策的重要性。实验结果表明，所提出的多智能体RL方法相对于孤立决策框架显著提高了盈利能力。此外，训练出的RL智能体的行为与理论模型的管理洞察高度一致。该模型还提升了可扩展性、部署灵活性以及对异构决策的收敛稳定性和适应性。

Conclusion: 该研究提供了一个可扩展、可解释的基于强化学习的解决方案，能够有效支持复杂商业环境中的跨职能协调，从而优化企业绩效。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [161] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一个接地气多模态大语言模型，能联合处理眼底彩照、OCT和文本，提供临床级别的眼科及全身性疾病诊断，并具有更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域的多模态大语言模型未能充分利用眼底彩照（CFP）和光学相干断层扫描（OCT）之间的协同效应，且对量化生物标志物的解释性有限。

Method: 引入GROK模型，一个接地气多模态大语言模型，能联合处理CFP、OCT和文本。它包含三个核心模块：知识引导指令生成、CLIP风格OCT生物标志物对齐和监督指令微调，共同建立从定量到定性的诊断思维链，模拟真实临床推理。为评估该方法，引入了Grounded Ophthalmic Understanding基准，涵盖六类疾病和三个任务。

Result: 仅使用7B参数的Qwen2骨干网络并结合LoRA微调，GROK在报告质量和精细临床指标上均优于7B和32B的同类基线模型，甚至超越了OpenAI o3。

Conclusion: GROK通过模拟临床推理链，有效整合多模态眼科影像和文本，显著提升了医疗诊断的准确性和可解释性，并超越了现有SOTA模型。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [162] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1是一个AI医生代理，旨在通过结合准确的医疗决策和战略性、富有同理心的问诊技巧，弥补现有大语言模型在临床咨询能力上的不足，并在多项评估中显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策准确性方面表现出色，但缺乏进行战略性、富有同理心的问诊能力，这在真实的临床场景中至关重要。

Method: 提出Doctor-R1框架，包含：一个多智能体交互环境；一个两层奖励架构，分别优化临床决策和沟通问诊技能；以及一个经验库，用于基于高质量历史轨迹进行策略学习。

Result: Doctor-R1在OpenAI HealthBench和MAQuE上表现卓越，以更高的参数效率显著超越了最先进的开源专业LLM，并优于强大的专有模型。此外，人工评估也显示出对Doctor-R1生成的临床对话的强烈偏好。

Conclusion: Doctor-R1通过有效结合医疗决策能力和战略性、富有同理心的问诊技巧，成功提升了AI医生在门诊服务中的专业性，展示了其在生成高质量临床对话方面的有效性。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [163] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 本文提出一个理论框架，用深度（推理长度）和宽度（能力多样性）来衡量任务复杂性，并发现大语言模型多智能体系统（LLM-MAS）相对于单智能体系统（LLM-SAS）的优势随任务深度和宽度增加而增大，尤其是在深度方面更显著，从而明确了LLM-MAS的适用场景。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统（LLM-MAS）有望实现更高级AI行为，但现有研究缺乏系统性实验设计来评估其相对于单智能体系统（LLM-SAS）的优势。因此，需要一个原则性的方法来理解任务复杂性，以有效评估LLM-MAS的效用。

Method: 我们提出了一个理论框架，用“深度”（推理长度）和“宽度”（能力多样性）两个维度来表征任务复杂性。接着，我们理论分析了代表性的多智能体辩论系统，并对其在不同深度和宽度的判别性与生成性任务中的性能进行了实证评估。

Result: 理论和实证结果均表明，LLM-MAS相对于LLM-SAS的优势随任务深度和宽度的增加而增大。其中，深度对LLM-MAS的效益影响更为显著。

Conclusion: 本研究澄清了LLM-MAS何时能带来优势，并为未来LLM-MAS方法和基准的设计奠定了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [164] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 本文提出“推测性动作”框架，通过使用更快的模型预测并并行执行多步动作，解决了AI代理执行缓慢的问题，在多个环境中显著降低了端到端延迟。


<details>
  <summary>Details</summary>
Motivation: AI代理在环境中的执行速度通常很慢，阻碍了训练、评估和部署。主要瓶颈在于代理行为是顺序展开的，每个动作都需要耗时的API调用。

Method: 借鉴微处理器中的推测执行和LLM推理中的推测解码思想，提出了一种无损的“推测性动作”框架。该框架使用更快的模型预测可能的动作，从而实现多步并行执行。

Result: 在游戏、电商、网页搜索等代理环境以及操作系统环境的“有损”扩展中，推测性动作实现了高达55%的下一动作预测准确率，并显著降低了端到端延迟。通过更强的猜测模型、Top-K动作预测、多步推测和不确定性感知优化，性能可进一步提升。

Conclusion: “推测性动作”框架为在现实世界中部署低延迟的AI代理系统开辟了一条有前景的道路。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [165] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter系统将离线轨迹提炼为紧凑、上下文感知的提示，高效改进LLM代理在陌生领域的表现，无需昂贵的在线交互或微调，并在多项任务中超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在陌生领域改进时，需昂贵的在线交互或专家数据微调，对闭源模型不切实际，对开源模型有灾难性遗忘风险。离线轨迹虽含知识，但原始轨迹冗长、嘈杂且任务绑定，导致基于演示的方法效果不佳。

Method: 提出JEF Hinter系统，通过“缩放机制”从离线轨迹中提取紧凑、上下文感知的提示，捕获策略和陷阱。该系统能利用成功和失败轨迹，即使仅有失败数据也能提取指导，并支持并行提示生成和独立于基准的提示。推理时，检索器选择相关提示提供有针对性、透明和可追溯的指导。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter持续优于包括基于人类和基于文档提示在内的强大基线。

Conclusion: JEF Hinter系统提供了一种有效且高效的方法，通过利用离线轨迹数据来改进LLM代理在陌生领域中的表现，克服了传统方法的局限性，并展现出卓越的性能和普适性。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [166] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 本文提出一种基于贝叶斯优化（BO-LLM）的方法，利用LLM驱动的高斯过程作为代理模型，高效优化LLM提示词以提升文本分类性能，旨在减少API调用并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）的提示词工程效率低下，且评估成本高昂、调用次数受限。研究旨在通过高效优化提示词来增强LLM在文本分类中的表现。

Method: 采用贝叶斯优化（BO）框架。使用LLM驱动的高斯过程（GP）作为代理模型来估计不同提示词候选的性能。LLM负责生成基于种子提示词扩展的候选提示词。通过结合GP后验和上限置信区间（UCB）采集函数来评估这些候选。优化过程迭代地基于数据子集细化提示词，利用LLM-GP的预测不确定性提高分类准确性并减少API调用。

Result: 所提出的BO-LLM算法在两个数据集上进行了评估，并详细讨论了其优势。

Conclusion: BO-LLM算法能够有效且高效地优化LLM提示词，以提升文本分类的准确性，同时减少API调用次数，为LLM的提示词工程提供了一种有前景的解决方案。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [167] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究通过心理网络分析比较了人类和大型语言模型（LLM）的内部世界模型（IWM），发现两者在想象网络结构上存在显著差异，表明其IWM不相似。


<details>
  <summary>Details</summary>
Motivation: 探讨想象力的计算目标，提出想象力服务于访问内部世界模型（IWM），而非仅为最大化奖励。旨在比较人类和LLM的IWM。

Method: 采用心理网络分析方法，通过两份问卷评估人类和LLM的想象生动性评分，并据此构建想象网络。

Result: 人类的想象网络显示出不同中心性度量（如预期影响力、强度和接近度）之间的相关性。而LLM的想象网络缺乏聚类，且在不同提示和会话记忆条件下，其中心性度量之间的相关性较低。

Conclusion: 研究结果表明人类和LLM智能体内部世界模型（IWMs）之间缺乏相似性。本研究提供了一种比较人类和AI内部生成表征的新方法，为在AI中开发类人想象力提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [168] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 研究发现，在自我改进系统中，追求即时效用优化与保持可靠学习能力之间存在结构性矛盾。若模型容量无限制增长，效用驱动的修改可能损害学习的统计前提，导致任务不可学习。为保持学习能力，要求模型族容量必须有界。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超级智能，代理的自我改进能力日益重要。然而，理解这种自我修改可能带来的内在冲突及其对学习能力的影响，对于确保系统的安全与稳定发展至关重要。

Method: 本研究通过五轴分解和决策层对自我改进进行了形式化，分离了激励与学习行为，并孤立分析各轴。通过理论分析识别了效用-学习张力，并利用数值实验验证了理论，对比了破坏性效用策略与能保持可学习性的“双门策略”。

Result: 核心发现是自我修改系统中存在显著的效用-学习张力：追求即时或预期效用的改变可能侵蚀可靠学习和泛化的统计前提。研究表明，仅当策略可达模型族容量一致有界时，无分布学习保证才得以保留；当容量可以无限增长时，效用驱动的自我修改可使可学习任务变得不可学习。在实际常用假设下，这些轴简化为同一容量准则，提供了安全自我修改的单一边界。

Conclusion: 为实现安全的自我修改，未来超级智能系统必须在追求效用最大化的同时，严格控制模型容量的增长，以防止其学习能力的丧失。维持模型容量有界是保证系统可学习性的关键，且提出的“双门策略”在实践中能有效保持可学习性。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [169] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 本文提出解耦奖励策略优化（DRPO）框架，旨在解决大型推理模型（LRMs）的过度思考问题。DRPO通过解耦正确与不正确推理的长度奖励信号，显著缩短了推理路径，同时将性能损失降至最低（例如，在GSM8k上实现77%长度缩减，仅1.1%性能损失）。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理任务上表现出色，但常过度思考，即使是简单问题也生成冗长且冗余的推理，导致计算成本和响应延迟大幅增加。现有长度奖励方法（如GRPO）虽然试图缩短推理，却导致显著的性能下降。作者发现这是因为GRPO的优势函数错误地惩罚了正确但较长的推理，从而抑制了有效的推理过程。

Method: 本文提出解耦奖励策略优化（DRPO）框架。DRPO的核心在于将正确与不正确推理的基于长度的学习信号解耦。它确保正确推理的奖励信号仅在正样本组内进行规范化，不受负样本的干扰。DRPO的目标是将一个优化的正数据分布（在KL正则化下最大化基于长度的奖励）整合到一个判别目标中。该方法导出了此分布的闭式解，允许仅使用在线策略数据和重要性权重进行高效计算。该公式还具有通用性，可以纳入除了长度之外的其他正数据偏好奖励。

Result: 在数学推理任务上的实验表明，DRPO显著优于六个基线推理方法。具体而言，使用1.5B模型，DRPO在GSM8k等简单问题上实现了77%的长度缩减，而性能损失仅为1.1%。相比之下，现有最佳基线方法在牺牲4.3%性能的情况下仅达到68%的长度缩减。

Conclusion: DRPO成功解决了LRMs过度思考导致的推理冗长问题，通过创新的奖励解耦机制，在显著提升推理简洁性的同时，有效避免了性能大幅下降。该框架具有通用性，可应用于其他偏好奖励。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: 本文提出PARS，一个提示感知的LLM任务调度器，通过成对排序近似最短作业优先（SJF）调度，集成至vLLM，显著提升了推理效率、降低延迟，并展现出良好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推理任务调度策略（如FCFS）存在队头阻塞问题，导致长任务延迟短任务，阻碍实现低延迟和高吞吐，尤其对于推理能力强的LLM。

Method: 引入PARS，一个提示感知的LLM任务调度器。它通过使用带边际排序损失的成对排序来近似最短作业优先（SJF）调度，以预测基于响应长度的任务排序。PARS专注于有影响力的调度决策，并无缝集成到先进的LLM服务系统vLLM中。

Result: 广泛实验表明，PARS显著提升了多LLM和真实世界推理数据集上的性能，包括推理工作负载，有效降低了延迟且开销极小。跨模型评估证实其设计泛化性良好，即使预测器在不同LLM上训练也能实现有效调度。

Conclusion: PARS通过创新的SJF近似方法有效解决了LLM推理调度中的效率和延迟问题。它显著提高了性能，并在不同LLM之间展现出强大的泛化能力，为LLM服务提供了高效且通用的调度解决方案。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [171] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一种跨模态预测模型，它将多元时间序列转化为图像，利用大型视觉模型（LVM）提取跨通道模式，并与时间序列特征融合，从而高效捕捉变量间关系。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型常采用通道独立架构，忽略了关键的跨通道依赖；当前多模态方法未能充分利用LVM解释时空数据的能力；以及利用不同模态信息提升时间序列预测性能的潜力尚未完全挖掘。

Method: VIFO将多元时间序列渲染成图像，使预训练的LVM能够提取复杂的跨通道模式。这些视觉特征随后与时间序列模态的表示进行对齐和融合。模型通过冻结LVM，仅训练其7.45%的参数。

Result: VIFO在多个基准测试中取得了有竞争力的性能。

Conclusion: VIFO为捕获多变量时间序列中的跨变量关系提供了一个高效且有效的解决方案。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [172] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 为解决DNN在噪声和扰动下的可靠性问题，本文提出一种可迁移的频率感知对抗攻击，并基于此开发了新型归因方法FAMPE，显著提升了DNN的可解释性，在Insertion Score上平均提高了13.02%。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在真实世界噪声和有意扰动下的可靠性面临挑战。现有归因方法（attribution methods）的效果不佳，亟需改进。

Method: 本文提出一种新型的可迁移频率感知对抗攻击（transferable frequency-aware attacks），通过高频和低频分量进行频率感知探索。在此基础上，提出了一种名为Frequency-Aware Model Parameter Explorer (FAMPE) 的新型归因方法，旨在提高DNN的可解释性。

Result: 相较于当前最先进方法AttEXplore，FAMPE在插入分数（Insertion Score）上平均提高了13.02%，性能优于现有方法。此外，通过详细的消融研究，探讨了高频和低频分量在可解释性中的作用。

Conclusion: FAMPE作为一种基于频率感知的归因方法，有效提高了深度神经网络的可解释性，并通过实验证明其性能优于现有最先进方法。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [173] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE是一种ADMM框架，通过分治策略实现内存高效的结构化剪枝，性能媲美全局剪枝，同时大幅降低内存消耗至O($\sqrt{N}$)，支持十亿参数模型部署。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）剪枝面临挑战：全局剪枝内存开销大(O(N))，局部剪枝性能不佳且忽视层间依赖，结构化剪枝通常依赖全局剪枝且在局部优化下性能易退化。研究目标是同时实现结构化剪枝的硬件效率和局部剪枝的内存效率。

Method: 提出一种分治策略，将全局剪枝问题分解为跨模块的协调子问题，每个子问题可适应有限的GPU内存。设计了基于ADMM的STRUPRUNE框架，将结构化稀疏性集成到剪枝过程中，结合了局部剪枝的内存效率和结构化方法的硬件兼容性。推导了结构化剪枝掩码的闭式解析解，提供了层级稀疏性分配规则；并开发了基于能量的渐近框架，生成softmax形式的分配方案，简化优化并适应层间重要性差异。

Result: STRUPRUNE在困惑度（perplexity）上与全局结构化剪枝性能持平，同时将内存成本从O(N)降低到O($\sqrt{N}$)，使其能够实际部署在十亿参数规模的模型上。

Conclusion: STRUPRUNE通过创新的分治策略和ADMM框架，成功实现了结构化剪枝的内存高效和高性能，为大型语言模型的实用化剪枝提供了可行方案，克服了现有方法的内存和性能瓶颈。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [174] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了首个针对未对齐多模态数据的活跃学习框架，旨在降低跨模态对齐的标注成本，其算法结合了不确定性和多样性原则，并有效减少了标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有的活跃学习（AL）算法主要关注单模态数据，忽略了多模态学习中巨大的标注负担，尤其是在CLIP和SigLIP等现代多模态管道中，获取高质量跨模态对齐的成本非常高。

Method: 引入了首个针对未对齐数据的多模态活跃学习框架，其中学习器主动获取跨模态对齐而非预对齐数据的标签。开发了一种新算法，该算法将不确定性和多样性原则结合在模态感知设计中，实现了线性时间采集，并无缝适用于基于池和基于流的设置。

Result: 在基准数据集上进行的广泛实验表明，该方法在保持性能的同时，持续降低了多模态标注成本；例如，在ColorSwap数据集上，它将标注需求削减了高达40%，且没有损失准确性。

Conclusion: 所提出的多模态活跃学习框架及其算法能有效解决多模态学习中的标注瓶颈，通过高效地获取跨模态对齐，显著降低了标注成本，同时保持了模型性能。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [175] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 本研究评估了神经算子架构，以实现创伤性脑损伤（TBI）中脑位移场的快速、患者特异性预测，将计算时间从数小时缩短至毫秒。


<details>
  <summary>Details</summary>
Motivation: 创伤性脑损伤是一个严重的公共卫生问题，传统的有限元（FE）模型虽然精度高，但计算成本昂贵（每次模拟需数小时），限制了其在临床上进行快速决策的实用性。因此，需要开发一种能够实时进行TBI建模的方法。

Method: 研究将TBI建模表述为一个算子学习问题，将患者特定的MRI解剖图像、磁共振弹性成像（MRE）硬度图以及人口统计学特征映射到全场3D脑位移预测。评估了四种神经算子（NO）架构：傅里叶神经算子（FNO）、分解式FNO（F-FNO）、多网格FNO（MG-FNO）和深度算子网络（DeepONet）。这些模型在249个MRE数据集上进行训练和评估，涵盖了生理相关的频率（20-90 Hz）。

Result: MG-FNO取得了最高的准确性（MSE = 0.0023，94.3%的空间保真度）并能保留精细尺度特征。F-FNO收敛速度比标准FNO快2倍。DeepONet提供了最快的推理速度（14.5次迭代/秒），比MG-FNO快7倍，适用于嵌入式或边缘计算应用。所有神经算子都将计算时间从数小时缩短到毫秒，同时不牺牲解剖学真实性。

Conclusion: 神经算子为预测脑变形提供了一种高效、分辨率不变的方法，为实时、患者特异性的TBI风险评估、临床分诊支持和防护设备优化铺平了道路。这表明基于神经算子的人脑数字孪生具有巨大潜力，可在临床和人群健康背景下实现可扩展、按需的生物力学建模。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [176] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出一种新的参数化方法来解决可微分逻辑门网络（DLGNs）因底层参数化导致的梯度消失、训练成本高及深度扩展性差等问题，实现模型小型化、训练加速并保持或提升精度。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门网络（DLGNs）在推理效率和精度方面表现出色，但受限于梯度消失、离散化误差和高训练成本，且增加深度会损害精度。这些问题的根本原因在于逻辑门神经元本身的底层参数化。

Method: 提出了一种新的参数化方法，该方法不仅解决了现有DLGNs的上述问题，还能以对数方式缩小每个逻辑门输入的参数量。

Result: 对于二元输入，模型大小减少4倍，反向传播速度提高高达1.86倍，训练步数减少8.5倍。在CIFAR-100数据集上，模型精度保持稳定，有时甚至优于原始参数化方法。

Conclusion: 所提出的新参数化方法有效克服了DLGNs的扩展性障碍，显著提高了训练效率，同时保持或提升了模型精度。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [177] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: Numerion模型利用多维超复数空间和RHR-MLP结构，通过自然频率分解实现时间序列预测，并在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法受限于计算复杂性和假设鲁棒性。研究发现，在超复数空间中时间序列的特征频率会自然降低，这为新的预测方法提供了启发。

Method: 提出Numerion模型，它将线性层和激活函数推广到超复数空间，并引入Real-Hypercomplex-Real域多层感知机（RHR-MLP）架构。Numerion利用多个RHR-MLP将时间序列映射到不同维度的超复数空间进行自然分解和独立建模，并通过动态融合机制自适应地融合潜在模式。

Result: 模型在多个公共数据集上取得了最先进的预测性能。可视化和定量分析证明了多维RHR-MLP能自然分解时间序列，且高维超复数空间倾向于捕获低频特征。

Conclusion: Numerion模型通过利用超复数空间自然分解时间序列的特性，提供了一种计算高效且鲁棒的预测方法，尤其擅长捕获不同频率特征，实现了卓越的预测效果。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [178] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 本文提出Diffusion Router (DR)框架，一个统一的基于扩散的模型，用于通用多域翻译 (UMDT)。DR通过中心域路由实现任意K个域之间的翻译，仅需K-1个中心域配对数据集，解决了现有MDT方法的局限性，并在多个UMDT基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多域翻译 (MDT) 方法要么需要完全对齐的元组数据，要么只能处理训练中见过的域对，这限制了它们的实用性并排除了许多跨域映射。因此，本文引入了通用多域翻译 (UMDT) 任务，旨在仅使用K-1个与中心域配对的数据集，实现任意K个域之间的翻译。

Method: 本文提出了Diffusion Router (DR) 框架，一个统一的、基于扩散的方法。它使用一个单一的噪声预测器，通过条件化源域和目标域标签，来建模所有中心域与非中心域之间的翻译。DR通过路由中心域实现间接的非中心域翻译。此外，还引入了一种新颖的可扩展学习策略，结合了变分界目标和高效的Tweedie细化过程，以支持直接的非中心域映射。

Result: DR在三个大规模UMDT基准测试中进行了评估，对间接和直接翻译都取得了最先进 (state-of-the-art) 的结果。同时，它降低了采样成本，并解锁了草图↔分割等新型任务。

Conclusion: 研究结果表明，DR是一个可扩展且多功能的框架，适用于跨多个域的通用翻译任务。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [179] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: 本文提出分层偏好学习（HPL）框架，通过整合轨迹、步长和创新的组级偏好优化，并结合双层课程学习，有效解决大型语言模型（LLM）代理在复杂任务中面临的粒度不匹配问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的离线方法（如DPO）在对LLM代理进行对齐时，面临粒度不匹配问题：轨迹级信号过于粗糙，难以精确分配信用；步长级信号又过于短视，无法捕捉多步行为的价值，从而限制了LLM代理解决复杂、长周期问题的能力。

Method: 引入分层偏好学习（HPL）框架，通过结合轨迹级和步长级DPO，核心创新在于组级偏好优化。HPL将专家轨迹分解为语义连贯的动作组，并生成对比性的次优组进行学习。同时，引入双层课程调度器，根据组长度（子任务复杂性）和样本难度（偏好与非偏好组之间的奖励差距）组织学习过程，从简到繁进行优化。

Result: 在三个具有挑战性的代理基准测试中，HPL的性能优于现有最先进的方法。分析表明，分层DPO损失能有效整合多粒度偏好信号，而双层课程对于代理解决从简单行为到复杂多步序列的广泛任务至关重要。

Conclusion: HPL通过其独特的多粒度偏好学习和双层课程机制，成功克服了LLM代理对齐中的粒度不匹配挑战，显著提升了代理解决复杂任务的能力和性能。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [180] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 针对对抗性机器学习中现有悲观双层优化模型因对抗者不受限制而导致不切实际的攻击及分类器性能下降的问题，本文提出了一种受约束的悲观双层优化模型，通过限制对抗者的行动使其攻击更符合现实，并实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在对抗性机器学习中，现有的悲观双层优化方法由于假设对抗者拥有无限的攻击能力，容易导致模型过于悲观和不切实际。这种模型可能使对抗者生成的数据失去其原始性质，无法真实反映现实世界的攻击，进而导致训练出的分类器在实际数据上表现不佳。

Method: 本文构建了一个受约束的悲观双层优化模型。该模型通过限制对抗者的行动和攻击方式，确保对抗性数据的生成更符合现实情景，从而寻找到一个更真实有效的解决方案。

Result: 通过实验验证，本文提出的受约束的悲观双层优化模型，在平均性能上优于现有的非受限方法。

Conclusion: 通过对对抗者行动施加合理约束，所提出的悲观双层优化模型能够更准确地反映现实，从而训练出对真实世界对抗性攻击更具鲁棒性和更有效能的分类器。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [181] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 本文引入SciTS基准测试集和TimeOmni框架，旨在解决大型语言模型在处理复杂科学时间序列数据时存在的挑战，提升其理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）对时间序列这种基本科学数据模态的处理方法不足（如编码为文本或图像），难以实现全面的科学时间序列理解与生成。现有统一时间序列模型过于专业化，对非周期、异构科学信号的有效性尚不明确，存在研究空白。

Method: 1. 构建SciTS基准：涵盖12个科学领域、43项任务，包含超过5万个实例，信号长度从$10^0$到$10^7$，频率高达10 MHz。2. 对17种模型进行基准测试：包括纯文本LLMs、多模态LLMs和统一时间序列模型。3. 提出TimeOmni框架：使LLMs能够理解和生成时间序列，同时兼容通用LLM的训练范式。

Result: 1. 通用LLMs比专业时间序列模型展现出更强的泛化能力。2. 将时间序列表示为文本或图像会因序列过长或数值精度损失而限制LLMs的性能。

Conclusion: 本研究填补了科学时间序列专用基准和建模框架的空白，为LLMs理解和生成复杂的时序科学数据奠定了基础。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [182] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出一种基于多层移动边缘计算（MLMEC）并结合知识蒸馏（KD）的跌倒检测系统，有效提升精度并显著降低数据延迟。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口增加，跌倒检测（FD）系统的重要性日益凸显。现有FD系统通常依赖边缘设备-云中心架构，但面临边缘设备模型大小受限和数据传输到云中心时的延迟挑战。

Method: 本文提出一种多层移动边缘计算（MLMEC）框架，旨在平衡精度与延迟。该框架将系统架构划分为多个站点，每个站点部署一个神经网络模型。当前端设备无法可靠检测跌倒时，数据会被传输到具有更强大后端计算能力的站点。为提高前端检测精度，文中采用了知识蒸馏（KD）方法，利用高功率后端站点的知识经验来增强前端学习。

Result: 仿真结果表明，知识蒸馏（KD）方法使SisFall数据集上的检测精度提高了11.65%，FallAllD数据集上提高了2.78%。与没有KD的MLMEC相比，结合KD的MLMEC在FallAllD数据集上将数据延迟率降低了54.15%，在SisFall数据集上降低了46.67%。

Conclusion: 所提出的MLMEC跌倒检测系统结合知识蒸馏技术，在提高检测精度的同时显著降低了数据延迟。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [183] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 本文提出Triple-BERT，一种集中式单智能体强化学习(SARL)方法，通过动作分解和BERT网络应对大规模网约车订单调度的观察和动作空间挑战，显著提升了调度效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临复杂的实时订单匹配和系统不确定性挑战，大规模的司乘数量导致观察空间巨大，使得集中式调度变得困难。现有独立多智能体强化学习(MARL)方法缺乏全局信息和协作性，而集中训练去中心化执行(CTDE)的MARL方法则面临维度灾难。

Method: 我们提出Triple-BERT，一种基于TD3变体的集中式单智能体强化学习(SARL)方法，专为大规模网约车订单调度设计。为处理庞大的动作空间，它采用动作分解策略，将联合动作概率分解为单个司机的动作概率。为应对广泛的观察空间，引入了一种新型BERT网络，通过参数重用缓解参数增长，并利用注意力机制有效捕捉大量司乘之间的复杂关系。

Result: Triple-BERT在曼哈顿真实网约车数据集上进行验证，相比当前最先进方法，整体性能提升约11.95%，其中服务订单量增加4.26%，接驾时间减少22.25%。

Conclusion: Triple-BERT通过其新颖的动作分解策略和BERT网络，有效解决了大规模网约车订单调度中的观察和动作空间挑战，显著提高了调度效率、服务订单量和用户满意度，为未来网约车平台提供了一种强大的解决方案。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [184] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: 本文提出POEM，一种新的测试时自适应（TTA）方法，通过利用先前被忽视的可靠样本和一个额外的自适应分支网络，显著提升了TTA在各种分布偏移场景下的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的TTA方法常依赖熵作为置信度指标进行模型优化，但对预设的熵阈值敏感，导致许多潜在可靠的样本被忽略和未充分利用。这些被忽视的样本本可以提供稳定的监督信息和有益的梯度来指导模型自适应。

Method: 本文提出了一种名为**POEM**（Promote TTA via eXploring the PreviOusly unexplorEd reliable saMples）的通用方法。POEM的核心是探索并利用那些先前未被发现的可靠样本。此外，还引入了一个额外的**自适应分支网络（Adapt Branch network）**，以平衡提取领域无关的表示和在目标数据上实现高性能。POEM的核心思想还可以作为一种增强策略来提升现有TTA方法的性能。

Result: POEM在多种架构和具有挑战性以及真实世界的领域偏移场景下，均持续优于现有的TTA方法，同时保持了计算效率。其有效性通过广泛的分析和彻底的消融研究得到了验证。

Conclusion: POEM是一种通用且有效的TTA方法，它通过有效利用可靠样本解决了现有熵基TTA方法的局限性。POEM不仅在性能和效率上超越了现有方法，其核心思想还能作为增强策略提升其他TTA方法的表现。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [185] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 本文提出MASA（Meta-Awareness via Self-Alignment）训练方法，通过自生成信号增强大型推理模型的元认知能力，从而显著提升了模型在域内任务的准确性和训练效率，并增强了域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知（即“知道如何思考”）能力，表现为真实执行路径与预测元信息之间存在严重错位。作者假设，对齐元预测与真实执行路径将显著提升模型性能。

Method: 设计了一套名为MASA（Meta-Awareness via Self-Alignment）的训练流程。该方法不依赖外部训练源，而是利用模型自生成的信号来训练元认知。为提高训练效率，MASA会过滤掉琐碎或无法解决的零方差提示，并截断不太可能得出正确答案的冗长执行路径。

Result: MASA策略显著提升了模型在域内任务的准确性和训练效率，并展现出强大的域外泛化能力。具体表现为：GRPO训练速度提升超过1.28倍；AIME25准确率提高19.3%；六个数学基准测试平均准确率提高6.2%；GPQA-Diamond域外泛化能力提升3.87%；跨逻辑、科学和编程等13个基准测试整体准确率提升2.08%。

Conclusion: 增强模型的元认知能力可以直接转化为准确率的提升。通过MASA方法，可以在不依赖外部训练数据的情况下，有效提升模型的性能和训练效率，且元认知指导还能增强模型的域外泛化能力。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [186] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 零样本学习中的语义空间常含冗余属性，本文提出一种分区方案和两种特征选择策略（RFS和GA），有效提升了归纳式ZSL在未见类上的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本学习（ZSL）是通用人工智能系统中的重要范式，尤其在开放世界场景中，系统需动态适应新任务。语义空间在连接已知类和未知类方面发挥关键作用，但它们通常包含噪声、冗余或不相关属性，从而阻碍系统性能。

Method: 引入一种分区方案，用于在最具挑战性的归纳式设置中模拟未见条件，无需访问未见类的语义信息即可评估属性相关性。在此框架下，研究了两种互补的特征选择策略：1. 自适应的嵌入式特征选择（RFS），将模型驱动的排名转化为有意义的语义剪枝；2. 进化计算（GA），直接更广泛地探索属性子集空间。

Result: 在AWA2、CUB、SUN、aPY、FLO五个基准数据集上进行的实验表明，两种方法都能通过减少冗余，持续提高未见类的准确性，且方式互补：RFS高效且具竞争力但依赖关键超参数，而GA成本更高但探索搜索空间更广且避免了超参数依赖。

Conclusion: 研究结果证实语义空间本质上是冗余的，并强调所提出的分区方案是在归纳条件下精炼语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [187] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 现有DeepJSCC在多厂商场景中因潜在空间不匹配而失效，本文提出语义信道均衡方法，通过线性映射、轻量级神经网络或零样本均衡器对齐异构潜在空间，并分析了其在图像重建中的性能权衡，为异构AI无线网络部署提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有DeepJSCC方案假设收发端存在共享潜在空间并可协同训练，这在多厂商部署中不适用，导致潜在空间不匹配并引入“语义噪声”，从而降低重建质量和下游任务性能。

Method: 提出语义信道均衡方法，增加一个处理阶段以对齐物理和语义缺陷下的异构潜在空间。研究了三类对齐器：(i) 线性映射；(ii) 轻量级神经网络；(iii) 零样本Parseval-frame均衡器。通过AWGN和衰落信道上的图像重建实验进行评估。

Result: 通过大量实验，量化了不同对齐器在复杂性、数据效率和保真度之间的权衡。

Conclusion: 为在异构AI原生无线网络中部署DeepJSCC提供了指导方针。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [188] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 本文提出一种新范式，利用神经网络预测机床内部高精度温度和热通量场，以实现灵活、通用且模块化的热误差修正，并优化了传感器选择。


<details>
  <summary>Details</summary>
Motivation: 传统及现有数据驱动的热误差修正方法受限于特定误差类型、位置或机器配置，通用性和适应性不足，限制了其应用范围。

Method: 引入神经网络预测机床内部的温度和热通量场。使用有限元方法生成的数据进行训练，并结合基于相关性的测量点选择策略以减少硬件需求。对包括RNN、GRU、LSTM、Bi-LSTM、Transformer和TCN在内的多种时间序列NN架构进行了基准测试，训练了专用模型和通用模型以评估其在不同初始条件和未知场景下的性能。

Result: 研究结果表明，该方法能够准确且低成本地预测温度和热通量场。对多种神经网络架构的基准测试也验证了其有效性。

Conclusion: 该工作为机床环境中灵活、通用且可扩展的热误差修正奠定了基础，通过预测温度/热通量场而非直接误差值，克服了现有方法的局限性。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [189] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛Dropout方法，旨在确保LoRA合并时语义向量的正交性，但实证分析表明其未能实现语义解耦或组合性。


<details>
  <summary>Details</summary>
Motivation: 当合并多个LoRA模块（例如生成特定风格的物体）时，其语义向量可能相互干扰，影响组合效果。

Method: 引入正交蒙特卡洛Dropout机制，在理论和运行时层面强制合并后的稀疏语义向量保持严格正交，且不增加额外时间复杂度。

Result: 实证分析显示，尽管确保了LoRA之间的正交性，但这并未实现先前工作中强调的语义解耦或组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，这促使研究人员需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [190] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 本文探讨文生图模型中机器遗忘的挑战，引入“记忆自再生”任务和MemoRa策略来分析模型遗忘与知识恢复能力，并指出遗忘存在短时和长时两种形式。


<details>
  <summary>Details</summary>
Motivation: 现代文生图模型虽强大，但存在被滥用生成有害内容的风险，催生了机器遗忘需求。然而，当前遗忘技术难以彻底移除特定知识，模型仍能通过对抗性提示召回“已遗忘”概念，表明遗忘过程复杂且评估方法不足。

Method: ['提出“记忆自再生（Memory Self-Regeneration）”任务，用于研究模型遗忘和回忆知识的能力。', '提出MemoRa策略，这是一种支持有效恢复先前遗失知识的再生方法。', '建议将知识检索的鲁棒性作为开发更稳健有效遗忘技术的一个关键但未充分探索的评估指标。']

Result: ['遗忘以两种截然不同的方式发生：短时遗忘（概念可快速召回）和长时遗忘（恢复更具挑战性）。', '通过提出的任务和策略，揭示了模型知识遗忘和恢复的复杂性。']

Conclusion: 机器遗忘是复杂且充满挑战的任务，模型即使“遗忘”了知识也可能通过特定机制恢复。知识检索的鲁棒性是评估遗忘技术有效性的关键指标。区分短时和长时遗忘有助于深入理解并开发更稳健、有效的机器遗忘技术。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [191] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 研究发现，将推理数据在LLM预训练阶段早期引入对其推理能力提升至关重要（平均提升19%），且效果优于后期监督微调，并提出了预训练需多样性、微调需高质量数据的不对称分配原则。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理能力提升主要依赖后训练，但推理数据在预训练阶段的作用及其与后训练的相互影响尚不明确，缺乏系统性研究来探究早期引入推理数据是否更优或有潜在风险。

Method: 进行了首次系统性研究，探讨了不同规模、多样性和质量的推理数据在LLM训练的不同阶段（预训练和后训练）引入时，如何影响模型的性能。

Result: 早期将推理数据融入预训练至关重要（平均提升19%），其奠定的基础能力是后期监督微调（SFT）无法完全复制的。数据分配存在不对称原则：预训练阶段最受益于推理模式的广泛多样性（平均提升11%），而SFT对数据质量更为敏感（平均提升15%）。高质量的预训练数据具有潜在效应，仅在SFT后激活；盲目扩增SFT数据反而可能有害，抵消早期推理注入的益处。

Conclusion: 研究结果挑战了语言建模与推理的传统分离，为在整个训练流程中战略性地分配数据以构建更强大的LLM提供了指导原则。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [192] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 本文提出MindCraft框架和概念树，通过对大模型每层进行谱分解，揭示概念如何从共享表示中分化，形成层次结构，从而实现对模型内部概念表示的深度解释。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在多任务上表现出色，但其内部如何构建和稳定概念的机制尚不明确，仍是一个有待解决的问题。

Method: 受因果推断启发，研究引入了基于概念树的MindCraft框架。该方法通过对模型每一层应用谱分解，并将主方向连接成分支的概念路径，以重建概念的层次化形成过程，揭示它们何时从共享表示中分化为线性可分离的子空间。

Result: 在医学诊断、物理推理和政治决策等多个学科的场景中进行的实证评估表明，概念树能够恢复语义层次、解耦潜在概念，并具有广泛的跨领域适用性。

Conclusion: 概念树建立了一个普遍适用且强大的框架，能够对深度模型中的概念表示进行深入分析，标志着可解释人工智能基础研究迈出了重要一步。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [193] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究创新性地使用变分自编码器（VAE）检测陆地初级生产力（GPP）的极端事件，并与奇异谱分析（SSA）进行比较。结果显示，两种方法在识别极端事件频率空间模式上高度一致，并预测未来负碳循环极端事件将增加。VAE在处理非线性依赖方面展现出计算优势和更强能力。


<details>
  <summary>Details</summary>
Motivation: 气候异常对陆地碳循环动态影响显著，因此急需开发稳健方法来检测和分析植物生产力中的异常行为。

Method: 本研究提出将变分自编码器（VAE）应用于社群地球系统模型（CESM2）模拟的美国大陆四个AR6区域的GPP数据，以识别极端事件。VAE架构包含三个密集层和一个潜在空间，输入序列长度为12个月，通过重构误差识别异常。研究将VAE与传统奇异谱分析（SSA）方法在1850-80、1950-80和SSP585情景下的2050-80三个时间段进行比较。极端事件定义为异常的5%阈值。

Result: VAE与SSA方法在极端事件频率的空间模式上表现出很强的区域一致性，尽管VAE产生的阈值更高（VAE为179-756 GgC，SSA为100-784 GgC）。两种方法均显示，到2050-80年，负碳循环极端事件的强度和频率将增加，尤其是在北美西部和中部。

Conclusion: VAE方法表现出与SSA技术相当的性能，并提供计算优势，同时增强了捕获碳循环变异中非线性时间依赖性的能力。与SSA不同，VAE方法无需预先定义数据中信号的周期性，而是能够从数据中自动发现它们。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [194] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT$^2$-LLM：一种针对LLMs的训练后三值化框架，通过两阶段量化和结构重排序，在降低内存和加速推理的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的巨大内存和计算需求阻碍了部署。尽管三值化是一种有前景的压缩技术，但在训练后量化（PTQ）设置中，由于缺乏训练参数优化、异常值和权重分散等问题，其潜力尚未被充分开发。

Method: 提出PT$^2$-LLM框架，核心是非对称三值量化器，配备两阶段细化：1) 迭代三值拟合（ITF），用于构建最优三值网格和灵活舍入以最小化量化误差；2) 激活感知网格对齐（AGA），进一步优化网格以匹配全精度输出。此外，引入了结构相似性重排序（SSR）策略来简化量化并缓解异常值影响。

Result: PT$^2$-LLM在与最先进的2比特PTQ方法相比时，能以更低的内存成本提供具有竞争力的性能，并加速预填充和解码过程，实现端到端的推理加速。

Conclusion: PT$^2$-LLM通过创新的量化器和重排序策略，成功解决了LLMs训练后三值化面临的挑战，实现了在保持性能的同时大幅降低内存消耗并加速推理，证明了其在实际部署中的巨大潜力。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [195] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 该论文通过理论框架分析了多模态对比学习中的模态间隙问题，指出维度坍塌是其根本原因，并提出了消除该间隙的方法。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习旨在将不同模态数据嵌入共享空间，但存在模态间隙现象（不同模态表示占据独立区域），且其对下游任务性能影响不一致。本文旨在探究模态间隙的成因及其对下游任务的影响。

Method: 引入首个理论框架，用于分析多模态对比学习的收敛最优表示和模态对齐情况。

Result: 理论证明无约束或锥形约束下模态间隙收敛为零；子空间约束下（即维度坍塌导致表示落入不同超平面）模态间隙收敛为两超平面间的最小角度，从而识别出“维度坍塌”是模态间隙的根本原因。模态间隙通过影响样本对齐来影响下游性能。此外，在子空间约束下，可通过超平面旋转和共享空间投影两种方式实现模态间的完美对齐。

Conclusion: 模态间隙的根本原因在于维度坍塌，它通过影响样本对齐来影响下游任务。该研究为理解和解决多模态对比学习中的模态间隙问题提供了理论基础，并指明了实现模态对齐的有效途径。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [196] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [197] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的开源扩散编码模型，结合预训练、中训练和指令调优，在保持推理效率的同时，性能超越了部分7B参数的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具备自回归编码器所缺乏的双向上下文和填充能力，但现有系统过于庞大，实用性受限。

Method: 引入CoDA，一个1.7B参数的扩散编码器。通过大规模扩散预训练、以代码为中心的中训练和指令调优，并采用置信度引导采样来确保推理延迟具有竞争力。

Result: 在Humaneval、MBPP和EvalPlus基准测试中，CoDA-1.7B-Instruct的性能与高达7B参数的扩散模型相当或超越。

Conclusion: CoDA提供了一个轻量级、高性能的扩散编码助手解决方案，并开源了模型、评估工具和训练流程，以加速相关研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [198] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 针对LLM决策边界难以构建的问题，本文提出“决策势能面”(DPS)及其近似算法K-DPS，首次实现了LLM决策边界的近似构建，并提供了严格的理论误差分析与实验验证。


<details>
  <summary>Details</summary>
Motivation: 分析LLM决策边界对于揭示模型核心属性和解释行为至关重要。然而，由于LLM庞大的词汇序列空间和自回归特性，目前尚无法在计算上可行地构建主流LLM的决策边界。

Method: 提出“决策势能面”（DPS）这一新概念来分析LLM决策边界，其定义基于区分不同采样序列的置信度。理论证明DPS中的零高度等高线等价于决策边界。基于DPS，开发了K-DPS近似构建算法，该算法仅需K次有限序列采样即可近似LLM决策边界。

Result: 理论上，证明了DPS的零高度等高线与决策边界等价。K-DPS算法能够以可忽略的误差近似LLM决策边界。论文推导了K-DPS与理想DPS之间绝对误差、期望误差和误差集中度的上限，并证明这些误差可与采样次数进行权衡。实验结果也验证了方法的有效性。

Conclusion: 本文首次通过DPS和K-DPS算法，实现了计算上可行的LLM决策边界近似构建，解决了长期存在的挑战。这为理解LLM的决策机制提供了新的视角和工具，并获得了全面的理论支持和经验验证。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [199] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 本文将Transformer建模为连续偏微分方程（PDE）动力系统，理论性地解释了残差连接和层归一化作为必要数学稳定器的作用，避免了表征漂移和训练不稳定。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在人工智能领域取得革命性成功，但其内部机制的理论理解仍不清晰。

Method: 引入一种新颖的分析框架，将Transformer的离散分层结构重构为由主偏微分方程控制的连续时空动力系统。将自注意力、前馈网络、残差连接和层归一化分别映射为非局部交互、局部反应和稳定机制。通过将标准Transformer与缺乏显式稳定器的PDE模拟器进行比较实验验证。

Result: 实验证明，缺乏残差连接会导致灾难性的表征漂移；而缺少层归一化则会导致不稳定、爆炸性的训练动态。

Conclusion: 残差连接和层归一化并非启发式技巧，而是驯服强大但本质不稳定的连续系统所需的根本数学稳定器。此工作为Transformer的设计提供了第一性原理的解释，并建立了通过连续动力学分析深度神经网络的新范式。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [200] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: SID是一个新型深度学习训练框架，通过解耦模块依赖实现并行训练，解决了BP的更新锁定和高内存消耗问题，并在性能上与BP相当或更优。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播(BP)存在更新锁定（模块需等待整个反向传播完成）和高内存消耗（需存储激活用于梯度计算）两大可扩展性瓶颈。

Method: 提出协同信息蒸馏(SID)框架，将深度网络构建为模块管道，每个模块通过局部目标（平衡目标保真度和与前序模块信念一致性）细化概率信念，从而解耦反向依赖实现并行训练。

Result: SID消除了更新锁定，大幅减少内存需求，保持标准前向推理，理论上保证性能随深度单调提升。经验上，分类精度与BP持平或超越，具有更优的可扩展性和对标签噪声的鲁棒性。

Conclusion: SID作为BP的通用替代方案，有效解决了BP的可扩展性限制，实现了高效并行训练和内存优化，并在性能和鲁棒性方面表现出色。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [201] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: Quant-dLLM是一个专门为扩散大语言模型（dLLMs）设计的2比特超低精度后训练量化（PTQ）框架，通过引入掩码校准模拟、数据感知任意顺序量化器和自适应块级混合精度，解决了现有PTQ方法在dLLMs上性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）作为自回归（AR）LLMs的替代方案正在兴起，但其模型规模持续增长，需要进行权重压缩以部署。然而，适用于AR LLMs的后训练量化（PTQ）方法直接应用于dLLMs的2比特量化时，性能不尽如人意。

Method: 本文提出了Quant-dLLM框架，包含三部分：1) 掩码校准模拟（MCS），以使校准与dLLMs的时间步依赖掩码生成对齐，获得更可靠的校准数据；2) 数据感知任意顺序量化器（DAQ），通过优化算法和模拟校准数据，学习超低比特权重表示；3) 自适应块级混合精度（ABMP），一种基于敏感度的精度分配方案，在严格的2比特预算下自适应分配通道组的比特宽度。

Result: 在2比特精度限制下，Quant-dLLM在dLLMs上持续取得了比当前最先进的（SOTA）AR-transfer PTQ方法更高的准确性。

Conclusion: Quant-dLLM是一个为dLLMs量身定制的超低比特PTQ框架，通过创新的校准和量化策略，有效解决了dLLMs在2比特量化下的性能挑战，实现了高效的模型压缩。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [202] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: 本研究提出了SDQ-LLM框架，利用Sigma-Delta量化实现1比特大语言模型，具有连续可调的过采样率（OSR）、哈达玛平滑和多层次OSR分配策略，从而在极端低比特量化下仍能保持高效和高精度性能，以应对LLM的计算和内存挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）面临巨大的计算和内存挑战，因此极低比特量化对其高效部署至关重要。

Method: SDQ-LLM框架采用Sigma-Delta量化器结合上采样技术，将LLM权重二值化或三值化（1比特或1.58比特），用加法代替线性层中的乘法操作，显著提高推理效率。它引入了连续可调的过采样率（OSR），以动态适应内存或VRAM限制，实现模型大小和精度的最佳权衡。为减少量化精度损失，量化前融入基于哈达玛的权重平滑。此外，针对量化敏感度与权重方差的相关性，提出了一种细粒度的、基于层和线性层内部分配的OSR策略（MultiOSR），根据权重方差和参数规模分布OSR。

Result: 在OPT和LLaMA模型家族上进行的广泛实验表明，即使在极具攻击性的低OSR设置下，SDQ-LLM也能实现更高效和高精度的性能。

Conclusion: SDQ-LLM框架通过创新的量化策略，成功解决了LLM在计算和内存方面的挑战，使其能够在极低比特量化下保持强大的语言推理能力和高效的部署性能。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [203] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 该论文提出一种轻量级二次增强器，通过引入二次变换到深度神经网络中以增加非线性，从而在图像分类、文本分类和大语言模型微调等任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络主要依赖线性变换和非线性激活函数。为进一步增加非线性并增强现有架构的性能，研究者探索引入二次变换。

Method: 提出一种轻量级二次增强器，利用低秩性、权重共享和稀疏化技术，在网络的每一层引入特征间的二次交互，同时仅增加可忽略不计的模型参数和前向计算量。

Result: 在图像分类、文本分类和大型语言模型微调三项概念验证实验中，所提出的方法均展现出明显且实质性的性能提升。

Conclusion: 通过引入轻量级的二次变换，可以有效提升深度神经网络的非线性能力，从而在多种任务中实现显著的性能增益。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [204] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 针对B-PINNs中物理约束对网络不确定性解释的影响不明问题，本文提出一种可扩展的拉普拉斯框架，通过分解后验Hessian来量化并分析各约束对损失函数地形的塑造作用。


<details>
  <summary>Details</summary>
Motivation: B-PINNs在不确定性下求解微分方程时，其不确定性和过度自信的解释需谨慎，因为物理约束对网络的影响尚不明确。过度自信可能源于约束带来的精确性而非误校准，因此需要进一步阐明单个物理约束如何塑造这些网络。

Method: 引入一个可扩展、无矩阵的拉普拉斯框架，该框架能够将后验Hessian分解为各物理约束的贡献，并提供指标来量化它们在损失地形上的相对影响。

Result: 将该方法应用于Van der Pol方程，结果显示物理约束如何塑造网络几何结构，并通过Hessian直接揭示了改变单一损失权重如何非平凡地重新分配曲率和各约束间的有效支配力。

Conclusion: 该方法成功揭示了B-PINNs中物理约束对网络几何和损失地形的影响机制及其复杂相互作用，为理解其不确定性提供了新的分析工具。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [205] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 本文分析了Mamba模型的长程记忆衰减机制，并提出了MemMamba架构，通过状态摘要和多层注意力机制，解决了长程遗忘问题，同时保持了线性复杂度，并在长序列任务上取得了显著改进和效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有长序列模型在效率和内存之间存在固有限制。Mamba等选择性状态空间模型虽然高效，但其长程记忆呈指数级衰减，急需理解其记忆机制并加以改进。

Method: 通过数学推导和信息论分析揭示Mamba的记忆衰减机制；引入水平-垂直记忆保真度指标量化信息损失；提出MemMamba架构，结合状态摘要机制、跨层和跨token注意力来缓解长程遗忘。

Result: MemMamba在PG19和Passkey Retrieval等长序列基准测试上显著优于现有Mamba变体和Transformers，推理效率提升48%。

Conclusion: MemMamba在复杂性-内存权衡上取得了突破，为超长序列建模提供了一个新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [206] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 本文提出了Quokka，这是首个针对扩散语言模型（DLMs）的系统性缩放定律，涵盖计算和数据受限场景，并研究了关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 旨在弥补扩散语言模型领域系统性缩放定律的空缺，为DLM训练提供实用指导，并为整个AI社区带来启发。

Method: 通过引入Quokka，系统性地研究了扩散语言模型在计算受限和数据受限两种情况下的缩放定律，并分析了关键的建模和优化设计。

Result: Quokka是Chinchilla的延伸，提供了更广泛的研究范围。

Conclusion: 研究结果有望为扩散语言模型训练提供短期实践指导，并为整个AI社区带来长期启发。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [207] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 本研究提出HAP（混合归因和剪枝）框架，结合归因修补和边剪枝，实现了更快、更忠实的语言模型电路发现，有效提升了可解释性研究的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型电路发现算法面临归因修补（快速但不忠实）和边剪枝（忠实但计算昂贵）之间的基本权衡。

Method: 提出混合归因和剪枝（HAP）框架。该方法首先使用归因修补识别高潜力子图，然后应用边剪枝从该子图中提取忠实电路。

Result: HAP比基线算法快46%，且不牺牲电路忠实性。在间接宾语识别任务的案例研究中，HAP能保留归因修补在高稀疏度下会剪枝的协作电路组件（例如S-抑制头）。

Conclusion: HAP是一种有效的方法，可以提高机械可解释性研究向更大模型的可扩展性。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [208] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个混合LLM系统，通过迭代级调度和智能内存管理，在边缘服务器上协同推理和微调，以平衡延迟、吞吐量和模型更新鲜度。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上的LLM在时延敏感应用中，因用户数据非平稳性需频繁重训练。这导致推理延迟与模型精度在受限GPU资源下产生矛盾。现有重训练策略无法有效解决此问题，尤其忽视了迭代级重训练粒度的重要性，而这对于在不违反服务水平目标下适应模型漂移至关重要。

Method: 本文提出MACE系统，一种混合LLM系统，协同执行并发推理（prefill, decode）和微调。MACE采用智能内存管理，最大化任务性能同时保证推理吞吐量。其核心在于识别并非所有模型更新对输出对齐影响相同，并据此分配GPU周期，以平衡吞吐量、延迟和更新鲜度，关键在于迭代级调度。

Result: MACE在跟踪驱动评估中，性能媲美或超越持续重训练，并将推理延迟降低高达63%，在资源受限下保持吞吐量。与周期性重训练相比，MACE改善了prefill、decode和微调阶段的延迟分解，并在NVIDIA AGX Orin上将GPU利用率维持在85%以上。

Conclusion: 迭代级混合调度是为边缘平台部署具备持续学习能力的LLM的一个有前景的方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [209] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: 该论文提出了Edge-FIT，一个可扩展的边缘联邦指令调优框架，通过结合联邦学习和4比特QLORA，解决了LLM在传统联邦学习中的通信和计算开销问题，并在Llama 2(7B)上实现了0.89的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）在面对大型语言模型（LLMs）的巨大参数量时会失效，导致高昂的通信和计算开销，阻碍了LLMs的去中心化部署。

Method: Edge-FIT框架将联邦学习与4比特量化低秩适配（QLORA）技术相结合，以减轻通信和计算开销。通过对用于IoT领域的Databricks Dolly 15k数据集进行过滤，并在Llama 2(7B)和Phi-3-mini模型上进行实验验证。

Result: 经过Edge-FIT调优的Llama 2(7B)模型实现了0.89的F1分数。同时，使用3.8B的Phi-3-mini模型也展示了可行的性能与资源权衡。

Conclusion: Edge-FIT是一个可扩展的框架，能够有效支持在家庭计算网关等边缘设备上进行LLM的去中心化部署，成功解决了现有联邦学习方法在LLM应用中的核心问题。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [210] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一种基于主动域适应的日志异常检测模型，结合迁移学习和主动学习，通过少量人工标注解决了冷启动和数据分布差异问题，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法过度依赖难以获取的标注数据。已有的迁移学习和主动学习方法又面临源目标系统数据分布差异和冷启动问题，限制了其有效性。

Method: LogAction整合了迁移学习和主动学习技术。一方面，利用成熟系统的标注数据训练基础模型以缓解主动学习的冷启动问题；另一方面，采用基于自由能和不确定性的采样策略选择位于分布边界的日志进行人工标注，以最小化标注工作量解决迁移学习中的数据分布差异。

Result: 在六种不同数据集组合上，LogAction仅需2%的人工标注即可达到平均93.01%的F1分数，并且比一些最先进的方法性能高出26.28%。

Conclusion: LogAction通过创新性地结合迁移学习和主动学习，有效克服了日志异常检测中人工标注成本高、冷启动和数据分布差异的挑战，以极低的标注投入实现了高精度检测。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [211] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 掩码扩散语言模型在并行生成和双向注意力方面存在固有困难，本文分析了其原因并提出了最优的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因支持并行生成和双向注意力而备受关注，但对于流行的掩码扩散模型，其能否真正实现这些优势以及面临的挑战尚不明确。

Method: 本文通过论证阐述了掩码扩散模型在实现并行生成和双向注意力方面的固有困难，并在此基础上提出了最有效的训练和推理策略。

Result: 研究揭示了掩码扩散模型在实现并行生成和双向注意力方面存在内在困难。同时，论文提出了针对掩码扩散模型的最优训练和推理策略。

Conclusion: 掩码扩散模型在利用扩散模型的并行生成和双向注意力优势时面临基础性挑战，但可以通过本文提出的优化训练和推理策略来提高其有效性。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [212] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文通过分析和优化Clifford卷积层的内部计算结构，显著提升了其推理速度，同时保持了正确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于物理科学中对具有等变性网络的兴趣日益增长，Clifford神经网络层作为一种实现E(n)和O(n)等变性的方法脱颖而出。然而，其推理过程的效率有待提高。

Method: 研究首先分析了Clifford代数的理论基础，以消除冗余的矩阵分配和计算。随后，系统性地应用了既定的优化技术来进一步提升性能。

Result: 实验结果显示，相对于基线实现，优化后的方法在十一个函数上实现了平均21.35倍的加速。在六个案例中，其运行时性能与原始PyTorch实现相当甚至更快。在其余案例中，性能与原始库处于同一数量级。

Conclusion: 通过对Clifford卷积层的结构分析和优化，本文成功地大幅提升了其推理速度，使其在实际应用中更具可行性。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [213] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: 针对大型语言模型（LLMs）的高昂计算和内存成本，本文提出UniPruning，一种统一的后训练剪枝框架。它结合局部度量的速度和全局协调的稳定性，通过基于镜像下降的优化，在不更新模型权重的情况下，高效地实现了LLMs的稀疏化，并在多种LLM家族上取得了竞争或优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs计算和内存成本高昂，而剪枝虽有潜力但现有方法存在局限：局部剪枝在稀疏度高时易崩溃，全局剪枝则需昂贵的权重更新或受限于结构。因此，需要一种既高效又鲁棒的统一剪枝方案。

Method: 提出UniPruning，一个统一的后训练剪枝框架。它结合了局部显著性度量的速度和全局协调的稳定性，通过基于镜像下降的优化实现，且无需更新模型权重。该方法利用快速的逐层评分和轻量级全局控制器来分配统一的稀疏度预算，支持非结构化和半结构化N:M剪枝，能一次性生成任意稀疏度的剪枝掩码，并适应硬件约束。

Result: 在多个预训练LLM家族和标准基准测试上，UniPruning一致地实现了有竞争力或更优的困惑度（perplexity）和零样本（zero-shot）准确性。消融研究也证实了镜像下降和局部显著性锚定的重要性。

Conclusion: UniPruning为大规模LLMs的稀疏化提供了一个高效、有原则且可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [214] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: MoE模型推理时因专家负载不均衡导致性能下降。本文提出LASER，一种即插即用的推理时路由算法，它能根据门控分数分布自适应地平衡专家负载，同时保持准确性，从而降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) 模型在推理时面临内存消耗大的问题，限制了每设备专家数量。更重要的是，条件路由导致专家负载不均衡（部分专家过载，部分未充分利用），当专家映射到GPU时，这直接导致系统性能下降，表现为高延迟、低吞吐量和高成本。

Method: 本文提出了LASER，一种即插即用的推理时路由算法，旨在平衡专家负载并保持准确性。LASER根据门控分数的分布形态进行自适应路由：当分数显示出明确偏好时，它路由到分数最高的专家；当分数分布更均匀时，它会扩大潜在专家的集合，并路由到其中负载最轻的专家。LASER仅依赖于训练模型的门控分数，无需重新训练或微调即可直接集成到现有MoE推理管道中。

Result: LASER在Mixtral-8x7B和DeepSeek-MoE-16b-chat模型上，以及ARC-Easy、ARC-Challenge、MMLU和GSM8K四个数据集上进行了评估。结果表明，LASER显著改善了负载均衡，从而降低了延迟并提高了吞吐量，同时对模型准确性的影响微不足道。

Conclusion: LASER成功解决了MoE模型推理时的负载不均衡问题，通过其自适应路由机制，在不牺牲模型准确性的前提下，显著提升了系统性能（降低延迟，提高吞吐量），并且具有易于集成的优势。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [215] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: 提出CAFL-L，一种基于拉格朗日对偶优化的联邦学习扩展，旨在显式处理设备资源约束，并通过动态调整超参数来优化资源使用。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦平均（FedAvg）未充分考虑边缘设备的资源限制（如能耗、通信、内存、散热），阻碍了其在资源受限环境下的实际部署。

Method: 采用拉格朗日对偶优化动态调整训练超参数（如冻结深度、本地步数、批量大小、通信压缩），并通过梯度累积的令牌预算保留机制来维持训练稳定性。

Result: 在字符级语言模型上，CAFL-L比标准FedAvg展现出更优的约束满足能力（内存使用减少20%，通信量减少95%），同时保持了有竞争力的验证性能。

Conclusion: CAFL-L能够有效解决资源受限边缘设备上的部署问题，通过满足严格的资源预算同时保持模型性能，使其具有实际应用价值。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [216] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 本文提出一种自适应集成框架，通过元学习、不确定性量化和特征重要性，动态结合XGBoost和神经网络，实现卓越的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在开发更智能、更灵活的机器学习系统，并提升预测性能和可解释性。

Method: 引入一种新颖的自适应集成框架，通过复杂的元学习，协同结合XGBoost和神经网络。该方法利用先进的不确定性量化技术和特征重要性整合来动态调度模型选择和组合。

Result: 实验结果表明，该方法在不同数据集上均展现出卓越的预测性能和增强的可解释性。

Conclusion: 该研究有助于开发更智能、更灵活的机器学习系统。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [217] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 现有的T2I模型概念擦除技术在新型架构中失效，且仅制造“遗忘”的假象，擦除本质上可逆。本文提出RevAm，一种基于强化学习的方法，能高效复活被“擦除”的概念，揭示了当前安全机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型中用于安全和版权的概念擦除技术在Flux等下一代架构上效果显著下降。研究发现这些方法并未真正遗忘，仅是偏离采样轨迹，使得擦除可逆。这促使需要区分表层安全与真正的概念移除。

Method: 提出RevAm（Revoking Amnesia），一个基于强化学习的轨迹优化框架。该框架通过动态引导去噪过程来复活被擦除的概念，且不修改模型权重。它将群相对策略优化（GRPO）应用于扩散模型，通过轨迹级奖励探索多样化的恢复轨迹，以克服现有方法的局部最优问题。

Result: RevAm实现了卓越的概念复活保真度，并将计算时间缩短了10倍。实验结果揭示了当前安全机制的关键漏洞。

Conclusion: 当前的概念擦除技术并未实现真正的遗忘，而仅是轨迹操纵造成的“遗忘”假象，其擦除本质上是可逆的。现有的安全机制存在严重漏洞，需要开发更鲁棒的、超越轨迹操纵的擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [218] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本文分析了机器学习在气候建模中的应用案例，旨在综合ML驱动的气候建模工作流设计模式，并提供一个确保科学ML严谨性和促进跨学科合作的框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在气候建模中被广泛应用于系统仿真加速、数据驱动参数推断等，以解决物理一致性、多尺度耦合等挑战。本研究的动机在于深入分析ML在气候建模中的设计选择和工作流结构，以综合其设计模式，进而提升科学ML的严谨性、透明度与可复现性，并降低跨学科合作的门槛。

Method: 本文通过分析气候建模中应用机器学习研究的一系列案例，重点关注设计选择和工作流结构。研究方法是综合ML驱动气候建模中的工作流设计模式，包括代理建模、ML参数化、概率编程、基于模拟的推理和物理信息迁移学习等，并阐明这些工作流如何基于物理知识、模拟数据并整合观测。

Result: 研究结果旨在综合ML驱动气候建模中的各类工作流设计模式，并提出一个框架。该框架旨在通过更透明的模型开发、批判性评估、知情适应和可复现性来确保科学机器学习的严谨性。

Conclusion: 本文旨在提供一个确保科学机器学习严谨性的框架，并通过促进透明的模型开发、批判性评估、知情适应和可复现性，降低数据科学与气候建模交叉领域进行跨学科合作的障碍。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [219] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 该研究提出一种基于轻量级对比桥接的方法，在不进行大规模预训练的情况下，通过冻结单模态编码器和投影头对齐化学与文本表示，实现药物发现中的计算高效对齐。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在药物发现中前景广阔，但现有方法依赖于繁重的预训练或大规模多模态语料，效率低下。

Method: 通过在冻结的单模态编码器上使用轻量级投影头（“薄型对比桥接”），将ECFP4分子指纹与生物医学句子嵌入对齐。方法利用ChEMBL的配对机制，通过双线性投影和对比目标训练。为处理共享治疗靶点的药物，引入了难负样本加权和边界损失。

Result: 在要求跨不相交化学核心泛化的骨架分割评估下，该方法实现了非平凡的跨模态对齐，并相对于冻结基线显著提高了靶点内部的鉴别能力。

Conclusion: 研究结果表明，薄型桥接提供了一种计算高效的替代方案，可替代大规模多模态预训练，能够实现骨架感知的药物-文本对齐和精准医疗中的靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [220] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: 该研究评估了LLM在运营管理中模拟人类行为的能力，发现LLM能复现大多数假设层面的效应但分布存在差异，且通过CoT提示和超参数调优可有效减少这种不一致。


<details>
  <summary>Details</summary>
Motivation: LLMs正成为模拟人类行为的新兴工具，为商业、经济和社会科学提供低成本的替代方案。本研究旨在评估LLMs在运营管理领域复现人类行为的准确性。

Method: 使用九个已发表的行为运营实验，通过假设检验结果的复现和基于Wasserstein距离的分布一致性来评估LLM。此外，还测试了思维链提示（chain-of-thought prompting）和超参数调优两种轻量级干预措施。

Result: LLMs能够复现大多数假设层面的效应，捕捉关键决策偏差，但其响应分布与人类数据存在差异，即使是强大的商业模型也不例外。思维链提示和超参数调优这两种干预措施能减少这种不一致，并能使小型或开源模型达到或超越大型系统的表现。

Conclusion: LLMs在运营管理中模拟人类行为方面具有潜力，能够捕捉决策偏差，但在数据分布一致性方面仍有不足。通过简单的提示工程和参数调优，可以显著改善LLM的表现，甚至使小型模型具备竞争力。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [221] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 引入数据质量参数Q，提出并验证了扩展Chinchilla框架的质量感知缩放定律，以指导大规模预训练。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型缩放定律未将数据质量形式化，导致无法有效指导数据管理和模型规模平衡。

Method: 引入无量纲数据质量参数Q，并将其整合至Chinchilla框架，提出预测损失与模型大小、数据量和数据质量联合相关的缩放定律。通过有效样本量和信息论视角支撑，并提供两种Q的实用估计器（损坏率代理和缺陷度量）。在神经机器翻译和自回归建模中通过合成实验验证。

Result: 损失与数据质量可预测地缩放，高质量数据能显著降低模型大小和计算需求。有效数据随质量呈亚线性衰减，且对中度数据损坏具有鲁棒性。定律的预测形式通过样本外评估得到验证。

Conclusion: 建立了显式且可泛化的数据质量缩放定律，为大规模预训练中平衡数据策展努力和模型规模提供了具体、普适的指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [222] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出一种基于神经网络的方法，用于快速重建正弦信号频率（10毫秒内）并自动分类物理扰动（地震事件准确率99-100%），优于传统傅里叶方法，适用于环形激光陀螺仪等地球物理应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如环形激光陀螺仪信号频率重建）需要数秒数据，延迟高，无法实现快速触发。同时需要识别信号中的物理扰动（如激光不稳定性、地震事件）。

Method: 采用神经网络方法实现正弦信号的频率重建；引入自动化分类框架识别信号中的物理扰动（如激光不稳定性、地震事件）。

Result: 神经网络能在约10毫秒内重建数百赫兹的频率，频率估计精度比标准傅里叶技术提高2倍。自动化分类框架在独立测试数据集上对地震类扰动识别准确率达99%至100%。

Conclusion: 本研究标志着人工智能在地球物理信号分析领域集成方面迈出重要一步。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [223] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 针对强化学习训练过程中的震荡和不稳定问题，本文提出CIC框架，通过选择性更新代表策略和自适应机制，有效提升算法稳定性及性能，且无额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 强化学习的训练过程常遭遇剧烈震荡，导致算法不稳定和性能下降。

Method: 提出“变动世界中的不变”（CIC）框架。该框架维护一个代表策略和一个当前策略，并仅在当前策略表现出优越性时才选择性地更新代表策略。此外，CIC采用自适应调整机制，使两个策略共同促进评论家（critic）的训练。

Result: 在五个MuJoCo环境下的评估结果显示，CIC在不增加额外计算成本的前提下，提升了传统算法的性能。

Conclusion: CIC框架通过增强算法稳定性，有效改善了强化学习的性能，并保持了计算效率。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [224] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate通过LLM从非结构化数据中提取语义上下文，解决了深度学习调度器缺乏上下文的问题，显著提升了GPU集群的调度性能，平均作业完成时间缩短高达1.91倍。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习调度器在GPU集群中缺乏对作业的语义上下文感知，导致其依赖有限元数据，从而引发高分析开销、不可靠的时长估算、不足的故障处理和糟糕的可观察性。

Method: 提出SchedMate框架，通过系统地从被忽视的非结构化数据源（源代码、运行时日志和历史作业）中提取深层洞察力，弥补了语义鸿沟。SchedMate通过三个基于LLM的组件，非侵入式地增强现有调度器。

Result: 在128个GPU的物理集群上进行的评估以及基于生产痕迹的广泛模拟表明，SchedMate将平均作业完成时间减少高达1.91倍。

Conclusion: 语义感知在现代深度学习调度中扮演着关键角色，SchedMate显著提升了调度性能，证明了其重要性。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [225] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 本文研究了点云扩散模型中旋转对齐步骤的理论基础和效果。我们发现对齐操作是小噪声下的零阶近似，并基于此推导出更优的近似器。实验表明，现有对齐方法在关键噪声水平下已“足够好”。


<details>
  <summary>Details</summary>
Motivation: 在训练用于点云（如分子、蛋白质）的扩散模型时，由于没有规范的朝向，通常会通过随机旋转进行数据增强，并在计算损失前对去噪预测进行旋转对齐。然而，这种对齐步骤的效果尚未得到充分研究。

Method: 我们理论上证明了最优去噪器可以表示为$SO(3)$上的矩阵Fisher分布。在此基础上，我们发现对齐操作对应于该分布的众数采样，并且是小噪声水平下的零阶近似，从而解释了其有效性。我们还基于此视角推导出了在小噪声极限下更优的近似器。

Result: 研究结果表明，对齐步骤是小噪声水平下的零阶近似，解释了其有效性。尽管提出了新的近似器，但实验突出显示，对于训练扩散模型最重要的噪声水平，现有的对齐方法通常已是一个“足够好”的近似。

Conclusion: 对齐步骤在点云扩散模型中是一种有效且在关键噪声水平下表现“足够好”的近似方法。其有效性可从矩阵Fisher分布和零阶近似的角度进行理论解释。尽管存在理论上更优的近似，但现有方法在实践中已能满足需求。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [226] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 本文通过理论分析和跨模态实证研究，深入探索了Transformer模型中池化操作的关键作用，揭示了其对模型表达能力和性能的影响，并为池化机制的选择与设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管池化操作对Transformer模型的行为和下游任务性能至关重要，但其作用在现有文献中仍未得到充分探索，研究重心大多集中在注意力机制上。

Method: 本文引入了一个理论框架，通过推导表示能力和区分相似输入的闭式边界，严格刻画了配备常用池化方法的Transformer模型的表达能力，并扩展到不同的注意力公式变体。同时，在计算机视觉、自然语言处理和时间序列分析三大模态中，经验性评估了需要全局和局部上下文理解的池化策略。

Result: 理论分析得到了模型表达能力和区分相似输入的闭式边界。实证结果揭示了池化选择如何一致地影响模型在不同任务中的准确性、敏感性和优化行为。

Conclusion: 本文统一了理论和实证视角，为特定任务选择或设计合适的池化机制提供了实用指导。研究将池化定位为Transformer模型的关键架构组件，为超越单纯注意力机制的更原则性模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [227] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 该研究开发了一个结合多目标强化学习和新型SDE模拟器的框架，用于设计和评估平衡疾病控制与社会经济稳定的流行病干预策略，并成功应用于COVID-19及其他病原体。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了在疾病遏制和社会经济稳定之间取得平衡的干预策略的迫切需求。

Method: 研究设计了一个建模和评估疾病传播预防策略的框架，该框架结合了多目标强化学习（MORL）与一个新的随机微分方程（SDE）大流行模拟器。该模拟器已根据全球COVID-19数据进行校准和验证，并在此基础上训练了一个Pareto-Conditioned Network (PCN) 代理。

Result: 研究展示了COVID-19流行病学控制与经济稳定之间的直接政策权衡。此外，该框架的通用性通过将其扩展到脊髓灰质炎和流感等不同流行病学特征的病原体得到证实，并揭示了不同的干预政策。模型还应用于麻疹爆发，量化了疫苗接种覆盖率下降5%对所需干预措施严格性和成本的影响。

Conclusion: 这项工作提供了一个稳健且适应性强的框架，可支持透明、基于证据的政策制定，以缓解公共卫生危机。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [228] [Scalable Ground Station Selection for Large LEO Constellations](https://arxiv.org/abs/2510.03438)
*Grace Ra Kim,Duncan Eddy,Vedant Srinivas,Mykel J. Kochenderfer*

Main category: cs.NI

TL;DR: 本文提出一个可扩展的分层框架，用于低地球轨道（LEO）卫星星座的地面站选择，以解决传统方法在大规模问题上难以处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 有效的地面站选择对于LEO卫星星座至关重要，能最小化运营成本、最大化数据下行量并减少通信空白。然而，传统基于现有基础设施和混合整数规划的方法，在大规模、多供应商和大型卫星星座情境下，很快变得难以找到全局最优解。

Method: 引入了一个可扩展的分层框架。该框架将全球选择问题分解为单颗卫星、短时间窗口的子问题。从每个子问题中选择的最佳站点进行聚类，以识别所有分解案例中持续高价值的位置。然后将聚类级集合与最接近的GSaaS候选站点进行匹配，从而产生一个全局可行的解决方案。

Result: 在合成Walker-Star测试案例（1-10颗卫星，1-10个地面站）中，该方法实现了接近全局IP最优解95%的性能。在Capella Space（5颗卫星）、ICEYE（40颗）和Planet's Flock（96颗）的真实世界评估中，虽然精确IP解决方案无法扩展，但该框架仍能提供高质量的站点选择。

Conclusion: 该方法实现了可扩展的协调，同时保持了接近最优的性能，有效解决了大规模LEO卫星地面站选择问题中的可扩展性挑战。

Abstract: Effective ground station selection is critical for low Earth orbiting (LEO)
satellite constellations to minimize operational costs, maximize data downlink
volume, and reduce communication gaps between access windows. Traditional
ground station selection typically begins by choosing from a fixed set of
locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps
reduce the problem scope to optimizing locations over existing infrastructure.
However, finding a globally optimal solution for stations using existing
mixed-integer programming methods quickly becomes intractable at scale,
especially when considering multiple providers and large satellite
constellations. To address this issue, we introduce a scalable, hierarchical
framework that decomposes the global selection problem into single-satellite,
short time-window subproblems. Optimal station choices from each subproblem are
clustered to identify consistently high-value locations across all decomposed
cases. Cluster-level sets are then matched back to the closest GSaaS candidate
sites to produce a globally feasible solution. This approach enables scalable
coordination while maintaining near-optimal performance. We evaluate our
method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10
stations), achieving solutions within 95% of the global IP optimum for all test
cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and
Planet's Flock (96) show that while exact IP solutions fail to scale, our
framework continues to deliver high-quality site selections.

</details>


### [229] [Short-circuiting Rings for Low-Latency AllReduce](https://arxiv.org/abs/2510.03491)
*Sarah-Michelle Hammer,Stefan Schmid,Rachee Singh,Vamsi Addanki*

Main category: cs.NI

TL;DR: 本文挑战了AllReduce中Ring算法只适用于大消息、Recursive Doubling适用于小消息的普遍认知。研究发现，在考虑实际延迟和拥塞后，Ring在环形GPU拓扑中对短消息也可能最优。作者提出了利用新兴光互连的自适应拓扑，通过电路交换启发式算法使Recursive Doubling超越Ring的性能。


<details>
  <summary>Details</summary>
Motivation: 分布式ML和HPC应用中高效的集体通信至关重要。传统的观点认为，Ring算法仅对大消息最优，而Recursive Doubling因其对数级步数而对小消息更优。本文旨在挑战这一长期假设，探究在真实传播延迟和链路容量限制下，Ring算法在环形GPU拓扑中对短消息是否依然能保持最优，并受此启发探索自适应拓扑（特别是光互连）的可能性。

Method: 通过分析Ring和Recursive Doubling算法在考虑实际传播延迟和链路容量限制下的性能，比较两者在环形GPU-to-GPU拓扑中的总传播延迟和拥塞程度（基于跳数）。设计了一个“简单快速”的电路交换启发式算法，使Recursive Doubling能利用动态可重构的光子路径，平衡重构延迟、传播延迟和链路拥塞以最小化完成时间。进行了初步评估，使用真实的重构延迟来比较其电路交换方案与静态环形拓扑上的Ring AllReduce。

Result: 研究发现，在考虑实际传播延迟和链路容量限制后，Ring算法在环形GPU-to-GPU拓扑中对短消息也能保持最优。Ring和Recursive Doubling的总传播延迟基本相同，但后者因跳数更多而导致更高的拥塞，增加了完成时间。所提出的电路交换调度（用于Recursive Doubling在可重构光子路径上）能够实现更快的完成时间，甚至优于静态环形拓扑上的Ring AllReduce。

Conclusion: 在考虑实际网络限制时，Ring算法在静态环形拓扑中即使对短消息也可能最优。通过利用新兴的光子互连和动态重构，自适应拓扑中的Recursive Doubling算法能够克服静态拓扑的限制，实现更优的性能。未来的工作将集中于解决实现实用的集体内部光子交换所面临的关键挑战。

Abstract: Efficient collective communication is critical for many distributed ML and
HPC applications. In this context, it is widely believed that the Ring
algorithm for the AllReduce collective communication operation is optimal only
for large messages, while Recursive Doubling is preferable for small ones due
to its logarithmic number of steps compared to the linear number for Ring. In
this paper, we challenge this long-held assumption and show that the Ring
algorithm can remain optimal even for short messages in ring-based GPU-to-GPU
topologies, once realistic propagation delays and link capacity constraints are
accounted for. We find that the total propagation delay for both Ring and
Recursive Doubling essentially sums to the same value, but the latter incurs
significantly higher congestion due to longer hop counts, leading to increased
completion times. This surprising result motivates our case for in-collective
adaptive topologies, particularly in the context of emerging photonic
interconnects, which can break through the limitations of static topology
designs at the collective communication granularity. We design a \emph{simple
and fast} heuristic for circuit-switching that enables Recursive Doubling to
exploit dynamically reconfigurable photonic paths, carefully balancing
reconfiguration delays, propagation latencies, and link congestion to minimize
overall completion time. Our preliminary evaluations, using realistic
reconfiguration delays, show that our circuit-switching schedules enable faster
completion times for Recursive Doubling, even compared to Ring AllReduce on
static ring topologies. We conclude by highlighting key challenges and future
research directions for realizing practical, in-collective photonic switching.

</details>


### [230] [A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem](https://arxiv.org/abs/2510.03524)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 针对物联网能耗和网络寿命问题，本文提出一种基于多准则的簇头选择与平衡树分层数据传输方法，以提升数据传输效率并延长网络寿命。


<details>
  <summary>Details</summary>
Motivation: 物联网设备能源有限且电池不可更换，导致网络寿命短。因此，降低能耗、加速数据传输并缩短响应时间是物联网网络中的重要挑战，特别需要优化簇头选择过程以减少数据传输延迟。

Method: 簇头节点根据距离、剩余能量、接收信号强度和链路过期时间等多个重要准则进行选择。随后，对象通过平衡树以分层方式将处理过的数据发送到服务器。

Result: 仿真结果表明，所提出的方法在数据包投递率、延迟、响应时间和网络寿命方面均优于能量高效质心路由协议（EECRP）和基于全局信息决策的应急响应物联网（ERGID）。

Conclusion: 本文提出的基于多准则的簇头选择和平衡树分层数据传输方法，能有效提高物联网网络的性能，包括提升数据包投递率、降低延迟和响应时间，并延长网络寿命。

Abstract: Fog computing integrates cloud and edge resources. According to an
intelligent and decentralized method, this technology processes data generated
by IoT sensors to seamlessly integrate physical and cyber environments.
Internet of Things uses wireless and smart objects. They communicate with each
other, monitor the environment, collect information, and respond to user
requests. These objects have limited energy resources since they use batteries
to supply energy. Also, they cannot replace their batteries. As a result, the
network lifetime is limited and short. Thus, reducing energy consumption and
accelerating the data transmission process are very important challenges in IoT
networks to reduce the response time. In the data transmission process,
selecting an appropriate cluster head node is very important because it can
reduce the delay when sending data to the fog. In this paper, cluster head
nodes are selected based on several important criteria such as distance,
residual energy, received signal strength, and link expiration time. Then,
objects send the processed data to the server hierarchically through a balanced
tree. The simulation results show that the proposed method outperforms the
energy-efficient centroid-based routing protocol (EECRP) and the Emergency
Response IoT based on Global Information Decision (ERGID) in terms of packet
delivery rate, delay, response time, and network lifetime.

</details>


### [231] [An efficient grey theory-driven path selection for energy efficiency control in the Internet of Things using fog and cloud computing](https://arxiv.org/abs/2510.03533)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 提出MFCT-IoT重叠聚类方法，优化物联网雾计算环境中的数据传输，降低延迟并提高网络性能，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 物联网大数据交换中，基于云计算存在高延迟等挑战，影响网络性能。雾计算作为一种靠近对象处理数据的方案，有望解决这一问题，从而提升网络性能。

Method: 提出一种名为MFCT-IoT的重叠聚类方法，用于选择最佳簇头节点，以确保物联网中从对象到雾节点的快速数据传输。选定的簇头负责将数据发送到最近的雾节点进行处理；若无法立即响应，则合并并传输数据至云服务器（作为分层树的根节点）。该方法与ERGID和EECRP两种方案进行比较，评估指标包括响应时间、数据包投递率、端到端延迟、网络生命周期和能耗。

Result: 实验结果表明，所提出的MFCT-IoT方法在响应时间、数据包投递率、端到端延迟、网络生命周期和能耗等所有评估指标上均优于ERGID和EECRP方案。

Conclusion: MFCT-IoT方法通过优化簇头选择和数据传输机制，在物联网雾计算环境中有效降低了网络延迟，提高了数据传输效率和整体网络性能。

Abstract: Due to the big data exchange on the Internet of Things, proper routing and
selecting the best routes for fast data transmission improve network
performance. There are major challenges, like high delay, when cloud computing
is used. Therefore, one solution is to use other schemes, such as fog
computing. In fog computing, all data is not sent to the cloud and the fog
nodes close to objects are used for data processing. This reduces the network
delay. In this paper, we propose an overlapping clustering method called
MFCT-IoT to select the best cluster head nodes to guarantee the fast data
transfer from objects to fog nodes. The selected cluster head nodes are
responsible for sending the collected data to the closest fog nodes in the
network edge. Upon receiving the data, the fog nodes process it, and if a
response is ready, they respond immediately to the object. Otherwise, they
merge and transmit the data to the cloud servers, which are considered as the
root node of the proposed hierarchical tree. After processing, the merged data
is sent to the object. We compare the proposed scheme with two schemes,
including ERGID and EECRP. These schemes are evaluated based on various
criteria, including the response time, packet delivery ratio, end-to-end delay,
network lifetime, and energy consumption. The results indicate that the
proposed method outperforms others in terms of all criteria.

</details>


### [232] [A Position- and Energy-Aware Routing Strategy for Subterranean LoRa Mesh Networks](https://arxiv.org/abs/2510.03714)
*Nalith Udugampola,Xiaoyu Ai,Binghao Li,Henry Gong,Aruna Seneviratne*

Main category: cs.NI

TL;DR: 本文提出一种针对地下LoRa网状网络的，基于位置和能量感知的路由策略，显著提高了最大吞吐量并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: LoRa网状网络路由面临低数据速率和ALOHA MAC的挑战。现有方法（如改编传统协议或基于洪泛）由于控制开销、确认和冗余重传，在低数据速率网络中低效利用有限带宽。因此，需要设计一种能提高吞吐量、能效并保持高数据包投递率的地下LoRa网状网络路由方案。

Method: 引入一种新型的、基于位置和能量感知的路由策略。该机制包括一个轻量级的位置学习阶段，中继器在此阶段确定相对位置并收集路由信息。随后，网络进入自适应路由阶段，利用备用LoRa中继器从数据包冲突和丢失中恢复，并采用能量感知路由切换来平衡中继器之间的电池消耗。

Result: 在代表性地下网络上的仿真结果显示，与先前优化的高流量洪泛方法相比，最大吞吐量提高了185%，能耗降低了75%，同时保持了高数据包投递率。

Conclusion: 所提出的位置和能量感知路由策略能够显著提高地下LoRa网状网络的吞吐量和能效，为该类网络提供了一种更优的路由解决方案。

Abstract: Although LoRa is predominantly employed with the single-hop LoRaWAN protocol,
recent advancements have extended its application to multi-hop mesh topologies.
Designing efficient routing for LoRa mesh networks remains challenging due to
LoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional
protocols for low-traffic, aboveground networks with strict duty cycle
constraints or uses flooding-based methods in subterranean environments.
However, these approaches inefficiently utilize the limited available network
bandwidth in these low-data-rate networks due to excessive control overhead,
acknowledgments, and redundant retransmissions. In this paper, we introduce a
novel position- and energy-aware routing strategy tailored for subterranean
LoRa mesh networks aimed at enhancing maximum throughput and power efficiency
while also maintaining high packet delivery ratios. Our mechanism begins with a
lightweight position learning phase, during which LoRa repeaters ascertain
their relative positions and gather routing information. Afterwards, the
network becomes fully operational with adaptive routing, leveraging standby
LoRa repeaters for recovery from packet collisions and losses, and energy-aware
route switching to balance battery depletion across repeaters. The simulation
results on a representative subterranean network demonstrate a 185% increase in
maximum throughput and a 75% reduction in energy consumption compared to a
previously optimized flooding-based approach for high traffic.

</details>


### [233] [6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection](https://arxiv.org/abs/2510.03807)
*Vaskar Chakma,Wooyeol Choi*

Main category: cs.NI

TL;DR: 本研究开发了一个6G赋能的数字孪生框架，通过太赫兹通信和边缘AI，实现了0.8ms超低延迟和97.7%高精度轴承故障检测，显著优于5G和WiFi-6网络。


<details>
  <summary>Details</summary>
Motivation: 当前集成了数字孪生技术的赛博物理系统(CPS)在任务关键型工业应用中面临实时性能的严峻限制。现有5G系统延迟超过10ms，无法满足自主工业控制和预测性维护等需要亚毫秒级响应时间的应用需求。

Method: 本研究提出了一个五层架构的6G赋能数字孪生框架，该框架集成了太赫兹通信(0.1-1 THz)、智能反射面和边缘人工智能。通过使用Case Western Reserve University (CWRU)轴承数据集进行实验验证，采用了15个时频域特征提取和随机森林分类算法。系统性能与传统WiFi-6和5G网络在分类准确率、端到端延迟和可扩展性等多个指标上进行了对比评估。

Result: 该框架实现了97.7%的故障分类准确率和0.8ms的端到端延迟。与WiFi-6（12.5ms）相比，延迟性能提升了15.6倍；与5G（4.2ms）相比，提升了5.25倍。系统展示出卓越的可扩展性（处理时间呈亚线性增长），并在四种轴承故障类别（正常、内圈、外圈和滚珠故障）上均保持了超过97%的宏平均F1得分。

Conclusion: 该6G赋能的数字孪生框架成功解决了工业应用中超低延迟通信和实时同步的挑战，为任务关键型工业控制和预测性维护提供了高性能、高可靠性和高可扩展性的解决方案。

Abstract: Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)
technology face critical limitations in achieving real-time performance for
mission-critical industrial applications. Existing 5G-enabled systems suffer
from latencies exceeding 10ms, which are inadequate for applications requiring
sub-millisecond response times, such as autonomous industrial control and
predictive maintenance. This research aims to develop and validate a 6G-enabled
Digital Twin framework that achieves ultra-low latency communication and
real-time synchronization between physical industrial assets and their digital
counterparts, specifically targeting bearing fault detection as a critical
industrial use case. The proposed framework integrates terahertz communications
(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence
within a five-layer architecture. Experimental validation was conducted using
the Case Western Reserve University (CWRU) bearing dataset, implementing
comprehensive feature extraction (15 time and frequency domain features) and
Random Forest classification algorithms. The system performance was evaluated
against traditional WiFi-6 and 5G networks across multiple metrics, including
classification accuracy, end-to-end latency, and scalability. It achieved 97.7%
fault classification accuracy with 0.8ms end-to-end latency, representing a
15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)
networks. The system demonstrated superior scalability with sub-linear
processing time growth and maintained consistent performance across four
bearing fault categories (normal, inner race, outer race, and ball faults) with
macro-averaged F1-scores exceeding 97%.

</details>


### [234] [A4FN: an Agentic AI Architecture for Autonomous Flying Networks](https://arxiv.org/abs/2510.03829)
*André Coelho,Pedro Ribeiro,Helder Fontes,Rui Campos*

Main category: cs.NI

TL;DR: 本文提出了A4FN，一个基于生成式AI和LLM的智能体AI架构，用于无人机飞行网络中实现意图驱动的自动化和实时网络控制。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应等任务关键、基础设施受限的场景中，需要为飞行网络提供意图驱动的自动化、实时上下文感知网络控制，以及适应性重配置和动态资源管理能力。

Method: A4FN通过一个分布式智能体系统实现，利用生成式AI和大型语言模型（LLM）。它包含两个核心组件：感知智能体（PA），负责语义解释多模态输入（图像、音频、遥测数据）以推导服务等级规范（SLS）；决策-行动智能体（DAA），根据推断的意图重新配置网络。该架构具备智能体AI的自主性、目标驱动推理和持续感知-行动循环等特性。

Result: A4FN架构能够支持自适应重配置、动态资源管理，并与新兴无线技术实现互操作性，适用于任务关键型和基础设施受限场景。

Conclusion: 论文详细阐述了A4FN架构及其核心创新，并指出了多智能体协调和智能体AI在下一代飞行网络中集成等开放研究挑战。

Abstract: This position paper presents A4FN, an Agentic Artificial Intelligence (AI)
architecture for intent-driven automation in Flying Networks (FNs) using
Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI
and Large Language Models (LLMs) to enable real-time, context-aware network
control via a distributed agentic system. It comprises two components: the
Perception Agent (PA), which semantically interprets multimodal input --
including imagery, audio, and telemetry data -- from UAV-mounted sensors to
derive Service Level Specifications (SLSs); and the Decision-and-Action Agent
(DAA), which reconfigures the network based on inferred intents. A4FN embodies
key properties of Agentic AI, including autonomy, goal-driven reasoning, and
continuous perception-action cycles. Designed for mission-critical,
infrastructure-limited scenarios such as disaster response, it supports
adaptive reconfiguration, dynamic resource management, and interoperability
with emerging wireless technologies. The paper details the A4FN architecture,
its core innovations, and open research challenges in multi-agent coordination
and Agentic AI integration in next-generation FNs.

</details>


### [235] [Analysis of LTE/5G Network Performance Parameters in Smartphone Use Cases: A Study of Packet Loss, Delay, and Slice Types](https://arxiv.org/abs/2510.04035)
*Almamoon Alauthman,Abeer Al-Hyari*

Main category: cs.NI

TL;DR: 该论文使用元启发式算法优化LTE和5G网络中的丢包和延迟。结果显示WOA表现最佳，其次是PSO和ABC，强调了根据网络切片选择算法的重要性。


<details>
  <summary>Details</summary>
Motivation: 为提升智能手机用户体验，需要优化LTE和5G网络关键路径中的两个重要性能参数：丢包和延迟。

Method: 研究了包括WOA、PSO和ABC在内的九种元启发式算法在eMBB、URLLC和mMTC等多种网络切片中优化网络性能的有效性。

Result: WOA表现最佳，丢包率降低31%，延迟减少6.3毫秒；PSO紧随其后，丢包率降低30%，延迟减少6.1毫秒；ABC在多数场景，特别是mMTC中，丢包率降低29%，延迟减少6毫秒。

Conclusion: 根据目标网络切片选择合适的优化算法对于提升资源利用率和网络效率至关重要。本研究提供了一个量化框架，并鼓励未来在混合优化技术和实时适应机制方面进行更多研究。

Abstract: The paper addresses optimizing two of the most important performance
parameters, packet loss, and delay, in the critical path optimization of LTE
and 5G networks using metaheuristic algorithms to play a vital role in the
smartphone user experience. In this context, nine metaheuristic algorithms,
such as WOA, PSO, and ABC, have been studied for their effectiveness in various
slices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that
WOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO
followed closely with a 30% packet loss reduction with a decrease of 6.1 ms in
delay. In most scenarios, ABC accomplished good results with a packet loss
reduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results
emphasize how selecting appropriate algorithms based on the intended network
slice is crucial for optimizing resource utilization and network efficiency. It
provides a quantitative framework for assessing and improving the reliability
and responsiveness of an LTE/5G network. It encourages more research in hybrid
optimization techniques and real-time adaptation mechanisms for further
improvements

</details>


### [236] [The Door to Policy Portability might be an IP Overlay](https://arxiv.org/abs/2510.04052)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 提出一种将L3网络策略集成到服务网格的方法，通过叠加网络实现基础设施无关的L3-L7策略强制执行，并具有低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的服务网格在L4-L7策略执行方面具备可移植性，但在L3网络策略上仍依赖于基础设施。在异构环境中一致地执行L3策略具有挑战性，且许多基础设施（如Kubernetes集群）缺乏L3策略强制执行，导致数据平面流量的安全风险。

Method: 提出将网络策略强制执行与服务网格集成，以实现可移植、基础设施无关的数据平面流量保护。具体方法是构建一个叠加L3网络，通过特定的策略强制点路由流量并利用授权密钥来执行L3策略，从而实现L3到L7的集成策略定义和执行。

Result: 在Kubernetes和Istio上进行了原型验证，结果显示该方法仅增加不到1毫秒的延迟，并且能够实现与Kubernetes原生网络策略相当的复杂策略。该方法还可用于服务环境之外，对终端用户流量强制执行策略，提供端到端的安全扩展叠加。

Conclusion: 通过在服务网格中集成可移植的叠加L3网络策略强制执行，能够实现跨任何基础设施的集成L3-L7策略定义和一致执行，有效保护数据平面流量，且引入的性能开销极低。

Abstract: Portable service mesh implementations enable layer 4 to layer 7 policy
enforcement across diverse infrastructures, but they remain tied to
infrastructure-specific layer 3 network policies. Network policies enable
control over IP traffic flow regardless of whether traffic is authorized at the
application level. However, not all infrastructure supports enforcing them, and
achieving consistent enforcement across heterogeneous environments is
challenging. For example, studies have shown that the majority of Kubernetes
clusters do not enforce any network policies. We propose integrating network
policy enforcement with service meshes to protect data-plane traffic in a
portable, infrastructure-agnostic way. This enables developers to define
integrated layer 3 to layer 7 policies and ensure they are enforced across any
infrastructure. Additionally, due to its portability, our approach can be used
outside the service environment to enforce policies on end-user traffic and
provide an end-to-end secure extended overlay. Our solution builds an overlay
layer 3 network and enforces layer 3 policies by routing traffic through
specific policy enforcement points and utilizing authorization keys. We
prototyped our idea using Kubernetes and Istio, and show that while it adds
less than 1ms latency, it can implement complex policies comparable to
Kubernetes native network policies.

</details>


### [237] [Dynamic Adaptive Federated Learning for mmWave Sector Selection](https://arxiv.org/abs/2510.04183)
*Lucas Pacheco,Torsten Braun,Kaushik Chowdhury,Denis Rosário,Batool Salehi,Eduardo Cerqueira*

Main category: cs.NI

TL;DR: 本文提出eDAFL，一种动态分层和基于聚类的联邦学习算法，用于自动驾驶车辆毫米波束扇区选择，显著提高了模型精度、推理速度并减小了模型大小。


<details>
  <summary>Details</summary>
Motivation: 传统的毫米波束扇区选择方案需要大量搜索，导致高延迟和通信开销，影响自动驾驶车辆网络效率。

Method: 提出eDAFL（增强型动态自适应联邦学习）算法。该算法动态选择机器学习模型中最重要的层进行聚合，以减少网络开销和故障风险，并结合集群内和集群间方法来减少过拟合并提高抽象级别。

Result: 在真实多模态数据集上，eDAFL相比现有方法，模型精度提升约6.76%，推理时间减少84.04%，模型大小缩小高达52.20%。

Conclusion: eDAFL算法通过动态分层和聚类联邦学习，有效解决了自动驾驶车辆毫米波束扇区选择的效率问题，显著提升了模型性能、推理速度并降低了资源消耗。

Abstract: Beamforming techniques use massive antenna arrays to formulate narrow
Line-of-Sight signal sectors to address the increased signal attenuation in
millimeter Wave (mmWave). However, traditional sector selection schemes involve
extensive searches for the highest signal-strength sector, introducing extra
latency and communication overhead. This paper introduces a dynamic layer-wise
and clustering-based federated learning (FL) algorithm for beam sector
selection in autonomous vehicle networks called enhanced Dynamic Adaptive FL
(eDAFL). The algorithm detects and selects the most important layers of a
machine learning model for aggregation in the FL process, significantly
reducing network overhead and failure risks. eDAFL also considers intra-cluster
and inter-cluster approaches to reduce overfitting and increase the abstraction
level. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating
improved model accuracy by approximately 6.76% compared to existing methods,
while reducing inference time by 84.04% and model size by up to 52.20%.

</details>


### [238] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 提出一种环境感知的LoRaWAN室内路径损耗框架，通过引入环境协变量和多项式模型，显著提高传播预测精度和可靠性，适用于室内物联网规划。


<details>
  <summary>Details</summary>
Motivation: 传统的对数距离模型和对数正态阴影衰落假设难以准确描述室内LoRaWAN传播，因为其受到结构和时变环境因素的挑战。

Method: 通过在240平方米的八楼办公室进行为期12个月的测量活动，并使用防泄漏交叉验证评估框架。该框架在对数距离多墙均值模型中加入了环境协变量（相对湿度、温度、二氧化碳、颗粒物、气压）和信噪比。比较了多种线性回归、贝叶斯线性回归和选择性二阶多项式模型。使用核密度估计和非参数族（如高斯混合）对阴影衰落进行剖析，并通过移动块自举量化不确定性，在保留集上进行验证。

Result: 多项式均值模型将交叉验证的RMSE从8.07 dB降至7.09 dB，R^2从0.81提高到0.86。折叠外残差为非高斯分布，一个三成分混合模型能捕获其尖锐核心和轻宽尾部。在99%的数据包传输率下，环境感知多项式模型所需的衰落裕度为25.7 dB，而线性基线模型为27.7至27.9 dB。

Conclusion: 该研究提供了一个可解释、部署就绪且具有校准可靠性控制的工作流，适用于室内物联网规划，并符合6G目标，通过预测精度转化为可靠性。

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


### [239] [Rethinking HTTP API Rate Limiting: A Client-Side Approach](https://arxiv.org/abs/2510.04516)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 本文提出两种无需中心控制的自适应客户端重试机制ATB和AATB，通过推断系统拥塞来安排HTTP API重试，相较于指数退避，可将HTTP 429错误减少高达97.3%。


<details>
  <summary>Details</summary>
Motivation: 现代互联网服务中的HTTP API流量受到配额限制，当共享配额超限时，客户端会被限流并重试。服务器端控制在共享配额下效率低下，因客户端无法感知其他负载；而普遍采用的指数退避等简单客户端策略会导致过度重试和显著成本。集中协调虽能解决，但有实际局限性。

Method: 设计了无需中心控制、仅依赖最少反馈的自适应客户端机制。提出了两种算法：ATB（通过Service Worker部署的离线方法）和AATB（利用聚合遥测数据增强重试行为）。两种算法均通过推断系统拥塞来调度重试。通过对真实世界流量和最多100个客户端的合成数据集进行仿真来评估。

Result: 与指数退避相比，所提出的算法将HTTP 429错误减少了高达97.3%。完成时间略有增加，但错误率的显著降低弥补了这一增长。

Conclusion: 自适应客户端重试机制（ATB和AATB）能够有效管理共享HTTP API配额下的流量，大幅减少429错误，并在较低的完成时间开销下优于简单策略。

Abstract: HTTP underpins modern Internet services, and providers enforce quotas to
regulate HTTP API traffic for scalability and reliability. When requests exceed
quotas, clients are throttled and must retry. Server-side enforcement protects
the service. However, when independent clients' usage counts toward a shared
quota, server-only controls are inefficient; clients lack visibility into
others' load, causing their retry attempts to potentially fail. Indeed, retry
timing is important since each attempt incurs costs and yields no benefit
unless admitted. While centralized coordination could address this, practical
limitations have led to widespread adoption of simple client-side strategies
like exponential backoff. As we show, these simple strategies cause excessive
retries and significant costs. We design adaptive client-side mechanisms
requiring no central control, relying only on minimal feedback. We present two
algorithms: ATB, an offline method deployable via service workers, and AATB,
which enhances retry behavior using aggregated telemetry data. Both algorithms
infer system congestion to schedule retries. Through emulations with real-world
traces and synthetic datasets with up to 100 clients, we demonstrate that our
algorithms reduce HTTP 429 errors by up to 97.3% compared to exponential
backoff, while the modest increase in completion time is outweighed by the
reduction in errors.

</details>


### [240] [Impossible Cloud Network: A Decentralized Internet Infrastructure Layer](https://arxiv.org/abs/2510.04620)
*Siu Kei Chung,Francisco Carpio,Andrei Navoichyk,Siarhei Valasovich,Jordan Moore,Slobodan Sudaric-Hefner,Daniel Baker,Thomas Demoor,Maurizio Binello,Christian Kaul,Kai Wawrzinek*

Main category: cs.NI

TL;DR: 互联网面临中心化危机，Web3方案有局限性。ICN提出多层去中心化基础设施，旨在解决数字主权和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 互联网权力集中在少数超大规模公司，导致中心化、用户控制权丧失、审查风险和单点故障。现有Web3方案难以平衡可扩展性、去中心化和安全性，并依赖中心化基础设施。

Method: 提出“不可能云网络（ICN）”，通过创建多层去中心化基础设施层解决问题，包括可组合服务层、企业级硬件资源层和用于性能保障的HyperNode网络，战略性地解耦并去中心化各层。

Result: ICN旨在提供开放、高度可扩展的基础设施，确保数字主权，消除信任单点，实现服务可编程性，并提供解耦架构以应对未来互联网的无限可能。

Conclusion: ICN通过其多层、解耦和去中心化架构，为解决当前互联网的中心化危机和Web3方案的局限性提供了一个全面的解决方案，旨在构建一个更开放、主权和可扩展的未来互联网。

Abstract: The internet faces a sovereignty crisis due to power concentration and data
growth among a few hyperscalers, leading to centralization and loss of user
control. This consolidation risks censorship and creates single points of
failure. While Web3 offers decentralized solutions, they often sacrifice either
scalability, decentralization, or security, which are key elements in the
blockchain trilemma. These solutions also struggle with limited access to
enterprise-grade hardware and frequently rely on centralized infrastructure.
The Impossible Cloud Network (ICN) addresses these issues by creating a
multi-tiered, decentralized infrastructure layer. ICN offers a composable
service layer, an enterprise-grade hardware resource layer, and a transparent,
permissionless HyperNode network for performance enforcement. By strategically
decoupling and decentralizing each layer, ICN aims to provide an open,
extensively scalable infrastructure that ensures digital sovereignty,
eliminates single points of trust, enables service programmability, and offers
a decoupled architecture for limitless possibilities in the future internet.

</details>


### [241] [Satellite Direct-to-Device from Low Earth Orbit: Techno-Economic Analysis of a Global Non-Terrestrial Network](https://arxiv.org/abs/2510.04651)
*Adnan Aijaz,Peizheng Li,Sajida Gufran*

Main category: cs.NI

TL;DR: 本文提出了一个综合技术经济分析框架，评估LEO卫星直连设备（D2D）服务的可行性、成本效益和盈利能力。结果显示其每月用户成本与地面服务相当，投资回报率为正，并强调了Open RAN技术在实现成本效益方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LEO卫星直连设备（D2D）技术生态系统的快速发展，评估这些服务的可行性、成本效益和盈利能力对于利益相关者做出明智的投资、开发和部署决策至关重要。

Method: 论文提出了一个综合的技术经济分析（TEA）框架，用于评估基于LEO的卫星D2D系统。该框架整合了全球卫星星座模型、符合ITU-R建议的无线电传播模型、3GPP兼容的容量计算、真实的全球人口数据，以及涵盖空间和地面段资本与运营支出的全面成本模型。此外，该框架还评估了三种不同的全球非地面网络（NTN）架构选项，并基于开源工具进行了增强实现，以确保可重复性。

Result: 经济评估表明，全球卫星D2D服务可以以与地面服务相当的每月用户成本提供，同时实现正的投资回报率（ROI）。此外，结果还显示Open RAN技术在实现经济高效的卫星D2D服务方面具有巨大潜力。

Conclusion: LEO卫星D2D服务具备与现有地面服务相媲美的成本效益和盈利能力，是下一代全球连接的可行解决方案，并且Open RAN技术有望进一步降低成本，推动其广泛部署。

Abstract: Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D)
technology are at the heart of the next-generation global connectivity which
promises direct access to space-based broadband services for unmodified
3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to
evaluate the feasibility, cost-effectiveness, and profitability of these
services. By assessing the technological aspects as well as economic
implications, stakeholders can make informed decisions about investment,
development, and deployment strategies. This paper presents a comprehensive
techno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D
systems. The framework integrates a global satellite constellation model, radio
propagation aspects including atmospheric and rainfall attenuation models
compliant with ITU-R recommendations, 3GPP-compliant capacity calculations,
realistic global population data, and an all-encompassing cost model accounting
for both capital and operational expenses associated with space and ground
segments. Further, the framework evaluates three different architectural
options for realizing a global non-terrestrial network (NTN) for satellite D2D
services. With an emphasis on reproducibility, the framework has been
implemented through significant enhancements to an open-source tool. The
economic assessment reveals that global satellite D2D services can be provided
at a monthly cost per subscriber which is comparable to terrestrial services
while achieving a positive return on investment (ROI). Moreover, the results
show the potential of Open RAN technology for realizing cost-effective
satellite D2D services.

</details>


### [242] [Evaluating UORA-Based Polling Mechanism for Latency-Sensitive Uplink Traffic in Wi-Fi Networks](https://arxiv.org/abs/2510.04731)
*Douglas Dziedzorm Agbeve,Andrey Belogaev,Chris Blondia,Jeroen Famaey*

Main category: cs.NI

TL;DR: 本文研究了Wi-Fi 6中Uplink OFDMA-based Random Access (UORA) 如何解决传统OFDMA上行调度因轮询效率低下导致的可扩展性问题，并通过ns-3仿真证明UORA在密集和稀疏流量环境下均能显著提高上行调度效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: IEEE 802.11ax (Wi-Fi 6) 的OFDMA上行调度需要AP轮询站台（STA）以获取缓冲状态报告，但随着设备密度增加，这种轮询方式变得效率低下且不可扩展，特别对延迟敏感的数据流构成挑战。

Method: 通过在ns-3仿真环境中对不同的轮询策略进行性能评估，比较UORA与其他方案的表现。

Result: 仿真结果表明：1) 在部署密集的、具有异构上行流量模式的网络环境中，基于UORA的轮询方案优于其他替代方案。2) 在高度稀疏和偶发的流量条件下，基于UORA的轮询方案与调度访问 (SA) OFDMA 相比，可将延迟降低40%以上。

Conclusion: UORA通过机会性地识别未调度站台的缓冲流量，实现了高效的上行调度，并在协调性和可扩展性之间取得了平衡，显著改善了延迟敏感数据流的可扩展性和延迟表现。

Abstract: IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple
Access (OFDMA), which enables simultaneous transmissions through centralized
resource allocation. However, effective uplink scheduling requires the Access
Point (AP) to identify which stations (STAs) have data to transmit. This
typically necessitates polling for buffer status reports, a process that
becomes increasingly inefficient and unscalable with growing device density. In
this paper, we study how the Uplink OFDMA-based Random Access (UORA) feature
improves the scalability and delay experienced by latency-sensitive data
streams. We show that UORA enables efficient uplink scheduling while
opportunistically identifying buffered traffic from unscheduled STAs, striking
a balance between coordination and scalability. Performance evaluation of
different polling strategies is done by means of simulation in ns-3. The
results indicate that UORA-based polling outperforms alternative schemes in
densely deployed network environments with heterogeneous uplink traffic
patterns. Furthermore, under highly sparse and sporadic traffic conditions,
UORA-based polling yields over 40% delay reduction compared to Scheduled Access
(SA) OFDMA.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [243] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: RFold通过动态调整作业形状和集群拓扑，在环面集群中同时实现了机器学习作业资源分配中网络争用最小化和集群利用率最大化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在多租户环面拓扑集群中，为分布式机器学习作业分配资源时，需要在最小化网络争用和最大化集群利用率之间取得平衡。现有调度器难以兼顾这两个目标，导致性能受限。

Method: 本文提出了RFold方法，在运行时自适应调整作业形状和底层集群拓扑。这通过两种技术实现：1) 识别支持作业通信需求的同态作业形状；2) 重新配置光路交换机(OCS)使能的拓扑，以支持更多样化的作业形状。

Result: 在4096节点环面集群模拟器上的初步评估显示，RFold能将绝对集群利用率提高57%，并将作业完成时间相对于现有方法减少高达11倍。

Conclusion: RFold成功证明了通过运行时自适应作业形状和集群拓扑，可以在多租户环面集群中同时实现网络争用最小化和集群利用率最大化，从而显著提升分布式机器学习作业的执行效率。

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [244] [PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance](https://arxiv.org/abs/2510.04619)
*Ivan Homoliak,Martin Perešíni,Marek Tamaškovič,Timotej Ponek,Lukáš Hellebrandt,Kamil Malinka*

Main category: cs.CR

TL;DR: PoS-CoPOR协议通过集成原生洋葱路由机制，解决了PoS协议中预选举领导者易受DoS攻击的问题，实现了高吞吐量和强大的抗DoS能力。


<details>
  <summary>Details</summary>
Motivation: 现有的PoS共识协议常面临性能与安全性之间的权衡，特别是预选举领导者的机制容易遭受拒绝服务（DoS）攻击，导致网络中断和活性受损。

Method: 本文提出了PoS-CoPOR，一个单链PoS共识协议。它将权益加权的概率领导者选举与原生的洋葱路由匿名化层相结合，以隐藏下一区块提议者的网络身份，从而防止对领导者的目标性DoS攻击。

Result: PoS-CoPOR在6个节点环境下，即使有匿名化层的开销，也能达到110 tx/s的吞吐量。

Conclusion: 原生匿名化可以提供强大的DoS抵抗能力，同时对性能影响较小，为构建安全且可扩展的PoS区块链提供了一种解决方案。

Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between
performance and security. Protocols that pre-elect leaders for subsequent
rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the
network and compromise liveness. In this work, we present PoS-CoPOR, a
single-chain PoS consensus protocol that mitigates this vulnerability by
integrating a native onion routing mechanism into the consensus protocol
itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an
anonymization layer that conceals the network identity of the next block
proposer. This approach prevents targeted DoS attacks on leaders before they
produce a block, thus enhancing network resilience. We implemented and
evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to
110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The
results show that native anonymization can provide robust DoS resistance with
only a modest impact on performance, offering a solution to build secure and
scalable PoS blockchains.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [245] [The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems](https://arxiv.org/abs/2510.04413)
*Muhammad Umar Farooq Qaisar,Weijie Yuan,Onur Günlü,Taneli Riihonen,Yuanhao Cui,Lin Zhang,Nuria Gonzalez-Prelcic,Marco Di Renzo,Zhu Han*

Main category: eess.SP

TL;DR: 本教程全面概述了集成传感与通信 (ISAC) 在6G网络中的关键作用，探讨了其演进、核心原理、使能技术、研究挑战以及对未来无线通信的影响。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的到来，通信与传感的融合成为支持下一代应用的基本转变。ISAC是实现这一融合的关键概念，旨在提升频谱效率、降低延迟并支持智能城市、自主系统等多样化用例，因此有必要深入探讨其在6G中的作用。

Method: 本文采用教程形式，首先回顾了ISAC自5G以来的演进和技术驱动因素，然后介绍了ISAC的核心原理和系统变体，并深入讨论了其使能技术。文章还分析了当前的研究方向、关键挑战、开放问题和新兴趋势，并提出了设计见解和建议。

Result: 本教程提供了ISAC在6G网络中角色的全面概述，揭示了其技术演进、核心原则和实现技术。它明确了当前的研究挑战、开放问题和新兴趋势，并为未来的发展和实施提供了设计见解和建议，旨在回答ISAC为何对6G至关重要、带来哪些创新以及如何塑造未来无线通信的问题。

Conclusion: ISAC是6G网络不可或缺的核心技术，它通过集成通信和传感功能带来显著创新，并将在根本上塑造未来无线通信的面貌和能力。

Abstract: The commencement of the sixth-generation (6G) wireless networks represents a
fundamental shift in the integration of communication and sensing technologies
to support next-generation applications. Integrated sensing and communication
(ISAC) is a key concept in this evolution, enabling end-to-end support for both
communication and sensing within a unified framework. It enhances spectrum
efficiency, reduces latency, and supports diverse use cases, including smart
cities, autonomous systems, and perceptive environments. This tutorial provides
a comprehensive overview of ISAC's role in 6G networks, beginning with its
evolution since 5G and the technical drivers behind its adoption. Core
principles and system variations of ISAC are introduced, followed by an
in-depth discussion of the enabling technologies that facilitate its practical
deployment. The paper further analyzes current research directions to highlight
key challenges, open issues, and emerging trends. Design insights and
recommendations are also presented to support future development and
implementation. This work ultimately try to address three central questions:
Why is ISAC essential for 6G? What innovations does it bring? How will it shape
the future of wireless communication?

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [246] [Multi-Agent Distributed Optimization With Feasible Set Privacy](https://arxiv.org/abs/2510.05068)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究多智能体分散式约束优化问题，旨在由领导者在保持个体可行集隐私的前提下学习最优解集。文章提出了在名义信息泄露下的可行方案，并在环形和星形网络拓扑下分析了其通信成本，证明其在通信效率上优于直接使用PSI协议。


<details>
  <summary>Details</summary>
Motivation: 多个智能体需要共同学习最优解集，但其各自的私有可行集信息不希望向其他智能体（特别是领导者）泄露超出学习解集所必需的“名义”信息。传统方法（如通过PSI协议学习联合可行集）会导致过度的信息泄露。

Method: 开发了在名义信息泄露下获取解集的可行方案。在环形和星形两种通信设置下，对这些方案的通信成本进行了特性描述。研究了所提方案与阈值私有集合交集（ThPSI）之间的关联。

Result: 所开发的方案实现了名义信息泄露。证明了如果领导者首先通过现有私有集合交集（PSI）协议学习联合可行集，信息泄露量将大于名义值。对于目标函数$f$的各种随机实现，所提方案与通过PSI检索整个可行集相比，以高概率实现了更高的通信效率。

Conclusion: 本研究成功提出了在分散式约束优化中，以名义信息泄露获取最优解集的有效方案。这些方案在保护隐私的同时，在特定网络设置和目标函数条件下，表现出比直接使用PSI协议更优的通信效率。

Abstract: We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

</details>
