<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.NI](#cs.NI) [Total: 6]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 4]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.NE](#cs.NE) [Total: 3]
- [math.OC](#math.OC) [Total: 5]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DL](#cs.DL) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TL;DR: 本文介绍了一种针对多模态大语言模型（LLM）的新型不确定性量化（UQ）校准方法。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法依赖模型自身多次响应的一致性，但在模型持续输出错误答案时，这些方法会错误地报告高置信度，导致置信度与准确性校准不佳。

Method: 该方法利用跨模态一致性（将文本响应锚定到视觉输入）和自洽性来改进校准。具体地，使用一个锚定模型的置信度来校准整体置信度，并对该锚定模型自身的不确定性应用温度缩放进行校准。

Result: 在医疗问答（Slake）和视觉问答（VQAv2）等多个多模态任务上，使用LLaVA-Med和LLaVA等模型进行的实验表明，所提出的框架在这两个任务上均取得了显著改进的校准效果。

Conclusion: 该提出的框架通过结合跨模态一致性与温度缩放校准技术，能够显著提高多模态大语言模型的不确定性量化校准性能。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


### [2] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
*Gianluca Manzo,Julia Ive*

Main category: cs.CL

TL;DR: 该研究探讨了深度学习模型在胸部X光片解读中的预测不确定性与人类语言不确定性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 在医学影像诊断中，仅优化深度学习模型的预测性能是不够的，量化其预测的不确定性至关重要，以便更好地辅助临床决策。

Method: 研究使用BERT模型，通过蒙特卡洛Dropout和深度集成等贝叶斯深度学习近似方法来估计预测不确定性，并将其与从自由文本放射学报告中提取的人类/语言不确定性进行比较。同时评估了不确定性标签的不同二值化方法。

Result: 模型表现出良好的预测性能，但其预测的不确定性与人类语言不确定性之间的相关性不大，表明机器的不确定性与人类解读的细微差别之间存在对齐挑战。

Conclusion: 尽管贝叶斯近似方法为量化不确定性提供了有价值的估计，但仍需进一步完善，以充分捕捉和利用临床应用中人类不确定性的复杂性。

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>


### [3] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
*Lucia Zheng,Neel Guha,Javokhir Arifov,Sarah Zhang,Michal Skreta,Christopher D. Manning,Peter Henderson,Daniel E. Ho*

Main category: cs.CL

TL;DR: 该研究针对法律领域检索增强大语言模型（RAG）系统开发中缺乏真实基准的问题，提出了两个新的法律RAG基准（Bar Exam QA 和 Housing Statute QA），并评估了现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 法律领域缺乏能够真实反映法律检索和下游法律问答复杂性的RAG基准测试，这阻碍了专业RAG系统的发展。

Method: 引入了两个新的法律RAG基准：律师资格考试问答（Bar Exam QA）和住房法规问答（Housing Statute QA）。这些基准的设计旨在模拟真实的法律研究任务，并通过类似法律研究的注释过程构建。研究描述了这些基准的构建过程，并测试了现有检索器管线的性能。

Result: 研究结果表明，在这些新的、更真实的法律RAG基准上，现有检索器管线的表现仍有不足，说明法律RAG仍然是一个具有挑战性的应用领域。

Conclusion: 法律领域的检索增强生成模型应用仍然面临挑战，新提出的基准测试有助于推动未来研究以改进系统性能。

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>


### [4] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
*Jiale Liu,Yifan Zeng,Shaokun Zhang,Chi Zhang,Malte Højmark-Bertelsen,Marie Normann Gadeberg,Huazheng Wang,Qingyun Wu*

Main category: cs.CL

TL;DR: 提出了一种名为细粒度优化 (FGO) 的可扩展框架，通过分解和逐步合并优化任务，以解决大型数据集下 LLM 优化器的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统 LLM 优化方法在处理大规模训练数据时，因单次处理整个轨迹导致上下文窗口溢出和模式识别能力下降，难以扩展。

Method: 提出了细粒度优化 (FGO) 框架：将大型优化任务分解为可管理的子集，对子集进行针对性优化，然后通过渐进式合并系统地组合优化后的组件。

Result: FGO 在 ALFWorld、LogisticsQA 和 GAIA 基准测试中性能比现有方法高出 1.6-8.6%，同时平均提示词 token 消耗减少了 56.3%，并在各种训练数据集大小下均表现出最一致的性能增益。

Conclusion: FGO 为扩展基于 LLM 的日益复杂的智能体系统优化提供了一个实用、可扩展且高效的解决方案。

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>


### [5] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Timothy Ossowski,Yu Gu,Ying Jin,Sid Kiblawi,Sam Preston,Mu Wei,Paul Vozila,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: 本文研究了推理能力是否可以跨模态和跨领域泛化，并提出X-Reasoner，一个仅在通用领域文本上进行后训练的视觉语言模型，证明了这种泛化是可行的。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的专有模型已展示出强大的多模态推理能力，但大多数现有开源研究集中于训练纯文本推理模型，且评估主要限于数学和通用领域任务。因此，如何有效地将推理能力扩展到文本输入和通用领域之外尚不清楚。

Method: 提出X-Reasoner，一个视觉语言模型。它仅使用通用领域文本进行后训练，采用两阶段方法：首先是使用蒸馏的长思维链进行监督微调，然后是使用可验证奖励的强化学习。对于特定领域（如医疗），在领域特定的纯文本数据上进行持续训练以增强性能（X-Reasoner-Med）。

Result: X-Reasoner 成功地将推理能力迁移到多模态和领域外设置，在各种通用和医学基准测试中超越了使用领域内和多模态数据训练的现有SOTA模型。其医学专业版本X-Reasoner-Med在众多纯文本和多模态医学基准测试中达到了新的SOTA水平。

Conclusion: 研究证实推理能力可以跨模态和跨领域泛化。仅基于通用领域文本的后训练能够实现这种强大的可泛化推理。此外，通过在特定领域的纯文本数据上进行持续训练，可以进一步增强模型在专业领域的性能。

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>


### [6] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
*Darren Yow-Bang Wang,Zhengyuan Shen,Soumya Smruti Mishra,Zhichao Xu,Yifei Teng,Haibo Ding*

Main category: cs.CL

TL;DR: SLOT是一种模型无关的后处理方法，通过微调轻量级语言模型将大型语言模型（LLM）的非结构化输出转换为精确的结构化格式，显著提升模式准确性和内容保真度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在需要结构化输出的关键应用中，其输出常偏离预定义模式，这严重阻碍了可靠的应用开发。

Method: 提出SLOT（Structured LLM Output Transformer），一种模型无关的方法。它使用一个微调的轻量级语言模型作为后处理层，将LLM的非结构化输出转换为精确的结构化格式。同时，引入了系统性的数据管理与合成流程，以及用于量化模式准确性和内容保真度的正式评估方法。

Result: 实验表明，采用SLOT（一个经过微调的Mistral-7B模型，并结合约束解码）实现了近乎完美的模式准确率（99.5%）和内容相似度（94.0%），显著优于Claude-3.5-Sonnet（分别高出25和20个百分点）。即使是像Llama-3.2-1B这样的小型模型，配备SLOT后，其结构化输出能力也能达到或超过更大的专有模型。

Conclusion: SLOT能够实现可靠的LLM结构化输出生成，尤其适用于资源受限的环境，使得小型模型也能高效地生成结构化数据。

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>


### [7] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
*Xu Huang,Yuefeng Huang,Weiwen Liu,Xingshan Zeng,Yasheng Wang,Ruiming Tang,Hong Xie,Defu Lian*

Main category: cs.CL

TL;DR: 该研究提出了个性化工具调用，使大型语言模型能根据用户偏好和画像更智能地使用工具，并为此构建了数据合成框架PTool和评估基准PTBench。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）工具调用研究主要关注基本的问题解决能力，而忽略了用户在工具选择和参数推断方面的个性化约束。

Method: 1. 提出“个性化工具调用”概念，并定义了“工具偏好”和“依赖用户画像的查询”两个关键任务。2. 提出了一个数据合成框架 PTool 用于个性化工具调用。3. 构建了首个个性化工具调用评估基准 PTBench。

Result: 通过在多种开源模型上进行微调，验证了所提框架 PTool 的有效性，并为个性化工具调用提供了有价值的见解。PTBench 基准已公开。

Conclusion: 该工作引入了个性化工具调用的概念、框架和基准，有效提升了LLMs在工具调用方面的个性化能力，并为后续研究提供了基础。

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>


### [8] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
*Mengxian Lyu,Xiaohan Li,Ziyi Chen,Jinqian Pan,Cheng Peng,Sankalp Talankar,Yonghui Wu*

Main category: cs.CL

TL;DR: 这篇论文系统综述了自然语言生成（NLG）在医学领域的应用，重点关注数据模态、模型架构、临床应用和评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的突破，自然语言生成（NLG）在医疗领域的应用日益广泛，涉及异构多样的医疗数据模态。因此，需要对NLG在医疗领域的方法和应用进行全面回顾。

Method: 研究人员遵循PRISMA指南，从3988篇NLG相关文章中筛选并系统回顾了113篇科学出版物，分析了数据模态、模型架构、临床应用和评估方法。

Result: 该研究对NLG的关键方法进行了分类，识别了其在临床上的应用，并评估了这些应用的能力、局限性以及面临的新兴挑战。

Conclusion: 这篇及时的综述涵盖了关键的NLG技术及其在医疗领域的应用，为未来利用NLG推动医学发现和医疗保健变革的研究提供了宝贵的见解。

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>


### [9] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu,Michael M. K. Cheung,Henry W. H. Chan,Anne S. Y. Cheung,Felix W. H. Chan,Yongxi Chen*

Main category: cs.CL

TL;DR: 该研究提出了一种三步法，将复杂的法律文件转化为公众易于理解的知识，并重点探讨了利用大型语言模型（如GPT-3）构建法律问题库（LQB）的技术。


<details>
  <summary>Details</summary>
Motivation: 法律信息对公众获取司法公正至关重要，但其高度专业性使其难以被非法律专业人士理解和查阅。

Method: 研究提出了一个三步法：1. 将法律条文翻译成通俗易懂的片段（CLIC-pages）；2. 构建法律问题库（LQB），其答案可在CLIC-pages中找到，本文重点使用GPT-3等大型语言模型生成问题；3. 设计交互式CLIC推荐器（CRec），根据用户描述推荐相关问题和CLIC-pages。

Result: 研究表明，使用大型语言模型（如GPT-3）可以生成法律问题。机器生成的问题（MGQs）在可扩展性、成本效益和多样性方面表现更优，而人工编写的问题（HCQs）则更为精确。同时，研究展示了一个CRec原型，并通过示例证明了三步法的有效性。

Conclusion: 该研究提出的三步法，特别是利用大型语言模型生成法律问题库的方法，能够有效地将相关法律知识传递给公众，提高法律信息的可访问性和可理解性。

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [10] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
*Vihaan Miriyala,Smrithi Bukkapatnam,Lavanya Prahallad*

Main category: cs.CL

TL;DR: 该研究探讨了使用思维链（CoT）提示与大型语言模型（LLMs）来提高应用商店评论中细粒度情感分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的数字和基于极性的评分通常无法捕捉用户反馈中嵌入的细微情感。

Method: 研究人员在2000条亚马逊应用评论上，通过比较每种方法（思维链提示与简单提示）的预测与人类判断，评估了思维链提示的有效性。

Result: 思维链提示将分类准确率从84%提高到93%。

Conclusion: 显式推理（如思维链提示）有助于增强情感分析的性能。

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [11] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
*Variath Madhupal Gautham Nair,Vishal Varma Dantuluri*

Main category: cs.CL

TL;DR: 该研究引入UTCB基准，一个动态可扩展的数据集，用于评估大型语言模型（LLM）在图像生成中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在图像生成方面的内容安全检查仍易受提示词越狱攻击。初步测试表明，即使简短自然的提示也能导致生成伪造文件或公众人物篡改图像等不当内容。

Method: 引入UTCB（Unmasking the Canvas Benchmark）基准。结合结构化提示工程、多语言混淆技术（如祖鲁语、盖尔语、Base64编码），并使用Groq托管的LLaMA-3进行评估。该流程支持零样本和后备提示策略，包含风险评分和自动标记功能，并将所有生成内容及元数据分级（青铜、白银、黄金）存储。

Result: 成功创建了一个动态且可扩展的基准数据集UTCB，用于评估LLM在图像生成方面的漏洞。

Conclusion: UTCB旨在作为一个随时间演进的工具，用于系统性评估和应对LLM图像生成中的安全漏洞，适应新的数据源、提示模板和模型行为。

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>


### [12] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
*Manas Satish Bedmutha,Feng Chen,Andrea Hartzler,Trevor Cohen,Nadir Weibel*

Main category: cs.CL

TL;DR: 本文研究了利用大型语言模型（LLMs）追踪医患对话中社交信号的能力，旨在改善医疗沟通。


<details>
  <summary>Details</summary>
Motivation: 医患间的有效沟通（包括社交信号）对健康和护理结果至关重要。大型语言模型在仅分析文本信息时也展现出推断情感和社交行为的能力，这为自动分析医患对话中的社交行为提供了潜力。

Method: 研究者设计了特定任务的提示，并使用一个包含20种不同社交信号（如医生主导性、患者热情等）的非平衡注释数据集，评估了多种大型语言模型架构和提示风格的性能。

Result: 提出了首个能够追踪所有这20种编码社交信号的系统，并揭示了大型语言模型的行为模式。对模型配置和临床背景的进一步分析为提升大型语言模型在医疗场景中社交信号处理任务的性能提供了见解。

Conclusion: 大型语言模型在追踪临床对话中的社交信号方面具备基础能力，通过进一步分析模型配置和临床背景，可以增强其在医疗保健环境中处理社交信号任务的性能。

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>


### [13] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
*Maria Marina,Nikolay Ivanov,Sergey Pletenev,Mikhail Salnikov,Daria Galimzianova,Nikita Krayko,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: 研究提出了一种轻量级、独立于大语言模型的自适应检索方法，利用外部信息提升检索增强生成（RAG）的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）易产生幻觉，检索增强生成（RAG）虽能缓解，但计算成本高昂且有误传风险。现有自适应检索方法依赖LLM进行不确定性估计，效率低下且不实用。

Method: 本研究引入了基于外部信息的轻量级、独立于LLM的自适应检索方法。研究人员调查了27个特征（分为7组）及其混合组合，并在6个问答（QA）数据集上评估了这些方法的问答性能和效率。

Result: 实验结果表明，该研究提出的方法在性能上与复杂的基于LLM的方法相当，同时实现了显著的效率提升。

Conclusion: 基于外部信息的自适应检索方法具有巨大潜力，能够有效平衡问答系统的性能与效率。

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>


### [14] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
*Sofia Jamil,Aryan Dabad,Bollampalli Areen Reddy,Sriparna Saha,Rajiv Misra,Adil A. Shakur*

Main category: cs.CL

TL;DR: 该研究针对癌症治疗中药物不良事件（ADE）的报告，提出了分组摘要任务，并创建了MCADRS数据集。同时，引入了GASCADE框架，结合大型语言模型（LLM）和T5模型进行信息提取与摘要生成，实验证明其在自动和人工评估中均表现优越。


<details>
  <summary>Details</summary>
Motivation: 在癌症治疗中，总结患者报告的药物不良事件（ADE）对加强药物警戒和改善决策至关重要。现有研究多关注一般性疾病而非癌症领域，且癌症药物警戒资源有限，亟需专门针对癌症ADE的摘要方法。

Method: 提出了MCADRS数据集，包含癌症患者药物警戒帖子、提取的标签（药物名称、ADE、严重性等）及ADE摘要。开发了GASCADE框架，该框架是一个结合大型语言模型（LLM）进行信息提取和编码器-解码器T5模型进行摘要生成的流水线。首次将对齐技术（如直接偏好优化DPO）应用于使用合成数据集进行摘要任务的编码器-解码器模型。

Result: 通过广泛的实验，GASCADE框架在多种自动化评估指标和人工评估中均表现出卓越性能，优于现有方法。

Conclusion: 该研究的多任务方法增强了药物相关的决策制定，加深了对患者关切的理解，为个性化和响应迅速的癌症护理铺平了道路。代码和数据集已公开。

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>


### [15] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
*Dario Garcia-Gasulla,Jordi Bayarri-Planas,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Adrian Tormos,Daniel Hinjos,Pablo Bernabeu-Perez,Anna Arias-Duart,Pablo Agustin Martin-Torres,Marta Gonzalez-Mallo,Sergio Alvarez-Napagao,Eduard Ayguadé-Parra,Ulises Cortés*

Main category: cs.CL

TL;DR: 本研究通过优化数据预处理和训练，并结合DPO和RAG技术，开发了具有竞争力的开源医疗大语言模型Aloe Beta系列，提升了模型的安全性和有效性，并提出新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域大型语言模型（LLM）的进步，需要有竞争力的开源模型来保护公众利益，并推动开放医疗LLM领域的发展。

Method: 基于Llama 3.1和Qwen 2.5等基础模型，使用自定义数据集（包含合成思维链示例）增强公共数据；通过直接偏好优化（DPO）进行模型对齐，强调伦理和策略；评估方法包括封闭式、开放式、安全性和人工评估。

Result: Aloe Beta系列模型在医疗基准测试和各医学领域表现出与最佳私有模型相当的竞争力，且常受医疗专业人士青睐；在偏见和毒性方面显著提高了安全性，能抵抗越狱攻击；发布了模型并附带详细的医疗保健风险评估。

Conclusion: Aloe Beta模型及其开发方法是开源医疗LLM领域的重大贡献，它们在保持高伦理标准的同时提供了顶尖性能，并为医疗保健领域开发和报告对齐的LLM设定了新标准。

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [16] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
*David Exler,Mark Schutera,Markus Reischl,Luca Rettenberger*

Main category: cs.CL

TL;DR: 该研究使用德国Wahl-O-Mat工具量化了大型语言模型（LLMs）的政治偏见，发现在德国政治背景下，LLMs（尤其是大型模型）表现出向左翼政党倾斜的偏见，并且交互语言会影响其政治观点。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的普及，LLMs已成为主要信息来源，但它们可能存在偏见，误导用户并影响舆论。与可验证的“幻觉”不同，偏见更难察觉，因此评估和缓解LLM中的偏见至关重要，以促进负责任的使用。

Method: 研究人员利用德国联邦议院近期投票背景下的Wahl-O-Mat评分系统，量化了流行LLMs的政治偏见。他们比较了模型的政治立场一致性得分，并分析了交互语言、模型来源和发布日期等因素对其政治偏好的影响。

Result: 研究发现LLMs普遍表现出政治偏见，尤其向左翼政党倾斜，这种现象在规模较大的LLMs中更为明显。此外，与模型交互时使用的语言也会影响其表达的政治观点。

Conclusion: 研究结果表明LLMs容易表现出政治偏见。开发LLMs的大型公司有责任控制这些偏见，因为它们可能影响选民的决策过程并大规模地影响公众舆论。

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>


### [17] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
*Aidar Valeev,Roman Garaev,Vadim Lomshakov,Irina Piontkovskaya,Vladimir Ivanov,Israel Adewuyi*

Main category: cs.CL

TL;DR: 本文提出了YABLoCo，一个用于评估大型语言模型在C/C++大型代码库（高达200万行代码）中生成函数体能力的新基准，解决了现有基准上下文窗口过小的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的代码生成性能评估通常基于中小型上下文（数千行代码）的基准测试，而真实世界的软件项目代码库规模可达数百万行，这导致了评估LLM在大型项目真实表现方面的差距。

Method: 1. 构建了一个名为YABLoCo的长上下文代码生成基准。2. 该基准测试集包含从四个拥有数千个函数的大型C/C++代码库（20万至200万行代码）中选取的215个函数。3. 数据集提供了函数元数据、不同依赖级别的上下文、文档字符串、函数体以及每个代码库的调用图。4. 基准专注于先前研究较少覆盖的C和C++语言中的大型代码库函数体生成。5. 开发了可扩展的评估流水线以高效计算目标指标，并提供了一个可视化分析生成代码的工具。

Result: 成功构建了YABLoCo基准测试及其配套工具。该基准的主要成果包括：1. 一个专注于C/C++大型代码库（20万至200万行代码）函数体生成的测试框架。2. 一个包含215个测试函数及相关元数据、上下文和调用图的数据集。3. 一个可扩展的评估流水线和代码可视化分析工具。这些共同支持了在大型C/C++代码库中对代码生成模型进行评估。

Conclusion: YABLoCo基准的提出，通过提供大规模代码上下文、对C/C++语言的支持以及高效的评估工具，填补了现有基准测试在评估大型语言模型处理真实大型代码库能力方面的空白，从而能够更准确地评估其在C/C++大型项目中的代码生成性能。

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>


### [18] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
*Xiaoyu Xu,Minxin Du,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 提出了一种名为OBLIVIATE的框架，旨在从大型语言模型中移除特定数据，同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练过程中可能记忆敏感、受版权保护或有害内容，需要一种方法来安全地移除这些数据。

Method: OBLIVIATE框架通过提取目标token，构建保留集，并使用包含掩蔽、蒸馏和世界事实三个组件的定制损失函数进行微调。该过程利用低秩适配器（LoRA）以保证效率。

Result: 实验表明，OBLIVIATE能有效移除目标数据（抵抗成员推断攻击），最小化对保留数据的影响，并在多种数据集和评估指标下表现出鲁棒性，同时保持模型效用和流畅性。

Conclusion: OBLIVIATE是一个有效且鲁棒的大型语言模型数据遗忘框架，能够在不牺牲模型效用的前提下移除特定数据。

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>


### [19] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TL;DR: 论文提出使用自动语言异常检测来过滤创意生成模型训练集中的低质量文本，以提高生成诗歌等内容的质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型（尤其是诗歌等创意任务）的训练数据质量对模型性能至关重要，但网络来源的数据质量参差不齐，导致生成内容存在缺陷。

Method: 比较了无监督和有监督的文本异常检测方法，使用了合成数据集和人工标记数据集，并引入了一个用于跨句语法错误检测的俄语人工标记诗歌数据集RUPOR。

Result: 提供了不同文本异常检测方法的全面比较，并发布了RUPOR数据集和完整的评估代码。

Conclusion: 该工作旨在为社区提供工具和见解，通过自动识别和过滤低质量文本来提高创意领域生成模型训练数据集的质量。

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>


### [20] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
*Yehui Tang,Yichun Yin,Yaoyuan Wang,Hang Zhou,Yu Pan,Wei Guo,Ziyang Zhang,Miao Rang,Fangcheng Liu,Naifu Zhang,Binghan Li,Yonghan Dong,Xiaojun Meng,Yasheng Wang,Dong Li,Yin Li,Dandan Tu,Can Chen,Youliang Yan,Fisher Yu,Ruiming Tang,Yunhe Wang,Botian Huang,Bo Wang,Boxiao Liu,Changzheng Zhang,Da Kuang,Fei Liu,Gang Huang,Jiansheng Wei,Jiarui Qin,Jie Ran,Jinpeng Li,Jun Zhao,Liang Dai,Lin Li,Liqun Deng,Peifeng Qin,Pengyuan Zeng,Qiang Gu,Shaohua Tang,Shengjun Cheng,Tao Gao,Tao Yu,Tianshu Li,Tianyu Bi,Wei He,Weikai Mao,Wenyong Huang,Wulong Liu,Xiabing Li,Xianzhi Yu,Xueyu Wu,Xu He,Yangkai Du,Yan Xu,Ye Tian,Yimeng Wu,Yongbing Huang,Yong Tian,Yong Zhu,Yue Li,Yufei Wang,Yuhang Gai,Yujun Li,Yu Luo,Yunsheng Ni,Yusen Sun,Zelin Chen,Zhe Liu,Zhicheng Liu,Zhipeng Tu,Zilin Ding,Zongyuan Zhan*

Main category: cs.CL

TL;DR: 本文提出了一种在昇腾NPU上高效训练超大规模稀疏MoE语言模型（如7180亿参数的盘古Ultra MoE）的方法，通过仿真优化模型配置及系统级专家并行和内存效率改进。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE大语言模型（近万亿参数）的巨大规模给底层软硬件系统带来了显著挑战，研究旨在探索如何在昇腾NPU上有效利用此类模型的规模，提高计算资源利用率并实现预期性能。

Method: 通过仿真比较不同模型超参数以选择适合昇腾NPU的配置（最终形成盘古Ultra MoE），并优化专家并行以减少NPU设备间通信开销，同时优化设备内内存效率以降低参数和激活管理开销。

Result: 在6000张昇腾NPU上训练盘古Ultra MoE时，实现了30.0%的MFU（模型有效算力利用率），性能与DeepSeek R1相当，证明了昇腾系统能够支持最先进语言模型的所有训练阶段。

Conclusion: 该研究提出的方法能够高效训练基于MoE的大规模稀疏语言模型，并为未来研究此类模型的行为特性提供了参考。

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>


### [21] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 这篇论文首次系统综述了针对低资源语言（LRL）生成式语言建模中数据稀缺问题的策略，旨在促进更具包容性的人工智能发展。


<details>
  <summary>Details</summary>
Motivation: 主流生成式语言模型（如ChatGPT、Gemini）主要服务于高资源语言（如英语），加剧了自然语言处理领域的语言不平等，亟需关注低资源语言。

Method: 通过对54项研究进行分析，识别、分类和评估了解决低资源语言生成模型数据稀缺的技术方法，包括单语数据增强、回译、多语言训练和提示工程等，并分析了模型架构、语系覆盖和评估方法的趋势。

Result: 研究发现现有方法高度依赖Transformer架构，研究主要集中在少数几种低资源语言上，并且各研究之间缺乏一致的评估标准。

Conclusion: 论文提出了将现有方法推广到更多低资源语言的建议，并概述了构建公平的生成式语言系统所面临的挑战，以支持为代表性不足的语言开发包容性AI工具，保护语言多样性。

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>


### [22] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
*Hao Sun,Zile Qiao,Jiayan Guo,Xuanbo Fan,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.CL

TL;DR: ZeroSearch是一种新颖的强化学习框架，它通过模拟搜索引擎环境而非直接交互，有效提升大语言模型的搜索与推理能力，同时解决了文档质量不可控和API成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有利用强化学习和真实搜索引擎提升大语言模型搜索能力的方法面临两大挑战：搜索引擎返回的文档质量不可控导致训练噪声和不稳定，以及强化学习训练所需的大量API调用导致成本过高且扩展性受限。

Method: ZeroSearch首先通过轻量级监督微调将大语言模型转换为一个能根据查询生成相关及噪声文档的检索模块。随后，在强化学习训练阶段，采用基于课程的rollout策略，逐步降低生成文档的质量，从而在日益复杂的检索情景中激发模型的推理能力，整个过程无需与真实搜索引擎交互。

Result: 实验表明，ZeroSearch能有效提升大语言模型的搜索能力（以3B模型作为检索模块）。一个7B参数的检索模块能达到与真实搜索引擎相当的性能，而14B参数的检索模块甚至能超越真实搜索引擎。该方法在不同参数规模的基础模型、指令微调模型以及多种强化学习算法上均表现出良好的泛化性。

Conclusion: ZeroSearch框架能够有效且经济地激励大语言模型的搜索能力，通过内部生成和控制文档质量，避免了与真实搜索引擎交互带来的成本和质量控制问题。

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TL;DR: 该研究评估了视觉语言模型（VLMs）在视觉视角采择任务上的能力，发现其在场景理解上表现良好，但在空间推理和视角采择方面能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 探究当前视觉语言模型在执行视觉视角采择任务方面的能力，并识别其在此类高级视觉认知任务中的局限性。

Method: 研究者设计了一套包含144个独特视觉任务的新颖数据集，这些任务基于精心控制的场景（人形微缩模型与单个物体，系统性地改变空间配置和视角），并结合7个诊断性问题来评估模型的场景理解、空间推理和视觉视角采择三个层次的视觉认知能力。

Result: 对多种先进VLM（包括GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct及Claude Sonnet变体）的评估显示，尽管它们在场景理解方面表现出色，但在空间推理方面的性能显著下降，在视角采择方面的性能进一步恶化。

Conclusion: 研究表明，视觉语言模型在表面物体识别与复杂视觉任务所需的深层空间及视角推理之间存在差距，指出未来VLM发展需要整合显式的几何表征和定制化的训练协议。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [24] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种基于机器学习的非接触、原位蚀刻深度预测框架，利用工艺参数和数字图像比色法（DIC）数据，旨在提高半导体制造中的监测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统离线分析方法在监测蚀刻深度和绝缘材料厚度方面存在时间延迟和污染风险，影响器件性能和良率，因此需要非接触、原位的实时监测技术。

Method: 研究探索了两种方案：1) 使用工艺参数训练人工神经网络 (ANN) 预测平均蚀刻深度，并使用贝叶斯神经网络 (BNN) 捕捉不确定性；2) 使用数字图像比色法 (DIC) 的RGB数据作为输入进行蚀刻深度预测。

Result: 在第一种方案中，ANN模型的均方误差显著低于线性基线模型，BNN能够提供可靠的不确定性估计。在第二种方案中，即使没有明确的工艺参数，使用DIC的RGB数据进行蚀刻深度预测也表现出良好的性能。

Conclusion: 结合数字图像比色法 (DIC) 和机器学习 (ML) 为等离子体蚀刻过程中的实时、原位、无创监测提供了一种可行且经济高效的替代方案，有助于提高工艺稳定性和生产效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [25] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: 这篇综述全面分析了视频大型语言模型 (VideoLLMs) 的基准测试和评估方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的快速发展推动了视频理解技术的显著进步，因此需要对专为视频大型语言模型 (VideoLLM) 设计或使用的基准和评估方法进行全面分析。

Method: 研究 examine 了当前视频理解基准的现状，讨论了它们的特点、评估协议和局限性，并分析了各种评估方法，包括封闭集、开放集以及针对时域和时空理解任务的专门评估。

Result: 论文强调了最先进VideoLLM在这些基准上的性能趋势，并指出了当前评估框架中存在的关键挑战。

Conclusion: 提出了未来研究方向以增强基准设计、评估指标和协议，包括对更多样化、多模态和关注可解释性的基准的需求，旨在为研究人员提供有效评估VideoLLM的结构化理解，并推动该领域发展。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [26] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: 本文综述了现有的视频伪造检测取证技术，重点关注其在验证监控录像真实性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的进步，数字录像篡改日益容易，引发了对其真实性的担忧，尤其是在监控录像对安全、执法和司法过程至关重要的情况下。

Method: 本文对现有的视频伪造检测取证技术进行了全面回顾，探讨了包括基于压缩的分析、帧复制检测和基于机器学习的方法等多种技术。

Result: 研究结果强调，为了应对不断发展的伪造方法，迫切需要更强大的取证技术。

Conclusion: 加强视频取证能力对于确保监控录像作为法律证据保持可信度和可采性至关重要。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [27] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TL;DR: 提出PointExplainer，一种可解释的帕金森病手绘信号诊断策略，通过识别关键手绘区域来解释模型决策，且不降低诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度神经网络的帕金森病手绘信号诊断方法缺乏明确的可解释性，这阻碍了其在临床上的信任和应用。

Method: 提出PointExplainer策略：1) 诊断模块将手绘信号编码为3D点云表示轨迹；2) 解释模块训练可解释的代理模型来近似黑盒诊断模型的局部行为，为手绘片段分配离散归因值以量化其贡献。引入一致性度量以确保解释的忠实度。

Result: 在两个基准数据集和一个新建数据集上的大量实验表明，PointExplainer能够提供直观的解释，并且不会导致诊断性能下降。

Conclusion: PointExplainer是一种有效的方法，可以为基于手绘信号的帕金森病诊断模型提供可信的、直观的解释，有助于增强临床信任。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [28] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TL;DR: 本文提出了一种名为SDD的类激活映射（CAM）技术，用于可视化解释深度学习人脸识别系统的决策，以增强其透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别系统通常像黑箱模型一样运作，不提供决策解释，导致用户信任度低。

Method: 采用一种基于类激活映射（CAM）的判别性定位技术，称为缩放有向散度（Scaled Directed Divergence, SDD），来精细定位与深度学习模型预测相关的面部特征，从而提供可视化解释。

Result: 实验表明，与传统CAM相比，SDD CAM能够更具体、更准确地突出显示相关的面部特征。

Conclusion: 通过对相关特征进行精确的可视化解释，可以为基于深度学习的人脸识别系统提供必要的透明度和信任。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [29] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: 本文提出GAME，一种图增强多模态编码器，用于从短视频进行表观人格分析，能有效融合视、听、文多源特征，并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 短视频中的表观人格分析因视觉、听觉和文本线索的复杂交织而极具挑战性。

Method: 提出GAME（图增强多模态编码器）：视觉上构建面部图，采用结合GCN与CNN和注意力机制的双分支Geo双流网络捕捉结构与外观线索，辅以ResNet18/VGGFace提取全局/身份特征，BiGRU处理时序动态；音频用VGGish；文本用XLM-Roberta。通过基于通道注意力的融合模块整合多模态特征，并用MLP回归头预测人格特质。

Result: 大量实验表明，GAME在多个基准测试中持续优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME是一种有效且具有泛化能力的短视频表观人格自动预测方法。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [30] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TL;DR: 提出了一种结合深度拓扑数据分析（TDA）、自监督学习和迁移学习的先进聚类框架，用于半导体图像的无监督聚类，以改善缺陷识别和过程监控。


<details>
  <summary>Details</summary>
Motivation: 半导体制造产生大量图像数据，手动检测能力有限，传统聚类方法难以有效处理高维、未标记数据以捕捉细微模式。

Method: 引入一个集成了深度拓扑数据分析（TDA）、自监督学习和迁移学习的先进聚类框架。TDA捕捉内在拓扑特征，自监督学习从未标记数据中提取有意义的表示，迁移学习增强框架的适应性和可扩展性。

Result: 在合成和开源半导体图像数据集上的验证表明，该框架成功识别了与缺陷模式和工艺变化一致的聚类。

Conclusion: 该研究突出了结合TDA、自监督学习和迁移学习的变革潜力，为半导体制造及其他大规模图像数据集领域的主动过程监控和质量控制提供了可扩展的解决方案。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [31] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TL;DR: 本文提出了一种基于主动推理框架的内隐和外显视觉注意模型，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能体在处理复杂高维感官输入时，选择性注意并过滤干扰的能力至关重要。

Method: 本文提出了一种基于主动推理框架的内隐和外显视觉注意模型。该模型通过动态优化感觉精确度来最小化自由能，并根据当前环境信念和感觉输入来确定视觉感觉精确度。使用Posner提示任务和简单的目标聚焦任务（2D视觉数据）对模型进行测试，测量反应时间以研究外源性/内源性注意及有效/无效提示的影响。

Result: 实验结果表明：1. 外源性和有效提示通常比内源性和无效提示导致更快的反应时间。2. 模型表现出类似返回抑制的行为。3. 对于外显注意，非自愿的反射性眼跳比有意的眼跳更快，但适应性较差。

Conclusion: 该模型成功地模拟了视觉注意中的多种关键行为，包括外源性/内源性注意差异、返回抑制以及外显注意中不同眼跳类型的特性。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [32] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为 AttUKAN 的新型注意力 U 形 Kolmogorov-Arnold 网络和一种新的标签引导像素级对比损失函数，用于视网膜血管分割，在多个数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜血管分割方法未能充分利用编码器提供的特征级细粒度表示来增强特征辨别能力，主要关注最小化解码器输出与标签之间的差异，导致模型提取的特征辨别力不足。

Method: 提出了一种新颖的注意力 U 形 Kolmogorov-Arnold 网络 (AttUKAN)，将注意力门 (Attention Gates) 整合到 Kolmogorov-Arnold 网络 (KANs) 中，以增强模型对相关特征的敏感度和可解释性。同时，设计了一种新的标签引导像素级对比损失 (Label-guided Pixel-wise Contrastive Loss) 来监督 AttUKAN 提取更具辨别力的特征。

Result: 在 DRIVE、STARE、CHASE_DB1、HRF 四个公共数据集和一个私有数据集上的实验表明，AttUKAN 在 F1 分数和 MIoU 分数上均取得了最高成绩（F1 分数分别为 82.50%, 81.14%, 81.34%, 80.21%, 80.09%；MIoU 分数分别为 70.24%, 68.64%, 68.59%, 67.21%, 66.94%），优于其他 11 种对比网络。

Conclusion: 提出的 AttUKAN 网络结合标签引导的像素级对比损失，通过增强特征的辨别能力，有效提升了视网膜血管分割的性能，定量和定性结果均表明其达到了行业领先水平。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [33] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CNN和ESPCNN的框架，用于高效处理基础设施图像，首先筛选出缺陷图像，然后仅对缺陷图像进行超分辨率处理，以提高检测准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有无人机图像超分辨率技术应用于基础设施资产管理时，存在计算成本高、误报率高的问题，尤其是在处理所有图像（包括无缺陷和有缺陷的图像）时。

Method: 开发了一个包含卷积神经网络（CNN）和高效亚像素卷积神经网络（ESPCNN）的框架。首先使用CNN对图像进行分类（有缺陷/无缺陷），然后仅对CNN识别出的有缺陷图像应用ESPCNN进行超分辨率处理。

Result: CNN准确地对图像进行了分类。ESPCNN在所有超分辨率评估指标上均优于双三次插值法。CNN和ESPCNN的组合有效预处理了无缺陷的基础设施图像，降低了后续超分辨率步骤的计算成本和误报率。视觉检查表明ESPCNN能捕捉到裂纹扩展和复杂几何形状的微小裂纹。

Conclusion: 该提出的框架通过先分类后选择性超分辨率的方式，能够有效降低基础设施图像处理的计算成本和误报，有助于公路机构准确进行缺陷检测和高效的资产管理。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [34] [Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges](https://arxiv.org/abs/2505.03991)
*Hao Xu,Arbind Agrahari Baniya,Sam Well,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TL;DR: 这篇综述回顾了体育视频事件检测领域，重点介绍了三种关键任务、最新深度学习方法、数据集和评估指标，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 体育视频事件检测对自动化识别关键时刻、提升性能分析、观众参与度和转播效率至关重要，深度学习的进步推动了该领域的发展，需要进行全面梳理。

Method: 本文对体育视频事件检测中的时序动作定位（TAL）、动作识别（AS）和精确事件识别（PES）三个关键任务进行了全面概述，回顾和分类了体育背景下的数据集与评估指标，并分析了包括多模态方法、自监督学习和知识蒸馏在内的前沿技术。

Result: 综述清晰阐述了不同任务的差异、应用和方法演变，总结了现有数据集和评估指标的优缺点，分析了最先进的技术，并讨论了该领域面临的关键开放挑战。

Conclusion: 此综述为未来开发更通用、高效、鲁棒且适用于多种体育项目的事件检测框架奠定了基础，并为高效、可泛化和多模态体育事件检测指明了有前景的研究方向。

Abstract: Video event detection has become an essential component of sports analytics,
enabling automated identification of key moments and enhancing performance
analysis, viewer engagement, and broadcast efficiency. Recent advancements in
deep learning, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly improved accuracy and efficiency in Temporal
Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting
(PES). This survey provides a comprehensive overview of these three key tasks,
emphasizing their differences, applications, and the evolution of
methodological approaches. We thoroughly review and categorize existing
datasets and evaluation metrics specifically tailored for sports contexts,
highlighting the strengths and limitations of each. Furthermore, we analyze
state-of-the-art techniques, including multi-modal approaches that integrate
audio and visual information, methods utilizing self-supervised learning and
knowledge distillation, and approaches aimed at generalizing across multiple
sports. Finally, we discuss critical open challenges and outline promising
research directions toward developing more generalized, efficient, and robust
event detection frameworks applicable to diverse sports. This survey serves as
a foundation for future research on efficient, generalizable, and multi-modal
sports event detection.

</details>


### [35] [The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics](https://arxiv.org/abs/2505.04006)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 这篇综述探讨了眼科组学（oculomics）——利用人工智能驱动的视网膜成像分析来获取眼部和全身健康信息，并展望了其未来方向。


<details>
  <summary>Details</summary>
Motivation: 人眼独特的血管化视网膜结构为观察人体健康提供了一个窗口，有潜力用于眼部及全身性疾病的早期检测、监测和干预。人工智能的进步为利用这一机会提供了可能。

Method: 本文是一篇综述性论文，回顾了视网膜成像技术的演变，探讨了人工智能驱动分析的整合需求，以及视网膜成像从传统技术向眼科组学的转变。

Result: 该综述阐述了视网膜成像的演进、AI整合的必要性以及眼科组学的兴起，同时讨论了其发展中可能遇到的障碍、研究空白和未来方向。

Conclusion: 眼科组学作为眼科学的新前沿，在探索眼部和全身性疾病方面正获得越来越多的关注，它有望通过眼部系统揭示全身健康状况，并为及时干预和识别提供无创性标志物。

Abstract: The unique vascularized anatomy of the human eye, encased in the retina,
provides an opportunity to act as a window for human health. The retinal
structure assists in assessing the early detection, monitoring of disease
progression and intervention for both ocular and non-ocular diseases. The
advancement in imaging technology leveraging Artificial Intelligence has seized
this opportunity to bridge the gap between the eye and human health. This track
paves the way for unveiling systemic health insight from the ocular system and
surrogating non-invasive markers for timely intervention and identification.
The new frontiers of oculomics in ophthalmology cover both ocular and systemic
diseases, and getting more attention to explore them. In this survey paper, we
explore the evolution of retinal imaging techniques, the dire need for the
integration of AI-driven analysis, and the shift of retinal imaging from
classical techniques to oculomics. We also discuss some hurdles that may be
faced in the progression of oculomics, highlighting the research gaps and
future directions.

</details>


### [36] [FoodTrack: Estimating Handheld Food Portions with Egocentric Video](https://arxiv.org/abs/2505.04055)
*Ervin Wang,Yuhao Chen*

Main category: cs.CV

TL;DR: 提出了一种名为FoodTrack的框架，使用第一人称视角视频来跟踪和测量手持食物的体积，该方法对遮挡具有鲁棒性且适应不同姿态。


<details>
  <summary>Details</summary>
Motivation: 传统食物消耗跟踪方法存在局限性，如需特定角度、无遮挡图像或依赖手势识别，且通常基于咬合大小假设而非直接测量食物体积。

Method: 提出了FoodTrack框架，利用第一人称视角视频直接估计食物体积，无需依赖摄入姿势或固定的咬合大小假设。

Result: 在手持食物物体上实现了约7.01%的绝对百分比损失，优于先前方法在限制条件下最佳情况下16.40%的平均绝对百分比误差。

Conclusion: FoodTrack提供了一种更准确、适应性更强的食物消耗跟踪解决方案，通过直接测量体积克服了传统方法的限制。

Abstract: Accurately tracking food consumption is crucial for nutrition and health
monitoring. Traditional approaches typically require specific camera angles,
non-occluded images, or rely on gesture recognition to estimate intake, making
assumptions about bite size rather than directly measuring food volume. We
propose the FoodTrack framework for tracking and measuring the volume of
hand-held food items using egocentric video which is robust to hand occlusions
and flexible with varying camera and object poses. FoodTrack estimates food
volume directly, without relying on intake gestures or fixed assumptions about
bite size, offering a more accurate and adaptable solution for tracking food
consumption. We achieve absolute percentage loss of approximately 7.01% on a
handheld food object, improving upon a previous approach that achieved a 16.40%
mean absolute percentage error in its best case, under less flexible
conditions.

</details>


### [37] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/abs/2505.04058)
*Feng Xiao,Hongbin Xu,Guocan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的2D辅助3D视觉定位框架，通过构建带有指代对象辨别的语义空间场景图来进行关系感知，有效解决了在多个相似干扰物中区分目标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉定位方法在区分具有相似特征的多个物体时面临挑战，尤其是在理解复杂的空间关系方面。这些方法通常以目标为中心，忽略了对被指代对象的充分感知，导致在3D与语言模态间存在显著差距。

Method: 提出了一种2D辅助的3D视觉定位框架。该框架构建带有指代对象辨别的语义空间场景图以增强关系感知。它包含一个双分支视觉编码器，利用2D预训练属性指导多模态对象编码，并采用一个使用图注意力的跨模态交互模块来促进面向关系的信息融合。通过增强的对象表示和迭代关系学习实现3D视觉与指代描述的有效对齐。

Result: 在主流基准测试上的实验结果表明，所提出的方法性能优于当前最先进的方法，尤其在处理多个相似干扰物的挑战方面表现突出。

Conclusion: 该研究提出的2D辅助框架通过语义空间场景图和指代对象辨别，显著提升了3D视觉定位在复杂场景中（尤其是有多个相似干扰物时）的性能，实现了更有效的视觉与语言对齐。

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>


### [38] [SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation](https://arxiv.org/abs/2505.04087)
*Zixuan Hu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了一种名为 SEVA 的新测试时自适应 (TTA) 方法，通过优化熵损失的上界，在单一步骤中整合多次数据增强的效果，从而在不增加计算成本的情况下提升模型对分布变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有 TTA 方法依赖单轮熵训练，无法充分利用可靠样本，影响自适应效率；而数据增强策略虽能发掘可靠样本潜力，但计算成本高，难以实时应用。

Method: 提出了单步邻近增强集成 (SEVA) 方法。它不直接生成增强数据，而是构建理论框架分析多种增强的影响，并优化熵损失的一个上界，将多轮增强训练的效果集成到单一步骤中。该上界损失还有助于筛选有害样本。

Result: 在多种网络架构和具挑战性的测试场景下的综合实验表明，SEVA 表现出优异的性能和广泛的适应性。

Conclusion: SEVA 通过其高效的损失函数和互补的选择策略，能够同时激发可靠样本的潜力并满足 TTA 严格的时间要求，有效提升了模型在分布变化下的鲁棒性和自适应效率。

Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against
distribution shifts through rapid model adaptation during inference. While
existing TTA methods often rely on entropy-based unsupervised training and
achieve promising results, the common practice of a single round of entropy
training is typically unable to adequately utilize reliable samples, hindering
adaptation efficiency. In this paper, we discover augmentation strategies can
effectively unleash the potential of reliable samples, but the rapidly growing
computational cost impedes their real-time application. To address this
limitation, we propose a novel TTA approach named Single-step Ensemble of
Vicinal Augmentations (SEVA), which can take advantage of data augmentations
without increasing the computational burden. Specifically, instead of
explicitly utilizing the augmentation strategy to generate new data, SEVA
develops a theoretical framework to explore the impacts of multiple
augmentations on model adaptation and proposes to optimize an upper bound of
the entropy loss to integrate the effects of multiple rounds of augmentation
training into a single step. Furthermore, we discover and verify that using the
upper bound as the loss is more conducive to the selection mechanism, as it can
effectively filter out harmful samples that confuse the model. Combining these
two key advantages, the proposed efficient loss and a complementary selection
strategy can simultaneously boost the potential of reliable samples and meet
the stringent time requirements of TTA. The comprehensive experiments on
various network architectures across challenging testing scenarios demonstrate
impressive performances and the broad adaptability of SEVA. The code will be
publicly available.

</details>


### [39] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/abs/2505.04088)
*Shang Zhang,Huanbin Zhang,Dali Feng,Yujie Cui,Ruoyan Xiong,Cen He*

Main category: cs.CV

TL;DR: 本文提出了一种名为SMMT的新型Siamese运动Mamba跟踪器，通过集成双向状态空间模型和自注意力机制，并采用参数共享策略和运动边缘感知损失，以解决热红外目标跟踪中的遮挡、运动模糊和背景杂波问题。


<details>
  <summary>Details</summary>
Motivation: 热红外（TIR）目标跟踪常因目标遮挡、运动模糊和背景杂波等挑战导致性能下降。

Method: 提出Siamese Motion Mamba Tracker (SMMT)。1) 将Motion Mamba模块（双向状态空间模型+自注意力）引入Siamese架构，提取运动特征并恢复边缘细节。2) 采用Siamese参数共享策略减少计算冗余。3) 设计运动边缘感知回归损失以提高对运动模糊目标的跟踪精度。

Result: 在LSOTB-TIR、PTB-TIR、VOT-TIR2015和VOT-TIR2017四个TIR跟踪基准上的实验结果表明，SMMT在TIR目标跟踪方面取得了优越的性能。

Conclusion: SMMT通过整合双向状态空间模型、自注意力机制、参数共享和特定损失函数，有效提升了在复杂场景下热红外目标跟踪的性能。

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [40] [MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction](https://arxiv.org/abs/2505.04105)
*Andrew Zhang,Hao Wang,Shuchang Ye,Michael Fulham,Jinman Kim*

Main category: cs.CV

TL;DR: 提出MAISY方法，利用SAM模型和VS-SSIM损失函数解决医学图像运动伪影问题，在保留局部细节和处理图像变化方面表现更优，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 患者在医学图像采集中产生的运动会导致图像伪影，影响判读。现有基于GAN和SSIM损失的方法在处理局部细节和变化的像素强度方面存在局限性。

Method: 提出MAISY（Motion-Aware Image SYnthesis）方法：首先表征运动，然后利用此信息进行校正。(a) 使用SAM模型动态学习解剖边界处运动伪影最明显的空间模式；(b) 引入VS-SSIM损失函数，自适应强调高像素方差区域以保留解剖细节。

Result: 在胸部和头部CT数据集上的实验表明，MAISY模型性能优于当前SOTA方法，峰值信噪比（PSNR）提升40%，结构相似性（SSIM）提升10%，Dice系数提升16%。

Conclusion: MAISY方法通过结合SAM模型和VS-SSIM损失，能有效校正医学图像运动伪影，并在保留关键解剖细节和适应图像变化方面展现出优越性，从而提高了图像质量。

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging.Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>


### [41] [One2Any: One-Reference 6D Pose Estimation for Any Object](https://arxiv.org/abs/2505.04109)
*Mengya Liu,Siyuan Li,Ajad Chhatkuli,Prune Truong,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: 提出了一种名为One2Any的新方法，仅使用单个参考-单个查询RGB-D图像即可估计6自由度物体姿态，无需3D模型或多视图数据，并能很好地泛化到新物体。


<details>
  <summary>Details</summary>
Motivation: 现有6D物体姿态估计方法依赖完整的3D模型、多视图图像或特定物体类别的训练，导致难以泛化到没有这些先验信息的新物体。

Method: 将物体姿态估计视为编码-解码过程。首先，从单个参考视图中提取参考物体姿态嵌入（ROPE），编码物体的形状、方向和纹理。然后，使用基于U-Net的姿态解码模块为新视图生成参考物体坐标（ROC），从而实现姿态估计。

Result: 在多个基准数据集上的实验表明，该模型能很好地泛化到新物体，达到了最先进的准确性和鲁棒性，甚至可以与需要多视图或CAD输入的方法相媲美，且计算成本显著降低。

Conclusion: One2Any方法能够有效地从单个RGB-D图像中为新物体估计6D姿态，无需先验模型，展现了优越的性能、鲁棒性和可扩展性。

Abstract: 6D object pose estimation remains challenging for many applications due to
dependencies on complete 3D models, multi-view images, or training limited to
specific object categories. These requirements make generalization to novel
objects difficult for which neither 3D models nor multi-view images may be
available. To address this, we propose a novel method One2Any that estimates
the relative 6-degrees of freedom (DOF) object pose using only a single
reference-single query RGB-D image, without prior knowledge of its 3D model,
multi-view data, or category constraints. We treat object pose estimation as an
encoding-decoding process, first, we obtain a comprehensive Reference Object
Pose Embedding (ROPE) that encodes an object shape, orientation, and texture
from a single reference view. Using this embedding, a U-Net-based pose decoding
module produces Reference Object Coordinate (ROC) for new views, enabling fast
and accurate pose estimation. This simple encoding-decoding framework allows
our model to be trained on any pair-wise pose data, enabling large-scale
training and demonstrating great scalability. Experiments on multiple benchmark
datasets demonstrate that our model generalizes well to novel objects,
achieving state-of-the-art accuracy and robustness even rivaling methods that
require multi-view or CAD inputs, at a fraction of compute.

</details>


### [42] [GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model](https://arxiv.org/abs/2505.04119)
*Zixiang Ai,Zichen Liu,Yuanhang Lei,Zhenyu Cui,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 该研究提出了一种名为GAPrompt的几何感知点云提示方法，用于高效微调预训练3D视觉模型，通过捕捉几何信息，以少量参数实现与全量微调相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 全量微调预训练3D视觉模型计算和存储成本高昂，而现有的参数高效微调（PEFT）方法难以有效捕捉点云的几何信息，导致性能受限。

Method: 提出了几何感知点云提示（GAPrompt）方法，包括：1. 引入“点提示”（Point Prompt）作为辅助输入以捕捉细粒度几何细节；2. 设计“点移位提示器”（Point Shift Prompter）提取全局形状信息进行实例特定的几何调整；3. 采用“提示传播机制”（Prompt Propagation）将形状信息融入特征提取过程。

Result: GAPrompt在多个基准测试中显著优于现有的PEFT方法，并取得了与全量微调相当的性能，同时仅使用了2.19%的可训练参数。

Conclusion: GAPrompt通过有效利用几何线索，为预训练3D视觉模型提供了一种参数高效且性能优越的微调方案，解决了点云数据中几何信息捕捉不足的问题。

Abstract: Pre-trained 3D vision models have gained significant attention for their
promising performance on point cloud data. However, fully fine-tuning these
models for downstream tasks is computationally expensive and storage-intensive.
Existing parameter-efficient fine-tuning (PEFT) approaches, which focus
primarily on input token prompting, struggle to achieve competitive performance
due to their limited ability to capture the geometric information inherent in
point clouds. To address this challenge, we propose a novel Geometry-Aware
Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the
adaptability of 3D vision models. First, we introduce a Point Prompt that
serves as an auxiliary input alongside the original point cloud, explicitly
guiding the model to capture fine-grained geometric details. Additionally, we
present a Point Shift Prompter designed to extract global shape information
from the point cloud, enabling instance-specific geometric adjustments at the
input level. Moreover, our proposed Prompt Propagation mechanism incorporates
the shape information into the model's feature extraction process, further
strengthening its ability to capture essential geometric characteristics.
Extensive experiments demonstrate that GAPrompt significantly outperforms
state-of-the-art PEFT methods and achieves competitive results compared to full
fine-tuning on various benchmarks, while utilizing only 2.19% of trainable
parameters. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [43] [Vision Graph Prompting via Semantic Low-Rank Decomposition](https://arxiv.org/abs/2505.04121)
*Zixiang Ai,Zichen Liu,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉图提示（VGP）的新框架，专门为视觉图结构设计，通过利用图中语义连接组件的低秩特性，实现了对ViG模型的高效参数微调。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调技术（如视觉提示）主要为Transformer模型设计，忽略了图表示中节点和边的拓扑关系，限制了其对复杂语义的建模能力。因此，需要一种专门为视觉图结构（如ViG）设计的提示方法。

Method: 提出了视觉图提示（VGP）框架。核心思想是图中语义连接的组件具有低秩特性。基于此，引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑上的提示相结合，以捕获全局结构模式和细粒度语义依赖。

Result: 大量实验表明，所提出的方法显著提高了ViG在各种下游任务上的迁移性能，达到了与全量微调相当的效果，同时保持了参数效率。

Conclusion: VGP框架通过利用图结构中语义连接组件的低秩特性，为视觉GNN提供了一种新颖且高效的参数微调方法，能够有效提升模型在下游任务中的迁移性能，同时保持较低的参数量。

Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as
graph structures, providing a more natural way to capture irregular semantic
patterns beyond traditional grid or sequence-based representations. To
efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning
techniques like visual prompting become increasingly essential. However,
existing prompting methods are primarily designed for Transformer-based models,
neglecting the rich topological relationships among nodes and edges in
graph-based representations, limiting their capacity to model complex
semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel
framework tailored for vision graph structures. Our core insight reveals that
semantically connected components in the graph exhibit low-rank properties.
Building on this observation, we introduce a semantic low-rank prompting method
that decomposes low-rank semantic features and integrates them with prompts on
vision graph topologies, capturing both global structural patterns and
fine-grained semantic dependencies. Extensive experiments demonstrate our
method significantly improves ViG's transfer performance on diverse downstream
tasks, achieving results comparable to full fine-tuning while maintaining
parameter efficiency. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [44] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TL;DR: 本文提出了 R^3-VQA，一个用于复杂社交推理的视频问答数据集，包含精细的社交事件和心理状态标注，并评估了大型视觉语言模型 (LVLM) 在此任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的社交推理任务和数据集缺乏复杂性，无法反映真实社交场景的挑战，需要更高级的基准来评估和提升AI的社交推理能力。

Method: 构建了一个名为 R^3-VQA 的高质量视频数据集，包含对社交事件、心理状态（信念、意图、愿望、情感）和社交因果链的精细标注，并包含人工及模型生成的问答对。该任务包括社交事件理解、心理状态估计和社交因果推理。研究者们以此为基准全面评估了当前大型视觉语言模型 (LVLM) 的社交推理能力。

Result: 实验表明，大型视觉语言模型 (LVLM) 在复杂社交场景中的一致性社交推理能力仍远未达到人类水平。此外，心智理论 (ToM) 提示有助于提升 LVLM 在社交推理任务上的表现。

Conclusion: R^3-VQA 数据集的提出揭示了当前 LVLM 在复杂社交推理方面的局限性，并表明心智理论提示是一种有潜力的改进方法。该数据集为未来研究提供了宝贵的资源。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [45] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TL;DR: 提出了一种名为OSLSP的新型弱监督学习方法，用于肌肉恢复阶段的分类，该方法利用序数信息并优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 肌肉组织再生评估对肌肉研究至关重要，但传统方法依赖人工视觉检查，既耗时又主观。现有机器学习方法在标记数据有限的情况下存在特征提取器适应性和序数信息丢失的问题。

Method: 提出了一种名为“从相似性比例中学习序数尺度”（OSLSP）的方法。该方法使用源自两个包组合的相似性比例损失，并通过类别比例对类别序数尺度的注意力来更新特征提取器。

Result: 采用OSLSP的模型在骨骼肌恢复阶段的分类任务中，其性能优于大规模预训练模型和微调模型。

Conclusion: OSLSP通过有效利用类别间的序数关系和更新特征提取器，为标记数据有限情况下的肌肉组织再生评估提供了一种更准确、更客观的自动化解决方案。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [46] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 本文提出一种结合ResNet、Vision Transformer、可变形卷积、检索增强生成和CRF的新型端到端文本识别框架，在多个基准数据集上实现了领先的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 自然图像中的文本识别是一项具有挑战性但至关重要的任务，在计算机视觉和自然语言处理领域有广泛应用，现有方法仍有提升空间以增强特征表示和OCR性能。

Method: 提出了一种新颖的端到端框架：1) 结合ResNet和Vision Transformer作为骨干网络；2) 在ResNet的第三和第四模块中使用可变形卷积替代标准卷积层；3) 整合了检索增强生成技术；4) 采用条件随机场（CRF）进行更精细的序列建模；5) 利用自适应dropout进行正则化。

Result: 在IC13、IC15、SVT、IIIT5K、SVTP和CUTE80六个基准数据集上进行了广泛实验，分别取得了97.32%、58.26%、88.10%、74.13%、82.17%和66.67%的准确率，平均准确率达到77.77%。

Conclusion: 该方法在文本识别方面建立了新的SOTA（state-of-the-art）水平，证明了其在多样化和具有挑战性的数据集上的鲁棒性和有效性。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [47] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TL;DR: 论文提出了一种名为S3D的新框架，能将简单的手绘草图转换为细节丰富的3D模型。


<details>
  <summary>Details</summary>
Motivation: 从二维草图生成高质量三维模型具有挑战性，因为草图数据本身具有固有的模糊性和稀疏性。

Method: S3D框架使用基于U-Net的编码器-解码器架构将草图转换为面部分割掩码，进而生成3D表示。引入了一种新颖的风格对齐损失函数来增强草图与3D输出间的一致性，并应用了数据增强技术提高鲁棒性。

Result: S3D框架能有效地从草图输入生成高质量的3D模型，风格对齐损失显著提高了重建保真度。

Conclusion: S3D框架证明了其在从草图输入生成高质量3D模型方面的有效性。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [48] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA是首个计算病理学大型多模态模型，整合三种图像场景模仿病理学家诊断过程，生成组织学描述和诊断报告。


<details>
  <summary>Details</summary>
Motivation: 当前的计算病理学方法难以模拟病理学家结合多种图像信息（单张、关键帧、分割视频）进行诊断的自然流程，且高质量的教学数据制作耗时且量少。

Method: 提出了VideoPath-LLaVA模型，并创建了VideoPath-Instruct数据集（包含4278个视频和诊断思维链教学对）。通过从单图像指令数据集迁移知识，先在弱标注的关键帧提取片段上训练，再在手动分割视频上微调。

Result: VideoPath-LLaVA在病理视频分析领域建立了新的基准，能够生成详细的组织学描述并最终给出明确的签出诊断。

Conclusion: VideoPath-LLaVA为未来通过整合视觉和诊断推理来支持临床决策的人工智能系统奠定了有前景的基础。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [49] [SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios](https://arxiv.org/abs/2505.04201)
*Ning Cheng,Jinan Xu,Jialing Chen,Wenjuan Han*

Main category: cs.CV

TL;DR: 论文提出SToLa框架和新数据集，利用专家混合模型(MoE)解决触觉与语言多模态推理中的模态差异和数据稀缺问题，提升了开放场景触觉常识推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能系统在融合触觉进行多模态常识推理时，面临两大核心挑战：一是现有模型常将触觉视为语言的子模态，导致“模态差异”；二是缺乏多样性、开放性和复杂性的“开放式触觉数据稀缺”，阻碍了对物理世界的有效推理。

Method: 1. 提出SToLa (自适应触觉-语言框架)，采用专家混合模型 (MoE) 动态处理、统一和管理触觉与语言模态，以捕捉各自独有特征。
2. 构建了一个全面的触觉常识推理数据集和基准，包含自由形式问答、8种物理属性、4种交互特性和多样化常识知识。

Result: 实验表明，SToLa在PhysiCLeAR基准测试和自建数据集上均表现出与现有模型相当甚至更优的竞争力。这证明了专家混合模型架构在多模态管理中的有效性，以及其在开放场景触觉常识推理任务中的性能优势。

Conclusion: SToLa框架及其配套数据集有效解决了触觉-语言模型中的模态差异和数据稀缺挑战，显著提升了系统在开放场景下进行触觉常识推理的能力，证明了所提方法的有效性和潜力。

Abstract: This paper explores the challenges of integrating tactile sensing into
intelligent systems for multimodal reasoning, particularly in enabling
commonsense reasoning about the open-ended physical world. We identify two key
challenges: modality discrepancy, where existing large touch-language models
often treat touch as a mere sub-modality of language, and open-ended tactile
data scarcity, where current datasets lack the diversity, open-endness and
complexity needed for reasoning. To overcome these challenges, we introduce
SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of
Experts (MoE) to dynamically process, unify, and manage tactile and language
modalities, capturing their unique characteristics. Crucially, we also present
a comprehensive tactile commonsense reasoning dataset and benchmark featuring
free-form questions and responses, 8 physical properties, 4 interactive
characteristics, and diverse commonsense knowledge. Experiments show SToLa
exhibits competitive performance compared to existing models on the PhysiCLeAR
benchmark and self-constructed datasets, proving the effectiveness of the
Mixture of Experts architecture in multimodal management and the performance
advantages for open-scenario tactile commonsense reasoning tasks.

</details>


### [50] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TL;DR: 该研究创建了一个RGB-D图像数据集（PothRGBD），并提出了一种改进的YOLOv8模型，用于坑洼检测及其物理特征（周长和深度）分析。


<details>
  <summary>Details</summary>
Motivation: 现有的坑洼检测方法通常仅基于2D RGB图像，无法准确分析坑洼的物理特性，而坑洼会导致车辆损坏和交通事故，造成严重的安全和经济问题，因此早期准确检测至关重要。

Method: 1. 使用Intel RealSense D415深度相机收集道路表面的RGB和深度数据，创建了包含1000张图像的PothRGBD数据集，并以YOLO格式进行标注。2. 提出了一种基于YOLOv8n-seg架构的改进YOLO模型，结构上引入了动态蛇形卷积（DSConv）、简单注意力模块（SimAM）和高斯误差线性单元（GELU）。3. 该模型用于坑洼分割，并利用深度图进行周长和深度测量。

Result: 所提出的模型在坑洼分割方面更准确地处理了不规则边缘结构，并在深度图上高精度地进行了周长和深度测量。与标准YOLOv8n-seg模型相比（精确度91.9%，召回率85.2%，mAP@50为91.9%），新模型的精确度提高到93.7%（提升1.96%），召回率提高到90.4%（提升6.13%），mAP@50提高到93.8%（提升2.07%）。

Conclusion: 该研究提出了一种轻量级且有效的模型，能够高精度地检测坑洼并测量其周长和深度，由于其模型复杂度低，适用于实时应用，可用于基于深度学习的智能交通解决方案。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [51] [CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models](https://arxiv.org/abs/2505.04214)
*Fabian Wolf,Oliver Tüselmann,Arthur Matei,Lukas Hennies,Christoph Rass,Gernot A. Fink*

Main category: cs.CV

TL;DR: 论文提出了CM1数据集，用于评估大视觉语言模型(LVLM)在少样本手写文档中提取关键信息（如姓名、生日）的能力，并发现LVLM在训练样本有限时表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 手写文档中关键信息的自动提取对档案数字化至关重要，尤其是在标注数据稀少的情况下，大视觉语言模型(LVLM)展现出解决此问题的潜力，但缺乏专门用于评估其少样本能力的基准数据集。

Method: 1. 构建了一个新的手写文档数据集CM1（二战后欧洲的历史表格）。2. 在该数据集上设立了三个提取姓名和出生日期的基准任务，并考虑了不同训练集大小。3. 对比了两种LVLM和一个已有的全页提取模型的性能。

Result: 传统的全页提取模型表现出很强的竞争力。然而，实验表明，当训练样本数量非常有限时，所研究的大视觉语言模型(LVLM)因其模型规模和大量的预训练而受益，其性能优于传统方法。

Conclusion: 大视觉语言模型(LVLM)在手写文档关键信息提取任务中，尤其是在训练数据稀缺的情况下，展现出优于传统方法的潜力。CM1数据集为评估LVLM的少样本能力提供了有价值的基准。

Abstract: The automatic extraction of key-value information from handwritten documents
is a key challenge in document analysis. A reliable extraction is a
prerequisite for the mass digitization efforts of many archives. Large Vision
Language Models (LVLM) are a promising technology to tackle this problem
especially in scenarios where little annotated training data is available. In
this work, we present a novel dataset specifically designed to evaluate the
few-shot capabilities of LVLMs. The CM1 documents are a historic collection of
forms with handwritten entries created in Europe to administer the Care and
Maintenance program after World War Two. The dataset establishes three
benchmarks on extracting name and birthdate information and, furthermore,
considers different training set sizes. We provide baseline results for two
different LVLMs and compare performances to an established full-page extraction
model. While the traditional full-page model achieves highly competitive
performances, our experiments show that when only a few training samples are
available the considered LVLMs benefit from their size and heavy pretraining
and outperform the classical approach.

</details>


### [52] [A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation](https://arxiv.org/abs/2505.04229)
*Theophilus Aidoo,Till Koebe,Akansh Maurya,Hewan Shrestha,Ingmar Weber*

Main category: cs.CV

TL;DR: 该研究提出了一种利用低分辨率卫星图像和粗略时间标签来估计停车场占用率的弱监督方法，以减少对昂贵高分辨率数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 标记的高分辨率遥感图像稀缺且成本高昂，尤其是在缺乏高分辨率数据的低收入地区，这限制了遥感技术的应用。

Method: 提出一个弱监督学习框架，利用粗略的时间标签（例如，假设德国大型超市和五金店的停车场周六通常是满的，周日是空的）来训练一个成对比较模型，该模型使用3米分辨率的卫星图像估计停车场占用率。

Result: 该方法在大型停车场上取得了0.92的AUC（曲线下面积），表明其有效性。

Conclusion: 所提出的方法最大限度地减少了对昂贵高分辨率图像的依赖，为可扩展的城市交通分析提供了前景，并且可以适用于评估弱势社区的交通模式和资源分配，为改善最需要帮助群体的福祉提供数据驱动的基础。

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>


### [53] [Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting](https://arxiv.org/abs/2505.04262)
*Feng Yang,Wenliang Qian,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TL;DR: 提出耦合分数蒸馏（CSD）方法，通过多视角联合优化和直接优化3D高斯溅射，解决文本到3D生成中的几何不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分数蒸馏采样（SDS）方法在文本到3D生成中忽略了多视角相关性，导致生成的3D内容存在几何不一致和多面伪影。

Method: 提出耦合分数蒸馏（CSD）框架：1. 将优化问题重构为多视角联合优化问题，以耦合多视角联合分布先验，指导跨视角的优化。2. 直接优化随机初始化的3D高斯溅射（3D-GS）以生成几何一致的3D内容。3. 采用从3D-GS初始化并通过CSD优化的可变形四面体网格，以生成高质量的精细化网格。

Result: 定量和定性实验结果表明，所提出的CSD方法在效率和生成质量上均具有竞争力。

Conclusion: CSD框架通过有效耦合多视角先验和优化3D高斯溅射，能够生成几何一致且高质量的3D内容，解决了传统SDS方法中的几何不一致和多面伪影问题。

Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.

</details>


### [54] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种名为OSGNet的物体-镜头增强型定位网络，用于解决第一人称视频定位问题，通过融合物体信息和镜头运动特征，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理第一人称视频定位时，主要关注与第三人称视频的分布差异，常忽略第一人称视频的关键特性（如频繁的镜头运动、佩戴者注意力）以及问题式查询强调的细粒度信息。

Method: 提出了OSGNet网络。具体方法包括：1. 从视频中提取物体信息以丰富视频表征，特别关注文本查询中提及但视频特征未直接捕捉的物体。2. 分析第一人称视频中频繁的镜头运动，利用这些特征提取佩戴者的注意力信息，以增强模态对齐能力。

Result: 在三个数据集上的实验结果表明，OSGNet 达到了当前最先进的性能水平。

Conclusion: OSGNet通过有效利用物体信息和镜头运动分析（提取佩戴者注意力），显著提升了第一人称视频定位的准确性，证明了该方法的有效性。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [55] [HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation](https://arxiv.org/abs/2505.04276)
*Yajie Fu,Chaorui Huang,Junwei Li,Hui Kong,Yibin Tian,Huakang Li,Zhiyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为HDiffTG的新型3D人体姿态估计方法，该方法集成了Transformer、GCN和扩散模型，以提高姿态估计的准确性和鲁棒性，同时保持轻量化设计。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有3D人体姿态估计方法在准确性、鲁棒性（尤其是在遮挡和复杂场景中）以及计算效率方面的挑战。

Method: HDiffTG框架将Transformer用于捕获全局时空依赖，GCN用于建模局部骨骼结构，扩散模型用于逐步优化微调。该方法还包括对集成模型的轻量化优化和目标函数改进，以平衡全局与局部特征并减少计算开销。

Result: 在Human3.6M和MPI-INF-3DHP数据集上的评估表明，HDiffTG在MPI-INF-3DHP数据集上达到了SOTA（state-of-the-art）性能，并在准确性、计算效率以及在有噪声和遮挡环境下的鲁棒性方面表现出色。

Conclusion: HDiffTG是一种有效的3D人体姿态估计方法，通过整合多种先进技术，成功地在精度、鲁棒性和计算效率之间取得了平衡，特别适用于具有挑战性的环境。

Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that
integrates Transformer, Graph Convolutional Network (GCN), and diffusion model
into a unified framework. HDiffTG leverages the strengths of these techniques
to significantly improve pose estimation accuracy and robustness while
maintaining a lightweight design. The Transformer captures global
spatiotemporal dependencies, the GCN models local skeletal structures, and the
diffusion model provides step-by-step optimization for fine-tuning, achieving a
complementary balance between global and local features. This integration
enhances the model's ability to handle pose estimation under occlusions and in
complex scenarios. Furthermore, we introduce lightweight optimizations to the
integrated model and refine the objective function design to reduce
computational overhead without compromising performance. Evaluation results on
the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves
state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling
in both accuracy and computational efficiency. Additionally, the model exhibits
exceptional robustness in noisy and occluded environments. Source codes and
models are available at https://github.com/CirceJie/HDiffTG

</details>


### [56] [TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement](https://arxiv.org/abs/2505.04281)
*Yi Li,Zhiyuan Zhang,Jiangnan Xia,Jianghan Cheng,Qilong Wu,Junwei Li,Yibin Tian,Hui Kong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的两阶段扩散模型 (TS-Diff)，用于增强极低光RAW图像，该模型在预训练阶段学习通用噪声特征，在对齐阶段适应特定相机噪声，并引入颜色校正器和新数据集QID，取得了领先的去噪、泛化和颜色一致性效果。


<details>
  <summary>Details</summary>
Motivation: 解决极低光RAW图像增强中存在的去噪效果不佳、对不同相机泛化能力弱以及颜色偏移等问题。

Method: 1. 提出两阶段扩散模型 (TS-Diff)：预训练阶段通过构建基于噪声空间的虚拟相机合成噪声图像，并设计相机特征集成 (CFI) 模块学习通用特征；对齐阶段将CFI平均化为目标特定CFI^T，并用少量真实RAW数据微调，再通过结构重参数化简化模型以便高效部署。
2. 引入颜色校正器：动态调整全局颜色分布以确保颜色一致性。
3. 构建新数据集QID：包含可量化光照水平和宽动态范围，用于极端低光条件下的训练与评估。

Result: TS-Diff在多个数据集（包括QID、SID和ELD）上均达到了业界领先水平，在去噪、对不同相机和光照水平的泛化能力以及颜色一致性方面表现优异。

Conclusion: TS-Diff模型具有强大的鲁棒性和多功能性，为低光成像应用提供了一个实用的解决方案。

Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing
extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes
noisy images by constructing multiple virtual cameras based on a noise space.
Camera Feature Integration (CFI) modules are then designed to enable the model
to learn generalizable features across diverse virtual cameras. During the
aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is
fine-tuned using a small amount of real RAW data to adapt to the noise
characteristics of specific cameras. A structural reparameterization technique
further simplifies CFI$^T$ for efficient deployment. To address color shifts
during the diffusion process, a color corrector is introduced to ensure color
consistency by dynamically adjusting global color distributions. Additionally,
a novel dataset, QID, is constructed, featuring quantifiable illumination
levels and a wide dynamic range, providing a comprehensive benchmark for
training and evaluation under extreme low-light conditions. Experimental
results demonstrate that TS-Diff achieves state-of-the-art performance on
multiple datasets, including QID, SID, and ELD, excelling in denoising,
generalization, and color consistency across various cameras and illumination
levels. These findings highlight the robustness and versatility of TS-Diff,
making it a practical solution for low-light imaging applications. Source codes
and models are available at https://github.com/CircccleK/TS-Diff

</details>


### [57] [MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition](https://arxiv.org/abs/2505.04306)
*Qiannan Fan,Zhuoyang Li,Jitong Li,Chenyang Cao*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的身份门控专家混合（MoDE）方法，以改善遮挡人脸识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遮挡人脸识别（OFR）算法缺乏对遮挡的先验知识，导致在处理现实中不同类型和严重程度的遮挡人脸时性能不佳，影响了人们日常生活的便利性。

Method: 提出了一种身份门控扩散专家混合模型（MoDE）。每个基于扩散的生成专家为遮挡人脸估计一个可能的完整图像。引入身份门控网络来评估每个重建人脸对身份的贡献，并在决策空间中自适应地整合预测。该模型可作为即插即用模块用于大多数现有的人脸识别模型。

Result: 在三个公共人脸数据集和两个真实场景数据集上的大量实验表明，与竞争方法相比，该方法在处理各种遮挡时均表现出更优越的性能。

Conclusion: 所提出的MoDE方法通过利用多个扩散专家生成和身份门控网络自适应融合的策略，有效解决了遮挡人脸识别的挑战，并显著提升了识别性能。

Abstract: With the continuous impact of epidemics, people have become accustomed to
wearing masks. However, most current occluded face recognition (OFR) algorithms
lack prior knowledge of occlusions, resulting in poor performance when dealing
with occluded faces of varying types and severity in reality. Recognizing
occluded faces is still a significant challenge, which greatly affects the
convenience of people's daily lives. In this paper, we propose an
identity-gated mixture of diffusion experts (MoDE) for OFR. Each
diffusion-based generative expert estimates one possible complete image for
occluded faces. Considering the random sampling process of the diffusion model,
which introduces inevitable differences and variations between the inpainted
faces and the real ones. To ensemble effective information from
multi-reconstructed faces, we introduce an identity-gating network to evaluate
the contribution of each reconstructed face to the identity and adaptively
integrate the predictions in the decision space. Moreover, our MoDE is a
plug-and-play module for most existing face recognition models. Extensive
experiments on three public face datasets and two datasets in the wild validate
our advanced performance for various occlusions in comparison with the
competing methods.

</details>


### [58] [Multi-turn Consistent Image Editing](https://arxiv.org/abs/2505.04320)
*Zijun Zhou,Yingying Deng,Xiangyu He,Weiming Dong,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种多轮图像编辑框架，通过迭代优化来解决单步编辑方法在处理复杂任务时的不足，提高了编辑成功率和视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑方法主要关注单步修改，难以处理用户意图模糊、复杂转换或需要渐进式调整的情况，导致结果不一致或不符合预期。实际应用需要更灵活和迭代的编辑方式。

Method: 提出了一种多轮图像编辑框架。该框架利用流匹配（flow matching）进行精确的图像反演，采用双目标线性二次调节器（LQR）进行稳定采样以减少误差累积。此外，通过分析Transformer的逐层作用，引入了一种自适应注意力高亮方法，以增强可编辑性并保持多轮编辑的一致性。

Result: 大量实验表明，与现有方法相比，该框架显著提高了编辑成功率和视觉保真度。

Conclusion: 该多轮图像编辑框架能够让用户通过迭代优化逐步达到更满意的编辑效果，有效解决了现有单步编辑方法的局限性。

Abstract: Many real-world applications, such as interactive photo retouching, artistic
content creation, and product design, require flexible and iterative image
editing. However, existing image editing methods primarily focus on achieving
the desired modifications in a single step, which often struggles with
ambiguous user intent, complex transformations, or the need for progressive
refinements. As a result, these methods frequently produce inconsistent
outcomes or fail to meet user expectations. To address these challenges, we
propose a multi-turn image editing framework that enables users to iteratively
refine their edits, progressively achieving more satisfactory results. Our
approach leverages flow matching for accurate image inversion and a
dual-objective Linear Quadratic Regulators (LQR) for stable sampling,
effectively mitigating error accumulation. Additionally, by analyzing the
layer-wise roles of transformers, we introduce a adaptive attention
highlighting method that enhances editability while preserving multi-turn
coherence. Extensive experiments demonstrate that our framework significantly
improves edit success rates and visual fidelity compared to existing methods.

</details>


### [59] [CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion](https://arxiv.org/abs/2505.04347)
*Yanyu Li,Pencheng Wan,Liang Han,Yaowei Wang,Liqiang Nie,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CountDiffusion的免训练框架，旨在通过两阶段方法（预测与计数、校正）提升文本到图像模型生成具有准确对象数量图像的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Stable Diffusion在文本到图像合成方面取得了进展，但训练模型生成具有准确对象数量的图像仍然困难，原因在于计算成本高昂以及模型难以学习数量这一抽象概念。

Method: CountDiffusion是一个免训练框架，分为两个阶段：第一阶段，通过扩散模型生成中间去噪结果以预测最终图像，并使用计数模型统计图像中的对象数量；第二阶段，使用校正模块通过通用引导改变对象的注意力图来修正对象数量。该框架可直接应用于任何基于扩散的文本到图像（T2I）生成模型，无需额外训练。

Result: 实验结果表明，所提出的CountDiffusion显著提高了T2I模型生成准确对象数量的能力。

Conclusion: CountDiffusion是一个有效的免训练框架，能够大幅度提升现有文本到图像模型在按照文本描述生成正确数量对象方面的性能。

Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to
generate images with accurate object quantity is still difficult due to the
high computational cost and the challenge of teaching models the abstract
concept of quantity. In this paper, we propose CountDiffusion, a training-free
framework aiming at generating images with correct object quantity from textual
descriptions. CountDiffusion consists of two stages. In the first stage, an
intermediate denoising result is generated by the diffusion model to predict
the final synthesized image with one-step denoising, and a counting model is
used to count the number of objects in this image. In the second stage, a
correction module is used to correct the object quantity by changing the
attention map of the object with universal guidance. The proposed
CountDiffusion can be plugged into any diffusion-based text-to-image (T2I)
generation models without further training. Experiment results demonstrate the
superiority of our proposed CountDiffusion, which improves the accurate object
quantity generation ability of T2I models by a large margin.

</details>


### [60] [WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing](https://arxiv.org/abs/2505.04369)
*Jie Sun,Heng Liu,Yongzhen Wang,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出一种名为 WDMamba 的图像去雾新框架，利用雾霾主要存在于低频分量的新发现，通过低频恢复和细节增强两阶段处理，并结合自引导对比正则化，实现了优于现有方法的去雾效果。


<details>
  <summary>Details</summary>
Motivation: 当前对雾霾降级特性的理解尚不充分。本研究通过小波变换分析发现，雾霾相关信息主要存在于图像的低频分量中，这为设计更有效的去雾算法提供了新的视角。

Method: 提出 WDMamba 框架，采用两阶段去雾策略：1. 低频恢复阶段：利用 Mamba 模块高效重建全局结构，去除整体雾霾，生成粗略恢复图像。2. 细节增强阶段：恢复在前一阶段可能丢失的精细纹理信息。训练过程中引入自引导对比正则化，将粗略恢复输出作为难负样本，以增强模型对细节的辨别能力。

Result: 在公开的去雾基准测试中，WDMamba 方法在定性视觉效果和定量评价指标上均超越了当前最先进的（state-of-the-art）方法。

Conclusion: 本文提出的 WDMamba 框架，通过利用新发现的雾霾特定小波退化先验，并结合分阶段处理和自引导对比正则化，能够有效地去除图像雾霾，恢复高质量的无雾图像，为图像去雾领域贡献了新的有效方案。

Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior
observed through wavelet transform analysis, which shows that haze-related
information predominantly resides in low-frequency components. Exploiting this
insight, we propose a novel dehazing framework, WDMamba, which decomposes the
image dehazing task into two sequential stages: low-frequency restoration
followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to
effectively capture features specific to each stage of the dehazing process,
resulting in high-quality restored images. Specifically, in the low-frequency
restoration stage, we integrate Mamba blocks to reconstruct global structures
with linear complexity, efficiently removing overall haze and producing a
coarse restored image. Thereafter, the detail enhancement stage reinstates
fine-grained information that may have been overlooked during the previous
phase, culminating in the final dehazed output. Furthermore, to enhance detail
retention and achieve more natural dehazing, we introduce a self-guided
contrastive regularization during network training. By utilizing the coarse
restored output as a hard negative example, our model learns more
discriminative representations, substantially boosting the overall dehazing
performance. Extensive evaluations on public dehazing benchmarks demonstrate
that our method surpasses state-of-the-art approaches both qualitatively and
quantitatively. Code is available at https://github.com/SunJ000/WDMamba.

</details>


### [61] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TL;DR: 研究视觉Transformer模型在标签噪声和主动学习环境下，不同模型大小和配置对分类准确性和校准性的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练卷积神经网络的微调已很成熟，但视觉Transformer模型大小在类似场景下，尤其是在存在标签噪声时，其性能影响仍有待探索。本研究旨在探究Transformer架构在低预算和含噪标签条件下的实用性。

Method: 在CIFAR10和CIFAR100数据集上，评估了四种视觉Transformer（ViT Base/Large，16x16/32x32 patch）和三种Swin Transformer（Tiny/Small/Base）配置。研究在不同对称标签噪声率和主动学习设置下，模型分类准确性和校准性的变化。

Result: 研究发现：1. 较大的ViT模型（特别是ViTl32）在准确性和校准性上始终优于较小模型，即使在中高标签噪声下也是如此。2. Swin Transformer在所有噪声水平下鲁棒性较差。3. 较小的patch size（如ViTl16）并不总是带来更好性能，反而计算成本更高。4. 基于信息的主动学习策略仅在中等标签噪声率下能显著提高准确性，但在高噪声率下会导致校准性变差（相较于随机获取标签训练的模型）。

Conclusion: 研究结果为在资源受限环境下部署视觉Transformer提供了实践指导，强调了在模型微调或蒸馏中平衡模型复杂度、标签噪声和计算效率的重要性。较大的ViT模型（如ViTl32）在含噪标签场景下表现更优，而主动学习策略在高噪声下可能损害校准性。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [62] [Label-efficient Single Photon Images Classification via Active Learning](https://arxiv.org/abs/2505.04376)
*Zili Zhang,Ziting Wen,Yiheng Qiang,Hongzhou Dong,Wenle Dong,Xinyang Li,Xiaofan Wang,Xiaoqiang Ren*

Main category: cs.CV

TL;DR: 本文提出首个用于单光子图像分类的主动学习框架，通过一种感知成像条件的采样策略，以显著减少的标注样本实现了高分类精度。


<details>
  <summary>Details</summary>
Motivation: 单光子图像的语义解译因高昂的标注成本和低效的标记策略而研究不足，现有研究主要集中于三维场景重建。

Method: 提出一种成像条件感知的采样策略，该策略集成了合成增强技术来模拟不同成像条件下的变化。通过识别模型既不确定又对这些条件敏感的样本，选择性地标注信息最丰富的样本。

Result: 在合成数据集上，使用1.5%的标记样本达到97%的准确率；在真实数据集上，使用8%的标记样本达到90.63%的准确率，比表现最佳的基线方法高4.51%。

Conclusion: 主动学习使得单光子图像的分类性能能够达到与经典图像相当的水平，为单光子数据在实际应用中的大规模集成提供了可能。

Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme
environments through quantum-level photon detection technology. Current
research primarily focuses on reconstructing 3D scenes from sparse photon
events, whereas the semantic interpretation of single-photon images remains
underexplored, due to high annotation costs and inefficient labeling
strategies. This paper presents the first active learning framework for
single-photon image classification. The core contribution is an imaging
condition-aware sampling strategy that integrates synthetic augmentation to
model variability across imaging conditions. By identifying samples where the
model is both uncertain and sensitive to these conditions, the proposed method
selectively annotates only the most informative examples. Experiments on both
synthetic and real-world datasets show that our approach outperforms all
baselines and achieves high classification accuracy with significantly fewer
labeled samples. Specifically, our approach achieves 97% accuracy on synthetic
single-photon data using only 1.5% labeled samples. On real-world data, we
maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher
than the best-performing baseline. This illustrates that active learning
enables the same level of classification performance on single-photon images as
on classical images, opening doors to large-scale integration of single-photon
data in real-world applications.

</details>


### [63] [Tetrahedron-Net for Medical Image Registration](https://arxiv.org/abs/2505.04380)
*Jinhai Xiang,Shuai Guo,Qianru Han,Dantong Shi,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: 提出Tetrahedron-Net，一种通过增加额外解码器来增强医学图像配准特征表示的新网络结构。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net类医学图像配准网络在单一编码器和解码器架构下的特征交互探索不足，限制了其表示能力。

Method: 引入Tetrahedron-Net，在U-Net基础上增加一个额外的解码器。这个新解码器与原始编码器和解码器都进行交互，形成一个“四面体”结构，以增强特征表示。

Result: Tetrahedron-Net在多个医学图像配准基准数据集上取得了优越的性能，并且其设计可以轻松集成到其他流行的U-Net类架构中，带来一致的性能提升。

Conclusion: Tetrahedron-Net通过引入一个额外的交互式解码器，有效地增强了医学图像配准的特征表示能力和配准精度，并证明了其设计的通用性和有效性。

Abstract: Medical image registration plays a vital role in medical image processing.
Extracting expressive representations for medical images is crucial for
improving the registration quality. One common practice for this end is
constructing a convolutional backbone to enable interactions with skip
connections among feature extraction layers. The de facto structure, U-Net-like
networks, has attempted to design skip connections such as nested or full-scale
ones to connect one single encoder and one single decoder to improve its
representation capacity. Despite being effective, it still does not fully
explore interactions with a single encoder and decoder architectures. In this
paper, we embrace this observation and introduce a simple yet effective
alternative strategy to enhance the representations for registrations by
appending one additional decoder. The new decoder is designed to interact with
both the original encoder and decoder. In this way, it not only reuses feature
presentation from corresponding layers in the encoder but also interacts with
the original decoder to corporately give more accurate registration results.
The new architecture is concise yet generalized, with only one encoder and two
decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.
Three instantiations of Tetrahedron-Net are further constructed regarding the
different structures of the appended decoder. Our extensive experiments prove
that superior performance can be obtained on several representative benchmarks
of medical image registration. Finally, such a ``Tetrahedron'' design can also
be easily integrated into popular U-Net-like architectures including
VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.

</details>


### [64] [DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution](https://arxiv.org/abs/2505.04384)
*Ming-Hui Liu,Xiao-Qian Liu,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 论文提出DATA框架，一种多解耦对比学习方法，旨在提升开放世界半监督Deepfake溯源任务中对新伪造类型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake溯源方法过分关注特定伪造痕迹导致过拟合，忽略了通用伪造特征，并且难以在开放世界场景中区分不确定的新伪造类别。

Method: 提出DATA框架：1) 定义“正交Deepfake基”以解耦特定方法的特征，减少对伪造无关信息的过拟合；2) 设计增强记忆机制辅助新类别发现和对比学习，通过实例级解耦获得新类别的清晰边界；3) 使用基对比损失和中心对比损失增强特征的标准化与区分度。

Result: 在开放世界半监督Deepfake溯源（OSS-DFA）基准测试中，DATA框架取得了当前最佳性能，与现有方法相比，在不同设置下准确率分别提升了2.55%和5.7%。

Conclusion: DATA框架通过有效解耦特定方法特征与通用伪造特征，并利用增强记忆机制促进新类别发现，显著提升了在开放世界半监督Deepfake溯源任务中对新类别的识别能力和整体性能。

Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different
facial manipulation techniques, thereby mitigating the detrimental effects of
forgery content on the social order and personal reputations. However, previous
methods focus only on method-specific clues, which easily lead to overfitting,
while overlooking the crucial role of common forgery features. Additionally,
they struggle to distinguish between uncertain novel classes in more practical
open-world scenarios. To address these issues, in this paper we propose an
innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to
enhance the generalization ability on novel classes for the open-world
semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all
generation techniques can be abstracted into a similar architecture, DATA
defines the concept of 'Orthonormal Deepfake Basis' for the first time and
utilizes it to disentangle method-specific features, thereby reducing the
overfitting on forgery-irrelevant information. Furthermore, an augmented-memory
mechanism is designed to assist in novel class discovery and contrastive
learning, which aims to obtain clear class boundaries for the novel classes
through instance-level disentanglements. Additionally, to enhance the
standardization and discrimination of features, DATA uses bases contrastive
loss and center contrastive loss as auxiliaries for the aforementioned modules.
Extensive experimental evaluations show that DATA achieves state-of-the-art
performance on the OSS-DFA benchmark, e.g., there are notable accuracy
improvements in 2.55% / 5.7% under different settings, compared with the
existing methods.

</details>


### [65] [Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle](https://arxiv.org/abs/2505.04392)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TL;DR: 提出了一种通过视觉跟踪前方车辆来预测性检测路面异常的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有直接观测方法难以应对未知类型的路面异常、低能见度或异常被前方车辆遮挡的情况。本研究旨在开发一种能够预测性检测各类路面异常的方法，以便车辆提前调整系统或规划避障。

Method: 通过视觉跟踪前方车辆的运动来间接检测路面异常。为解决自身车辆振动导致的摄像头俯仰角变化对跟踪信号的干扰，采用了一种迭代鲁棒估计器来补偿摄像头的俯仰旋转。

Result: 在受控环境和真实交通条件下的实验表明，该方法即使在自身车辆行驶于不平路面等挑战性情况下，也能可靠地远距离检测路面异常，并且可以在标准消费级硬件上实时运行。

Conclusion: 该方法通过视觉跟踪前车，能够有效且实时地预测性检测路面异常，为车辆提前配置系统或规划避让策略提供了支持。

Abstract: A novel approach to detect road surface anomalies by visual tracking of a
preceding vehicle is proposed. The method is versatile, predicting any kind of
road anomalies, such as potholes, bumps, debris, etc., unlike direct
observation methods that rely on training visual detectors of those cases. The
method operates in low visibility conditions or in dense traffic where the
anomaly is occluded by a preceding vehicle. Anomalies are detected
predictively, i.e., before a vehicle encounters them, which allows to
pre-configure low-level vehicle systems (such as chassis) or to plan an
avoidance maneuver in case of autonomous driving. A challenge is that the
signal coming from camera-based tracking of a preceding vehicle may be weak and
disturbed by camera ego motion due to vibrations affecting the ego vehicle.
Therefore, we propose an efficient method to compensate camera pitch rotation
by an iterative robust estimator. Our experiments on both controlled setup and
normal traffic conditions show that road anomalies can be detected reliably at
a distance even in challenging cases where the ego vehicle traverses imperfect
road surfaces. The method is effective and performs in real time on standard
consumer hardware.

</details>


### [66] [SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer](https://arxiv.org/abs/2505.04394)
*Young-Hu Park,Rae-Hong Park,Hyung-Min Park*

Main category: cs.CV

TL;DR: 本文提出SwinLip，一种基于Swin Transformer的高效唇读视觉编码器，通过结合改进的Conformer时间嵌入，降低计算复杂度，提升了唇读性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于ResNet的唇读模型计算复杂度高，难以高效捕捉时空特征，且在多模态研究中易引入延迟。

Method: 应用Swin Transformer的分层结构和窗口自注意力机制，设计了轻量级SwinLip视觉语音编码器，并集成了修改后的Conformer时间嵌入与传统空间嵌入。

Result: SwinLip在应用于各种词汇和句子识别的骨干网络时，显著提升了唇读网络的性能和推理速度，同时降低了计算负载。在英语LRW和中文LRW-1000数据集上均表现鲁棒，并在中文LRW-1000数据集上以更少计算量达到了最先进水平。

Conclusion: SwinLip是一种高效且性能优越的唇读视觉语音编码器，它通过改进的架构有效降低了计算量，并在多个基准测试中展现了领先或SOTA的性能。

Abstract: This paper presents an efficient visual speech encoder for lip reading. While
most recent lip reading studies have been based on the ResNet architecture and
have achieved significant success, they are not sufficiently suitable for
efficiently capturing lip reading features due to high computational complexity
in modeling spatio-temporal information. Additionally, using a complex visual
model not only increases the complexity of lip reading models but also induces
delays in the overall network for multi-modal studies (e.g., audio-visual
speech recognition, speech enhancement, and speech separation). To overcome the
limitations of Convolutional Neural Network (CNN)-based models, we apply the
hierarchical structure and window self-attention of the Swin Transformer to lip
reading. We configure a new lightweight scale of the Swin Transformer suitable
for processing lip reading data and present the SwinLip visual speech encoder,
which efficiently reduces computational load by integrating modified
Convolution-augmented Transformer (Conformer) temporal embeddings with
conventional spatial embeddings in the hierarchical structure. Through
extensive experiments, we have validated that our SwinLip successfully improves
the performance and inference speed of the lip reading network when applied to
various backbones for word and sentence recognition, reducing computational
load. In particular, our SwinLip demonstrated robust performance in both
English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art
performance on the Mandarin LRW-1000 dataset with less computation compared to
the existing state-of-the-art model.

</details>


### [67] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TL;DR: 提出了一种深度积单元残差神经网络 (PURe)，通过将积单元集成到残差块中，提高了深度卷积网络的表达能力和参数效率，并在多个基准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 标准的求和神经元可能限制深度卷积网络对复杂模式的表达能力，研究旨在通过引入积单元来提升网络的表达力和参数效率。

Method: 提出PURe网络，将二维积单元整合到残差块的第二层，替代传统卷积层，并移除这些层中的非线性激活函数。在Galaxy10 DECaLS, ImageNet, 和 CIFAR-10 数据集上进行验证。

Result: 在Galaxy10 DECaLS上，PURe34准确率达84.89%，优于ResNet152，收敛快5倍且对噪声鲁棒。在ImageNet上，PURe34准确率（top-1 80.27%, top-5 95.78%）优于更深的ResNet变体，参数更少。在CIFAR-10上，PURe272准确率达95.01%，与ResNet1001相当但模型更小。

Conclusion: PURe网络在准确性、效率和鲁棒性之间取得了良好平衡，相比传统残差网络，它具有更快的收敛速度、更少的参数和更强的噪声鲁棒性，展示了积单元架构在计算机视觉深度学习中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [68] [MFSeg: Efficient Multi-frame 3D Semantic Segmentation](https://arxiv.org/abs/2505.04408)
*Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: 提出了一种名为MFSeg的高效多帧3D语义分割框架，通过特征级聚合和轻量级解码器减少计算开销并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的多帧3D语义分割方法计算开销大，且需要处理过去帧的冗余点。研究旨在提升分割效率和准确性，同时降低计算负担。

Method: MFSeg在特征层面聚合点云序列，并对特征提取和聚合过程进行正则化。采用基于轻量级MLP的点解码器，避免对过去帧的冗余点进行上采样。

Result: MFSeg在保持高精度的同时减少了计算开销。在nuScenes和Waymo数据集上的实验表明，MFSeg的性能优于现有方法，展示了其有效性和高效性。

Conclusion: MFSeg是一个有效且高效的多帧3D语义分割框架，通过优化特征聚合和解码过程，成功提升了性能并降低了计算成本。

Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation
framework. By aggregating point cloud sequences at the feature level and
regularizing the feature extraction and aggregation process, MFSeg reduces
computational overhead while maintaining high accuracy. Moreover, by employing
a lightweight MLP-based point decoder, our method eliminates the need to
upsample redundant points from past frames. Experiments on the nuScenes and
Waymo datasets show that MFSeg outperforms existing methods, demonstrating its
effectiveness and efficiency.

</details>


### [69] [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/abs/2505.04410)
*Junjie Wang,Bin Chen,Yulin Li,Bin Kang,Yichi Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: 提出DeCLIP框架，通过解耦自注意力机制增强CLIP的局部特征表示，显著提升其在开放词汇密集视觉预测任务（如目标检测和语义分割）中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统密集视觉预测任务受限于预定义类别，不适用于视觉概念无限的真实场景。现有视觉语言模型（如CLIP）直接应用于密集预测时，因局部特征表示能力不足导致性能不佳，其图像标记难以有效聚合相关区域信息，导致特征缺乏局部辨别性和空间一致性。

Method: 提出DeCLIP框架：1. 解耦CLIP的自注意力模块，分别得到“内容”和“上下文”特征。2. “内容”特征与图像裁剪块的表示对齐，以提高局部辨别能力。3. “上下文”特征在视觉基础模型（如DINO）的指导下学习保留空间相关性。

Result: DeCLIP在多个开放词汇密集预测任务（包括目标检测和语义分割）上均显著优于现有方法。

Conclusion: DeCLIP通过改进CLIP的局部特征表示和空间一致性，有效提升了其在开放词汇密集视觉预测任务中的表现，证明了所提方法的有效性。

Abstract: Dense visual prediction tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense prediction often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. The ``content'' features are aligned with image crop
representations to improve local discriminability, while ``context'' features
learn to retain the spatial correlations under the guidance of vision
foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP
significantly outperforms existing methods across multiple open-vocabulary
dense prediction tasks, including object detection and semantic segmentation.
Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.

</details>


### [70] [RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation](https://arxiv.org/abs/2505.04424)
*Jing Hu,Chengming Feng,Shu Hu,Ming-Ching Chang,Xin Li,Xi Wu,Xin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的轻量级任意风格迁移框架RLMiniStyler，能以较低成本生成高质量、多样化的风格化图像序列。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习任意风格迁移方法在生成多样化风格结果时，通常需要显著的计算成本。

Method: 提出RLMiniStyler框架：1. 利用统一的强化学习策略，通过探索和利用风格化反馈来迭代指导风格迁移过程，实现模型轻量化。2. 引入不确定性感知多任务学习策略，自动调整损失权重以适应不同训练阶段的内容与风格平衡需求，加速模型收敛。

Result: 在多种图像分辨率上的实验表明，RLMiniStyler在生成高质量、多样化的艺术图像序列方面，以更低的成本优于其他SOTA方法。

Conclusion: RLMiniStyler是一种有效且高效的任意风格迁移框架，能以较低成本生成高质量、多样化的风格化结果。

Abstract: Arbitrary style transfer aims to apply the style of any given artistic image
to another content image. Still, existing deep learning-based methods often
require significant computational costs to generate diverse stylized results.
Motivated by this, we propose a novel reinforcement learning-based framework
for arbitrary style transfer RLMiniStyler. This framework leverages a unified
reinforcement learning policy to iteratively guide the style transfer process
by exploring and exploiting stylization feedback, generating smooth sequences
of stylized results while achieving model lightweight. Furthermore, we
introduce an uncertainty-aware multi-task learning strategy that automatically
adjusts loss weights to adapt to the content and style balance requirements at
different training stages, thereby accelerating model convergence. Through a
series of experiments across image various resolutions, we have validated the
advantages of RLMiniStyler over other state-of-the-art methods in generating
high-quality, diverse artistic image sequences at a lower cost. Codes are
available at https://github.com/fengxiaoming520/RLMiniStyler.

</details>


### [71] [Learning Real Facial Concepts for Independent Deepfake Detection](https://arxiv.org/abs/2505.04460)
*Ming-Hui Liu,Harry Cheng,Tianyi Wang,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 提出了一种名为RealID的新方法，通过学习真实人脸的全面概念并独立评估真伪概率，以解决Deepfake检测模型在未知数据集上泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: Deepfake检测模型在处理未见过的数据集时泛化能力不足，常常将真实图像误判为伪造图像。这主要是因为模型过度依赖伪造痕迹，并且对真实人脸的理解有限。

Method: 提出了RealID方法，包含两个关键模块：真实概念捕获模块 (RealC2) 和独立双决策分类器 (IDC)。RealC2借助MultiReal Memory维护真实人脸的多种原型以捕捉全面的真实概念；IDC则基于真实类别概念和伪造痕迹的存在进行独立决策，从而重新定义分类策略。

Result: 在五个广泛使用的数据集上进行的广泛实验表明，RealID 方法显著优于当前最先进的方法，平均准确率提高了1.74%。

Conclusion: RealID通过增强对真实人脸概念的理解并独立进行真伪判断，有效减轻了伪造无关模式的影响，显著提升了Deepfake检测模型在未见数据集上的泛化能力。

Abstract: Deepfake detection models often struggle with generalization to unseen
datasets, manifesting as misclassifying real instances as fake in target
domains. This is primarily due to an overreliance on forgery artifacts and a
limited understanding of real faces. To address this challenge, we propose a
novel approach RealID to enhance generalization by learning a comprehensive
concept of real faces while assessing the probabilities of belonging to the
real and fake classes independently. RealID comprises two key modules: the Real
Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier
(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various
prototypes for real faces, allowing the model to capture a comprehensive
concept of real class. Meanwhile, IDC redefines the classification strategy by
making independent decisions based on the concept of the real class and the
presence of forgery artifacts. Through the combined effect of the above
modules, the influence of forgery-irrelevant patterns is alleviated, and
extensive experiments on five widely used datasets demonstrate that RealID
significantly outperforms existing state-of-the-art methods, achieving a 1.74%
improvement in average accuracy.

</details>


### [72] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/abs/2505.04481)
*Jiahao Li,Weijian Ma,Xueyang Li,Yunzhong Lou,Guichun Zhou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: 本研究提出CAD-Llama框架，利用大型语言模型生成计算机辅助设计（CAD）模型的参数化序列，以创建参数化三维形状。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用文本生成方面表现出色，但将其应用于特定领域（如CAD模型的参数化序列生成）面临挑战，因为模型缺乏相关预训练数据和三维结构意识。

Method: 提出了CAD-Llama框架：1. 开发分层注释流程和类代码格式（SPCC）来转换参数化三维CAD命令序列。2. 采用基于SPCC的自适应预训练和CAD特定指南的指令微调，以赋予LLM处理参数化序列的空间知识。

Result: 实验结果显示，所提出的CAD-Llama框架在生成参数化三维CAD模型方面显著优于先前的自回归方法和现有的大型语言模型基线。

Conclusion: 该研究成功展示了一种通过特定数据表示和训练策略增强预训练LLM以生成参数化三维CAD模型的方法，为LLM在设计领域的应用开辟了新途径。

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>


### [73] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/abs/2505.04485)
*Ali Alawieh,Alexandru P. Condurache*

Main category: cs.CV

TL;DR: 提出FA-KPConv，一种基于KPConv的神经网络架构，通过帧平均实现对欧几里得变换的精确不变性/等变性，提升了3D点云分析性能。


<details>
  <summary>Details</summary>
Motivation: KPConv等现有网络在处理欧几里得变换时仅能近似实现不变性/等变性，通常依赖大规模数据或大量数据增强。

Method: 采用帧平均技术封装现有的KPConv网络，使其能够对输入点云的平移、旋转和/或反射实现精确的不变性或等变性，同时不增加可学习参数且不损失输入信息。

Result: FA-KPConv在点云分类和点云配准任务中表现出优势，尤其在训练数据稀少或测试数据随机旋转等挑战性情况下效果显著。

Conclusion: FA-KPConv通过嵌入几何先验知识，有效提升了KPConv类网络处理点云任务的鲁棒性和性能，特别是在数据受限或变换复杂的场景下。

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>


### [74] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TL;DR: 提出了一种名为Latent-CFM的新型流匹配模型，它通过预训练的深度潜变量模型来整合多模态数据结构，从而以更少的训练和计算量提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配模型在学习从简单源分布到目标数据的流时，大多未显式建模目标数据中的潜在结构/流形，导致学习效率低下，尤其对于高维真实世界数据集。现有整合流形的方法训练成本高昂且性能欠佳。

Method: 提出了Latent-CFM模型，该模型利用预训练的深度潜变量模型来简化训练/推断策略，从而有效地整合多模态数据结构。

Result: 在多模态合成数据和广泛使用的图像基准数据集上的实验表明，Latent-CFM 在生成质量上有所提高，并且训练量（某些情况下减少约50%）和计算量显著低于当前最优的流匹配模型。在2D Darcy流数据集上，该方法能生成物理上更准确的样本。此外，通过潜空间分析，证明了该方法可用于基于潜特征的条件图像生成。

Conclusion: Latent-CFM通过整合预训练的深度潜变量模型，有效地将多模态数据结构融入流匹配过程，从而以更低的训练和计算成本实现了更高质量的样本生成，并扩展了其在条件生成和物理精度方面的应用潜力。

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [75] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TL;DR: 该研究首次系统评估了实时视频语言模型（VideoLLMs）辅助视障人士的有效性，构建了基准数据集，并针对模型在动态环境中感知危险的挑战提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有视觉辅助技术多关注静态内容，无法满足视障人士在动态复杂环境中的实时感知需求，且缺乏对实时VideoLLMs在此领域有效性的系统评估。

Method: 构建了针对视障人士日常辅助任务的基准数据集（VisAssistDaily）；评估了不同VideoLLMs（如GPT-4o）的表现；进行了用户研究以探索实际应用挑战；构建了环境感知数据集SafeVid并引入轮询机制以主动检测环境风险。

Result: GPT-4o在VisAssistDaily任务中成功率最高；用户研究发现当前模型难以感知动态环境中的潜在危险；提出的SafeVid数据集和轮询机制旨在提升模型对环境风险的感知能力。

Conclusion: 该工作为未来利用VideoLLMs辅助视障人士的研究提供了宝贵的见解和启发，特别是在提升实时环境理解和风险感知方面。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [76] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: 该论文提出了量化指标来评估生成式AI模型的创造力，以帮助用户选择合适的模型，并在图像生成模型上验证了这些指标与人类直觉的一致性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的创造力在科学界一直存在争议且无定论，缺乏从实践角度出发的评估方法来帮助用户选择模型。

Method: 从实践角度研究创造力，引入量化衡量标准，并使用这些标准评估了多种流行的图像到图像生成模型。

Result: 评估结果表明，提出的量化衡量标准与人类的直觉相符。

Conclusion: 该研究提出的量化指标为从实践角度评估和选择生成式AI模型提供了有效工具。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [77] [Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition](https://arxiv.org/abs/2505.04502)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 该研究旨在通过并发和流水线技术，最大化利用边缘GPU的多个硬件引擎，以提升视频人脸检测与识别任务（包括视频解码）的吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将任务分配给单一硬件引擎，缺乏统一自动化框架来同时利用所有可用引擎，并且常忽略视频解码引入的延迟，影响实时应用。

Method: 通过并发和流水线技术，同时利用边缘GPU（如NVIDIA Orin）上的多个硬件引擎来处理视频解码、人脸检测和人脸识别任务。

Result: 在NVIDIA Orin GPU上，同时使用所有硬件引擎处理真实视频流，实现了更高的吞吐量，节省了约300mW（约5%）的功耗，并满足实时性能约束。处理多个视频流时性能进一步提升。

Conclusion: 同时利用边缘GPU的多个硬件引擎能有效提升视频人脸处理的性能和能效。论文还指出Tensor RT框架产生的shuffle层数量对性能有影响，并建议对现有边缘GPU处理器进行硬件改进以进一步提升性能。

Abstract: Video face detection and recognition in public places at the edge is required
in several applications, such as security reinforcement and contactless access
to authorized venues. This paper aims to maximize the simultaneous usage of
hardware engines available in edge GPUs nowadays by leveraging the concurrency
and pipelining of tasks required for face detection and recognition. This also
includes the video decoding task, which is required in most face monitoring
applications as the video streams are usually carried via Gbps Ethernet
network. This constitutes an improvement over previous works where the tasks
are usually allocated to a single engine due to the lack of a unified and
automated framework that simultaneously explores all hardware engines. In
addition, previously, the input faces were usually embedded in still images or
within raw video streams that overlook the burst delay caused by the decoding
stage. The results on real-life video streams suggest that simultaneously using
all the hardware engines available in the recent NVIDIA edge Orin GPU, higher
throughput, and a slight saving of power consumption of around 300 mW,
accounting for around 5%, have been achieved while satisfying the real-time
performance constraint. The performance gets even higher by considering several
video streams simultaneously. Further performance improvement could have been
obtained if the number of shuffle layers that were created by the tensor RT
framework for the face recognition task was lower. Thus, the paper suggests
some hardware improvements to the existing edge GPU processors to enhance their
performance even higher.

</details>


### [78] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/abs/2505.04512)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Sen Liang,Yuan Zhou,Qin Lin,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出HunyuanCustom，一个多模态定制视频生成框架，强调主体一致性并支持图像、音频、视频和文本条件。


<details>
  <summary>Details</summary>
Motivation: 现有定制视频生成方法在身份一致性和输入模态多样性方面存在局限，难以满足灵活的用户定义条件。

Method: 基于HunyuanVideo构建，针对图文条件引入LLaVA文本-图像融合模块和时间拼接图像ID增强模块；针对音视频条件，提出AudioNet模块（空间交叉注意力）和视频驱动注入模块（patchify特征对齐）。

Result: HunyuanCustom在单主体和多主体场景中，其ID一致性、真实感和文本-视频对齐方面均显著优于现有SOTA方法，并在音频和视频驱动的下游任务中表现出鲁棒性。

Conclusion: 多模态条件化和身份保持策略能有效推动可控视频生成技术的发展，HunyuanCustom证明了其有效性。

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>


### [79] [Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model](https://arxiv.org/abs/2505.04522)
*Pengfei Guo,Can Zhao,Dong Yang,Yufan He,Vishwesh Nath,Ziyue Xu,Pedro R. A. S. Bassi,Zongwei Zhou,Benjamin D. Simon,Stephanie Anne Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: 提出 Text2CT，一种使用扩散模型从自由文本描述生成 3D CT 容积的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理自由格式文本输入以生成 3D CT 图像；利用自由文本生成 CT 在诊断和研究中有巨大潜力。

Method: 采用 Text2CT 框架：通过新颖的提示词公式（prompt formulation），将自由医学文本编码为潜在表示，再利用扩散模型解码生成高分辨率 3D CT 扫描。

Result: Text2CT 在保持解剖保真度和捕捉文本描述细节方面表现优越，评估表明其达到了最先进水平。

Conclusion: 该方法为医学诊断和数据增强提供了有前景的应用潜力。

Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a
transformative opportunity in diagnostics and research. In this paper, we
introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual
descriptions using the diffusion model. Unlike previous methods that rely on
fixed-format text input, Text2CT employs a novel prompt formulation that
enables generation from diverse, free-text descriptions. The proposed framework
encodes medical text into latent representations and decodes them into
high-resolution 3D CT scans, effectively bridging the gap between semantic text
inputs and detailed volumetric representations in a unified 3D framework. Our
method demonstrates superior performance in preserving anatomical fidelity and
capturing intricate structures as described in the input text. Extensive
evaluations show that our approach achieves state-of-the-art results, offering
promising potential applications in diagnostics, and data augmentation.

</details>


### [80] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 本文提出了一种在NVIDIA Jetson AGX Orin上优化人脸检测与识别系统的软硬件协同设计方法，通过同时利用所有硬件引擎并集成人脸跟踪器，显著提升了处理吞吐量并降低了功耗。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸检测与识别系统虽然性能高，但在边缘设备上的吞吐量和功耗仍有改进空间，尤其对于公共场所的实时监控应用至关重要。

Method: 1. 在NVIDIA Jetson AGX Orin平台上，采用软硬件协同方法，同时利用其所有硬件引擎（而非传统上仅CPU或GPU核心）进行处理以优化处理时间。2. 集成人脸跟踪模块，避免对每一帧重复运行人脸识别算法，仅在新面孔出现时才运行识别。

Result: 实验表明，在1920x1080分辨率、平均每帧6张人脸的输入下，该方法实现了290 FPS的高吞吐量。与仅使用CPU/GPU引擎且未集成跟踪器的方案相比，功耗降低了约800毫瓦。

Conclusion: 这种软硬件协同设计方法为在边缘设备上构建高性能、低功耗的机器视觉系统铺平了道路，尤其适用于公共场所的多摄像头视频监控场景。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [81] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种名为DFVO的新型网络，用于可见光与红外图像融合，通过一体化处理解耦与融合，在低光照条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有可见光与红外图像融合方法在处理光照严重退化的可见光图像时，融合结果模糊暗淡，对自动驾驶等高级视觉任务构成挑战。

Method: 提出了一种名为DFVO（Darkness-Free network for Visible and infrared image disentanglement and fusion all at Once）的网络。该网络采用级联多任务方法，一体化处理图像解耦与融合，以替代传统的两阶段（先增强后融合）训练，避免信息熵损失。具体包括：构建潜在共同特征提取器（LCFE）；设计细节提取模块（DEM）获取高频语义信息；设计超交叉注意力模块（HCAM）提取低频信息并保留纹理特征；设计了相关的损失函数指导网络学习。

Result: 实验表明，所提方法在定性和定量评估上均优于现有SOTA方法。特别是在黑暗环境下，DFVO能生成更清晰、信息更丰富、光照更均匀的融合结果。在LLVIP数据集上取得了63.258 dB的PSNR和0.724的CC，达到了最佳性能。

Conclusion: DFVO网络能有效处理可见光与红外图像融合问题，尤其在暗光环境下表现优越，为高级视觉任务提供了更有效的信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [82] [RAFT: Robust Augmentation of FeaTures for Image Segmentation](https://arxiv.org/abs/2505.04529)
*Edward Humes,Xiaomin Lin,Uttej Kallakuri,Tinoosh Mohsenin*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RAFT的新框架，用于解决图像分割中合成数据到真实场景（Syn2Real）的适应性问题。RAFT通过数据增强、特征增强和主动学习，使用最少的真实世界标记数据来调整模型，并在多个基准测试中超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 图像分割技术在场景理解中非常强大，但其在真实世界的部署受到对高质量、精细标注数据集的依赖所阻碍。合成数据虽能提供高质量标签并减少人工数据收集和标注的需求，但基于合成数据训练的深度神经网络在真实世界部署时常面临Syn2Real问题，导致性能不佳。

Method: 提出RAFT框架，一种利用数据增强、特征增强以及主动学习技术，通过最少量已标注的真实世界数据来调整图像分割模型的新方法。

Result: 在SYNTHIA->Cityscapes基准测试中，RAFT在域适应后使mIoU提升了2.1%，达到了79.9%。在GTAV->Cityscapes基准测试中，mIoU提升了0.4%，达到了78.2%。这两种情况下均超越了先前的SOTA方法HALO。此外，在真实到真实基准Cityscapes->ACDC的测试中，RAFT同样超越了HALO，适应后mIoU提升了1.3%，达到了73.2%。论文还研究了标注预算和RAFT各组件对最终迁移mIoU的影响。

Conclusion: RAFT框架能够有效缓解图像分割中的Syn2Real问题，通过利用极少量真实标注数据，显著提升模型在真实场景中的性能，并优于此前的最佳方法。

Abstract: Image segmentation is a powerful computer vision technique for scene
understanding. However, real-world deployment is stymied by the need for
high-quality, meticulously labeled datasets. Synthetic data provides
high-quality labels while reducing the need for manual data collection and
annotation. However, deep neural networks trained on synthetic data often face
the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a
novel framework for adapting image segmentation models using minimal labeled
real-world data through data and feature augmentations, as well as active
learning. To validate RAFT, we perform experiments on the synthetic-to-real
"SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass
the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an
improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes
experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach
on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO,
with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the
effect of the allocated annotation budget and various components of RAFT upon
the final transfer mIoU.

</details>


### [83] [Registration of 3D Point Sets Using Exponential-based Similarity Matrix](https://arxiv.org/abs/2505.04540)
*Ashutosh Singandhupe,Sanket Lokhande,Hung Manh La*

Main category: cs.CV

TL;DR: 提出了一种改进的ICP算法（ESM-ICP），通过动态指数加权相似性矩阵，有效解决点云配准中大旋转差异和传感器噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的点云配准技术在处理具有大旋转差异的点云或受传感器噪声严重损坏的数据时常常遇到困难，导致配准错误和三维重建不准确。

Method: 提出一种名为指数相似性矩阵ICP (ESM-ICP) 的方法，该方法对经典ICP算法进行修改，集成了一个高斯启发的指数加权方案来构建一个在迭代过程中动态调整的相似性矩阵，以改进旋转和平移分量的估计。

Result: ESM-ICP在处理源点云和目标点云之间存在大旋转差异以及数据被非高斯噪声损坏的两种挑战性场景中，其性能优于传统几何配准技术以及几种近期的基于学习的方法。

Conclusion: ESM-ICP是一种鲁棒的点云配准算法，能有效克服大旋转差异和传感器噪声的限制，提高了配准的准确性。该方法的完整实现已在GitHub上公开。

Abstract: Point cloud registration is a fundamental problem in computer vision and
robotics, involving the alignment of 3D point sets captured from varying
viewpoints using depth sensors such as LiDAR or structured light. In modern
robotic systems, especially those focused on mapping, it is essential to merge
multiple views of the same environment accurately. However, state-of-the-art
registration techniques often struggle when large rotational differences exist
between point sets or when the data is significantly corrupted by sensor noise.
These challenges can lead to misalignments and, consequently, to inaccurate or
distorted 3D reconstructions. In this work, we address both these limitations
by proposing a robust modification to the classic Iterative Closest Point (ICP)
algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),
integrates a Gaussian-inspired exponential weighting scheme to construct a
similarity matrix that dynamically adapts across iterations. This matrix
facilitates improved estimation of both rotational and translational components
during alignment. We demonstrate the robustness of ESM-ICP in two challenging
scenarios: (i) large rotational discrepancies between the source and target
point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show
that ESM-ICP outperforms traditional geometric registration techniques as well
as several recent learning-based methods. To encourage reproducibility and
community engagement, our full implementation is made publicly available on
GitHub. https://github.com/aralab-unr/ESM_ICP

</details>


### [84] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种名为KA-Prompt的新型基于提示的领域增量学习方法，通过组件感知的提示-知识对齐，解决了现有方法中因组件错位导致的知识冲突问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的领域增量学习方法中，领域特定提示之间的组件错位会导致知识集成冲突和预测性能下降，因为知识组件在提示中的位置是随机的，不相关的组件融合会引入干扰。

Method: 提出了组件化提示-知识对齐（KA-Prompt）方法，包含两个阶段：1）初始组件结构配置：通过贪婪搜索挖掘与新领域相关的旧提示，用于初始化新提示，实现知识复用和初始对齐。2）在线对齐保持：动态识别目标旧提示，并在新提示演化时应用自适应的组件一致性约束。

Result: 在领域增量学习基准测试上的大量实验证明了KA-Prompt方法的有效性。

Conclusion: KA-Prompt通过在训练过程中引入组件感知的提示-知识对齐，显著提升了模型在领域增量学习中的学习和推理能力，有效解决了提示组件错位导致的问题。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [85] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 该研究提出了一种新的多目标强化学习框架，用于从欠采样的k空间数据中进行全面、顺序的诊断评估，旨在推动MRI成为经济实惠的即时诊断（PoC）设备。


<details>
  <summary>Details</summary>
Motivation: MRI成本高、复杂性大，限制了其作为即时诊断（PoC）设备的应用。现有研究虽能从少量样本进行单一诊断，但实现PoC MRI需要以最少样本进行多个顺序诊断决策。

Method: 提出了一种新的多目标强化学习框架，该框架在推理过程中能主动适应顺序决策以优化采样。训练方法采用逐步加权奖励函数，识别对每个诊断目标贡献最大的样本。

Result: 该框架在膝关节病理评估任务（ACL扭伤检测和软骨厚度损失评估）中，在疾病检测、严重程度量化和整体顺序诊断方面取得了与基准方法相当的性能，同时显著减少了k空间样本量。

Conclusion: 该方法为MRI发展成为一种全面且经济实惠的即时诊断（PoC）设备铺平了道路。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


### [86] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/abs/2505.04594)
*Zhihao Zhang,Abhinav Kumar,Girish Chandar Ganesan,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出MonoCoP，一种受LLM中思维链启发的单目3D目标检测方法，通过顺序和条件性预测3D属性（尤其是深度）来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D目标检测方法在预测深度时，忽略了3D属性间的内在关联性，这限制了深度预测的准确性和整体稳定性。

Method: 提出了MonoCoP方法，利用“预测链”（Chain-of-Prediction, CoP）顺序地、条件性地预测3D属性。具体设计包括：1) 为每个3D属性使用轻量级AttributeNet (AN)学习特定特征；2) 构建显式链条传播学习到的特征；3) 使用残差连接聚合链上各属性特征，确保后续属性预测基于先前所有属性。

Result: MonoCoP在KITTI排行榜上无需额外数据即达到SOTA水平，并在Waymo和nuScenes前视数据上超越了现有方法。

Conclusion: MonoCoP通过利用属性间的相互依赖关系，有效提升了单目3D目标检测的准确性和稳定性，其提出的“预测链”策略是有效的。

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [87] [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/abs/2505.04601)
*Xianhang Li,Yanqing Liu,Haoqin Tu,Hongru Zhu,Cihang Xie*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 OpenVision 的全开放、高性价比的视觉编码器系列，其性能可媲美或超越 OpenAI 的 CLIP。


<details>
  <summary>Details</summary>
Motivation: 现有的高性能视觉编码器（如 CLIP 的替代品）并非完全开放，其训练数据或训练方法不公开，本研究旨在填补这一空白。

Method: OpenVision 基于现有工作（如使用 CLIPS 训练框架和 Recap-DataComp-1B 训练数据），并在此基础上揭示了提升编码器质量的关键见解。

Result: OpenVision 在集成到 LLaVA 等多模态框架时，其性能与 OpenAI 的 CLIP 相当或更优。该研究发布了参数量从 5.9M 到 632.1M 不等的多种视觉编码器，提供了容量和效率的灵活权衡。

Conclusion: OpenVision 为从业者提供了一系列完全开放、性能优越且具有不同参数规模的视觉编码器，为构建多模态模型提供了灵活的选择，并推动了多模态模型的发展。

Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of
vision encoder for building multimodal foundation models. Although recent
alternatives such as SigLIP have begun to challenge this status quo, to our
knowledge none are fully open: their training data remains proprietary and/or
their training recipes are not released. This paper fills this gap with
OpenVision, a fully-open, cost-effective family of vision encoders that match
or surpass the performance of OpenAI's CLIP when integrated into multimodal
frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for
training framework and Recap-DataComp-1B for training data -- while revealing
multiple key insights in enhancing encoder quality and showcasing practical
benefits in advancing multimodal models. By releasing vision encoders spanning
from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible
trade-off between capacity and efficiency in building multimodal models: larger
models deliver enhanced multimodal performance, while smaller versions enable
lightweight, edge-ready multimodal deployments.

</details>


### [88] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/abs/2505.04612)
*Jiahao Li,Haochen Wang,Muhammad Zubair Irshad,Igor Vasiljevic,Matthew R. Walter,Vitor Campagnolo Guizilini,Greg Shakhnarovich*

Main category: cs.CV

TL;DR: 提出了一种名为FastMap的快速、简洁的全局运动恢复结构（SfM）方法。它通过GPU友好操作和高效优化，在大型场景下比现有方法快一到两个数量级，且精度相当。


<details>
  <summary>Details</summary>
Motivation: 现有的全局SfM方法（如COLMAP和GLOMAP）虽然精度高，但在处理大量匹配关键点对时可扩展性差，主要原因是并行化不足和优化步骤计算成本高昂。

Method: 设计了一个完全依赖GPU友好操作的SfM框架，使其易于并行化。其优化步骤的运行时间与图像对数量成线性关系，且与关键点对或3D点数量无关。

Result: 实验表明，FastMap在大型场景中的处理速度比COLMAP和GLOMAP快一到两个数量级，并且相机姿态估计精度相当。

Conclusion: FastMap成功地提高了全局SfM方法的速度和可扩展性，为大规模场景重建提供了一个高效且准确的解决方案。

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is one to two orders of magnitude faster than
COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.

</details>


### [89] [Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait](https://arxiv.org/abs/2505.04616)
*Feng Liu,Nicholas Chimitt,Lanqing Guo,Jitesh Jain,Aditya Kane,Minchul Kim,Wes Robbins,Yiyang Su,Dingqiang Ye,Xingguang Zhang,Jie Zhu,Siddharth Satyakam,Christopher Perry,Stanley H. Chan,Arun Ross,Humphrey Shi,Zhangyang Wang,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出FarSight系统，一个集成了人脸、步态和体型多模态生物特征的端到端全身行人识别统一框架，专为在远距离、高视角和恶劣大气条件等非受限监控环境中有效识别人体而设计，并在BRIAR数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决在非受限环境下进行全身行人识别的难题，尤其是在如IARPA BRIAR项目所面临的远距离、高视角和恶劣大气条件（如湍流和强风速）下，生物特征数据采集质量差的问题。

Method: 提出了一个名为FarSight的统一端到端系统。该系统整合了人脸、步态和体型三种互补的生物特征线索，并包含四个核心模块：多目标检测与跟踪、识别感知的视频恢复、特定模态的生物特征编码以及质量引导的多模态融合。这些组件被设计用于在图像质量下降、姿态和尺度变化大以及跨域差异等挑战下协同工作。

Result: 在BRIAR数据集上的大量实验表明，FarSight系统效果显著。与初步系统相比，该系统在1:1验证准确率（TAR@0.1% FAR）上实现了34.1%的绝对增益，在闭集识别（Rank-20）中提升了17.8%，在开集识别错误（FNIR@1% FPIR）上减少了34.3%。此外，FarSight系统还在2025 NIST RTE FIVE评估中得到了验证。

Conclusion: FarSight系统被证实是一种在具有挑战性的真实世界条件下进行操作性生物特征识别的先进解决方案。

Abstract: We address the problem of whole-body person recognition in unconstrained
environments. This problem arises in surveillance scenarios such as those in
the IARPA Biometric Recognition and Identification at Altitude and Range
(BRIAR) program, where biometric data is captured at long standoff distances,
elevated viewing angles, and under adverse atmospheric conditions (e.g.,
turbulence and high wind velocity). To this end, we propose FarSight, a unified
end-to-end system for person recognition that integrates complementary
biometric cues across face, gait, and body shape modalities. FarSight
incorporates novel algorithms across four core modules: multi-subject detection
and tracking, recognition-aware video restoration, modality-specific biometric
feature encoding, and quality-guided multi-modal fusion. These components are
designed to work cohesively under degraded image conditions, large pose and
scale variations, and cross-domain gaps. Extensive experiments on the BRIAR
dataset, one of the most comprehensive benchmarks for long-range, multi-modal
biometric recognition, demonstrate the effectiveness of FarSight. Compared to
our preliminary system, this system achieves a 34.1% absolute gain in 1:1
verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set
identification (Rank-20), and a 34.3% reduction in open-set identification
errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE
Face in Video Evaluation (FIVE), which conducts standardized face recognition
testing on the BRIAR dataset. These results establish FarSight as a
state-of-the-art solution for operational biometric recognition in challenging
real-world conditions.

</details>


### [90] [On Path to Multimodal Generalist: General-Level and General-Bench](https://arxiv.org/abs/2505.04620)
*Hao Fei,Yuan Zhou,Juncheng Li,Xiangtai Li,Qingshan Xu,Bobo Li,Shengqiong Wu,Yaoting Wang,Junbao Zhou,Jiahao Meng,Qingyu Shi,Zhiyuan Zhou,Liangtao Shi,Minghe Gao,Daoan Zhang,Zhiqi Ge,Weiming Wu,Siliang Tang,Kaihang Pan,Yaobo Ye,Haobo Yuan,Tao Zhang,Tianjie Ju,Zixiang Meng,Shilin Xu,Liyu Jia,Wentao Hu,Meng Luo,Jiebo Luo,Tat-Seng Chua,Shuicheng Yan,Hanwang Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一个名为General-Level的评估框架和General-Bench基准，用于更全面地评估多模态大语言模型（MLLM）的通用性和在不同任务、模态间的协同能力。


<details>
  <summary>Details</summary>
Motivation: 现有对MLLM的评估方法可能无法真实反映模型的能力以及其向通用人工智能（AGI）发展的程度，高分并不直接等同于更强的通用能力。

Method: 提出了General-Level五级评估框架和Synergy（协同性）概念来衡量模型在理解与生成、多模态间的能力一致性；并构建了包含700多个任务和32.5万实例的General-Bench基准。

Result: 对超过100个现有SOTA MLLM进行了评估，揭示了通用模型的能级排行，并强调了实现真正人工智能（AGI）所面临的挑战。

Conclusion: 该项目为评估下一代多模态基础模型提供了坚实的基础设施，有望指导未来研究并加速AGI的实现。

Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid
growth, driven by the advanced capabilities of LLMs. Unlike earlier
specialists, existing MLLMs are evolving towards a Multimodal Generalist
paradigm. Initially limited to understanding multiple modalities, these models
have advanced to not only comprehend but also generate across modalities. Their
capabilities have expanded from coarse-grained to fine-grained multimodal
understanding and from supporting limited modalities to arbitrary ones. While
many benchmarks exist to assess MLLMs, a critical question arises: Can we
simply assume that higher performance across tasks indicates a stronger MLLM
capability, bringing us closer to human-level AI? We argue that the answer is
not as straightforward as it seems. This project introduces General-Level, an
evaluation framework that defines 5-scale levels of MLLM performance and
generality, offering a methodology to compare MLLMs and gauge the progress of
existing systems towards more robust multimodal generalists and, ultimately,
towards AGI. At the core of the framework is the concept of Synergy, which
measures whether models maintain consistent capabilities across comprehension
and generation, and across multiple modalities. To support this evaluation, we
present General-Bench, which encompasses a broader spectrum of skills,
modalities, formats, and capabilities, including over 700 tasks and 325,800
instances. The evaluation results that involve over 100 existing
state-of-the-art MLLMs uncover the capability rankings of generalists,
highlighting the challenges in reaching genuine AI. We expect this project to
pave the way for future research on next-generation multimodal foundation
models, providing a robust infrastructure to accelerate the realization of AGI.
Project page: https://generalist.top/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/abs/2505.03770)
*Mouad Abrini,Omri Abend,Dina Acklin,Henny Admoni,Gregor Aichinger,Nitay Alon,Zahra Ashktorab,Ashish Atreja,Moises Auron,Alexander Aufreiter,Raghav Awasthi,Soumya Banerjee,Joe M. Barnby,Rhea Basappa,Severin Bergsmann,Djallel Bouneffouf,Patrick Callaghan,Marc Cavazza,Thierry Chaminade,Sonia Chernova,Mohamed Chetouan,Moumita Choudhury,Axel Cleeremans,Jacek B. Cywinski,Fabio Cuzzolin,Hokin Deng,N'yoma Diamond,Camilla Di Pasquasio,Guillaume Dumas,Max van Duijn,Mahapatra Dwarikanath,Qingying Gao,Ashok Goel,Rebecca Goldstein,Matthew Gombolay,Gabriel Enrique Gonzalez,Amar Halilovic,Tobias Halmdienst,Mahimul Islam,Julian Jara-Ettinger,Natalie Kastel,Renana Keydar,Ashish K. Khanna,Mahdi Khoramshahi,JiHyun Kim,MiHyeon Kim,YoungBin Kim,Senka Krivic,Nikita Krasnytskyi,Arun Kumar,JuneHyoung Kwon,Eunju Lee,Shane Lee,Peter R. Lewis,Xue Li,Yijiang Li,Michal Lewandowski,Nathan Lloyd,Matthew B. Luebbers,Dezhi Luo,Haiyun Lyu,Dwarikanath Mahapatra,Kamal Maheshwari,Mallika Mainali,Piyush Mathur,Patrick Mederitsch,Shuwa Miura,Manuel Preston de Miranda,Reuth Mirsky,Shreya Mishra,Nina Moorman,Katelyn Morrison,John Muchovej,Bernhard Nessler,Felix Nessler,Hieu Minh Jord Nguyen,Abby Ortego,Francis A. Papay,Antoine Pasquali,Hamed Rahimi,Charumathi Raghu,Amanda Royka,Stefan Sarkadi,Jaelle Scheuerman,Simon Schmid,Paul Schrater,Anik Sen,Zahra Sheikhbahaee,Ke Shi,Reid Simmons,Nishant Singh,Mason O. Smith,Ramira van der Meulen,Anthia Solaki,Haoran Sun,Viktor Szolga,Matthew E. Taylor,Travis Taylor,Sanne Van Waveren,Juan David Vargas,Rineke Verbrugge,Eitan Wagner,Justin D. Weisz,Ximing Wen,William Yeoh,Wenlong Zhang,Michelle Zhao,Shlomo Zilberstein*

Main category: cs.AI

TL;DR: 该文集收录了2025年AAAI“通过心智理论推进人工智能”研讨会的精选论文。


<details>
  <summary>Details</summary>
Motivation: 为心智理论（ToM）和人工智能（AI）研究社区提供一个开放获取和精心策划的论文选集。

Method: 汇编在特定研讨会上展示的论文。

Result: 出版了一本关于心智理论与人工智能的开放获取论文集。

Conclusion: 该论文集为ToM和AI领域的研究者提供了有价值的学术资源。

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>


### [92] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: 开发了一款手写矩阵识别与计算过程分步可视化系统，旨在帮助学生理解抽象数学公式和复杂计算。


<details>
  <summary>Details</summary>
Motivation: 解决学生在学习数学时，因公式抽象、计算步骤复杂而难以理解的问题。

Method: 系统集成人工智能与可视化动画技术：引入Mamba主干网络提升手写矩阵识别精度，使用YOLO模型进行数字提取与矩阵重建，结合CoordAttention坐标注意力机制提高字符空间位置的准确性，并通过Manim动画引擎逐帧展示计算过程。

Result: 系统能精确识别手写矩阵内容，准确把握字符空间位置，并通过动画生动展示数学计算的每一步，具有高模块化和灵活性，可根据需求实时生成不同计算任务的动画。

Conclusion: 该系统作为一种直观、用户友好且高效的教育辅助工具，能够帮助学生深入理解数学运算，实现“每一步都看懂”的学习体验，具有良好的可扩展性和交互性。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


### [93] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/abs/2505.03941)
*Matan Shamir,Reuth Mirsky*

Main category: cs.AI

TL;DR: 本文提出了一种名为GRAML的新型目标识别方法，利用度量学习和孪生网络，实现了对新目标的快速适应，同时保持了高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的目标识别方法需要预定义目标集，且当出现新目标时需要耗时的重新训练，缺乏对新目标的快速适应能力。

Method: 引入GRAML（Goal Recognition As Metric Learning）方法。该方法使用孪生网络将目标识别视为一个深度度量学习任务，通过一个循环神经网络（RNN）学习一个嵌入空间上的度量，使得导向不同目标的观察序列的嵌入表示距离较远，而导向相同目标的序列嵌入表示距离较近。

Result: 在多种环境下的评估表明，GRAML相比现有SOTA（state-of-the-art）的目标识别方法，在速度、灵活性和运行时方面均有改进，同时保持了准确的识别能力，并且能有效适应新目标，即使每个新目标只有一个观察序列示例。

Conclusion: GRAML是一种有效的目标识别框架，它能够自动化模型学习过程，并能快速适应新出现的目标，解决了现有方法的局限性。

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [94] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/abs/2505.03947)
*Xiang Li,Yiyang Hao,Doug Fulop*

Main category: cs.AI

TL;DR: 该研究展示了最新的推理大型语言模型（LLM）在零样本设置下玩雅达利游戏 Frogger 的能力，并探讨了上下文学习和推理努力对性能的影响，同时提出了一种利用 LLM 演示来引导传统强化学习方法以提高性能和样本效率的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习（RL）智能体在掌握雅达利游戏等任务时训练缓慢且成本高昂，阻碍了通用智能体的开发。本研究旨在探索利用大型语言模型（LLM）来快速适应和掌握新任务的可能性。

Method: 1. 使用经过领域外强化学习后训练的最新推理 LLM，在零样本设置下玩雅达利游戏 Frogger。
2. 研究上下文学习（in-context learning）和推理努力（reasoning effort）对 LLM 性能的影响。
3. 提出一种利用 LLM 生成的演示来引导（bootstrap）传统强化学习方法。

Result: 1. LLM 能够在零样本设置下成功玩具有挑战性的雅达利游戏 Frogger。
2. 上下文学习和推理努力的增加可以提升 LLM 的游戏性能。
3. 利用 LLM 演示引导传统 RL 方法显著提高了其性能和样本效率。

Conclusion: 大型语言模型在零样本或少样本场景下展现出解决复杂强化学习任务的潜力，并且可以有效地辅助传统强化学习方法，提高其学习效率和最终性能。

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>


### [95] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
*Gerrit Großmann,Larisa Ivanova,Sai Leela Poduru,Mohaddeseh Tabrizian,Islam Mesabah,David A. Selby,Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: 本研究探讨共享叙事（故事）如何影响大型语言模型（LLM）智能体在公共物品博弈中的合作行为。


<details>
  <summary>Details</summary>
Motivation: 受尤瓦尔·赫拉利关于共享叙事驱动人类大规模合作观点的启发，本研究旨在探索此类叙事是否能同样引导LLM智能体进行合作。

Method: 研究采用有限重复公共物品博弈，让LLM智能体选择合作或自利支出策略。通过向智能体灌输不同程度强调团队合作的故事，并测试其对谈判结果的影响，探索了叙事对谈判行为的影响、共享相同与不同故事的差异、智能体数量增加时的变化以及智能体对自私谈判者的韧性。

Result: 基于故事的启动显著影响了谈判策略和成功率。共同的故事能改善合作，使每个智能体受益。相反，用不同的故事启动智能体会逆转这种效果，那些被引向自利方向的智能体占据上风。

Conclusion: 研究结果对多智能体系统设计和人工智能对齐具有潜在影响。

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [96] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/abs/2505.03985)
*Zirong Chen,Ziyan An,Jennifer Reynolds,Kristin Mullen,Stephen Martini,Meiyi Ma*

Main category: cs.AI

TL;DR: LogiDebrief是一个AI框架，通过结合信号时序逻辑(STL)和大型语言模型(LLM)自动化9-1-1呼叫的汇报评估，以提升质保效率。


<details>
  <summary>Details</summary>
Motivation: 传统的人工9-1-1呼叫质量保证（QA）方法在处理大量呼叫时效率低下，导致覆盖率低和评估延迟。

Method: LogiDebrief采用AI驱动框架，集成信号时序逻辑(STL)与大型语言模型(LLM)。它将呼叫处理要求形式化为逻辑规范，并通过三步验证过程（上下文理解、基于STL与LLM集成的运行时检查、自动生成质保报告）进行评估。

Result: LogiDebrief成功部署于纳什维尔市应急通信部门，辅助汇报了1701个真实呼叫，节省了311.85小时的人工参与时间。真实数据评估证实了其准确性，案例研究和用户研究突显了其在提升呼叫处理表现方面的有效性。

Conclusion: LogiDebrief是一个有效的AI驱动解决方案，能够自动化并改进9-1-1呼叫汇报和质量保证流程，已在实际应用中展现出显著影响和性能提升。

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>


### [97] [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)
*Marie Davidsen Buhl,Jacob Pfau,Benjamin Hilton,Geoffrey Irving*

Main category: cs.AI

TL;DR: 论文探讨了使用“辩论”机制来确保超人类AI系统的安全性，使其即使能力远超人类，也能被有效引导并保持诚实。


<details>
  <summary>Details</summary>
Motivation: 当AI系统在多种任务上达到或超越人类能力时，人类难以有效判断其行为，从而难以利用人类反馈引导AI向期望的特性发展。需要新方法来确保AI安全。

Method: 提出利用另一个超人类AI系统通过“辩论”来指出被评估AI系统输出中的缺陷。论文构建了一个“对齐安全案例”，以防止AI研发智能体（例如在AI公司内部）通过产生虚假结果等方式破坏研究。该智能体通过辩论进行训练，并保证探索性，以教会系统保持诚实，并在部署期间通过在线训练维持诚实性。

Result: 该安全案例依赖于四个关键主张：(1) 智能体在辩论游戏中表现良好；(2) 辩论游戏中的良好表现意味着系统基本诚实；(3) 系统在部署期间不会变得显著不诚实；(4) 部署环境能够容忍一些错误。论文识别了若干开放性研究问题，若能解决，则可使这一安全论证更具说服力。

Conclusion: “辩论”机制为AI安全提供了一个有价值的框架，但其有效性依赖于特定假设的成立以及对相关开放研究问题的进一步探索和解决，才能构成一个令人信服的AI安全论证。

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [98] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/abs/2505.04019)
*Matteo Ceschin,Leonardo Arrighi,Luca Longo,Sylvio Barbon Junior*

Main category: cs.AI

TL;DR: 本研究提出了一种基于决策谓词图（DPG）和内点-异常点传播分数（IOP-Score）的新型可解释人工智能（XAI）方法，用于提供孤立森林（iForest）异常检测模型的全局解释，阐明异常点识别过程和特征贡献。


<details>
  <summary>Details</summary>
Motivation: 尽管模型可解释性已受重视，但对预处理方法（如孤立森林异常检测）的理解同样关键。孤立森林因其决策过程不透明，尤其在集成多个树学习器时，难以解释异常点的选择和决策边界，因此需要全局解释方法。

Method: 研究引入了一种新的可解释人工智能（XAI）方法，通过构建决策谓词图（DPG）来阐明孤立森林这类集成方法的逻辑。同时，提出了一种基于图的度量——内点-异常点传播分数（IOP-Score），用以解释样本如何被识别为异常点。

Result: 该方法显著增强了孤立森林的可解释性，提供了对模型决策过程的全面视图，详细说明了哪些特征导致样本被识别为异常点以及模型如何使用这些特征。同时，它还揭示了决策边界，并全面展示了特征在异常点识别中的整体使用情况。

Conclusion: 该方法通过提供对决策边界的洞察和对特征在异常点识别中使用的全面视图，推动了现有技术的发展，有助于实现完全可解释的机器学习流程。

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>


### [99] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/abs/2505.04115)
*Luise Ge,Brendan Juba,Kris Nilsson*

Main category: cs.AI

TL;DR: 该论文提出了一种新的一阶关系概率推理方法，平衡了表达能力和计算可处理性，即使在对象数量未知或无限时也能在多项式时间内完成推理。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域在不确定性下的推理面临着语言表达能力与计算问题可处理性之间的尖锐矛盾。

Method: 受人类推理启发，将期望的平方和逻辑（sum-of-squares logic of expectation）扩展到关系设置，用于处理混合（离散和连续）变量的一阶关系概率推理。该方法在有界度片段和有界量词秩的知识库中进行提升推理（lifted reasoning）。

Result: 证明了即使在对象集合先验未知或可数无限的情况下，对于有界度片段和有界量词秩的知识库，提升推理也可以在多项式时间内执行。能够推导出给定度和大小的证明所能证明的最紧界限，并为固定度数下的平方和反驳（sum-of-squares refutations）建立了完备性。

Conclusion: 该研究引入了一种有效的一阶关系概率推理方法，它同时满足了表达能力和计算可处理性的要求，能够在复杂条件下（如混合变量、对象数量未知或无限）实现多项式时间推理，并将可处理性的概念框架化于证明论术语中。

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>


### [100] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/abs/2505.04310)
*Simo Alami C.,Rim Kaddah,Jesse Read,Marie-Paule Cani*

Main category: cs.AI

TL;DR: 论文提出了一种使用归一化流的新型分布式强化学习架构，能够灵活、无界地建模回报分布，并在ATARI-5基准测试中表现优于基于PDF的方法，且与分位数方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式强化学习方法在回报分布的表示上存在局限（如C51的固定或有界表示），难以捕捉分布的复杂特性（如多模态、偏度、尾部行为），且参数效率不高。同时，常用的训练指标（如KL散度、Wasserstein距离）存在尺度不敏感或样本梯度有偏的问题。

Method: 1. 提出一种新的分布式强化学习架构，使用归一化流 (normalizing flows) 对回报分布进行建模，以实现灵活和无界的分布支持。2. 提出一种新颖的克拉默距离 (Cramér distance) 的代理指标，该指标具有几何感知能力，可直接从回报分布的概率密度函数 (PDF) 计算，避免了昂贵的累积分布函数 (CDF) 计算。

Result: 在ATARI-5子基准测试中，该方法优于基于概率密度函数 (PDF) 的模型，并且与基于分位数的方法相比具有竞争力。

Conclusion: 该研究提出的基于归一化流的分布式强化学习架构及其新的训练指标，为回报分布建模提供了一种更灵活、参数效率更高且性能优越的方法。

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>


### [101] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/abs/2505.04313)
*Stephen Richard Varey,Alessandro Di Stefano,The Anh Han*

Main category: cs.AI

TL;DR: 本文介绍 KERAIA，一个用于符号知识工程的新型框架和软件平台，旨在解决动态、复杂和情境敏感环境中知识表示、推理和执行的挑战。


<details>
  <summary>Details</summary>
Motivation: 核心研究问题是如何将非结构化、通常是隐性的人类专业知识有效地转化为人工智能系统可以高效利用的计算可处理算法，以弥合人类专业知识与 AI 系统之间的差距。

Method: KERAIA 建立在 Minsky 框架推理和 K-lines 等概念之上，并引入创新，如用于动态聚合的知识云、用于情境敏感继承的动态关系 (DRels)、用于可追溯推理的显式思维链 (LoTs) 以及用于自适应知识转换的云阐述。该框架以可解释性人工智能 (XAI) 为核心原则，并包含 KSYNTH 表示语言和通用范式构建器 (GPPB)。

Result: 通过在海军作战模拟、水处理厂工业诊断和 RISK 游戏战略决策等多个案例研究中进行详细分析，验证了 KERAIA 的多功能性、表达能力和实际应用性。此外，还与已建立的知识表示范式进行了比较分析。

Conclusion: KERAIA 提供了一个新颖且以可解释性为核心的符号知识工程方法，能够有效应对动态复杂环境中的知识挑战，超越了传统静态知识表示范式的局限性，并展示了其在转化人类专业知识为 AI 可用算法方面的潜力。

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>


### [102] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/abs/2505.04317)
*Ruize Zhang,Sirui Xiang,Zelai Xu,Feng Gao,Shilong Ji,Wenhao Tang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本文提出了一种分层强化学习框架（HCSP），用于解决3v3多无人机排球这一新型具身对抗任务，实现了从零开始学习高层策略和底层控制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决3v3多无人机排球这一新型具身对抗任务中的挑战，该任务需要高层策略协调和底层敏捷控制，并具有长时序依赖、紧密智能体耦合和欠驱动动力学等难点。

Method: 使用方法是提出分层协同自博弈（HCSP）框架，将中心化的高层策略决策与去中心化的底层运动控制分离。设计了一个三阶段的基于种群的训练流程：(I) 训练多样化的底层技能，(II) 通过固定底层控制器的自博弈学习高层策略，(III) 通过协同自博弈进行联合微调。

Result: 研究结果表明，HCSP取得了优越性能，胜过非分层自博弈和基于规则的分层基线（平均胜率82.9%），并以71.5%的胜率击败了两阶段变体。协同自博弈还促使了角色切换和协调编队等涌现性团队行为。

Conclusion: 结论是，该研究提出的HCSP分层设计和训练方案能有效应对复杂的多无人机排球任务，并能学习到高级别的团队协作策略。

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>


### [103] [Uncertain Machine Ethics Planning](https://arxiv.org/abs/2505.04352)
*Simon Kolker,Louise A. Dennis,Ramon Fraga Pereira,Mengwei Xu*

Main category: cs.AI

TL;DR: 本文研究机器在不确定性下，如何综合多种冲突的道德理论进行序贯决策，以达成长期更优的伦理结果。


<details>
  <summary>Details</summary>
Motivation: 机器伦理决策需考虑决策的不确定性、行动序列的长期影响，以及不同道德理论（如功利主义、道义论、美德伦理）可能产生的冲突性判断和不同的情境表征需求。

Method: 将问题形式化为多道德马尔可夫决策过程 (Multi-Moral Markov Decision Process) 和多道德随机最短路径问题 (Multi-Moral Stochastic Shortest Path Problem)。开发了一种基于多目标AO*的启发式算法，并利用Sven-Ove Hansson的假设性回溯程序 (Hypothetical Retrospection procedure) 进行不确定性下的伦理推理。

Result: 通过机器伦理文献中的一个案例研究（是否为需要者偷窃胰岛素的问题）验证了所提出方法的有效性。

Conclusion: 本文提出的方法为机器在不确定和多道德考量冲突的环境下做出决策提供了一种形式化框架和启发式算法支持。

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>


### [104] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2505.04480)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.AI

TL;DR: 本文提出TrajEvo框架，利用大语言模型（LLMs）自动设计轨迹预测启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统手工规则的启发式方法精度不足，而深度学习方法计算成本高、缺乏可解释性且泛化能力差，限制了其实际应用。

Method: TrajEvo采用进化算法，利用大语言模型（LLMs）从历史轨迹数据中生成和优化预测启发式方法。引入了跨代精英采样以促进种群多样性，并设计了统计反馈回路让LLM分析备选预测。

Result: TrajEvo在ETH-UCY数据集上优于先前的启发式方法，并且在泛化到未见过的SDD数据集时，其性能显著优于启发式和深度学习方法。

Conclusion: TrajEvo为自动设计快速、可解释且具有良好泛化能力的轨迹预测启发式方法迈出了第一步。

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>


### [105] [On some improvements to Unbounded Minimax](https://arxiv.org/abs/2505.04525)
*Quentin Cohen-Solal,Tristan Cazenave*

Main category: cs.AI

TL;DR: 本文首次实验评估了无界最佳优先Minimax算法的四种先前未测试的修改，并分析了它们对算法效率的影响。


<details>
  <summary>Details</summary>
Motivation: 评估并验证先前未曾测试过的针对无界最佳优先Minimax算法的修改方案，以期提升该博弈树搜索算法的效率。

Method: 通过实验评估了四种修改：1) 使用置换表；2) 比较原始算法与Cohen-Solal变体的不同反向传播策略；3) 用学习的启发式函数替换精确的终端评估函数；4) 应用优先处理已解决获胜状态并避免已解决失败状态的完成技术。

Result: 置换表和完成技术均能提升算法性能。Cohen-Solal的反向传播策略在涉及值平局或置换表时略微提高性能。学习的启发式函数在精确评估代价高昂时有益，但在代价低廉的场景下会降低性能。

Conclusion: 研究结果表明，有针对性的修改可以有效增强无界最佳优先Minimax算法的效率。

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>


### [106] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
*Qi Liu,Xinhao Zheng,Renqiu Xia,Xingzhi Qi,Qinxiang Cao,Junchi Yan*

Main category: cs.AI

TL;DR: 该研究提出了将问题解决形式化为确定性马尔可夫决策过程的方法，并开发了FPS及D-FPS框架，利用形式化定理证明环境实现过程可验证的问题解决。研究还构建了新基准测试并提出RPE评估方法，实验表明现有模型在这些任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对问题解决的通用且具体的表述，同时随着基于AI的问题解决代理的发展，对过程级可验证性的需求迅速增加但研究不足。

Method: 1. 将问题解决表述为一个确定性马尔可夫决策过程。
2. 提出FPS（形式化问题解决）框架，利用现有的FTP（形式化定理证明）环境进行过程可验证的问题解决。
3. 提出D-FPS（演绎式FPS）框架，将求解与答案验证分离，以更好地与人类对齐。
4. 证明了所提出框架的表达性、健全性和完备性。
5. 构建了三个问题解决基准：FormalMath500、MiniF2F-Solving 和 PutnamBench-Solving。
6. 提出了RPE（受限命题等价）方法，一种通过形式验证确定答案正确性的符号方法。

Result: 1. FPS和D-FPS框架的表达性、健全性和完备性得到证明。
2. 在新构建的基准测试上，四种流行的FTP模型和两种提示方法作为基线，在FormalMath500上最高解决了23.77%，在MiniF2F-Solving上最高解决了27.47%，在PutnamBench-Solving上最高解决了0.31%。

Conclusion: 本文为问题解决提供了一个原则性的形式化表述和可验证的框架（FPS和D-FPS），并引入了新的基准和评估方法。实验结果显示，现有AI模型在过程可验证的问题解决方面仍有很大提升空间，表明了新框架和基准的重要性。

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>


### [107] [Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs](https://arxiv.org/abs/2505.04539)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Kafshdar Goharshady,Mehrdad Karrabi,Ali Shafiee*

Main category: cs.AI

TL;DR: 论文研究了鲁棒马尔可夫决策过程 (RMDP) 中可达性和奇偶校验目标的定性分析问题，提出了无需结构假设、基于预言机的高效算法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 经典马尔可夫决策过程 (MDP) 假设转移概率精确，而鲁棒马尔可夫决策过程 (RMDP) 通过定义一组可能的转移函数来处理转移概率的不确定性。本研究旨在解决在没有任何结构假设的 RMDP 中，如何确定是否能以概率1保证可达性或奇偶校验目标的问题。

Method: 针对没有任何结构假设（如单链或非周期性）的 RMDP，开发了基于预言机访问不确定性集合的高效算法，用于解决可达性和奇偶校验目标的定性分析问题（即判断目标是否能以概率1实现）。

Result: 实验结果表明，所提出的基于预言机的方法在处理文献中的经典 RMDP 示例时表现出有效性，并且能够成功扩展到包含数千个状态的较大规模问题。

Conclusion: 该研究为 RMDP 中可达性和奇偶校验目标的定性分析问题提供了高效的、基于预言机的算法。这些算法不依赖于 RMDP 的特定结构，并通过实验证明了其在较大规模问题上的实用性和有效性。

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/abs/2505.03774)
*Tao Yin,Chen Zhao,Xiaoyan Liu,Minglai Shao*

Main category: cs.LG

TL;DR: 提出了一种名为 OODHG 的新方法，用于检测异构图中的分布外 (OOD) 节点，并对分布内 (ID) 节点进行分类。该方法利用节点能量值、元路径能量传播和能量约束机制。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络在处理真实世界中常见的异构图及其分布外 (OOD) 节点检测方面存在不足，而异构图的 OOD 检测是一个重要且未被充分研究的领域，异构性带来了额外的复杂性和信息。

Method: 该研究提出了 OODHG 方法：1) 为异构图中的每个节点学习表示；2) 计算能量值以判断节点是否为 OOD；3) 对 ID 节点进行分类。核心创新包括引入基于元路径的能量传播机制和能量约束，以增强 ID 和 OOD 节点之间的区分度。

Result: 大量的实验结果表明，OODHG 方法简单有效，在 OOD 检测任务上显著优于基线模型，并且在 ID 节点分类方面也表现出准确性。

Conclusion: OODHG 是一种有效的异构图 OOD 检测新方法，它成功地利用了异构图的结构信息，提升了 OOD 检测和 ID 节点分类的性能。

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>


### [109] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/abs/2505.03775)
*Linqing Chen,Weilei Wang,Wentao Wu,Hanmeng Zhong*

Main category: cs.LG

TL;DR: 本文提出了一种名为概率级别约束（PLC）的生成式框架，将分层极端多标签分类任务重新定义为分层多标签生成（HMG），实现了最先进的性能和更好的输出控制。


<details>
  <summary>Details</summary>
Motivation: 传统的分层极端多标签分类因标签间复杂的层级关系和海量标签数量而面临巨大困难。现有研究或依赖复杂的多阶段处理（如聚类），或无法有效控制生成式模型的输出。

Method: 作者将任务重新定义为分层多标签生成（HMG），并提出了一种带有概率级别约束（PLC）的生成式框架。该方法能为每个文档生成所有层级的相关标签，无需聚类等预处理操作，并能精确控制模型输出的数量、长度和层级。

Result: 实验表明，该方法在HMG任务上取得了新的SOTA（state-of-the-art）性能，并且在模型输出的约束方面远优于以往的研究工作。

Conclusion: 提出的带有PLC的HMG框架通过直接、可控地生成分层标签，有效地解决了分层极端多标签分类问题，其性能和输出可控性均优于现有方法。

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>


### [110] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/abs/2505.03776)
*Hansi Denis,Siegfried Mercelis,Ngoc-Quang Luong*

Main category: cs.LG

TL;DR: 本文提出了一种名为PAPN的新型路由预测方法，该方法结合了近邻注意力机制和指针网络，用于优化最后一公里配送和第一公里取件问题，并在真实数据集上取得了优于现有监督学习方法并与强化学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 优化最后一公里配送和第一公里取件对于降低成本、提高资源效率和提升服务质量至关重要，而这需要精确的路线和时间预测系统。本研究旨在解决其中的核心问题——路线预测。

Method: 提出了一种新的PAPN（近邻注意力编码器和指针网络解码器）模型。该模型在编码器-解码器架构中引入了新颖的近邻注意力机制，并结合指针网络进行解码。它通过多头注意力转换器编码器计算全局上下文，并将其与局部嵌入聚合，实现全局和局部注意力的结合。解码过程中也使用近邻注意力来辅助预测。

Result: 在名为LaDE的大规模真实行业数据集上进行训练、验证和测试，该方法在多数基准测试指标上优于所有现有的监督学习方法，并且与表现最佳的强化学习方法DRL4Route具有竞争力。

Conclusion: 所提出的PAPN方法在最后一公里配送和第一公里取件的路线预测方面展现出显著的潜力，为该领域的优化提供了有前景的新途径。

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>


### [111] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/abs/2505.03777)
*LG AI Research,Sehyun Chun,Jiye Kim,Ahra Jo,Yeonsik Jo,Seungyul Oh,Seungjun Lee,Kwangrok Ryoo,Jongmin Lee,Seunghwan Kim,Byung Jun Kang,Soonyoung Lee,Jun Ha Park,Chanwoo Moon,Jiwon Ham,Haein Lee,Heejae Han,Jaeseung Byun,Soojong Do,Minju Ha,Dongyun Kim,Kyunghoon Bae,Woohyung Lim,Edward Hwayoung Lee,Yongmin Park,Jeongsang Yu,Gerrard Jeongwon Jo,Yeonjung Hong,Kyungjae Yoo,Sehui Han,Jaewan Lee,Changyoung Park,Kijeong Jeon,Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole：一种从文档中自动提取化学数据的新型深度学习框架，并引入了配套基准测试。


<details>
  <summary>Details</summary>
Motivation: 科学文献中化学信息提取因格式多样、非结构化和文档布局复杂而面临挑战，且缺乏标准的页面级基准测试和评估指标。

Method: 提出MolMole，一个基于视觉的深度学习框架，将分子检测、反应图解析和光学化学结构识别（OCSR）整合到单一流程中。同时，创建了一个包含550页标注数据的新测试集和评估指标。

Result: 实验结果表明，MolMole在提出的新基准测试集和公共数据集上均优于现有工具包。

Conclusion: MolMole为自动从文档中提取化学数据提供了有效的解决方案，其提出的基准测试集将公开可用，有助于推动该领域的研究。

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [112] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/abs/2505.03778)
*Jonathan Viquerat,Paul Garnier,Amirhossein Bateni,Elie Hachem*

Main category: cs.LG

TL;DR: Dragonfly是一个注重模块化的深度强化学习库，旨在简化实验和开发。


<details>
  <summary>Details</summary>
Motivation: 提升深度强化学习实验的便捷性、模块化程度以及开发效率，减少代码维护成本。

Method: 采用模块化设计，依赖JSON序列化实现构建模块的灵活替换和参数扫描，并针对CPU密集型环境（如数值模拟）进行了特定功能设计。

Result: 在标准智能体和通用基准测试中，Dragonfly的性能表现良好，与文献中的其他方法相比具有竞争力。

Conclusion: Dragonfly是一个高效、模块化的深度强化学习库，通过其设计特性简化了实验流程和代码维护，尤其适用于CPU密集型任务。

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>


### [113] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/abs/2505.03779)
*Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C. L. Wang*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，用于同时优化纤维增强复合材料的结构、分层和纤维路径，以增强其各向异性强度并保证可制造性。


<details>
  <summary>Details</summary>
Motivation: 在纤维增强热塑性复合材料中，如何同时优化结构以获得高各向异性强度并确保其通过多轴3D打印的可制造性是一个挑战。

Method: 采用一个包含三个隐式神经场的计算框架，分别表示几何形状、层序列和纤维方向。将设计目标（各向异性强度、体积）和可制造性目标（机器运动控制、层曲率、层厚度）整合为损失函数，进行一体化、可微分的优化。

Result: 物理实验表明，与采用序贯优化方法生成的复合材料相比，该协同优化方法生成的复合材料在失效载荷方面可实现高达33.1%的提升。

Conclusion: 该协同优化框架能有效提升复合材料的力学强度，同时确保其在多轴3D打印平台上的可制造性，优于传统的序贯优化方法。

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>


### [114] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/abs/2505.03781)
*Jin Yu,JaeHo Park,TaeJun Park,Gyurin Kim,JiHyun Lee,Min Sung Lee,Joon-myoung Kwon,Jeong Min Son,Yong-Yeon Jo*

Main category: cs.LG

TL;DR: 本文提出了一种基于检索增强生成（RAG）的零样本心电图（ECG）诊断框架，通过整合专家知识来提升诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 单独使用RAG在医疗等专业领域（如ECG分析）生成可靠、基于证据的结果面临挑战，需要提升诊断的准确度和可解释性。

Method: 提出了一种基于RAG的零样本ECG诊断框架，该框架整合了专家筛选的知识库。

Result: 在PTB-XL数据集上的评估表明，该框架是有效的，并突显了结构化领域专业知识在自动化ECG解读中的价值。

Conclusion: 该框架通过整合专家知识，提高了ECG诊断的准确性和可解释性，能够支持全面的ECG分析，并具有超越测试数据集的潜在应用。

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>


### [115] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/abs/2505.03783)
*Tian Chen,Shengping Liu,Li Liu,Heng Yong*

Main category: cs.LG

TL;DR: 提出了一种新颖的串并联多网络架构，结合物理信息神经网络（PINNs）和专用子网络，在数据稀疏条件下构建可泛化的封闭项模型，以解决复杂的工程预测模拟问题。


<details>
  <summary>Details</summary>
Motivation: 在工程和科学研究中，尤其是在数据稀疏（稀缺或不完整）的情况下，精确建模封闭项是一项关键挑战，这使得开发广泛适用的模型变得困难。

Method: 提出一种串并联多网络架构：1) 整合物理信息神经网络（PINNs）以融入物理约束和来自多个初始与边界条件的异构数据；2) 采用专用子网络独立建模未知的封闭项以增强泛化能力；3) 将这些封闭模型集成到精确的偏微分方程（PDE）求解器中。

Result: 该方法能够为工程应用中复杂的预测模拟提供鲁棒的解决方案，并增强了模型在不同问题间的泛化能力。

Conclusion: 所提出的串并联多网络架构为在数据稀疏场景下构建封闭模型提供了一种有效的新方法，能够改进工程预测模拟的准确性和鲁棒性。

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>


### [116] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TL;DR: 论文提出了一种在设备端使用大型语言模型（LLM）进行跨层无线漫游控制的方法，通过应用层的高级推理优化AP选择和漫游决策，优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 传统的基于阈值或启发式的无线漫游方案常常导致“粘性切换”（切换延迟）或“过度切换”（切换频繁）问题，无法保证动态移动环境下的无缝连接。

Method: 首次采用跨层方法，在设备端部署大型语言模型（LLM）。LLM在应用层进行高级推理，发出实时指令由物理层/MAC层执行。LLM负责两项任务：(i) 基于上下文感知（如位置、时间）的AP选择；(ii) 动态调整漫游阈值。为适应边缘硬件的低延迟和资源限制，采用了思维链提示、参数高效微调和量化等优化技术。

Result: 在室内和室外数据集上的实验表明，该方法在漫游稳定性和信号质量方面均优于传统的启发式算法和深度强化学习（DRL）基线方法，实现了更好的平衡。

Conclusion: 应用层的大型语言模型推理为未来边缘系统中的底层无线控制提供了有前景的解决方案。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [117] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/abs/2505.03784)
*Ahmed A. Metwally,A. Ali Heydari,Daniel McDuff,Alexandru Solot,Zeinab Esmaeilpour,Anthony Z Faranesh,Menglian Zhou,David B. Savage,Conor Heneghan,Shwetak Patel,Cathy Speed,Javier L. Prieto*

Main category: cs.LG

TL;DR: 该研究开发了一种深度学习模型，利用可穿戴设备数据和常见血液生物标志物预测胰岛素抵抗，为2型糖尿病的早期检测提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 目前胰岛素抵抗的检测方法昂贵、不易普及，限制了早期干预的机会，因此需要更便捷、经济的早期筛查方法。

Method: 研究团队远程招募了1165名参与者，收集了他们的可穿戴设备时间序列数据和血液生物标志物（包括作为金标准的HOMA-IR值）。基于这些数据，开发了深度神经网络模型来预测胰岛素抵抗，并在一个独立验证队列中验证了模型性能。此外，还将预测的胰岛素抵抗值整合到大型语言模型中，以辅助解读和提供个性化建议。

Result: 结合可穿戴设备数据和易获取的血液生物标志物的模型，在预测胰岛素抵抗方面表现优于单独使用任一数据源（R2=0.5, auROC=0.80, 敏感性76%, 特异性84%）。特别是在肥胖和久坐这一高风险亚群中，模型显示出93%的敏感性和95%的调整后特异性。模型性能在独立验证队列中得到了重现。

Conclusion: 这项工作为早期识别2型糖尿病风险人群提供了潜力，有助于更早地实施预防策略，并且可以将预测结果与大型语言模型结合，以促进对HOMA-IR值的理解和安全个性化推荐。

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>


### [118] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/abs/2505.03785)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.LG

TL;DR: 介绍了一个名为mAIstro的开源自主多智能体框架，该框架基于大型语言模型，可实现医疗AI模型的端到端开发和部署，无需用户编码。


<details>
  <summary>Details</summary>
Motivation: 旨在自动化医疗AI中的复杂工作流程，并为用户提供一个无需编码即可开发和部署医疗AI模型的平台。

Method: 开发了mAIstro，一个模块化的自主多智能体框架。它通过自然语言界面协调探索性数据分析、放射组学特征提取、图像分割、分类和回归等任务，支持开源和闭源大型语言模型。

Result: mAIstro在16个涵盖多种成像模式、解剖区域和数据类型的开源数据集上成功执行了所有任务，生成了可解释的输出和经过验证的模型。

Conclusion: mAIstro是首个能够统一数据分析、AI模型开发和跨多种医疗应用推理的智能体框架，为临床和研究AI集成提供了可复现和可扩展的基础。

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>


### [119] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TL;DR: 研究比较了推理型LLM（如DeepSeek-R1）与非推理型LLM在文本到SQL任务的规划框架中评估候选者的能力，发现推理型LLM在判别任务上表现更优，但在生成任务上则不然。


<details>
  <summary>Details</summary>
Motivation: 具有推理能力的大型语言模型（LLM）在改进规划框架中的候选者评估方面潜力巨大，但它们与传统非推理模型在判别能力上的相对性能尚未得到充分研究。

Method: 在一个生成器-判别器LLM规划框架中，针对文本到SQL任务，将一个蒸馏的1.5B参数推理模型（DeepSeek-R1）与多个非推理LLM进行基准测试。引入了一种从思维链（CoT）输出中提取软分数的新方法，用于对候选者进行细粒度排序。

Result: 尽管参数显著减少，蒸馏的DeepSeek-R1-1.5B在F1分数上比CodeLlama-7B高出87%，判别准确率高出3.7%，执行准确率比CodeLlama-13B高出3.7%。研究还发现推理模型的逻辑能力存在上限，增加上下文或计算预算不足以提升其判别性能。此外，推理模型在生成任务上比判别任务更具挑战性，可能不如较小的非推理模型。

Conclusion: 推理模型作为智能体框架中的判别器具有巨大潜力，远超其作为生成器的能力，这为它们在LLM规划基础设施中的最佳定位提供了启示。

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>


### [120] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/abs/2505.03787)
*Zuraiz Baig,Sidra Nasir,Rizwan Ahmed Khan,Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TL;DR: 该研究提出了两种轻量级1D卷积神经网络（ArrhythmiNet V1和V2），用于在边缘设备上高效、实时地进行心律失常分类，模型体积小、准确率高且具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 心律失常是导致生命危险心脏事件的主要原因，迫切需要准确及时的检测。传统心电图（ECG）人工判读耗时、依赖专家经验且易出错。现有深度学习模型虽有进展，但常忽略信号的内在时序和形态特征、缺乏可解释性，且计算量大，难以部署于资源受限平台。

Method: 提出了两种新型轻量级一维卷积神经网络ArrhythmiNet V1和V2。这些模型借鉴了MobileNet的深度可分离卷积设计，针对边缘设备上的高效实时心律失常分类进行了优化。同时，集成了SHAP（Shapley Additive Explanations）和Grad-CAM（Gradient-weighted Class Activation Mapping）技术以实现局部和全局可解释性。

Result: ArrhythmiNet V1和V2的模型内存占用分别仅为302.18 KB和157.76 KB，在MIT-BIH心律失常数据集的五个类别（正常窦性心律、左束支传导阻滞、右束支传导阻滞、房性早搏和室性早搏）上分别达到了0.99和0.98的分类准确率。可解释性技术突出了对模型预测有贡献的生理学意义模式，如QRS波群和T波。

Conclusion: 研究结果表明，在实用、可穿戴和嵌入式心电监护系统中，将可解释性、预测准确性和计算效率相结合是可行的。尽管存在数据集多样性和泛化性方面的局限，但所提出的模型为资源受限环境下的实时心律失常监测提供了有效方案。

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>


### [121] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/abs/2505.03789)
*Syoiti Ninomiya,Yuming Ma*

Main category: cs.LG

TL;DR: 提出了一种基于随机微分方程(SDEs)高阶弱近似算法的新型深度学习神经网络架构，可有效学习鞅过程，并已应用于金融衍生品定价。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型学习鞅过程的效率，特别是在解决随机微分方程相关问题（如金融衍生品定价）时的需求。

Method: 提出一种新的深度学习神经网络架构。该架构的核心是显式龙格-库塔型高阶弱近似算法，通过目标SDE向量场的迭代组合和线性组合实现近似。

Result: 该架构使得深度学习模型能够高效学习鞅过程。基于此架构的深度神经网络在金融衍生品定价问题上的行为得到了检验。

Conclusion: 该研究提出了一种基于SDE高阶弱近似算法的新型深度学习架构，能够有效提升模型学习鞅的能力，并在金融衍生品定价等领域展现了应用潜力。

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>


### [122] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/abs/2505.03790)
*Yuren Zhang,Zhongnan Pu,Lei Jing*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型和Transformer模型的简单有效的时间序列数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型需要大量数据，但时间序列数据的增强研究相对较少，本研究旨在解决此问题。

Method: 结合扩散模型和Transformer模型：使用调整后的扩散去噪模型生成初始时间步动作数据，再用Transformer模型预测后续动作，并采用加权损失函数实现收敛。

Result: 与未使用数据增强或传统增强方法相比，该方法能生成高质量的增强数据，并提升了后续模型的性能。

Conclusion: 所提出的结合扩散模型和Transformer模型的方法能有效生成高质量的时间序列增强数据。

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>


### [123] [Practical Boolean Backpropagation](https://arxiv.org/abs/2505.03791)
*Simon Golbert*

Main category: cs.LG

TL;DR: 提出了一种纯布尔神经网络的反向传播方法，并初步验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 实值神经网络硬件效率不高，而纯布尔训练方法研究不足，存在探索空间。

Method: 提出一种实用的纯布尔反向传播方法，该方法基于特定选择的逻辑门，直接在布尔代数中运算，不涉及数值计算。

Result: 初步实验证实了该方法的可行性。

Conclusion: 该研究成功展示了一种可行的纯布尔反向传播训练方法，为硬件高效的布尔神经网络模型提供了新途径。

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>


### [124] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/abs/2505.03792)
*Lang Feng,Weihao Tan,Zhiyi Lyu,Longtao Zheng,Haiyang Xu,Ming Yan,Fei Huang,Bo An*

Main category: cs.LG

TL;DR: 本文提出了一种名为 CoSo 的新颖在线微调方法，通过反事实推理来动态评估单个词元对动作的因果影响，从而更有效地探索视觉语言模型（VLM）代理的文本动作空间，提升其在动态环境中的多步、目标导向能力。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习微调VLM代理在实现多步、目标导向能力方面展现了潜力，但其开放式文本动作空间和非端到端的动作生成给在线探索带来了巨大挑战，例如探索空间爆炸。现有方法对所有词元分配统一的不确定性，效率不高。

Method: 提出了反事实软强化学习（CoSo）方法。CoSo利用反事实推理来动态评估单个词元对后处理动作的因果影响，优先探索对动作关键的词元，同时减少语义冗余或低影响词元的影响，从而实现更具针对性和效率的在线部署过程。提供了理论分析证明其收敛性和策略改进保证。

Result: 广泛的实证评估支持了CoSo的有效性。在包括安卓设备控制、纸牌游戏和具身人工智能在内的多种代理任务中，CoSo显著提升了探索效率并带来了一致的性能增益。

Conclusion: CoSo是一种有效的在线微调方法，通过优先探索对动作至关重要的词元，显著提高了VLM代理的探索效率和任务性能。

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [125] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/abs/2505.03793)
*Xinyue Zeng,Haohui Wang,Junhong Lin,Jun Wu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 提出 LENSLLM，一个通过建模微调动态和预测泛化能力，实现准确高效大语言模型选择的理论框架与模型。


<details>
  <summary>Details</summary>
Motivation: 由于微调所有候选大语言模型（LLM）的计算成本过高，需要高效的模型选择方法。现有研究对 LLM 在微调过程中的动态行为及其在不同下游任务上的泛化性能理解不足。

Method: 首先推导了一个基于 Hessian 矩阵的 PAC-Bayes 泛化界来揭示 LLM 的微调动态，然后引入了 LENSLLM，一个基于神经正切核 (NTK) 的修正缩放模型，用于准确预测不同任务上的性能并保持计算效率。

Result: 在三个大规模基准测试中，LENSLLM 在 LLM 选择任务上实现了高达 91.1% 的准确率，将计算成本降低了多达 88.5%，并且其性能优于五种当前最先进的方法。

Conclusion: 提出的 LENSLLM 框架能够有效建模 LLM 的微调动态，为跨不同下游任务的 LLM 选择提供了一种准确且高效的解决方案。

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>


### [126] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/abs/2505.03794)
*İrfan Işik,Ibrahim Karahan,Okan Erkaymaz*

Main category: cs.LG

TL;DR: 本文提出了一种改进的具有双惯性参数的前向后向分裂算法，用于解决算子零点问题，并展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 旨在寻找一个点，使得一个余强制算子（co-coercive operator）与一个极大单调算子（maximal monotone operator）之和在该点为零。

Method: 提出了一种包含两个惯性参数的改进前向后向分裂算法。

Result: 算法在标准假设下表现出弱收敛性。实验结果表明，该算法在回归和数据分类问题上优于现有算法。

Conclusion: 所提出的具有双惯性参数的算法是有效的，并且在与现有文献中的其他相关算法进行基准比较时，取得了更优的结果。

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>


### [127] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/abs/2505.03797)
*Andrew Millard,Joshua Murphy,Simon Maskell,Zheng Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于序贯蒙特卡洛（SMC）的部分贝叶斯神经网络（pBNN）训练方法，通过引导提议和梯度马尔可夫核提高了高维问题的可扩展性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的pBNN结合SMC的推理方法虽有优势，但在处理高维问题时其可扩展性需要提升。

Method: 引入一种新的基于SMC的pBNN训练方法，该方法利用引导提议（guided proposal）并整合了基于梯度的马尔可夫核（gradient-based Markov kernels）。

Result: 新方法在预测性能和最优损失方面均优于当前最先进的技术。此外，pBNNs能够很好地适应更大的批量大小，从而显著减少训练时间，并常常能获得更好的性能。

Conclusion: 所提出的新SMC训练方法增强了pBNNs在高维问题上的可扩展性和性能，使其训练更高效，效果更优。

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>


### [128] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/abs/2505.03798)
*Yiqing Shen,Hao Ding,Lalithkumar Seenivasan,Tianmin Shu,Mathias Unberath*

Main category: cs.LG

TL;DR: 当前基础模型依赖离散词元表示，限制其理解真实世界。本文主张采用数字孪生（DT）表示作为替代方案，以构建更强大的基础模型。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型（FMs）因其词元表示法将连续多模态数据离散化，难以学习显式领域知识，导致在语义一致性、时空动态捕捉和因果推理方面存在局限，且这些问题无法通过简单扩大规模解决。

Method: 提出使用数字孪生（DT）表示作为构建基础模型时词元表示的替代方案。DT是面向结果的数字表示，用作创建物理过程虚拟副本的基础。

Result: （预期）数字孪生表示能够提供物理接地的、显式编码领域知识并保留真实世界过程连续性的表示，从而解决当前FMs面临的挑战。

Conclusion: 机器学习社区应考虑采用数字孪生（DT）表示来构建基础模型，以克服当前基于词元表示的模型的局限性，提升模型对真实世界的理解和处理能力。

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>


### [129] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
*Hyun Lee,Chris Yi,Maminur Islam,B. D. S. Aritra*

Main category: cs.LG

TL;DR: 提出了一种名为SDM-InstructGLM的新型图语言模型框架，无需图神经网络（GNN），通过基于相似度和度中心性的偏置随机游走机制，高效地将图结构编码到大语言模型中，提升了在图相关任务上的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在图相关问题上的应用受限于可扩展性和缺乏专门的图结构处理机制。现有方法多将LLMs与GNNs结合，而直接在LLMs中编码图结构（尤其针对大规模图）的研究不足，且存在token限制问题。

Method: 提出了SDM-InstructGLM框架，这是一种指令微调的图语言模型。该方法引入了一种基于相似度和度中心性的偏置随机游走机制，根据节点特征相似性和度中心性选择性地采样和编码图信息，以实现在LLM内的自适应和结构化表示，无需依赖GNN。

Result: 该方法显著提高了token效率，减轻了随机采样造成的信息损失，并增强了在节点分类和链接预测等图任务上的性能。实验结果证明了仅使用LLM进行图处理的可行性，实现了可通过指令微调优化的可扩展且可解释的图语言模型。

Conclusion: 这项工作为无GNN的图学习方法开辟了道路，展示了LLMs作为独立图推理模型的潜力，并为未来的图语言模型研究奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>


### [130] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
*Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的免训练方法，通过使用带序列排序的沃尔什-哈达玛变换和分组序列排列旋转（GSR）来构建改进的旋转矩阵，从而显著提升大语言模型在极低比特（如2比特）量化下的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）因计算成本高昂而面临部署挑战。现有的基于旋转的训练后量化（PTQ）方法在极低比特宽度（如2比特）下性能不佳，难以有效压缩模型。

Method: 提出了一种无需训练的方法来构建改进的旋转矩阵。核心技术包括：1. 利用带序列排序（sequency ordering）的沃尔什-哈达玛变换（Walsh-Hadamard transform），以聚集相似频率分量，减少量化误差。2. 提出分组序列排列旋转（Grouped Sequency-arranged Rotation, GSR），使用带有较小沃尔什块的块对角矩阵，以有效隔离异常值的影响。

Result: 该方法在推理任务和WikiText-2数据集的困惑度（PPL）得分上均表现出稳健的性能。无需任何训练即可达到与基于优化的方法相当的性能，并且在应用于现有学习旋转技术之上时也能增强其结果。

Conclusion: 本文提出的免训练旋转矩阵构建新方法，通过利用沃尔什-哈达玛变换的序列排序特性和GSR策略，显著改善了LLM在极低比特量化下的性能，为低成本高效部署LLM提供了一种有效的途径。

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>


### [131] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/abs/2505.03801)
*Changhai Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出一种新颖的两阶段LLM压缩方法，通过全局秩和稀疏性优化，有效解决低秩与稀疏分量交互及层间权重分配难题，性能超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）低秩稀疏复合近似压缩方法在处理低秩与稀疏矩阵的交互合作以及因层间冗余度差异导致的权重分配问题上存在挑战，这影响了压缩性能。

Method: 提出一种两阶段LLM压缩方法：第一阶段利用鲁棒主成分分析（RPCA）将LLM的权重矩阵分解为低秩和稀疏分量，从而定义了包含最终低秩和稀疏矩阵的低维和稀疏空间，以缩小优化空间；第二阶段提出一种概率全局优化技术，在上述两个空间内联合识别低秩和稀疏结构。

Result: 大量的实验结果表明，该方法在稀疏化和复合近似方面显著优于当前最先进的技术。

Conclusion: 该方法能够自动检测不同层的冗余度，并有效管理稀疏和低秩分量之间的相互作用，从而在LLM压缩方面取得了显著的性能提升。

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>


### [132] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/abs/2505.03802)
*Changhai Zhou,Yuhua Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出QR-Adaptor，一种通过联合优化量化组件和低秩空间秩来持续改进量化大模型微调性能的无梯度策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于SVD或分别优化低秩子空间与量化组件的方法，在提升量化模型微调性能方面存在局限，未能充分考虑二者的协同作用。

Method: 提出QR-Adaptor，一种统一的、无梯度的策略。该策略使用部分校准数据，为模型每一层联合搜索量化组件和低秩空间的秩，将精度和秩的分配视为一个由实际下游任务性能和内存使用引导的离散优化问题。

Result: 与当前最先进的量化LoRA微调方法相比，QR-Adaptor在GSM8K数据集上实现了4.89%的准确率提升，并且在某些情况下，其性能甚至超过了16位微调模型，同时保持了与4位设置相当的内存占用。

Conclusion: QR-Adaptor通过联合优化量化精度和低秩空间的秩分配，能够有效且持续地提升量化大语言模型微调的性能，同时兼顾内存效率。

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [133] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/abs/2505.03803)
*Chen Xu,Yuxuan Yue,Zukang Xu,Xing Hu,Jiangyong Yu,Zhixuan Chen,Sifan Zhou,Zhihang Yuan,Dawei Yang*

Main category: cs.LG

TL;DR: 提出了一种名为RWKVQuant的后训练量化框架，专为RWKV模型设计，以解决其在量化时性能显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: RWKV模型虽然性能与Transformer相当，但在资源受限设备上部署时面临挑战。传统的后训练量化（PTQ）技术应用于RWKV时会导致严重的性能下降，这主要是由于RWKV自身的非线性算子阻碍参数融合以及大量均匀分布的权重对聚类量化不友好。

Method: 提出了RWKVQuant框架，包含两个创新技术：(1) 一种粗到细的代理机制，能够通过评估权重的均匀性和识别异常值来智能选择不同的量化方法。(2) 一种码本优化算法，旨在提升RWKV模型中逐元素乘法操作的聚类量化方法的性能。

Result: 实验证明，RWKVQuant能够将RWKV-6-14B模型量化到大约3比特，同时准确率损失低于1%，并且推理速度提升了2.14倍。

Conclusion: RWKVQuant框架成功解决了RWKV模型在后训练量化中遇到的性能瓶颈，使其能够在保持较高准确率的同时，显著减小模型尺寸并提升推理速度，从而更适用于资源受限的设备。

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>


### [134] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/abs/2505.03804)
*Xing Hu,Zhixuan Chen,Dawei Yang,Zukang Xu,Chen Xu,Zhihang Yuan,Sifan Zhou,Jiangyong Yu*

Main category: cs.LG

TL;DR: 混合专家（MoE）大语言模型存在内存开销大、传统量化方法效果差的问题。本文提出MoEQuant框架，通过专家平衡自采样和亲和力引导量化，显著提升了MoE模型量化后的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）大语言模型虽然性能高、计算成本低，但面临显著的内存开销，限制了其实际部署。现有的训后量化（PTQ）方法应用于MoE模型时，会导致严重的精度下降和泛化性能减弱。这主要是由于MoE模型的稀疏和动态特性导致的专家间样本分布不均（Inter-expert imbalance）和专家内样本与专家关联度不同（Intra-expert imbalance）两大挑战。

Method: 提出了一种专为MoE大语言模型设计的量化框架MoEQuant。该框架包含两个核心技术：1) 专家平衡自采样（EBSS）：一种高效的采样方法，通过利用令牌的累积概率和专家平衡指标构建专家分布均衡的校准集。2) 亲和力引导量化（AGQ）：将专家与样本之间的亲和力纳入量化过程，从而准确评估单个样本对MoE层内不同专家的影响。

Result: 实验表明，MoEQuant在4位量化下，能在DeepSeekMoE-16B模型的HumanEval任务上取得超过10个点的准确率提升，并提高了效率。

Conclusion: MoEQuant框架通过解决专家间和专家内不平衡问题，有效提升了混合专家大语言模型的量化性能和效率，为其实际部署提供了可行的压缩方案。

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>


### [135] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
*Prudhviraj Naidu,Zixian Wang,Leon Bergen,Ramamohan Paturi*

Main category: cs.LG

TL;DR: Transformer模型在学习算法任务时，损失曲线并非平稳下降，而是呈现阶段性突变，这与内部特征的“安静”积累和“响亮”激活有关。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型在算法任务上学习曲线为何偏离既有的幂律缩放趋势，并出现明显的阶段性转变现象。

Method: 在十个基础算法任务上训练Transformer语言模型，探测模型内部表征，并进行特征消融实验。

Result: 模型损失曲线在计算资源投入到一定程度后，会从几乎不改善的停滞期突然急剧下降。内部表征分析显示，停滞期学习“安静特征”，随后突然获得“响亮特征”，这与损失的急剧下降同时发生。消融实验证明了这些特征对任务性能的因果作用。

Conclusion: 下一词元预测损失并不能总是可靠地追踪模型的渐进式学习进展。关键的内部特征可能在“表面之下”发展，直到它们汇聚并触发性能的快速提升。

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>


### [136] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/abs/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TL;DR: 提出将随机上坡攀爬（RUC）推广为一个模型无关的多元时间序列预测特征优化框架。


<details>
  <summary>Details</summary>
Motivation: 为解决现有深度学习方法在特征发现中迭代周期长、能耗高、可解释性差的问题，并为资源受限机构提供准确、透明的预测工具。

Method: 通过随机组合领域特定语法的算子合成候选特征程序，使用廉价代理模型在滚动窗口上快速评分，并通过嵌套交叉验证和信息论收缩过滤不稳定性。

Result: 该方法有望实现更快的迭代周期、更低的能源消耗和更高的可解释性，通过将特征发现与GPU密集的深度学习解耦。

Conclusion: 推广RUC为特征优化框架，可为多元时间序列预测提供高效、节能且可解释的特征发现，赋能数据驱动决策。

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>


### [137] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
*Tuochao Chen,Nicholas Batchelder,Alisa Liu,Noah Smith,Shyamnath Gollakota*

Main category: cs.LG

TL;DR: LlamaPIE 是一款通过可听戴设备提供实时、主动、简洁指导以增强人类对话的新型助手。


<details>
  <summary>Details</summary>
Motivation: 克服传统语言模型需要用户明确调用且可能打断对话的局限性，并解决主动辅助中的何时响应、响应内容、个性化及实时性等挑战。

Method: 构建了一个半合成对话数据集，并提出一个双模型流水线：一个小模型决定何时响应，一个大模型生成响应内容。

Result: 在真实世界数据集上的评估表明该方法能有效提供有帮助且不打扰的辅助。用户研究显示，相较于无辅助或反应式模型，用户更偏爱 LlamaPIE 主动助手。

Conclusion: LlamaPIE 通过提供主动、实时的指导，展现了其增强现场对话体验的潜力。

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>


### [138] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/abs/2505.03806)
*Mehran Mazandarani,Marzieh Najariyan*

Main category: cs.LG

TL;DR: 本文介绍了一种名为感知信息神经网络 (PrINNs) 的新框架，它将基于感知的信息融入神经网络，用于处理具有已知或未知物理定律的系统，并扩展了物理信息神经网络 (PINNs) 的概念。


<details>
  <summary>Details</summary>
Motivation: 研究旨在弥合传统基于物理的建模与现代数据驱动方法之间的差距，使神经网络能够有效地整合物理定律（无论已知或未知）以及专家知识和不精确的感知信息来对系统进行建模。

Method: PrINNs 通过损失函数将专家知识和多种形式的感知精确化信息（如奇异信息、概率分布、可能性分布、区间和模糊图）整合到神经网络中。该框架扩展了PINNs，并引入了具体变体，如混合专家信息神经网络 (MOEINNs) 用于融合异构专家知识，转换知识信息神经网络 (TKINNs) 用于整合元信息，以及模糊信息神经网络 (FINNs) 利用模糊逻辑约束实现无需预训练的在线学习。

Result: PrINNs 使得神经网络能够同时从结构化的物理定律和灵活的基于感知的规则中学习，从而对动态系统进行建模。该方法使神经网络能够在不确定环境中运行，建模复杂系统，并有潜力发现新的微分方程形式。例如，FINNs实现了无需预训练的在线训练，且无需去模糊化过程。

Conclusion: PrINNs 是连接传统物理建模和现代数据驱动方法的一个重要进步，为神经网络提供了一个强大的工具，使其能够在不确定环境中操作、建模复杂系统并发现新的微分方程，从而推动计算科学和工程领域的发展。

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>


### [139] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: 本研究介绍了一种集成多源遥感数据（Sentinel-2、DEM、NOAA气候数据）和人工智能模型（树模型与神经网络集成）的方法，用于高效、准确地检测和分类有害藻华的严重程度。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对全球内陆水质和公共健康构成日益严重的威胁，因此迫切需要高效、准确且具有成本效益的检测方法。

Method: 该方法整合了哥白尼Sentinel-2光学影像、数字高程模型(DEM)和NOAA高分辨率快速刷新(HRRR)气候数据等多种开源遥感数据，利用谷歌地球引擎(GEE)和微软行星计算机(MPC)进行数据检索。通过结合基于树的机器学习模型和神经网络模型，构建了一个集成学习模型来对藻华的严重程度进行分类。

Result: 研究发现，Sentinel-2的近红外(NIR)和两个短波红外(SWIR)波段、海拔高度、温度、风速以及经纬度是最重要的特征。虽然树模型单独表现强劲，但加入神经网络增强了模型的鲁棒性，并展示了深度学习模型有效利用多样化遥感输入的能力。

Conclusion: 该方法利用高分辨率卫星影像和人工智能驱动的分析，能够动态监测藻华，并具有全球应用的潜力。该研究证明了遥感数据与人工智能的融合在应对关键环境挑战方面的有效性，其完整代码已开源可供进一步应用。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [140] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的在线数据训练框架，首次统一了动态数据选择和数据增强，以同时提高训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的动态数据选择会降低数据多样性，而数据增强与选择过程分离导致无法充分利用两者协同效应，影响模型泛化能力和训练效率。

Method: 提出一个在线数据训练框架，通过估计样本的局部密度和多模态语义一致性的联合分布，有针对性地选择适合增强的样本，并抑制噪声或模糊数据，从而统一动态数据选择和增强过程。

Result: 该方法在多个基准数据集和架构上优于现有SOTA方法，例如在ImageNet-1k上可减少50%的训练成本且性能无损。同时，它还增强了模型的抗噪性和鲁棒性。

Conclusion: 提出的统一框架能够有效提升训练效率、模型泛化能力、抗噪性和鲁棒性，具有实际应用价值。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [141] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/abs/2505.03811)
*Surajit Chakrabarty,Rukma Talwadker,Tridib Mukherjee*

Main category: cs.LG

TL;DR: 本文介绍了一种名为 ScarceGAN 的方法，专注于从具有少量弱标签先验的多维纵向遥测数据中识别极其罕见的样本。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决在多维纵向遥测数据中识别极端稀有样本时面临的几大挑战：1. 正类别样本因数据固有偏差和标签极度有限而严重稀缺；2. 负样本具有多类别、密度分布不均和特征部分重叠的特性；3. 大量未标记数据导致正负类别先验信息微弱，且未标记集中可能存在未知行为。

Method: 提出了 ScarceGAN 方法，它通过以下方式改进了半监督生成对抗网络（GAN）：1. 调整框架以适应弱标记的多类别负样本和可用的正样本；2. 为具有噪声先验的样本引入“leeway”项，放宽监督判别器对负样本精确区分的约束；3. 修改判别器（监督和非监督路径）及生成器的成本目标函数。

Result: 在技能游戏中识别风险玩家的应用中，ScarceGAN 在稀缺类别上的召回率超过85%（相比原始半监督GAN提升约60%），且在未知空间中的冗余信息极少。此外，ScarceGAN 在正类别不平衡识别方面优于现有基于GAN的专门模型，并在KDDCUP99入侵检测数据集中成功识别了一种占比仅0.09%的罕见攻击类别，建立了新的基准。

Conclusion: ScarceGAN 是一种有效识别多维数据中极端稀有样本的方法，尤其在标签稀疏且负类复杂的场景下表现出色。它通过改进半监督GAN框架，在多个数据集上显著提升了稀有类别的识别性能，并建立了新的性能基准。

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>


### [142] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/abs/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TL;DR: 信息过滤网络 (IFNs) 是一种通过全局稀疏、局部密集且可解释的结构来建模复杂系统并捕捉多变量依赖的强大框架。本综述全面介绍了其理论基础、构建方法及多样化应用。


<details>
  <summary>Details</summary>
Motivation: 应对高维数据驱动建模中的挑战，例如提高模型的可解释性、计算效率和预测性能，特别是在需要有效捕捉复杂系统中多变量依赖关系时。

Method: 本文综述了信息过滤网络 (IFNs) 的理论基础和构建方法，包括从早期网络模型演进到如三角化最大过滤图 (TMFG) 和最大过滤团森林 (MFCF) 等高级公式。这些方法构建的是本质上为高阶网络并生成单纯复形的结构。

Result: 综述显示，IFNs 在金融、生物、心理和人工智能等多个领域展现了应用价值，能够提升模型的可解释性、计算效率和预测性能。特别是在图模型中，IFNs 在估计稀疏逆协方差矩阵方面比传统方法（如Graphical LASSO）更准确、可扩展性更强。

Conclusion: IFNs 不仅是连接经典网络理论与当代数据驱动范式的桥梁，还具有与机器学习和深度学习进一步融合的巨大潜力，甚至可能影响未来深度学习模型的架构设计。

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>


### [143] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/abs/2505.03818)
*Antonio Valerio Miceli-Barone,Vaishak Belle,Ali Payani*

Main category: cs.LG

TL;DR: 提出一种基于语义不等价游戏 (SInQ) 的方法，通过合成代码推理训练数据，提升大语言模型解决复杂编程任务的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在处理需要复杂程序语义推理的任务时表现不佳，且获取相关训练数据具有挑战性。

Method: 提出一种名为 SInQ 的语义不等价游戏。通过一个生成器代理创建语义不同的程序变体，一个评估器代理识别导致原始程序和变体行为差异的输入，两者半对抗性训练以生成合成训练数据。

Result: 该方法在多个代码生成和理解基准测试上取得显著效果。例如，在跨语言漏洞检测中，仅用 Python 数据训练即可改善 C/C++ 漏洞检测；在 LLM 表现不佳的 Python 内置标识符交换基准测试中也取得了实质性改进。

Conclusion: 通过 SInQ 游戏合成的训练数据能有效提升大型语言模型在复杂代码推理任务（如跨语言漏洞检测和语义理解）上的性能。

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>


### [144] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/abs/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: 提出了两种新的测试时微调方法，用于改善模型在面对不确定预测时的表现，这些方法无需辅助数据，仅使用给定的测试实例。


<details>
  <summary>Details</summary>
Motivation: 提升模型在初始预测显示高不确定性时的预测准确性，并使预测更倾向于为不太可能的结果分配零概率。

Method: 在推理过程中，引入对“可能类别”的额外关注步骤，并通过单步梯度下降来优化那些初始前向传播显示高不确定性的预测。

Result: 实验评估表明，对于文本和图像领域的多种模型，在使用相同超参数的情况下，该方法在具有高决策不确定性的样本上均实现了准确性提升。

Conclusion: 所提出的测试时微调方法能够有效改进模型在不确定性预测上的表现，提高高不确定性样本的预测准确率。

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>


### [145] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/abs/2505.03822)
*Hao Wu,Jialiang Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种双重正则化二阶潜因子(DRSLF)模型，通过结合L1和L2范数正则化以及二阶优化信息，旨在提高云服务中高维稀疏QoS数据的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的潜因子分析(LFA)模型在处理云服务QoS数据时，多依赖一阶优化器和L2范数正则化，这可能导致QoS预测精度较低。

Method: 提出了一种双重正则化二阶潜因子(DRSLF)模型，其核心思想包括：a) 整合L1范数和L2范数正则化项以增强低秩表示性能；b) 通过在共轭梯度法的每一步中计算Hessian向量积来引入二阶信息。

Result: 在两个真实的响应时间QoS数据集上的实验结果表明，DRSLF模型相比两个基线方法具有更高的低秩表示能力。

Conclusion: DRSLF模型通过结合L1/L2双重正则化和利用二阶信息，能够有效提升对高维稀疏QoS数据的低秩表示性能，从而有潜力提高QoS预测的准确性。

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>


### [146] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/abs/2505.03825)
*Anushiya Arunan,Yan Qin,Xiaoli Li,Yuen Chau*

Main category: cs.LG

TL;DR: 该论文提出ITA-CTF框架，通过智能增强和对比张量分解，解决低训练数据下多维时间序列分类的难题。


<details>
  <summary>Details</summary>
Motivation: 在低训练数据环境下，标准深度学习模型难以学习多维时间序列中的复杂特征（如跨维度依赖和类内变化），易导致过拟合，影响分类性能。

Method: 提出了智能增强对比张量分解（ITA-CTF）框架。该框架包含：1) CTF模块，学习时间序列的核心解释成分及其联合依赖，并引入对比损失优化以增强类别区分性；2) ITA模块，通过动态采样“软”类原型指导数据增强，生成针对性的信息增强样本，以突出真实的类内模式并保留类别属性。

Result: 在五个不同的分类任务上进行评估，与标准张量分解和多种深度学习基准相比，所提出的ITA-CTF方法取得了显著的性能提升，最高提升幅度达到18.7%。

Conclusion: ITA-CTF框架能够有效从多维时间序列中学习表示，通过智能增强和对比张量分解，即使在训练数据有限的情况下，也能识别复杂的类内变化并寻求不变的类别属性，从而显著提高分类准确性。

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [147] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/abs/2505.03827)
*Xin Wang,Ling Feng,Huijun Zhang,Lei Cao,Kaisheng Zeng,Qi Li,Yang Ding,Yi Dai,David Clifton*

Main category: cs.LG

TL;DR: 本研究提出了一种基于元学习的社交媒体压力源估计新框架，旨在识别用户帖子中更具体的压力源，并解决了少样本学习和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现代社会压力普遍，可能导致严重的健康问题。现有研究主要关注压力状态和类别的分类，而对具体压力源的识别不足。压力源种类繁多、样本稀疏且不断出现新的压力源，给机器学习带来了挑战。

Method: 将压力源估计问题置于少样本学习场景中，提出了一种新颖的基于元学习的压力源估计框架。该框架通过元知识继承机制得到增强，使其能够学习通用的压力源上下文，并对仅有少量标记数据的新压力源具有良好的泛化能力，同时防止在适应新压力源时发生灾难性遗忘。

Result: 实验结果表明，所提出的模型与基线模型相比，取得了最先进的性能。此外，构建并公开了一个基于社交媒体的压力源估计数据集，可用于训练人工智能模型，促进人类福祉。

Conclusion: 该研究提出的元学习框架能有效估计社交媒体帖子中的具体压力源，尤其在处理少样本和新压力源方面表现优异，并为该领域提供了宝贵的数据集资源。

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>


### [148] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TL;DR: 提出一种结合形式化验证的混合方法，以确保高维逆问题中参数空间约束的物理和数学正确性。


<details>
  <summary>Details</summary>
Motivation: 核聚变和高能天体物理学中的高维逆问题面临参数有效性挑战，现有降维方法无法保证参数组合的物理有效性或数学一致性。

Method: 倡导采用一种混合方法，将数值算法的形式化验证方法与参数空间约束相结合。

Result: 旨在构建具有可证明的数学和物理正确性属性的参数空间限制，同时能够兼顾实验不确定性和物理过程本身的不确定性。

Conclusion: 建议采用结合形式化验证的混合方法来处理高维逆问题中的参数空间约束，以确保其物理和数学上的正确性，并有效管理不确定性。

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


### [149] [Machine Learning: a Lecture Note](https://arxiv.org/abs/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TL;DR: 该讲义为数据科学等相关专业的硕博研究生提供机器学习基础，涵盖监督学习、无监督学习及强化学习等进阶主题，为深入研究做准备。


<details>
  <summary>Details</summary>
Motivation: 为数据科学或相关学科的初级硕博研究生提供机器学习领域的核心基础概念，使他们能够为后续更高级别的学习和研究做好准备。

Method: 讲义从机器学习的基本概念入手（以分类为核心任务，包括损失函数、反向传播、随机梯度下降、泛化、模型选择及神经网络基础），接着深入探讨无监督学习的概率方法（如定向潜变量模型、专家乘积模型、生成对抗网络、自回归模型），最后涵盖强化学习、集成方法和元学习等一系列进阶主题。

Result: 阅读本讲义后，学生应能为学习和研究机器学习及更广泛的人工智能领域中更高级的课题做好准备。

Conclusion: 该讲义为学生提供了进入机器学习和人工智能高级课题研究的必要基础知识，是进一步学习和研究的奠基石。

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>


### [150] [Explaining Anomalies with Tensor Networks](https://arxiv.org/abs/2505.03911)
*Hans Hohenfeld,Marius Beuerle,Elie Mounzer*

Main category: cs.LG

TL;DR: 该研究将张量网络（矩阵乘积态和新引入的树张量网络）应用于实值数据领域的异常检测，并验证了其预测性能和解释异常样本的能力。


<details>
  <summary>Details</summary>
Motivation: 将先前在离散值网络安全任务中基于矩阵乘积态的可解释异常检测框架扩展到实值数据域，并引入树张量网络以处理此类任务，从而拓宽张量网络的应用范围。

Method: 1. 将基于矩阵乘积态的异常检测框架扩展至处理实值数据。2. 引入树张量网络用于可解释的异常检测。3. 使用三个基准问题对这两种方法进行验证，并与多种基线模型进行性能比较。4. 展示了两种张量网络架构解释异常样本的能力。

Result: 在三个基准问题上，所提出的张量网络方法（矩阵乘积态和树张量网络）均表现出与基线模型相当的预测性能，并且两种架构都能有效地解释检测到的异常样本。

Conclusion: 该研究成功地将张量网络应用于实值数据的可解释异常检测，证明了其有效性，扩展了张量网络的应用场景，并为未来探索更复杂的张量网络架构奠定了基础。

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>


### [151] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/abs/2505.03923)
*Pedram Pad,Hadi Hammoud,Mohamad Dia,Nadim Maamari,L. Andrea Dunbar*

Main category: cs.LG

TL;DR: 提出了一种新颖的、非侵入式的特征选择层，能在神经网络训练过程中自动识别并选择k个最重要的特征，无需再训练或调整超参数。


<details>
  <summary>Details</summary>
Motivation: 现有的先进特征选择方法通常需要后选择再训练和大量的超参数调整，这使得它们的应用变得复杂。

Method: 引入了一个新的非侵入式特征选择层。该层通过公式 $\tilde{x}_i = a_i x_i + (1-a_i)z_i$ 运作，其中 $x_i$ 是输入特征，$z_i$ 是高斯噪声，$a_i$ 是可训练增益且满足 $\sum_i{a_i^2}=k$（目标特征数）。此设计通过加权噪声失真和增益归一化，自动将k个 $a_i$ 增益驱动至1（选择信息特征），其余驱动至0（丢弃冗余特征），无需修改损失函数、网络架构或进行后选择再训练。

Result: 该方法在标准基准数据集和一个新的真实世界数据集上均达到了最先进的性能，其表现优于或持平现有方法，且无需为k进行超参数搜索或再训练。在线性回归背景下的理论分析进一步验证了其有效性。

Conclusion: 该研究证明了特征选择中的简单性和高性能可以兼得，为机器学习提供了一个强大而直接的特征选择工具。

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>


### [152] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/abs/2505.03949)
*John Christopher Tidwell,John Storm Tidwell*

Main category: cs.LG

TL;DR: 该项目提出了一种结合卷积神经网络（CNN）、长短期记忆网络（LSTM）和深度Q网络（DQN）的集成深度学习框架，用于自动化股票交易，旨在解决传统方法和直接强化学习在处理市场噪音、复杂性和泛化方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法和直接强化学习（RL）在自动化股票交易中难以有效处理市场噪音、应对市场复杂性，并且在泛化能力方面存在不足。

Method: 采用一种集成的深度学习框架：首先，使用卷积神经网络（CNN）从图像化的技术指标中识别模式；其次，利用长短期记忆网络（LSTM）捕捉价格历史和技术指标的时间依赖性；最后，通过深度Q网络（DQN）智能体，根据CNN和LSTM提取的特征学习最优的交易策略（买入、卖出、持有）。

Result: 该框架中的深度Q网络（DQN）智能体能够基于卷积神经网络（CNN）和长短期记忆网络（LSTM）所提取的特征，学习到最优的交易策略（如买入、卖出或持有）。

Conclusion: 提出的集成深度学习框架（结合CNN、LSTM和DQN）为自动化股票交易提供了一种有潜力的新方法，旨在通过整合不同网络的优势来克服现有方法的局限性，并提升交易决策的质量。

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>


### [153] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/abs/2505.03953)
*Noah Schutte,Grigorii Veviurko,Krzysztof Postek,Neil Yorke-Smith*

Main category: cs.LG

TL;DR: 本文研究了在不确定性优化中，何时应使用单值预测或分布估计，并为决策导向学习（DFL）提出了有效的决策代理方法。


<details>
  <summary>Details</summary>
Motivation: 在决策导向学习（DFL）中，预测不确定参数时，现有方法通常要么预测单一值，要么估计其完整分布，但缺乏关于何时应选择哪种方法的明确指导和理论依据。

Method: 本文首先研究了何种问题特性分别支持使用单值预测或分布估计的假设。基于此分析，提出了新的决策代理（decision proxies）方法用于DFL，这些方法在学习任务的复杂性上妥协很小。

Result: 实验结果表明，所提出的方法在处理包含连续和离散变量，以及目标函数和约束条件中存在不确定性的各类优化问题上均表现有效。

Conclusion: 本文通过识别关键问题属性，为DFL中选择单值预测还是分布估计提供了指导，并开发了有效的决策代理，能够在学习复杂性增加有限的情况下，提升决策质量。

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>


### [154] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/abs/2505.03955)
*Charupriya Sharma,Iñaki Estella Aguerri,Daniel Guimarans*

Main category: cs.LG

TL;DR: 该研究提出 FlowRec，一种基于网络流优化的分层预测一致性新方法，它克服了现有方法在网络结构和计算效率上的限制，并能高效处理动态更新。


<details>
  <summary>Details</summary>
Motivation: 现有分层预测一致性方法（如MinT）主要局限于树状结构，计算成本高昂，且难以适应通用网络结构和动态变化的需求，限制了其在需要多层级聚合预测一致性的组织中的应用。

Method: 将分层预测一致性问题重新表述为网络流优化问题，提出 FlowRec 方法。该方法利用网络结构信息，证明了在 $\ell_{p > 0}$ 范数和严格凸且连续可微损失函数下的多项式时间可解性。FlowRec 还支持高效的局部更新和有误差界限的近似一致化，以应对动态场景。

Result: FlowRec 扩展了MinT方法以支持通用网络结构，并将计算复杂度从MinT的 $O(n^3)$ 降低到稀疏网络下的 $O(n^2\log n)$。实验表明，FlowRec 在准确性上有所提升，运行时间加快了3-40倍，内存使用减少了5-7倍。此外，它能有效地进行局部更新并保持最优性。

Conclusion: FlowRec 是一种强大、高效且灵活的分层预测一致化工具，特别适用于大规模和动态的分层预测应用，显著优于现有方法。

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>


### [155] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/abs/2505.03977)
*Guilherme S. Imai Aldeia,Hengzhe Zhang,Geoffrey Bomarito,Miles Cranmer,Alcides Fonseca,Bogdan Burlacu,William G. La Cava,Fabrício Olivetti de França*

Main category: cs.LG

TL;DR: 本文提出了一个更新版的符号回归基准测试工具SRBench，扩展了评估方法、改进了指标和可视化，并分析了模型复杂性、准确性和能耗的权衡，强调没有单一最优算法，并呼吁社区共同维护和发展该基准。


<details>
  <summary>Details</summary>
Motivation: 符号回归（SR）方法多样，缺乏统一的基准测试标准，使得比较和评估SR算法变得困难。

Method: 扩展了原有的SRBench，增加了近一倍的评估方法，改进了评估指标，并使用更优的可视化方法来理解性能。同时，分析了模型复杂度、准确性和能耗之间的权衡关系。

Result: 研究结果表明，没有单一的符号回归算法能在所有数据集上都表现最佳。

Conclusion: 呼吁符号回归社区共同维护和发展SRBench，将其作为一个反映领域最新进展的动态基准，通过标准化超参数调整、执行约束和计算资源分配来实现。同时，提出了基准维护的弃用标准和改进SR算法的最佳实践，如自适应超参数调整和节能实现。

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>


### [156] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/abs/2505.03980)
*Aroon Sankoh,Victor Wickerhauser*

Main category: cs.LG

TL;DR: 本文比较了最大似然估计（MLE）和循环神经网络（RNN）在估计 Ornstein-Uhlenbeck 过程参数方面的准确性和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统的随机微分方程参数估计方法可能不如新兴的深度学习技术（如循环神经网络RNN）精确，后者有望提供更精确的估计器。

Method: 通过一系列实验，比较统计方法（MLE）和深度学习模型（RNN）在估计 Ornstein-Uhlenbeck 过程参数时的准确性和计算成本。

Result: 研究展示了 MLE 和 RNN 在 Ornstein-Uhlenbeck 过程参数估计的准确性和计算开销方面的比较结果。

Conclusion: 该研究为评估深度学习模型（RNN）在 Ornstein-Uhlenbeck 过程参数估计中相较于传统统计方法（MLE）的潜力提供了实验比较。

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>


### [157] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/abs/2505.03983)
*Hengyuan Hu,Aniket Das,Dorsa Sadigh,Nima Anari*

Main category: cs.LG

TL;DR: 该研究提出了一种名为自推测解码 (ASD) 的新方法，通过证明DDPM增量的可交换性，显著加速了DDPM的推理过程，且无需辅助草稿模型。


<details>
  <summary>Details</summary>
Motivation: Denoising Diffusion Probabilistic Models (DDPMs) 虽然是强大的生成模型，但其固有的顺序计算需求导致了严重的推理时间瓶颈。

Method: 研究利用DDPM与随机定位之间的联系，证明了在适当的重新参数化下，DDPM的增量满足可交换性。基于这一特性，将自回归模型中广泛使用的推测解码算法扩展到DDPM，提出了自推测解码 (ASD) 方法，该方法不需要任何辅助的草稿模型。

Result: 理论分析表明，ASD 相较于 K 步顺序 DDPM 可实现约 $K^{1/3}$ 倍的并行运行时加速。实际应用也证明，ASD 在各种领域均显著加速了 DDPM 的推理过程。

Conclusion: 通过揭示DDPM增量的可交换性并引入ASD方法，本研究为DDPM的性能优化开辟了新途径，使其能够借鉴自回归模型的加速技术，有效解决了推理速度慢的问题。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>


### [158] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/abs/2505.03992)
*Jarren Briscoe,Garrett Kepler,Daryl Deford,Assefaw Gebremedhin*

Main category: cs.LG

TL;DR: 论文揭示了分类评估指标中因组合学效应产生的样本量偏差，特别是在比较大小悬殊的群体时，这会影响偏差评估的准确性，并为此提出了一种模型无关的评估与校正技术。


<details>
  <summary>Details</summary>
Motivation: 现有模型评估方法在评估社会影响，特别是比较不同规模群体时的偏差时，未能充分考虑到分类指标本身因组合学原理而产生的样本量偏差，这可能导致对算法公平性的误判。

Method: 通过分析多种常用分类评估指标中存在的组合学偏差，提出了一种模型无关的评估和校正技术。同时，分析了指标计算中未定义情况对评估结果的潜在误导。

Result: 证实了组合学效应会导致分类指标产生样本量偏差，尤其在比较规模差异大的群体时，会降低偏差评估的精确度。不当处理指标计算中的未定义情况也会导致评估失准。

Conclusion: 该研究指出了标准评估实践中一个先前未被充分认识的挑战——组合学偏差，并提出了改进方法，以促进更公平和可信的分类模型评估。

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>


### [159] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/abs/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: 论文指出 Muon 优化器在大规模应用时，其迭代正交化过程可能因随机矩阵奇异值随规模缩小而出现问题，并通过理论和实验验证了此现象。


<details>
  <summary>Details</summary>
Motivation: Muon 优化器作为 Adam 优化器的潜在替代品受到关注，但其在大规模场景下迭代正交化过程的稳定性存在潜在问题，即随机矩阵的奇异值会随着规模的增大而缩小。

Method: 通过理论分析和对随机矩阵的经验性（实验）研究，展示了随机矩阵奇异值随规模变化的缩放行为。

Result: 理论和实验均表明，随机矩阵的奇异值会随着规模的增大而缩小，这可能对 Muon 优化器中的正交化过程产生不利影响。

Conclusion: 论文揭示了 Muon 优化器在大规模应用时其迭代正交化过程因随机矩阵奇异值随规模缩小而存在的潜在问题，但并未提出解决方案。

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


### [160] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/abs/2505.04046)
*Xuyang Wang,Siyuan Duan,Qizhi Li,Guiduo Duan,Yuan Sun,Dezhong Peng*

Main category: cs.LG

TL;DR: 提出了一种名为RDML的框架，通过证据解耦、特征重校准和视图级证据注意力机制来解决可信多视图学习中的对抗性不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 现有的可信多视图学习方法假设数据安全，但在实际应用中，多视图数据常受对抗性扰动威胁，导致对抗性不可靠问题（AUP）。

Method: 提出了可靠解耦多视图学习（RDML）框架：1. 使用证据解耦学习将各视图分解为干净和对抗性部分；2. 采用特征重校准模块减轻对抗扰动影响并提取信息特征；3. 设计视图级证据注意力机制忽略不可修复的对抗干扰。

Result: 在带有对抗性攻击的多视图分类任务上，RDML的表现显著优于当前最先进的多视图学习方法。

Conclusion: RDML框架能有效应对可信多视图学习中的对抗性不可靠问题，提升了模型在对抗攻击下的性能和可靠性。

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>


### [161] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/abs/2505.04075)
*Teddy Foley,Spencer Guo,Henry Josephson,Anqi Qu,Jack Sanderson*

Main category: cs.LG

TL;DR: 本文研究了在不增加计算资源的情况下，大语言模型（LLM）通过算法创新是否能持续进步，并区分了计算依赖型和计算无关型创新。


<details>
  <summary>Details</summary>
Motivation: 鉴于监管机构主要限制高性能硬件的获取，研究旨在探讨LLM在计算资源受限环境下能否继续发展，以及算法创新在此类条件下的表现。

Method: 引入一个新的分类框架，区分计算依赖型创新（如Transformer架构）和计算无关型创新（如RoPE、FlashAttention）。使用“计算等效增益”（CEG）指标量化贡献，并通过缩减版GPT-2模型的小规模训练实验进行验证。

Result: 实验表明，计算无关型创新即使在资源受限的情况下也能带来显著性能提升（CEG高达3.5倍），而计算依赖型创新在小规模实验中效益甚微甚至降低性能。

Conclusion: 算法创新，特别是计算无关型创新，可以在不增加额外计算资源的情况下显著提升LLM能力。然而，计算资源的可用性对某些算法（计算依赖型）的收益仍然至关重要。

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>


### [162] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/abs/2505.04083)
*Aditya K. Ranjan,Siddharth Singh,Cunyang Wei,Abhinav Bhatele*

Main category: cs.LG

TL;DR: Plexus是一种针对大规模图神经网络（GNN）全图训练的三维并行方法，旨在解决现有方法的内存限制、精度损失、训练缓慢、高通信开销和负载不均衡问题，并实现了显著的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有GNN训练方法在处理大规模图时面临挑战：1. 小批量采样虽能扩展，但可能导致精度下降及训练缓慢；2. 分布式全图训练存在高通信开销和负载不均衡问题。

Method: 提出Plexus，一种用于全图训练的三维（3D）并行方法。该方法包含优化措施，如用于负载均衡的置换方案（permutation scheme）和用于预测最优3D配置的性能模型。

Result: Plexus可扩展至十亿边规模的图，并在Perlmutter（高达2048个GPU）和Frontier（高达2048个GCD）上进行了评估。与现有方法相比，Plexus实现了2.3倍至12.5倍的加速，并将求解时间在Perlmutter上缩短了5.2至8.7倍，在Frontier上缩短了7至54.2倍。

Conclusion: Plexus通过其三维并行方法及优化策略，有效解决了大规模GNN全图训练中的瓶颈问题，显著提升了训练速度和可扩展性。

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>


### [163] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/abs/2505.04104)
*Sarah Hartman,Cheng Soon Ong,Julia Powles,Petra Kuhnert*

Main category: cs.LG

TL;DR: 论文主张采用负责任的、应用驱动的方法（RAD-AI）进行人工智能研究，以实现有意义的科学和社会进步。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能日益融入社会，研究人员需关注其应用的具体情境，回应伦理、法律、技术和社会约束及公众讨论。

Method: 提出一个三阶段方法推动RAD-AI研究：(1) 建立跨学科团队和以人为中心的研究；(2) 解决特定情境的方法、伦理承诺、假设和度量标准；(3) 通过分阶段的试验平台和实践社区测试并维持效能。

Result: （预期）通过适应社群具体需求和价值观的技术可行方法，释放新的价值。

Conclusion: 展望未来，应用驱动的人工智能研究将通过适应其最终服务社群的情境需求和价值观的技术可行方法，来解锁新的价值。

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>


### [164] [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)
*David Noever,Forrest McKee*

Main category: cs.LG

TL;DR: 一项利用金融建模世界杯 (FMWC) Excel 竞赛题目评估大型语言模型 (LLM) 的新基准。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估缺乏针对实际商业任务的基准，本研究旨在建立一个更贴近实际应用的评估框架，弥合学术AI基准与实际商业应用之间的差距。

Method: 将113个FMWC Excel竞赛题目转换为可编程评估的JSON格式，并用此数据集比较多个主流LLM的性能。

Result: 不同LLM在各类挑战中表现差异显著，模型在模式识别任务上表现出特定优势，但在复杂数值推理方面存在困难。

Conclusion: 该基准为评估LLM在真实商业导向任务中的能力提供了标准化框架，并通过将Excel熟练程度作为有意义的评估指标，为AI基准测试领域做出了贡献。

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>


### [165] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/abs/2505.04139)
*Hongyi Li,Jun Xu,William Ward Armstrong*

Main category: cs.LG

TL;DR: 本文介绍了一种名为学习超平面树 (LHT) 的新型倾斜决策树模型，它采用非迭代、统计驱动的方法构建分裂超平面，旨在实现富有表现力且可解释的分类。


<details>
  <summary>Details</summary>
Motivation: 现有倾斜决策树方法常依赖迭代优化或启发式搜索，缺乏一种直接、统计驱动且易于解释的超平面构建机制，同时对模型表达能力和可解释性有较高需求。

Method: LHT通过非迭代的统计方法构建分裂超平面：直接基于节点内不同类别间特征期望的差异计算特征权重，进而确定超平面参数。叶节点利用局部最小二乘法拟合的分段线性隶属函数进行预测。

Result: 证明了LHT分裂过程的收敛性，确保生成非空有效分区。构建深度为d的LHT的时间复杂度为O(mnd)。实验结果表明，LHT在基准数据集上表现出有竞争力的准确性，并且其显式特征加权提供了良好的可解释性。

Conclusion: LHT是一种实用、具有坚实理论基础且可解释的倾斜决策树模型，为现有树基模型提供了一个有竞争力的新选择。

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>


### [166] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04158)
*Yulong Wang,Yushuo Liu,Xiaoyi Duan,Kai Wang*

Main category: cs.LG

TL;DR: FilterTS是一种新的基于频域滤波的多变量时间序列预测模型，通过动态和静态滤波模块有效提取周期和趋势成分，提高了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多元时间序列预测中难以准确捕捉复杂的周期性和趋势性成分，而准确提取这些成分对提升预测性能至关重要。

Method: 提出了FilterTS模型，该模型在频域运行。它包含一个动态跨变量滤波模块，利用其他变量作为滤波器来提取和加强共享频率成分；以及一个静态全局滤波模块，用于捕捉整个训练集中的稳定频率成分。模型将时域卷积转换为频域乘法运算以提高效率。

Result: 在八个真实世界数据集上的大量实验结果表明，FilterTS在预测准确性和计算效率方面均显著优于现有方法。

Conclusion: FilterTS通过其新颖的频域滤波技术，能够有效提取多元时间序列中的复杂模式，从而显著提升预测性能和计算效率。

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>


### [167] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/abs/2505.04161)
*Baida Zhang,Yakai Chen,Huichun Li,Zhenghu Zu*

Main category: cs.LG

TL;DR: 本研究提出了一种基于个体智能体传播模型（Covasim）的强化学习决策框架，用于优化传染病干预策略，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传染病爆发对健康和经济造成严重影响，制定有效干预措施面临挑战。现有强化学习方法多局限于微分方程模型，或采用过于简化的个体模型，无法充分模拟传染病传播的复杂性和动态性。

Method: 建立了一个基于个体智能体传播模型（修改版Covasim）的决策框架，利用强化学习来持续探索和制定策略函数。对多种算法在不同行动空间的应用效果进行了全面探索，并对“时间覆盖”问题进行了初步理论分析。

Result: 实验结果有力地验证了该研究方法框架的有效性和可行性。由此获得的应对策略在抑制疫情规模扩大和维护经济系统稳定方面非常有效。

Conclusion: 该研究提出的方法框架为制定全球公共卫生安全策略提供了重要的参考视角，其策略能有效控制疫情并保护经济。

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>


### [168] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/abs/2505.04163)
*Sungwon Han,Seungeon Lee,Meeyoung Cha,Sercan O Arik,Jinsung Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种名为RAFT的检索增强时间序列预测方法，通过检索与当前输入最相似的历史数据模式来辅助预测，并显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 为时间序列预测模型提供更充分的归纳偏置，并补充模型的学习能力，以更好地利用历史数据中的模式。

Method: RAFT方法：在预测未来时，从训练数据集中直接检索与输入模式最相似的历史数据候选项，并利用这些候选项的未来值与当前输入共同进行预测。

Result: 在十个基准数据集上的实验评估显示，RAFT方法持续优于当前的基准方法，平均胜率达到86%。

Conclusion: 通过检索模块从外部提供历史模式信息，RAFT这种简单的方法能够有效增强模型的预测能力，为时间序列预测提供了一种有效的新途径。

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>


### [169] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04167)
*Yulong Wang,Xiaofeng Hu,Xiaojian Cui,Kai Wang*

Main category: cs.LG

TL;DR: STRGCN是一种无需预对齐、直接在图上建模不规则多变量时间序列（IMTS）的时空关系图卷积网络，通过全连接图表示和分层结构优化，实现了高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有处理不规则多变量时间序列（IMTS）的方法通常依赖预对齐，这会扭曲数据固有模式并增加计算和内存开销。因此，需要一种能直接处理异步数据且高效的方法。

Method: 提出STRGCN（时空关系图卷积网络）。它将IMTS表示为一个全连接图，每个观测值为一个节点，从而直接捕捉异步关系，无需预对齐。此外，采用分层的“三明治”结构聚合节点，优化图嵌入，减少计算开销。

Result: 在四个公共数据集上的实验表明，STRGCN达到了最先进的准确率，并具有有竞争力的内存使用和训练速度。

Conclusion: STRGCN模型能够有效处理IMTS的异步性，无需预对齐，并在保持局部和全局上下文的同时，实现了高精度和计算效率，优于现有方法。

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>


### [170] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/abs/2505.04173)
*Zixiao Wang,Wenqian Zhao,Yunheng Shen,Yang Bai,Guojin Chen,Farzan Farnia,Bei Yu*

Main category: cs.LG

TL;DR: DiffPattern-Flex 是一种新方法，它结合离散扩散模型和优化型合法性评估，高效生成可靠的版图图案。


<details>
  <summary>Details</summary>
Motivation: 现有的深度生成模型在版图图案生成中难以保证合法性，这在实际应用中引发了担忧。

Method: 提出了 DiffPattern-Flex：1) 使用离散扩散模型生成多样化拓扑，同时保持无损且计算高效的版图表示；2) 采用基于优化的、基于特定设计规则的白盒图案评估过程来确保图案合法性；3) 采用快速采样和高效合法化技术加速生成过程。

Result: 实验结果表明，在各种基准测试中，DiffPattern-Flex 显著优于现有方法，并且在生成可靠的版图图案方面表现出色。

Conclusion: DiffPattern-Flex 是一种能够高效生成可靠版图图案的新颖方法。

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>


### [171] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/abs/2505.04193)
*Bang You,Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 该研究提出了一种通过最小化动作轨迹熵来学习简单且鲁棒的强化学习策略的方法，从而提高智能体在复杂控制任务中的性能和对扰动的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制任务中表现优异，但其策略容易捕捉观察与动作间的复杂和虚假关联，导致在环境发生微小扰动时失败。因此，需要引入对简单策略的归纳偏置以增强鲁棒性。

Method: 通过最小化整个动作轨迹的熵来引入对简单策略的归纳偏置。该轨迹熵（描述智能体观察状态轨迹后动作轨迹中信息所需的比特数）通过学习一个变分参数化动作预测模型进行有效估计，并用于构建信息正则化的奖励函数。作者提出了一个名为“轨迹熵强化学习”的算法，用于联合优化策略和预测模型。

Result: 在多个高维运动任务上的实验评估表明，所学习的策略能产生更具周期性和一致性的动作轨迹，并且在性能、对噪声的鲁棒性以及对动态变化的适应性方面均优于现有顶尖方法。

Conclusion: 最小化动作轨迹熵是一种有效的归纳偏置，能够促使强化学习智能体学习到更简单、更鲁棒且性能更优的策略。

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>


### [172] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/abs/2505.04196)
*Sung Yoo Lim,Hyunsoo Yun,Prateek Bansal,Dong-Kyu Kim,Eui-Jin Kim*

Main category: cs.LG

TL;DR: 提出一种结合大型语言模型（LLM）和贝叶斯网络（BN）的微调方法，通过BN的拓扑排序控制LLM的自回归生成，以生成更可行且多样化的合成人口数据，优于传统深度生成模型和专有LLM。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在生成合成人口时，难以平衡包含稀有但合理的组合与排除不合理的组合，影响了下游活动模拟的有效性。

Method: 采用一种混合方法：对大型语言模型（LLM）进行微调，并通过从贝叶斯网络（BN）推导出的拓扑排序来显式控制其自回归生成过程。

Result: 该混合LLM-BN方法在可行性上达到约95%，显著高于传统深度生成模型（约80%）和少样本学习的专有LLM（如ChatGPT-4o），同时保持了相当的多样性。该方法基于轻量级开源LLM，可在标准个人计算环境进行微调和推理。

Conclusion: 该方法能够生成高质量的合成人口，提高了基于活动的模型（ABM）仿真的整体可靠性，减少了下游误差传播，并且由于基于开源轻量级LLM，具有成本效益和可扩展性，适用于大规模应用。

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>


### [173] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/abs/2505.04200)
*Ahmed Sayeed Faruk,Jason Sulskis,Elena Zheleva*

Main category: cs.LG

TL;DR: 本文提出了基于聚类的多臂老虎机（MAB）算法，用于在存在干扰的社交网络中估计总治疗效应，同时最大化预期回报。


<details>
  <summary>Details</summary>
Motivation: 传统的A/B测试在社交网络中面临两大挑战：个体间的干扰效应（一个人的处理可能影响他人结果）以及当某个治疗方案效果不佳时可能导致的高昂性能损失。因此，需要一种能够随时间自适应调整并有效学习网络中总治疗效应的策略。

Method: 研究引入了两种基于聚类的多臂老虎机（MAB）算法。这些算法通过在探索（学习新信息）和利用（应用当前最佳方案）之间进行权衡，逐步估计网络中的总治疗效应，并旨在最大化累积奖励。研究人员将所提出的MAB算法与忽略聚类效应的普通MAB算法以及相应的随机对照试验（RCT）方法，在模拟了干扰效应的半合成数据上进行了性能比较。

Result: 普通MAB算法虽然获得了较高的奖励-行动比率，但由于未考虑个体间的溢出效应，导致其在治疗效应估计上的误差较大。相比之下，基于聚类的MAB算法不仅比其对应的RCT方法展现出更高的奖励-行动比率，而且在治疗效应估计的准确性方面没有显著损失。

Conclusion: 基于聚类的多臂老虎机算法在存在网络干扰的情况下，能够有效地平衡最大化回报和准确估计治疗效应这两个目标，表现优于传统的随机对照试验和忽略聚类结构的普通MAB算法。

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>


### [174] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/abs/2505.04204)
*Mateo Lopez-Ledezma,Gissel Velarde*

Main category: cs.LG

TL;DR: 本文评估了多种机器学习分类器和不平衡学习技术在网络安全二元分类任务中的表现，发现不平衡学习技术效果不一，且不同数据集的最佳模型不同。


<details>
  <summary>Details</summary>
Motivation: 网络安全日益重要，自动化处理海量日常操作势在必行。许多网络安全应用（如异常检测、欺诈检测）可归结为二元分类问题，这些问题常伴有数据不平衡的挑战。

Method: 研究通过三个实验进行：1) 评估了多种单一分类器（随机森林、LightGBM、XGBoost、逻辑回归、决策树、梯度提升决策树）；2) 测试了不同的采样技术（过采样、欠采样、SMOTE、Self-Paced Ensembling）；3) 评估了Self-Paced Ensembling及其基分类器的数量。

Result: 研究发现，不平衡学习技术既可能产生积极影响，也可能产生消极影响。此外，对于不同的数据集，表现最佳的分类器各不相同。

Conclusion: 结论强调，应谨慎应用不平衡学习技术。建议针对每一个新的数据集和应用场景（尤其是在涉及不平衡数据的网络安全领域），都应分别测试单一分类器和不平衡学习技术的实际效果。

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>


### [175] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/abs/2505.04223)
*Sanghyeon Park,Soo-Mook Moon*

Main category: cs.LG

TL;DR: FRAIN 是一种新的异步联邦学习方法，通过 FastSync 策略和球面线性插值 (SLERP) 聚合参数，解决了现有方法的局限性，实现了更稳定和鲁棒的收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法存在一些问题：同步方法（如 FedAvg）会被慢速设备拖慢；异步方法（如 FedAsync）在非独立同分布数据和陈旧更新下易导致客户端漂移；基于区块链的方法（如 BRAIN）在数据异构性高或陈旧度高时性能仍会下降，且其无聚合器架构引入了新的同步开销。

Method: 提出了 FRAIN (Fast-and-Reliable AI Network)，一种新的异步联邦学习方法，包含两个关键思想：1. FastSync 策略：新加入者和不频繁参与者无需重放历史模型版本即可高效近似全局模型。2. SLERP (球面线性插值)：在合并参数时保留模型的方向，减轻发散的本地训练带来的破坏性干扰。

Result: 实验表明，在使用 CNN 图像分类模型和基于 Transformer 的语言模型时，FRAIN 比 FedAvg、FedAsync 和 BRAIN 实现了更稳定和鲁棒的收敛，尤其在非独立同分布数据、网络延迟和需要频繁重新同步以及存在恶意节点的恶劣环境下表现更优。

Conclusion: FRAIN 通过创新的 FastSync 策略和 SLERP 参数聚合方法，有效缓解了现有异步联邦学习方法的局限性，提高了模型训练的稳定性和鲁棒性，特别适用于复杂的联邦学习环境。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>


### [176] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/abs/2505.04241)
*Grzegorz Miebs,Rafał A. Bachorz*

Main category: cs.LG

TL;DR: 本文提出了一种从产品3D模型直接预测制造步骤及其持续时间的数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 传统生产时间估算方法依赖专家分析或历史数据，在动态或定制化生产环境中往往表现不佳，难以满足高效制造调度的需求。

Method: 通过将3D模型渲染成多个2D图像，并利用一种受生成式查询网络（Generative Query Network）启发的神经网络，学习将几何特征映射到预定义生产步骤的时间估算。

Result: 该方法能够实现跨不同产品类型的可扩展、自适应和精确的流程规划。

Conclusion: 所提出的数据驱动方法为直接从3D模型进行生产时间估算和流程规划提供了一种有效且适应性强的途径，有助于提升制造计划的精确度。

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>


### [177] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/abs/2505.04263)
*Jan Blechschmidt,Tom-Christian Riemer,Max Winkler,Martin Stoll,Jan-F. Pietschmann*

Main category: cs.LG

TL;DR: 提出一种基于物理信息深度学习（DeepONet）和边域分解的方法，用于解决度量图上的非线性漂移-扩散方程。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理应用于细胞运输、人群运动等领域的度量图上的非线性漂移-扩散方程时，尤其在模型设计和参数辨识方面，定制化程度高且复杂。

Method: 首先针对代表性的流入边、内部边和流出边分别训练三个DeepONet模型，然后通过基于边的域分解方法耦合这些模型，以解决度量图上的漂移-扩散问题。

Result: 该框架能够准确评估图耦合的物理模型，并且非常适合解决这些耦合网络上的优化或反问题。

Conclusion: 所提出的方法为度量图上的非线性漂移-扩散方程及其相关的优化和反问题提供了一个准确且通用的新框架。

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>


### [178] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.04278)
*Weiwei Ye,Zhuopeng Xu,Ning Gui*

Main category: cs.LG

TL;DR: NsDiff是一种基于扩散模型的新型时间序列预测框架，它通过采用位置尺度噪声模型（LSNM）和不确定性感知的噪声调度，有效捕捉了时间序列中随时间变化的不确定性，并在预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去噪扩散概率模型（DDPMs）通常依赖加性噪声模型（ANM），其固有的恒定方差假设限制了模型捕捉时间序列中普遍存在的非平稳（时变）不确定性的能力。

Method: 本文提出了非平稳扩散模型（NsDiff）。核心创新在于：1. 使用位置尺度噪声模型（LSNM）替代ANM，以适应变化的方差。2. NsDiff 将基于去噪扩散的条件生成模型与预训练的条件均值和方差估计器相结合，实现自适应端点分布建模。3. 提出一种不确定性感知的噪声调度机制，该机制动态调整噪声水平以反映数据在每一步的不确定性，并将时变方差融入扩散过程。

Result: 在九个真实世界数据集和合成数据集上进行的大量实验结果表明，NsDiff 在时间序列概率预测方面显著优于现有的基线方法。

Conclusion: NsDiff 通过有效建模时间序列的动态不确定性，为非平稳时间序列的概率预测提供了一个更准确和鲁棒的框架，并验证了其优越性。

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>


### [179] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/abs/2505.04318)
*Jacob Glenn Ayers,Buvaneswari A. Ramanan,Manzoor A. Khan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>


### [180] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/abs/2505.04335)
*Swagato Das,Arghya Pratihar,Swagatam Das*

Main category: cs.LG

TL;DR: 提出了一种新的聚类算法HypeFCM，它结合模糊聚类、双曲几何和权重过滤机制，以更好地表示非欧几里得空间中的数据关系。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法（如FCM）在处理复杂、高维、非欧几里得数据集时表现不佳，因为欧几里得空间假设限制了其捕捉复杂数据结构的能力。

Method: 引入了基于过滤的双曲模糊C均值（HypeFCM）算法。该算法整合了模糊聚类原理与双曲几何，并使用基于权重的过滤机制。它通过狄利克雷分布初始化权重，并在庞加莱圆盘模型中使用双曲度量迭代更新聚类中心和隶属度。

Result: 广泛的实验评估表明，HypeFCM在非欧几里得设置中显著优于传统的模糊聚类方法。

Conclusion: HypeFCM算法在非欧几里得空间的数据聚类中展现了其鲁棒性和有效性。

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>


### [181] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04338)
*Zichen Liu,Wei Zhang,Christof Schütte,Tiejun Li*

Main category: cs.LG

TL;DR: 提出了一种黎曼去噪扩散概率模型 (RDDPMs)，用于在由函数水平集定义的欧几里得子流形上学习数据分布。


<details>
  <summary>Details</summary>
Motivation: 现有流形上的生成模型依赖于大量的几何信息（如测地线或拉普拉斯-贝尔特拉米算子的特征函数），这限制了它们在无法轻易获得此类信息的更一般流形上的应用。

Method: 提出基于投影方案的 RDDPMs，该方法仅需评估定义子流形的函数值及其一阶导数，从而避免了对复杂几何信息的依赖。

Result: RDDPMs 成功应用于先前研究的数据集以及两个新的高维流形（SO(10) 和具有固定二面角的丙氨酸二肽分子系统的构型空间）采样的数据集。理论分析揭示了 RDDPMs 与流形上基于分数的生成模型在连续时间极限下的联系。

Conclusion: RDDPMs 是一种更通用的流形数据生成方法，它降低了对几何信息的要求，从而扩展了可应用流形的范围，并在多个数据集上展示了其有效性。

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>


### [182] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.04339)
*Hao Peng,Xiang Huang,Shuo Sun,Ruitong Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体强化学习的自适应鲁棒DBSCAN算法 (AR-DBSCAN)，以解决传统DBSCAN在处理多密度数据集时的参数选择难题。


<details>
  <summary>Details</summary>
Motivation: 传统DBSCAN算法在处理具有不同密度尺度的数据集时，难以获得满意的聚类结果，而这在实际应用中很常见。

Method: 1. 将初始数据集建模为两级编码树，并根据信息不确定性将数据点分为不同密度分区。2. 为每个分区分配一个智能体，通过多智能体深度强化学习自动搜索最佳聚类参数，将参数调整过程建模为马尔可夫决策过程，并使用弱监督奖励训练策略网络。3. 提出一种适应数据规模的递归搜索机制。

Result: 在九个人工数据集和一个真实世界数据集上的实验表明，AR-DBSCAN在NMI和ARI指标上分别将聚类精度提高了144.1%和175.3%，并且能够稳健地找到主导参数。

Conclusion: AR-DBSCAN框架能够有效处理不同密度分布的数据集，通过多智能体强化学习自适应地寻找最优聚类参数，显著提高了聚类准确性和鲁棒性。

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>


### [183] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/abs/2505.04340)
*Hong Jin,Kaicheng Zhou,Jie Yin,Lan You,Zhifeng Zhou*

Main category: cs.LG

TL;DR: 本文提出MGA-HHN，一种基于多粒度注意力的异构超图神经网络，通过构建元路径引导的异构超图和创新的注意力机制，有效捕获高阶语义关系并缓解信息过度挤压，提升异构图节点表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有异构图神经网络（HeteGNNs）主要依赖基于元路径的成对消息传递，难以捕获节点间的高阶关系，并且在长程消息传递中存在“过度挤压”导致信息失真的问题，限制了模型性能。

Method: 提出MGA-HHN模型，核心方法包括：1) 构建基于元路径的异构超图，从多视图显式建模高阶语义信息；2) 设计节点级和超边级相结合的多粒度注意力机制，以捕捉同语义上下文内的细粒度交互并保留不同语义类型的多样性。

Result: 在多个真实世界基准数据集上的实验表明，MGA-HHN在节点分类、节点聚类和可视化任务上均显著优于当前最先进的模型。

Conclusion: MGA-HHN通过其创新的异构超图构建方法和多粒度注意力机制，能够有效学习更具表达力的节点表示，缓解信息扭曲，并捕获高阶语义关系，为异构图表示学习提供了更优的解决方案。

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>


### [184] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/abs/2505.04346)
*Arghya Pratihar,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 提出了一种新的拓扑聚类算法，利用Vietoris-Rips复形、Betti数过滤和Betti序列来聚类复杂交织的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法，特别是拓扑聚类算法，在处理高度复杂、形状交织的数据集时存在局限性，未能充分利用拓扑结构，导致性能不一致。

Method: 通过Vietoris-Rips复形和Betti数过滤识别拓扑相似的邻居，并引入Betti序列概念以灵活捕捉拓扑结构的基本特征。

Result: 在多个合成数据集和真实数据集上的实验表明，与一些著名的基于拓扑的聚类算法相比，该算法表现出值得称赞的性能。

Conclusion: 该算法能够有效地对数据集中复杂、交织的形状进行聚类，克服了现有拓扑聚类算法的一些局限性。

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>


### [185] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/abs/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TL;DR: 本论文研究利用深度学习技术开发工具，旨在减少住宅能耗和优化电动汽车充电，以加速能源转型。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型面临挑战，高碳排放能源淘汰缓慢，可再生能源难以独力实现气候目标。因此，需探索减少住宅和交通等关键领域能耗的替代路径以加速转型，应对气候变化。

Method: 开发新颖的深度学习技术：1) 针对住宅能耗，利用非侵入式负荷监控（NILM）赋能终端用户；2) 针对道路交通脱碳，采用深度强化学习优化电动汽车充电。

Result: 成功开发了基于深度学习的新技术和工具，用于通过非侵入式负荷监控减少住宅能耗，以及通过深度强化学习优化电动汽车充电策略以促进道路交通脱碳。

Conclusion: 所开发的深度学习技术为减少住宅能源消耗和实现道路交通脱碳提供了有效的解决方案，有助于加速能源转型进程。

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>


### [186] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/abs/2505.04371)
*Filipe Santos,João Paulo Fernandes,Luís Macedo*

Main category: cs.LG

TL;DR: 该研究将经典和量子版本的基于标记的强化学习探索策略应用于四子棋游戏。结果显示，两种标记策略均优于epsilon-greedy策略，量子版本能更快采样到标记动作，但两者的最终胜率相同。


<details>
  <summary>Details</summary>
Motivation: 研究旨在检验基于标记的强化学习探索策略在不同于跳棋的四子棋游戏中的性能和泛化能力，并引入新的评估指标（获取标记动作的平均迭代次数），同时探究四子棋中后手劣势对该方法性能的影响。

Method: 研究训练并测试了经典和量子强化学习智能体，这些智能体在四子棋游戏中分别执先手或后手，对抗一个随机化的Negamax对手。智能体采用了基于标记的探索策略，并与epsilon-greedy策略进行比较。

Result: 结果表明，无论是经典还是量子版本的基于标记的探索策略，其性能都明显优于简单的epsilon-greedy策略。此外，量子智能体确实能以更少的迭代次数采样到标记动作。尽管如此，经典版本和量子版本方法之间的胜率是相同的。

Conclusion: 基于标记的探索策略是有效的。量子版本在采样标记动作方面展现了迭代次数上的优势，但在当前四子棋的简单训练场景下，这种优势并未转化为更高的胜率。这可能归因于所选训练场景的简单性。

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>


### [187] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/abs/2505.04389)
*Jenni Lampainen,Kaisa Joki,Napsu Karmitsa,Marko M. Mäkelä*

Main category: cs.LG

TL;DR: 介绍了一种名为 Clust-Splitter 的高效聚类算法，该算法基于非光滑优化，旨在解决超大规模数据集中的最小平方和聚类问题。


<details>
  <summary>Details</summary>
Motivation: 聚类是数据挖掘和机器学习中的基本任务，尤其对于大规模数据分析至关重要。现有方法在处理超大规模数据集的最小平方和聚类问题时可能面临效率挑战，因此需要更高效的算法。

Method: 提出了 Clust-Splitter 算法。该算法通过一系列三个非光滑优化问题（两个辅助问题生成起始点，一个主聚类问题）来解决聚类任务。它结合了限制内存束方法和增量方法来求解这些问题。

Result: 在具有大量属性和数据点的真实数据集上进行了评估。实验结果表明，Clust-Splitter 在处理超大规模数据集时效率很高，并且其解决方案的质量与现有最佳方法相当。

Conclusion: 提出的 Clust-Splitter 方法对于超大规模数据集的聚类是有效的，并且能够提供与现有最佳方法相媲美的高质量解决方案。

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>


### [188] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/abs/2505.04396)
*Jingnan Wang,Jie Chao,Shangshang Yang,Congyi Nai,Kaijun Ren,Kefeng Deng,Xi Chen,Yaxin Liu,Hanqiuzi Wen,Ziniu Xiao,Lifeng Zhang,Xiaodong Wang,Jiping Guan,Baoxiang Pan*

Main category: cs.LG

TL;DR: 提出一种结合学习的高分辨率气候先验与粗网格预报的新方法，可为风电场提供准确、精细、快速的天气预报，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可再生能源（尤其是风电）的规划和运营依赖准确、及时、高分辨率的天气信息，而现有降尺度方法面临尺度不一致、计算成本高和不确定性来源复杂等挑战。

Method: 通过学习目标风电场的高分辨率数值天气模拟的气候分布，并将其作为先验信息与粗网格大尺度预报进行优化组合。

Result: 与现有数值/统计预报降尺度方法相比，该方法在确定性/概率性预报技巧和经济效益方面均表现出优势。此外，一个包含100个成员、持续10天、空间分辨率1公里、输出频率15分钟的预报任务，在GPU上耗时小于1小时，远低于传统数值模拟所需的数千CPU小时。

Conclusion: 该方法在大幅降低计算成本的同时保持了预报准确性，为更高效、可靠的可再生能源规划和运营铺平了道路。

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [189] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/abs/2505.04412)
*Ren Wang,Pengcheng Zhou*

Main category: cs.LG

TL;DR: 该研究提出了一种基于自编码器的新方法，通过集成流形重构层，旨在从噪声数据中发现潜在流形结构，并在降维过程中保持其拓扑和几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有的流形学习方法在处理高维噪声数据时，往往难以同时捕捉局部细节与全局拓扑完整性，或难以实现平衡的降维，从而导致嵌入结果失真或断裂。

Method: 该研究提出了一种基于自编码器（AutoEncoder）的方法。该方法集成了一个流形重构层，用于从噪声点云中发现潜在的流形结构，并在降维过程中对拓扑和几何特性施加正则化。流形重构和正则化这两个组件在训练过程中相互促进。

Result: 在点云数据集上的实验表明，所提出的方法在从噪声数据中发现流形结构并通过降维保留这些结构方面，其性能优于 t-SNE、UMAP 和拓扑自编码器（Topological AutoEncoders）等基线方法。结果通过可视化和定量指标进行了验证。

Conclusion: 该研究表明，将流形重构与流形学习相结合，对于实现潜在流形的可靠表示至关重要，尤其是在处理充满噪声的真实世界数据时。

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>


### [190] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/abs/2505.04417)
*Georg A. Gottwald,Shuigen Liu,Youssef Marzouk,Sebastian Reich,Xin T. Tong*

Main category: cs.LG

TL;DR: 提出一种局部化扩散模型，通过利用数据局部结构克服高维生成任务中的维度灾难，并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型在估计高维分数函数时可能面临维度灾难问题，因此需要更好地理解和利用目标分布中的低维结构（如局部性）来提高模型效率和性能。

Method: 提出局部化扩散模型。该模型利用数据中的局部性结构（组件间的稀疏依赖），假设分数函数因此是有效低维的。通过使用局部化分数匹配损失，在局部化假设空间内训练一个局部化的神经网络来估计分数函数。

Result: 理论和实验均证明，这种局部化方法能使扩散模型规避维度灾难，但会引入额外的局部化误差。通过选择适度的局部化半径，可以在统计误差和局部化误差之间取得平衡，从而提升整体性能。此外，局部化结构也有利于并行训练，提高大规模应用效率。

Conclusion: 局部化扩散模型通过利用数据的局部性结构，有效克服了高维数据生成中的维度灾难问题，并通过平衡统计误差与局部化误差提高了性能，同时为大规模并行训练提供了潜力。

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>


### [191] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/abs/2505.04435)
*Vahideh Hayyolalam,Öznur Özkasap*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦黑寡妇优化 (FedBWO) 技术，通过让客户端仅传输性能得分而非完整的模型权重，来减少联邦学习中的通信数据量，并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，资源受限的客户端由于传输容量有限，传输大量模型权重会导致通信瓶颈，影响系统性能和可扩展性。

Method: 引入联邦黑寡妇优化 (FedBWO) 技术。在该技术中，客户端仅向服务器传输一个性能得分，而不是本地模型的全部权重。同时，利用黑寡妇优化 (BWO) 算法来改进本地模型的更新过程。

Result: 实验结果表明，FedBWO 显著提升了全局模型的性能和整个系统的通信效率。与 FedAvg 相比，全局模型准确率平均提高了21%；与 FedGWO 相比，平均提高了12%。此外，FedBWO 大幅降低了通信成本。

Conclusion: FedBWO 是一种有效的联邦学习优化方法，能够显著减少客户端与服务器之间的通信数据量，同时提高全局模型的准确性，解决了资源受限设备在联邦学习中的通信瓶颈问题。

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>


### [192] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/abs/2505.04440)
*Xiaozheng Qu,Zhaochuan Li,Zhuang Qi,Xiang Li,Haibei Huang,Lei Meng,Xiangxu Meng*

Main category: cs.LG

TL;DR: 提出IR-ART算法，通过迭代优化（簇稳定性检测、不稳定簇删除、警戒区域扩展）来降低Fuzzy ART聚类对警戒参数的敏感性，提高其鲁棒性和易用性。


<details>
  <summary>Details</summary>
Motivation: Fuzzy ART聚类算法的性能高度依赖于预设的警戒参数，其值的偏差会导致聚类结果显著波动，限制了非专家用户的实际应用。现有方法通常引入额外超参数或复杂框架，违背了算法的简洁性初衷。

Method: 提出迭代优化自适应共振理论 (IR-ART)，包含三个关键阶段的统一迭代框架：(1) 簇稳定性检测：通过分析迭代中簇内样本数量的变化来识别不稳定簇。(2) 不稳定簇删除：进化剪枝模块，消除低质量簇。(3) 警戒区域扩展：自适应调整相似度阈值的机制。这三个阶段独立于聚类执行，专注于分析迭代过程中的隐性知识，调整权重和警戒参数。

Result: 在15个数据集上的实验评估表明，IR-ART提高了对次优警戒参数值的容忍度，同时保留了Fuzzy ART的参数简洁性。案例研究直观地证实了该算法通过迭代优化的自优化能力。

Conclusion: IR-ART通过其迭代优化机制，有效提升了Fuzzy ART对警戒参数的鲁棒性，同时保持了算法的简洁性，使其特别适合资源受限场景下的非专家用户。

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>


### [193] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/abs/2505.04441)
*Mirazul Haque,Petr Babkin,Farima Farmahinifarahani,Manuela Veloso*

Main category: cs.LG

TL;DR: 本文研究了在自动程序修复（APR）中，如何通过将程序执行轨迹整合到大型语言模型（LLM）的提示中来提升其性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR方法大多仅依赖程序的静态分析，忽略了其运行时行为，本研究旨在通过引入程序执行轨迹来弥补这一缺陷。

Method: 研究团队将程序执行轨迹整合到标准APR提示中，使用GPT系列模型在三个APR数据集上进行评估，并探索了多种利用轨迹的提示策略，包括LLM优化的提示。

Result: 简单整合执行轨迹仅在少数情况下带来有限性能提升，且轨迹复杂性增加会降低其有效性。然而，LLM优化的提示策略能更稳定地超越无轨迹基线。基于轨迹的提示优于在小数据集上微调小型LLM，且执行轨迹能补充LLM的推理能力。

Conclusion: 虽然简单整合执行轨迹效果不显著，但通过精心设计的提示策略，特别是LLM优化的提示，程序执行轨迹能够有效增强大型语言模型在自动程序修复任务中的性能，并补充其推理能力。

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>


### [194] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2505.04461)
*Pengfei Jiao,Hongjiang Chen,Xuan Guo,Zhidong Zhao,Dongxiao He,Di Jin*

Main category: cs.LG

TL;DR: 本文综述了时序交互图表示学习（TIGRL），提出了一种新的分类方法，整理了相关数据集和基准，并探讨了未来的研究挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 时序交互图（TIGs）因其能建模复杂动态系统行为而应用广泛。时序交互图表示学习（TIGRL）旨在学习节点的低维表示以保留结构和时序信息，从而提升下游任务性能，因此近年来备受关注。

Method: 本文首先介绍TIGs的基础概念并强调时间依赖性。接着，基于学习过程中利用的信息类型，提出一个全面的先进TIGRL方法分类体系。此外，还整理了数据集和基准的来源，并探讨了开放性挑战和未来研究方向。

Result: 本文提供了一个TIGRL领域的概览，包括：对TIGs及其时间依赖重要性的介绍；一个基于信息利用类型的TIGRL方法分类体系；一个整理过的数据集和基准资源列表；以及对未来研究方向和开放挑战的探讨。

Conclusion: 本文为时序交互图表示学习（TIGRL）领域提供了基础性概述和系统性分类，整理了研究资源，并指出了未来的研究方向和挑战，为该领域的进一步发展奠定了基础。

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>


### [195] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/abs/2505.04464)
*Louis Ohl,Fredrik Lindsten*

Main category: cs.LG

TL;DR: 提出了一种基于集成聚类和一致性矩阵的新颖聚类模型评估和排序方法，该方法能有效处理具有不同聚类定义和约束的模型。


<details>
  <summary>Details</summary>
Motivation: 评估聚类模型性能具有挑战性，现有指标难以处理具有不同聚类定义的多个模型，也难以集成约束条件。

Method: 受一致性聚类启发，提出一种通过集成聚类构建判别性排序的方法。该方法基于聚类模型的连通性与（通过集成多个模型得到的）一致性矩阵之间的距离。

Result: 提出的评分方法在合成数据上验证有效，能将最匹配一致性结果的模型排在前面。在比较不同聚类算法（簇数不固定且兼容约束条件）时，该排序得分显著优于其他评分方法。

Conclusion: 提出了一种有效的聚类模型评估和排序方法，该方法能够处理不同聚类定义和约束，并通过与一致性结果的比较来对模型进行排序。

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>


### [196] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/abs/2505.04468)
*Hyeju Shin,Kyudan Jung,Seongwon Yun,Juyoung Yun*

Main category: cs.LG

TL;DR: 本文介绍了一种名为FFTKF的差分隐私优化方法，通过结合频域噪声整形和卡尔曼滤波来提升梯度质量，在保持隐私保证的同时改善模型效用。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私随机梯度下降（DP-SGD）中，为了保护隐私而添加的噪声通常会降低模型效用。本研究旨在解决这一挑战，提高模型性能。

Method: 提出FFT增强卡尔曼滤波器（FFTKF）。该方法首先在傅里叶域使用高频整形掩码将差分隐私噪声集中到信息量较少的光谱分量中，以保护低频梯度信号。然后，使用带有有限差分Hessian近似的标量增益卡尔曼滤波器进一步优化去噪后的梯度。

Result: FFTKF在MNIST、CIFAR-10、CIFAR-100和Tiny-ImageNet等数据集上，使用CNN、Wide ResNets和Vision Transformers进行测试时，其测试准确性优于DP-SGD和DiSK。每次迭代的复杂度为O(d log d)。理论分析证实FFTKF在保持等效隐私保证的同时，通过减少噪声和控制偏差实现了更优的隐私-效用权衡。

Conclusion: FFTKF是一种有效的差分隐私优化方法，它通过频域噪声整形和卡尔曼滤波，在不牺牲隐私保证的前提下，显著提升了模型的效用，实现了更紧密的隐私-效用权衡。

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>


### [197] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/abs/2505.04471)
*Vincent Souveton,Sébastien Terrana*

Main category: cs.LG

TL;DR: 提出了一种基于哈密顿信息归一化流的新方法，用于高效模拟 Vlasov-Poisson 等保守物理系统的相空间演化。


<details>
  <summary>Details</summary>
Motivation: Vlasov-Poisson 方程等复杂保守物理系统的解析解难以获得，而现有数值方法（如 Particle-In-Cell）计算成本高，需要更高效的建模与预测工具。

Method: 使用一种哈密顿信息归一化流（特别是固定动能神经哈密顿流的变体），通过一系列源自哈密顿动力学的可逆、保体积变换，将初始高斯相空间分布转换为最终分布。模型在由数值模拟生成的、固定时间 T 的初始和最终状态数据集上进行训练。

Result: 训练后的模型能够从任何给定的初始状态快速采样最终分布。此外，通过自动学习一个可解释的物理势，模型能够泛化到训练期间未见过的中间状态。

Conclusion: 该方法不仅实现了对系统最终状态的快速预测，还能通过学习物理上有意义的势函数，提供对系统随时间演化过程的洞察，并能推广到未见过的中间状态。

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>


### [198] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TL;DR: 提出MAD算法，高效融合多摄像头视图用于机器人操作，提高样本效率并支持轻量化部署。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，使用多摄像头扩展视野以实现鲁棒的视觉伺服，但这带来了巨大的计算挑战，且传统融合方案部署成本高。

Method: 引入一种“合并与解耦”（MAD）算法，该算法有效地合并多个视图以提高样本效率，同时用单视图特征进行增强，以实现轻量化部署并确保策略的鲁棒性。

Result: 在Meta-World和ManiSkill3基准测试中，该方法展示了其效率和鲁棒性。

Conclusion: MAD算法通过高效的多视图融合和单视图特征增强，为机器人操作提供了样本高效、部署轻量且策略鲁棒的解决方案。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


### [199] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/abs/2505.04535)
*Michail Theologitis,Vasilis Samoladas,Antonios Deligiannakis*

Main category: cs.LG

TL;DR: 本文提出FDA-Opt算法，一种用于联邦学习中微调大型语言模型的新方法，它通过动态同步优化了通信效率和模型性能，优于现有的FedOpt和FDA算法。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中微调大型语言模型面临参数通信频繁、开销大的挑战。现有主流算法FedOpt依赖固定的同步间隔，而FDA算法虽引入动态同步但存在参数难调和同步方案僵化的问题。

Method: 提出了FDA-Opt算法系列。该方法统一并扩展了FDA和FedOpt的原理，旨在解决它们的核心局限性，实现更灵活高效的模型同步。

Result: 在多种自然语言处理下游任务的语言模型微调实验中，FDA-Opt的表现持续优于FedOpt，即使在针对FedOpt优化的超参数设置下也是如此。

Conclusion: FDA-Opt是FedOpt在现代联邦学习库和系统中的一个实用、即插即用的替代方案，无需额外配置即可提供卓越的开箱即用性能。

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>


### [200] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/abs/2505.04558)
*Wenzhao Liu,Haoran Li,Congying Han,Zicheng Zhang,Anqi Li,Tiande Guo*

Main category: cs.LG

TL;DR: 该论文揭示了旅行商问题 (TSP) 的纯度定律 (PuLa)，并提出纯度策略优化 (PUPO) 方法，通过将神经解与此定律对齐来增强神经 TSP 求解器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经方法在解决旅行商问题 (TSP) 时，跨不同规模和分布的泛化能力不足，因为它们难以学习到识别通用模式和推导最优解的鲁棒原则。

Method: 首先，论文揭示了纯度定律 (PuLa)，即边出现的频率随周围顶点稀疏度的增加而指数级增长。基于此，提出了纯度策略优化 (PUPO)，一种新的训练范式，在解的构建过程中将神经解的特征与 PuLa 对齐。

Result: PUPO 可以无缝集成到流行的神经求解器中，显著提高它们的泛化性能，并且在推理过程中不会引入额外的计算开销。

Conclusion: 通过将神经解与新发现的纯度定律 (PuLa) 相结合，纯度策略优化 (PUPO) 能够有效提升神经 TSP 求解器在不同实例间的泛化表现。

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [201] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/abs/2505.04560)
*Guanghui Wang,Zhiyong Yang,Zitai Wang,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.LG

TL;DR: 该研究提出ABKD，一个基于α-β散度的知识蒸馏框架，旨在解决现有方法中“硬度集中”和“置信度集中”效应的失衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法（如前向KLD和反向KLD）在平衡学生模型对“难学习模式”（硬度集中）的关注和对“高置信度模式”（置信度集中）的关注时存在极端情况：前向KLD中两种效应均太弱，导致学生模型难以聚焦目标类别；反向KLD中两种效应均太强，导致学生模型过度强调目标类别而忽略教师提供的广泛分布信息。

Method: 提出ABKD框架，采用α-β散度作为知识蒸馏的损失函数。该框架通过调整α和β参数，能够在前向KLD和反向KLD之间进行平滑插值，从而实现对硬度集中效应和置信度集中效应的有效权衡。

Result: 理论分析表明，ABKD能够提供从前向KLD到反向KLD的平滑过渡，并有效平衡两种模式集中效应。在17个语言/视觉数据集和12种不同的师生模型设置下进行的大量实验，均证实了ABKD框架的有效性。

Conclusion: ABKD通过引入α-β散度，为知识蒸馏提供了一个更灵活且有效的框架，能够更好地平衡关键的模式集中效应，从而提升学生模型的性能，优于传统的基于前向KLD或反向KLD的方法。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [202] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/abs/2505.04566)
*Lucas R. C. Farias,Talita P. Silva,Pedro H. M. Araujo*

Main category: cs.LG

TL;DR: 本文提出了一种基于LSTM的多任务学习模型，用于同时预测巴西累西腓市的虫媒病毒（登革热、基孔肯雅热、寨卡病毒）暴发和病例数。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决同时预测虫媒病毒暴发（检测）和病例数（预测）的需求，并探索在数据有限的公共卫生场景下，统一建模策略在可扩展流行病预测中的可行性和优势。

Method: 采用基于长短期记忆（LSTM）网络的多任务学习方法。利用DataSUS 2017-2023年的历史公共卫生数据，模型同时执行二元分类（暴发检测）和回归（病例数预测）任务。使用滑动窗口策略（输入长度为60、90、120天）构建时间特征，并通过Keras Tuner进行超参数优化。采用时间序列交叉验证和2023年保留测试集进行模型评估。

Result: 结果表明，较长的时间窗口能提高登革热回归任务的准确性，而分类任务的性能在中间长度的窗口达到最佳，这表明序列长度和泛化能力之间存在最佳权衡。该多任务架构在不同疾病和任务上均表现出有竞争力的性能。

Conclusion: 该研究证明了所提出的多任务LSTM模型在统一预测虫媒病毒暴发和病例数方面的可行性和优势，为数据有限的公共卫生场景下可扩展的流行病预测提供了有前景的统一建模策略。

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>


### [203] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/abs/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 研究表明恶意强化学习微调能高效破坏大语言模型的安全护栏，并提出了一种名为“奖励中和”的新防御框架以有效抵抗此类攻击。


<details>
  <summary>Details</summary>
Motivation: 恶意强化学习（RL）微调能够高效地移除大语言模型的安全机制，尤其威胁可访问参数的开源模型。现有的针对监督微调的防御方法对此类动态反馈攻击无效，因此需要新的防御策略。

Method: 首先，通过实验验证了恶意强化学习微调攻击的有效性。然后，提出了一种名为“奖励中和”（Reward Neutralization）的防御框架。该框架通过训练模型产生信息量极少的拒绝回答，使攻击者无法利用反馈进行恶意优化，从而使恶意奖励信号失效。

Result: 实验表明，恶意强化学习微调仅需50步和少量对抗性提示即可将模型的有害性评分从0-2提升至7-9。而提出的“奖励中和”方法在200次攻击步骤后仍能将模型的有害性评分维持在较低水平（不高于2），而标准模型则迅速恶化。

Conclusion: 这项工作首次建设性地证明了针对日益普及的强化学习攻击的稳健防御是可行的，填补了开放权重模型在安全方面的一个关键空白。

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>


### [204] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/abs/2505.04599)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文研究了在 $(L_0, L_1)$-光滑条件下，非凸随机优化中自适应算法（如AdaGrad）的复杂度下界，表明这些算法对问题参数的依赖性（例如二次方）高于标准光滑设定下的SGD，揭示了 $(L_0, L_1)$-光滑设定对某些自适应算法本质上更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 尽管自适应算法（如AdaGrad）在 $(L_0, L_1)$-光滑条件下收敛，但其收敛率对问题参数的依赖是高阶多项式，可能显著大于SGD在标准 $L$-光滑设定下的最优复杂度。目前尚不清楚这种高阶依赖性是否可以被改善或是否是固有的。

Method: 研究者们通过分析几种自适应优化算法（包括三种AdaGrad变体和一类具有自适应步长的SGD）在 $(L_0, L_1)$-光滑设定下的复杂度下界，重点关注其对问题参数 $\Delta, L_0, L_1$ 的依赖程度。

Result: 研究为三种AdaGrad变体提供了复杂度下界，显示它们对问题参数 $\Delta, L_0, L_1$ 至少存在二次方依赖。值得注意的是，AdaGrad-Norm的去相关变体找到一个 $\epsilon$-稳定点至少需要 $\Omega ( \Delta^2 L_1^2 \sigma^2 \epsilon^{-4} )$ 次随机梯度查询。此外，还为一类具有自适应步长的SGD提供了下界。

Conclusion: 研究结果表明，对于某些自适应算法，在初始最优性差距和光滑常数方面，$(L_0, L_1)$-光滑设定从根本上比标准光滑设定更困难。这意味着观察到的高阶依赖性可能并非当前分析的局限，而是该问题设定的固有特性。

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>


### [205] [Testing Juntas Optimally with Samples](https://arxiv.org/abs/2505.04604)
*Lorenzo Beretta,Nathaniel Harms,Caleb Koch*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>


### [206] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/abs/2505.04608)
*Drew Prinster,Xing Han,Anqi Liu,Suchi Saria*

Main category: cs.LG

TL;DR: 本文提出一种加权共形检验鞅 (WCTMs) 方法，用于在线监测AI/ML系统中任何意外的数据分布变化点，同时控制误报，并能适应轻微的协变量漂移。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中负责任地部署人工智能/机器学习系统，不仅需要系统可靠性的证明，还需要持续的部署后监控以快速检测和解决任何不安全行为。现有监测方法在监测假设类别、报警标准以及在线适应数据漂移方面存在局限性。

Method: 提出了一种加权共形检验鞅 (WCTMs) 的通用框架。基于此框架，设计了具体的WCTM算法，该算法能够在线适应轻微的协变量（边际输入分布）漂移，并对更严重的漂移（如概念漂移或难以适应的极端协变量漂移）发出警报。

Result: 在真实世界数据集上的实验表明，所提出的WCTM方法相对于现有最先进的基线方法，在性能上有所提升。

Conclusion: WCTMs为在线监测AI/ML系统中任意意外的数据分布变化点提供了一个理论基础，并能有效控制误报。所提出的具体算法能够适应轻微的协变量漂移，同时对严重的分布变化发出警报，扩展了现有监测方法的适用范围。

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [207] [Minimum Congestion Routing of Unsplittable Flows in Data-Center Networks](https://arxiv.org/abs/2505.03908)
*Miguel Ferreira,Nirav Atre,Justine Sherry,Michael Dinitz,João Luís Sobrinho*

Main category: cs.NI

TL;DR: 本文研究了 Clos 网络中不可分割流的最小拥塞路由问题。研究表明，最小拥塞至少为3/2，且难以近似到此因子以下。论文提出了一种多项式时间算法，可将拥塞保证在9/5以内，并证明在线算法无法达到优于2的近似比。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心通常依赖启发式方法路由不可分割流，其最差情况下的拥塞为2。本研究旨在探索是否可能实现更低的拥塞，并为此设计高效的路由算法，以优化网络性能。

Method: 1. 通过理论分析，证明了最小拥塞的下界（3/2）以及近似该问题的NP-hard性。
2. 设计了一种新的多项式时间路由算法，用于处理不可分割流，并保证其拥塞上界。
3. 对在线路由场景进行了分析，证明了其近似性能的下限。

Result: 1. 证明了在某些情况下，Clos 网络中不可分割流的最小拥塞至少为3/2，且在小于3/2的因子内近似最小拥塞路由是NP-hard问题。
2. 提出了一种多项式时间算法，该算法保证对于任何流集合，拥塞最多为9/5，并提供了对最小拥塞路由的9/5近似。
3. 证明了在线环境下，没有任何算法（包括随机算法）能够以小于2的因子近似最小拥塞路由。

Conclusion: 本研究不仅成功将 Clos 网络中不可分割流的已知拥塞上界从2改进到9/5，还揭示了离线和在线路由设置之间存在本质的性能差异，即在线算法在近似最小拥塞方面无法达到与离线算法相同的性能水平。

Abstract: Millions of flows are routed concurrently through a modern data-center. These
networks are often built as Clos topologies, and flow demands are constrained
only by the link capacities at the ingress and egress points. The minimum
congestion routing problem seeks to route a set of flows through a data center
while minimizing the maximum flow demand on any link. This is easily achieved
by splitting flow demands along all available paths. However, arbitrary flow
splitting is unrealistic. Instead, network operators rely on heuristics for
routing unsplittable flows, the best of which results in a worst-case
congestion of $2$ (twice the uniform link capacities). But is $2$ the lowest
possible congestion? If not, can an efficient routing algorithm attain
congestion below $2$?
  Guided by these questions, we investigate the minimum congestion routing
problem in Clos networks with unsplittable flows. First, we show that for some
sets of flows the minimum congestion is at least $\nicefrac{3}{2}$, and that it
is $NP$-hard to approximate a minimum congestion routing by a factor less than
$\nicefrac{3}{2}$. Second, addressing the motivating questions directly, we
present a polynomial-time algorithm that guarantees a congestion of at most
$\nicefrac{9}{5}$ for any set of flows, while also providing a
$\nicefrac{9}{5}$ approximation of a minimum congestion routing. Last, shifting
to the online setting, we demonstrate that no online algorithm (even
randomized) can approximate a minimum congestion routing by a factor less than
$2$, providing a strict separation between the online and the offline setting.

</details>


### [208] [Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in Covert Communications](https://arxiv.org/abs/2505.04068)
*Yuanai Xie,Zhaozhi Liu,Xiao Zhang,Shihua Zhang,Rui Hou,Minrui Xu,Ruichen Zhang,Dusit Niyato*

Main category: cs.NI

TL;DR: 该研究提出影子无线智能（SWI），一种集成大语言模型（LLMs）与检索增强生成技术的新方法，用于在6G网络中实现隐蔽通信（CC）的智能决策和实时策略自适应。


<details>
  <summary>Details</summary>
Motivation: 传统基于人工噪声、功率控制和信道操纵的隐蔽通信优化方法，难以适应6G动态和对抗环境中高维度、非线性和严格实时隐蔽性的要求。

Method: 引入影子无线智能（SWI），利用具有强化学习推理能力的混合专家大语言模型DeepSeek-R1，结合领域知识的实时检索增强生成。该方法开发了结构化的隐蔽通信知识库，支持上下文感知检索和语义优化，使LLM能够实时生成和调整隐蔽通信策略。

Result: 在全双工隐蔽通信场景中优化人工噪声功率的案例研究中，DeepSeek-R1在符号推导方面实现了85%的准确率，在仿真代码生成方面实现了94%的正确率，表现优于基线模型。

Conclusion: 影子无线智能（SWI）被验证为6G网络中LLM驱动的智能隐蔽无线系统的一个强大、可解释和自适应的基础。

Abstract: Covert Communications (CC) can secure sensitive transmissions in industrial,
military, and mission-critical applications within 6G wireless networks.
However, traditional optimization methods based on Artificial Noise (AN), power
control, and channel manipulation might not adapt to dynamic and adversarial
environments due to the high dimensionality, nonlinearity, and stringent
real-time covertness requirements. To bridge this gap, we introduce Shadow
Wireless Intelligence (SWI), which integrates the reasoning capabilities of
Large Language Models (LLMs) with retrieval-augmented generation to enable
intelligent decision-making in covert wireless systems. Specifically, we
utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning,
combined with real-time retrieval of domain-specific knowledge to improve
context accuracy and mitigate hallucinations. Our approach develops a
structured CC knowledge base, supports context-aware retrieval, and performs
semantic optimization, allowing LLMs to generate and adapt CC strategies in
real time. In a case study on optimizing AN power in a full-duplex CC scenario,
DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in
the generation of simulation code, outperforming baseline models. These results
validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven
intelligent covert wireless systems in 6G networks.

</details>


### [209] [Satellite-Assisted Low-Altitude Economy Networking: Concepts, Applications, and Opportunities](https://arxiv.org/abs/2505.04098)
*Shizhao He,Jiacheng Wang,Ying-Chang Liang,Geng Sun,Dusit Niyato*

Main category: cs.NI

TL;DR: 该文研究了卫星辅助低空经济（LAE）网络，提出一个包含分布式MIMO和双时间尺度优化的框架，以解决地面网络覆盖不足及功能局限的问题，并提升卫星与低空飞行器（LAV）间的通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有地面网络在支持低空经济（LAE）方面存在覆盖范围有限（尤其偏远地区）和无法充分支持定位导航等功能的挑战。卫星因其广泛覆盖和多样功能，为支持低空飞行器（LAV）提供了潜力。

Method: 提出了一种卫星辅助的低空经济网络框架，该框架包含分布式卫星MIMO、分布式低空飞行器MIMO以及一个双时间尺度优化方案，旨在解决卫星与低空飞行器通信中严重的路径损耗和高动态性问题。

Result: 案例研究表明，所提出的分布式MIMO架构能有效降低所需传输功率并延长服务持续时间，而双时间尺度优化方案则平衡了性能与控制信令开销。

Conclusion: 卫星辅助网络，特别是采用分布式MIMO和双时间尺度优化方案的框架，能够有效提升低空经济运营中的通信可靠性和效率，是一种有前景的技术方案。

Abstract: The low-altitude economy (LAE) is a new economic paradigm that leverages
low-altitude vehicles (LAVs) to perform diverse missions across diverse areas.
To support the operations of LAE, it is essential to establish LAE networks
that enable LAV management and communications.Existing studies mainly reuse
terrestrial networks to construct LAE networks. However, the limited coverage
of terrestrial networks poses challenges for serving LAVs in remote areas.
Besides, efficient LAV operations also require support such as localization and
navigation, which terrestrial networks designed for communications cannot fully
provide. Due to ubiquitous coverage and diverse functions, satellites are a
promising technology to support LAVs. Therefore, this article investigates
satellite-assisted LAE networking. First, we introduce an overview of LAE and
satellites, discussing their features, applications, and architectures. Next,
we investigate opportunities for satellites to assist LAE from aspects of
communication, control, and computation. As all assistance depends on reliable
satellite-LAV communications, we propose a satellite-assisted LAE framework to
tackle issues caused by the severe path loss and high dynamics in
satellite-assisted LAE networks.The case study demonstrates that the
distributed MIMO architecture efficiently reduces the required transmission
power and extends service duration, while the two-timescale optimization scheme
balances the performance and control signaling overheads. Specifically, the
proposed framework comprises distributed satellite MIMO, distributed LAV MIMO,
and a two-timescale optimization scheme.

</details>


### [210] [Joint Task Offloading and Channel Allocation in Spatial-Temporal Dynamic for MEC Networks](https://arxiv.org/abs/2505.04272)
*Tianyi Shi,Tiankui Zhang,Jonathan Loo,Rong Huang,Yapeng Wang*

Main category: cs.NI

TL;DR: 该研究针对移动边缘计算（MEC）中由终端移动性和任务依赖性引起动态时空特性挑战，提出了一种基于双重决斗深度Q网络（D3QN）的联合优化任务卸载与资源分配方案，以最小化长期延迟-能量权衡成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算系统中，终端移动性导致计算请求在空间上动态分布，任务依赖性带来时间约束，现有资源难以满足大规模复杂应用的需求，因此需要优化计算卸载和资源分配策略。

Method: 首先设计优先级评估方案解耦任务依赖；然后构建分组背包问题进行信道分配，考虑当前数据负载和信道状态；接着采用双重决斗深度Q网络（D3QN）进行卸载决策，并将信道分配结果作为D3QN奖励的一部分，实现任务卸载和信道分配的联合优化。

Result: 全面的仿真结果表明，所提出的算法在延迟-能量权衡成本方面表现优越，并且对各种应用场景具有良好的适应性。

Conclusion: 该研究提出的联合优化算法能够有效应对MEC系统中任务卸载和资源分配的动态时空挑战，在延迟和能耗之间取得了良好的平衡。

Abstract: Computation offloading and resource allocation are critical in mobile edge
computing (MEC) systems to handle the massive and complex requirements of
applications restricted by limited resources. In a multi-user multi-server MEC
network, the mobility of terminals causes computing requests to be dynamically
distributed in space. At the same time, the non-negligible dependencies among
tasks in some specific applications impose temporal correlation constraints on
the solution as well, leading the time-adjacent tasks to experience varying
resource availability and competition from parallel counterparts. To address
such dynamic spatial-temporal characteristics as a challenge in the allocation
of communication and computation resources, we formulate a long-term
delay-energy trade-off cost minimization problem in the view of jointly
optimizing task offloading and resource allocation. We begin by designing a
priority evaluation scheme to decouple task dependencies and then develop a
grouped Knapsack problem for channel allocation considering the current data
load and channel status. Afterward, in order to meet the rapid response needs
of MEC systems, we exploit the double duel deep Q network (D3QN) to make
offloading decisions and integrate channel allocation results into the reward
as part of the dynamic environment feedback in D3QN, constituting the joint
optimization of task offloading and channel allocation. Finally, comprehensive
simulations demonstrate the performance of the proposed algorithm in the
delay-energy trade-off cost and its adaptability for various applications.

</details>


### [211] [Design and Evaluation of an NDN-Based Network for Distributed Digital Twins](https://arxiv.org/abs/2505.04326)
*Chen Chen,Zihan Jia,Ze Wang,Lin Cui,Fung Po Tso*

Main category: cs.NI

TL;DR: 本文提出在命名数据网络（NDN）上使用分布式数字孪生（DT），以解决传统IP网络在处理DT海量数据时响应延迟高的问题，并通过仿真验证了其在边缘场景下能显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DT）产生海量数据，现有基于IP的网络在数据获取方面存在响应延迟高和数据分发效率低下的问题，难以满足DT应用的需求。

Method: 提出在命名数据网络（NDN）上以分布式方式部署数字孪生。利用NDN的数据中心特性、内容名称路由、网络内缓存和自适应路由来优化数据获取。通过仿真比较了NDN和IP网络在边缘场景与云场景下DT的性能。

Result: 广泛的仿真结果表明，在边缘场景下，通过NDN使用数字孪生，响应延迟相比IP网络降低了10.2倍。

Conclusion: 命名数据网络（NDN）凭借其网络内缓存和自适应路由等优势，非常适合未来G网络环境下的数字孪生应用，尤其是在分布式边缘场景中。该文是对NDN上分布式DT的初步探索性研究。

Abstract: Digital twins (DT) have received significant attention due to their numerous
benefits, such as real-time data analytics and cost reduction in production. DT
serves as a fundamental component of many applications, encompassing smart
manufacturing, intelligent vehicles, and smart cities. By using Machine
Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently
facilitate decision-making and productivity by simulating the status and
changes of a physical entity. To handle the massive amount of data brought by
DTs, it is challenging to achieve low response latency for data fetching over
existing IP-based networks. IP-based networks use host addresses for end-to-end
communication, making data distribution between DTs inefficient. Thus, we
propose to use DTs in a distributed manner over Named Data Networking (NDN)
networks. NDN is data-centric where data is routed based on content names,
dynamically adjusting paths to optimize latency. Popular data is cached in
network nodes, reducing data transmission and network congestion. Since data is
fetched by content names, users and mobile devices can move freely without IP
address reassignment. By using in-network caching and adaptive routing, we
reckon NDN is an ideal fit for Future G Networks in the context of Digital
Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and
IP-based networks to validate our insights. Extensive simulation results show
that using DT in the edge reduces response latency by 10.2x. This position
paper represents an initial investigation into the gap in distributed DTs over
NDN, serving as an early-stage study.

</details>


### [212] [Pipelining Split Learning in Multi-hop Edge Networks](https://arxiv.org/abs/2505.04368)
*Wei Wei,Zheng Lin,Tao Li,Xuanheng Li,Xianhao Chen*

Main category: cs.NI

TL;DR: 该研究提出一种针对多跳边缘网络的流水线化分裂学习方案，通过联合优化模型切分、放置和微批次大小，以最小化端到端训练延迟。


<details>
  <summary>Details</summary>
Motivation: 现有分裂学习（SL）大多关注两层模型切分，而针对多跳SL的现有方案未能解决资源闲置问题，导致显著的网络空闲时间。

Method: 1. 提出流水线化分裂学习方案，将模型切分与放置（MSP）问题映射为最小化瓶颈成本和线性成本加权和的问题。
2. 基于图论设计了一种瓶颈感知的最短路径算法来获得MSP的最优解。
3. 推导了给定MSP结果下流水线中微批次大小的闭式解。
4. 开发了一种MSP和微批次大小的交替优化算法，以最小化端到端训练延迟。

Result: 大量仿真实验表明，与未使用流水线并行性的现有基准相比，所提出的算法具有显著优势，能有效减少端到端训练延迟。

Conclusion: 通过将流水线并行应用于分裂学习，并联合优化模型切分、放置以及微批次大小，可以有效解决多跳边缘网络中的资源闲置问题，并显著降低大规模模型训练的延迟。

Abstract: To support large-scale model training, split learning (SL) enables multiple
edge devices/servers to share the intensive training workload. However, most
existing works on SL focus solely on two-tier model splitting. Moreover, while
some recent works have investigated the model splitting and placement problems
for multi-hop SL, these solutions fail to overcome the resource idleness issue,
resulting in significant network idle time. In this work, we propose a
pipelined SL scheme by addressing the joint optimization problem of model
splitting and placement (MSP) in multi-hop edge networks. By applying pipeline
parallelism to SL, we identify that the MSP problem can be mapped to a problem
of minimizing the weighted sum of a bottleneck cost function (min-max) and a
linear cost function (min-sum). Based on graph theory, we devise a
bottleneck-aware shortest-path algorithm to obtain the optimal solution.
Besides, given the MSP outcomes, we also derive the closed-form solution to the
micro-batch size in the pipeline. Finally, we develop an alternating
optimization algorithm of MSP and micro-batch size to solve the joint
optimization problem to minimize the end-to-end training latency. Extensive
simulations have demonstrated the significant advantages of our algorithm
compared to existing benchmarks without pipeline parallelism.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [213] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TL;DR: EchoInk-R1是一个强化学习框架，通过新数据集AVQA-R1-6K，增强了多模态大语言模型在同步音视频输入上的结构化跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在处理结构化的跨模态推理，特别是在整合音频和视觉信号方面存在困难。

Method: 提出了EchoInk-R1，一个基于Qwen2.5-Omni-7B并使用组相对策略优化（GRPO）进行优化的强化学习框架。该框架针对同步音视频对上的多项选择问答任务，并为此创建了AVQA-R1-6K数据集。

Result: EchoInk-R1-7B在验证集上达到了85.77%的准确率，显著优于基础模型的80.53%，且仅用了562个强化学习步骤。此外，模型展现出反思性推理能力。

Conclusion: 轻量级的强化学习微调能够有效增强MLLMs的跨模态推理能力。EchoInk-R1是首个通过强化学习统一音频、视觉和文本模态以实现通用开放世界推理的框架。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


### [214] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/abs/2505.04419)
*Sumit Kumar,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>


### [215] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382)
*Anton Selitskiy,Maitreya Kocharekar*

Main category: eess.AS

TL;DR: 本文提出了一种基于离散最优传输映射的语音转换方法，并证明了其有效性，同时发现该技术可能导致合成音频被误判为真实音频。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换任务中，不同说话者之间音频嵌入的对齐问题，以提升转换质量。

Method: 使用基于向量的接口，并采用离散最优传输映射来对齐不同说话者之间的音频嵌入。

Result: 评估结果表明，所提出的语音转换方法具有高质量和有效性。此外，研究还发现，将离散最优传输作为音频生成的后处理步骤，可能导致合成音频被错误地分类为真实音频。

Conclusion: 该研究成功展示了一种有效的基于离散最优传输的语音转换方法，并揭示了该技术在音频后处理中可能导致合成音频被误判为真实音频的现象。

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [216] [An Empirical Study of OpenAI API Discussions on Stack Overflow](https://arxiv.org/abs/2505.04084)
*Xiang Chen,Jibin Wang,Chaoyang Gao,Xiaolin Ju,Zhanqi Cui*

Main category: cs.SE

TL;DR: 本文通过分析Stack Overflow上关于OpenAI API的讨论，实证研究了开发者在使用这些API时遇到的挑战，并提出了相应的建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）如OpenAI的GPT系列发展迅速，但其API带来了提示工程复杂性、基于token的成本管理、输出不确定性以及黑箱操作等独特挑战。目前缺乏对开发者使用OpenAI API时所遇挑战的实证研究。

Method: 研究分析了来自流行问答论坛Stack Overflow的2,874条与OpenAI API相关的讨论。首先评估了这些帖子的热度和难度，然后将其手动分为九个与OpenAI API相关的类别，并通过主题建模分析识别了每个类别相关的具体挑战。

Result: 研究识别了与九个OpenAI API相关类别相关的具体挑战。 （摘要未详述具体挑战内容，但指明已识别）

Conclusion: 基于实证研究结果，论文为开发者、大型语言模型供应商和研究人员提出了应对使用OpenAI API所遇挑战的可行性建议。

Abstract: The rapid advancement of large language models (LLMs), represented by
OpenAI's GPT series, has significantly impacted various domains such as natural
language processing, software development, education, healthcare, finance, and
scientific research. However, OpenAI APIs introduce unique challenges that
differ from traditional APIs, such as the complexities of prompt engineering,
token-based cost management, non-deterministic outputs, and operation as black
boxes. To the best of our knowledge, the challenges developers encounter when
using OpenAI APIs have not been explored in previous empirical studies. To fill
this gap, we conduct the first comprehensive empirical study by analyzing 2,874
OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We
first examine the popularity and difficulty of these posts. After manually
categorizing them into nine OpenAI API-related categories, we identify specific
challenges associated with each category through topic modeling analysis. Based
on our empirical findings, we finally propose actionable implications for
developers, LLM vendors, and researchers.

</details>


### [217] [Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering](https://arxiv.org/abs/2505.04251)
*Krishna Ronanki*

Main category: cs.SE

TL;DR: 本文提出了一种基于RACI的框架，用于在软件工程中以可信赖的方式在人与基于LLM的多智能体自主系统（LMA系统）之间分配任务，以促进高效协作并降低风险。


<details>
  <summary>Details</summary>
Motivation: 在软件工程领域引入基于LLM的多智能体自主系统（LMA系统）时，如何在人与LMA系统之间进行战略性且可信赖的任务分配是一个主要挑战。

Method: 提出一个基于RACI（责任、问责、咨询、知情）的框架，并提供了实施指南和示例实现。

Result: 提出的框架旨在促进高效协作，确保问责制，减轻与LLM驱动自动化相关的潜在风险，并与可信赖AI指南保持一致。

Conclusion: 本文提出了一种RACI框架来应对LMA系统中人机任务分配的挑战，该框架有助于实现可信赖和高效的协作。未来的工作将进行实证验证。

Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that
spans across multiple domains than singular autonomous agents. This holds true
within the field of software engineering (SE) as well. The state-of-the-art
research on MAS within SE focuses on integrating LLMs at the core of autonomous
agents to create LLM-based multi-agent autonomous (LMA) systems. However, the
introduction of LMA systems into SE brings a plethora of challenges. One of the
major challenges is the strategic allocation of tasks between humans and the
LMA system in a trustworthy manner. To address this challenge, a RACI-based
framework is proposed in this work in progress article, along with
implementation guidelines and an example implementation of the framework. The
proposed framework can facilitate efficient collaboration, ensure
accountability, and mitigate potential risks associated with LLM-driven
automation while aligning with the Trustworthy AI guidelines. The future steps
for this work delineating the planned empirical validation method are also
presented.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [218] [The Evolution of Rough Sets 1970s-1981](https://arxiv.org/abs/2505.03747)
*Viktor Marek,Ewa Orłowska,Ivo Düntsch*

Main category: math.HO

TL;DR: 本文回顾了Zdzisław Pawlak及其合作者在20世纪70年代和1981年的研究与出版物，重点关注其灵感来源以及1981年后粗糙集和信息系统的发展。


<details>
  <summary>Details</summary>
Motivation: 回顾和分析Zdzisław Pawlak的早期研究，识别其灵感来源，并概述其工作（特别是粗糙集和信息系统理论）的后续发展。

Method: 通过回顾和分析Zdzisław Pawlak及其合作者在20世纪70年代和1981年发表的文献。

Result: 识别了Pawlak早期研究的灵感来源，并概述了自1981年以来在粗糙集和信息系统领域的相关进展。

Conclusion: 该研究追溯了Pawlak关于粗糙集和信息系统理论的早期工作及其灵感，并勾勒了其后续发展脉络。

Abstract: In this note research and publications by Zdzis{\l}aw Pawlak and his
collaborators from 1970s and 1981 are recalled. Focus is placed on the sources
of inspiration which one can identify on the basis of those publications.
Finally, developments from 1981 related to rough sets and information systems
are outlined.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [219] [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03745)
*Yanbiao Liang,Huihong Shi,Haikuo Shao,Zhongfeng Wang*

Main category: cs.AR

TL;DR: AccLLM是一个算法-硬件协同设计框架，通过剪枝、Λ形注意力和新颖的量化方案，以及专用的FPGA加速器，加速边缘设备上的长上下文大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型（LLMs）部署到资源受限的边缘设备面临计算密集、模型庞大、内存带宽需求高以及长序列处理能力有限等挑战。

Method: 提出了AccLLM框架。算法层面：集成剪枝、Λ形注意力和W2A8KV4（2位权重、8位激活、4位KV缓存）量化方案。硬件层面：设计了专用的基于FPGA的加速器，配备可重构计算引擎，以适应压缩算法带来的多样化操作。

Result: 在Xilinx Alveo U280 FPGA上进行验证，与现有SOTA工作FlightLLM相比，AccLLM实现了4.07倍的能效提升和2.98倍的吞吐量提升。

Conclusion: AccLLM通过算法和硬件协同设计，为在边缘设备上实现高效、快速的长上下文大语言模型推理提供了一个有效的解决方案。

Abstract: Recently, large language models (LLMs) have achieved huge success in the
natural language processing (NLP) field, driving a growing demand to extend
their deployment from the cloud to edge devices. However, deploying LLMs on
resource-constrained edge devices poses significant challenges, including (1)
intensive computations and huge model sizes, (2) great memory and bandwidth
demands introduced by the autoregressive generation process, and (3) limited
scalability for handling long sequences. To address these challenges, we
propose AccLLM, a comprehensive acceleration framework that enables efficient
and fast long-context LLM inference through algorithm and hardware co-design.
At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped
attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and
4-bit KV cache) quantization scheme, thus effectively reducing memory and
bandwidth requirements while facilitating LLMs' long-sequence generation. At
the hardware level, we design a dedicated FPGA-based accelerator with a
reconfigurable computing engine to effectively and flexibly accommodate diverse
operations arising from our compression algorithm, thereby fully translating
the algorithmic innovations into tangible hardware efficiency. We validate
AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency
and a 2.98x throughput compared to the state-of-the-art work FlightLLM.

</details>


### [220] [APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03748)
*Yonghao Tan,Pingcheng Dong,Yongkun Wu,Yu Liu,Xuejiao Liu,Peng Luo,Shih-Yang Liu,Xijie Huang,Dong Zhang,Luhong Liang,Kwang-Ting Cheng*

Main category: cs.AR

TL;DR: 该研究提出了一种新颖的加性部分和量化（APSQ）方法，用于压缩DNN加速器中的部分和（PSUM），在几乎不损失模型性能的情况下，显著降低了能耗。


<details>
  <summary>Details</summary>
Motivation: DNN加速器中高精度部分和（PSUM）的频繁访问导致过高的内存需求和功耗，而传统压缩策略通常忽略了对PSUM的量化，后者可能占总功耗的69%。

Method: 引入了一种新颖的加性部分和量化（APSQ）方法，将PSUM的累加过程无缝集成到量化框架中。此外，还提出了一种结合APSQ与可重构架构增强的PSUM量化分组策略。

Result: APSQ方法在BERT、Segformer和EfficientViT等自然语言处理（NLP）和计算机视觉（CV）任务中表现几乎无损，同时将PSUM压缩至INT8格式，从而使能耗成本降低了28-87%。在LLaMA2-7B上的扩展实验也证明了APSQ在大型语言模型上的应用潜力。

Conclusion: APSQ是一种有效的PSUM量化技术，能够在保持模型性能的同时显著减少DNN加速器的能耗，并对大型语言模型具有应用前景。

Abstract: DNN accelerators, significantly advanced by model compression and specialized
dataflow techniques, have marked considerable progress. However, the frequent
access of high-precision partial sums (PSUMs) leads to excessive memory demands
in architectures utilizing input/weight stationary dataflows. Traditional
compression strategies have typically overlooked PSUM quantization, which may
account for 69% of power consumption. This study introduces a novel Additive
Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM
accumulation into the quantization framework. A grouping strategy that combines
APSQ with PSUM quantization enhanced by a reconfigurable architecture is
further proposed. The APSQ performs nearly lossless on NLP and CV tasks across
BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This
leads to a notable reduction in energy costs by 28-87%. Extended experiments on
LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is
available at https://github.com/Yonghao-Tan/APSQ.

</details>


### [221] [AI-Powered Agile Analog Circuit Design and Optimization](https://arxiv.org/abs/2505.03750)
*Jinhai Hu,Wang Ling Goh,Yuan Gao*

Main category: cs.AR

TL;DR: 本文介绍了如何运用人工智能（AI）技术，通过多目标贝叶斯优化（MOBO）进行晶体管尺寸调整和AI集成的电路传递函数建模，来改进模拟电路设计，并在跨导器和关键词识别应用中得到验证。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI技术自动化模拟电路设计中的器件级调优和系统级协同优化，以提升性能并减少设计迭代工作。

Method: 集成了两种方法：1. 使用多目标贝叶斯优化（MOBO）进行AI辅助的晶体管尺寸调整，以直接优化电路参数（应用于线性可调跨导器）。2. 使用AI集成的电路传递函数建模，在关键词识别（KWS）应用中进行系统级优化（通过在机器学习训练循环中优化模拟带通滤波器）。

Result: 研究展示了AI能够成功优化晶体管参数（如在线性可调跨导器中）以及在系统层面优化模拟组件（如在KWS应用中优化带通滤波器），从而提升模拟电路性能。

Conclusion: 人工智能可以显著提高模拟电路的性能，减少设计迭代的工作量，并能够实现模拟组件与应用级指标的联合优化。

Abstract: Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.

</details>


### [222] [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](https://arxiv.org/abs/2505.03756)
*Hang Zhang,Jiuchen Shi,Yixiao Wang,Quan Chen,Yizhou Shan,Minyi Guo*

Main category: cs.AR

TL;DR: 提出了一个名为FASTLIBRA的多LoRA缓存系统，通过优化LoRA和KV缓存的依赖关系和交换策略，显著改善了多LoRA服务的推理性能，特别是首个令牌生成时间（TTFT）。


<details>
  <summary>Details</summary>
Motivation: 现有的多LoRA推理系统在缓存LoRA适配器和KV缓存时，未能充分优化如首个令牌生成时间（TTFT）之类的服务性能，并且忽略了它们之间的使用依赖关系。

Method: 提出了FASTLIBRA，一个包含依赖感知缓存管理器和性能驱动缓存交换器的多LoRA缓存系统。缓存管理器在统一缓存池中维护LoRA和KV缓存的使用依赖；缓存交换器则基于统一成本模型，根据高带宽内存（HBM）的空闲或繁忙状态，决定缓存的换入或换出。

Result: 实验结果表明，与当前最先进的工作相比，FASTLIBRA平均将首个令牌生成时间（TTFT）减少了63.4%。

Conclusion: FASTLIBRA通过其依赖感知缓存管理和性能驱动的交换策略，有效提升了多LoRA服务的性能，尤其在降低TTFT方面表现优异。

Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for
task-specific Large Language Model (LLM) applications. For multi-LoRA serving,
caching hot KV caches and LoRA adapters in high bandwidth memory of
accelerations can improve inference performance. However, existing Multi-LoRA
inference systems fail to optimize serving performance like Time-To-First-Toke
(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore
propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving
performance. FASTLIBRA comprises a dependency-aware cache manager and a
performance-driven cache swapper. The cache manager maintains the usage
dependencies between LoRAs and KV caches during the inference with a unified
caching pool. The cache swapper determines the swap-in or out of LoRAs and KV
caches based on a unified cost model, when the HBM is idle or busy,
respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on
average, compared to state-of-the-art works.

</details>


### [223] [Splitwiser: Efficient LM inference with constrained resources](https://arxiv.org/abs/2505.03763)
*Asad Aali,Adney Cardoza,Melissa Capo*

Main category: cs.AR

TL;DR: Splitwiser是一种通过将LLM推理的提示计算和令牌生成两个阶段拆分到同一GPU上执行，以减少开销、提高内存和缓存利用率的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理效率低下，尤其是在计算密集的提示计算阶段和内存密集的令牌生成阶段之间存在计算资源利用不充分的问题。

Method: 提出了Splitwiser方法，将LLM推理的提示计算和令牌生成两个阶段拆分并调度到同一个GPU上执行，从而减少数据传输开销，改善内存访问和缓存利用率。该方法在Huggingface和vLLM两种LLM架构上通过多进程设计实现。

Result: 论文描述了所提出流水线的基本结构，并分享了初步的实验结果和分析。相关的实现代码已在GitHub上开源。

Conclusion: Splitwiser通过在同一GPU上智能拆分LLM推理的两个主要阶段，有潜力减少推理开销并提高计算资源利用率，从而提升LLM的整体推理效率。

Abstract: Efficient inference of LLMs remains a crucial challenge, with two main
phases: a compute-intensive prompt computation and a memory-intensive token
generation. Despite existing batching and scheduling techniques, token
generation phases fail to fully utilize compute resources, especially when
compared to prompt computation phases. To address these challenges, we propose
Splitwiser, a methodology that splits the two phases of an LLM inference
request onto the same GPU, thereby reducing overhead and improving memory
access and cache utilization. By eliminating the need to transfer data across
devices, Splitwiser aims to minimize network-related overheads. In this report,
we describe the basic structure of our proposed pipeline while sharing
preliminary results and analysis. We implement our proposed multiprocessing
design on two widely-used and independent LLM architectures: Huggingface and
vLLM. We open-source our code for the respective implementations: 1)
Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM
(https://github.com/adney11/vllm-sysml).

</details>


### [224] [GPU Performance Portability needs Autotuning](https://arxiv.org/abs/2505.03780)
*Burkhard Ringlein,Thomas Parnell,Radu Stoica*

Main category: cs.AR

TL;DR: 该研究提出结合即时编译（JIT）和核心参数自动调优，以实现大型语言模型（LLM）在不同硬件上的可移植高性能执行，特别是在Flash Attention核心上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对单一硬件平台的依赖限制了其可移植性，造成供应商锁定，并为新型AI硬件的出现设置了障碍。

Method: 结合使用即时编译（JIT）与核心参数自动调优技术，并以广泛应用的性能关键LLM核心Flash Attention为例进行验证。

Result: 该方法探索的核心参数配置增加了15倍，生成了更多样化的代码，性能比供应商优化实现高出230%，同时核心代码大小减少了70倍，并消除了手动代码优化。

Conclusion: 自动调优是解锁LLM在不同GPU供应商之间模型可移植性的一条有前景的途径。

Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable, state-of-the-art performance LLM execution without code
changes. Focusing on flash attention -- a widespread performance-critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [225] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TL;DR: 提出了一种基于残差的神经网络校正策略，通过学习初始几何变换后的系统性畸变来提高坐标变换精度。


<details>
  <summary>Details</summary>
Motivation: 传统坐标变换模型难以有效处理非线性和空间相关的畸变，导致在地理空间应用中产生显著的残差。

Method: 采用一种残差神经网络校正策略：首先进行初步的几何变换，然后利用神经网络学习并校正该变换留下的系统性残差模式。

Result: 与直接神经网络坐标转换器和经典变换模型相比，所提出的基于残差的神经网络校正方法在具有挑战性的条件下（如稀疏或结构化控制点）能提供更准确、更稳定的结果，并在理想情况下保持相当的性能。

Conclusion: 残差建模是一种轻量级且鲁棒的替代方案，可有效提高坐标变换的精度。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [226] [Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework](https://arxiv.org/abs/2505.03746)
*Silvia García-Méndez,Francisco De Arriba-Pérez*

Main category: cs.SI

TL;DR: 提出了一种结合流式机器学习和大型语言模型的新型实时网络欺凌检测系统，性能优越。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的普及带来了网络欺凌问题，现有AI方法在处理动态变化的网络言论方面仍有提升空间。

Method: 采用流式机器学习模型增量处理数据，并利用大型语言模型（LLM）进行特征工程，以实时检测网络欺凌。同时提供可解释性仪表板。

Result: 实验数据显示，该方法在各项评估指标上均取得了近90%的优异性能，超越了文献中的现有方法。

Conclusion: 该研究提出的方案能及时发现网络欺凌行为，有助于维护在线社区安全，减少社会负面影响。

Abstract: Social media platforms enable instant and ubiquitous connectivity and are
essential to social interaction and communication in our technological society.
Apart from its advantages, these platforms have given rise to negative
behaviors in the online community, the so-called cyberbullying. Despite the
many works involving generative Artificial Intelligence (AI) in the literature
lately, there remain opportunities to study its performance apart from
zero/few-shot learning strategies. Accordingly, we propose an innovative and
real-time solution for cyberbullying detection that leverages stream-based
Machine Learning (ML) models able to process the incoming samples incrementally
and Large Language Models (LLMS) for feature engineering to address the
evolving nature of abusive and hate speech online. An explainability dashboard
is provided to promote the system's trustworthiness, reliability, and
accountability. Results on experimental data report promising performance close
to 90 % in all evaluation metrics and surpassing those obtained by competing
works in the literature. Ultimately, our proposal contributes to the safety of
online communities by timely detecting abusive behavior to prevent long-lasting
harassment and reduce the negative consequences in society.

</details>


### [227] [The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing](https://arxiv.org/abs/2505.03769)
*Yibo Hu,Yiqiao Jin,Meng Ye,Ajay Divakaran,Srijan Kumar*

Main category: cs.SI

TL;DR: 研究发现，重写源自YouTube视频的Reddit帖子标题能显著提升用户参与度，特别是当标题具有情感共鸣、词汇丰富并符合社区规范时。


<details>
  <summary>Details</summary>
Motivation: 在当今跨平台的社交媒体环境中，理解驱动多模态内容（尤其是文本与视觉结合的内容）用户参与度的因素仍然复杂，特别是从YouTube视频标题改编的Reddit帖子标题如何影响用户参与度。

Method: 1. 构建并分析了一个包含分享YouTube视频的Reddit帖子的大型数据集。2. 对数据集进行统计分析。3. 设计了一个对照的多阶段实验，以中和视频流行度、发布时间和社区规范等混淆因素，从而严格分离文本变异的影响。4. 使用BERT分类器进行成对排序预测实验。

Result: 研究发现21%的帖子标题仅做了少量修改。统计分析表明，标题重写能显著提高参与度。有效的标题重写倾向于具有情感共鸣、词汇丰富以及与特定社区规范的一致性。微调的BERT分类器在成对排序预测中达到了74%的准确率，显著优于包括GPT-4o在内的基线模型。

Conclusion: 该研究揭示了用户参与度的动态机制，并为未来的跨平台、多模态内容策略提供了一个强大的框架。通过控制混淆变量，文本特征对用户参与度的影响得以有效验证和学习。

Abstract: In today's cross-platform social media landscape, understanding factors that
drive engagement for multimodal content, especially text paired with visuals,
remains complex. This study investigates how rewriting Reddit post titles
adapted from YouTube video titles affects user engagement. First, we build and
analyze a large dataset of Reddit posts sharing YouTube videos, revealing that
21% of post titles are minimally modified. Statistical analysis demonstrates
that title rewrites measurably improve engagement. Second, we design a
controlled, multi-phase experiment to rigorously isolate the effects of textual
variations by neutralizing confounding factors like video popularity, timing,
and community norms. Comprehensive statistical tests reveal that effective
title rewrites tend to feature emotional resonance, lexical richness, and
alignment with community-specific norms. Lastly, pairwise ranking prediction
experiments using a fine-tuned BERT classifier achieves 74% accuracy,
significantly outperforming near-random baselines, including GPT-4o. These
results validate that our controlled dataset effectively minimizes confounding
effects, allowing advanced models to both learn and demonstrate the impact of
textual features on engagement. By bridging quantitative rigor with qualitative
insights, this study uncovers engagement dynamics and offers a robust framework
for future cross-platform, multimodal content strategies.

</details>


### [228] [Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics](https://arxiv.org/abs/2505.03795)
*Jacob W. Crandall,Jonathan Skaggs*

Main category: cs.SI

TL;DR: 该研究比较了多种在策略网络博弈（初中游戏）中学习人类行为模型的方法，发现一种关注社区感知行为并对行为分布建模的方法（hCAB）最能模拟人类群体动态和个体行为。


<details>
  <summary>Details</summary>
Motivation: 人类网络对重要的社会结果（如不平等、贫困）有巨大影响，理解人类网络对于促进有利的社会结果至关重要。本研究旨在通过学习人类行为模型来更好地理解这些网络。

Method: 研究使用了“初中游戏”（JHG）这一策略网络博弈，比较了不同的人类行为建模方法。这些方法在参数化行为的假设（行为匹配 vs. 社区感知行为）和建模的统计矩（均值 vs. 分布）上有所不同。表现最佳的模型（hCAB）在小型社会中进行了测试，并进行了用户研究以验证其行为的真实性。

Result: 结果显示，对群体行为分布进行建模并假设人类使用社区感知行为的方法（hCAB）表现最佳。该模型在小型社会中能较好地模拟人类群体的动态，并且人类参与者在用户研究中无法区分hCAB智能体与其他人类。

Conclusion: hCAB模型能够在该策略网络博弈中合理地模拟人类行为，包括群体动态和个体行为，这为更深入理解人类网络提供了有价值的工具。

Abstract: Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning models of human behavior in a strategic network
game called the Junior High Game (JHG). These modeling methods differ with
respect to the assumptions they use to parameterize human behavior (behavior
vs. community-aware behavior) and the statistical moments they model (mean vs.
distribution). Results show that the highest-performing method models the
population's distribution rather than the mean and assumes humans use
community-aware behavior rather than behavior matching. When applied to small
societies (6-11 individuals), this learned model, called hCAB, closely mirrors
the population dynamics of human groups (with some differences). Additionally,
a user study reveals that human participants were unable to distinguish hCAB
agents from other humans, thus illustrating that individual hCAB behavior
plausibly mirrors human behavior in this strategic network game.

</details>


### [229] [Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries](https://arxiv.org/abs/2505.03816)
*Bidyarthi Paul,Fariha Tasnim Chowdhury,Dipta Biswas,Meherin Sultana*

Main category: cs.SI

TL;DR: 通过分析纽约市出租车和达卡外卖数据，识别需求模式、高峰时间和地理热点，以优化城市交通服务。


<details>
  <summary>Details</summary>
Motivation: 城市交通在现代城市生活中扮演着至关重要的角色，影响着人与货物的流动效率。本研究旨在通过分析纽约市和达卡的交通数据，识别需求的关键趋势、高峰时段及重要的地理热点区域。

Method: 首先进行探索性数据分析（EDA）理解数据基本特征；接着进行地理空间分析以绘制高低需求区域图；然后使用SARIMAX模型进行时间序列分析以预测需求模式；最后应用聚类技术识别显著的高低需求区域。

Result: 研究结果为客运和食品配送服务中的车队管理和资源分配优化提供了有价值的见解。

Conclusion: 这些见解有助于提高服务效率，更好地满足客户需求，并增强不同城市环境下的城市交通系统。

Abstract: Urban transportation plays a vital role in modern city life, affecting how
efficiently people and goods move around. This study analyzes transportation
patterns using two datasets: the NYC Taxi Trip dataset from New York City and
the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify
key trends in demand, peak times, and important geographical hotspots. We start
with Exploratory Data Analysis (EDA) to understand the basic characteristics of
the datasets. Next, we perform geospatial analysis to map out high-demand and
low-demand regions. We use the SARIMAX model for time series analysis to
forecast demand patterns, capturing seasonal and weekly variations. Lastly, we
apply clustering techniques to identify significant areas of high and low
demand. Our findings provide valuable insights for optimizing fleet management
and resource allocation in both passenger transport and food delivery services.
These insights can help improve service efficiency, better meet customer needs,
and enhance urban transportation systems in diverse urban environments.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [230] [Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching](https://arxiv.org/abs/2505.04603)
*Wenhui Sophia Lu,Wing Hung Wong*

Main category: stat.ME

TL;DR: 本文提出了一种自适应贝叶斯推理（ABI）框架，旨在解决近似贝叶斯计算（ABC）在高维或弥散先验下的计算效率低下问题。ABI通过在后验空间直接比较分布，利用新颖的MSW距离和自适应采样方案，实现了更优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统近似贝叶斯计算（ABC）方法在似然函数解析不可用或计算棘手时，尤其在高维设置或弥散先验条件下，存在严重的计算效率问题。

Method: 提出自适应贝叶斯推理（ABI）框架。该框架不依赖传统数据空间差异，而是通过非参数分布匹配直接在后验空间比较分布。ABI利用新颖的边际增强切片Wasserstein（MSW）距离度量后验测度，并通过其分位数表示将后验分布散度测量转化为一系列一维条件分位数回归任务。此外，引入了新的自适应拒绝采样方案，通过生成式密度估计迭代更新提议分布以改进后验近似。

Result: 理论上，为修剪后的MSW距离建立了参数收敛速率，并证明当容忍阈值消失时，ABI后验会收敛到真实后验。大量实证评估表明，ABI在高维或依赖观测情况下，性能显著优于基于数据的Wasserstein ABC、基于摘要的ABC以及最先进的无似然模拟器。

Conclusion: ABI框架通过直接在后验空间进行分布比较并结合自适应采样，有效克服了传统ABC方法在高维和弥散先验下的计算效率瓶颈，为近似贝叶斯推断提供了一种更高效和准确的方法。

Abstract: When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [231] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
*Shigeki Karita,Yuma Koizumi,Heiga Zen,Haruko Ishikawa,Robin Scheibler,Michiel Bacchiani*

Main category: cs.SD

TL;DR: 该论文介绍了 Miipher-2，一种专为百万小时规模数据设计的语音恢复模型，用于清洗大型生成模型（如大语言模型）的训练数据，它具有多语言泛化能力、无需显式条件和高计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大规模训练数据清洗的需求，特别是针对大型生成模型。关键挑战包括对未知语言的泛化、无需文本或说话人ID等显式条件的操作，以及计算效率。

Method: Miipher-2 利用一个冻结的预训练通用语音模型（USM，支持超过300种语言）作为无条件特征提取器。它结合并行适配器从噪声输入中预测干净的USM特征，并使用WaneFit神经声码器合成波形。这些组件在3000小时的多语言录音室质量数据（带有增强退化）上训练，USM参数保持不变。

Result: 实验表明，Miipher-2 在词错误率、说话人相似度以及客观和主观音质评分方面，在所有测试语言中均优于或媲美传统语音恢复模型。该模型在消费级加速器上高效运行，实时因子为0.0078，使用100个加速器可在约三天内处理一百万小时的语音数据。

Conclusion: Miipher-2 是一种高效且可扩展的语音恢复解决方案，适用于清洗大规模多语言语音数据，解决了在语言泛化、无条件操作和计算效率方面的挑战，对训练大型生成模型具有重要意义。

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


### [232] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/abs/2505.04451)
*Yohannis Telila,Tommaso Cucinotta,Davide Bacciu*

Main category: cs.SD

TL;DR: 本文设计了一个处理流程，使用恒定Q变换提取特征，并将其输入卷积神经网络，以将古典钢琴WAV音频文件自动转录为乐谱。


<details>
  <summary>Details</summary>
Motivation: 自动音乐转录（AMT）是一个具有挑战性的问题，尤其是在处理包含同时演奏多个音符的复调音乐时，目标是能从音频中生成乐谱表示。

Method: 设计了一个处理流程：首先使用恒定Q变换（CQT）从.wav格式的古典钢琴音频文件中提取特征，然后将得到的CQT系数作为卷积神经网络（CNN）模型的输入，以生成乐谱表示。

Result: 该研究设计了一个能够将古典钢琴音频文件（.wav格式）转换为乐谱表示的处理流程。

Conclusion: 本研究提出了一种利用恒定Q变换和卷积神经网络相结合的方法，可以实现古典钢琴音频的自动音乐转录。

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>


### [233] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/abs/2505.04621)
*Jessie Richter-Powell,Antonio Torralba,Jonathan Lorraine*

Main category: cs.SD

TL;DR: 介绍了一种名为 Audio-SDS 的方法，它将分数蒸馏采样 (SDS) 推广到文本条件的音频扩散模型，利用单个预训练模型实现多种音频任务，无需专门数据集。


<details>
  <summary>Details</summary>
Motivation: 将最初为图像扩散设计的SDS方法的核心思想（即把强大的生成先验蒸馏到参数化表示中）扩展到音频领域，以利用单个预训练模型解决多种音频任务，避免对专门数据集的依赖。

Method: 提出 Audio-SDS，一种将分数蒸馏采样 (SDS) 推广到文本条件的音频扩散模型的方法。它利用单个预训练模型，通过蒸馏其生成先验来指导不同的参数化音频任务。

Result: 成功展示了 Audio-SDS 在指导物理声学模拟、校准调频合成（FM-synthesis）参数以及执行基于文本提示的声源分离等任务中的有效性。

Conclusion: 研究结果表明，基于蒸馏的方法具有跨模态的通用性，并且 Audio-SDS 为未来在音频任务中应用生成先验提供了坚实的基础。

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [234] [Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures](https://arxiv.org/abs/2505.03764)
*Logan Larsh,Raiyan Siddique,Sarah Sharif Yaser Mike Banad*

Main category: cs.NE

TL;DR: 该研究在7nm FinFET技术下对三种脉冲神经元电路（LIF, ML, AH）进行了比较，发现AH设计具有最高吞吐量，ML设计在低功耗方面表现优异，并揭示了7nm技术在提升能效和速度的同时，也增加了亚阈值泄漏。


<details>
  <summary>Details</summary>
Motivation: 神经拟态计算旨在复制大脑的能源效率和并行处理能力，以支持大规模人工智能应用。因此，需要优化脉冲神经元电路以实现超低功耗和高计算吞吐量。

Method: 在7nm FinFET工艺下，对三种脉冲神经元电路架构——Leaky-Integrate-and-Fire (LIF), Morris-Lecar (ML), 和 Axon-Hillock (AH)——进行了全面的比较研究。通过广泛的SPICE仿真，探索了脉冲频率、单位脉冲能量和静态功耗的优化。

Result: AH设计实现了最高的吞吐量（高达3GHz的放电速率）和阿焦耳级的能量成本。相比之下，ML架构在亚阈值到近阈值区域表现优异，提供了稳健的低功耗操作（低至0.385 aJ/脉冲）和生物学上的脉冲簇行为。LIF在高频操作中表现良好，但在较高电源电压下静态泄漏略高。与旧工艺节点相比，7nm FinFET显著提高了能效和速度，但在深亚阈值区域增加了泄漏。

Conclusion: 该工作通过量化每种神经元架构的设计权衡，为在先进纳米技术中优化脉冲神经元电路提供了一个路线图，旨在开发出既能实现超低功耗操作又能达到高计算吞吐量的神经拟态硬件。

Abstract: Neuromorphic computing aims to replicate the brain's remarkable energy
efficiency and parallel processing capabilities for large-scale artificial
intelligence applications. In this work, we present a comprehensive comparative
study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire
(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET
technology. Through extensive SPICE simulations, we explore the optimization of
spiking frequency, energy per spike, and static power consumption. Our results
show that the AH design achieves the highest throughput, demonstrating
multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By
contrast, the ML architecture excels in subthreshold to near-threshold regimes,
offering robust low-power operation (as low as 0.385 aJ/spike) and biological
bursting behavior. Although LIF benefits from a decoupled current mirror for
high-frequency operation, it exhibits slightly higher static leakage compared
to ML and AH at elevated supply voltages. Comparisons with previous node
implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically
boost energy efficiency and speed albeit at the cost of increased subthreshold
leakage in deep subthreshold regions. By quantifying design trade-offs for each
neuron architecture, our work provides a roadmap for optimizing spiking neuron
circuits in advanced nanoscale technologies to deliver neuromorphic hardware
capable of both ultra-low-power operation and high computational throughput.

</details>


### [235] [Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks](https://arxiv.org/abs/2505.04034)
*Ayana Moshruba,Hamed Poursiami,Maryam Parsa*

Main category: cs.NE

TL;DR: 本文提出了两种新的输入层面神经元脉冲转换方法（Poisson-Burst 和 Delayed-Burst），将生物神经元的脉冲时间变异性引入到标准的LIF神经元中，从而在可扩展的脉冲神经网络（SNN）训练中，提升模型的隐私性、泛化能力和学习性能。


<details>
  <summary>Details</summary>
Motivation: 生物神经元展现出多样化的时间脉冲模式，这被认为支持高效、鲁棒和自适应的神经信息处理。然而，像Izhikevich这样能复制多种放电动态的模型，其复杂性给直接将其集成到可扩展的SNN训练流程带来了挑战。

Method: 提出了两种概率驱动的、输入层面的时间脉冲转换方法：Poisson-Burst（基于输入强度调制脉冲簇的发生）和Delayed-Burst（通过脉冲簇的起始时间编码输入强度）。这些方法直接应用于标准的Leaky Integrate-and-Fire (LIF) 神经元。

Result: 实验表明，Poisson-Burst在保持有竞争力的准确率和较低资源开销的同时，表现出对成员推断攻击的增强隐私鲁棒性；而Delayed-Burst以适度的准确率牺牲为代价，提供了更强的隐私保护。

Conclusion: 研究结果强调了基于生物学的脉冲时间动态在改进神经形态学习系统的隐私性、泛化能力和生物学合理性方面的潜力。

Abstract: Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.

</details>


### [236] [TS-SNN: Temporal Shift Module for Spiking Neural Networks](https://arxiv.org/abs/2505.04165)
*Kairong Yu,Tianqing Zhang,Qi Xu,Gang Pan,Hongwei Wang*

Main category: cs.NE

TL;DR: 该研究提出TS-SNN，一种通过时间位移模块在脉冲神经网络中整合多时间步脉冲特征的方法，以低能耗实现高精度。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在生物合理性和能效方面具有优势，但在有效利用时间特征同时保持低能耗方面存在挑战。

Method: 引入一种新颖的时间位移（TS）模块，通过位移操作在单个时间步内整合过去、现在和未来的脉冲特征。同时使用残差组合方法防止信息丢失。该模块轻量级，易于集成。

Result: TS-SNN在CIFAR-10（96.72%）、CIFAR-100（80.28%）和ImageNet（70.61%）等基准测试中取得了当前最佳性能，且使用了更少的时间步并保持了低能耗。

Conclusion: 这项工作显著推动了高效、准确的SNN架构的发展。

Abstract: Spiking Neural Networks (SNNs) are increasingly recognized for their
biological plausibility and energy efficiency, positioning them as strong
alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing
applications. SNNs inherently process temporal information by leveraging the
precise timing of spikes, but balancing temporal feature utilization with low
energy consumption remains a challenge. In this work, we introduce Temporal
Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel
Temporal Shift (TS) module to integrate past, present, and future spike
features within a single timestep via a simple yet effective shift operation. A
residual combination method prevents information loss by integrating shifted
and original features. The TS module is lightweight, requiring only one
additional learnable parameter, and can be seamlessly integrated into existing
architectures with minimal additional computational cost. TS-SNN achieves
state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100
(80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low
energy consumption. This work marks a significant step forward in developing
efficient and accurate SNN architectures.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [237] [A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions](https://arxiv.org/abs/2505.03899)
*Danial Davarnia,Mohammadreza Kiaghadi*

Main category: math.OC

TL;DR: 论文提出了一种基于决策图的新型全局优化方法，用于解决含广义范数约束（如 $\ell_0$, SCAD, MCP）的问题，无需辅助变量，保证收敛。


<details>
  <summary>Details</summary>
Motivation: 现有解决范数约束优化问题（尤其含零范数或非凸惩罚）的方法存在局限性，如增加问题维度、缺乏通用性或仅能局部优化，因此需要一种更有效的全局优化方法。

Method: 该研究提出了一种基于图的方法：1. 利用决策图在原始变量空间中为广义范数约束（包括 $\ell_p$ 范数 $p \in [0, \infty)$ 及SCAD、MCP等非凸惩罚）构建强凸松弛，避免引入辅助变量或人工边界。2. 将此方法集成到空间分支剪切框架中，保证全局收敛。

Result: 初步计算实验表明，该方法在基准稀疏线性回归问题上有效，尤其能处理现有全局优化技术难以解决的、涉及复杂非凸惩罚项的问题，并保证收敛到全局最优解。

Conclusion: 本文提出的基于决策图的全局优化方法为解决带广义范数约束（包括非凸惩罚）的优化问题提供了一条有效且通用的新途径，特别是在处理复杂非凸问题时具有优势。

Abstract: Optimization problems with norm-bounding constraints arise in a variety of
applications, including portfolio optimization, machine learning, and feature
selection. A common approach to these problems involves relaxing the norm
constraint via Lagrangian relaxation, transforming it into a regularization
term in the objective function. A particularly challenging class includes the
zero-norm function, which promotes sparsity in statistical parameter
estimation. Most existing exact methods for solving these problems introduce
binary variables and artificial bounds to reformulate them as
higher-dimensional mixed-integer programs, solvable by standard solvers. Other
exact approaches exploit specific structural properties of the objective,
making them difficult to generalize across different problem types. Alternative
methods employ nonconvex penalties with favorable statistical characteristics,
but these are typically addressed using heuristic or local optimization
techniques due to their structural complexity. In this paper, we propose a
novel graph-based method to globally solve optimization problems involving
generalized norm-bounding constraints. Our approach encompasses standard
$\ell_p$-norms for $p \in [0, \infty)$ and nonconvex penalties such as SCAD and
MCP. We leverage decision diagrams to construct strong convex relaxations
directly in the original variable space, eliminating the need for auxiliary
variables or artificial bounds. Integrated into a spatial branch-and-cut
framework, our method guarantees convergence to the global optimum. We
demonstrate its effectiveness through preliminary computational experiments on
benchmark sparse linear regression problems involving complex nonconvex
penalties, which are not tractable using existing global optimization
techniques.

</details>


### [238] [Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems](https://arxiv.org/abs/2505.04596)
*Mohammad Merati,David Castañón*

Main category: math.OC

TL;DR: 本文提出了一种优化PTZ摄像机调度与控制的新方法，通过结合卡尔曼滤波器进行运动预测和动态网络流模型进行任务规划，以提高动态监控环境下的视频捕捉效率。


<details>
  <summary>Details</summary>
Motivation: 旨在提高动态监控环境中实时视频捕捉的效率，解决传统系统在可扩展性、适应性和响应及时性方面的不足。

Method: 该方法集成卡尔曼滤波器预测目标未来位置，并将其与动态网络流模型结合进行摄像机任务的优化调度。同时，引入组跟踪节点以减少冗余监控，并采用基于价值的系统对摄像机动作进行优先级排序，通过调整价值衰减率确保对紧急任务的及时响应。

Result: 大量仿真实验表明，与传统主从式摄像机系统相比，该方法在提高覆盖率、减少平均等待时间以及最小化事件遗漏方面表现更优。

Conclusion: 该方法显著增强了监控系统的效率、可扩展性和有效性，尤其适用于动态和拥挤的环境。

Abstract: This paper presents a novel approach for optimizing the scheduling and
control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.
The proposed method integrates Kalman filters for motion prediction with a
dynamic network flow model to enhance real-time video capture efficiency. By
assigning Kalman filters to tracked objects, the system predicts future
locations, enabling precise scheduling of camera tasks. This prediction-driven
approach is formulated as a network flow optimization, ensuring scalability and
adaptability to various surveillance scenarios. To further reduce redundant
monitoring, we also incorporate group-tracking nodes, allowing multiple objects
to be captured within a single camera focus when appropriate. In addition, a
value-based system is introduced to prioritize camera actions, focusing on the
timely capture of critical events. By adjusting the decay rates of these values
over time, the system ensures prompt responses to tasks with imminent
deadlines. Extensive simulations demonstrate that this approach improves
coverage, reduces average wait times, and minimizes missed events compared to
traditional master-slave camera systems. Overall, our method significantly
enhances the efficiency, scalability, and effectiveness of surveillance
systems, particularly in dynamic and crowded environments.

</details>


### [239] [Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows](https://arxiv.org/abs/2505.04354)
*Wenhao Li,Bo Jin,Mingyi Hong,Changhong Lu,Xiangfeng Wang*

Main category: math.OC

TL;DR: 本文主张优化问题解决可以从依赖专家转变为进化智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 传统优化实践依赖人类专家进行问题建模、算法选择和超参数调整，这造成了瓶颈，阻碍了前沿方法在工业界的推广应用。

Method: 提出一种进化智能体工作流，该工作流由基础模型和进化搜索驱动，能够自主地在优化空间（包括问题、公式、算法和超参数空间）中进行导航。

Result: 通过在云资源调度和ADMM参数自适应方面的案例研究，展示了该方法如何能够弥合学术创新与工业实施之间的差距。

Conclusion: 本文挑战了以人为中心的优化工作流的现状，并倡导采用一种更具可扩展性、适应性强的方法来解决现实世界的优化问题。

Abstract: This position paper argues that optimization problem solving can transition
from expert-dependent to evolutionary agentic workflows. Traditional
optimization practices rely on human specialists for problem formulation,
algorithm selection, and hyperparameter tuning, creating bottlenecks that
impede industrial adoption of cutting-edge methods. We contend that an
evolutionary agentic workflow, powered by foundation models and evolutionary
search, can autonomously navigate the optimization space, comprising problem,
formulation, algorithm, and hyperparameter spaces. Through case studies in
cloud resource scheduling and ADMM parameter adaptation, we demonstrate how
this approach can bridge the gap between academic innovation and industrial
implementation. Our position challenges the status quo of human-centric
optimization workflows and advocates for a more scalable, adaptive approach to
solving real-world optimization problems.

</details>


### [240] [Learning based convex approximation for constrained parametric optimization](https://arxiv.org/abs/2505.04037)
*Kang Liu,Wei Peng,Jianchen Hu*

Main category: math.OC

TL;DR: 提出了一种基于输入凸神经网络 (ICNN) 的自监督学习框架，通过结合增广拉格朗日方法和约束校正机制，以解决连续约束优化问题，并展现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种新的学习框架，用于解决连续约束优化问题，该框架能够确保非严格约束可行性、改善最优性差距，并实现比现有先进学习方法更快的收敛速度。

Method: 采用基于输入凸神经网络 (ICNN) 的自监督学习框架，该框架集成了增广拉格朗日方法 (ALM) 与约束校正机制。

Result: 该框架保证了非严格约束可行性，并相较于当前先进的学习方法，取得了更优的 optimality gap 和最佳收敛速率。理论分析证明算法收敛至KKT点，且近似误差有界。在二次规划、非凸规划和大规模交流最优潮流等基准测试中，该方法相较于OSQP、IPOPT、DC3、PDL等现有求解器和学习方法，在准确性、可行性和计算效率上实现了更优的平衡。

Conclusion: 所提出的基于ICNN的自监督学习框架是一种解决连续约束优化问题的有效方法，在准确性、可行性和计算效率的综合表现上优于现有主流求解器和最新的学习方法。

Abstract: We propose an input convex neural network (ICNN)-based self-supervised
learning framework to solve continuous constrained optimization problems. By
integrating the augmented Lagrangian method (ALM) with the constraint
correction mechanism, our framework ensures \emph{non-strict constraint
feasibility}, \emph{better optimality gap}, and \emph{best convergence rate}
with respect to the state-of-the-art learning-based methods. We provide a
rigorous convergence analysis, showing that the algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal
solver is a neural network, and the approximation error is bounded. We test our
approach on a range of benchmark tasks including quadratic programming (QP),
nonconvex programming, and large-scale AC optimal power flow problems. The
results demonstrate that compared to existing solvers (e.g., \texttt{OSQP},
\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our
approach achieves a superior balance among accuracy, feasibility, and
computational efficiency.

</details>


### [241] [A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance](https://arxiv.org/abs/2505.04494)
*Axel Friedrich Wolter,Tobias Sutter*

Main category: math.OC

TL;DR: 提出了一种名为PGDA-RL的新型原始-对偶强化学习算法，它结合了正则化线性规划和随机逼近理论，旨在利用离策略数据进行在策略探索，并能收敛到正则化马尔可夫决策过程的最优解。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够有效利用离策略数据（off-policy data），同时保持在策略探索（on-policy exploration）能力的强化学习算法。

Method: 提出了一种名为PGDA-RL的新型原始-对偶投影梯度下降-上升算法。该算法集成了基于经验回放的梯度估计和双时间尺度分解，用于解决正则化马尔可夫决策过程（MDPs）。算法异步运行，通过单一轨迹的关联数据与环境交互，并根据与MDP占用测度相关的对偶变量在线更新其策略。

Result: 证明了PGDA-RL算法几乎必然收敛到正则化MDP的最优值函数和策略。其收敛性分析在比现有原始-对偶RL方法更弱的假设下成立。

Conclusion: PGDA-RL是一种有理论保证收敛性的强化学习算法，它有效地结合了离策略数据利用和在策略探索，并且与现有方法相比，它对假设条件的要求更宽松，例如不需要模拟器或固定的行为策略。

Abstract: We study reinforcement learning by combining recent advances in regularized
linear programming formulations with the classical theory of stochastic
approximation. Motivated by the challenge of designing algorithms that leverage
off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a
novel primal-dual Projected Gradient Descent-Ascent algorithm for solving
regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience
replay-based gradient estimation with a two-timescale decomposition of the
underlying nested optimization problem. The algorithm operates asynchronously,
interacts with the environment through a single trajectory of correlated data,
and updates its policy online in response to the dual variable associated with
the occupation measure of the underlying MDP. We prove that PGDA-RL converges
almost surely to the optimal value function and policy of the regularized MDP.
Our convergence analysis relies on tools from stochastic approximation theory
and holds under weaker assumptions than those required by existing primal-dual
RL approaches, notably removing the need for a simulator or a fixed behavioral
policy.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [242] [Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach](https://arxiv.org/abs/2505.03760)
*Arishi Orra,Aryan Bhambu,Himanshu Choudhary,Manoj Thakur,Selvaraju Natarajan*

Main category: q-fin.PM

TL;DR: 研究提出了一种波动率引导的深度强化学习（DRL）框架，用于根据投资者风险偏好动态构建投资组合。


<details>
  <summary>Details</summary>
Motivation: 现有DRL投资组合优化策略虽然具有适应性，但其成功依赖于资产的预选。本研究旨在通过结合波动率预测和投资者风险偏好来改进资产选择，从而优化投资策略。

Method: 1. 使用广义自回归条件异方差（GARCH）模型预测股票波动率，并根据波动率将股票分为激进型、稳健型和保守型，以匹配投资者风险偏好。 2. 采用DRL智能体通过与历史市场数据交互学习最优投资策略。

Result: 使用道琼斯30指数成分股进行的实验表明，所提出的针对特定投资者的DRL投资组合在产生持续的风险调整后回报方面优于基线策略。

Conclusion: 该研究提出的波动率引导的DRL投资组合优化框架能够根据投资者风险偏好动态构建投资组合，并取得优于基线策略的风险调整后回报，证明了其有效性。

Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the
risk and return tradeoff under dynamic market conditions. With the recent
advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in
providing adaptive and scalable strategies for portfolio optimization. However,
the success of these strategies depends not only on their ability to adapt to
market dynamics but also on the careful pre-selection of assets that influence
overall portfolio performance. Incorporating the investor's preference in
pre-selecting assets for a portfolio is essential in refining their investment
strategies. This study proposes a volatility-guided DRL-based portfolio
optimization framework that dynamically constructs portfolios based on
investors' risk profiles. The Generalized Autoregressive Conditional
Heteroscedasticity (GARCH) model is utilized for volatility forecasting of
stocks and categorizes them based on their volatility as aggressive, moderate,
and conservative. The DRL agent is then employed to learn an optimal investment
policy by interacting with the historical market data. The efficacy of the
proposed methodology is established using stocks from the Dow $30$ index. The
proposed investor-specific DRL-based portfolios outperformed the baseline
strategies by generating consistent risk-adjusted returns.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [243] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
*Kai Ruan,Mowen Huang,Ji-Rong Wen,Hao Sun*

Main category: cs.MA

TL;DR: 该研究引入SwarmBench，一个评估大型语言模型（LLMs）在模拟蜂群（局部感知与通信受限）的多智能体系统中进行去中心化协作能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理方面展现潜力，但其在严格限制（如局部感知和通信，类似自然蜂群）的多智能体系统（MAS）中的涌现协调能力，尤其是在蜂群智能方面，仍有待探索。现有基准未能充分捕捉在不完整时空信息下运作的去中心化协调所面临的独特挑战。

Method: 研究者引入了SwarmBench，这是一个包含五个基础MAS协调任务的可配置二维网格环境基准。在该基准中，智能体主要依赖局部感知输入（k x k视野）和局部通信。研究者提出了协调有效性指标，并分析了涌现的群体动态，在零样本（zero-shot）设置下评估了多个主流LLM。

Result: 评估结果显示，不同LLM在各项任务中的表现差异显著，凸显了局部信息限制带来的困难。虽然观察到了一些协调行为，但在这些去中心化场景中，LLM在不确定性下的鲁棒规划和策略形成方面存在局限性。

Conclusion: 在类似蜂群的条件下评估LLM对于发挥其在未来去中心化系统中的潜力至关重要。SwarmBench作为一个开放、可扩展的工具包被发布，旨在促进基于LLM的MAS协调和具身多智能体系统理论基础的可复现研究。

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [244] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/abs/2505.03864)
*Qiaomu Li,Ying Xie*

Main category: cs.MA

TL;DR: 论文分析了谷歌A2A和Anthropic MCP协议集成用于AI多智能体系统时，在语义互操作性、安全和治理方面出现的新兴挑战及其复杂性。


<details>
  <summary>Details</summary>
Motivation: AI正向多智能体系统发展，A2A（智能体间通信）和MCP（标准化工具访问）两大开放标准有望克服定制化集成的局限性。然而，有效集成这两者在其交叉点上带来了独特的、新兴的挑战，特别是在语义互操作性、复合安全风险和实际治理方面，这些问题尚未得到充分探讨。

Method: 本文进行批判性分析，超越了简单的综述，评估了结合A2A（横向集成）和MCP（纵向集成）标准的实际影响和固有困难。研究审视了集成的益处（如专业化、可扩展性），同时严格评估了其在集成环境下的依赖性和权衡，并识别了因集成而加剧的关键挑战。

Result: 研究发现，A2A和MCP的集成虽然提供了重要的架构基础和潜在优势（如专业化和可扩展性），但也显著增加了系统的复杂性。具体而言，集成功加剧了语义互操作性的难度，带来了新的安全漏洞、隐私复杂性、跨协议调试困难，并凸显了对强大语义协商机制的需求。

Conclusion: A2A与MCP的结合为多智能体系统提供了重要的架构基础，但要充分发挥其潜力，必须在管理其组合操作所带来的复杂性（尤其是在解决语义、安全和治理方面的挑战）方面取得重大进展。

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>


### [245] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/abs/2505.04379)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.MA

TL;DR: 本研究量化了自动驾驶汽车（AVs）在混合交通中，在安全性、交互质量和交通性能三方面达成共识的程度，发现完全共识非常罕见。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在复杂交通系统中的部署面临着在安全性、交互质量和交通性能之间达成共识的新挑战。本研究旨在将共识视为交通系统的基本属性并对其进行量化。

Method: 使用第三代模拟（TGSIM）数据集的高分辨率轨迹数据，对信号交叉口和弱势道路使用者（VRUs）周围的AV和人类驾驶车辆（HDV）行为进行实证分析。评估了包括碰撞时间（TTC）、侵占后时间（PET）、减速模式、车头时距和车队稳定性等关键指标，覆盖三个性能维度。

Result: 研究发现，在安全性、交互质量和交通性能三个维度上完全达成共识的情况非常罕见，仅有1.63%的AV-VRU交互帧同时满足所有三个条件。

Conclusion: 研究结果强调，需要开发能够明确平衡混合交通环境中多维度性能的AV模型。

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>


### [246] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/abs/2505.04579)
*Stéphane Aroca-Ouellette,Miguel Aroca-Ouellette,Katharina von der Wense,Alessandro Roncone*

Main category: cs.MA

TL;DR: 提出了一种名为HA$^2$的分层强化学习框架，通过学习共享任务抽象，使智能体在协作任务中能更好地适应新队友。


<details>
  <summary>Details</summary>
Motivation: 自主智能体在协作任务中适应新队友的能力远不如人类，这主要是因为缺乏人类依赖的共享任务抽象机制以进行隐式对齐，从而限制了零样本协调能力。

Method: 引入HA$^2$（Hierarchical Ad Hoc Agents）框架，该框架利用分层强化学习来模仿人类在协作中使用的结构化方法，旨在学习共享任务抽象。

Result: 在Overcooked环境中的评估表明，HA$^2$与未见过的智能体和人类配对时，相比现有基线有统计学上的显著改进，对环境变化具有更好的适应性，并且优于所有最先进的方法。

Conclusion: HA$^2$框架通过学习共享任务抽象，有效提升了智能体在协作任务中与陌生队友的零样本协调能力和适应性。

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [247] [In-Context Adaptation to Concept Drift for Learned Database Operations](https://arxiv.org/abs/2505.04404)
*Jiaqi Zhu,Shaofeng Cai,Yanyan Shen,Gang Chen,Fang Deng,Beng Chin Ooi*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine learning has demonstrated transformative potential for database
operations, such as query optimization and in-database data analytics. However,
dynamic database environments, characterized by frequent updates and evolving
data distributions, introduce concept drift, which leads to performance
degradation for learned models and limits their practical applicability.
Addressing this challenge requires efficient frameworks capable of adapting to
shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that
introduces a new paradigm called \textit{in-context adaptation} for learned
database operations. FLAIR leverages the inherent property of data systems,
i.e., immediate availability of execution results for predictions, to enable
dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,|
\,\mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic
context memory, FLAIR delivers predictions aligned with the current concept,
eliminating the need for runtime parameter optimization. To achieve this, FLAIR
integrates two key modules: a Task Featurization Module for encoding
task-specific features into standardized representations, and a Dynamic
Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly
using contextual information at runtime. Extensive experiments across key
database tasks demonstrate that FLAIR outperforms state-of-the-art baselines,
achieving up to 5.2x faster adaptation and reducing error by 22.5% for
cardinality estimation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [248] [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
*Nouar Aldahoul,Hazem Ibrahim,Matteo Varvello,Aaron Kaufman,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 研究挑战了大型语言模型政治偏见较小的普遍看法，发现其偏见是特定议题上极端观点的抵消结果，并且LLM能说服用户采纳其政治偏好。


<details>
  <summary>Details</summary>
Motivation: 挑战学术界普遍认为大型语言模型（LLMs）政治偏见较小的观点，并探究其潜在的政治影响力。

Method: 1. 比较31个LLMs与立法者、法官和美国选民代表样本的政治偏好。 2. 通过随机实验，让选民与LLM聊天机器人讨论政治议题，观察LLM对选民偏好的影响。

Result: 1. LLMs表面上较小的整体党派偏好是特定议题上极端观点相互抵消的结果，类似于温和派选民。 2. LLM聊天机器人能显著影响选民的政治偏好，使其观点与聊天机器人趋同（影响幅度可达5个百分点），且这种影响不受用户对LLM的熟悉度、新闻消费或政治兴趣的调节。

Conclusion: 大型语言模型，特别是那些由私营公司或政府控制的模型，可能成为一种强大且具有针对性的政治影响工具。

Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally
changing how people obtain information and interact with the world. As people
become increasingly reliant on them for an enormous variety of tasks, a body of
academic research has developed to examine these models for inherent biases,
especially political biases, often finding them small. We challenge this
prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a
nationally representative sample of U.S. voters, we show that LLMs' apparently
small overall partisan preference is the net result of offsetting extreme views
on specific topics, much like moderate voters. Second, in a randomized
experiment, we show that LLMs can promulgate their preferences into political
persuasiveness even in information-seeking contexts: voters randomized to
discuss political issues with an LLM chatbot are as much as 5 percentage points
more likely to express the same preferences as that chatbot. Contrary to
expectations, these persuasive effects are not moderated by familiarity with
LLMs, news consumption, or interest in politics. LLMs, especially those
controlled by private companies or governments, may become a powerful and
targeted vector for political influence.

</details>


### [249] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TL;DR: 研究揭示了可生成深度伪造（deepfakes）的文本到图像（T2I）模型变体在网络上（尤其是在Civitai平台）数量激增且易于获取，这些模型主要针对女性，并常用于生成非自愿私密图像（NCII），对监管和平台治理构成严峻挑战。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像（T2I）模型的普及，其被滥用于制作针对个人的深度伪造（deepfakes）的风险日益增加。本研究旨在探究这些深度伪造模型变体在网络上的可获取程度。

Method: 通过对两个流行模型库（Hugging Face和Civitai）上数千个公开可下载的模型变体进行元数据分析。

Result: 研究识别出近35,000个公开可下载的深度伪造模型变体，主要托管在Civitai上，自2022年11月以来下载量近1500万次。这些模型针对从全球名人到粉丝数不足1万的普通用户，其中96%针对女性，且许多模型意图生成非自愿私密图像（NCII）。研究发现，这些模型主要基于Stable Diffusion和Flux，常通过低秩适应（LoRA）技术创建，制作门槛低（仅需少量图像、24GB显存和15分钟）。

Conclusion: 尽管这些模型违反了托管平台的服务条款，并且有法规试图阻止其传播，但研究结果强调了采取更强有力行动以应对深度伪造和非自愿私密图像（NCII）创作的迫切需求。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>


### [250] [Coverage Biases in High-Resolution Satellite Imagery](https://arxiv.org/abs/2505.03842)
*Vadim Musienko,Axel Jacquet,Ingmar Weber,Till Koebe*

Main category: cs.CY

TL;DR: 研究表明，由于轨道物理特性、社会经济因素和地缘政治事件，全球高分辨率光学卫星影像的覆盖和历史可用性存在显著的不平等。


<details>
  <summary>Details</summary>
Motivation: 探究地球上不同地区是否能平等地从日益增长的卫星影像信息中受益，并揭示潜在的覆盖偏差及其原因。

Method: 1. 基于卫星轨道路径，估算未来30天内不同地点的重访频率，分析物理因素造成的潜在覆盖偏差。2. 收集主要卫星影像供应商的元数据，分析历史影像可用性与地面社会经济因素的关系。3. 对加沙、苏丹和乌克兰等近期冲突地区进行案例研究，揭示地缘政治事件对影像可用性的影响。

Result: 1. 物理因素：离赤道越远的地区，研究中的卫星星座重访频率通常更高。2. 社会经济因素：历史卫星影像的可用性偏向于发达和人口稠密的地区，欠发达、人口稀少地区的影像较少。3. 地缘政治因素：在加沙、苏丹和乌克兰的案例中，地缘政治事件显著影响卫星影像的可用性，暗示了商业模式决策的潜在影响。

Conclusion: 卫星影像带来的数字红利在全球范围内并非平均分配，存在由物理、社会经济和地缘政治因素共同导致的覆盖和可用性不平等现象。

Abstract: Satellite imagery is increasingly used to complement traditional data
collection approaches such as surveys and censuses across scientific
disciplines. However, we ask: Do all places on earth benefit equally from this
new wealth of information? In this study, we investigate coverage bias of major
satellite constellations that provide optical satellite imagery with a ground
sampling distance below 10 meters, evaluating both the future on-demand tasking
opportunities as well as the availability of historic images across the globe.
Specifically, forward-looking, we estimate how often different places are
revisited during a window of 30 days based on the satellites' orbital paths,
thus investigating potential coverage biases caused by physical factors. We
find that locations farther away from the equator are generally revisited more
frequently by the constellations under study. Backward-looking, we show that
historic satellite image availability -- based on metadata collected from major
satellite imagery providers -- is influenced by socio-economic factors on the
ground: less developed, less populated places have less satellite images
available. Furthermore, in three small case studies on recent conflict regions
in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical
events play an important role in satellite image availability, hinting at
underlying business model decisions. These insights lay bare that the digital
dividend yielded by satellite imagery is not equally distributed across our
planet.

</details>


### [251] [AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions](https://arxiv.org/abs/2505.04592)
*Peter Barnett,Aaron Scher*

Main category: cs.CY

TL;DR: 该论文分析了高级人工智能发展的潜在灾难性风险，并提出通过国际合作建立“关闭开关”并最终暂停前沿人工智能活动是降低风险的最佳方案。


<details>
  <summary>Details</summary>
Motivation: 高级人工智能系统可能很快在所有认知领域超越人类专家，其默认发展轨迹有极高可能导致包括人类灭绝在内的灾难。

Method: 描述人工智能发展的战略格局，并列出重要的治理研究问题。通过分析四种高级地缘政治应对方案（国际限制与暂停、美国国家项目、轻度干预、破坏与威慑），并评估其风险。

Result: 除了建立“关闭开关”并最终国际协调暂停前沿人工智能活动的方案外，其他所有方案（如美国单方面主导、当前轻度干预、或通过破坏手段减缓发展）都带有不可接受的灾难性风险。

Conclusion: 美国国家安全界和人工智能治理生态系统亟需采取紧急行动，回答关键研究问题，建立阻止危险人工智能活动的能力，并为国际人工智能协议做好准备，以避免灾难性后果。

Abstract: Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [252] [Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions](https://arxiv.org/abs/2505.04553)
*Shanyu Han,Yang Liu,Xiang Yu*

Main category: q-fin.MF

TL;DR: 提出了一种针对广泛凸风险目标的强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在处理具有凸评分函数的风险目标（如方差、预期短缺等）时可能存在时间不一致性问题。

Method: 引入增广状态空间和辅助变量，将问题转化为双状态优化问题；提出定制的 Actor-Critic 算法和受交替最小化启发的辅助变量采样方法。

Result: 建立了理论近似保证，且结果不要求马尔可夫决策过程的连续性；辅助变量采样方法在特定条件下收敛；通过金融统计套利交易的仿真实验验证了算法的有效性。

Conclusion: 该研究提出的强化学习框架和算法能够有效处理广泛的凸风险目标，并在金融应用中得到验证。

Abstract: We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [253] [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
*Jean-Michel Tucny,Mihir Durve,Sauro Succi*

Main category: physics.comp-ph

TL;DR: 研究发现PINN在解决玻尔兹曼方程问题时，其权重矩阵与物理结构无明显关联，更像随机高斯矩阵，这可能意味着深度学习与传统数值解是获取物理知识的不同路径，并对可解释AI提出挑战。


<details>
  <summary>Details</summary>
Motivation: 探究基于物理信息神经网络（PINN）解决稀薄气体动力学问题（玻尔兹曼方程）时，其权重矩阵是否能反映物理问题的数学结构。

Method: 分析将基于PINN的深度学习应用于玻尔兹曼方程描述的稀薄气体动力学问题时，其权重矩阵的特性。

Result: PINN的权重矩阵与物理问题的数学结构之间没有明显联系，反而看起来接近高斯分布的随机矩阵。

Conclusion: 深度学习和玻尔兹曼方程的数值解可能是通往相同物理知识的两条等效但基本不同的路径。因此，可解释人工智能（Explainable AI）可能是一个不切实际的目标，甚至可能是一个不适定的目标。

Abstract: It is shown that the weight matrices of a Physics-informed neural network
(PINN)-based deep learning application to a rarefied gas dynamics problem
described by the Boltzmann equation bear no evident link to the mathematical
structure of the physical problem. Instead, the weights appear close to
Gaussian distributed random matrices. Although significantly more work is
needed to support a robust assessment in this direction, these results suggest
that deep-learning and the numerical solution of the Boltzmann equation
represent two equivalent, but largely distinct paths to the same physical
knowledge. If so, Explainable AI might be an unrealistic target and possibly
even an ill-posed one.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [254] [High-speed multiwavelength photonic temporal integration using silicon photonics](https://arxiv.org/abs/2505.04405)
*Yi Zhang,Nikolaos Farmakidis,Ioannis Roumpos,Miltiadis Moralis-Pegios,Apostolos Tsakyridis,June Sang Lee,Bowei Dong,Yuhan He,Samarth Aggarwal,Nikolaos Pleros,Harish Bhaskaran*

Main category: physics.optics

TL;DR: 该研究提出了一种利用慢热耗散进行时间积分的光子计算架构，以克服光学硬件在处理大规模AI任务时的扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统光学加速器在为AI任务扩展以处理大矢量尺寸方面存在挑战。

Method: 通过在时间上展开标量运算，并引入一种“光路内光子加热器（PHIL）”单元，利用慢热耗散过程对50GHz高速调制的光信号进行全光学时间积分。

Result: 该架构支持端到端的光学信号处理，消除了低效的电光转换，并在统一框架内实现了线性和非线性运算，展示了通过热驱动积分实现高速可扩展光子计算的路径。

Conclusion: 这项工作通过热驱动积分提出了一种可扩展的高速光子计算方法，为解决AI任务中光学硬件的扩展性问题提供了新途径。

Abstract: Optical systems have been pivotal for energy-efficient computing, performing
high-speed, parallel operations in low-loss carriers. While these predominantly
analog optical accelerators bypass digitization to perform parallel
floating-point computations, scaling optical hardware to map large-vector sizes
for AI tasks remains challenging. Here, we overcome this limitation by
unfolding scalar operations in time and introducing a
photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.
Counterintuitively, we exploit a slow heat dissipation process to integrate
optical signals modulated at 50 GHz bridging the speed gap between the widely
applied thermo-optic effects and ultrafast photonics. This architecture
supports optical end-to-end signal processing, eliminates inefficient
electro-optical conversions, and enables both linear and nonlinear operations
within a unified framework. Our results demonstrate a scalable path towards
high-speed photonic computing through thermally driven integration.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [255] [Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication](https://arxiv.org/abs/2504.13777)
*Anqi Shao*

Main category: cs.HC

TL;DR: 本文将AI幻觉视为一种独特的虚假信息形式，区别于传统以人类意图为中心的研究，并呼吁传播学界关注其社会影响。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息理论难以解释无人类意图的AI幻觉现象，且AI幻觉已成为具有社会后果的传播问题，需要新的理论框架进行理解。

Method: 提出一个概念框架，借鉴供需模型和分布式能动性理论，分析AI幻觉在产生、感知和制度反应上与人类虚假信息的差异。

Result: 阐明了AI幻觉的独特性，并为传播学者提出了一个从宏观（制度）、中观（群体）和微观（个体）层面研究幻觉内容的出现、传播和受众接收的研究议程。

Conclusion: AI幻觉应被视为重要的传播现象而非单纯技术故障，传播学研究者需重新思考虚假信息理论的边界，以适应日益融入知识生产的概率性非人类行为者。

Abstract: This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.

</details>


### [256] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TL;DR: 本文提出了一种基于用户意图的交互式视频故事系统，利用VLM、RAG和MAS技术，实现了个性化的、角色与场景可演进的叙事体验，并通过《哈利·波特》系列进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的视频故事交互方法在用户选择、叙事设计和个性化定制方面存在局限性，缺乏灵活性。

Method: 提出了一个基于用户意图的交互系统。该系统分三阶段：1) 使用视觉语言模型（VLM）和先验知识处理视频故事，模拟人类对多模态故事的理解；2) 通过多智能体系统（MAS）交互，根据用户查询和故事阶段创建成长型角色；3) 扩展和可视化对话中提及的各种故事场景。核心技术包括VLM、检索增强生成（RAG）和多智能体系统（MAS）。

Result: 在《哈利·波特》系列中的应用表明，该系统能有效地描绘角色突现的社交行为和成长，提升了视频故事世界中的互动体验。

Conclusion: 该系统通过支持角色行为的动态演化和场景的个性化定制，显著增强了用户在视频故事中的互动体验和沉浸感。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>


### [257] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)
*Stefania Druga,Amy J. Ko*

Main category: cs.HC

TL;DR: 本文介绍了一款集成在类Scratch环境中的AI编程助手Cognimates Scratch Copilot，旨在帮助儿童进行创意构思、代码生成、调试和素材创建，并探讨了其对儿童创造性编码过程及自主性的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管像Scratch这样的创意编码平台降低了儿童编程的门槛，但许多幼儿在将想象中的想法转化为功能代码时仍面临巨大障碍。现有的AI编程助手主要面向成人程序员，缺乏针对儿童在积木式编程环境中的工具。

Method: 研究团队开发了Cognimates Scratch Copilot，一个集成到类Scratch环境中的AI助手，提供构思、代码生成、调试和素材创建的实时支持。并通过对18名7至12岁的国际儿童进行了探索性的定性评估。

Result: AI Copilot有效地支持了关键的创意编码过程，尤其在构思和调试方面帮助显著。儿童在使用AI时表现出强烈的主动性，通过调整或拒绝AI建议来保持创作控制。互动也揭示了提供辅助与培养独立解决问题之间的设计张力，以及从AI的局限性和错误中学习的机会。

Conclusion: Cognimates Scratch Copilot有潜力提升儿童的创造性自我效能感和参与度。基于这些发现，研究者提出了优先考虑青少年自主性和批判性互动，同时提供支持性脚手架的AI编程助手初步设计指南。

Abstract: Creative coding platforms like Scratch have democratized programming for
children, yet translating imaginative ideas into functional code remains a
significant hurdle for many young learners. While AI copilots assist adult
programmers, few tools target children in block-based environments. Building on
prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present
Cognimates Scratch Copilot: an AI-powered assistant integrated into a
Scratch-like environment, providing real-time support for ideation, code
generation, debugging, and asset creation. This paper details the system
architecture and findings from an exploratory qualitative evaluation with 18
international children (ages 7--12). Our analysis reveals how the AI Copilot
supported key creative coding processes, particularly aiding ideation and
debugging. Crucially, it also highlights how children actively negotiated the
use of AI, demonstrating strong agency by adapting or rejecting suggestions to
maintain creative control. Interactions surfaced design tensions between
providing helpful scaffolding and fostering independent problem-solving, as
well as learning opportunities arising from navigating AI limitations and
errors. Findings indicate Cognimates Scratch Copilot's potential to enhance
creative self-efficacy and engagement. Based on these insights, we propose
initial design guidelines for AI coding assistants that prioritize youth agency
and critical interaction alongside supportive scaffolding.

</details>


### [258] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)
*Jessica Y. Bo,Tianyu Xu,Ishan Chatterjee,Katrina Passarella-Ward,Achin Kulshrestha,D Shin*

Main category: cs.HC

TL;DR: 本文研究如何通过激活转向技术使大型语言模型（LLMs）更好地适应用户的个性化偏好，从而提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为个人AI助手时，输出符合用户个性化偏好的回复至关重要，但普通用户难以清晰表达其潜在偏好，现有方法也存在局限性。

Method: 研究利用激活转向技术，在模型推理时引导LLMs对齐可解释的偏好维度。该方法轻量级，用户可通过线性强度因子轻松控制。研究将此机制嵌入三种不同的聊天机器人界面，并进行了用户研究。

Result: 用户研究结果表明，基于偏好的激活转向能有效地使真实对话与用户的隐藏偏好对齐。此外，研究还发现用户在控制、可用性和透明度等方面的不同价值观会影响他们对不同交互界面的偏好。

Conclusion: 激活转向是一种有效的轻量级方法，可以帮助LLMs对齐用户偏好，提升个性化对话体验。用户对不同个性化控制界面的偏好受其价值观影响。

Abstract: As large language models (LLMs) improve in their capacity to serve as
personal AI assistants, their ability to output uniquely tailored, personalized
responses that align with the soft preferences of their users is essential for
enhancing user satisfaction and retention. However, untrained lay users have
poor prompt specification abilities and often struggle with conveying their
latent preferences to AI assistants. To address this, we leverage activation
steering to guide LLMs to align with interpretable preference dimensions during
inference. In contrast to memory-based personalization methods that require
longer user history, steering is extremely lightweight and can be easily
controlled by the user via an linear strength factor. We embed steering into
three different interactive chatbot interfaces and conduct a within-subjects
user study (n=14) to investigate how end users prefer to personalize their
conversations. The results demonstrate the effectiveness of preference-based
steering for aligning real-world conversations with hidden user preferences,
and highlight further insights on how diverse values around control, usability,
and transparency lead users to prefer different interfaces.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912)
*Can Cui,Pengxiang Ding,Wenxuan Song,Shuanghao Bai,Xinyang Tong,Zirui Ge,Runze Suo,Wanqi Zhou,Yang Liu,Bofang Jia,Han Zhao,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: 该论文针对双系统VLA架构开源工作不足的问题，将通过总结比较、实证评估现有架构，并提供一个低成本开源模型以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 双系统视觉-语言-动作 (VLA) 架构是具身智能研究的热点，但目前缺乏足够的开源工作以支持进一步的性能分析和优化。

Method: 本文将总结和比较现有双系统架构的结构设计，并对这些架构的核心设计元素进行系统性的实证评估。

Result: 最终，本研究将提供一个低成本的开源模型，用于该领域的进一步探索。

Conclusion: 该项目旨在通过提供一个基础开源模型并持续更新更多实验结论和性能更优的模型，为研究社区提供支持，促进该领域的进一步发展。

Abstract: Dual-system VLA (Vision-Language-Action) architectures have become a hot
topic in embodied intelligence research, but there is a lack of sufficient
open-source work for further performance analysis and optimization. To address
this problem, this paper will summarize and compare the structural designs of
existing dual-system architectures, and conduct systematic empirical
evaluations on the core design elements of existing dual-system architectures.
Ultimately, it will provide a low-cost open-source model for further
exploration. Of course, this project will continue to update with more
experimental conclusions and open-source models with improved performance for
everyone to choose from. Project page: https://openhelix-robot.github.io/.

</details>


### [260] [Scalable Aerial GNSS Localization for Marine Robots](https://arxiv.org/abs/2505.04095)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Charlotte Morissette,Chloe Si,Bobak Baghi,Gregory Dudek*

Main category: cs.RO

TL;DR: 提出了一种利用配备GNSS的无人机追踪和定位近水面水下机器人的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的水下机器人GNSS定位方法因水面信号反射和高成本而困难或无效，现有其他方法（如惯性导航、DVL、SLAM、声学方法）存在误差累积和高计算复杂度等挑战，因此需要更高效和可扩展的解决方案。

Method: 利用配备GNSS定位的空中无人机来追踪和定位接近水面的水下机器人。

Result: 该方法能够实现精确的单机器人和多机器人水下机器人定位。

Conclusion: 这种新颖的适应性方法为水下机器人提供了准确的定位方案。

Abstract: Accurate localization is crucial for water robotics, yet traditional onboard
Global Navigation Satellite System (GNSS) approaches are difficult or
ineffective due to signal reflection on the water's surface and its high cost
of aquatic GNSS receivers. Existing approaches, such as inertial navigation,
Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face
challenges like error accumulation and high computational complexity.
Therefore, a more efficient and scalable solution remains necessary. This paper
proposes an alternative approach that leverages an aerial drone equipped with
GNSS localization to track and localize a marine robot once it is near the
surface of the water. Our results show that this novel adaptation enables
accurate single and multi-robot marine robot localization.

</details>


### [261] [RGB-Event Fusion with Self-Attention for Collision Prediction](https://arxiv.org/abs/2505.04258)
*Pietro Bonazzi,Christian Vogt,Michael Jost,Haotong Qin,Lyes Khacef,Federico Paredes-Valles,Michele Magno*

Main category: cs.RO

TL;DR: 本文提出了一种融合RGB和事件相机的神经网络框架，用于预测无人机与动态障碍物的碰撞时间和位置，并评估了其性能、计算成本以及事件相机模型的量化。


<details>
  <summary>Details</summary>
Motivation: 确保自主机器人在动态真实环境中安全运行，需要鲁棒且实时的障碍物规避能力。

Method: 提出了一种包含RGB和事件数据两个独立编码器分支的神经网络架构，并通过自注意力机制进行融合以提高预测精度。使用了ABCD数据集进行基准测试。

Result: 在50Hz的相同预测吞吐量下，融合模型相比单模态方法，在预测精度上平均提高1%，在超过0.5米的距离上提高10%，但内存增加71%，FLOPs增加105%。事件相机模型在位置误差方面优于RGB模型4%，时间误差方面优于26%，且计算成本相似。对事件相机模型进行1至8位量化评估了预测性能和计算效率之间的权衡。

Conclusion: 研究结果揭示了在机器人应用中使用RGB和事件相机进行多模态感知时的性能与成本权衡，并表明事件相机是一种有竞争力的替代方案。

Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe
operation of autonomous robots in dynamic, real-world environments. This paper
proposes a neural network framework for predicting the time and collision
position of an unmanned aerial vehicle with a dynamic object, using RGB and
event-based vision sensors. The proposed architecture consists of two separate
encoder branches, one for each modality, followed by fusion by self-attention
to improve prediction accuracy. To facilitate benchmarking, we leverage the
ABCD [8] dataset collected that enables detailed comparisons of single-modality
and fusion-based approaches. At the same prediction throughput of 50Hz, the
experimental results show that the fusion-based model offers an improvement in
prediction accuracy over single-modality approaches of 1% on average and 10%
for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%
in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for
position and 26% for time error at a similar computational cost, making it a
competitive alternative. Additionally, we evaluate quantized versions of the
event-based models, applying 1- to 8-bit quantization to assess the trade-offs
between predictive performance and computational efficiency. These findings
highlight the trade-offs of multi-modal perception using RGB and event-based
cameras in robotic applications.

</details>


### [262] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文综述了用于机器人任务级控制的基于模型的规划和执行系统，讨论了现有系统的设计选择、问题、解决方案，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着用于机器人任务级控制的基于模型的系统日益增多，需要对这些系统进行梳理、分析其设计选择、面临的问题、现有解决方案，并探讨未来发展方向。

Method: 本文通过回顾和分析现有的机器人模型规划与执行系统（特别是自ROSPlan系统以来出现的系统），考察其多样化的设计选择、试图解决的问题以及已提出的不同解决方案。

Result: 识别并讨论了现有基于模型的机器人任务级控制系统中的多种设计选择、关键问题以及已提出的解决方案。

Conclusion: 论文为基于模型的机器人规划与执行系统的未来发展提供了方向性建议。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [263] [Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees](https://arxiv.org/abs/2505.04583)
*Nathaniel Dennler,Zhonghao Shi,Uksang Yoo,Stefanos Nikolaidis,Maja Matarić*

Main category: cs.RO

TL;DR: 该研究提出一种基于因果树的方法，根据用户表现个性化康复训练难度，以适应卒中患者的个体差异。


<details>
  <summary>Details</summary>
Motivation: 现有康复机器人通常采用统一的运动难度标准，未能考虑到卒中患者对难度感知的个体差异，这可能影响康复效果和患者的积极性。

Method: 采用一种基于因果树的方法，根据用户的实际运动表现来计算和评估运动难度。

Result: 该方法能够准确地模拟特定用户的运动难度，并提供了一个易于理解的模型，解释了为什么某个运动对该用户而言是困难的。

Conclusion: 基于因果树的个性化难度计算方法能有效评估康复运动难度，并为用户和护理人员提供了可解释的难度模型。

Abstract: Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [264] [Differentially Private Densest-$k$-Subgraph](https://arxiv.org/abs/2505.03858)
*Alireza Khayatian,Anil Vullikanti,Aritra Konar*

Main category: cs.DS

TL;DR: 本文首次为稠密k子图（DkS）问题设计了具有差分隐私（DP）保证的算法，主要基于图邻接矩阵的私有主成分分析。


<details>
  <summary>Details</summary>
Motivation: 许多图数据集包含敏感网络数据，因此需要隐私保护的图挖掘方法。稠密k子图（DkS）问题是图挖掘的关键部分，但现有算法缺乏隐私保护。

Method: 该研究通用方法是基于图邻接矩阵的主成分（PC）在边差分隐私（edge DP）下输出k个顶点子集。具体探索了输出扰动、基于实例特定敏感性和平滑敏感性的方法，并设计了一种新颖高效的Propose-Test-Release（PTR）框架以及使用了迭代私有幂方法（PPM）来实现私有PC。

Result: 研究发现局部敏感性与全局敏感性差距较大，而平滑敏感性可接近全局敏感性。提出的PTR框架在DkS问题上实现了良好的效用，并且运行时间远快于PPM（平均快180倍），尽管可能需要稍大的隐私预算。实验在多个真实大型网络上验证了良好的隐私-效用权衡。

Conclusion: 本文成功为DkS问题设计了首批差分隐私算法。其中，基于PTR框架的私有主成分分析方法在效用、隐私和计算效率之间取得了良好平衡，尤其适用于大规模网络。

Abstract: Many graph datasets involve sensitive network data, motivating the need for
privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a
key primitive in graph mining that aims to extract a subset of $k$ vertices
with the maximum internal connectivity. Although non-private algorithms are
known for D$k$S, this paper is the first to design algorithms that offer formal
differential privacy (DP) guarantees for the problem. We base our general
approach on using the principal component (PC) of the graph adjacency matrix to
output a subset of $k$ vertices under edge DP. For this task, we first consider
output perturbation, which traditionally offer good scalability, but at the
expense of utility. Our tight on the local sensitivity indicate a big gap with
the global sensitivity, motivating the use of instance specific sensitive
methods for private PC. Next, we derive a tight bound on the smooth sensitivity
and show that it can be close to the global sensitivity. This leads us to
consider the Propose-Test-Release (PTR) framework for private PC. Although
computationally expensive in general, we design a novel approach for
implementing PTR in the same time as computation of a non-private PC, while
offering good utility for \DkS{}. Additionally, we also consider the iterative
private power method (PPM) for private PC, albeit it is significantly slower
than PTR on large networks. We run our methods on diverse real-world networks,
with the largest having 3 million vertices, and show good privacy-utility
trade-offs. Although PTR requires a slightly larger privacy budget, on average,
it achieves a 180-fold improvement in runtime over PPM.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [265] [LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling](https://arxiv.org/abs/2505.04101)
*AbdulAziz AbdulGhaffar,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型（LLMs）在网络安全领域的适用性，特别是通过STRIDE威胁建模对5G威胁进行分类。


<details>
  <summary>Details</summary>
Motivation: 目前几乎没有研究分析大型语言模型（LLMs）在网络安全领域的适用性，本研究旨在填补这一空白。

Method: 采用四种提示技术（prompting techniques）和五种不同的大型语言模型（LLMs），对5G威胁进行STRIDE分类。

Result: 评估结果揭示了LLMs在特定威胁建模中行为的关键发现、详细见解以及影响其行为的可能潜在因素。

Conclusion: 数值结果和见解表明，为了更好地应用于网络安全用例，有必要对大型语言模型（LLMs）进行调整和微调。

Abstract: Artificial Intelligence (AI) is expected to be an integral part of
next-generation AI-native 6G networks. With the prevalence of AI, researchers
have identified numerous use cases of AI in network security. However, there
are almost nonexistent studies that analyze the suitability of Large Language
Models (LLMs) in network security. To fill this gap, we examine the suitability
of LLMs in network security, particularly with the case study of STRIDE threat
modeling. We utilize four prompting techniques with five LLMs to perform STRIDE
classification of 5G threats. From our evaluation results, we point out key
findings and detailed insights along with the explanation of the possible
underlying factors influencing the behavior of LLMs in the modeling of certain
threats. The numerical results and the insights support the necessity for
adjusting and fine-tuning LLMs for network security use cases.

</details>


### [266] [AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection](https://arxiv.org/abs/2505.03796)
*Lokesh Koli,Shubham Kalra,Rohan Thakur,Anas Saifi,Karanpreet Singh*

Main category: cs.CR

TL;DR: 本文提出了一种人工智能驱动的内部风险管理 (IRM) 系统，该系统通过行为分析、动态风险评分和实时策略执行，以高准确性和适应性检测和缓解内部威胁。


<details>
  <summary>Details</summary>
Motivation: 内部威胁因其隐蔽性和情境性，常能规避传统基于规则的检测系统，对组织安全构成重大挑战。

Method: 该系统集成行为分析、动态风险评分和实时策略执行。引入了一种混合评分机制，从静态PRISM模型过渡到基于自编码器神经网络的自适应AI模型，该模型使用专家标注的用户活动数据进行训练，并通过迭代反馈和持续学习进行优化。

Result: 系统将误报率降低了59%，真正例检测率提高了30%。平台每日可处理高达1000万个日志事件，查询延迟低于300毫秒，并支持自动执行策略违规操作。部署后，事件响应时间减少了47%。

Conclusion: 这项工作建立了一个可扩展且主动的框架，用于缓解本地和混合环境中新兴的内部风险，显著提高了检测精度和运营效率。

Abstract: Insider threats pose a significant challenge to organizational security,
often evading traditional rule-based detection systems due to their subtlety
and contextual nature. This paper presents an AI-powered Insider Risk
Management (IRM) system that integrates behavioral analytics, dynamic risk
scoring, and real-time policy enforcement to detect and mitigate insider
threats with high accuracy and adaptability. We introduce a hybrid scoring
mechanism - transitioning from the static PRISM model to an adaptive AI-based
model utilizing an autoencoder neural network trained on expert-annotated user
activity data. Through iterative feedback loops and continuous learning, the
system reduces false positives by 59% and improves true positive detection
rates by 30%, demonstrating substantial gains in detection precision.
Additionally, the platform scales efficiently, processing up to 10 million log
events daily with sub-300ms query latency, and supports automated enforcement
actions for policy violations, reducing manual intervention. The IRM system's
deployment resulted in a 47% reduction in incident response times, highlighting
its operational impact. Future enhancements include integrating explainable AI,
federated learning, graph-based anomaly detection, and alignment with Zero
Trust principles to further elevate its adaptability, transparency, and
compliance-readiness. This work establishes a scalable and proactive framework
for mitigating emerging insider risks in both on-premises and hybrid
environments.

</details>


### [267] [Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning](https://arxiv.org/abs/2505.03817)
*Aditya Shinde,Prashant Doshi*

Main category: cs.CR

TL;DR: 本文提出一种利用逆强化学习从系统审计日志中建模攻击者行为偏好的方法。


<details>
  <summary>Details</summary>
Motivation: 当前攻击者建模依赖于不断变化的工具和技术，而攻击者的内在行为偏好更为稳定，研究这些偏好有助于改进威胁归因。

Method: 将攻击者视为专家决策智能体，从审计日志的攻击溯源图中提取状态-行为轨迹，并使用逆强化学习（IRL）推断其未知行为偏好。

Result: 该方法首次证明了可以从底层取证数据中自动揭示攻击者的主观偏好，这些偏好在不同工具间具有一致性，反映了攻击者的固有倾向。

Conclusion: 推断出的偏好可作为攻击者的独特行为签名，有助于改进威胁归因，并为网络安全提供新的分析维度。

Abstract: This paper presents a holistic approach to attacker preference modeling from
system-level audit logs using inverse reinforcement learning (IRL). Adversary
modeling is an important capability in cybersecurity that lets defenders
characterize behaviors of potential attackers, which enables attribution to
known cyber adversary groups. Existing approaches rely on documenting an
ever-evolving set of attacker tools and techniques to track known threat
actors. Although attacks evolve constantly, attacker behavioral preferences are
intrinsic and less volatile. Our approach learns the behavioral preferences of
cyber adversaries from forensics data on their tools and techniques. We model
the attacker as an expert decision-making agent with unknown behavioral
preferences situated in a computer host. We leverage attack provenance graphs
of audit logs to derive a state-action trajectory of the attack. We test our
approach on open datasets of audit logs containing real attack data. Our
results demonstrate for the first time that low-level forensics data can
automatically reveal an adversary's subjective preferences, which serves as an
additional dimension to modeling and documenting cyber adversaries. Attackers'
preferences tend to be invariant despite their different tools and indicate
predispositions that are inherent to the attacker. As such, these inferred
preferences can potentially serve as unique behavioral signatures of attackers
and improve threat attribution.

</details>


### [268] [Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles](https://arxiv.org/abs/2505.03850)
*Hanlin Chen,Simin Chen,Wenyu Li,Wei Yang,Yiheng Feng*

Main category: cs.CR

TL;DR: 本文提出了一种基于推理时攻击的自动驾驶汽车影响分析方法，并证明了此类攻击对车辆安全的威胁。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车（AVs）的网络安全和相关安全问题是重要研究课题。其感知模块易受攻击，但当前研究多关注感知正确性，对其他攻击类型（如推理时攻击）及其对安全的影响研究不足。

Method: 提出一种针对自动驾驶汽车的基于推理时攻击（inference time attacks）的影响分析方法。通过仿真系统进行验证。

Result: 在仿真系统中证明了推理时攻击能够威胁到自动驾驶车辆自身以及其他交通参与者的安全。

Conclusion: 推理时攻击对自动驾驶汽车的安全性构成实际威胁，需要关注除感知正确性之外的其他攻击维度。

Abstract: As a safety-critical cyber-physical system, cybersecurity and related safety
issues for Autonomous Vehicles (AVs) have been important research topics for a
while. Among all the modules on AVs, perception is one of the most accessible
attack surfaces, as drivers and AVs have no control over the outside
environment. Most current work targeting perception security for AVs focuses on
perception correctness. In this work, we propose an impact analysis based on
inference time attacks for autonomous vehicles. We demonstrate in a simulation
system that such inference time attacks can also threaten the safety of both
the ego vehicle and other traffic participants.

</details>


### [269] [Data-Driven Falsification of Cyber-Physical Systems](https://arxiv.org/abs/2505.03863)
*Atanu Kundu,Sauvik Gon,Rajarshi Ray*

Main category: cs.CR

TL;DR: 本文提出了一个框架，将网络物理系统（CPS）的证伪与深度神经网络（DNN）的证伪联系起来，并利用决策树的可解释性来加速CPS的证伪。


<details>
  <summary>Details</summary>
Motivation: 网络物理系统（CPS）广泛应用于医疗、航空和自动驾驶等安全关键领域，因此其操作安全性的形式化验证至关重要。本研究关注证伪问题，即在系统中主动搜索不安全的执行路径，而非证明其完全不存在。

Method: 该框架通过以下步骤实现：(1) 为被测CPS构建代理模型，可以是DNN模型或决策树；(2) 应用各种DNN证伪工具来证伪CPS；(3) 提出一种新的证伪算法，该算法利用从CPS模型的决策树代理中提取的安全违规解释进行指导。

Result: 该框架（实现为工具FlexiFal）能够检测CPS中难以发现的反例，包括具有线性和非线性动态的系统。决策树引导的证伪在ARCH-COMP 2024证伪基准测试中高效地找到了多个反例，显示出良好效果。

Conclusion: 提出的框架通过结合DNN证伪技术和决策树的可解释性，为CPS提供了一种有效的证伪方法，能够发现系统中的安全漏洞，特别是在复杂动态系统中，并已在基准测试中证明了其有效性。

Abstract: Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.

</details>


### [270] [AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience](https://arxiv.org/abs/2505.03945)
*Shamnad Mohamed Shaffi,Sunish Vengathattil,Jezeena Nikarthil Sidhick,Resmi Vijayan*

Main category: cs.CR

TL;DR: 论文探讨了人工智能 (AI) 如何通过预测分析、基于行为的威胁检测和 AI 驱动的加密来增强云安全，并讨论了其优势、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统云安全解决方案难以有效应对日益增长的复杂实时威胁，因此需要更先进的技术来保护云数据架构。

Method: 应用人工智能技术，包括机器学习、计算基础设施的统计可视化、安全漏洞检测及反制措施，特别是通过预测分析、基于行为的安全威胁检测和 AI 驱动的加密来增强云安全。

Result: AI 能够通过审查网络活动和预防异常行为来提升云安全防护能力，克服了传统安全模型的一些问题。然而，AI 应用本身也带来了数据隐私、模型偏见和合规性等新问题。

Conclusion: AI 显著改善了云计算环境的保护，但需要在可靠性、模块化和伦理方面进一步努力。未来可以将 AI 与区块链等新技术结合，以进一步加强安全框架。

Abstract: Cloud security concerns have been greatly realized in recent years due to the
increase of complicated threats in the computing world. Many traditional
solutions do not work well in real-time to detect or prevent more complex
threats. Artificial intelligence is today regarded as a revolution in
determining a protection plan for cloud data architecture through machine
learning, statistical visualization of computing infrastructure, and detection
of security breaches followed by counteraction. These AI-enabled systems make
work easier as more network activities are scrutinized, and any anomalous
behavior that might be a precursor to a more serious breach is prevented. This
paper examines ways AI can enhance cloud security by applying predictive
analytics, behavior-based security threat detection, and AI-stirring
encryption. It also outlines the problems of the previous security models and
how AI overcomes them. For a similar reason, issues like data privacy, biases
in the AI model, and regulatory compliance are also covered. So, AI improves
the protection of cloud computing contexts; however, more efforts are needed in
the subsequent phases to extend the technology's reliability, modularity, and
ethical aspects. This means that AI can be blended with other new computing
technologies, including blockchain, to improve security frameworks further. The
paper discusses the current trends in securing cloud data architecture using AI
and presents further research and application directions.

</details>


### [271] [MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models](https://arxiv.org/abs/2505.04015)
*Soheil Zibakhsh Shabgahi,Yaman Jandali,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: 本文提出了一种名为MergeGuard的新方法，用于缓解AI木马攻击，该方法通过线性化和合并全连接层来提高模型泛化性和性能，并有效降低木马攻击成功率。


<details>
  <summary>Details</summary>
Motivation: AI模型中的木马攻击会导致嵌入触发器的输入被错误分类到攻击者指定的目标类别，这对由不受信任的第三方训练的模型的可用性构成了重大威胁。

Method: MergeGuard的核心是一种新的训练后方法，用于线性化和合并模型的全连接层。

Result: 在Transformer模型上的概念验证评估表明，MergeGuard在保持模型准确性的同时，显著降低了木马攻击的成功率，并且其性能优于常用的基于微调的（训练后）木马缓解方法。

Conclusion: MergeGuard是一种有效的训练后AI木马攻击缓解方法，它不仅能抵御攻击，还能同时提高模型的泛化能力和整体性能。

Abstract: This paper proposes MergeGuard, a novel methodology for mitigation of AI
Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers
to be misclassified to an adversary's target class, posing a significant threat
to model usability trained by an untrusted third party. The core of MergeGuard
is a new post-training methodology for linearizing and merging fully connected
layers which we show simultaneously improves model generalizability and
performance. Our Proof of Concept evaluation on Transformer models demonstrates
that MergeGuard maintains model accuracy while decreasing trojan attack success
rate, outperforming commonly used (post-training) Trojan mitigation by
fine-tuning methodologies.

</details>


### [272] [A Comprehensive Analysis of Adversarial Attacks against Spam Filters](https://arxiv.org/abs/2505.03831)
*Esra Hotoğlu,Sevil Sen,Burcu Can*

Main category: cs.CR

TL;DR: 研究评估了对抗性攻击对基于深度学习的垃圾邮件检测系统的影响，并引入新方法以提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 日益复杂的对抗性攻击对深度学习邮件过滤器的有效性构成了重大挑战，需要研究其影响以增强安全性。

Method: 在真实数据集上评估了六种深度学习模型，分析了词、字符、句子和AI生成段落级别的对抗性攻击，并引入了包括垃圾邮件权重和注意力权重在内的新评分函数。

Result: 全面分析揭示了垃圾邮件过滤器在面对不同层面和经过优化的对抗性攻击时的漏洞。

Conclusion: 该研究揭示了垃圾邮件过滤器的脆弱性，并为改进其安全性以应对不断演变的对抗性威胁做出了贡献。

Abstract: Deep learning has revolutionized email filtering, which is critical to
protect users from cyber threats such as spam, malware, and phishing. However,
the increasing sophistication of adversarial attacks poses a significant
challenge to the effectiveness of these filters. This study investigates the
impact of adversarial attacks on deep learning-based spam detection systems
using real-world datasets. Six prominent deep learning models are evaluated on
these datasets, analyzing attacks at the word, character sentence, and
AI-generated paragraph-levels. Novel scoring functions, including spam weights
and attention weights, are introduced to improve attack effectiveness. This
comprehensive analysis sheds light on the vulnerabilities of spam filters and
contributes to efforts to improve their security against evolving adversarial
threats.

</details>


### [273] [Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper](https://arxiv.org/abs/2505.04265)
*Abdulrahman S Almuhaidib,Azlan Mohd Zain,Zalmiyah Zakaria,Izyan Izzati Kamsani,Abdulaziz S Almuhaidib*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型 (LLM) 在自动化和改进漏洞评估 (VA) 报告验证过程中的应用，旨在减少误报并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注大型语言模型 (LLM) 在网络安全防御方面的应用，而其在攻击性应用，特别是漏洞评估 (VA) 报告验证方面的研究相对匮乏。

Method: 通过对相关文献的批判性回顾，本文提出了一种利用大型语言模型 (LLM) 实现漏洞评估 (VA) 报告分析和验证过程自动化的新方法。

Result: 研究结果显示，利用大型语言模型 (LLM) 实现自动化，有望改进漏洞评估 (VA) 报告的验证过程，从而提高准确性，减少人工投入，并增强整体安全态势。

Conclusion: 本文的研究为大型语言模型 (LLM) 在攻击性和防御性网络安全方面的能力提供了进一步证据，有助于据此设计更合适的网络安全策略和工具。

Abstract: This, with the ever-increasing sophistication of cyberwar, calls for novel
solutions. In this regard, Large Language Models (LLMs) have emerged as a
highly promising tool for defensive and offensive cybersecurity-related
strategies. While existing literature has focused much on the defensive use of
LLMs, when it comes to their offensive utilization, very little has been
reported-namely, concerning Vulnerability Assessment (VA) report validation.
Consequentially, this paper tries to fill that gap by investigating the
capabilities of LLMs in automating and improving the validation process of the
report of the VA. From the critical review of the related literature, this
paper hereby proposes a new approach to using the LLMs in the automation of the
analysis and within the validation process of the report of the VA that could
potentially reduce the number of false positives and generally enhance
efficiency. These results are promising for LLM automatization for improving
validation on reports coming from VA in order to improve accuracy while
reducing human effort and security postures. The contribution of this paper
provides further evidence about the offensive and defensive LLM capabilities
and therefor helps in devising more appropriate cybersecurity strategies and
tools accordingly.

</details>


### [274] [Guardians of the Web: The Evolution and Future of Website Information Security](https://arxiv.org/abs/2505.04308)
*Md Saiful Islam,Li Xiangdong*

Main category: cs.CR

TL;DR: 本文探讨了网站信息安全的发展历程、当前实践方法以及未来趋势，强调了持续研究与创新的必要性。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，网站信息安全已成为一个关键问题，网络威胁不断演变，亟需有效的安全措施来保护信息和维护信任。

Method: 本文通过回顾和分析网站信息安全领域的历史发展、当前的实践方法以及预测未来的发展方向进行研究。

Result: 研究阐述了网站信息安全从早期（60-80年代ARPANET、TCP/IP、公钥加密等）到转型期（90年代SSL、密码、防火墙等）的演进。当前实践涉及加密、安全编码、定期审计和用户教育等多层次方法。未来趋势将受人工智能、区块链、量子计算以及国际合作与标准化的影响。

Conclusion: 随着网络威胁的持续演进，网站信息安全的持续研究和创新对于保护敏感信息、维护数字世界的信任至关重要。

Abstract: Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [275] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac是一个基于Web的AI平台，用于自动分割4D心脏图像和分类心脏疾病，以辅助临床诊断。


<details>
  <summary>Details</summary>
Motivation: 对心脏成像数据进行精确有效处理的需求，以改善心血管疾病的识别和管理。

Method: 采用基于深度学习的分割模型（在公开ACDC数据集上训练）对心脏结构（左右心室、心肌）进行分割，随后通过一个两步分类流程，利用从分割结构中提取的特征将心脏图像分为五种诊断类别。

Result: 分割模块的整体准确率达到92.6%，而基于分割心脏结构特征训练的分类模块在五个类别上的准确率达到98%，这些结果优于现有同时集成分割和分类模型的先进方法。

Conclusion: IntelliCardiac支持实时可视化、工作流程集成和AI辅助诊断，具有作为可扩展、准确的临床决策辅助工具的巨大潜力，可用于心脏成像和诊断。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [276] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TL;DR: 本文提出了一种利用大型预训练扩散模型和空间条件化技术，将卫星SAR图像转换为机载SAR图像，并提升模拟SAR图像真实感的方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率机载SAR图像获取成本高昂且数量有限，同时缺乏开放、标注良好或易于利用的SAR图文数据集，这限制了现有基础模型在遥感应用中的使用。

Method: 基于ONERA存档的11万张机载SAR图像构建训练集，利用一个35亿参数的预训练潜在扩散模型，并引入空间条件化技术，以实现从卫星SAR图像到机载SAR图像的转换，以及增强模拟SAR图像的真实感。

Result: 该方法能够有效地将卫星SAR图像转换为机载SAR图像，并能提升由ONERA物理模拟器EMPRISE生成的模拟图像的真实感。

Conclusion: 该研究提出了一种新颖的方法，利用AI推进SAR成像技术，并首次在文献中引入此途径，表明其在增强SAR图像真实性和扩充稀缺数据方面的有效性。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [277] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TL;DR: 该研究利用深度学习模型（特别是Video Swin Tiny）通过分析帕金森病患者的面部视频来评估抑郁症状的存在和严重程度。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者中抑郁症状普遍存在，但由于与运动症状（如面具脸）重叠，常常被漏诊。本研究旨在探索利用深度学习进行面部视频分析，以辅助诊断帕金森病患者的抑郁症状。

Method: 研究使用了三种深度学习模型：ViViT、Video Swin Tiny 和带有注意力层的 3D CNN-LSTM。通过分析来自178名帕金森病患者的1875个面部视频，评估老年抑郁量表（GDS）检测到的抑郁症状。研究还考虑了患者在服药（ON状态）和停药12小时（OFF状态）后的差异。

Result: Video Swin Tiny 模型在所有测试模型中表现最佳。在二分类任务（是否存在抑郁症状）中，准确率达到94%，F1分数达到93.7%。在多分类任务（无、轻度或重度抑郁症状）中，准确率达到87.1%，F1分数达到85.4%。

Conclusion: 研究表明，深度学习模型，特别是Video Swin Tiny，能够通过面部视频分析有效评估帕金森病患者的抑郁症状及其严重程度，为临床诊断提供了有前景的工具。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [278] [Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification](https://arxiv.org/abs/2505.04003)
*Feng Gao,Sheng Liu,Chuanzheng Gong,Xiaowei Zhou,Jiayi Wang,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: 提出了一种基于原型的信息补偿网络（PICNet），用于高光谱图像（HSI）和SAR/LiDAR数据的土地覆盖分类，以解决多源特征耦合和互补信息探索不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多源遥感数据联合分类方法在不同频率的多源特征耦合以及互补信息探索的一致性方面存在挑战，影响分类的准确性和可靠性。

Method: 提出了PICNet模型：1. 设计频率交互模块，将多源特征解耦为高低频分量再重耦，以增强特征提取中的跨频耦合。2. 设计基于原型的信息补偿模块，引入可学习的模态原型来表示全局模态信息，并通过原型向量与原始特征间的交叉注意力实现跨模态特征集成与对齐。

Result: 在三个公开数据集上的大量实验表明，PICNet显著优于当前最先进的方法。

Conclusion: PICNet通过频率交互和基于原型的信息补偿，有效解决了多源遥感数据分类中的特征耦合和互补信息利用问题，提升了土地覆盖分类的准确性和可靠性。

Abstract: Multi-source remote sensing data joint classification aims to provide
accuracy and reliability of land cover classification by leveraging the
complementary information from multiple data sources. Existing methods confront
two challenges: inter-frequency multi-source feature coupling and inconsistency
of complementary information exploration. To solve these issues, we present a
Prototype-based Information Compensation Network (PICNet) for land cover
classification based on HSI and SAR/LiDAR data. Specifically, we first design a
frequency interaction module to enhance the inter-frequency coupling in
multi-source feature extraction. The multi-source features are first decoupled
into high- and low-frequency components. Then, these features are recoupled to
achieve efficient inter-frequency communication. Afterward, we design a
prototype-based information compensation module to model the global
multi-source complementary information. Two sets of learnable modality
prototypes are introduced to represent the global modality information of
multi-source data. Subsequently, cross-modal feature integration and alignment
are achieved through cross-attention computation between the modality-specific
prototype vectors and the raw feature representations. Extensive experiments on
three public datasets demonstrate the significant superiority of our PICNet
over state-of-the-art methods. The codes are available at
https://github.com/oucailab/PICNet.

</details>


### [279] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TL;DR: 开发了一种3D卷积神经网络，通过随机噪声注入增强，可有效区分健康与阿尔茨海默症的脑部MRI，测试集准确率达0.912。


<details>
  <summary>Details</summary>
Motivation: 旨在评估三维卷积神经网络结合简单数据增强方法（随机噪声注入）在T1加权脑部MRI扫描中对阿尔茨海默症进行分类的有效性。

Method: 构建了一个包含3D卷积层、池化层、批量归一化层、全连接ReLU层和Sigmoid输出层的三维卷积神经网络。采用随机噪声注入作为数据增强手段，并使用五折交叉验证进行模型训练和评估。

Result: 模型在测试集上实现了0.912的准确率和0.961的ROC曲线下面积（AUC）。与仅使用图像大小调整相比，性能提升了约0.027。灵敏度和特异性均超过0.90。

Conclusion: 研究结果表明，简单的随机噪声注入增强方法能有效提升3D MRI分类的性能。这激励了未来对更高级增强方法（如3D U-Net和视觉Transformer等架构）的探索。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [280] [Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems](https://arxiv.org/abs/2505.03946)
*Matthew Sgambati,Aleksandar Vakanski,Matthew Anderson*

Main category: cs.DC

TL;DR: 研究提出了一种基于去中心化分布式近端策略优化 (DD-PPO) 的新型强化学习调度器，用于解决HPC环境中现有调度算法在可扩展性和大规模数据集处理上的局限性，并提升调度性能。


<details>
  <summary>Details</summary>
Motivation: 传统HPC调度算法难以适应日益增长的系统异构性和规模，而现有强化学习调度方法在处理大规模数据集时面临可扩展性问题，且常依赖小规模数据集进行训练。

Method: 采用去中心化分布式近端策略优化 (DD-PPO) 算法构建强化学习调度器。该方法支持在多个工作节点上进行大规模分布式训练，无需每一步都进行参数同步，从而提高了可扩展性、训练效率和样本利用率。

Result: 通过在包含超过1150万条真实HPC作业轨迹的数据集上进行验证，实验结果表明，与传统的基于规则的调度器和现有的基于强化学习的调度算法相比，DD-PPO调度器在调度性能方面均有显著提升。

Conclusion: DD-PPO算法为HPC作业调度提供了一种更具可扩展性和高效性的强化学习方案，能够有效处理大规模数据并改善调度性能，优于传统方法和其他现有RL方法。

Abstract: Resource allocation in High Performance Computing (HPC) environments presents
a complex and multifaceted challenge for job scheduling algorithms. Beyond the
efficient allocation of system resources, schedulers must account for and
optimize multiple performance metrics, including job wait time and system
utilization. While traditional rule-based scheduling algorithms dominate the
current deployments of HPC systems, the increasing heterogeneity and scale of
those systems is expected to challenge the efficiency and flexibility of those
algorithms in minimizing job wait time and maximizing utilization. Recent
research efforts have focused on leveraging advancements in Reinforcement
Learning (RL) to develop more adaptable and intelligent scheduling strategies.
Recent RL-based scheduling approaches have explored a range of algorithms, from
Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,
hybrid methods that integrate Graph Neural Networks with RL techniques.
However, a common limitation across these methods is their reliance on
relatively small datasets, and these methods face scalability issues when using
large datasets. This study introduces a novel RL-based scheduler utilizing the
Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,
which supports large-scale distributed training across multiple workers without
requiring parameter synchronization at every step. By eliminating reliance on
centralized updates to a shared policy, the DD-PPO scheduler enhances
scalability, training efficiency, and sample utilization. The validation
dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO
performance between traditional and advanced scheduling approaches, and the
experimental results demonstrate improved scheduling performance in comparison
to both rule-based schedulers and existing RL-based scheduling algorithms.

</details>


### [281] [Can Large Language Models Predict Parallel Code Performance?](https://arxiv.org/abs/2505.03988)
*Gregory Bolet,Giorgis Georgakoudis,Harshitha Menon,Konstantinos Parasyris,Niranjan Hasabnis,Hayden Estes,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 本文研究了使用大型语言模型（LLMs）在无需硬件剖析的情况下，预测GPU代码是计算密集型还是带宽密集型的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的GPU代码性能分析依赖于在目标硬件上进行运行时剖析，但由于高端GPU的获取受限，这种方法日益困难，因此需要探索替代方案。

Method: 研究将问题框架化为一个Roofline分类任务：给定GPU核心的源代码和目标GPU的硬件规格，LLM预测其是计算密集型还是带宽密集型。构建了一个包含340个GPU核心（来自HeCBench，用CUDA和OpenMP编写）及其通过经验性GPU剖析获得的真实标签的平衡数据集。在四种场景下评估LLMs：(1) 提供核心源代码的剖析数据；(2) 仅提供源代码（零样本）；(3) 提供代码和标签对（少样本）；(4) 在小型自定义数据集上进行微调。

Result: 结果表明，当提供明确的剖析数据时，最先进的LLMs对Roofline模型有深刻理解，分类准确率达到100%。在零样本和少样本设置下，具备推理能力的LLMs在无需剖析信息的情况下，对GPU源代码的准确率可达64%，显著优于标准LLMs。此外，LLM微调需要远超当前可用数据量的数据。

Conclusion: 这项工作是首批使用LLMs进行源码级Roofline性能分类预测的研究之一，展示了在运行时剖析不可行时，LLMs指导优化工作的潜力。研究结果表明，通过更好的数据集和提示策略，LLMs可能成为HPC性能分析和性能可移植性的实用工具。

Abstract: Accurate determination of the performance of parallel GPU code typically
requires execution-time profiling on target hardware -- an increasingly
prohibitive step due to limited access to high-end GPUs. This paper explores
whether Large Language Models (LLMs) can offer an alternative approach for GPU
performance prediction without relying on hardware. We frame the problem as a
roofline classification task: given the source code of a GPU kernel and the
hardware specifications of a target GPU, can an LLM predict whether the GPU
kernel is compute-bound or bandwidth-bound?
  For this study, we build a balanced dataset of 340 GPU kernels, obtained from
HeCBench benchmark and written in CUDA and OpenMP, along with their
ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs
across four scenarios: (1) with access to profiling data of the kernel source,
(2) zero-shot with source code only, (3) few-shot with code and label pairs,
and (4) fine-tuned on a small custom dataset.
  Our results show that state-of-the-art LLMs have a strong understanding of
the Roofline model, achieving 100% classification accuracy when provided with
explicit profiling data. We also find that reasoning-capable LLMs significantly
outperform standard LLMs in zero- and few-shot settings, achieving up to 64%
accuracy on GPU source codes, without profiling information. Lastly, we find
that LLM fine-tuning will require much more data than what we currently have
available.
  This work is among the first to use LLMs for source-level roofline
performance prediction via classification, and illustrates their potential to
guide optimization efforts when runtime profiling is infeasible. Our findings
suggest that with better datasets and prompt strategies, LLMs could become
practical tools for HPC performance analysis and performance portability.

</details>


### [282] [Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving](https://arxiv.org/abs/2505.04021)
*Shan Yu,Jiarong Xing,Yifan Qiao,Mingyuan Ma,Yangmin Li,Yang Wang,Shuo Yang,Zhiqiang Xie,Shiyi Cao,Ke Bao,Ion Stoica,Harry Xu,Ying Sheng*

Main category: cs.DC

TL;DR: Prism是一个多LLM服务系统，通过创新的跨模型内存协调和动态调度策略，在GPU共享环境下显著降低成本并提高服务等级目标（SLO）达成率。


<details>
  <summary>Details</summary>
Motivation: 服务大型语言模型（LLMs）成本高昂，尤其对于托管多个模型的提供商而言。现有GPU共享系统无法在运行时动态调整资源分配和共享策略，难以在波动的工作负载下满足延迟服务等级目标（SLO）。核心问题在于缺乏跨模型内存协调能力。

Method: Prism系统通过两种关键设计实现跨模型内存协调：1. 支持按需内存分配，通过动态映射物理到虚拟内存页，实现GPU上模型间灵活的内存重新分配。2. 采用两级调度策略，根据模型的运行时需求动态调整共享策略，提高内存效率。

Result: 在真实世界追踪数据上的评估显示，与最先进的系统相比，Prism实现了超过2倍的成本节省和3.3倍的服务等级目标（SLO）达成率。

Conclusion: Prism通过有效利用GPU共享，成功地在多LLM服务场景下同时实现了成本效益和服务等级目标的达成，解决了现有系统在动态工作负载下资源分配不灵活的问题。

Abstract: Serving large language models (LLMs) is expensive, especially for providers
hosting many models, making cost reduction essential. The unique workload
patterns of serving multiple LLMs (i.e., multi-LLM serving) create new
opportunities and challenges for this task. The long-tail popularity of models
and their long idle periods present opportunities to improve utilization
through GPU sharing. However, existing GPU sharing systems lack the ability to
adjust their resource allocation and sharing policies at runtime, making them
ineffective at meeting latency service-level objectives (SLOs) under rapidly
fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full
potential of GPU sharing to achieve both cost efficiency and SLO attainment. At
its core, Prism tackles a key limitation of existing
systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$,
which is essential for flexibly sharing GPU memory across models under dynamic
workloads. Prism achieves this with two key designs. First, it supports
on-demand memory allocation by dynamically mapping physical to virtual memory
pages, allowing flexible memory redistribution among models that space- and
time-share a GPU. Second, it improves memory efficiency through a two-level
scheduling policy that dynamically adjusts sharing strategies based on models'
runtime demands. Evaluations on real-world traces show that Prism achieves more
than $2\times$ cost savings and $3.3\times$ SLO attainment compared to
state-of-the-art systems.

</details>


### [283] [MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models](https://arxiv.org/abs/2505.03906)
*Asif Rahman,Veljko Cvetkovic,Kathleen Reece,Aidan Walters,Yasir Hassan,Aneesh Tummeti,Bryan Torres,Denise Cooney,Margaret Ellis,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: MARCO是一个多智能体框架，通过反馈循环和网络搜索优化技术，增强大语言模型生成的高性能计算（HPC）代码。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在生成高性能计算（HPC）代码时，难以处理并行化、内存效率和特定架构等专门优化，且缺乏对最新优化技术的了解。

Method: 提出了MARCO框架，采用多智能体架构（代码生成智能体和性能评估智能体），通过反馈循环逐步优化代码。关键创新是集成了网络搜索组件，从最新会议和研究出版物中检索实时优化技术。

Result: 在LeetCode 75问题集上的评估显示，与单独使用Claude 3.5 Sonnet相比，MARCO平均运行时减少了14.6%；集成网络搜索组件后，MARCO系统性能比基础版提升了30.9%。

Conclusion: 多智能体系统有潜力解决高性能代码生成的专业化需求，为特定领域模型的微调提供了一种经济高效的替代方案。

Abstract: Large language models (LLMs) have transformed software development through
code generation capabilities, yet their effectiveness for high-performance
computing (HPC) remains limited. HPC code requires specialized optimizations
for parallelism, memory efficiency, and architecture-specific considerations
that general-purpose LLMs often overlook. We present MARCO (Multi-Agent
Reactive Code Optimizer), a novel framework that enhances LLM-generated code
for HPC through a specialized multi-agent architecture. MARCO employs separate
agents for code generation and performance evaluation, connected by a feedback
loop that progressively refines optimizations. A key innovation is MARCO's
web-search component that retrieves real-time optimization techniques from
recent conference proceedings and research publications, bridging the knowledge
gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem
set demonstrates that MARCO achieves a 14.6% average runtime reduction compared
to Claude 3.5 Sonnet alone, while the integration of the web-search component
yields a 30.9% performance improvement over the base MARCO system. These
results highlight the potential of multi-agent systems to address the
specialized requirements of high-performance code generation, offering a
cost-effective alternative to domain-specific model fine-tuning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [284] [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
*Yogesh Gajula*

Main category: cs.IR

TL;DR: 本文综述了2023年至2025年初在自然语言处理视角下的情感感知推荐系统，重点讨论了其在电子商务中提升预测准确性和可解释性的潜力。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台产生了大量用户反馈，但多数推荐引擎仅依赖数字评分，忽略了自由文本中丰富的细微情感意见。

Method: 本文回顾并分类了近期研究工作，主要包括四种方法：结合情感嵌入的深度学习分类器、用于细致特征提取的Transformer方法、传播情感信号的图神经网络以及实时适应用户反馈的对话式推荐器。

Result: 整合情感分析能够通过详细的意见提取来增强电子商务推荐器的预测准确性和可解释性。研究总结了模型架构，并展示了情感如何在推荐流程中流动。关键挑战包括处理嘈杂或讽刺性文本、动态用户偏好和偏见缓解。

Conclusion: 论文概述了当前研究的空白，并为开发更智能、更公平、更以用户为中心的推荐工具提供了路线图。

Abstract: E-commerce platforms generate vast volumes of user feedback, such as star
ratings, written reviews, and comments. However, most recommendation engines
rely primarily on numerical scores, often overlooking the nuanced opinions
embedded in free text. This paper comprehensively reviews sentiment-aware
recommendation systems from a natural language processing perspective, covering
advancements from 2023 to early 2025. It highlights the benefits of integrating
sentiment analysis into e-commerce recommenders to enhance prediction accuracy
and explainability through detailed opinion extraction. Our survey categorizes
recent work into four main approaches: deep learning classifiers that combine
sentiment embeddings with user item interactions, transformer based methods for
nuanced feature extraction, graph neural networks that propagate sentiment
signals, and conversational recommenders that adapt in real time to user
feedback. We summarize model architectures and demonstrate how sentiment flows
through recommendation pipelines, impacting dialogue-based suggestions. Key
challenges include handling noisy or sarcastic text, dynamic user preferences,
and bias mitigation. Finally, we outline research gaps and provide a roadmap
for developing smarter, fairer, and more user-centric recommendation tools.

</details>


### [285] [Memory Assisted LLM for Personalized Recommendation System](https://arxiv.org/abs/2505.03824)
*Jiarui Chen*

Main category: cs.IR

TL;DR: 提出了一种名为 MAP 的记忆辅助个性化大语言模型，通过构建用户历史画像并提取相关记忆融入提示，以增强推荐效果，解决了现有方法成本高、效率低或无法及时更新用户历史的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化方法在捕捉多样化用户偏好方面成本高、效率低，或者未能有效考虑用户历史的及时更新。

Method: 提出了记忆辅助个性化大语言模型（MAP）：首先为每个用户创建历史画像（包含偏好和历史项目评分等）；在推荐时，根据相似度提取相关历史记忆，并将其整合到提示（prompts）中以增强个性化推荐。

Result: 在单领域和跨领域的序贯评分预测任务中，MAP 均优于直接通过提示设计整合用户历史的常规大语言模型推荐器。并且，随着用户历史数据的增长，MAP 的优势在两种场景下都更加明显。

Conclusion: MAP 方法能有效提升大语言模型在推荐任务中的个性化能力，并且随着用户历史的积累表现更佳，因此更适合处理连续的个性化用户请求。

Abstract: Large language models (LLMs) have demonstrated significant potential in
solving recommendation tasks. With proven capabilities in understanding user
preferences, LLM personalization has emerged as a critical area for providing
tailored responses to individuals. Current studies explore personalization
through prompt design and fine-tuning, paving the way for further research in
personalized LLMs. However, existing approaches are either costly and
inefficient in capturing diverse user preferences or fail to account for timely
updates to user history. To address these gaps, we propose the Memory-Assisted
Personalized LLM (MAP). Through user interactions, we first create a history
profile for each user, capturing their preferences, such as ratings for
historical items. During recommendation, we extract relevant memory based on
similarity, which is then incorporated into the prompts to enhance personalized
recommendations. In our experiments, we evaluate MAP using a sequential rating
prediction task under two scenarios: single domain, where memory and tasks are
from the same category (e.g., movies), and cross-domain (e.g., memory from
movies and recommendation tasks in books). The results show that MAP
outperforms regular LLM-based recommenders that integrate user history directly
through prompt design. Moreover, as user history grows, MAP's advantage
increases in both scenarios, making it more suitable for addressing successive
personalized user requests.

</details>


### [286] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TL;DR: 该研究设计了一个渐进式甲骨文拓片复制件发现框架，结合低级关键点匹配和高级文本内容匹配，高效地发现了新的甲骨文复制件。


<details>
  <summary>Details</summary>
Motivation: 甲骨文复制件的识别是甲骨文研究中的一个基础性问题，现有方法可能存在不足。

Method: 设计了一个渐进式的甲骨文复制件发现框架，该框架首先使用无监督的低级关键点匹配进行初步筛选，然后结合高级的以文本为中心的内容匹配进行细化和排序，以增强语义感知和可解释性。

Result: 该方法在召回率上与现有先进方法相当，但在Top-5和Top-15检索结果的简化平均倒数排名分数上表现最佳，并且计算效率显著提高。实际应用中发现了超过60对以往被研究者遗漏的新甲骨文复制件。

Conclusion: 提出的框架能够有效且高效地发现甲骨文复制件，并在实际研究中取得了重要的新发现，证明了其在甲骨文研究领域的实用价值。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>


### [287] [CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation](https://arxiv.org/abs/2505.03840)
*Cairong Yan,Jinyi Han,Jin Ju,Yanting Zhang,Zijian Wang,Xuan Shao*

Main category: cs.IR

TL;DR: 提出了一种名为 CoCoB 的自适应协同组合老虎机算法，通过双边老虎机架构改进推荐系统中用户相似性定义和独特偏好用户推荐效果的问题。


<details>
  <summary>Details</summary>
Motivation: 现有聚类老虎机方法在推荐系统中对相似用户定义不清，且当用户偏好独特、缺乏合适邻居时，依赖错误识别的邻居会降低推荐质量。

Method: 提出了 CoCoB 算法，采用双边老虎机架构（用户侧和物品侧）。用户老虎机使用增强贝叶斯模型探索用户相似性，并基于相似性概率阈值识别邻居。物品老虎机将物品视为臂，根据用户老虎机的输出生成推荐。算法可动态适应，有邻居时利用其偏好，否则仅关注目标用户。

Result: 在线性上下文老虎机设置下进行了遗憾分析，并在三个真实世界数据集上进行实验，结果表明 CoCoB 相比现有SOTA方法，F1分数平均提高了2.4%。

Conclusion: CoCoB 算法能够有效解决现有方法的局限性，通过自适应机制和双边老虎机架构提升了推荐系统的性能，尤其在处理用户相似性和独特偏好方面表现更优。

Abstract: Clustering bandits have gained significant attention in recommender systems
by leveraging collaborative information from neighboring users to better
capture target user preferences. However, these methods often lack a clear
definition of similar users and face challenges when users with unique
preferences lack appropriate neighbors. In such cases, relying on divergent
preferences of misidentified neighbors can degrade recommendation quality. To
address these limitations, this paper proposes an adaptive Collaborative
Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided
bandit architecture, applying bandit principles to both the user and item
sides. The user-bandit employs an enhanced Bayesian model to explore user
similarity, identifying neighbors based on a similarity probability threshold.
The item-bandit treats items as arms, generating diverse recommendations
informed by the user-bandit's output. CoCoB dynamically adapts, leveraging
neighbor preferences when available or focusing solely on the target user
otherwise. Regret analysis under a linear contextual bandit setting and
experiments on three real-world datasets demonstrate CoCoB's effectiveness,
achieving an average 2.4% improvement in F1 score over state-of-the-art
methods.

</details>


### [288] [To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay](https://arxiv.org/abs/2505.04209)
*Soumik Dey,Hansi Wu,Binbin Li*

Main category: cs.IR

TL;DR: 研究提出使用LLM作为卖家判断的代理来训练电商广告关键词相关性模型，以提升推荐效果和系统协调性。


<details>
  <summary>Details</summary>
Motivation: 现有基于点击/销售/搜索信号训练的广告关键词相关性模型未能充分对齐人类（卖家）判断，影响卖家对推荐关键词的采纳率和广告系统整体健康度。

Method: 将广告关键词相关性视为卖家判断、广告系统和搜索系统三者间的复杂互动。通过eBay广告案例研究，提出利用“LLM作为法官”大规模地作为卖家判断的可扩展代理，来训练相关性模型，并强调需结合基于业务指标的严谨评估框架。

Result: 使用LLM作为卖家判断的代理来训练相关性模型，能够在卖家判断、广告系统和搜索系统之间实现更好的协调。

Conclusion: 在基于业务指标的严谨评估框架的约束下，利用LLM作为卖家判断的规模化代理来训练关键词相关性模型，可以有效地在卖家、广告和搜索三个系统间达成更好的和谐。

Abstract: E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). The relevance
of advertiser keyphrases plays an important role in preventing the inundation
of search systems with numerous irrelevant items that compete for attention in
auctions, in addition to maintaining a healthy seller perception. In this work,
we describe the shortcomings of training Advertiser keyphrase relevance filter
models on click/sales/search relevance signals and the importance of aligning
with human judgment, as sellers have the power to adopt or reject said
keyphrase recommendations. In this study, we frame Advertiser keyphrase
relevance as a complex interaction between 3 dynamical systems -- seller
judgment, which influences seller adoption of our product, Advertising, which
provides the keyphrases to bid on, and Search, who holds the auctions for the
same keyphrases. This study discusses the practicalities of using human
judgment via a case study at eBay Advertising and demonstrate that using
LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our
relevance models achieves a better harmony across the three systems -- provided
that they are bound by a meticulous evaluation framework grounded in business
metrics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [289] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TL;DR: 提出了一种启发式集成的深度强化学习框架，用于优化大规模可重构智能表面（RIS）的离散相移。


<details>
  <summary>Details</summary>
Motivation: 大规模可重构智能表面（RIS）中离散相移的优化因其非凸和非线性特性而具有挑战性。

Method: 该框架采用双深度Q网络（DDQN）进行RIS列向控制（利用多步累积动作），并在每个深度强化学习（DRL）步骤中集成贪婪算法（GA）以进行单元级精细优化来改善状态。

Result: 通过从包含贪婪算法优化的状态中学习，该方法能够在较小的DRL动作空间内有效解决RIS优化问题，并展示了其优化大规模RIS相移配置的能力。

Conclusion: 所提出的启发式集成DRL框架为大规模RIS的离散相移优化提供了一种有效的方法。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [290] [TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models](https://arxiv.org/abs/2505.04050)
*Kazuki Higo,Toshiki Kanai,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 提出了一种使用潜在扩散模型联合生成地形高度图和纹理的方法，并支持用户草图输入进行控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多分别生成高度图或纹理，未能充分考虑两者之间的固有相关性，影响了生成地形的真实感。

Method: 采用潜在扩散模型。首先进行无监督训练，使模型能随机生成配对的高度图和纹理。然后，通过监督学习训练一个外部适配器，以实现用户通过手绘草图进行控制。

Result: 实验表明，该方法能够实现直观的地形生成，同时保持了高度图和纹理之间的相关性。

Conclusion: 该方法通过联合生成和用户草图控制，有效地解决了地形高度图与纹理相关性的问题，能够生成更真实且可控的3D地形模型。

Abstract: 3D terrain models are essential in fields such as video game development and
film production. Since surface color often correlates with terrain geometry,
capturing this relationship is crucial to achieving realism. However, most
existing methods generate either a heightmap or a texture, without sufficiently
accounting for the inherent correlation. In this paper, we propose a method
that jointly generates terrain heightmaps and textures using a latent diffusion
model. First, we train the model in an unsupervised manner to randomly generate
paired heightmaps and textures. Then, we perform supervised learning of an
external adapter to enable user control via hand-drawn sketches. Experiments
show that our approach allows intuitive terrain generation while preserving the
correlation between heightmaps and textures.

</details>


### [291] [Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control](https://arxiv.org/abs/2505.04052)
*Shun Masuda,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 该研究提出两种新方法，利用3D身体模型和潜在扩散模型，将人物以可控姿态自然地合成到场景图像中，有效处理遮挡问题，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有将人物图像合成到场景中的方法，在处理前景物体对人物的遮挡方面存在不足，常将人物置于最前景，且对人物姿态的控制能力有限。

Method: 提出了两种方法：均使用3D身体模型控制姿态，并利用潜在扩散模型在合适的深度合成人物，无需遮挡掩码即可处理遮挡。第一种方法分两阶段，先通过监督学习获取带人物的场景深度图，然后据此合成人物；第二种方法隐式学习遮挡关系，直接从输入数据合成人物，无需显式深度监督。

Result: 定量和定性评估表明，所提出的两种方法在保持场景一致性、准确反映遮挡关系和用户指定的姿态方面均优于现有方法。

Conclusion: 本文提出的两种方法能够更自然、更可控地将人物合成到场景中，有效解决了现有方法在遮挡处理和姿态控制方面的局限性。

Abstract: Compositing human figures into scene images has broad applications in areas
such as entertainment and advertising. However, existing methods often cannot
handle occlusion of the inserted person by foreground objects and unnaturally
place the person in the frontmost layer. Moreover, they offer limited control
over the inserted person's pose. To address these challenges, we propose two
methods. Both allow explicit pose control via a 3D body model and leverage
latent diffusion models to synthesize the person at a contextually appropriate
depth, naturally handling occlusions without requiring occlusion masks. The
first is a two-stage approach: the model first learns a depth map of the scene
with the person through supervised learning, and then synthesizes the person
accordingly. The second method learns occlusion implicitly and synthesizes the
person directly from input data without explicit depth supervision.
Quantitative and qualitative evaluations show that both methods outperform
existing approaches by better preserving scene consistency while accurately
reflecting occlusions and user-specified poses.

</details>


### [292] [Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control](https://arxiv.org/abs/2505.04387)
*Amin Fadaeinejad,Abdallah Dib,Luiz Gustavo Hafemann,Emeline Got,Trevor Anderson,Amaury Depierre,Nikolaus F. Troje,Marcus A. Brubaker,Marc-André Carbonneau*

Main category: cs.GR

TL;DR: 提出了一种新颖的框架，通过几何感知的纹理合成管线，为艺术家提供对生成的3D头部的直观控制，简化了逼真3D头部资产的创建过程。


<details>
  <summary>Details</summary>
Motivation: 为虚拟角色创建符合精确艺术构想的逼真3D头部资产仍然是一项劳动密集型的工作。

Method: 采用一种几何感知的纹理合成管线，学习不同人群头部几何形状与皮肤纹理贴图之间的相关性。该框架提供三个层次的艺术控制：整体头部几何操作、肤色调整（同时保留面部特征）以及皱纹或面部毛发等细节的精细编辑。艺术家编辑单个纹理贴图后，系统会自动将更改一致地传播到其他所需纹理贴图。

Result: 实验表明，该方法能够产生具有清晰几何形状的多样化结果。展示了其在艺术家直观控制方面的实际应用，如肤色调整、添加年龄相关细节或移除扫描模型中不需要的特征等简化编辑流程。

Conclusion: 这种集成方法旨在简化虚拟角色创建中的艺术工作流程。

Abstract: Creating realistic 3D head assets for virtual characters that match a precise
artistic vision remains labor-intensive. We present a novel framework that
streamlines this process by providing artists with intuitive control over
generated 3D heads. Our approach uses a geometry-aware texture synthesis
pipeline that learns correlations between head geometry and skin texture maps
across different demographics. The framework offers three levels of artistic
control: manipulation of overall head geometry, adjustment of skin tone while
preserving facial characteristics, and fine-grained editing of details such as
wrinkles or facial hair. Our pipeline allows artists to make edits to a single
texture map using familiar tools, with our system automatically propagating
these changes coherently across the remaining texture maps needed for realistic
rendering. Experiments demonstrate that our method produces diverse results
with clean geometries. We showcase practical applications focusing on intuitive
control for artists, including skin tone adjustments and simplified editing
workflows for adding age-related details or removing unwanted features from
scanned models. This integrated approach aims to streamline the artistic
workflow in virtual character creation.

</details>


### [293] [PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers](https://arxiv.org/abs/2505.04002)
*Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng*

Main category: cs.GR

TL;DR: PARC是一个利用机器学习和物理模拟来迭代增强运动数据集的框架，旨在扩展角色控制器在复杂地形中的敏捷穿越能力。


<details>
  <summary>Details</summary>
Motivation: 由于敏捷地形穿越行为的动作捕捉数据稀缺且获取成本高昂，在模拟角色中复现这些敏捷运动具有挑战性。

Method: PARC首先在小型核心技能数据集上训练一个运动生成器，然后用它为新地形生成合成数据。接着，训练一个基于物理的跟踪控制器来模仿并纠正生成运动中的瑕疵（如错误接触或不连续性）。纠正后的运动被添加回数据集中，用于下一轮迭代训练运动生成器。

Result: PARC的迭代过程共同扩展了运动生成器和跟踪器的能力，从而创建了能够与复杂环境进行敏捷、多功能交互的模型。

Conclusion: PARC提供了一种开发敏捷地形穿越控制器的有效方法，解决了运动数据稀缺与对多功能角色控制器需求之间的矛盾。

Abstract: Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.

</details>


### [294] [TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization](https://arxiv.org/abs/2505.04590)
*Alexandre Binninger,Ruben Wiersma,Philipp Herholz,Olga Sorkine-Hornung*

Main category: cs.GR

TL;DR: 介绍了一种名为 TetWeave 的新型等值面表示方法，用于基于梯度的网格优化，它能实时构建四面体网格并联合优化网格点和方向性有符号距离。


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义网格的等值面提取方法在灵活性和内存效率方面存在不足，难以生成高质量的自适应网格。

Method: TetWeave 通过基于梯度的优化方法，实时利用 Delaunay 三角剖分构建四面体网格，并联合优化网格点的位置和一种新颖的方向性有符号距离。该方法还包含一个重采样策略，用于在高重建误差区域放置新点，并鼓励网格的平滑性。

Result: 该方法能够生成高质量、自适应的网格，保证水密、双流形且无自相交。与预定义网格相比，TetWeave 在输出网格顶点数量方面表现出近线性的内存缩放，内存占用极小且优化参数少。它已成功应用于多视角三维重建、网格压缩和几何纹理生成等任务。

Conclusion: TetWeave 是一种新颖的等值面表示方法，通过其灵活的网格构建和联合优化策略，能够生成高质量、内存高效的自适应网格，为计算机图形学和视觉领域的多种挑战性任务提供了有效的解决方案。

Abstract: We introduce TetWeave, a novel isosurface representation for gradient-based
mesh optimization that jointly optimizes the placement of a tetrahedral grid
used for Marching Tetrahedra and a novel directional signed distance at each
point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay
triangulation, enabling increased flexibility compared to predefined grids. The
extracted meshes are guaranteed to be watertight, two-manifold and
intersection-free. The flexibility of TetWeave enables a resampling strategy
that places new points where reconstruction error is high and allows to
encourage mesh fairness without compromising on reconstruction error. This
leads to high-quality, adaptive meshes that require minimal memory usage and
few parameters to optimize. Consequently, TetWeave exhibits near-linear memory
scaling relative to the vertex count of the output mesh - a substantial
improvement over predefined grids. We demonstrate the applicability of TetWeave
to a broad range of challenging tasks in computer graphics and vision, such as
multi-view 3D reconstruction, mesh compression and geometric texture
generation.

</details>


### [295] [PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer](https://arxiv.org/abs/2505.04622)
*Jingwen Ye,Yuze He,Yanning Zhou,Yiqin Zhu,Kaiwen Xiao,Yong-Jin Liu,Wei Yang,Xiao Han*

Main category: cs.GR

TL;DR: PrimitiveAnything是一个将形状基元抽象重新定义为基元装配生成任务的新框架，它能从大规模人工抽象中学习，生成更符合人类感知的高质量3D形状基元分解。


<details>
  <summary>Details</summary>
Motivation: 现有的形状基元抽象方法要么依赖于几何优化但语义理解有限，要么从小规模、特定类别的数据集学习，难以推广到多样的形状类别。

Method: 提出了PrimitiveAnything框架，将形状基元抽象重新表述为基元装配生成任务。该框架包含一个用于自回归生成的形状条件基元转换器 (shape-conditioned primitive transformer) 和一个统一表示多种基元类型的无歧义参数化方案。它直接从大规模人工构建的抽象中学习基元装配过程。

Result: 实验表明，PrimitiveAnything能够生成高质量的基元装配，这些装配在不同形状类别中更好地与人类感知对齐，同时保持了几何保真度。

Conclusion: 该框架有益于多种3D应用，并显示出在游戏中实现基于基元的用户生成内容 (UGC) 的潜力。

Abstract: Shape primitive abstraction, which decomposes complex 3D shapes into simple
geometric elements, plays a crucial role in human visual cognition and has
broad applications in computer vision and graphics. While recent advances in 3D
content generation have shown remarkable progress, existing primitive
abstraction methods either rely on geometric optimization with limited semantic
understanding or learn from small-scale, category-specific datasets, struggling
to generalize across diverse shape categories. We present PrimitiveAnything, a
novel framework that reformulates shape primitive abstraction as a primitive
assembly generation task. PrimitiveAnything includes a shape-conditioned
primitive transformer for auto-regressive generation and an ambiguity-free
parameterization scheme to represent multiple types of primitives in a unified
manner. The proposed framework directly learns the process of primitive
assembly from large-scale human-crafted abstractions, enabling it to capture
how humans decompose complex shapes into primitive elements. Through extensive
experiments, we demonstrate that PrimitiveAnything can generate high-quality
primitive assemblies that better align with human perception while maintaining
geometric fidelity across diverse shape categories. It benefits various 3D
applications and shows potential for enabling primitive-based user-generated
content (UGC) in games. Project page: https://primitiveanything.github.io

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [296] [The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea](https://arxiv.org/abs/2505.03835)
*Simon Suh,Jihyuk Bang,Ji Woo Han*

Main category: cs.DL

TL;DR: 该研究分析了2015-2024年间，COVID-19和ChatGPT发布这两大全球颠覆性事件对美国、欧洲和韩国人工智能政策研究中预印本引用区域趋势的影响。


<details>
  <summary>Details</summary>
Motivation: 开放科学迅速改变了人工智能政策研究的全球传播方式，本研究旨在探究全球颠覆性事件如何影响不同地区在人工智能政策研究中对预印本的采纳。

Method: 研究使用了Web of Science的文献计量数据，追踪了2015年至2024年美国、欧洲和韩国人工智能政策研究中预印本的引用情况，并标记了COVID-19大流行和ChatGPT发布的时间点进行分析。

Result: 所有地区预印本引用量均有增长，但幅度和轨迹存在显著差异：美国呈现事件驱动的急剧增长；欧洲表现出制度性增长；韩国则保持了持续、线性的预印本采纳增长。

Conclusion: 全球性颠覆事件可能加速了预印本的采纳，但其程度和发展轨迹受到当地研究文化、政策环境和开放科学成熟度的影响。未来的人工智能治理策略需考虑研究传播的区域差异性。

Abstract: The adoption of open science has quickly changed how artificial intelligence
(AI) policy research is distributed globally. This study examines the regional
trends in the citation of preprints, specifically focusing on the impact of two
major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on
research dissemination patterns in the United States, Europe, and South Korea
from 2015 to 2024. Using bibliometrics data from the Web of Science, this study
tracks how global disruptive events influenced the adoption of preprints in AI
policy research and how such shifts vary by region. By marking the timing of
these disruptive events, the analysis reveals that while all regions
experienced growth in preprint citations, the magnitude and trajectory of
change varied significantly. The United States exhibited sharp, event-driven
increases; Europe demonstrated institutional growth; and South Korea maintained
consistent, linear growth in preprint adoption. These findings suggest that
global disruptions may have accelerated preprint adoption, but the extent and
trajectory are shaped by local research cultures, policy environments, and
levels of open science maturity. This paper emphasizes the need for future AI
governance strategies to consider regional variability in research
dissemination and highlights opportunities for further longitudinal and
comparative research to deepen our understanding of open-access adoption in AI
policy development.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [297] [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://arxiv.org/abs/2505.03853)
*Changxi Chi,Jun Xia,Jingbo Zhou,Jiabei Cheng,Chang Yu,Stan Z. Li*

Main category: q-bio.QM

TL;DR: 该研究提出了一种名为GRAPE的新型异构图神经网络，通过整合基因描述、DNA序列和生物型信息，并利用图结构学习来预测基因扰动，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的基因扰动预测方法未能充分利用基因相关信息（如描述、序列），构建的基因调控网络（GRN）较为粗糙，并且忽略了不同基因生物型（biotypes）之间的功能差异，这限制了捕获潜在基因相互作用和准确预测扰动效应的能力。

Method: 1. 利用预训练的大型语言模型和DNA序列模型分别从基因描述和DNA序列数据中提取特征，作为基因表示的初始化。 2. 首次在基因扰动研究中引入基因生物型信息，以模拟不同生物型基因在细胞过程中的独特作用。 3. 通过图结构学习（GSL）动态捕获和优化隐含的基因间关系。 4. 提出GRAPE，一种异构图神经网络（HGNN），该网络整合上述基因表示、生物型信息，并通过GSL动态优化GRN。

Result: 在公开数据集上的实验结果表明，所提出的GRAPE方法在基因扰动预测方面取得了当前最佳（state-of-the-art）的性能。

Conclusion: GRAPE模型通过有效整合多源基因信息（描述、序列、生物型）并利用异构图神经网络和图结构学习动态优化基因调控网络，显著提升了基因扰动预测的准确性，为实验前识别关键基因提供了更有效的计算工具。

Abstract: Predicting genetic perturbations enables the identification of potentially
crucial genes prior to wet-lab experiments, significantly improving overall
experimental efficiency. Since genes are the foundation of cellular life,
building gene regulatory networks (GRN) is essential to understand and predict
the effects of genetic perturbations. However, current methods fail to fully
leverage gene-related information, and solely rely on simple evaluation metrics
to construct coarse-grained GRN. More importantly, they ignore functional
differences between biotypes, limiting the ability to capture potential gene
interactions. In this work, we leverage pre-trained large language model and
DNA sequence model to extract features from gene descriptions and DNA sequence
data, respectively, which serve as the initialization for gene representations.
Additionally, we introduce gene biotype information for the first time in
genetic perturbation, simulating the distinct roles of genes with different
biotypes in regulating cellular processes, while capturing implicit gene
relationships through graph structure learning (GSL). We propose GRAPE, a
heterogeneous graph neural network (HGNN) that leverages gene representations
initialized with features from descriptions and sequences, models the distinct
roles of genes with different biotypes, and dynamically refines the GRN through
GSL. The results on publicly available datasets show that our method achieves
state-of-the-art performance.

</details>


### [298] [Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning](https://arxiv.org/abs/2505.04300)
*Isabella Caranzano,Corrado Pancotti,Cesare Rollo,Flavio Sartori,Pietro Liò,Piero Fariselli,Tiziana Sanavia*

Main category: q-bio.QM

TL;DR: 研究发现，生物通路信息对神经网络性能的提升可能源于其引入的稀疏性，而非生物学相关性本身；随机化通路信息的模型表现与生物通路模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 探究生物通路整合到神经网络中提升性能的真正原因，质疑其是否源于生物学相关性，并假设其益处可能来自引入的稀疏性。

Method: 对基于通路的神经网络预测模型进行全面分析和评估，选取有公开源码的模型，将其与对应的通路信息随机化版本进行性能和可解释性比较。

Result: 基于随机化信息的模型与基于生物学信息的模型在不同指标和数据集上表现相当，部分随机化模型甚至优于生物学模型。通路信息模型在可解释性方面也未显示明显优势，随机化模型仍能识别疾病生物标志物。

Conclusion: 当前的通路注释可能噪音过大或未被现有方法充分利用。论文提出一种基准测试方法，通过与随机化对应模型比较，来严格评估新的通路信息模型性能提升是否真正源于生物学见解。

Abstract: Biologically-informed neural networks typically leverage pathway annotations
to enhance performance in biomedical applications. We hypothesized that the
benefits of pathway integration does not arise from its biological relevance,
but rather from the sparsity it introduces. We conducted a comprehensive
analysis of all relevant pathway-based neural network models for predictive
tasks, critically evaluating each study's contributions. From this review, we
curated a subset of methods for which the source code was publicly available.
The comparison of the biologically informed state-of-the-art deep learning
models and their randomized counterparts showed that models based on randomized
information performed equally well as biologically informed ones across
different metrics and datasets. Notably, in 3 out of the 15 analyzed models,
the randomized versions even outperformed their biologically informed
counterparts. Moreover, pathway-informed models did not show any clear
advantage in interpretability, as randomized models were still able to identify
relevant disease biomarkers despite lacking explicit pathway information. Our
findings suggest that pathway annotations may be too noisy or inadequately
explored by current methods. Therefore, we propose a methodology that can be
applied to different domains and can serve as a robust benchmark for
systematically comparing novel pathway-informed models against their randomized
counterparts. This approach enables researchers to rigorously determine whether
observed performance improvements can be attributed to biological insights.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [299] [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
*Ganghua Wang,Zhaorun Chen,Bo Li,Haifeng Xu*

Main category: stat.ML

TL;DR: 本文提出了一种名为Cer-Eval的可认证且具有成本效益的大型语言模型（LLM）评估框架，该框架通过自适应选择测试样本，在保持评估准确性和提供置信保证的同时，节省了20-40%的测试点。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模的指数级增长，LLM的评估面临巨大挑战。现有评估方法缺乏关于测试数据充分性和信息样本选择的系统性指导，导致评估成本高昂。

Method: 研究提出了一种可认证且成本效益高的评估框架。该框架使用“测试样本复杂度”来量化所需测试点，并推导出其紧密界限。基于此理论，开发了名为Cer-Eval的基于分区的算法，该算法能够自适应地选择测试点以最小化评估成本，并输出包含真实值的高概率置信区间。

Result: 在真实世界的实验中，Cer-Eval能够在各种基准测试中节省20%至40%的测试点，同时保持与当前评估过程相当的估计误差水平，并提供95%的置信保证。

Conclusion: Cer-Eval框架为LLM提供了一种有效的评估方法，它显著降低了评估成本，同时保证了评估结果的可靠性和可认证性。

Abstract: As foundation models continue to scale, the size of trained models grows
exponentially, presenting significant challenges for their evaluation. Current
evaluation practices involve curating increasingly large datasets to assess the
performance of large language models (LLMs). However, there is a lack of
systematic analysis and guidance on determining the sufficiency of test data or
selecting informative samples for evaluation. This paper introduces a
certifiable and cost-efficient evaluation framework for LLMs. Our framework
adapts to different evaluation objectives and outputs confidence intervals that
contain true values with high probability. We use ``test sample complexity'' to
quantify the number of test points needed for a certifiable evaluation and
derive tight bounds on test sample complexity. Based on the developed theory,
we develop a partition-based algorithm, named Cer-Eval, that adaptively selects
test points to minimize the cost of LLM evaluation. Real-world experiments
demonstrate that Cer-Eval can save 20% to 40% test points across various
benchmarks, while maintaining an estimation error level comparable to the
current evaluation process and providing a 95% confidence guarantee.

</details>


### [300] [Categorical and geometric methods in statistical, manifold, and machine learning](https://arxiv.org/abs/2505.03862)
*Hông Vân Lê,Hà Quang Minh,Frederic Protin,Wilderich Tuschmann*

Main category: stat.ML

TL;DR: 本文讨论了概率态射范畴及其几何方法在统计、机器和流形学习问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 展示最初在 [Le2023] 中发展的概率态射范畴以及一些几何方法在多个学习问题类别中的应用潜力。

Method: 应用概率态射范畴和一些几何方法进行分析和讨论。

Result: 论文介绍了这些方法在统计、机器和流形学习中的应用案例，并指出这些内容以及许多其他主题将在即将出版的著作 [LMPT2024] 中得到深入探讨。

Conclusion: 概率态射范畴和几何方法为处理统计、机器和流形学习中的各类问题提供了有前景的工具，其更全面的内容将在后续出版物中呈现。

Abstract: We present and discuss applications of the category of probabilistic
morphisms, initially developed in \cite{Le2023}, as well as some geometric
methods to several classes of problems in statistical, machine and manifold
learning which shall be, along with many other topics, considered in depth in
the forthcoming book \cite{LMPT2024}.

</details>


### [301] [Variational Formulation of the Particle Flow Particle Filter](https://arxiv.org/abs/2505.04007)
*Yinzhuang Yi,Jorge Cortés,Nikolay Atanasov*

Main category: stat.ML

TL;DR: 论文从变分推断的角度阐述了粒子流粒子滤波器，并揭示其与Fisher-Rao梯度流的联系。


<details>
  <summary>Details</summary>
Motivation: 为粒子流粒子滤波器提供一种基于变分推断的理论解释和数学表述。

Method: 通过变分推断的框架，将粒子流粒子滤波器中使用的瞬态密度与Fisher-Rao梯度流联系起来。该梯度流被视为一种用于最小化KL散度的连续时间变分推断算法。

Result: 证明了用于推导粒子流粒子滤波器的瞬态密度遵循概率密度空间中Fisher-Rao梯度流的时间尺度轨迹。

Conclusion: 该研究成功地从变分推断的视角为粒子流粒子滤波器提供了一种新的公式化表述，深化了对该滤波器背后机制的理解。

Abstract: This paper provides a formulation of the particle flow particle filter from
the perspective of variational inference. We show that the transient density
used to derive the particle flow particle filter follows a time-scaled
trajectory of the Fisher-Rao gradient flow in the space of probability
densities. The Fisher-Rao gradient flow is obtained as a continuous-time
algorithm for variational inference, minimizing the Kullback-Leibler divergence
between a variational density and the true posterior density.

</details>


### [302] [A Tutorial on Discriminative Clustering and Mutual Information](https://arxiv.org/abs/2505.04484)
*Louis Ohl,Pierre-Alexandre Mattei,Frédéric Precioso*

Main category: stat.ML

TL;DR: 本文综述了判别式聚类方法的演进，聚焦其模型假设从决策边界到不变性准则的变化，强调了互信息的关键作用、局限性及簇数选择的挑战，并介绍了相关的Python包GemClus。


<details>
  <summary>Details</summary>
Motivation: 鉴于深度判别式聚类方法发展迅速，本文旨在提供一个关于此类方法演变（特别是其模型假设如何随时间变化）的易懂历史视角。

Method: 通过历史回顾与分析，追溯判别式聚类模型假设（从决策边界到不变性准则）的演变，探讨互信息在其中的核心作用、已知局限性以及相应的规避尝试，并讨论了簇数量选择的挑战。

Result: 论文阐明了判别式聚类模型假设的演变路径，强调了互信息是（深度）判别式聚类方法进步的历史基石，指出了互信息的局限性以及判别式聚类方法尝试规避这些局限的努力，讨论了判别式聚类在选择簇数量方面面临的挑战，并开发和展示了一个专用于判别式聚类的Python包GemClus。

Conclusion: 判别式聚类方法在其模型假设（从决策边界到不变性准则）上经历了显著演变，互信息是其发展的核心要素但也存在局限。尽管取得了进展，但在簇数量选择等问题上仍存在挑战。GemClus包为实践这些技术提供了工具支持。

Abstract: To cluster data is to separate samples into distinctive groups that should
ideally have some cohesive properties. Today, numerous clustering algorithms
exist, and their differences lie essentially in what can be perceived as
``cohesive properties''. Therefore, hypotheses on the nature of clusters must
be set: they can be either generative or discriminative. As the last decade
witnessed the impressive growth of deep clustering methods that involve neural
networks to handle high-dimensional data often in a discriminative manner; we
concentrate mainly on the discriminative hypotheses. In this paper, our aim is
to provide an accessible historical perspective on the evolution of
discriminative clustering methods and notably how the nature of assumptions of
the discriminative models changed over time: from decision boundaries to
invariance critics. We notably highlight how mutual information has been a
historical cornerstone of the progress of (deep) discriminative clustering
methods. We also show some known limitations of mutual information and how
discriminative clustering methods tried to circumvent those. We then discuss
the challenges that discriminative clustering faces with respect to the
selection of the number of clusters. Finally, we showcase these techniques
using the dedicated Python package, GemClus, that we have developed for
discriminative clustering.

</details>


### [303] [From Two Sample Testing to Singular Gaussian Discrimination](https://arxiv.org/abs/2505.04613)
*Leonardo V. Santoro,Kartik G. Waghmare,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 该研究证明，检验两个概率测度的相等性等价于检验它们在再生核希尔伯特空间中对应高斯测度的奇异性，后者在高维情况下更简单。


<details>
  <summary>Details</summary>
Motivation: 在信息论角度，尤其是在高维设置下，辨别两个奇异高斯分布比进行非参数双样本检验更为简单，研究旨在利用这一点简化分布比较问题。

Method: 通过概率测度的核均值和协方差嵌入定义相应的高斯测度，并利用希尔伯特空间上高斯分布奇异性/等价性的Feldman-Hajek准则来证明这种等价性。

Result: 研究成功建立了检验两个概率测度相等性与检验其在相应再生核希尔伯特空间（RKHS）中高斯嵌入奇异性之间的等价关系。在总体层面上，不同的概率测度会导致其高斯嵌入在本质上是分离的，分布间的差异通过高斯嵌入被显著放大。

Conclusion: 该发现揭示了一种新的“维度祝福”现象，并有潜力被广泛应用于设计高效的推断工具。

Abstract: We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.

</details>
