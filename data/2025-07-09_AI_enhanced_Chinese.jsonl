{"id": "2507.05261", "pdf": "https://arxiv.org/pdf/2507.05261", "abs": "https://arxiv.org/abs/2507.05261", "authors": ["Yingtai Xiao", "Yuqing Zhu", "Sirat Samyoun", "Wanrong Zhang", "Jiachen T. Wang", "Jian Du"], "title": "TokenShapley: Token Level Context Attribution with Shapley Value", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.", "AI": {"tldr": "\u63d0\u51faTokenShapley\uff0c\u4e00\u79cd\u7ed3\u5408Shapley\u503c\u548cKNN\u68c0\u7d22\u7684token\u7ea7\u5f52\u56e0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60c5\u5883\u5b66\u4e60\u65b9\u9762\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u751f\u6210\u56de\u590d\u7684\u6b63\u786e\u6027\u96be\u4ee5\u9a8c\u8bc1\u3002\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u505c\u7559\u5728\u53e5\u5b50\u7ea7\u522b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u5bf9\u56de\u590d\u4e2d\u7279\u5b9a\u5173\u952e\u8bcd\uff08\u5982\u6570\u5b57\u3001\u5e74\u4efd\u3001\u540d\u79f0\uff09\u8fdb\u884c\u7cbe\u7ec6\u5f52\u56e0\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faTokenShapley\uff0c\u4e00\u79cd\u65b0\u9896\u7684token\u7ea7\u5f52\u56e0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8eShapley\u503c\u7684\u6570\u636e\u5f52\u56e0\u4e0e\u53d7KNN\u589e\u5f3aLLMs\u542f\u53d1\u7684KNN\u68c0\u7d22\u6280\u672f\u3002\u901a\u8fc7\u5229\u7528\u9884\u8ba1\u7b97\u7684\u6570\u636e\u5b58\u50a8\u8fdb\u884c\u4e0a\u4e0b\u6587\u68c0\u7d22\u5e76\u8ba1\u7b97Shapley\u503c\u6765\u91cf\u5316token\u7684\u91cd\u8981\u6027\uff0cTokenShapley\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aTokenShapley\u5728token\u7ea7\u5f52\u56e0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e8611-23%\u3002", "conclusion": "TokenShapley\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u751f\u6210\u5185\u5bb9\u4e2d\u5173\u952e\u8bcd\u7684\u7ec6\u7c92\u5ea6\u5f52\u56e0\u96be\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5f52\u56e0\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9a8c\u8bc1LLM\u56de\u590d\u6b63\u786e\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.05266", "pdf": "https://arxiv.org/pdf/2507.05266", "abs": "https://arxiv.org/abs/2507.05266", "authors": ["Sougata Saha", "Monojit Choudhury"], "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama.", "AI": {"tldr": "\u7531\u4e8e\u6570\u636e\u6c61\u67d3\uff0c\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u4f5c\u4e3a\u4e00\u79cd\u7406\u8bba\u4e0a\u5408\u7406\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7535\u5f71\u548c\u97f3\u4e50\u63a8\u8350\u6570\u636e\u96c6\u6d4b\u8bd5\u4e86GPT-4o\u3001GPT-4o-mini\u548cLlama-3.1-8B-Instruct\uff0c\u7ed3\u679c\u663e\u793aGPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u6c61\u67d3\uff0c\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\u975e\u5e38\u56f0\u96be\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\u548c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0c\u786e\u4fdd\u8bad\u7ec3\u9636\u6bb5\u4efb\u52a1\u548c\u6d4b\u8bd5\u7528\u4f8b\u672a\u88ab\u89c1\u8fc7\u5c06\u53d8\u5f97\u51e0\u4e4e\u4e0d\u53ef\u80fd\u3002\u77e5\u8bc6\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u4e0d\u9002\u5408\u8861\u91cf\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u4e3aLLMs\u5e76\u975e\u4e3a\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7406\u8bba\u4e0a\u5408\u7406\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6cd5\u6765\u8861\u91cf\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u4e2d\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u662f\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u6765\u8861\u91cfLLMs\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u7535\u5f71\u548c\u97f3\u4e50\u63a8\u8350\u6570\u636e\u96c6\uff0c\u5e76\u5bf9GPT-4o\u3001GPT-4o-mini\u548cLlama-3.1-8B-Instruct\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u6240\u63d0\u6846\u67b6\u7684\u9884\u6d4b\u4e00\u81f4\u3002GPT-4o\u7684\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u548cLlama\u6a21\u578b\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\uff08\u5c24\u5176\u662fLlama\uff09\u5728\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u662f\u4e00\u4e2a\u7406\u8bba\u4e0a\u5408\u7406\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5f53\u524dLLMs\u5728\u8be5\u9886\u57df\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u80fd\u529b\uff0cGPT-4o\u8868\u73b0\u51fa\u76f8\u5bf9\u4f18\u52bf\uff0c\u4f46\u6574\u4f53\u4ecd\u6709\u663e\u8457\u7684\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2507.05271", "pdf": "https://arxiv.org/pdf/2507.05271", "abs": "https://arxiv.org/abs/2507.05271", "authors": ["Mohammad Zia Ur Rehman", "Aditya Shah", "Nagendra Kumar"], "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks", "categories": ["cs.CL"], "comment": null, "summary": "The global reach of social media has amplified the spread of hateful content,\nincluding implicit sexism, which is often overlooked by conventional detection\nmethods. In this work, we introduce an Adaptive Supervised Contrastive lEarning\nframework for implicit sexism detectioN (ASCEND). A key innovation of our\nmethod is the incorporation of threshold-based contrastive learning: by\ncomputing cosine similarities between embeddings, we selectively treat only\nthose sample pairs as positive if their similarity exceeds a learnable\nthreshold. This mechanism refines the embedding space by robustly pulling\ntogether representations of semantically similar texts while pushing apart\ndissimilar ones, thus reducing false positives and negatives. The final\nclassification is achieved by jointly optimizing a contrastive loss with a\ncross-entropy loss. Textual features are enhanced through a word-level\nattention module. Additionally, we employ sentiment, emotion, and toxicity\nfeatures. Evaluations on the EXIST2021 and MLSC datasets demonstrate that\nASCEND significantly outperforms existing methods, with average Macro F1\nimprovements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting\nits efficacy in capturing the subtle cues of implicit sexist language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faASCEND\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u9690\u6027\u6027\u522b\u6b67\u89c6\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u9608\u503c\u7684\u5bf9\u6bd4\u5b66\u4e60\u6765\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u7279\u5f81\uff0cASCEND\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u4ec7\u6068\u5185\u5bb9\uff08\u5305\u62ec\u9690\u6027\u6027\u522b\u6b67\u89c6\uff09\u6cdb\u6ee5\uff0c\u4f46\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5e38\u5ffd\u7565\u6216\u96be\u4ee5\u8bc6\u522b\u8fd9\u4e9b\u9690\u6027\u5f62\u5f0f\u3002", "method": "\u5f15\u5165ASCEND\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8e\u9608\u503c\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a\u4ec5\u5f53\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d85\u8fc7\u4e00\u4e2a\u53ef\u5b66\u4e60\u9608\u503c\u65f6\uff0c\u624d\u5c06\u6837\u672c\u5bf9\u89c6\u4e3a\u6b63\u4f8b\uff0c\u4ee5\u6b64\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\u3002\u6a21\u578b\u8054\u5408\u4f18\u5316\u5bf9\u6bd4\u635f\u5931\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u3002\u6587\u672c\u7279\u5f81\u901a\u8fc7\u8bcd\u7ea7\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\uff0c\u5e76\u7ed3\u5408\u60c5\u611f\u3001\u60c5\u7eea\u548c\u6bd2\u6027\u7279\u5f81\u3002", "result": "\u5728EXIST2021\u548cMLSC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cASCEND\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u591a\u4efb\u52a1\u4e2d\u5e73\u5747Macro F1\u5206\u6570\u5206\u522b\u63d0\u53479.86%\u300129.63%\u548c32.51%\u3002", "conclusion": "ASCEND\u80fd\u591f\u6709\u6548\u6355\u6349\u9690\u6027\u6027\u522b\u6b67\u89c6\u8bed\u8a00\u7684\u7ec6\u5fae\u7ebf\u7d22\uff0c\u8bc1\u660e\u5176\u5728\u68c0\u6d4b\u6b64\u7c7b\u5185\u5bb9\u65b9\u9762\u7684\u5f3a\u5927\u6548\u529b\u3002"}}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285", "abs": "https://arxiv.org/abs/2507.05285", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u521b\u65b0\u7684AI\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u8fdb\u884c\u9886\u57df\u60c5\u611f\u5206\u6790\u3001\u63d0\u793a\u5de5\u7a0b\u8bc6\u522b\u5b66\u4e1a\u538b\u529b\u6e90\u4ee5\u53ca\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdc\u7a0b\u5b66\u4e60\u5b66\u751f\u8f8d\u5b66\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5e72\u9884\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u8f8d\u5b66\u6311\u6218\u3002", "motivation": "\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u5b66\u751f\u8f8d\u5b66\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e26\u6765\u4e86\u6df1\u8fdc\u7684\u793e\u4f1a\u548c\u7ecf\u6d4e\u540e\u679c\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u5229\u7528\u7ed3\u6784\u5316\u7684\u793e\u4f1a\u4eba\u53e3\u548c\u884c\u4e3a\u6570\u636e\uff0c\u4f46\u672a\u80fd\u6355\u6349\u975e\u7ed3\u6784\u5316\u5b66\u751f\u4e92\u52a8\u4e2d\u7ec6\u81f4\u7684\u60c5\u611f\u548c\u60c5\u5883\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u53d8\u9769\u6027\u7684AI\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5229\u7528RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u6280\u672f\uff0c\u57fa\u4e8e\u6559\u5b66\u5185\u5bb9\u77e5\u8bc6\u5e93\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u60c5\u611f\u5206\u6790\uff0c\u4ee5\u6df1\u5ea6\u89e3\u8bfb\u5b66\u751f\u8bc4\u8bba\uff1b2) \u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\uff0c\u7cbe\u786e\u8bc6\u522b\u5e76\u89e3\u7801\u5b66\u751f\u9762\u4e34\u7684\u5b66\u4e1a\u538b\u529b\u6307\u6807\uff1b3) \u91c7\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u5c42\uff0c\u52a8\u6001\u6574\u5408\u6587\u672c\uff08\u60c5\u611f\u3001\u538b\u529b\uff09\u3001\u884c\u4e3a\uff08\u53c2\u4e0e\u6a21\u5f0f\uff09\u548c\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff0c\u6784\u5efa\u5168\u9762\u7684\u5b66\u751f\u98ce\u9669\u753b\u50cf\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e00\u4e2a\u5305\u542b4423\u540d\u5b66\u751f\u7684\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e8689%\u7684\u51c6\u786e\u7387\u548c0.88\u7684F1\u5206\u6570\u3002\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u6bd4\uff0c\u5176\u6027\u80fd\u63d0\u9ad8\u4e867%\uff0c\u5e76\u5c06\u5047\u9634\u6027\u51cf\u5c11\u4e8621%\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u9884\u6d4b\u8f8d\u5b66\u98ce\u9669\uff0c\u8fd8\u80fd\u901a\u8fc7\u68c0\u7d22\u60c5\u5883\u76f8\u5173\u7684\u7b56\u7565\uff08\u4f8b\u5982\uff0c\u4e3a\u5b64\u7acb\u5b66\u4e60\u8005\u63d0\u4f9b\u5bfc\u5e08\u8ba1\u5212\uff09\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5e72\u9884\u5efa\u8bae\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6210\u529f\u5f25\u5408\u4e86\u9884\u6d4b\u5206\u6790\u4e0e\u53ef\u64cd\u4f5c\u6559\u5b66\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5728\u5168\u7403\u6559\u80b2\u7cfb\u7edf\u4e2d\u6709\u6548\u964d\u4f4e\u5b66\u751f\u8f8d\u5b66\u98ce\u9669\u3002\u5b83\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u7cbe\u51c6\u7684\u9884\u6d4b\u80fd\u529b\u548c\u5b9e\u7528\u7684\u5e72\u9884\u6307\u5bfc\u3002"}}
{"id": "2507.05597", "pdf": "https://arxiv.org/pdf/2507.05597", "abs": "https://arxiv.org/abs/2507.05597", "authors": ["Yiming Zhao", "Xuanqi Meng", "Xinyu Tong", "Xiulong Liu", "Xin Xie", "Wenyu Qu"], "title": "Baton: Compensate for Missing Wi-Fi Features for Practical Device-free Tracking", "categories": ["cs.NI", "eess.SP"], "comment": "17 pages, 20 figures. Accepted and published in IEEE Transactions on\n  Mobile Computing on April 10, 2025. This is the accepted version. Final\n  published version: https://ieeexplore.ieee.org/document/10962318", "summary": "Wi-Fi contact-free sensing systems have attracted widespread attention due to\ntheir ubiquity and convenience. The integrated sensing and communication (ISAC)\ntechnology utilizes off-the-shelf Wi-Fi communication signals for sensing,\nwhich further promotes the deployment of intelligent sensing applications.\nHowever, current Wi-Fi sensing systems often require prolonged and unnecessary\ncommunication between transceivers, and brief communication interruptions will\nlead to significant performance degradation. This paper proposes Baton, the\nfirst system capable of accurately tracking targets even under severe Wi-Fi\nfeature deficiencies. To be specific, we explore the relevance of the Wi-Fi\nfeature matrix from both horizontal and vertical dimensions. The horizontal\ndimension reveals feature correlation across different Wi-Fi links, while the\nvertical dimension reveals feature correlation among different time slots.\nBased on the above principle, we propose the Simultaneous Tracking And\nPredicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi\nfeatures over time and across different links, akin to passing a baton. We\nimplement the system on commercial devices, and the experimental results show\nthat our system outperforms existing solutions with a median tracking error of\n0.46m, even when the communication duty cycle is as low as 20.00%. Compared\nwith the state-of-the-art, our system reduces the tracking error by 79.19% in\nscenarios with severe Wi-Fi feature deficiencies.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709Wi-Fi\u611f\u77e5\u7cfb\u7edf\u5728\u901a\u4fe1\u4e2d\u65ad\u6216\u7279\u5f81\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faBaton\u7cfb\u7edf\u548cSTAP\u7b97\u6cd5\uff0c\u901a\u8fc7\u6316\u6398Wi-Fi\u7279\u5f81\u5173\u8054\u6027\uff0c\u5728\u4f4e\u901a\u4fe1\u5360\u7a7a\u6bd4\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u76ee\u6807\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709Wi-Fi\u611f\u77e5\u7cfb\u7edf\u5e38\u9700\u957f\u65f6\u95f4\u901a\u4fe1\uff0c\u4e14\u77ed\u6682\u4e2d\u65ad\u5373\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u5728Wi-Fi\u7279\u5f81\u4e25\u91cd\u4e0d\u8db3\u7684\u573a\u666f\u4e0b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u667a\u80fd\u611f\u77e5\u5e94\u7528\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faBaton\u7cfb\u7edf\uff0c\u6838\u5fc3\u662f\u63a2\u7d22Wi-Fi\u7279\u5f81\u77e9\u9635\u7684\u6a2a\u5411\uff08\u8de8\u94fe\u8def\uff09\u548c\u7eb5\u5411\uff08\u8de8\u65f6\u9699\uff09\u5173\u8054\u6027\u3002\u57fa\u4e8e\u6b64\u539f\u7406\uff0c\u63d0\u51fa\u540c\u65f6\u8ddf\u8e2a\u4e0e\u9884\u6d4b\uff08STAP\uff09\u7b97\u6cd5\uff0c\u5b9e\u73b0Wi-Fi\u7279\u5f81\u5728\u65f6\u95f4\u548c\u94fe\u8def\u95f4\u7684\u65e0\u7f1d\u4f20\u9012\u3002", "result": "\u5728\u5546\u7528\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5e76\u9a8c\u8bc1\uff0c\u5373\u4f7f\u901a\u4fe1\u5360\u7a7a\u6bd4\u4f4e\u81f320.00%\uff0c\u7cfb\u7edf\u8ddf\u8e2a\u8bef\u5dee\u4e2d\u4f4d\u6570\u4ecd\u4e3a0.46m\u3002\u4e0e\u6700\u5148\u8fdb\u65b9\u6848\u76f8\u6bd4\uff0c\u5728Wi-Fi\u7279\u5f81\u4e25\u91cd\u4e0d\u8db3\u573a\u666f\u4e0b\uff0c\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8679.19%\u3002", "conclusion": "Baton\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86Wi-Fi\u7279\u5f81\u7f3a\u5931\u5bfc\u81f4\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05300", "pdf": "https://arxiv.org/pdf/2507.05300", "abs": "https://arxiv.org/abs/2507.05300", "authors": ["Nicholas Merchant", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Andrei Cristian Popescu", "Carlos Garcia Jurado Suarez"], "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7-page main paper + appendix, 18 figures", "summary": "We argue that generative text-to-image models often struggle with prompt\nadherence due to the noisy and unstructured nature of large-scale datasets like\nLAION-5B. This forces users to rely heavily on prompt engineering to elicit\ndesirable outputs. In this work, we propose that enforcing a consistent caption\nstructure during training can significantly improve model controllability and\nalignment. We introduce Re-LAION-Caption 19M, a high-quality subset of\nRe-LAION-5B, comprising 19 million 1024x1024 images with captions generated by\na Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part\ntemplate: subject, setting, aesthetics, and camera details. We fine-tune\nPixArt-$\\Sigma$ and Stable Diffusion 2 using both structured and randomly\nshuffled captions, and show that structured versions consistently yield higher\ntext-image alignment scores using visual question answering (VQA) models. The\ndataset is publicly available at\nhttps://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.05263", "pdf": "https://arxiv.org/pdf/2507.05263", "abs": "https://arxiv.org/abs/2507.05263", "authors": ["Kaichen Ouyang"], "title": "Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": "17 pages, 4 figures", "summary": "Graph Neural Networks (GNNs) have shown great potential in graph data\nanalysis due to their powerful representation capabilities. However, as the\nnetwork depth increases, the issue of over-smoothing becomes more severe,\ncausing node representations to lose their distinctiveness. This paper analyzes\nthe mechanism of over-smoothing through the analogy to Anderson localization\nand introduces participation degree as a metric to quantify this phenomenon.\nSpecifically, as the depth of the GNN increases, node features homogenize after\nmultiple layers of message passing, leading to a loss of distinctiveness,\nsimilar to the behavior of vibration modes in disordered systems. In this\ncontext, over-smoothing in GNNs can be understood as the expansion of\nlow-frequency modes (increased participation degree) and the localization of\nhigh-frequency modes (decreased participation degree). Based on this, we\nsystematically reviewed the potential connection between the Anderson\nlocalization behavior in disordered systems and the over-smoothing behavior in\nGraph Neural Networks. A theoretical analysis was conducted, and we proposed\nthe potential of alleviating over-smoothing by reducing the disorder in\ninformation propagation.", "AI": {"tldr": "\u6df1\u5165\u63a2\u8ba8GNN\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u901a\u8fc7\u4e0eAnderson\u5c40\u57df\u5316\u7c7b\u6bd4\uff0c\u5206\u6790\u4e86\u5176\u5185\u5728\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u53ef\u901a\u8fc7\u964d\u4f4e\u4fe1\u606f\u4f20\u64ad\u7684\u65e0\u5e8f\u6027\u6765\u7f13\u89e3\u8fc7\u5e73\u6ed1\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u65e5\u76ca\u4e25\u91cd\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5bfc\u81f4\u8282\u70b9\u8868\u793a\u5931\u53bb\u72ec\u7279\u6027\u3002", "method": "\u5c06GNN\u8fc7\u5e73\u6ed1\u673a\u5236\u7c7b\u6bd4\u4e3aAnderson\u5c40\u57df\u5316\uff0c\u5f15\u5165\u201c\u53c2\u4e0e\u5ea6\u201d\u4f5c\u4e3a\u91cf\u5316\u6307\u6807\u3002\u7cfb\u7edf\u56de\u987e\u4e24\u8005\u6f5c\u5728\u8054\u7cfb\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u53d1\u73b0GNN\u8fc7\u5e73\u6ed1\u53ef\u7406\u89e3\u4e3a\u4f4e\u9891\u6a21\u5f0f\u7684\u6269\u5c55\uff08\u53c2\u4e0e\u5ea6\u589e\u52a0\uff09\u548c\u9ad8\u9891\u6a21\u5f0f\u7684\u5c40\u57df\u5316\uff08\u53c2\u4e0e\u5ea6\u964d\u4f4e\uff09\uff0c\u5bfc\u81f4\u8282\u70b9\u7279\u5f81\u5728\u591a\u5c42\u6d88\u606f\u4f20\u9012\u540e\u540c\u8d28\u5316\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u964d\u4f4e\u4fe1\u606f\u4f20\u64ad\u4e2d\u7684\u65e0\u5e8f\u6027\u662f\u7f13\u89e3GNN\u8fc7\u5e73\u6ed1\u7684\u4e00\u79cd\u6f5c\u5728\u9014\u5f84\u3002"}}
{"id": "2507.05267", "pdf": "https://arxiv.org/pdf/2507.05267", "abs": "https://arxiv.org/abs/2507.05267", "authors": ["Markus B\u00f6ck"], "title": "Strongly Solving $7 \\times 6$ Connect-Four on Consumer Grade Hardware", "categories": ["cs.AI"], "comment": null, "summary": "While the game Connect-Four has been solved mathematically and the best move\ncan be effectively computed with search based methods, a strong solution in the\nform of a look-up table was believed to be infeasible. In this paper, we\nrevisit a symbolic search method based on binary decision diagrams to produce\nstrong solutions. With our efficient implementation we were able to produce a\n89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main\nmemory for the standard $7 \\times 6$ board size. In addition to this\nwin-draw-loss evaluation, we include an alpha-beta search in our open source\nartifact to find the move which achieves the fastest win or slowest loss.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u6548\u7684\u4e8c\u8fdb\u5236\u51b3\u7b56\u56fe\uff08BDD\uff09\u7b26\u53f7\u641c\u7d22\uff0c\u6210\u529f\u751f\u6210\u4e8689.6GB\u7684\u56db\u5b50\u68cb\u67e5\u627e\u8868\uff0c\u63a8\u7ffb\u4e86\u5148\u524d\u5173\u4e8e\u5176\u4e0d\u53ef\u884c\u7684\u89c2\u5ff5\u3002", "motivation": "\u5c3d\u7ba1\u56db\u5b50\u68cb\u5df2\u88ab\u6570\u5b66\u89e3\u51b3\u5e76\u901a\u8fc7\u641c\u7d22\u65b9\u6cd5\u53ef\u8ba1\u7b97\u6700\u4f73\u8d70\u6cd5\uff0c\u4f46\u4ee5\u67e5\u627e\u8868\u5f62\u5f0f\u5b58\u5728\u7684\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u6b64\u524d\u88ab\u8ba4\u4e3a\u4e0d\u53ef\u884c\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u5e76\u91c7\u7528\u57fa\u4e8e\u4e8c\u8fdb\u5236\u51b3\u7b56\u56fe\uff08BDD\uff09\u7684\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u9ad8\u6548\u5b9e\u73b0\u3002", "result": "\u5728\u5355CPU\u6838\u5fc3\uff08128GB\u5185\u5b58\uff09\u4e0a\uff0c\u752847\u5c0f\u65f6\u6210\u529f\u751f\u6210\u4e8689.6GB\u76847x6\u6807\u51c6\u56db\u5b50\u68cb\u67e5\u627e\u8868\u3002\u6b64\u5916\uff0c\u5176\u5f00\u6e90\u5de5\u4ef6\u8fd8\u5305\u542bAlpha-Beta\u641c\u7d22\u529f\u80fd\uff0c\u4ee5\u627e\u5230\u6700\u5feb\u83b7\u80dc\u6216\u6700\u6162\u5931\u8d25\u7684\u8d70\u6cd5\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u901a\u8fc7\u9ad8\u6548\u7684\u7b26\u53f7\u641c\u7d22\u5b9e\u73b0\u5927\u578b\u56db\u5b50\u68cb\u67e5\u627e\u8868\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u8be5\u6e38\u620f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u80dc\u5e73\u8d1f\u8bc4\u4f30\u53ca\u6700\u4f73\u8d70\u6cd5\u6307\u5bfc\uff0c\u6311\u6218\u4e86\u6b64\u524d\u4e0d\u53ef\u884c\u7684\u89c2\u70b9\u3002"}}
{"id": "2507.05319", "pdf": "https://arxiv.org/pdf/2507.05319", "abs": "https://arxiv.org/abs/2507.05319", "authors": ["Cheng Yuan", "Xinkai Rui", "Yongqi Fan", "Yawei Fan", "Boyang Zhong", "Jiacheng Wang", "Weiyan Zhang", "Tong Ruan"], "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review", "categories": ["cs.CL", "cs.AI"], "comment": "ACL Demo 2025", "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS.", "AI": {"tldr": "LCDS\u7cfb\u7edf\u901a\u8fc7\u6587\u672c\u76f8\u4f3c\u5ea6\u6620\u5c04\u548c\u903b\u8f91\u89c4\u5219\uff0c\u89e3\u51b3\u4e86LLM\u5728\u51fa\u9662\u6458\u8981\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u548c\u6eaf\u6e90\u95ee\u9898\uff0c\u751f\u6210\u53ef\u9760\u6458\u8981\u5e76\u652f\u6301\u4e13\u5bb6\u53cd\u9988\u4ee5\u5fae\u8c03LLM\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u81ea\u52a8\u5316\u51fa\u9662\u6458\u8981\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff08\u5982\u751f\u6210\u4e0d\u51c6\u786e\u5185\u5bb9\u6216\u51ed\u7a7a\u634f\u9020\u4fe1\u606f\uff09\u3002\u6b64\u5916\uff0c\u7535\u5b50\u75c5\u5386\u901a\u5e38\u6570\u636e\u91cf\u5927\uff0c\u5bfc\u81f4LLM\u96be\u4ee5\u5bf9\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u6eaf\u6e90\u3002", "method": "\u63d0\u51faLCDS\uff0c\u4e00\u4e2a\u903b\u8f91\u63a7\u5236\u7684\u51fa\u9662\u6458\u8981\u751f\u6210\u7cfb\u7edf\u3002LCDS\u901a\u8fc7\u8ba1\u7b97\u7535\u5b50\u75c5\u5386\u4e0e\u51fa\u9662\u6458\u8981\u4e4b\u95f4\u7684\u6587\u672c\u76f8\u4f3c\u5ea6\u6784\u5efa\u6eaf\u6e90\u6620\u5c04\u8868\uff0c\u4ee5\u9650\u5236\u6458\u8981\u5185\u5bb9\u7684\u8303\u56f4\u3002\u540c\u65f6\uff0cLCDS\u6574\u5408\u4e86\u4e00\u5957\u5168\u9762\u7684\u903b\u8f91\u89c4\u5219\uff0c\u4ee5\u751f\u6210\u66f4\u53ef\u9760\u7684\u201c\u94f6\u8d28\u201d\u51fa\u9662\u6458\u8981\u3002\u7cfb\u7edf\u8fd8\u652f\u6301\u5bf9\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u6eaf\u6e90\uff0c\u4fbf\u4e8e\u4e13\u5bb6\u5ba1\u67e5\u3001\u53cd\u9988\u548c\u7ea0\u9519\u3002", "result": "LCDS\u80fd\u591f\u751f\u6210\u66f4\u53ef\u9760\u7684\u3001\u9488\u5bf9\u4e0d\u540c\u4e34\u5e8a\u9886\u57df\u7684\u201c\u94f6\u8d28\u201d\u51fa\u9662\u6458\u8981\u3002\u7cfb\u7edf\u652f\u6301\u5185\u5bb9\u6eaf\u6e90\uff0c\u4f7f\u5f97\u4e13\u5bb6\u80fd\u591f\u9ad8\u6548\u5ba1\u67e5\u3001\u63d0\u4f9b\u53cd\u9988\u5e76\u7ea0\u6b63\u9519\u8bef\u3002\u7531\u6b64\u4ea7\u751f\u7684\u201c\u91d1\u8d28\u201d\u51fa\u9662\u6458\u8981\u53ef\u7528\u4e8eLLM\u7684\u589e\u91cf\u5fae\u8c03\u3002", "conclusion": "LCDS\u901a\u8fc7\u89e3\u51b3LLM\u7684\u5e7b\u89c9\u548c\u6eaf\u6e90\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fa\u9662\u6458\u8981\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u5176\u751f\u6210\u7684\u201c\u91d1\u8d28\u201d\u6458\u8981\u4e3aLLM\u7684\u589e\u91cf\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u9ad8LLM\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u6027\u80fd\u3002"}}
{"id": "2507.05731", "pdf": "https://arxiv.org/pdf/2507.05731", "abs": "https://arxiv.org/abs/2507.05731", "authors": ["Yuxin Zhang", "Jiahao Yang", "Zhe Chen", "Wenjun Zhu", "Jin Zhao", "Yue Gao"], "title": "A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "11 pages, 12 figures", "summary": "Recently, large vision-language models (LVLMs) unleash powerful analysis\ncapabilities for low Earth orbit (LEO) satellite Earth observation images in\nthe data center. However, fast satellite motion, brief satellite-ground station\n(GS) contact windows, and large size of the images pose a data download\nchallenge. To enable near real-time Earth observation applications (e.g.,\ndisaster and extreme weather monitoring), we should explore how to deploy LVLM\nin LEO satellite networks, and design SpaceVerse, an efficient satellite-ground\nsynergistic LVLM inference system. To this end, firstly, we deploy compact\nLVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs\nto handle computationally intensive tasks. Then, we propose a computing and\ncommunication co-design framework comprised of a progressive confidence network\nand an attention-based multi-scale preprocessing, used to identify on-satellite\ninferring data, and reduce data redundancy before satellite-GS transmission,\nseparately. We implement and evaluate SpaceVerse on real-world LEO satellite\nconstellations and datasets, achieving a 31.2% average gain in accuracy and a\n51.2% reduction in latency compared to state-of-the-art baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SpaceVerse\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u661f\u5730\u534f\u540c\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u63a8\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u5730\u7403\u89c2\u6d4b\u56fe\u50cf\u7684\u5b9e\u65f6\u5904\u7406\u4e2d\u9762\u4e34\u7684\u6570\u636e\u4e0b\u8f7d\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LVLM\u867d\u80fd\u5f3a\u5927\u5206\u6790LEO\u536b\u661f\u5730\u7403\u89c2\u6d4b\u56fe\u50cf\uff0c\u4f46\u5176\u5728\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\uff0c\u800c\u536b\u661f\u7684\u5feb\u901f\u79fb\u52a8\u3001\u4e0e\u5730\u9762\u7ad9\u77ed\u6682\u7684\u63a5\u89e6\u7a97\u53e3\u4ee5\u53ca\u56fe\u50cf\u7684\u5de8\u5927\u5c3a\u5bf8\u5171\u540c\u6784\u6210\u4e86\u6570\u636e\u4e0b\u8f7d\u7684\u5de8\u5927\u6311\u6218\uff0c\u963b\u788d\u4e86\u8fd1\u5b9e\u65f6\u5730\u7403\u89c2\u6d4b\u5e94\u7528\uff08\u5982\u707e\u5bb3\u548c\u6781\u7aef\u5929\u6c14\u76d1\u6d4b\uff09\u7684\u5b9e\u73b0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5728LEO\u536b\u661f\u7f51\u7edc\u4e2d\u63a2\u7d22LVLM\u7684\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u8bba\u6587\u8bbe\u8ba1\u4e86SpaceVerse\u7cfb\u7edf\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u661f\u5730\u534f\u540cLVLM\u63a8\u7406\u7cfb\u7edf\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u7d27\u51d1\u578bLVLM\u90e8\u7f72\u5728\u536b\u661f\u4e0a\u6267\u884c\u8f7b\u91cf\u7ea7\u4efb\u52a1\uff0c\u800c\u5e38\u89c4LVLM\u5728\u5730\u9762\u7ad9\u5904\u7406\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u30022) \u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u4e0e\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u6e10\u8fdb\u7f6e\u4fe1\u5ea6\u7f51\u7edc\uff08\u7528\u4e8e\u8bc6\u522b\u661f\u4e0a\u63a8\u7406\u6570\u636e\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5c3a\u5ea6\u9884\u5904\u7406\u6a21\u5757\uff08\u7528\u4e8e\u5728\u661f\u5730\u4f20\u8f93\u524d\u51cf\u5c11\u6570\u636e\u5197\u4f59\uff09\u3002", "result": "SpaceVerse\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u7684LEO\u536b\u661f\u661f\u5ea7\u548c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u73b0\u548c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7cfb\u7edf\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e8631.2%\uff0c\u5ef6\u8fdf\u964d\u4f4e\u4e8651.2%\u3002", "conclusion": "SpaceVerse\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u661f\u5730\u534f\u540cLVLM\u90e8\u7f72\u548c\u8ba1\u7b97\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u514b\u670d\u4e86LEO\u536b\u661f\u5730\u7403\u89c2\u6d4b\u56fe\u50cf\u5b9e\u65f6\u5904\u7406\u4e2d\u7684\u6570\u636e\u4e0b\u8f7d\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05302", "pdf": "https://arxiv.org/pdf/2507.05302", "abs": "https://arxiv.org/abs/2507.05302", "authors": ["Binjia Zhou", "Hengrui Lou", "Lizhe Chen", "Haoyuan Li", "Dawei Luo", "Shuai Chen", "Jie Lei", "Zunlei Feng", "Yijun Bei"], "title": "CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the swift progression of image generation technology, the widespread\nemergence of facial deepfakes poses significant challenges to the field of\nsecurity, thus amplifying the urgent need for effective deepfake\ndetection.Existing techniques for face forgery detection can broadly be\ncategorized into two primary groups: visual-based methods and multimodal\napproaches. The former often lacks clear explanations for forgery details,\nwhile the latter, which merges visual and linguistic modalities, is more prone\nto the issue of hallucinations.To address these shortcomings, we introduce a\nvisual detail enhanced self-correction framework, designated CorrDetail, for\ninterpretable face forgery detection. CorrDetail is meticulously designed to\nrectify authentic forgery details when provided with error-guided questioning,\nwith the aim of fostering the ability to uncover forgery details rather than\nyielding hallucinated responses. Additionally, to bolster the reliability of\nits findings, a visual fine-grained detail enhancement module is incorporated,\nsupplying CorrDetail with more precise visual forgery details. Ultimately, a\nfusion decision strategy is devised to further augment the model's\ndiscriminative capacity in handling extreme samples, through the integration of\nvisual information compensation and model bias reduction.Experimental results\ndemonstrate that CorrDetail not only achieves state-of-the-art performance\ncompared to the latest methodologies but also excels in accurately identifying\nforged details, all while exhibiting robust generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCorrDetail\u7684\u89c6\u89c9\u7ec6\u8282\u589e\u5f3a\u81ea\u6821\u6b63\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u89e3\u91ca\u6027\u548c\u5e7b\u89c9\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u8fc5\u901f\u53d1\u5c55\uff0c\u5bf9\u5b89\u5168\u9886\u57df\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u6025\u9700\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u89c6\u89c9\u548c\u591a\u6a21\u6001\u65b9\u6cd5\uff09\u666e\u904d\u5b58\u5728\u5bf9\u4f2a\u9020\u7ec6\u8282\u89e3\u91ca\u4e0d\u6e05\u6216\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u54cd\u5e94\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aCorrDetail\u7684\u89c6\u89c9\u7ec6\u8282\u589e\u5f3a\u81ea\u6821\u6b63\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u8bef\u5dee\u5f15\u5bfc\u63d0\u95ee\u6765\u7ea0\u6b63\u771f\u5b9e\u7684\u4f2a\u9020\u7ec6\u8282\uff0c\u4ee5\u63ed\u793a\u800c\u975e\u4ea7\u751f\u5e7b\u89c9\u54cd\u5e94\u7684\u4f2a\u9020\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u5b83\u6574\u5408\u4e86\u4e00\u4e2a\u89c6\u89c9\u7ec6\u7c92\u5ea6\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u4f2a\u9020\u89c6\u89c9\u7ec6\u8282\u3002\u6700\u7ec8\uff0c\u91c7\u7528\u878d\u5408\u51b3\u7b56\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u8865\u507f\u548c\u6a21\u578b\u504f\u5dee\u51cf\u5c11\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u6781\u7aef\u6837\u672c\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCorrDetail\u6846\u67b6\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u800c\u4e14\u5728\u51c6\u786e\u8bc6\u522b\u4f2a\u9020\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CorrDetail\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684SOTA\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05284", "pdf": "https://arxiv.org/pdf/2507.05284", "abs": "https://arxiv.org/abs/2507.05284", "authors": ["Mustafa Kamal", "Niyaz Bin Hashem", "Robin Krambroeckers", "Nabeel Mohammed", "Shafin Rahman"], "title": "Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction", "categories": ["cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Although most transformer-based time series forecasting models primarily\ndepend on endogenous inputs, recent state-of-the-art approaches have\nsignificantly improved performance by incorporating external information\nthrough exogenous inputs. However, these methods face challenges, such as\nredundancy when endogenous and exogenous inputs originate from the same source\nand limited ability to capture long-term dependencies due to fixed look-back\nwindows. In this paper, we propose a method that whitens the exogenous input to\nreduce redundancy that may persist within the data based on global statistics.\nAdditionally, our approach helps the exogenous input to be more aware of\npatterns and trends over extended periods. By introducing this refined,\nglobally context-aware exogenous input to the endogenous input without\nincreasing the lookback window length, our approach guides the model towards\nimproved forecasting. Our approach achieves state-of-the-art performance in\nfour benchmark datasets, consistently outperforming 11 baseline models. These\nresults establish our method as a robust and effective alternative for using\nexogenous inputs in time series forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u767d\u5316\u5916\u90e8\u8f93\u5165\u6765\u51cf\u5c11\u5197\u4f59\u5e76\u589e\u5f3a\u957f\u671f\u4f9d\u8d56\u6355\u83b7\u80fd\u529b\u7684\u65f6\u5e8f\u9884\u6d4b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u5728\u5229\u7528\u5916\u90e8\u8f93\u5165\u65f6\uff0c\u9762\u4e34\u6570\u636e\u5197\u4f59\uff08\u5f53\u5185\u5916\u90e8\u8f93\u5165\u6e90\u76f8\u540c\u65f6\uff09\u548c\u56fa\u5b9a\u56de\u6eaf\u7a97\u53e3\u5bfc\u81f4\u7684\u957f\u671f\u4f9d\u8d56\u6355\u83b7\u80fd\u529b\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5168\u5c40\u7edf\u8ba1\u91cf\u767d\u5316\u5916\u90e8\u8f93\u5165\u6765\u51cf\u5c11\u6570\u636e\u5197\u4f59\uff0c\u5e76\u4f7f\u5176\u66f4\u597d\u5730\u611f\u77e5\u957f\u671f\u6a21\u5f0f\u548c\u8d8b\u52bf\u3002\u5728\u4e0d\u589e\u52a0\u56de\u6eaf\u7a97\u53e3\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u8fd9\u79cd\u7cbe\u70bc\u3001\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5916\u90e8\u8f93\u5165\u4e0e\u5185\u90e8\u8f93\u5165\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e11\u4e2a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u4e00\u79cd\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6709\u6548\u4e14\u7a33\u5065\u5730\u4f7f\u7528\u5916\u90e8\u8f93\u5165\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.05283", "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.", "AI": {"tldr": "Chat2SPaT\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u7528\u6237\u5bf9\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7cbe\u786e\u7684\u4fe1\u53f7\u76f8\u4f4d\u548c\u65f6\u5e8f\uff08SPaT\uff09\u7ed3\u679c\uff0c\u5927\u5927\u7b80\u5316\u4e86\u4ea4\u901a\u4fe1\u53f7\u8ba1\u5212\u7684\u521b\u5efa\u548c\u7ba1\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bbe\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\uff08\u5982\u6309\u65f6\u6bb5\u6216\u6309\u5468\u8ba1\u5212\uff09\u521b\u5efa\u548c\u66f4\u65b0\u5de5\u4f5c\u7e41\u7410\u4e14\u8017\u8d39\u5927\u91cf\u4eba\u5de5\uff0c\u4e00\u4e2a\u4ea4\u53c9\u53e3\u5e38\u9700\u5173\u8054\u591a\u4e2a\u8ba1\u5212\uff0c\u5bfc\u81f4\u91cd\u590d\u6027\u4eba\u5de5\u53c2\u6570\u8f93\u5165\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Chat2SPaT\u65b9\u6cd5\u3002\u5b83\u9996\u5148\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u7528\u6237\u534a\u7ed3\u6784\u5316\u548c\u6a21\u7cca\u7684\u8ba1\u5212\u63cf\u8ff0\uff0c\u5e76\u5c06\u5176\u91cd\u6784\u4e3aJSON\u683c\u5f0f\u7684\u76f8\u4f4d\u5e8f\u5217\u548c\u5c5e\u6027\u7ed3\u679c\u3002\u63a5\u7740\uff0c\u5229\u7528Python\u811a\u672c\u5904\u7406LLM\u8f93\u51fa\uff0c\u4ee5\u5b9a\u4f4d\u5468\u671f\u5185\u7684\u76f8\u4f4d\u3001\u5904\u7406\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u6700\u7ec8\u7ec4\u88c5\u5b8c\u6574\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u3002\u8be5\u6d41\u7a0b\u53ef\u8fed\u4ee3\u7528\u4e8e\u8ba1\u5212\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cChat2SPaT\u5728\u5305\u542b300\u591a\u4e2a\u8ba1\u5212\u63cf\u8ff0\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u82f1\u8bed\u548c\u4e2d\u6587\u8ba1\u5212\u7684\u751f\u6210\u51c6\u786e\u7387\u5747\u8d85\u8fc794%\u3002", "conclusion": "Chat2SPaT\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u7406\u89e3\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u63cf\u8ff0\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e3a\u4ea4\u901a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u8ba1\u5212\u7ba1\u7406\u6d41\u7a0b\uff0c\u4e3aLLMs\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u9886\u57df\u66f4\u51c6\u786e\u3001\u66f4\u901a\u7528\u5730\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u65b0\u57fa\u7840\u3002"}}
{"id": "2507.05330", "pdf": "https://arxiv.org/pdf/2507.05330", "abs": "https://arxiv.org/abs/2507.05330", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled new applications\nin e-commerce customer service. However, their capabilities remain constrained\nin complex, multimodal scenarios. We present MindFlow, the first open-source\nmultimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it\nintegrates memory, decision-making, and action modules, and adopts a modular\n\"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via\nonline A/B testing and simulation-based ablation, MindFlow demonstrates\nsubstantial gains in handling complex queries, improving user satisfaction, and\nreducing operational costs, with a 93.53% relative improvement observed in\nreal-world deployments.", "AI": {"tldr": "MindFlow\u662f\u9996\u4e2a\u9762\u5411\u7535\u5546\u7684\u5f00\u6e90\u591a\u6a21\u6001LLM\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u5ba2\u6237\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7535\u5546\u5ba2\u670d\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u590d\u6742\u7684\u3001\u591a\u6a21\u6001\u7684\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u80fd\u529b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMindFlow\uff0c\u4e00\u4e2a\u57fa\u4e8eCoALA\u6846\u67b6\u7684\u5f00\u6e90\u591a\u6a21\u6001LLM\u667a\u80fd\u4f53\u3002\u5b83\u96c6\u6210\u4e86\u8bb0\u5fc6\u3001\u51b3\u7b56\u548c\u884c\u52a8\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u6a21\u5757\u5316\u7684\u201cMLLM\u5373\u5de5\u5177\u201d\u7b56\u7565\u8fdb\u884c\u6709\u6548\u7684\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\uff0cMindFlow\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u3001\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u89c2\u5bdf\u523093.53%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "MindFlow\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u591a\u6a21\u6001\u7535\u5546\u5ba2\u670d\u573a\u666f\u4e2d\u7684\u9650\u5236\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u670d\u52a1\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.05829", "pdf": "https://arxiv.org/pdf/2507.05829", "abs": "https://arxiv.org/abs/2507.05829", "authors": ["Zekai Sun", "Xiuxian Guan", "Zheng Lin", "Zihan Fang", "Xiangming Cai", "Zhe Chen", "Fangming Liu", "Heming Cui", "Jie Xiong", "Wei Ni", "Chau Yuen"], "title": "Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "14 pages, 19 figures", "summary": "Deploying deep neural networks (DNNs) on resource-constrained mobile devices\npresents significant challenges, particularly in achieving real-time\nperformance while simultaneously coping with limited computational resources\nand battery life. While Mobile Edge Computing (MEC) offers collaborative\ninference with GPU servers as a promising solution, existing approaches\nprimarily rely on layer-wise model partitioning and undergo significant\ntransmission bottlenecks caused by the sequential execution of DNN operations.\nTo address this challenge, we present Intra-DP, a high-performance\ncollaborative inference system optimized for DNN inference on MEC. Intra DP\nemploys a novel parallel computing technique based on local operators (i.e.,\noperators whose minimum unit input is not the entire input tensor, such as the\nconvolution kernel). By decomposing their computations (operations) into\nseveral independent sub-operations and overlapping the computation and\ntransmission of different sub-operations through parallel execution, Intra-DP\nmitigates transmission bottlenecks in MEC, achieving fast and energy-efficient\ninference. The evaluation demonstrates that Intra-DP reduces per-inference\nlatency by up to 50% and energy consumption by up to 75% compared to\nstate-of-the-art baselines, without sacrificing accuracy.", "AI": {"tldr": "Intra-DP\u7cfb\u7edf\u901a\u8fc7\u5e76\u884c\u5316\u672c\u5730\u7b97\u5b50\u7684\u8ba1\u7b97\u548c\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9762\u4e34\u5b9e\u65f6\u6027\u80fd\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u7535\u6c60\u5bff\u547d\u7684\u6311\u6218\u3002\u867d\u7136\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u901a\u8fc7\u4e0eGPU\u670d\u52a1\u5668\u534f\u4f5c\u63a8\u7406\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5c42\u7ea7\u6a21\u578b\u5206\u533a\uff0c\u5e76\u56e0DNN\u64cd\u4f5c\u7684\u987a\u5e8f\u6267\u884c\u5bfc\u81f4\u4e25\u91cd\u7684\u4f20\u8f93\u74f6\u9888\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Intra-DP\u7cfb\u7edf\uff0c\u4e00\u4e2a\u4e13\u4e3aMEC\u4e0aDNN\u63a8\u7406\u4f18\u5316\u7684\u534f\u4f5c\u63a8\u7406\u7cfb\u7edf\u3002\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u57fa\u4e8e\u5c40\u90e8\u7b97\u5b50\uff08\u5982\u5377\u79ef\u6838\uff09\uff0c\u5c06\u5176\u8ba1\u7b97\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u5b50\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u6267\u884c\u91cd\u53e0\u8ba1\u7b97\u548c\u4f20\u8f93\uff0c\u4ece\u800c\u7f13\u89e3\u4f20\u8f93\u74f6\u9888\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u76f8\u6bd4\uff0cIntra-DP\u5c06\u6bcf\u6b21\u63a8\u7406\u7684\u5ef6\u8fdf\u964d\u4f4e\u4e86\u9ad8\u8fbe50%\uff0c\u80fd\u8017\u964d\u4f4e\u4e86\u9ad8\u8fbe75%\uff0c\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "conclusion": "Intra-DP\u6210\u529f\u5730\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u884c\u8ba1\u7b97\u6280\u672f\u89e3\u51b3\u4e86MEC\u4e2d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u4f20\u8f93\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u8282\u80fd\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.05376", "pdf": "https://arxiv.org/pdf/2507.05376", "abs": "https://arxiv.org/abs/2507.05376", "authors": ["Aquino Joctum", "John Kandiri"], "title": "YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries", "categories": ["cs.CV", "I.4.8; I.2.6; I.2.9"], "comment": "Published in the International Journal of Computer Trends and\n  Technology (IJCTT), vol. 73, no. 6, 2024. The final version of record is\n  available at: https://doi.org/10.14445/22312803/IJCTT-V73I6P108", "summary": "Autonomous vehicle perception systems require robust pedestrian detection,\nparticularly on geometrically complex roadways like Type-S curved surfaces,\nwhere standard RGB camera-based methods face limitations. This paper introduces\nYOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework\nspecifically for this challenge. YOLO-APD integrates several key architectural\nmodifications: a parameter-free SimAM attention mechanism, computationally\nefficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale\nfeature pooling, the Mish activation function for improved optimization, and an\nIntelligent Gather & Distribute (IGD) module for superior feature fusion in the\nnetwork's neck. The concept of leveraging vehicle steering dynamics for\nadaptive region-of-interest processing is also presented. Comprehensive\nevaluations on a custom CARLA dataset simulating complex scenarios demonstrate\nthat YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%\nmAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly\noutperforming baseline models, including YOLOv8. Furthermore, it maintains\nreal-time processing capabilities at 100 FPS, showcasing a superior balance\nbetween accuracy and efficiency. Ablation studies validate the synergistic\ncontribution of each integrated component. Evaluation on the KITTI dataset\nconfirms the architecture's potential while highlighting the need for domain\nadaptation. This research advances the development of highly accurate,\nefficient, and adaptable perception systems based on cost-effective sensors,\ncontributing to enhanced safety and reliability for autonomous navigation in\nchallenging, less-structured driving environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faYOLO-APD\uff0c\u4e00\u4e2a\u57fa\u4e8eYOLOv8\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u521b\u65b0\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728S\u578b\u5f2f\u9053\u7b49\u590d\u6742\u8def\u51b5\u4e0b\u884c\u4eba\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728S\u578b\u5f2f\u9053\u7b49\u51e0\u4f55\u590d\u6742\u8def\u9762\u4e0a\uff0c\u6807\u51c6RGB\u76f8\u673a\u884c\u4eba\u68c0\u6d4b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5c40\u9650\uff0c\u4e9f\u9700\u63d0\u5347\u611f\u77e5\u7cfb\u7edf\u6027\u80fd\u3002", "method": "YOLO-APD\u57fa\u4e8eYOLOv8\u6846\u67b6\uff0c\u96c6\u6210\u65e0\u53c2\u6570SimAM\u6ce8\u610f\u529b\u673a\u5236\u3001\u9ad8\u6548C3Ghost\u6a21\u5757\u3001\u65b0\u578bSimSPPF\u591a\u5c3a\u5ea6\u7279\u5f81\u6c60\u5316\u6a21\u5757\u3001Mish\u6fc0\u6d3b\u51fd\u6570\u4ee5\u53caIGD\u7279\u5f81\u878d\u5408\u6a21\u5757\u3002\u540c\u65f6\u5f15\u5165\u5229\u7528\u8f66\u8f86\u8f6c\u5411\u52a8\u6001\u8fdb\u884c\u81ea\u9002\u5e94\u611f\u5174\u8da3\u533a\u57df\u5904\u7406\u7684\u6982\u5ff5\u3002\u5728\u81ea\u5b9a\u4e49CARLA\u6570\u636e\u96c6\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "YOLO-APD\u5728\u81ea\u5b9a\u4e49CARLA\u6570\u636e\u96c6\u4e0a\u8fbe\u523077.7% mAP@0.5:0.95\uff0c\u884c\u4eba\u53ec\u56de\u7387\u8d85\u8fc796%\uff0c\u663e\u8457\u4f18\u4e8eYOLOv8\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4fdd\u6301100 FPS\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u534f\u540c\u8d21\u732e\u3002KITTI\u6570\u636e\u96c6\u8bc4\u4f30\u786e\u8ba4\u5176\u6f5c\u529b\u4f46\u6307\u51fa\u57df\u9002\u5e94\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u57fa\u4e8e\u6210\u672c\u6548\u76ca\u4f20\u611f\u5668\u7684\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u611f\u77e5\u7cfb\u7edf\u53d1\u5c55\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6311\u6218\u6027\u3001\u975e\u7ed3\u6784\u5316\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.05286", "pdf": "https://arxiv.org/pdf/2507.05286", "abs": "https://arxiv.org/abs/2507.05286", "authors": ["Kimia Soroush", "Mohsen Raji", "Behnam Ghavami"], "title": "Compressing Deep Neural Networks Using Explainable AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable performance in many\ntasks but it often comes at a high computational cost and memory usage.\nCompression techniques, such as pruning and quantization, are applied to reduce\nthe memory footprint of DNNs and make it possible to accommodate them on\nresource-constrained edge devices. Recently, explainable artificial\nintelligence (XAI) methods have been introduced with the purpose of\nunderstanding and explaining AI methods. XAI can be utilized to get to know the\ninner functioning of DNNs, such as the importance of different neurons and\nfeatures in the overall performance of DNNs. In this paper, a novel DNN\ncompression approach using XAI is proposed to efficiently reduce the DNN model\nsize with negligible accuracy loss. In the proposed approach, the importance\nscore of DNN parameters (i.e. weights) are computed using a gradient-based XAI\ntechnique called Layer-wise Relevance Propagation (LRP). Then, the scores are\nused to compress the DNN as follows: 1) the parameters with the negative or\nzero importance scores are pruned and removed from the model, 2)\nmixed-precision quantization is applied to quantize the weights with\nhigher/lower score with higher/lower number of bits. The experimental results\nshow that, the proposed compression approach reduces the model size by 64%\nwhile the accuracy is improved by 42% compared to the state-of-the-art\nXAI-based compression method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\uff08\u5177\u4f53\u4e3aLRP\uff09\u8fdb\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u6743\u91cd\u91cd\u8981\u6027\u5206\u6570\u8fdb\u884c\u526a\u679d\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u5c3a\u5bf8\u7684\u5927\u5e45\u7f29\u5c0f\u548c\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u73b0\u6709\u538b\u7f29\u6280\u672f\uff08\u5982\u526a\u679d\u548c\u91cf\u5316\uff09\u53ef\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\u3002\u4f5c\u8005\u65e8\u5728\u5229\u7528XAI\u7406\u89e3DNN\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff08\u5982\u53c2\u6570\u91cd\u8981\u6027\uff09\uff0c\u4ece\u800c\u5f00\u53d1\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u7684DNN\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u57faXAI\u6280\u672f\uff08\u5206\u5c42\u76f8\u5173\u6027\u4f20\u64adLRP\uff09\u8ba1\u7b97DNN\u53c2\u6570\uff08\u6743\u91cd\uff09\u7684\u91cd\u8981\u6027\u5f97\u5206\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u5f97\u5206\u8fdb\u884c\u538b\u7f29\uff1a1) \u526a\u679d\u5e76\u79fb\u9664\u91cd\u8981\u6027\u5f97\u5206\u8d1f\u6216\u96f6\u7684\u53c2\u6570\uff1b2) \u5bf9\u91cd\u8981\u6027\u5f97\u5206\u9ad8\u7684\u6743\u91cd\u91c7\u7528\u66f4\u9ad8\u4f4d\u6570\uff0c\u5f97\u5206\u4f4e\u7684\u6743\u91cd\u91c7\u7528\u66f4\u4f4e\u4f4d\u6570\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u538b\u7f29\u65b9\u6cd5\u5c06\u6a21\u578b\u5c3a\u5bf8\u51cf\u5c0f\u4e8664%\uff0c\u540c\u65f6\u76f8\u6bd4\u4e8e\u6700\u5148\u8fdb\u7684XAI\u57fa\u538b\u7f29\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8642%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528XAI\u6307\u5bfc\u7684DNN\u538b\u7f29\u65b9\u6cd5\u80fd\u6709\u6548\u663e\u8457\u5730\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u5e76\u5728\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709XAI\u57fa\u538b\u7f29\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72DNN\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.05297", "pdf": "https://arxiv.org/pdf/2507.05297", "abs": "https://arxiv.org/abs/2507.05297", "authors": ["Zijun Meng"], "title": "Fuzzy Classification Aggregation for a Continuum of Agents", "categories": ["cs.AI", "econ.TH"], "comment": null, "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8fde\u7eed\u6a21\u7cca\u5206\u7c7b\u7684\u805a\u5408\u51fd\u6570\u5fc5\u7136\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c\u3002", "motivation": "\u65e8\u5728\u8bc6\u522b\u548c\u523b\u753b\u5728\u6ee1\u8db3\u6700\u4f18\u6027\u3001\u72ec\u7acb\u6027\u548c\u96f6\u4e00\u81f4\u6027\u7b49\u7279\u5b9a\u6027\u8d28\u65f6\uff0c\u5904\u7406\u8fde\u7eed\u4e2a\u4f53\u5206\u7c7b\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u7684\u5185\u5728\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u6570\u5b66\u8bc1\u660e\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u6765\u8bc1\u660e\u805a\u5408\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\u3002", "result": "\u5bf9\u4e8e\u5c06\u81f3\u5c113\u4e2a\u5bf9\u8c61\u5206\u4e3a2\u5230m\u79cd\u7c7b\u578b\u7684\u8fde\u7eed\u4e2a\u4f53\u5206\u7c7b\uff0c\u4efb\u4f55\u6700\u4f18\u3001\u72ec\u7acb\u4e14\u96f6\u4e00\u81f4\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u90fd\u5fc5\u987b\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c\u3002", "conclusion": "\u5728\u6240\u8bbe\u5b9a\u7684\u516c\u7406\u5316\u6761\u4ef6\u4e0b\uff0c\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u7684\u552f\u4e00\u5f62\u5f0f\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u503c\uff0c\u8fd9\u4e3a\u6a21\u7cca\u5206\u7c7b\u7684\u805a\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346", "abs": "https://arxiv.org/abs/2507.05346", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).", "AI": {"tldr": "\u9488\u5bf9\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4e13\u5bb6\u6cdb\u6ee5\u7684\u95ee\u9898\uff0c\u63d0\u51faLAG\u65b9\u6cd5\u4ee5\u9ad8\u6548\u9009\u62e9\u548c\u7ec4\u5408LoRA\u9002\u914d\u5668\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5e76\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u517c\u5bb9RAG\u3002", "motivation": "\u968f\u7740\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u548c\u9886\u57df\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4e13\u5bb6\u7684\u51fa\u73b0\uff0c\u4e9f\u9700\u9ad8\u6548\u9009\u62e9\u548c\u7ec4\u5408\u8fd9\u4e9b\u4e13\u5bb6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLoRA\u589e\u5f3a\u751f\u6210\uff08LAG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528\u5927\u578b\u77e5\u8bc6\u5e93\u548c\u4efb\u52a1\u4e13\u7528LoRA\u9002\u914d\u5668\u3002LAG\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\uff0c\u80fd\u591f\u4ee5\u9010\u4ee4\u724c\u548c\u9010\u5c42\u7684\u65b9\u5f0f\u9ad8\u6548\u8fc7\u6ee4\u3001\u68c0\u7d22\u548c\u5e94\u7528\u4e13\u5bb6\u3002", "result": "\u5728\u591a\u79cd\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\uff0cLAG\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65e0\u6570\u636e\u65b9\u6cd5\u3002\u5728\u6709\u989d\u5916\u6570\u636e\u53ef\u7528\u65f6\uff0cLAG\u8fd8\u5c55\u793a\u4e86\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u66ff\u4ee3\u89e3\u51b3\u65b9\u6848\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "LAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u5229\u7528\u73b0\u6709LoRA\u4e13\u5bb6\u5e93\u7684\u6709\u6548\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2507.05876", "pdf": "https://arxiv.org/pdf/2507.05876", "abs": "https://arxiv.org/abs/2507.05876", "authors": ["Nehal Baganal Krishna", "Anam Tahir", "Firas Khamis", "Mina Tahmasbi Arashloo", "Michael Zink", "Amr Rizk"], "title": "OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning", "categories": ["cs.NI", "cs.AR"], "comment": "17 pages, 11 figures", "summary": "Asynchronous Distributed Reinforcement Learning (DRL) can suffer from\ndegraded convergence when model updates become stale, often the result of\nnetwork congestion and packet loss during large-scale training. This work\nintroduces a network data-plane acceleration architecture that mitigates such\nstaleness by enabling inline processing of DRL model updates as they traverse\nthe accelerator engine. To this end, we design and prototype a novel queueing\nmechanism that opportunistically combines compatible updates sharing a network\nelement, reducing redundant traffic and preserving update utility.\nComplementing this we provide a lightweight transmission control mechanism at\nthe worker nodes that is guided by feedback from the in-network accelerator. To\nassess model utility at line rate, we introduce the Age-of-Model (AoM) metric\nas a proxy for staleness and verify global fairness and responsiveness\nproperties using a formal verification method. Our evaluations demonstrate that\nthis architecture significantly reduces update staleness and congestion,\nultimately improving the convergence rate in asynchronous DRL workloads.", "AI": {"tldr": "\u5f02\u6b65DRL\u8bad\u7ec3\u4e2d\u6a21\u578b\u66f4\u65b0\u5ef6\u8fdf\u5bfc\u81f4\u6536\u655b\u6027\u4e0b\u964d\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7f51\u7edc\u6570\u636e\u5e73\u9762\u52a0\u901f\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u7f51\u5185\u5904\u7406\u66f4\u65b0\u6765\u51cf\u5c11\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f02\u6b65\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60(DRL)\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u65f6\uff0c\u7f51\u7edc\u62e5\u585e\u548c\u4e22\u5305\u5bfc\u81f4\u6a21\u578b\u66f4\u65b0\u6ede\u540e\uff0c\u8fdb\u800c\u964d\u4f4e\u6536\u655b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7f51\u7edc\u6570\u636e\u5e73\u9762\u52a0\u901f\u67b6\u6784\uff0c\u652f\u6301\u5bf9DRL\u6a21\u578b\u66f4\u65b0\u7684\u5728\u7ebf\u5904\u7406\uff1b\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u6392\u961f\u673a\u5236\uff0c\u673a\u4f1a\u6027\u5730\u5408\u5e76\u517c\u5bb9\u66f4\u65b0\uff1b\u5f15\u5165\u8f7b\u91cf\u7ea7\u4f20\u8f93\u63a7\u5236\u673a\u5236\uff1b\u5e76\u63d0\u51faAge-of-Model (AoM)\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u65b0\u9c9c\u5ea6\uff0c\u91c7\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4fdd\u8bc1\u516c\u5e73\u6027\u548c\u54cd\u5e94\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u663e\u8457\u51cf\u5c11\u4e86\u66f4\u65b0\u7684\u6ede\u540e\u6027\u548c\u7f51\u7edc\u62e5\u585e\u3002", "conclusion": "\u8be5\u67b6\u6784\u6700\u7ec8\u63d0\u5347\u4e86\u5f02\u6b65DRL\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2507.05383", "pdf": "https://arxiv.org/pdf/2507.05383", "abs": "https://arxiv.org/abs/2507.05383", "authors": ["Alexandr A. Kalinin", "Paula Llanos", "Theresa Maria Sommer", "Giovanni Sestini", "Xinhai Hou", "Jonathan Z. Sexton", "Xiang Wan", "Ivo D. Dinov", "Brian D. Athey", "Nicolas Rivron", "Anne E. Carpenter", "Beth Cimini", "Shantanu Singh", "Matthew J. O'Meara"], "title": "Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling", "categories": ["cs.CV", "q-bio.QM", "I.4.9; J.3"], "comment": "ICML 2025 Generative AI and Biology (GenBio) Workshop", "summary": "Microscopy enables direct observation of cellular morphology in 3D, with\ntransmitted-light methods offering low-cost, minimally invasive imaging and\nfluorescence microscopy providing specificity and contrast. Virtual staining\ncombines these strengths by using machine learning to predict fluorescence\nimages from label-free inputs. However, training of existing methods typically\nrelies on loss functions that treat all pixels equally, thus reproducing\nbackground noise and artifacts instead of focusing on biologically meaningful\nsignals. We introduce Spotlight, a simple yet powerful virtual staining\napproach that guides the model to focus on relevant cellular structures.\nSpotlight uses histogram-based foreground estimation to mask pixel-wise loss\nand to calculate a Dice loss on soft-thresholded predictions for shape-aware\nlearning. Applied to a 3D benchmark dataset, Spotlight improves morphological\nrepresentation while preserving pixel-level accuracy, resulting in virtual\nstains better suited for downstream tasks such as segmentation and profiling.", "AI": {"tldr": "Spotlight\u662f\u4e00\u79cd\u65b0\u578b\u865a\u62df\u67d3\u8272\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u4f7f\u5176\u4e13\u6ce8\u4e8e\u7ec6\u80de\u7ed3\u6784\uff0c\u4ece\u800c\u63d0\u9ad8\u865a\u62df\u67d3\u8272\u56fe\u50cf\u8d28\u91cf\uff0c\u66f4\u5229\u4e8e\u540e\u7eed\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u7684\u635f\u5931\u51fd\u6570\u5bf9\u6240\u6709\u50cf\u7d20\u540c\u7b49\u5bf9\u5f85\uff0c\u5bfc\u81f4\u6a21\u578b\u91cd\u73b0\u80cc\u666f\u566a\u58f0\u548c\u4f2a\u5f71\uff0c\u800c\u975e\u5173\u6ce8\u751f\u7269\u5b66\u6709\u610f\u4e49\u7684\u4fe1\u53f7\u3002", "method": "\u5f15\u5165\u540d\u4e3a\u201cSpotlight\u201d\u7684\u865a\u62df\u67d3\u8272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u524d\u666f\u4f30\u8ba1\u906e\u853d\u50cf\u7d20\u7ea7\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5bf9\u8f6f\u9608\u503c\u9884\u6d4b\u8ba1\u7b97Dice\u635f\u5931\uff0c\u5b9e\u73b0\u5f62\u72b6\u611f\u77e5\u5b66\u4e60\u3002", "result": "\u57283D\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0cSpotlight\u63d0\u9ad8\u4e86\u5f62\u6001\u5b66\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "Spotlight\u751f\u6210\u7684\u865a\u62df\u67d3\u8272\u56fe\u50cf\u66f4\u9002\u5408\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f8b\u5982\u7ec6\u80de\u5206\u5272\u548c\u7279\u5f81\u5206\u6790\u3002"}}
{"id": "2507.05291", "pdf": "https://arxiv.org/pdf/2507.05291", "abs": "https://arxiv.org/abs/2507.05291", "authors": ["Manuel Ricardo Guevara Garban", "Yves Chemisky", "\u00c9tienne Pruli\u00e8re", "Micha\u00ebl Cl\u00e9ment"], "title": "Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.comp-ph"], "comment": "28 pages, 17 figures, pre-print", "summary": "We propose a physics-informed machine learning framework called P-DivGNN to\nreconstruct local stress fields at the micro-scale, in the context of\nmulti-scale simulation given a periodic micro-structure mesh and mean,\nmacro-scale, stress values. This method is based in representing a periodic\nmicro-structure as a graph, combined with a message passing graph neural\nnetwork. We are able to retrieve local stress field distributions, providing\naverage stress values produced by a mean field reduced order model (ROM) or\nFinite Element (FE) simulation at the macro-scale. The prediction of local\nstress fields are of utmost importance considering fracture analysis or the\ndefinition of local fatigue criteria. Our model incorporates physical\nconstraints during training to constraint local stress field equilibrium state\nand employs a periodic graph representation to enforce periodic boundary\nconditions. The benefits of the proposed physics-informed GNN are evaluated\nconsidering linear and non linear hyperelastic responses applied to varying\ngeometries. In the non-linear hyperelastic case, the proposed method achieves\nsignificant computational speed-ups compared to FE simulation, making it\nparticularly attractive for large-scale applications.", "AI": {"tldr": "\u63d0\u51faP-DivGNN\u6846\u67b6\uff0c\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u5c3a\u5ea6\u6a21\u62df\u4e2d\uff0c\u6839\u636e\u5468\u671f\u6027\u5fae\u89c2\u7ed3\u6784\u548c\u5b8f\u89c2\u5e94\u529b\u503c\u91cd\u5efa\u5fae\u89c2\u5c3a\u5ea6\u7684\u5c40\u90e8\u5e94\u529b\u573a\u3002", "motivation": "\u5c40\u90e8\u5e94\u529b\u573a\u7684\u9884\u6d4b\u5bf9\u4e8e\u65ad\u88c2\u5206\u6790\u6216\u5c40\u90e8\u75b2\u52b3\u51c6\u5219\u7684\u5b9a\u4e49\u81f3\u5173\u91cd\u8981\u3002", "method": "P-DivGNN\u6846\u67b6\u57fa\u4e8e\u5c06\u5468\u671f\u6027\u5fae\u89c2\u7ed3\u6784\u8868\u793a\u4e3a\u56fe\uff0c\u5e76\u7ed3\u5408\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u878d\u5165\u7269\u7406\u7ea6\u675f\u4ee5\u786e\u4fdd\u5c40\u90e8\u5e94\u529b\u573a\u5e73\u8861\uff0c\u5e76\u91c7\u7528\u5468\u671f\u6027\u56fe\u8868\u793a\u6765\u5f3a\u5236\u5468\u671f\u8fb9\u754c\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8d85\u5f39\u6027\u54cd\u5e94\u4ee5\u53ca\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u80fd\u591f\u4ece\u5b8f\u89c2\u5e73\u5747\u5e94\u529b\u503c\uff08\u7531\u5e73\u5747\u573a\u964d\u9636\u6a21\u578b\u6216\u6709\u9650\u5143\u6a21\u62df\u63d0\u4f9b\uff09\u4e2d\u83b7\u53d6\u5e76\u91cd\u5efa\u5c40\u90e8\u5e94\u529b\u573a\u5206\u5e03\u3002\u5728\u975e\u7ebf\u6027\u8d85\u5f39\u6027\u60c5\u51b5\u4e0b\uff0c\u4e0e\u6709\u9650\u5143\u6a21\u62df\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "P-DivGNN\u5728\u5904\u7406\u975e\u7ebf\u6027\u8d85\u5f39\u6027\u95ee\u9898\u65f6\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5176\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u5177\u6709\u7279\u522b\u7684\u5438\u5f15\u529b\u3002"}}
