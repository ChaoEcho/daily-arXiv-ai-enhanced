<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 本文提出一种“模型合成架构”（MSA），结合语言模型和概率程序，以计算方式模拟人类在新颖情境下整合背景知识进行连贯推理的能力，并在实验中显示出优于纯语言模型基线的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在探究人类如何在面对新颖情境时，从广泛背景知识中提取相关信息并进行连贯推理和预测。

Method: 提出“模型合成架构”（MSA），该架构利用语言模型进行全局相关性检索和模型合成，并使用概率程序构建定制的、连贯的世界模型。通过一项围绕“模型奥运会”体育情境设计的新颖推理数据集，评估MSA对人类判断的模拟能力。

Result: MSA在捕捉人类判断方面，表现优于仅使用语言模型的基线方法（无论是直接生成还是思维链生成）。

Conclusion: 研究结果表明，MSA能够以一种模仿人类能力的方式实现对全局相关变量的局部连贯推理，为理解和复制开放领域中的人类推理提供了一条途径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本研究通过识别语言模型中的“模态差异向量”，发现语言模型比之前认为的更具可靠的模态类别判断能力，并且这些向量的演变与模型能力提升一致，甚至能模拟人类的模态分类行为。


<details>
  <summary>Details</summary>
Motivation: 语言模型需能识别句子的模态类别（如可能、不可能、荒谬），但近期研究质疑了它们在模态分类方面的能力。本研究旨在深入探究语言模型对模态的理解能力。

Method: 通过机制可解释性技术，识别语言模型内部能区分模态类别的线性表征（即“模态差异向量”）。分析这些向量的特性及其在模型训练、层级和参数增长过程中的演变。将模态差异向量与人类对可解释特征的评分相关联，以探究人类的模态分类机制。

Result: 语言模型拥有比先前报告更可靠的模态分类判断能力。模态差异向量随模型能力的提升（训练步数、层数、参数量）以一致的顺序出现。这些向量能用于模拟人类精细的模态分类行为。

Conclusion: 本研究利用机制可解释性为语言模型的模态分类提供了新见解，并有潜力增进我们对人类模态分类机制的理解。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 本文发布了首个开源的车臣语与俄语之间的翻译模型及其数据集，并探索了将新语言集成到大型多语言模型NLLB-200中的微调能力。


<details>
  <summary>Details</summary>
Motivation: 为濒危的车臣语与俄语之间提供翻译能力，并构建首个开源的翻译模型和数据集，以填补该语言对的空白。

Method: 收集车臣语与俄语的平行语料库，并对大型多语言翻译系统NLLB-200进行微调，以包含车臣语。

Result: 模型在俄语到车臣语方向的BLEU/ChrF++分数分别为8.34/34.69，反方向分别为20.89/44.55。同时，研究发布了翻译模型、平行词汇/短语/句子语料库以及适应车臣语的多语言句子编码器。

Conclusion: 成功开发并开源了车臣语与俄语间的翻译模型及相关数据资源，为濒危语言的机器翻译提供了重要工具和数据支持。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 该研究利用自然语言处理（NLP）模型从法医报告的自由文本中自动识别药物过量死亡，特别是发现微调后的BioClinicalBERT在准确性和可扩展性方面表现出色，可显著加速药物滥用趋势的监测。


<details>
  <summary>Details</summary>
Motivation: 美国芬太尼导致的药物相关死亡率上升，需要及时准确的监测。然而，关键的过量数据常埋藏于自由文本法医报告中，手动编码至ICD-10分类导致延迟和信息丢失。之前的NLP应用有限。

Method: 研究使用2020年35,433份死亡记录用于模型训练和内部测试，并用2023-2024年3,335份新记录进行外部验证。评估了多种NLP方法，包括传统分类器、微调的编码器模型（如BERT、BioClinicalBERT）和解码器大型语言模型（如Qwen 3、Llama 3），以从非结构化死亡证明文本中分类特定药物参与。性能评估使用宏平均F1分数。

Result: 微调后的BioClinicalBERT模型在内部测试集上取得了接近完美的表现（宏F1分数≥0.998）。外部验证也证实了其稳健性（宏F1=0.966），表现优于传统机器学习、通用BERT模型和各种解码器大型语言模型。

Conclusion: NLP模型，特别是经过微调的临床专用变体如BioClinicalBERT，为从自由文本报告中分类过量死亡提供了一种高精度和可扩展的解决方案。这些方法能够显著加速监测工作流程，克服手动ICD-10编码的局限性，并支持近乎实时的药物滥用趋势检测。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: AdaptiSent是一个多模态方面情感分析（MABSA）新框架，通过自适应跨模态注意力机制，显著提升了情感分类和方面词提取的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多模态方面情感分析中可能未能有效捕捉细微的模态间关系，且无法动态调整关注点以提高情感和方面词提取的深度与准确性。

Method: 引入AdaptiSent框架，该框架采用自适应跨模态注意力机制，并整合了动态模态加权和上下文自适应注意力，通过聚焦文本线索与视觉上下文的交互来增强信息提取。

Result: 在标准Twitter数据集上的测试表明，AdaptiSent在准确率、召回率和F1分数上均超越了现有模型，尤其在识别细微模态间关系方面表现出色，其有效性源于模型动态调整关注焦点的能力。

Conclusion: AdaptiSent为多模态方面情感分析（MABSA）设定了新标准，显著优于现有方法，特别是在理解复杂多模态信息方面表现突出。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: EaGERS是一个无需训练、与模型无关的流水线，通过生成自然语言理由、将其与图像区域关联，并限制响应生成区域，从而提升DocVQA性能、透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA模型可能缺乏透明度和可复现性，且需要额外的模型微调。本研究旨在无需微调的情况下，提高DocVQA的准确性并增强其可解释性。

Method: EaGERS包含三个步骤：1) 使用视觉语言模型生成自然语言理由；2) 通过计算多模态嵌入相似度并结合多数投票，将理由映射到图像的子区域；3) 仅从被选择的相关区域生成响应。

Result: 在DocVQA数据集上的实验表明，最佳配置的EaGERS不仅在精确匹配准确率和平均标准化莱文斯坦相似度指标上优于基础模型，而且在不进行额外模型微调的情况下，增强了DocVQA的透明度和可复现性。

Conclusion: EaGERS是一种有效的方法，可以在不进行额外训练的情况下，显著提升DocVQA任务的性能，并增强模型的解释性和可靠性。

Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 本研究引入了一个新型多智能体AI辅导平台，旨在解决现有AI辅导系统（尤其在数学领域）的被动性问题，以提供结构化、个性化和工具辅助的学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前的AI辅导系统（特别是数学领域的）过于被动，仅提供直接答案，未能鼓励深度思考或整合结构化教学策略。研究旨在解决如何让AI辅导系统超越被动协助，实现结构化、个性化和工具辅助的学习体验这一问题。

Method: 开发了一个新型多智能体AI辅导平台，该平台结合了自适应和个性化反馈、结构化课程生成和教材知识检索，以支持模块化、工具辅助的学习过程。

Result: 该系统使学生能够学习新知识、识别并针对性地解决弱点、有效复习考试，并进行无限量的个性化练习。

Conclusion: 本文通过引入一个结合了教学智能体和AI驱动组件的新型平台，为教育人工智能领域做出了贡献，为数学教学提供了模块化且高效的系统。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [8] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 改进高速公路并道模拟，通过结合博弈论和动力学模型，生成更真实且计算高效的类人驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术发展急需模拟真实人类驾驶行为。现有高速公路并道模拟方法存在局限：或侧重操作层面，或战术决策模型行动集受限/效用函数复杂。本研究旨在通过改进战术决策模型来提升高速公路并道场景的模拟。

Method: 开发了一种改进效用函数和滞后动作的战术决策博弈论模型。将其与底层动力学模型结合，构建了统一的决策与动力学模型，旨在捕捉并道交互并实现可解释的真实模拟。

Result: 该模型在真实世界数据集上验证时，能很好地再现复杂交互。已集成到高保真模拟环境中，并证实具有足够的计算时间效率，适用于大规模仿真。

Conclusion: 所提出的统一博弈论与动力学模型，通过生成更真实、可解释且计算高效的类人智能体行为，有效改进了高速公路并道场景的模拟，为自动驾驶汽车开发提供了支持。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 研究展示了通过长时间强化学习，小型语言模型在数学、编码和逻辑谜题等推理任务上取得了显著进步，并提出了关键的训练技术。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型通过强化学习和可验证奖励在复杂推理任务（如数学和代码生成）中表现出色。本研究旨在探讨长时间强化学习对小型语言模型在多样推理领域的影响，并识别有效的训练要素。

Method: 研究采用强化学习，并改进了群组相对策略优化（GRPO）算法。引入了控制KL正则化、剪裁比率和周期性参考策略重置等技术以提高训练稳定性、泛化能力和长期性能。训练过程中使用了可验证的奖励任务。

Result: 该模型在数学任务上性能提升14.7%，在编码任务上提升13.9%，在逻辑谜题任务上提升54.8%，显著优于现有基线。

Conclusion: 通过长时间强化学习，结合可验证奖励任务、GRPO增强及特定的训练稳定化技术（如控制KL正则化、剪裁比率和周期性参考策略重置），可以有效提升小型语言模型在多领域推理任务上的表现。研究公开了模型以促进后续研究。

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [10] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 本文提出一种结合生成式AI增强深度强化学习（DRL）和速率分裂多址（RSMA）的方案，以优化无人机（UAV）低空移动边缘计算（MEC）系统的能效，同时有效缓解上行干扰。


<details>
  <summary>Details</summary>
Motivation: 6G对低延迟计算的需求推动了无人机低空MEC系统的发展。然而，有限的频谱导致地面终端（GTs）之间存在严重的上行干扰，需要有效的解决方案来提高能效并缓解干扰。

Method: 研究RSMA赋能的无人机低空MEC系统，其中无人机作为边缘服务器协助GTs卸载任务。构建一个联合优化问题，包括无人机3D轨迹、RSMA解码顺序、任务卸载决策和资源分配，旨在最大化能效和缓解多用户干扰。提出一个生成式AI增强的深度强化学习（DRL）框架来解决该高维、非凸且动态的优化问题。具体地，将扩散模型嵌入到actor网络中以生成高质量的动作样本，改善混合动作空间的探索并避免局部最优。此外，设计了一种基于优先级的RSMA解码策略，以实现低复杂度的有效连续干扰消除。

Result: 仿真结果表明，所提出的针对低空MEC系统的方法优于基线方法。将生成扩散模型（GDM）与RSMA集成可以显著提高能效性能。

Conclusion: 所提出的结合生成式AI增强DRL和RSMA的方法，能有效优化无人机低空MEC系统，显著提高能效并缓解干扰，为6G应用提供了一种高性能解决方案。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [11] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: 为解决6G AI RAN中用户意图到低层网络配置的映射难题，本文提出了多智能体框架RIDAS。它利用大语言模型将用户意图转化为最优资源配置，实验证明其在支持更多用户方面优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 6G网络要求AI与无线接入网络（RAN）紧密集成，以满足严格的服务质量（QoS）和资源效率需求。然而，现有解决方案难以弥合高层用户意图与低层参数化配置之间的差距，影响了最佳性能。

Method: 本文提出了RIDAS多智能体框架，由表征驱动代理（RDAs）和意图驱动代理（IDA）组成。RDAs暴露可调控制参数（如秩和量化比特），实现失真与传输速率的权衡。IDA采用大语言模型（LLM）驱动的两阶段规划（带宽预分配和再分配），将用户意图和系统状态映射为最优的RDA配置。

Result: 实验结果表明，在同等QoS约束下，RIDAS比WirelessAgent能够多支持44.71%的用户。

Conclusion: RIDAS能够有效捕获用户意图，并在AI RAN环境中更高效地分配资源，验证了其能力和优势。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>
