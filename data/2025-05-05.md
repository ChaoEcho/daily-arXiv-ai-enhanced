<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TL;DR: 提出了一种基于BERT的金融问答系统（FinBERT-QA），通过两阶段检索和重排序，显著提升了金融领域非事实性问答的性能。


<details>
  <summary>Details</summary>
Motivation: 金融行业对自动分析大规模非结构化和结构化数据的需求日益增长，问答系统能辅助金融顾问决策，但现有系统在金融领域面临数据稀缺和语言 специфичность 的限制。

Method: 将答案选择视为重排序问题。系统包含两部分：1) 使用BM25的答案检索器初步筛选候选答案列表；2) 使用预训练BERT模型变体的答案重排序器对候选答案进行重排序，选出最相关的答案。研究了多种BERT的学习、进一步预训练和微调方法。

Result: 实验表明，采用“迁移和适应”（Transfer and Adapt）进一步微调和逐点学习方法构建的FinBERT-QA模型效果最好，在FiQA数据集的任务2上，其MRR提升了16%，NDCG提升了17%，Precision@1提升了21%，优于当前最佳水平。

Conclusion: 提出的FinBERT-QA系统通过结合BM25检索和优化的BERT重排序，有效解决了金融问答的挑战，并显著提高了性能。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>


### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 这篇论文首次全面综述了基于大语言模型的人机协作系统 (LLM-HAS)，旨在通过整合人类反馈和控制来提升LLM智能体的性能、可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 完全自主的LLM智能体面临幻觉、处理复杂任务能力有限以及安全伦理风险等挑战，限制了其在现实世界中的应用。

Method: 本文对LLM-HAS进行了系统性的文献综述，阐述了基本概念，梳理了系统的核心组成部分（如环境、人类反馈、交互类型、编排通信），探讨了新兴应用，并讨论了挑战与机遇。

Result: 该研究提供了一个关于LLM-HAS的结构化概述，整合了当前知识，明确了构建这些系统的关键要素，并指出了人机协作的优势。

Conclusion: LLM-HAS是弥补纯LLM智能体不足的有前景的方法。本综述旨在通过提供一个结构化的概览，促进该交叉学科领域的进一步研究和创新。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TL;DR: 本文通过一个新的几何图形推理基准测试，系统评估了大型语言模型（LLMs）的简单推理能力及其对提示（prompt）的依赖性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理方面表现出色，但它们处理简单推理任务的能力常受质疑。本研究旨在全面分析LLMs的推理能力，特别是其对提示的依赖程度。

Method: 1. 构建了一个新的基准数据集，包含需要浅层逻辑推理的简单几何图形问题，排除世界知识干扰。 2. 对24个不同规模的LLMs进行了零样本（zero-shot）和少样本（few-shot）提示测试。 3. 对22个LLMs进行了思维链（chain-of-thought）提示测试，并比较了推理过程在答案之前或之后呈现的效果。

Result: 参数超过700亿的大型模型在零样本设置下表现更好，但仍有很大提升空间。思维链提示的效果（提升或损害性能）取决于推理过程是在答案之前还是之后被要求提供。

Conclusion: LLMs的简单推理能力很大程度上受模型规模和提示方式的影响，即使是大型模型也存在不足。思维链提示的应用需要谨慎设计才能有效提升性能。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>


### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TL;DR: 本研究系统评估了在统一框架下，向预训练语言模型（PLMs）添加上下文信息（如实体描述、知识图谱信息、分子结构）对生物医学关系抽取性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明上下文信息能提升PLMs在关系抽取任务上的表现，但因模型、数据、优化和评估方法的差异，难以直接比较和推广研究结果。本研究旨在填补这一空白。

Method: 研究首先评估了三种基线PLMs，并进行了广泛的超参数优化。然后，选取表现最佳的模型，并使用文本实体描述、知识图谱关系信息和分子结构编码等额外数据对其进行增强。整个评估过程在包含五种数据集（涵盖四种关系场景）的统一框架下进行，并包含消融研究。

Result: 研究发现，选择合适的底层语言模型和进行全面的超参数优化对于达到高抽取性能至关重要。加入上下文信息带来的整体性能提升较小，但消融研究表明，对于较小的PLMs，在微调时加入这些外部数据能带来显著的好处。

Conclusion: 基础模型的选择和超参数优化是生物医学关系抽取的关键。虽然添加上下文信息对大型模型的整体增益有限，但可以显著改善较小PLMs的性能。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>


### [5] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TL;DR: 该研究开发了DynaWrite应用，评估了LLM在英语语法动态评估（DA）中的扩展潜力，发现GPT-4o在提供高质量动态反馈方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统的动态评估难以规模化应用于大型学习群体，研究旨在探索大语言模型（LLM）是否能解决这一问题。

Method: 开发了模块化的语法辅导应用DynaWrite；初步测试了21个LLM；筛选出GPT-4o和neural chat进行深入测试，比较它们识别语法错误和生成DA提示的质量；进行了性能和稳定性测试。

Result: GPT-4o和neural chat在识别语法错误方面表现相似，但GPT-4o在生成清晰、一致且渐进明确的DA提示方面显著更优，并表现出足够的速度和稳定性。

Conclusion: 研究证明LLM（特别是GPT-4o）可用于有效扩展动态评估，使其能服务于比传统师生互动更大的群体。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>


### [6] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TL;DR: 推出 Llama-Nemotron 系列模型，这是一个开放的、具有优异推理能力和高效率的异构推理模型家族（8B、49B、253B），并提供开放的企业使用许可。


<details>
  <summary>Details</summary>
Motivation: 创建与顶尖推理模型（如 DeepSeek-R1）性能相当，但具有更高推理吞吐量和内存效率，并采用开放许可的推理模型，以满足企业需求和促进开放研究。

Method: 使用基于 Llama 3 的神经架构搜索（NAS）以加速推理，结合知识蒸馏、持续预训练，以及专注于推理的后训练阶段（包括监督微调和大规模强化学习）。引入了动态推理切换功能。

Result: 开发了 Llama-Nemotron 系列模型（Nano、Super、Ultra），性能与先进模型相当，推理效率更优。模型、后训练数据集和训练代码库（NeMo, NeMo-Aligner, Megatron-LM）均已开源，并首次在开源模型中支持动态推理切换。

Conclusion: Llama-Nemotron 系列提供了一套强大、高效且开放许可的推理模型，通过发布模型、数据和代码促进了开放研究和模型开发，其动态推理切换功能是一大特色。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [7] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TL;DR: 提出一种基于字符扩散的嵌入算法（CDEA），结合XLNet模型，利用敏感信息特性提升生成式语言隐写的文本质量和不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式语言隐写模型能力有限，且嵌入算法难以消除敏感信息特性（如语义、随机性）的负面影响，导致为保证信息提取而选择低概率词，损害了隐写文本的语义连贯性和流畅性，降低了文本质量。

Method: 提出字符级扩散嵌入算法（CDEA），该算法利用敏感信息的字符级统计特性和基于幂律分布的分组方法，提高高概率候选词的选择频率，降低低概率候选词的选择频率。同时引入XLNet模型处理长序列中的敏感信息转换。

Result: 实验结果表明，CDEA与XLNet的结合显著提高了生成隐写文本的质量，尤其在感知不可察觉性方面有明显改善。

Conclusion: 所提出的CDEA算法，特别是与XLNet结合使用时，通过利用敏感信息自身的特性，有效提升了生成式隐写文本的质量和隐蔽性。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [8] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TL;DR: 提出一种名为SoG的图上合成数据生成框架，通过构建知识图谱捕获跨文档关联，提升LLM在小语料库上的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理小型、专业、数据有限的语料库时效率低下，现有合成数据方法缺乏跨文档知识关联，限制了内容多样性和深度。

Method: 构建上下文图谱（提取实体概念表示跨文档关联），采用图游走策略进行知识关联采样生成合成数据，并结合思维链（CoT）和对比澄清（CC）提高数据质量。

Result: SoG在多跳文档问答数据集上优于当前最优（SOTA）方法，在阅读理解任务上表现与SOTA相当，并展现出更好的泛化能力。

Conclusion: SoG框架推动了合成数据生成技术的发展，为数据有限领域中LLM的高效知识获取提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes](https://arxiv.org/abs/2505.00734)
*Neil Joshi,Joshua Carney,Nathanael Kuo,Homer Li,Cheng Peng,Myron Brown*

Main category: cs.CV

TL;DR: 发布了一个包含地面、安防和空中多视角图像的公开基准数据集，旨在推动解决紧急响应场景下因图像数据受限而难以进行三维重建和新视角合成的问题。


<details>
  <summary>Details</summary>
Motivation: 在灾难救援或执法等紧急响应场景中，用于创建真实感三维模型的图像数据往往数量有限、来源多样（未标定相机）、光照不一致且视角差异巨大，现有方法难以有效重建。

Method: 构建并发布了首个结合了多种已校准的地面、安防高度和空中相机视角的公开基准数据集，专门用于三维重建和新视角合成的研究。

Result: 提供了能反映真实世界挑战的数据集；独立评估了未标定相机的标定效果和新视角渲染的质量；使用现有先进方法展示了基线性能；并指出了未来研究面临的挑战。

Conclusion: 该基准数据集的发布旨在促进学术界研发能够应对图像稀疏、相机异构、光照和视角变化等挑战的三维重建与新视角合成技术。

Abstract: Production of photorealistic, navigable 3D site models requires a large
volume of carefully collected images that are often unavailable to first
responders for disaster relief or law enforcement. Real-world challenges
include limited numbers of images, heterogeneous unposed cameras, inconsistent
lighting, and extreme viewpoint differences for images collected from varying
altitudes. To promote research aimed at addressing these challenges, we have
developed the first public benchmark dataset for 3D reconstruction and novel
view synthesis based on multiple calibrated ground-level, security-level, and
airborne cameras. We present datasets that pose real-world challenges,
independently evaluate calibration of unposed cameras and quality of novel
rendered views, demonstrate baseline performance using recent state-of-practice
methods, and identify challenges for further research.

</details>


### [10] [MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection](https://arxiv.org/abs/2505.00739)
*Qiushi Yang,Yuan Yao,Miaomiao Cui,Liefeng Bo*

Main category: cs.CV

TL;DR: MoSAM 通过引入运动引导提示和时空记忆选择机制，改进了 SAM2 在视频对象分割中的性能，解决了其对物体消失/遮挡敏感和缺乏运动感知的问题。


<details>
  <summary>Details</summary>
Motivation: 最新的 SAM2 模型在交互式分割上表现出色，但直接依赖过去固定帧的掩码记忆进行视频分割存在两大挑战：1) 缺乏物体运动信息，限制了长距离跟踪能力，导致物体消失时难以处理；2) 固定帧记忆易受物体消失或遮挡影响，因记忆中可能包含不准确的分割结果。

Method: 提出了 MoSAM 模型，包含两个关键策略：1) 运动引导提示 (MGP)：以稀疏和密集方式表示物体运动，并通过一组运动引导提示将其注入 SAM2，使模型关注运动方向，增强跟踪能力；2) 时空记忆选择 (ST-MS)：在像素级和帧级动态识别可能包含准确分割的帧，剔除潜在不准确的掩码预测，利用更可靠的记忆特征改进分割。

Result: 在多个视频对象分割 (VOS) 和视频实例分割 (VIS) 基准测试上的大量实验表明，MoSAM 相较于其他竞争方法取得了当前最佳 (SOTA) 结果。

Conclusion: 通过整合运动线索并建立更可靠的特征记忆，MoSAM 显著提高了基于 SAM2 的视频对象分割性能，尤其是在处理长距离跟踪、物体消失和遮挡等挑战方面。

Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional
capabilities in interactive object segmentation for both images and videos.
However, as a foundational model on interactive segmentation, SAM2 performs
segmentation directly based on mask memory from the past six frames, leading to
two significant challenges. Firstly, during inference in videos, objects may
disappear since SAM2 relies solely on memory without accounting for object
motion information, which limits its long-range object tracking capabilities.
Secondly, its memory is constructed from fixed past frames, making it
susceptible to challenges associated with object disappearance or occlusion,
due to potentially inaccurate segmentation results in memory. To address these
problems, we present MoSAM, incorporating two key strategies to integrate
object motion cues into the model and establish more reliable feature memory.
Firstly, we propose Motion-Guided Prompting (MGP), which represents the object
motion in both sparse and dense manners, then injects them into SAM2 through a
set of motion-guided prompts. MGP enables the model to adjust its focus towards
the direction of motion, thereby enhancing the object tracking capabilities.
Furthermore, acknowledging that past segmentation results may be inaccurate, we
devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically
identifies frames likely to contain accurate segmentation in both pixel- and
frame-level. By eliminating potentially inaccurate mask predictions from
memory, we can leverage more reliable memory features to exploit similar
regions for improving segmentation results. Extensive experiments on various
benchmarks of video object segmentation and video instance segmentation
demonstrate that our MoSAM achieves state-of-the-art results compared to other
competitors.

</details>


### [11] [Fast2comm:Collaborative perception combined with prior knowledge](https://arxiv.org/abs/2505.00740)
*Zhengbin Zhang,Yan Wu,Hongkun Zhang*

Main category: cs.CV

TL;DR: 提出Fast2comm框架，利用先验知识优化协同感知，以应对带宽限制和定位误差挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的协同感知在平衡感知性能与带宽限制以及处理定位误差方面存在持续挑战。

Method: 提出Fast2comm框架：(1) 采用先验监督的置信度特征生成方法区分前景背景；(2) 采用基于GT Bbox的空间先验特征选择策略，筛选信息特征，减少背景噪声，优化带宽，适应定位误差；(3) 解耦训练和测试阶段的特征融合策略以适应动态带宽。

Result: 在真实世界和模拟数据集上的大量实验证明了该模型的优越性能，并突显了所提出方法的必要性。

Conclusion: Fast2comm框架通过利用先验知识，有效提升了协同感知在带宽受限和定位不精确场景下的性能。

Abstract: Collaborative perception has the potential to significantly enhance
perceptual accuracy through the sharing of complementary information among
agents. However, real-world collaborative perception faces persistent
challenges, particularly in balancing perception performance and bandwidth
limitations, as well as coping with localization errors. To address these
challenges, we propose Fast2comm, a prior knowledge-based collaborative
perception framework. Specifically, (1)we propose a prior-supervised confidence
feature generation method, that effectively distinguishes foreground from
background by producing highly discriminative confidence features; (2)we
propose GT Bounding Box-based spatial prior feature selection strategy to
ensure that only the most informative prior-knowledge features are selected and
shared, thereby minimizing background noise and optimizing bandwidth efficiency
while enhancing adaptability to localization inaccuracies; (3)we decouple the
feature fusion strategies between model training and testing phases, enabling
dynamic bandwidth adaptation. To comprehensively validate our framework, we
conduct extensive experiments on both real-world and simulated datasets. The
results demonstrate the superior performance of our model and highlight the
necessity of the proposed methods. Our code is available at
https://github.com/Zhangzhengbin-TJ/Fast2comm.

</details>


### [12] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TL;DR: 利用卷积神经网络(CNN)和长短期记忆网络(LSTM)对植物叶片病害进行分类，CNN模型表现出更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重影响作物产量和食品质量，因此早期准确的检测和分类对于减少损失和改善作物管理至关重要。

Method: 使用包含70,295张训练图像和17,572张验证图像（涵盖38个病害类别）的数据集，应用了CNN和LSTM模型。CNN模型使用Adam优化器（学习率0.0001）和分类交叉熵损失函数进行训练，共训练10个周期。

Result: CNN模型在训练集上达到99.1%的准确率，在验证集上达到96.4%的准确率。LSTM模型的验证准确率为93.43%。通过精确率、召回率、F1分数和混淆矩阵评估了模型性能，确认了CNN方法的可靠性。

Conclusion: 深度学习模型，特别是CNN，为植物病害的准确和可扩展分类提供了一个有效的解决方案，可支持农业监测中的实际应用。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>


### [13] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TL;DR: 提出了一种名为 \SysName 的新视觉提示机制，通过动态高亮区域、保持空间信息和平衡预算，提升多模态大模型（MLLM）处理视觉细节的能力，同时减少 token 消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在处理需要精确物体识别和精细视觉细节的任务时表现不佳，且严格的 token 限制常导致关键视觉信息丢失。

Method: 引入 \SysName 视觉提示机制，包含三大创新：1) 感知提示的策略动态高亮相关图像区域；2) 空间保持的编排模式维护对象完整性；3) 感知预算的提示方法平衡全局上下文与关键视觉细节。

Result: 在多个数据集上的评估显示，\SysName 比基线方法表现更优，准确率最高提升了 26.9%，并显著降低了 token 消耗。

Conclusion: \SysName 能够有效增强 MLLM 在 token 限制内处理精确视觉信息的能力，提升了模型性能。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva,Juliane Päßler,S. Lizeth Tapia Tarifa,Einar Broch Johnsen,Carlos Hernández Corbato*

Main category: cs.AI

TL;DR: 提出了一种名为ROSA的基于知识的新框架，用于机器人自我适应，实现任务和架构的协同适应（TACA）。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要在多样化环境和不确定性下处理多任务，这给软件架构和任务决策算法设计带来了挑战，因为不同情境需要不同的任务逻辑和架构配置。

Method: ROSA利用一个知识模型捕获适应所需的特定应用知识，并在运行时基于该知识进行推理，以决定何时以及如何进行任务执行和软件架构的适应。提供了一个概念框架和基于ROS 2的开源参考实现。

Result: 在水下机器人应用中的实验评估证明了ROSA的可行性和性能，并突显了其在设计自适应机器人系统方面的可重用性和开发效率优势。

Conclusion: ROSA通过基于知识的方法，成功实现了机器人系统的任务与架构协同适应（TACA），提高了系统的灵活性、可重用性并降低了开发难度。

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>


### [15] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 本文针对霍华德策略迭代（HPI）算法在确定性马尔可夫决策过程（DMDPs）上的运行时间，提出了一个新的次指数级上界。


<details>
  <summary>Details</summary>
Motivation: 尽管HPI算法已存在60多年，但其已知的运行时间上界仍然是指数级的（即使在DMDPs上），与已知的线性下界之间存在巨大差距，需要更精确的分析。

Method: 通过理论分析，研究HPI在DMDPs上的运行时间，推导出依赖于奖励值比特大小、不依赖于折扣因子的新上界。

Result: 为HPI在DMDPs上找到了一个次指数级的运行时间上界。此上界与奖励值的比特大小相关，与折扣因子无关，并且同样适用于只有两种奖励值的DMDPs。

Conclusion: 该研究显著改进了对HPI效率的理论理解，为DMDPs提供了首个次指数级上界，缩小了其运行时间上界与下界之间的差距。

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>


### [16] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou,Danae Pla Karidi,Evaggelia Pitoura,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.AI

TL;DR: 本文提出利用AI可解释性方法来检测和解读不公平性，并探讨了此方法的关键问题和潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在关键领域日益普及，其对受保护群体可能产生的偏见引发了对公平性和透明度的担忧，需要有效方法来识别和理解这些偏见。

Method: 提出了一个集成局部事后解释（local post-hoc explanation）方法的流程来获取公平性相关的见解。研究了使用解释作为偏见检测器时出现的关键问题，如不同公平性概念的关系、移除受保护属性的影响、不同解释方法的一致性、聚合策略的影响以及解释本身的可信度。

Result: 结果表明，解释方法在用于公平性分析方面具有潜力，但也强调了仔细考虑方法选择、聚合策略和解释可信度等关键方面的重要性。

Conclusion: 利用可解释性方法检测和解读AI不公平性是可行的，但需要谨慎处理相关的方法论挑战，以确保评估的可靠性。

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>


### [17] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang,Xing Niu,Juyong Kim,Jie Shen,Tong Zhang,Jeremy C. Weiss*

Main category: cs.AI

TL;DR: 发布了一个包含 22,588,586 个临床时间序列事件的新数据集 (MIMIC-IV-Ext-22MCTS)，该数据集从非结构化的 MIMIC-IV-Note 出院小结中提取，并展示了其在提升医疗应用模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的临床数据集（如 MIMIC-IV-Note）是非结构化的，文本过长难以处理，且许多临床事件缺乏明确的时间戳，这阻碍了开发可靠的基于机器学习的临床风险预测模型。因此，需要一个高质量的时间序列临床事件数据集。

Method: 提出了一种新框架：1) 将冗长的出院小结分割成可管理的小文本块；2) 应用上下文 BM25 和上下文语义搜索来检索可能包含临床事件的文本块；3) 精心设计提示 (prompts) 来指导 Llama-3.1-8B 模型识别或推断文本块的时间信息，从而提取带有时间戳的临床事件。

Result: 成功创建了包含超过 2200 万个事件的 MIMIC-IV-Ext-22MCTS 数据集。基于该数据集微调的标准模型在医疗应用中表现显著提升：BERT 模型在医学问答任务上准确率提高 10%，在临床试验匹配任务上提高 3%；GPT-2 模型在临床问题上生成更可靠的结果。

Conclusion: 该研究成功构建了一个信息丰富且透明的大型临床时间序列数据集，证明了所提出框架的有效性。该数据集显著提高了标准模型在下游医疗保健任务中的性能。

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>


### [18] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike,Emanuel Moss,Elizabeth Anne Watkins,Saurav Sahay,Giuseppe Raffa,Lama Nachman*

Main category: cs.AI

TL;DR: 研究发现，在多智能体流程中，思维链（CoT）本身既不能提高输出质量，也不能为用户提供有效的可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体流程（多个LLM协同工作，人工干预少）给以人为中心的AI可解释性（HCXAI）带来了新的挑战和机遇，需要探索如何有效解释其内部运作。

Method: 实现了一个感知任务指导系统的多智能体流程，并通过定量和定性分析，评估了其中思维链（CoT）推理作为解释机制的效果。

Result: 单独使用思维链（CoT）推理并未改善多智能体流程的输出结果，也未能提供有效的可解释性。它产生的解释更像是“没有解释力的解释”，无法帮助最终用户更好地理解系统或实现目标。

Conclusion: 在多智能体流程中，仅靠思维链（CoT）不足以实现有效的可解释性，需要探索新的方法。

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre,Ana L. C. Bazzan,André Barreto,Bruno C. da Silva*

Main category: cs.LG

TL;DR: 提出了一种构建最优行为基的新方法，用于多任务强化学习，可以零样本识别线性任务的最优解，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务强化学习方法（如GPI、CCS、OK）在零样本泛化到新任务时存在计算成本高、依赖基础策略选择或无法保证最优性的问题。特别是，缺乏一个最优的基础策略集（行为基）来高效解决任意线性任务。

Method: 引入了一种新颖的方法，用于高效地构建一个“最优行为基”。这个行为基由一组基础策略组成，旨在能够零样本地组合出任何线性任务的最优策略。

Result: 该方法显著减少了确保新任务最优性所需的基础策略数量。理论证明，它比凸覆盖集（CCS）表达能力更强，能够最优地解决特定类别的非线性任务。实验证明，该技术在挑战性领域优于现有最先进方法，并且任务越复杂，优势越明显。

Conclusion: 该研究成功解决了一个开放问题，提供了一种构建最优行为基的方法，从而能够更高效、更具表达力地实现多任务强化学习中的零样本最优泛化。

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>


### [20] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen,Ngoc N. Tran,Khai Nguyen,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该研究提出利用 token 间相似性或注意力信息指导专家路由的新型 (S)MoE 方法（Similarity/Attention-Aware (S)MoE），以解决标准 SMoE 模型的路由波动和不鲁棒问题。


<details>
  <summary>Details</summary>
Motivation: 标准的稀疏专家混合（SMoE）模型在训练后期存在路由波动，导致模型性能下降和不鲁棒。研究认为这是因为其专家选择机制忽略了 token 间的相互作用（独立性假设）。

Method: 1. 使用概率图模型（PGM）框架分析并揭示了 SMoE 的局限性。 2. 提出了 Similarity-Aware (S)MoE，在专家选择中考虑 token 间的交互。 3. 提出了 Attention-Aware (S)MoE，利用注意力矩阵指导 token 路由。 4. 理论证明新方法有助于降低专家选择的熵，稳定路由。

Result: 在多个任务和领域的实验表明，与基线 MoE-Transformer 相比，提出的 Similarity-Aware 和 Attention-Aware (S)MoE 模型显著减少了路由波动，提高了准确率和模型鲁棒性。

Conclusion: 通过在 SMoE 的专家路由机制中考虑 token 间的相似性或注意力信息，可以有效提升模型的稳定性、准确性和鲁棒性。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>


### [21] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev,Dan A Calian,Luisa M Zintgraf,Gregory Farquhar,Hado van Hasselt*

Main category: cs.LG

TL;DR: 提出了一种名为MixFlow-MG的混合模式微分算法，用于优化双层优化中的元梯度计算，显著提高了内存和时间效率。


<details>
  <summary>Details</summary>
Motivation: 标准的自动微分库在处理双层优化中的“梯度之梯度”计算时，无法充分利用问题结构，导致计算性能（内存和时间）不佳。

Method: 分析了现有方法的不足，并提出了MixFlow-MG算法，该算法使用混合模式微分（mixed-mode differentiation）来构建更高效、可扩展的计算图。

Result: 与标准实现相比，MixFlow-MG在现代元学习设置中，内存占用减少了10倍以上，运行时间缩短了高达25%。

Conclusion: MixFlow-MG是一种实用且高效的算法，通过优化计算图，显著改善了基于梯度的双层优化中元梯度计算的性能。

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [22] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 本文定义了机制可解释性（MI），认为神经网络包含可提取的内在解释，并提出了评估解释和MI成功的原则。


<details>
  <summary>Details</summary>
Motivation: 为机制可解释性（MI）研究提供一个原则性的理论基础，证明其作为理解神经网络方法的合理性。

Method: 提出“解释性视图假说”；定义“解释性忠实度”；将MI定义为产生模型级、实体性、因果机制性、可证伪的解释；阐述“解释性乐观原则”。

Result: 论证了MI是一种原则性的方法；表明“解释性忠实度”是明确定义的；提供了MI的正式定义，区分了MI与其他范式并明确了其局限性；提出了MI成功的必要前提“解释性乐观原则”。

Conclusion: 机制可解释性是一种独特的、有原则的理解神经网络的方法，其通过提取模型内在的因果解释实现，但其成功可能依赖于解释性乐观原则。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>


### [23] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: 开发并评估了一种可扩展的方法，用于统一大规模临床数据集中的不一致单位，显著提高了数据互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模临床数据集中测量单位不一致的问题，该问题是实现数据互操作性的关键障碍。

Method: 设计了一个结合 BM25、句子嵌入、贝叶斯优化和基于双向 Transformer 的二元分类器的新型单位统一系统。该系统采用多阶段流程：过滤、识别、统一方案生成、自动重排序和手动验证。

Result: 混合检索方法（BM25+句子嵌入）的平均倒数排名（MRR）为 0.8833，显著优于纯词汇（0.7985）和纯嵌入（0.5277）方法。Transformer 重排序器将最终 MRR 提升至 0.9833。系统在排名 1 时的精确度为 83.39%，在排名前 5 时的召回率为 94.66%。

Conclusion: 该框架提供了一种高效、可扩展的临床数据单位统一解决方案，减少了人工工作量，提高了准确性，使统一后的数据能够无缝重用于多机构研究和元分析。

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [24] [SeLR: Sparsity-enhanced Lagrangian Relaxation for Computation Offloading at the Edge](https://arxiv.org/abs/2505.00848)
*Negar Erfaniantaghvayi,Zhongyuan Zhao,Kevin Chan,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 提出了一种新的迭代凸优化计算方法，用于将传感器数据处理任务卸载到边缘网络服务器，以优化准确性和处理时间。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据处理任务卸载中的准确性和延迟权衡问题。传统的混合整数规划（MIP）模型是非凸且NP难的，难以高效求解。

Method: 将离线卸载和路由问题建模为MIP。通过整数变量松弛、原始对偶优化和重加权L1最小化，将非凸问题转化为迭代凸优化问题求解。

Result: 与贪心启发式算法相比，该方法获得了更好的准确性与延迟的帕累托前沿；扩展性更好；相比最优求解器，在特定规模网络中显著降低了调度计算开销（7.72-9.17倍）。

Conclusion: 该迭代凸优化方法能有效求解传感器数据卸载问题，在解的质量、可扩展性和计算效率方面优于现有方法。

Abstract: This paper introduces a novel computational approach for offloading sensor
data processing tasks to servers in edge networks for better accuracy and
makespan. A task is assigned with one of several offloading options, each
comprises a server, a route for uploading data to the server, and a service
profile that specifies the performance and resource consumption at the server
and in the network. This offline offloading and routing problem is formulated
as mixed integer programming (MIP), which is non-convex and HP-hard due to the
discrete decision variables associated to the offloading options. The novelty
of our approach is to transform this non-convex problem into iterative convex
optimization by relaxing integer decision variables into continuous space,
combining primal-dual optimization for penalizing constraint violations and
reweighted $L_1$-minimization for promoting solution sparsity, which achieves
better convergence through a smoother path in a continuous search space.
Compared to existing greedy heuristics, our approach can achieve a better
Pareto frontier in accuracy and latency, scales better to larger problem
instances, and can achieve a 7.72--9.17$\times$ reduction in computational
overhead of scheduling compared to the optimal solver in hierarchically
organized edge networks with 300 nodes and 50--100 tasks.

</details>


### [25] [EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing](https://arxiv.org/abs/2505.01185)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 提出一种结合自适应滤波和环境参数的增强型LoRaWAN室内定位方法，显著提高了复杂环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN技术适用于大规模物联网部署，但在复杂室内环境中实现亚10米精度的定位仍具挑战性，主要受多径效应、瞬态障碍物等因素影响。

Method: 结合自适应卡尔曼滤波与扩展的对数距离多墙路径损耗和阴影（PLS）模型。该模型不仅使用传统LoRaWAN参数（RSSI、频率、SNR），还融入了动态环境指标（温度、湿度、二氧化碳、颗粒物、气压）。通过卡尔曼滤波器减少RSSI波动。基于6个月的130多万次现场测量数据，对比了三种模型：基准COST 231多墙模型（MWM）、增加环境参数的模型（MWM-EP）以及增加环境参数并使用卡尔曼滤波处理RSSI的模型（MWM-EP-KF）。

Result: MWM-EP-KF模型取得了最佳性能，平均绝对误差（MAE）为5.81米，优于MWM-EP（10.56米）和基准MWM（17.98米）。环境参数的加入使系统误差减少了41.22%，卡尔曼滤波在高RSSI波动下将鲁棒性平均提高了42.63%。

Conclusion: 该研究提出了一种可解释且高效的解决方案，通过融合环境因素和自适应滤波，可在动态变化的室内环境中实现精确的LoRaWAN定位。

Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender
for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor
localization remains challenging due to complex environmental conditions,
multipath fading, and transient obstructions. This paper proposes a lightweight
but robust approach combining adaptive filtering with an extended log-distance,
multi-wall path loss and shadowing (PLS) model. Our methodology augments
conventional models with critical LoRaWAN parameters (received signal strength
indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic
environmental indicators (temperature, humidity, carbon dioxide, particulate
matter, and barometric pressure). An adaptive Kalman filter reduces RSSI
fluctuations, isolating persistent trends from momentary noise. Using a
six-month dataset of 1,328,334 field measurements, we evaluate three models:
the baseline COST 231 multi-wall model (MWM), the baseline model augmented with
environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered
RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF
achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP
(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation
reduces systematic errors by 41.22%, while Kalman filtering significantly
enhances robustness under high RSSI volatility by 42.63%, on average across all
devices. These findings present an interpretable, efficient solution for
precise indoor LoRaWAN localization in dynamically changing environments.

</details>


### [26] [Performance of Cell-Free Massive MIMO in Realistic Urban Propagation Environments](https://arxiv.org/abs/2505.01222)
*Yunlu Xiao,Ljiljana Simić*

Main category: cs.NI

TL;DR: 研究对比了在常用对数距离信道模型和基于真实城市布局的射线追踪模型下，无蜂窝大规模MIMO的性能，发现后者性能显著降低，对其在城市部署的实际吸引力提出质疑。


<details>
  <summary>Details</summary>
Motivation: 虽然以用户为中心的无蜂窝大规模MIMO（CF-mMIMO）在均匀传播环境（对数距离模型）下表现出高且均匀的吞吐量，但其在真实的城市传播环境下的性能尚未得到充分研究。

Method: 首次对CF-mMIMO在广泛使用的对数距离信道模型和通过射线追踪（使用真实3D城市布局和实际AP位置）获得的现实城市传播环境下的性能进行了比较研究。

Result: 研究结果表明，使用射线追踪信道模型时，CF-mMIMO无法达到像对数距离信道模型所观察到的那样高且均匀的吞吐量性能。

Conclusion: 该研究结果对CF-mMIMO在实际城市部署中的吸引力提出了质疑，因为其性能可能被简化的信道模型高估了。

Abstract: While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform
throughput performance under the assumption of a uniform propagation
environment modeled by the log-distance path loss channel model, the
performance under a realistic urban propagation environment is not yet fully
addressed. In this paper we conduct the first comparative performance study of
CF-mMIMO under both the widely assumed log-distance channel model and the
realistic urban propagation environment obtained via raytracing using real 3D
city layouts and practical AP locations. Our results show that with the
raytracing channel model, CF-mMIMO cannot achieve as high and uniform
throughput performance as observed with the log-distance channel model, putting
into question the attractiveness in practice of CF-mMIMO for real urban
deployments.

</details>
