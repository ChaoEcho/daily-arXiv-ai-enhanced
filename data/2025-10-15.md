<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.CV](#cs.CV) [Total: 61]
- [cs.AI](#cs.AI) [Total: 46]
- [cs.LG](#cs.LG) [Total: 61]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在解决逻辑谜题时常表现出“幻影记忆”模式，即在谜题细节微调后，模型难以重新推理，揭示了其语言流畅性与深层逻辑理解之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有证据表明，大型语言模型在解决经典逻辑谜题时，可能更多依赖记忆模板而非真正的推理，导致在谜题稍作修改后性能急剧下降。本研究旨在系统探究LLMs是否已解决这些问题，解决程度如何，以及是否存在通用的提示方法来提升其表现。

Method: 引入PHANTOM RECALL基准，包含25个知名逻辑谜题及其149个精心设计的扰动版本（保留推理结构但改变表面细节和解法）。评估了11个主流LLMs，并识别出“幻影记忆”的失败模式。为探测和缓解此问题，贡献了三个工具：自动化逻辑等效判断器、细粒度推理错误分类法和基于这些类别的提示缓解框架。

Result: 尽管LLMs在未修改的谜题上表现接近完美，但在扰动后的谜题上显著低于人类表现，并表现出“幻影记忆”现象（自信地复现不适用于修改场景的记忆解法或错误理由）和过度阐述。

Conclusion: 研究结果揭示了一个关键局限性：LLMs在上下文线索发生变化时，往往无法重新进行推理，这突显了其语言流畅性与逻辑理解能力之间的鸿沟。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs作为世界模型在长程模拟中因幻觉和静态知识而受限。本研究探究了LLMs的世界模型能力，发现其在长程规划中表现不佳。为此，提出检索增强的世界模型（R-WoM），通过外部知识显著提升了LLMs在长程模拟中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）有潜力作为世界模型提升智能体决策，但其幻觉问题和依赖静态训练知识的限制导致长程模拟误差累积。因此，需要系统地调查LLMs是否适合世界建模以及如何克服其在长程模拟中的局限性。

Method: 首先，通过“下一状态识别”、“完整流程规划对齐”和“里程碑转换识别”三个任务，系统性地探究LLMs在未来状态预测和奖励估计方面的世界模型核心能力。其次，针对发现的局限性，提出了“检索增强的世界模型（R-WoM）”，其核心机制是通过整合从外部教程检索到的事实性、最新知识来增强LLM的模拟能力。

Result: 分析显示，LLMs能有效识别即时下一状态和关键状态转换，但在完整流程规划中表现迅速下降，暴露出其在长程环境动态建模上的局限性。提出的R-WoM相较基线取得了显著提升，在OSWorld上高达25.3%，在WebArena上高达18.1%，尤其在长程模拟中表现出明显优势。

Conclusion: 尽管LLMs在捕获即时状态和有意义转换方面表现良好，但在长程规划上存在显著局限性。通过引入检索增强机制（如R-WoM），可以有效克服这些限制，显著提升LLMs在长程世界建模中的可靠性和性能。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: LLM对微小输入变化的敏感性源于其内部知识表示的不稳定性：当输入与训练数据形式偏离时，其真假判断的内部表示能力会显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大型语言模型(LLM)性能脆弱，对输入变体过于敏感。本研究旨在探讨这种脆弱性是否是由于内部知识表示的不稳定造成的。

Method: 通过在经历表面变换（如错别字或改写）而成为分布外(OOD)的样本上，评估LLM内部表示对陈述真实性（真/假）的可分离性。研究横跨四种LLM家族、五个评估数据集和三种知识探测方法，分析分离性如何随样本OOD程度的增加而下降。

Result: 研究发现，当样本的呈现形式与预训练数据差异较大时，LLM内部对陈述真实性的表示会崩溃。LLM区分真假陈述的能力高度依赖于陈述的精确表面形式，而不是其深层语义。

Conclusion: 这些发现表明LLM可能学习到浅层、非鲁棒的知识表示，导致泛化能力有限，从而解释了基准测试性能的脆弱性。这向真实性探测的效用提出了根本性挑战，并呼吁进一步研究以提高学习知识表示的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）在机器翻译（MT）中使用“思维令牌”并未显著提升性能，除非这些令牌包含具体的翻译尝试。研究表明，蒸馏的思维链（CoT）微调不如标准输入-输出微调有效，而采用模块化翻译策略构建的中间令牌则能带来改进。最终，通过教师模型精炼译文或扩充语料比蒸馏CoT更具影响力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在数学和编程等任务中展现出强大的问题解决能力，但其通过自然语言思维过程应用于机器翻译（MT）领域的影响和效益尚未被充分探索。

Method: 本研究通过在不同资源量和设置下的多语言对机器翻译任务中，探索生成中间令牌（“思维令牌”）的效益。具体方法包括直接生成思维令牌，使用受人类翻译实践启发的蒸馏链式思考（CoT）对模型进行微调，以及结合模块化翻译特定提示策略构建中间令牌。

Result: 研究发现，“思维令牌”未能帮助LRMs更好地执行机器翻译。即使是使用蒸馏CoT解释（如逐步翻译细节）进行微调，其表现也未超过标准的输入-输出微调。然而，通过结合模块化翻译特定提示策略构建的中间令牌则带来了性能提升。这表明中间令牌的贡献高度依赖于其中是否包含具体的翻译尝试。

Conclusion: LRMs中间令牌在机器翻译中的作用和贡献，关键在于其是否包含实际的翻译尝试。与将教师模型的CoT解释蒸馏到“思考型”MT模型中相比，利用教师模型精炼目标翻译或扩展平行语料库是更有效提升机器翻译性能的方法。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 本文提出MIND，一个用户参与的事实核查管道，用于检测多语言问答知识库中的事实和文化差异，并能可靠地识别不一致性。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需确保跨语言的事实一致性，同时考虑主观回应的文化差异。

Method: 提出MIND，一个用户参与的事实核查管道，用于检测多语言QA知识库中的事实和文化差异。在母婴健康领域的双语QA系统上进行评估，并发布标注数据集，同时在其他领域数据集上测试其泛化能力。

Result: MIND在所有测试案例中都能可靠地识别事实和文化不一致性。

Conclusion: MIND支持开发更具文化意识和事实一致性的多语言问答系统。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 本文提出TopoAlign框架，通过将代码仓库解构并重组为结构对齐的数据，用于训练数学大语言模型（LLMs）以改进自动形式化能力，从而缓解高质量数据集稀缺问题，并取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学自动形式化（将非形式化语句转化为形式化）方面仍面临挑战，而自动形式化对于结合LLMs非形式化推理与形式化证明辅助工具至关重要。当前数学LLMs受限于缺乏大规模非形式化与形式化语句对数据集，且现有自然语言到代码模型的结构和语法差异限制了有效迁移学习。

Method: 本文提出TopoAlign框架，利用广泛可用的代码仓库作为数学LLMs的训练资源。TopoAlign将代码分解为文档字符串、主函数和依赖函数，并将其重组为结构上类似于形式化语句的对应物，从而生成无需额外人工标注的结构对齐代码数据。作者使用此方法训练了DeepSeek-Math和Herald两个SOTA模型，并在minif2f、Putnam和ProofNet基准上进行了评估。

Result: TopoAlign为DeepSeek-Math带来了显著提升，在BEq@10上提高了17.77%，在typecheck@10上提高了68.82%。即使对于专业模型Herald，在未引入新数学知识的情况下，其在BEq@10和typecheck@10上也分别获得了0.12%和1.09%的提升。

Conclusion: 研究表明，利用对齐的代码数据进行训练对数学LLMs（包括专业模型）有益。TopoAlign框架能有效利用现有代码资源，为解决自动形式化任务中的数据稀缺问题提供了可行方案，并显著提升了模型的性能。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY框架通过整合文化和心理框架生成合成的用户偏好数据，实现LLM个性化，显著减少对昂贵人工反馈的依赖，并在多文化环境下表现出卓越的用户偏好提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化依赖昂贵的人工反馈或交互日志，这限制了其可扩展性，并未能充分捕捉用户深层属性。

Method: 引入GRAVITY框架，该框架通过集成人口学、文化和心理学框架（如霍夫斯泰德文化维度、施瓦茨基本价值观、世界价值观调查和大五人格特质）生成合成的、基于用户画像的偏好数据对。该方法在400名亚马逊用户的书籍描述上进行了评估，并与基于提示的条件生成、标准微调和简单合成对生成进行了比较。

Result: 基于用户画像的合成数据持续改进了生成效果，尤其是在多文化（美国、巴西、日本、印度）环境下，相对于基线实现了超过4%的偏好增益。用户研究表明，GRAVITY的输出在超过86%的情况下受到用户偏好。

Conclusion: 研究结果表明，基于情景的合成数据能够捕捉更丰富的用户差异，减少对昂贵标注的依赖，并生成更具吸引力、以用户为中心的内容，为LLM个性化提供了一条可扩展的路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 现有RAG基准测试存在不切实际和易被“作弊”的问题。本文提出了CRUMQs管道，自动生成不可作弊、真实、不可回答的多跳查询，以更有效地评估RAG系统，并展示其能大幅提升挑战性和减少作弊性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，RAG系统常面临复杂、信息缺失或需要多跳推理的查询，但现有基准测试缺乏对这些场景的真实反映，容易被“作弊性”方法解决或仅需简单事实回忆，从而限制了对RAG系统真实局限性的揭示。

Method: 开发并提出了首个自动化、难度可控的管道（CRUMQs），用于创建不可作弊、真实、不可回答和多跳的查询。该管道可适应任何语料库和领域，并已用于两个流行RAG数据集，通过在领先的检索增强型LLM上进行基准实验来验证其有效性。

Result: 与现有RAG基准测试相比，CRUMQs对RAG系统更具挑战性，并成功将可作弊性分数降低了高达81.0%。

Conclusion: 所提出的CRUMQs管道为增强RAG基准测试的难度和真实性提供了一个简单有效的方法，有助于推动更强大RAG系统的发展。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出DMTD推理范式，利用LLM层级分工的特性，让后期层从已处理的隐藏状态生成多个token，避免重复遍历早期和中期层，从而实现高达2倍的推理加速且性能损失轻微，无需额外参数。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM中早期、中期和后期层可能承担不同职责（早期理解上下文，中期处理任务，后期转换表示为输出），研究者假设一旦早期和中期层处理完信息，后期层便能从得到的隐藏状态生成多个token，从而避免重复遍历整个模型，以提高推理效率。

Method: 提出直接多token解码（Direct Multi-Token Decoding, DMTD）推理范式。该方法基于假设，即早期和中期层处理完表示后，后续的多token生成可仅由后期层完成。DMTD不引入额外参数、辅助程序或后生成验证，与推测解码等现有方法不同。

Result: 通过在有限数据集上微调的DMTD Qwen3-4B模型，已实现高达2倍的推理加速，且仅伴随轻微的性能损失。此外，扩展性分析表明，随着训练数据集的增大，该方法的性能预计将进一步提升。

Conclusion: DMTD是一种无需额外参数或复杂验证的有效LLM推理加速方法，通过利用模型层级分工的特性，显著提高了生成效率。其性能已显示出潜力，并有望随着更大规模训练数据而进一步优化。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文提出Context-Folding框架和FoldGRPO强化学习方法，使LLM智能体能主动管理上下文，通过子任务分解和折叠来压缩中间步骤。在长任务上，该方法以10倍小的上下文超越或匹敌ReAct基线，并优于基于摘要的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能体在处理长周期任务时，其性能受到上下文长度的根本限制。

Method: 引入Context-Folding框架，使智能体能主动管理工作上下文。智能体可分支处理子任务，完成后折叠中间步骤并保留简洁摘要。为实现学习，开发了端到端强化学习框架FoldGRPO，通过特定过程奖励鼓励任务分解和上下文管理。

Result: 在复杂长周期任务（如Deep Research和SWE）上，Context-Folding智能体使用小10倍的活跃上下文，性能与ReAct基线持平或超越，并显著优于依赖摘要的上下文管理模型。

Conclusion: Context-Folding框架通过主动管理上下文，有效解决了LLM智能体在长任务中的上下文长度限制，显著提高了效率和性能。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 自动形式化中猜想是一个被忽视的关键步骤。LLMs的自动形式化能力被高估，因为评估时常假设猜想已给出。本文提出ConjectureBench和Lean-FIRe，证明猜想对自动形式化至关重要，并需独立研究以提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 自动形式化常被视为直接翻译，却忽略了前置的“猜想”步骤，许多数学问题无法直接形式化。大型语言模型（LLMs）在自动形式化上已面临挑战，其猜想能力评估有限且常与自动形式化或证明混淆，导致其影响难以理解。

Method: 增强现有数据集以创建ConjectureBench，并重新设计评估框架和指标，专门衡量LLMs的猜想能力（作为独立任务和自动形式化流程的一部分）。设计了一种推理时方法Lean-FIRe来改进猜想和自动形式化。

Result: 当评估中考虑猜想时，基础模型的自动形式化性能被大大高估，不应假设猜想已提供。本文提出的Lean-FIRe方法首次实现了对PutnamBench问题端到端的自动形式化，其中GPT-4.1成功解决13个，DeepSeek-V3.1解决7个。LLMs虽具备生成准确猜想的知识，但提升自动形式化性能需将猜想视为独立任务。

Conclusion: 猜想是形式化数学推理中一个被忽视但关键的步骤。提升自动形式化性能需要将猜想视为独立任务，并深入研究如何正确地将其整合到自动形式化流程中。研究为未来改进猜想这一领域提供了指导。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出SAGE框架，通过整合业务上下文知识（顶部业务逻辑和底部业务基础设施），改进多轮交互智能体的用户模拟评估，使其生成更真实、多样的交互，并能发现更多智能体错误。


<details>
  <summary>Details</summary>
Motivation: 评估多轮交互智能体通常需要人工评估，成本高且效率低。现有基于模拟用户的评估方法过于通用，未能融入领域特定原则，导致模拟的用户行为不够真实。

Method: 提出SAGE用户模拟框架，用于多轮智能体评估。SAGE整合了来自业务背景的知识，包括：1) 顶部知识：业务逻辑（如理想客户画像）以构建真实的客户角色；2) 底部知识：业务智能体基础设施（如产品目录、FAQ、知识库）以反映用户的信息需求和期望。

Result: 经验评估表明，SAGE生成了更真实、多样的交互，并且能够识别出多达33%的智能体错误。

Conclusion: SAGE是一个有效的评估工具，能支持智能体的错误发现和迭代改进，提高了评估的准确性和实用性。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对离散数学逻辑等价问题的自动问题生成（AQG）系统，通过形式语言和线性时间算法解决了现有AQG的效率和难度不均问题，并经验证其生成问题的准确性和难度与教材问题相当。


<details>
  <summary>Details</summary>
Motivation: 高等教育中学术不端行为零容忍，但在在线教学背景下，抄袭日益普遍。AQG可以为每位学生生成独特问题以减少抄袭，并提供大量练习题。现有AQG在生成逻辑等价问题时效率低下且问题难度不统一。

Method: 提出新方法，通过形式语言定义逻辑等价问题，将其转化为两组生成规则，并开发了一个线性时间的问题生成算法。

Result: 通过两项实验评估：1) 学生完成系统生成问题的准确性与教材问题相当。2) 生成问题的难度与教材问题和大型语言模型生成的问题相似。

Conclusion: 所提出的AQG系统能够有效生成高质量的离散数学逻辑等价问题，其准确性和难度与教材问题相当，有助于缓解抄袭问题并提供有效的练习。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 论文比较了农业领域神经符号（NS）和基于大语言模型（LLM）的信息抽取系统。LLM系统在性能上优于NS系统，但两者各有优劣，强调了实际部署中平衡性能、效率和控制的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前信息抽取（IE）领域过度依赖大语言模型，忽视了过去数十年的符号或统计IE系统经验，因此有必要对神经符号系统和LLM系统进行对比分析。

Method: 本文在农业领域（包括猪肉、奶制品和作物子领域）对一个神经符号（NS）IE系统和一个基于LLM的IE系统进行了比较评估，使用了九个访谈数据。

Result: LLM系统在F1分数上优于NS系统（总F1：69.4 vs 52.7；核心F1：63.0 vs 47.2）。然而，NS系统具有更快的运行时间、更好的控制和在无上下文任务中的高准确性，但泛化能力差、难以处理上下文细微差别且开发维护成本高。LLM系统表现更好、部署更快、维护更容易，但运行时间较慢、控制有限、存在模型依赖和幻觉风险。

Conclusion: 研究结果揭示了在实际应用中部署自然语言处理系统的“隐藏成本”，强调了在性能、效率和控制之间取得平衡的必要性。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: LLMs的幻觉问题部分源于不良提示。本文提出CPR框架，通过清洗和丰富提示来对齐用户意图，从而显著减少幻觉并提高生成质量，实验显示超过90%的胜率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)虽流畅，但常产生“幻觉”事实，损害用户信任。一个常见但被忽视的原因是用户使用结构不良或模糊的提示，导致LLMs基于假设而非真实意图生成响应。

Method: 引入了“治愈性提示精炼”(Curative Prompt Refinement, CPR)框架。这是一个即插即用的框架，利用一个微调的小型语言模型，分两步实现：1) 清理结构不良的提示；2) 生成额外的、信息丰富的任务描述，以对齐用户意图与提示。

Result: CPR应用于语言模型后，显著提高了生成质量，并有效减轻了幻觉。实证研究表明，经过CPR处理的提示与原始提示相比，在不依赖外部知识的情况下，胜率超过90%。

Conclusion: CPR框架通过优化和丰富用户提示，能够有效提高语言模型的生成质量并显著减轻幻觉，成功对齐用户意图。

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出多阶段提示词精炼框架（MPR），通过系统改进格式不佳的提示词来有效减少大型语言模型（LLMs）的幻觉并提高其输出准确性，具有超过85%的胜率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言任务中表现出色，但仍面临幻觉问题。虽然多种因素导致幻觉，但格式不佳的提示词（如歧义、语法错误、信息不完整）的影响尚未得到充分探索，因此需要一个解决方案来系统地改进这些提示词。

Method: 引入多阶段提示词精炼（MPR）框架。该框架分多个阶段系统地改进格式不佳的提示词，每个阶段使用经过微调的小型语言模型（SLMs）解决特定的错误类型（如标点、拼写、术语误用）。MPR通过增加上下文迭代增强提示词的清晰度，并采用带排名的自我反思机制来优先处理最相关输入。

Result: 在幻觉基准测试中，经MPR精炼的提示词比原始形式获得了超过85%的胜率，证明了其在减少幻觉和提高LLM输出准确性方面的有效性。此外，MPR可以与现有的事后幻觉缓解框架结合使用，进一步增强其多功能性。

Conclusion: MPR提供了一种轻量级且适应性强的解决方案，能够有效提升LLMs在各种领域的可靠性。

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 研究发现，在没有明确去偏的情况下，人类标注差异（HLV）训练方法对模型公平性有积极影响。


<details>
  <summary>Details</summary>
Motivation: 探讨人类标注差异（HLV）对模型公平性影响这一未探索主题。

Method: 通过比较基于多数投票标签的训练与一系列HLV训练方法来检查两者之间的相互作用。

Result: 实验表明，在没有明确去偏的情况下，HLV训练方法对模型公平性产生积极影响。

Conclusion: HLV训练方法能够在不进行显式去偏处理的情况下，提升模型的公平性。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在幻觉问题，不确定性量化（UQ）是检测其不可靠输出的关键。本文综述了UQ的基础概念、在LLM幻觉检测中的应用、现有方法分类、实证结果、当前局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理领域取得了显著突破，但在实际应用中易产生幻觉（听起来合理但事实错误），引发了对其可靠性和信任度的担忧。因此，迫切需要不确定性量化（UQ）来评估模型生成的可靠性。

Method: 本文首先介绍了不确定性量化（UQ）的基础知识，包括其正式定义、认知不确定性与偶然不确定性的区分，以及这些概念如何适应LLMs。在此基础上，探讨了UQ在幻觉检测中的作用，并系统性地分类了现有方法，展示了若干代表性方法的实证结果。

Result: 不确定性量化（UQ）为识别不可靠的LLM生成并提高其可靠性提供了有效机制。本文对LLM幻觉检测中UQ的现有方法进行了多维度系统分类，并展示了多种代表性方法的实证结果，揭示了其在检测幻觉方面的潜力。

Conclusion: 本文为LLM幻觉检测领域的不确定性量化现状提供了清晰的图景，讨论了当前局限性，并展望了未来有前景的研究方向。不确定性量化是解决LLM幻觉问题，增强其可靠性和信任度的核心途径。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 该研究提出一个基于LLM的提示词重写框架，通过DPO和奖励系统训练，用于优化T2I模型输入，从而提高图像质量、美观度和图文一致性，并展现出良好的可迁移性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）模型在处理简单或描述不足的提示词时表现不佳，导致图文对齐、美学和图像质量欠佳。

Method: 提出一个提示词重写框架，利用大语言模型（LLMs）在输入T2I主干模型前优化用户提示。该方法包含一个精心设计的奖励系统和一个迭代的直接偏好优化（DPO）训练流程，无需监督微调数据即可增强提示词。

Result: 我们的提示词重写器在各种T2I模型和基准测试中，持续改进了图文对齐、视觉质量和美学效果，超越了强基线。它还展示了强大的可迁移性，在某个T2I主干模型上训练后，无需重新训练即可有效泛化到其他模型。此外，性能增益与LLM容量呈正相关。

Conclusion: 提示词重写是一种有效、可扩展、实用且与模型无关的策略，可以显著改进T2I系统。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 本文提出分层对齐（Hierarchical Alignment）方法，针对LLM不同功能层（局部、中间、全局）应用定向DPO，有效提升模型性能并避免“对齐税”。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐技术（如DPO）将模型视为单一实体，忽略Transformer架构中各层的功能特化，导致优化压力均匀，可能产生“对齐税”（流畅性提升却损害逻辑推理）。

Method: 引入分层对齐方法，将DPO定向应用于模型层的不同功能块：局部（语法）、中间（逻辑）和全局（事实性）。使用LoRA对Llama-3.1-8B和Qwen1.5-7B等模型进行精细化微调，并由强大的LLM-as-Judge进行评估。

Result: 分层对齐策略实现显著且可预测的改进。局部对齐（Local-Align）增强语法流畅性；全局对齐（Global-Align）不仅提高事实一致性，更最有效地提升逻辑连贯性，超越所有基线。所有分层策略均成功避免了标准DPO中观察到的“对齐税”。

Conclusion: 研究结果表明，从整体优化转向结构感知的手术式微调，为LLM对齐提供了一条更资源高效、可控且可解释的路径，具有构建更先进、可靠LLM的巨大潜力。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 针对长上下文Transformer模型的内存和性能下降问题，本文提出APCE方法，通过语义相似度匹配选择关键输入片段，有效减少内存占用并维持或提升摘要性能。


<details>
  <summary>Details</summary>
Motivation: 部署长上下文Transformer模型（LCTMs）面临两大挑战：一是随着序列长度增加，自注意力机制和KV缓存导致内存占用呈二次和线性增长；二是“上下文退化（ContextRot）”现象，即性能随上下文长度增加而下降。研究动机是探讨能否通过选择最重要的输入片段来同时减少内存占用并缓解上下文退化效应。

Method: 本文提出APCE（context-aware solution），通过与当前查询进行低维语义相似度匹配，来选择最重要的输入片段。APCE直接作用于输入，不依赖于底层硬件或CUDA环境，具有良好的兼容性和可扩展性。

Result: 在长上下文摘要任务中，APCE仅使用50%-70%的输入序列，就实现了与完整密集基线相当或更优的摘要性能，并显著提升了KV缓存和自注意力机制的内存效率。

Conclusion: 研究结果肯定了通过上下文感知方法选择关键输入片段，可以有效解决长上下文Transformer模型的内存和性能问题。期望这些发现能激励更多针对其他相关长上下文任务的上下文感知效率解决方案的研究。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: 本研究评估了Verily行为健康安全过滤器（VBHSF）在精神健康危机检测方面的性能，并将其与现有开源内容审核工具进行了比较，发现VBHSF表现出优越且稳定的性能，尤其在高敏感度方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理精神健康紧急情况时常给出有害或不恰当的建议，甚至助长破坏性行为，因此需要一个有效的安全过滤器来解决这一问题。

Method: 研究使用了两个由临床医生标注的数据集（Verily精神健康危机数据集v1.0和NVIDIA Aegis AI内容安全数据集2.0）来评估Verily行为健康安全过滤器（VBHSF）。同时，将VBHSF的性能与两个开源内容审核防护措施（OpenAI Omni Moderation Latest和NVIDIA NeMo Guardrails）进行了比较，评估指标包括敏感度、特异度和F1-分数等。

Result: 在Verily数据集上，VBHSF在检测任何精神健康危机时表现出高敏感度（0.990）和高特异度（0.992），识别特定危机类别的F1-分数为0.939。在NVIDIA数据集上，VBHSF保持高敏感度（0.982）和准确性（0.921）。与NVIDIA NeMo和OpenAI Omni Moderation Latest相比，VBHSF在所有情况下敏感度显著更高（p < 0.001），且相对于NVIDIA NeMo特异度更高（p < 0.001）。竞争对手在特定危机类型上表现不一致，某些类别的敏感度低于0.10。

Conclusion: Verily行为健康安全过滤器（VBHSF）展现出鲁棒、可泛化的性能，并优先考虑敏感度以最大程度地减少危机的遗漏，这对于医疗保健应用而言是一个至关重要的特性。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 提出PACE方法评估LLM创造力，发现其与人类表现有异同，且专业人类优于LLM。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）的创造力是一个关键研究领域，但面临数据污染和昂贵人工评估的挑战。

Method: 受人类创造力评估启发，提出PACE（Parallel Association Chains for Evaluation）方法，通过LLMs生成并行联想链来评估其创造力。

Result: PACE方法有效避免数据污染，提供高效评估，并与Chatbot Arena创意写作排名高度相关（Spearman's $\rho = 0.739, p < 0.001$）。高水平LLMs的联想创造力与普通人类相当，但专业人类表现持续优于LLMs。此外，人类和LLMs在联想中都表现出具体性下降的趋势，但人类的联想模式多样性更高。

Conclusion: PACE为LLM创造力评估提供了一种高效且低污染的途径。研究表明，虽然高性能LLMs能达到普通人的创造力水平，但专业人类在创造力表现和联想模式多样性上仍持续超越LLMs。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本研究探讨LLM在多语言域适应中知识获取机制，提出了AdaXEval自适应评估方法，发现尽管有高质量语料库，跨语言知识迁移仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM多语言知识获取（包括领域知识在语言内学习和跨语言迁移）的机制探索不足，导致性能欠佳，尤其在低资源环境下。此外，以往多语言域适应研究中训练和评估数据集的知识覆盖常不匹配。

Method: 提出AdaXEval自适应评估方法，通过从用于训练的双语领域语料库构建多项选择问答数据集，直接研究多语言知识获取。通过使用不同数据配方对LLM进行持续训练，追踪LLM获取领域事实的过程，并找出领域训练数据转化为知识的机制。

Result: 对一个13B日英双语LLM的实验表明，即使使用了高质量的双语语料库，跨语言知识迁移仍然具有挑战性。

Conclusion: LLM在多语言域适应中实现有效的跨语言知识迁移仍然是一个重大挑战，即使在高质量双语语料库的支持下也如此。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文系统分析了大型语音语言模型（LSLMs）在语音和文本输入之间存在的“模态差距”及其与表示对齐模式的关系，并提出干预策略以改善语音输入性能。


<details>
  <summary>Details</summary>
Motivation: LSLMs在对话生成方面表现出色，但在语义理解上落后于传统系统。特别是，语音和文本输入之间的性能差距（即“模态差距”）显著，需要深入理解和解决。

Method: 通过系统实验揭示模态差距。分析了粗粒度（方向对齐与幅度发散）和细粒度（自发token级对齐模式）的文本和语音表示。引入“对齐路径分数”量化token级对齐质量。基于分析洞察，通过角度投影和长度归一化设计了关键token的干预策略。

Result: LSLMs存在明显的模态差距。粗粒度分析发现，深层语音和文本表示在方向上趋于对齐但在幅度上发散，且表示相似性与模态差距强相关。细粒度分析观察到自发token级对齐模式，且对齐路径分数与模态差距有更强关联。所设计的干预策略展示了改善语音输入正确性的潜力。

Conclusion: 本研究首次对LSLMs的模态差距和对齐机制进行了系统实证分析，为未来的模型优化提供了理论和方法指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 针对多模态大语言模型（MLLMs）在多轮对话中的安全问题，本文提出了SafeMT基准和安全指数（SI），发现现有模型安全机制不足，并提出了一种更有效的对话安全协调器。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的广泛应用导致其安全问题日益突出，尤其在更常见的“多轮对话”场景中风险更高，但现有基准未能充分考虑此类情况。

Method: 1. 引入SafeMT基准，包含10,000个含图像的有害查询生成的多轮对话样本，覆盖17种场景和4种越狱方法。 2. 提出了安全指数（SI）来评估MLLMs在对话中的通用安全性。 3. 提出了一个对话安全协调器，用于检测对话中隐藏的恶意意图并提供安全策略。

Result: 1. 评估17个模型后发现，有害对话轮次增加时，成功攻击模型的风险随之上升，表明现有安全机制不足。 2. 实验结果显示，所提出的对话安全协调器在降低多轮攻击成功率（ASR）方面，比现有防护模型更有效。

Conclusion: MLLMs在多轮对话中的安全机制存在明显不足。SafeMT基准和安全指数（SI）揭示了这一问题，而提出的对话安全协调器能有效提升MLLMs在多轮对话场景下的安全性。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: LLMs的幻觉源于Softmax导致的人为确定性。Credal Transformer通过可信度注意力机制，利用证据理论量化不确定性，有效识别分布外输入并减少自信错误，从而缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉问题，即生成事实错误但自信的断言。研究认为这源于Transformer的Softmax函数，它通过将模糊的注意力分数压缩成单一概率分布，丢弃不确定性信息，从而创建“人为确定性”。

Method: 提出Credal Transformer，用基于证据理论的可信度注意力机制（CAM）替换标准注意力。CAM生成一个“可信度集”（一组分布）而非单一注意力向量，其大小直接衡量模型不确定性。实现方式是将注意力分数重新概念化为Dirichlet分布的证据质量：充分证据恢复标准注意力，证据不足则产生扩散分布以代表模糊性。

Result: Credal Transformer能有效识别分布外输入，量化模型的不确定性，并通过弃权显著减少了对无法回答问题的自信错误。

Conclusion: 本研究贡献了一种缓解幻觉的新架构，并提出了一种将不确定性量化直接集成到模型中的设计范式，为构建更可靠的AI奠定了基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）中的并行推理范式，定义了其概念，基于新颖分类法（包括非交互式、交互式和效率导向策略）分析了先进技术，探讨了应用场景，并指出了现有挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力的增强，并行推理作为一种通过同时探索多条思路来提高推理鲁棒性的新范式应运而生。为克服标准顺序方法的脆弱性并提高实际性能，探索并行推理成为一个重要趋势。本研究旨在对并行推理的进展和挑战进行综述和总结。

Method: 本研究首先对并行推理进行了形式化定义，并阐明了其与Chain-of-Thought等相关概念的区别。接着，基于一种新颖的分类法，包括非交互式推理、交互式推理和效率导向的解码策略，组织和讨论了先进技术。此外，还探讨了多种应用场景，如解决复杂问题和增强LLM输出的可靠性。

Result: 作为一篇综述论文，其成果是提供了一个关于并行推理的全面路线图。具体包括：提出了并行推理的正式定义；构建了一个新颖的分类法来组织和讨论现有技术；探讨了并行推理在解决复杂问题和提高LLM可靠性方面的应用；并识别了核心挑战与未来研究方向。

Conclusion: 论文总结了并行推理的核心挑战，并提出了潜在的未来研究方向。作者希望这项工作能为初学者提供有用的路线图，并鼓励更多关于改进并行推理方法的研究。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 研究了在连续空间推理中应用多样本生成和奖励模型重排的效果，发现尽管有潜力，但因连续表示缺乏关键归纳偏置，现有PRM/ORM方法效果不佳，呼吁在模型训练中显式引入归纳偏置以增强区分能力。


<details>
  <summary>Details</summary>
Motivation: 离散（文本）空间推理中，多样本生成结合奖励模型（PRM/ORM）重排已被证明有效。本研究旨在探究这些成熟技术能否成功应用于连续空间推理，并以COCONUT连续空间推理LM为基础进行验证。

Method: 使用COCONUT连续空间推理LM作为基础模型，通过基于dropout的采样生成多样化连续推理路径，并进行Pass@N分析。尝试将离散空间中用于PRM和ORM的数据生成及训练方法应用于连续空间。通过探究几何特性和轨迹动力学等多个方面，分析了正确与错误推理之间的区分问题。

Result: 通过dropout采样能够生成多样化的连续推理路径，且Pass@N分析显示出显著的性能提升潜力。然而，在连续空间中实现这一增益面临独特挑战，离散空间中有效的PRM/ORM训练方法在连续空间仅带来微小改进。研究发现，主要限制源于连续思维表示中缺乏关键的归纳偏置，导致无法有效区分正确与错误的推理。

Conclusion: 连续空间推理大型语言模型的训练框架不仅应优化准确性，还需明确纳入归纳偏置，以在推理时有效区分正确和错误的思想。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: LLaDR框架利用大语言模型（LLMs）将常识性生物医学知识融入知识图谱嵌入，显著提升了药物重定向的性能。


<details>
  <summary>Details</summary>
Motivation: 现有药物重定向方法主要依赖生物医学知识图谱，但它们大多忽略了现实世界中常识性的生物医学概念知识，例如药物与特定治疗方法间的基本机制兼容性。

Method: 提出LLaDR框架，该框架利用大语言模型（LLMs）提取语义丰富的治疗相关文本表示，并用这些表示来微调知识图谱嵌入（KGE）模型，从而改善知识图谱中生物医学概念的表示。

Result: LLaDR在多个基准测试中取得了最先进的性能，并通过阿尔茨海默病等案例研究进一步证实了其鲁棒性和有效性。

Conclusion: LLaDR通过将大语言模型提取的治疗相关知识注入知识图谱嵌入，有效增强了生物医学概念的表示和对复杂疾病的语义理解，从而显著加速了药物重定向过程。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本研究首次系统性地揭示了大型音频语言模型（LALMs）在时间戳预测中存在的“时间偏差”，发现其预测事件时间系统性地提前或滞后，且该偏差普遍存在、随音频长度增加而增大，并随事件类型和位置变化。


<details>
  <summary>Details</summary>
Motivation: LALMs在音频理解和多模态推理领域应用日益广泛，但其定位事件发生时间的能力尚不明确。本研究旨在系统性地探究LALMs的时间偏差，揭示其在时间戳预测方面的关键局限。

Method: 研究通过在带时间戳数据集上进行受控实验，并引入“时间偏差指数（TBI）”来量化预测事件时间上的系统性错位，辅以可视化框架进行分析。

Result: 实验结果表明，时间偏差 (i) 普遍存在于不同数据集和模型中；(ii) 随音频长度增加而增大，在长录音中甚至累积到数十秒；(iii) 随事件类型和位置而变化。

Conclusion: 当前LALMs在时间戳预测上存在根本性局限。研究呼吁开发时间上更鲁棒的LALM架构。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本研究提出一种基于DPO训练的LLM分割框架，通过偏好对齐优化同声传译中的语音分割，超越现有预训练模型SHAS，提升翻译质量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的同声传译分割模型（如SHAS）虽优于启发式规则，但受限于监督学习，缺乏对人类偏好的对齐，这对于自然的实时口译至关重要。

Method: 本文提出了一个基于大型语言模型（LLMs）的分割框架，并使用直接偏好优化（DPO）进行训练，使其能够预测更符合实时翻译需求的自然分割点。实验在ACL 60/60语料库上进行，采用SeamlessM4T v2作为翻译骨干模型。

Result: DPO调优的LLM在分割准确性上优于SHAS，并在翻译质量（BLEU, COMET）和延迟（Average Lagging）方面取得了一致的改进。系统也受益于IWSLT基线进行直接比较。

Conclusion: 偏好调优的LLM在同声传译领域展现出巨大潜力，能够超越现有预训练分割模型，推动适应性、人类对齐的同声传译技术发展。

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文提出HALF框架，通过衡量危害严重性来评估LLMs在真实世界应用中的公平性和偏差，发现当前基准测试与实际部署之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被广泛部署于医疗、法律、招聘等高影响领域，因此在部署前进行公平性和偏差评估至关重要。然而，现有评估缺乏真实世界场景的考量，且未区分不同危害的严重程度（例如，手术中的偏见决策与文本摘要中的文体偏见不应等同对待）。

Method: 引入HALF（Harm-Aware LLM Fairness）框架，这是一个与部署对齐的框架，用于评估LLMs在现实应用中的偏差，并根据危害严重性对结果进行加权。HALF将九个应用领域分为三级（严重、中度、轻度），并采用五阶段流程。研究对八个LLMs进行了评估。

Result: 评估结果显示：(1) LLMs在不同领域并非始终公平；(2) 模型大小或性能并不能保证公平性；(3) 推理模型在医疗决策支持方面表现更好，但在教育领域表现更差。

Conclusion: HALF框架揭示了当前基准测试的成功与LLMs实际部署准备就绪之间存在明显差距。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究表明，微调后的LLMs会内化Knobe效应（一种道德偏见）。通过Layer-Patching分析，发现此偏见不仅在微调中学习，还能定位到特定层，并可通过替换这些层的激活来消除，实现了不需重训的偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在微调后会内化人类偏见，但这些偏见产生的机制尚不明确。本研究旨在探讨著名的Knobe效应（意向判断中的道德偏见）是否在微调后的LLMs中出现，以及其是否能追溯到模型的特定组件。

Method: 对3个开源LLMs进行了Layer-Patching分析。

Result: 1. Knobe效应在微调后的LLMs中显现。2. 该偏见不仅在微调期间习得，而且可被定位到模型中的特定层。3. 将相应预训练模型的激活补丁到少数几个关键层中，足以消除这种偏见。

Conclusion: LLMs中的社会偏见可以通过有针对性的干预进行解释、定位和缓解，而无需重新训练模型。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 为解决大型语言模型在多文档问答中长距离依赖和“失焦”问题，本文提出即插即用的DSAS方法，通过CGW和RAS模块提升关键信息处理能力，在主流LLMs上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多文档问答（Multi-doc QA）任务中面临挑战：一是长距离依赖建模能力弱，难以聚焦长文本关键信息；二是普遍存在“失焦”（lost-in-the-middle）问题，对长输入中间信息处理困难。现有方案或截断依赖或需昂贵微调，缺乏通用且简单的解决途径。

Method: 本文提出双阶段自适应锐化（Dual-Stage Adaptive Sharpening, DSAS）方法，作为无需架构修改或额外训练参数的即插即用解决方案。DSAS包含两个模块：
1.  **上下文门控加权（Contextual Gate Weighting, CGW）**：通过层级注意力跟踪和位置感知加权评估段落相关性，以缓解“失焦”问题。
2.  **互惠注意力抑制（Reciprocal Attention Suppression, RAS）**：通过抑制关键与无关文本间的信息交换，增强对关键段落的关注，从而缓解长距离依赖建模的局限性。

Result: DSAS在四个基准测试上，显著提升了主流LLMs（Llama, Qwen, Mistral, Deepseek）在Multi-doc QA任务中的表现。在Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct上，F1-score平均提升4.2%。消融研究证实了CGW和RAS模块的独立贡献，且DSAS具有良好的鲁棒性和可扩展性。

Conclusion: DSAS作为一种通用、简单且有效的即插即用方案，成功解决了大型语言模型在多文档问答任务中的长距离依赖建模和“失焦”问题，显著提升了模型处理长文本和关键信息的能力。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 研究发现，大语言模型在医疗多轮交互中鲁棒性极差，尤其在间接上下文干扰下，准确率大幅下降，揭示了其在临床部署中的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正快速应用于医疗临床，但其在现实、多轮交互下的可靠性尚不明确。现有评估框架仅关注理想条件下的单轮问答，未能涵盖医疗咨询中常见的复杂性（如冲突输入、误导性上下文）。

Method: 引入MedQA-Followup框架，系统评估医疗问答中的多轮鲁棒性。该框架区分浅层鲁棒性（抵抗初始误导上下文）和深层鲁棒性（在答案受质疑时保持准确性），并引入间接-直接轴（上下文框架与明确建议）。通过对MedQA数据集进行受控干预，评估了五种最先进的LLM。

Result: 模型在浅层干扰下表现尚可，但在多轮设置中表现出严重脆弱性，准确率从91.2%降至13.5%（Claude Sonnet 4）。出乎意料的是，间接的、基于上下文的干预往往比直接建议更有害，导致更大的准确率下降。进一步分析揭示了模型间的差异，一些模型在重复干预下性能持续下降，而另一些则部分恢复甚至提高。

Conclusion: 多轮鲁棒性是医疗LLM安全可靠部署的一个关键但尚未被充分探索的维度，本研究强调了其重要性。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了Chinese ModernBERT，一个专门为中文设计的Encoder-only Transformer，通过创新的架构和训练策略，在中文基准测试中表现出色，并在长序列处理和检索任务上展现了高效率和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有Encoder-only Transformer的进步未能完全应用于中文，因为中文在分词和形态学上与英文显著不同。

Method: 开发了Chinese ModernBERT，一个从头开始的中文编码器，主要方法包括：(i) 硬件感知32k BPE词汇表；(ii) 带有动态掩码课程的全词掩码（WWM）；(iii) 使用RoPE和交替局部/全局注意力，将上下文从1,024扩展到8,192的两阶段预训练管道；(iv) 阻尼余弦学习率调度。模型在约1.2T中文tokens上预训练，并通过少量开放对比数据在检索任务上进行微调。

Result: Chinese ModernBERT在CLUE基准上与强大的中文编码器表现相当。在bf16精度下，实现了高长序列吞吐量和强劲的短序列速度。在SimCLUE测试集上，通过加入T2Ranking数据微调后，达到0.505 (Pearson) / 0.537 (Spearman) 的性能，并超越Qwen-0.6B-embedding。

Conclusion: Chinese ModernBERT为中文编码器提供了有效的解决方案，通过定制化的设计在效率和性能上均有显著提升，并为语义相似性任务（STS）的进一步扩展指明了方向。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 提出并验证了一种可扩展的无监督管道，使用大型语言模型（LLMs）自动化大规模语料库的语法标注，以解决人工标注的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言语料库以前所未有的速度扩张，人工标注仍然是语料库语言学工作中一个显著的方法学瓶颈。

Method: 采用了一种无监督的四阶段工作流程：提示工程、事先评估、自动化批量处理和事后验证。该方法利用大型语言模型（如通过OpenAI API的GPT-5）进行自动化标注。

Result: 通过对英语“consider”结构变化的历时性案例研究，在不到60小时内标注了COHA中143,933个句子，并在两种复杂的标注程序上实现了98%以上的准确率。

Conclusion: 研究结果表明LLMs能够以最小的人工干预大规模执行数据准备任务，为基于语料库的研究开辟了新的可能性，但实施时需注意成本、许可和其他伦理考量。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出一个 novel 的RAG框架，将反驳言论生成建模为知识驱动的文本生成过程，以解决现有方法在可信度和连贯性方面的局限，并在多目标群体上取得了优于LLM基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的反驳言论生成方法（基于大型语言模型或NGO专家）在生成的文本可靠性、连贯性和可扩展性方面存在严重缺陷，无法有效对抗有害内容。

Method: 引入了一个新颖的框架，将反驳言论生成建模为知识驱动的文本生成过程，集成了先进的检索增强生成（RAG）管道。构建了一个包含32,792篇文本的知识库，来源包括联合国数字图书馆、EUR-Lex和欧盟基本权利机构。使用MultiTarget-CONAN数据集，并通过标准度量（JudgeLM）和人工评估来评估生成质量。

Result: 研究结果表明，该框架在标准度量和人工评估两方面均优于标准的LLM基线和竞争方法。

Conclusion: 该框架和知识库为研究可信赖且可靠的反驳言论生成（尤其是在仇恨言论领域）提供了新途径。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 研究发现，大语言模型与人脑活动的对齐（BA）和下一词预测（NWP）在所依赖的词语子集上存在显著差异：NWP侧重句法，BA侧重语义和篇章信息。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLM）与人脑活动的对齐，可以揭示语言处理的计算原理。具体动机是研究脑对齐（BA）与下一词预测（NWP）之间的关系，这是一个有争议的研究问题。

Method: 引入了一种细粒度的输入归因方法，以识别对脑-LLM对齐最重要的特定词语。并利用此方法研究BA与NWP的关系。

Result: BA和NWP依赖于截然不同的词语子集：NWP表现出近因和首因偏差，侧重句法；而BA优先处理语义和篇章级信息，具有更具目标性的近因效应。

Conclusion: 本工作增进了对LLM如何与人类语言处理相关的理解，并强调了BA和NWP在特征依赖上的差异。所提出的归因方法可广泛应用于探索模型预测在多种语言处理任务中的认知相关性。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE是一种即插即用的MoE推理卸载框架，通过混合大小专家（对不重要token使用半数专家加速，对重要token使用全部专家保证质量）和专门的预取机制，在消费级GPU系统上实现了1.60x至1.72x的加速，同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: MoE模型中的稀疏激活允许将活跃专家存放在GPU HBM，不活跃专家存放在CPU DRAM，但受限于CPU-GPU互连带宽。现有预取方法虽能加速，但存在训练开销大、对细粒度专家效果不佳的问题。

Method: 提出MoBiLE框架，采用“大小专家混合”策略，对不重要的token使用一半专家以加速，对重要的token使用全部专家以保证模型质量。此外，设计了专用的回退和预取机制来切换大小专家，以提高内存效率。

Result: 在四种典型现代MoE架构和生成任务上评估MoBiLE，结果显示其在消费级GPU系统上相比基线实现了1.60x至1.72x的加速，且精度下降可以忽略不计。

Conclusion: MoBiLE通过智能的专家管理和卸载策略，有效解决了MoE模型推理中的带宽瓶颈和现有预取方法的局限性，实现了显著的性能提升和高效的内存利用。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 研究发现LLM审稿人存在偏见：偏爱LLM生成论文并贬低包含批评性言论的人工撰写论文，对学术公平构成风险；但LLM审稿指导的修订有助于提升论文质量。


<details>
  <summary>Details</summary>
Motivation: LLMs被广泛整合进学术工作流程，但其在研究与审稿中的双重角色及潜在风险（尤其对学术公平的影响）尚待深入探讨。

Method: 通过模拟实验，设置研究代理（生成和修订论文）和审稿代理（评估投稿），模拟LLMs作为审稿人的过程。基于模拟结果，进行人工标注以识别LLM审稿与人类判断的差异。

Result: LLM审稿人系统性地提高LLM撰写论文的分数，并持续低估包含批评性言论的人工撰写论文，即使经过多次修订。这些偏见源于对LLM写作风格的语言特征偏好和对批评性言论的厌恶。然而，LLM审稿指导的修订在LLM和人类评估中都带来了质量提升。

Conclusion: 若不谨慎部署，LLM作为审稿人对人类作者和学术研究的公平性构成风险。但同时，它们也为早期研究者和提升低质量论文提供了潜力。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLM）的分词差异导致非拉丁语系和形态复杂语言的计算效率显著降低，成本更高，有效上下文利用率受损，揭示了AI系统中的结构性不平等。


<details>
  <summary>Details</summary>
Motivation: 分词差异是不同语言群体公平使用人工智能的重大障碍。本研究旨在通过大规模跨语言评估，系统地量化大型语言模型（LLM）中的计算不平等。

Method: 对200多种语言的分词效率进行了大规模跨语言评估。采用标准化实验框架，进行一致的预处理和归一化，并使用tiktoken库进行统一分词。收集了词元每句（TPS）和相对分词成本（RTC）等评估指标，并以英语为基准进行比较。

Result: 跨语言分析显示出显著且系统性的差异：拉丁语系语言的分词效率始终较高，而非拉丁语系和形态复杂的语言则面临显著的词元膨胀，RTC比率通常高出3-5倍。这些低效率导致了代表性不足语言的计算成本增加和有效上下文利用率降低。

Conclusion: 研究结果突出了当前AI系统中的结构性不平等，其中低资源和非拉丁语种的用户面临着不成比例的计算劣势。未来的研究应优先开发语言学上知情的分词策略和自适应词汇构建方法，以确保更具包容性和计算公平性的多语言AI系统。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出PRoH框架，通过动态规划和推理克服了现有基于知识超图（KH）的RAG在多跳问答中的局限性，实现了上下文感知规划、结构化问题分解和实体加权重叠引导的路径检索，显著提升了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识超图（KH）的RAG方法在多跳问答中存在三大局限：静态检索规划、非适应性检索执行以及对KH结构和语义的表面化利用。

Method: 提出PRoH框架，包含三项核心创新：1) 上下文感知规划模块，生成结构化推理计划；2) 结构化问题分解过程，将子问题组织成动态DAG以实现适应性探索；3) 实体加权重叠（EWO）引导的推理路径检索算法，优先选择语义一致的超边遍历。

Result: PRoH在多领域实验中取得了最先进的性能，F1分数平均超越先前SOTA模型HyperGraphRAG 19.73%，生成评估（G-E）分数提升8.41%，并在长程多跳推理任务中保持了强大的鲁棒性。

Conclusion: PRoH通过其动态规划和推理机制，显著提升了基于知识超图的RAG在多跳问答中的表现，实现了SOTA性能和强大的鲁棒性。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出CLEAR框架，通过探查LLM内部表征来解决RAG中因知识冲突导致的回复不忠实问题，显著提升准确性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在“不忠实”问题（模型回复与检索证据矛盾），且现有解决方案多为外部干预，忽视了LLM内部如何整合检索证据与参数记忆，特别是在知识冲突情境下。

Method: 首先，通过对LLM隐藏状态表征进行探查式分析，发现知识整合的层级性、冲突在句子层面表现为潜在信号，以及无关上下文在与参数知识对齐时常被放大。基于这些发现，提出CLEAR框架，该框架 (i) 将上下文分解为细粒度句子级知识，(ii) 利用隐藏状态探查定位冲突知识，(iii) 引入冲突感知微调来指导模型准确整合检索证据。

Result: 在三个基准测试中，CLEAR框架显著提高了模型的准确性和上下文忠实性，并在各种冲突条件下持续优于强基线模型。

Conclusion: 本研究通过深入探究LLM内部知识整合机制，特别是知识冲突的处理方式，提出并验证了CLEAR框架的有效性，成功解决了RAG系统中的忠实性问题，为RAG模型的未来发展提供了新的视角。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: LLM在形态泛化任务中展现出类人准确性，但其性能主要受语言资源丰富度而非语法复杂性驱动。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）的语言能力，特别是在涉及新词的形态泛化任务中，以确定其准确性是否接近人类，以及是受语言复杂性还是训练数据量影响。

Method: 采用多语言改编的Wug测试，在加泰罗尼亚语、英语、希腊语和西班牙语这四种部分不相关语言上测试了六个模型，并与人类说话者进行比较。

Result: 模型能够以类人准确性将形态过程泛化到未见过的新词。然而，模型准确性模式更与社区规模和数据可用性而非结构复杂性相关，资源更丰富的语言（如西班牙语、英语）准确性更高。

Conclusion: 模型行为主要由语言资源的丰富性而非对语法复杂性的敏感性驱动，其性能仅在表面上类似于人类语言能力。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出一种名为SMEC的嵌入压缩框架，通过引入SMRL、ADS和S-XBM模块，有效解决大语言模型高维嵌入带来的计算与存储问题，实现性能保持下的显著降维。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）生成的高维嵌入虽然捕捉丰富语义和句法信息，但其高维度特性导致计算复杂性与存储需求剧增，严重阻碍实际部署。

Method: 本文提出Sequential Matryoshka Embedding Compression (SMEC) 训练框架，包含以下核心组件：
1. Sequential Matryoshka Representation Learning (SMRL) 方法：旨在缓解训练过程中的梯度方差。
2. Adaptive Dimension Selection (ADS) 模块：用于减少维度剪枝时可能发生的信息降级。
3. Selectable Cross-batch Memory (S-XBM) 模块：用于增强高维和低维嵌入之间的无监督学习。

Result: 在图像、文本和多模态数据集上的实验表明，SMEC在实现显著降维的同时，能够保持模型性能。例如，在BEIR数据集上，与Matryoshka-Adaptor和Search-Adaptor模型相比，SMEC将压缩后的LLM2Vec嵌入（256维）性能分别提升了1.1和2.7点。

Conclusion: SMEC框架能够有效实现大语言模型高维嵌入的降维，并成功保持其性能，从而克服了计算和存储瓶颈，促进实际部署。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文首次研究个性化机生文本检测，提出基准数据集\dataset和方法\method。研究发现现有检测器在个性化设置下性能大幅下降，原因在于“特征反转陷阱”；\method能有效预测检测器性能变化，与实际差距相关性达85%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本生成方面日益强大，能够生成流畅文本甚至模仿个人风格，这同时增加了身份冒充的风险。据作者所知，目前尚无针对个性化机生文本（MGT）检测的研究。

Method: 1. 引入\dataset，首个用于评估检测器在个性化设置下鲁棒性的基准数据集，由文学和博客文本及其LLM模仿生成文本构成。
2. 提出\method，一种预测检测器在个性化设置下性能变化的简单可靠方法。该方法识别与反转特征对应的潜在方向，并构建主要沿这些特征不同的探测数据集来评估检测器依赖性。

Result: 1. 实验结果显示，在个性化设置下，检测器之间存在巨大性能差距，一些最先进模型性能显著下降。
2. 将此局限性归因于“特征反转陷阱”，即在通用领域具有区分性的特征在个性化文本中变得反转和误导。
3. \method能准确预测性能变化的趋势和幅度，与实际性能差距的相关性达到85%。

Conclusion: 本工作有望鼓励个性化文本检测领域的进一步研究。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 研究将LLM的测试时缩放技术应用于LeWiDi-2025任务，以评估标注分歧。发现模型平均和多数投票能提升性能，但Best-of-N方法无效。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放技术此前主要应用于有明确正确答案的领域（如数学和编程）。本研究旨在将其推广到LeWiDi任务，以解决其中存在的标注分歧问题。

Method: 实验使用了三种测试时缩放方法：模型平均（Model Averaging）、多数投票（Majority Voting）和Best-of-N抽样。

Result: 模型平均和多数投票两种基准方法持续提升了LLM在LeWiDi任务上的性能。然而，Best-of-N抽样方法未能提高性能。

Conclusion: Best-of-N方法目前未能从数学领域成功迁移到LeWiDi任务中。论文分析了导致此现象的潜在原因。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: VLM在面对非典型实例时，若视觉与文本输入不一致，其概念理解会下降，且这种下降受语用先验（输入一致性）的影响大于语义先验（普遍概念）。


<details>
  <summary>Details</summary>
Motivation: 探究VLM在分析非典型实例时，如何权衡语用先验（输入一致性）和语义先验（概念普遍性），以及这种权衡如何影响其概念理解。

Method: 引入新评估数据集VISaGE，包含典型和异常图像，并通过精心平衡的实验进行评估。

Result: 当输入图像与文本不一致（违反语用先验）时，VLM的概念理解能力显著下降。这种影响比语义先验在查询单个实例时产生的影响更强。

Conclusion: VLM在处理非典型实例时，其概念理解对输入的一致性（语用先验）的依赖性大于其内部存储的普遍概念知识（语义先验），输入一致性被破坏时模型性能受影响更大。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在表达自身不确定性时存在“忠实度差距”。本文提出了一种名为FUT的微调方法，旨在教授LLMs忠实地表达不确定性，同时不改变其核心答案分布，实验证明能有效缩小这一差距并保持问答准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在面对重复查询时常给出不同答案，但其生成的回应却往往不带或不恰当地带有不确定性提示，无法真实反映其知识的不确定状态，这导致了“忠实度差距”，即使是强大的LLMs也受此影响。

Method: 提出“忠实不确定性调优（FUT）”微调方法。通过根据样本一致性，使用不确定性提示词（如“可能”、“或许”）增强模型样本来构建训练数据，从而在不改变底层答案分布的情况下，教授指令调优的LLMs忠实地表达不确定性。该方法仅需模型和一组提示，无需额外监督。

Result: FUT显著缩小了忠实度差距，同时保持了问答准确性，并引入了最小的语义分布偏移。进一步分析表明，FUT在解码策略、不确定性提示词选择以及其他形式的不确定性表达（如数值）方面均表现出鲁棒性。

Conclusion: FUT被确立为一种简单有效的方法，可以教会大语言模型忠实地传达不确定性。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: StyleDecipher是一个鲁棒且可解释的检测框架，通过量化风格差异来识别LLM生成文本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的广泛应用使得检测机器生成文本成为确保内容真实性的关键。然而，现有方法因泛化能力有限、易受改写攻击及缺乏可解释性，难以应对现实世界中多样化的文体和人机混合内容。

Method: 本文提出StyleDecipher，一个鲁棒且可解释的检测框架。它通过结合离散风格指标和语义嵌入导出的连续风格表示，共同建模并量化人类和LLM输出在风格层面的差异，从而在统一表示空间中捕捉独特的风格分歧。该框架无需访问模型内部或标注片段，即可实现准确、可解释且领域无关的检测。

Result: StyleDecipher在新闻、代码、论文、评论和学术摘要等五个多样化领域进行实验。结果表明，它在域内检测中持续达到最先进的准确性。在跨域评估中，StyleDecipher超越现有基线高达36.30%，并对对抗性扰动和人机混合内容保持鲁棒性。定性和定量分析进一步证实，风格信号为区分机器生成文本提供了可解释的证据。

Conclusion: StyleDecipher提供了一个基于风格差异的鲁棒、准确、可解释且领域无关的机器生成文本检测解决方案，有效解决了现有方法的局限性，并为内容真实性保障提供了新的视角。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: ACADATA是一个高质量的学术翻译平行数据集，通过微调LLM在学术和长上下文翻译上取得显著提升，并超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 为了提供高质量的学术翻译平行数据集，并提升大型语言模型在学术和长上下文翻译领域的表现。

Method: 构建了包含ACAD-TRAIN（150万段落对）和ACAD-BENCH（6000条翻译）的ACADATA数据集，并使用ACAD-TRAIN对LLM进行微调，然后在ACAD-BENCH上进行基准测试。

Result: 微调LLM在学术翻译质量上平均提升了+6.1 (7B模型) 和+12.4 (2B模型) d-BLEU点；英译非英语时通用领域长上下文翻译提升高达24.9%；微调后的最佳模型超越了最佳专有和开源模型。

Conclusion: ACADATA数据集和微调模型为学术领域及长上下文翻译研究提供了宝贵的社区资源，有助于推动相关领域发展。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: 本研究引入了COSTAR-A，一个增强型提示工程框架，通过增加'Answer'组件，提高了小型LLMs（特别是Llama 3.1-8B）在资源受限环境下的输出一致性和决策性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示设计高度敏感，但现有COSTAR框架在小型、本地化模型上的性能不一致，尤其是在需要指令性或受限输出的任务中，这促使研究者寻求更优的提示技术。

Method: 引入了COSTAR-A，该框架在COSTAR（Context, Objective, Style, Tone, Audience, Response）基础上增加了'Answer'组件。通过对参数量不超过80亿的微调小型模型进行了一系列受控的提示-输出评估。

Result: COSTAR-A能增强本地化LLMs在特定任务中的输出结构和决策性，尽管其有效性因模型和用例而异。值得注意的是，Llama 3.1-8B模型在使用COSTAR-A提示时性能优于单独使用COSTAR。

Conclusion: COSTAR-A作为一个提示框架展现了良好的适应性和可扩展性，特别适用于资源受限硬件上的计算高效AI部署场景。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: LLMs在SFT+RLVR范式下推理能力显著，但SFT阶段人工标注推理路径成本高昂。本文发现对于模式化推理任务，推理模式而非标注数量或质量是关键。提出PARO框架，使LLM无需人工标注即可生成模式化推理路径，性能媲美10倍量级的人工标注，大幅降低成本。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）在SFT+RLVR范式下展现出卓越的推理能力，但其SFT阶段所需的高质量人工标注推理路径（rationales）成本过高，限制了该范式的广泛应用。

Method: 本文首先研究何时以及如何降低推理路径标注成本。识别出一类“模式化推理任务”，其中推理遵循固定、程序化的策略。以数值语义匹配任务为例，通过因果和行为证据证明推理模式是性能的关键决定因素。在此基础上，提出Pattern-Aware LLMs as Rationale AnnOtators (PARO) 框架，该框架使LLMs无需人工标注即可生成符合任务特定推理模式的推理路径。

Result: 研究发现，对于模式化推理任务，推理模式是性能的关键决定因素，而非推理路径标注的数量或质量。PARO框架生成的推理路径，在SFT+RLVR范式下的性能与数量大10倍的人工标注推理路径相当。

Conclusion: 大规模人工推理路径标注可以通过基于LLM的自动标注取代，而后者仅需对推理模式进行有限的人工监督。这为降低LLMs推理能力开发的成本和效率提供了新的途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 本文提出“有效生成空间大小（GSS）”概念，用于统一并解决大语言模型在开放式生成任务中多样性校准不足的问题（如创意任务过于单一，事实任务产生幻觉）。研究引入GSSBench评估基准，发现幻觉检测指标（特别是EigenScore）表现最佳，并展示GSS在检测提示歧义、解释推理模型行为和指导模型生成方面的应用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在开放式生成任务中对输出多样性的校准存在问题，具体表现为创意任务输出过于单一，而事实性任务则产生多样但错误的结果（幻觉）。研究旨在通过“有效生成空间大小（GSS）”的概念统一并解决这两种失败模式。

Method: 提出了GSSBench，一个包含具有真实GSS关系的提示对的任务套件，用于评估不同的GSS度量指标并分析模型偏离预期行为的原因。同时，研究侧重于利用模型内部信息来评估各种多样性和不确定性量化指标，以及幻觉检测指标。

Result: 研究发现，幻觉检测指标（特别是EigenScore）在评估GSS方面表现优于传统的多样性与不确定性量化指标，并且仅利用模型内部信息，提供了对模型内部任务表征的可解释性洞察。此外，研究展示了GSS的三个实际应用：1) 检测提示歧义并预测澄清问题；2) 解释推理模型中的过度思考和思考不足；3) 指导模型扩展生成空间以获得高质量和多样性的输出。

Conclusion: GSS是一个统一且有效的概念，可以解决大语言模型在开放式生成任务中多样性校准的失败模式。通过GSSBench和相关指标（如EigenScore），可以更好地理解和干预模型的生成行为，从而提升模型在不同任务中的表现，实现高质量和多样化的输出。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文系统性地研究了全能语言模型（OLMs）的细粒度多模态感知。提出了Omni-Detective数据生成管道以创建高细节低幻觉数据，并基于此训练了Audio-Captioner和Omni-Captioner两个模型，在多个基准测试中取得了SOTA。同时，设计了Omni-Cloze作为新的全能细粒度感知评估基准。


<details>
  <summary>Details</summary>
Motivation: 细粒度多模态信息感知对于人机交互至关重要，但现有全能语言模型（OLMs）捕获和描述细粒度细节的能力探索不足，且存在细节与幻觉“共同增长”的问题。此外，缺乏专门针对全能细粒度感知的评估基准。

Method: 1. 提出了Omni-Detective：一个集成了工具调用的代理式数据生成管道，用于自主生成高细节、低幻觉的多模态数据。2. 训练了两个字幕模型：Audio-Captioner（用于纯音频细粒度感知）和 Omni-Captioner（用于音视频细粒度感知），均基于Omni-Detective生成的数据。3. 设计了Omni-Cloze：一种新型完形填空式评估方法，用于稳定、高效、可靠地评估详细的音频、视觉和音视频字幕。

Result: 1. Audio-Captioner在MMAU和MMAR基准上超越所有开源模型，性能优于Gemini 2.5 Flash并与Gemini 2.5 Pro相当。2. Omni-Captioner在VDC详细字幕基准上取得了新的SOTA，并在video-SALMONN 2测试集上实现了细节与幻觉的最佳权衡。3. 实验证明Omni-Detective在生成高质量详细字幕方面有效。4. Omni-Cloze在评估此类详细字幕方面表现出优越性。

Conclusion: 本研究通过提出创新的数据生成管道Omni-Detective、训练高性能的细粒度感知模型以及设计可靠的评估基准Omni-Cloze，有效解决了全能语言模型在细粒度多模态感知中面临的细节捕捉、幻觉平衡和评估缺失等挑战，显著提升了该领域的能力。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本文扩展了对语言模型归纳偏见的现有研究，通过使用广义范畴语法（GCG）构建更接近自然语言的人工语言，并着重评估模型对未见长句的泛化能力，发现类型学上合理词序对语言模型而言更易泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究已探讨语言模型是否偏好类型学上常见的语法特性，但通常使用上下文无关的人工语言。本文旨在从两个方面扩展这些工作：一是使人工语言能覆盖更多自然语言的结构（如无界依存和轻度上下文敏感结构），二是更侧重评估语言模型处理未见长句的泛化能力。

Method: 1. 采用广义范畴语法（GCG）扩展了现有研究的上下文无关人工语言形式化，以覆盖无界依存和轻度上下文敏感等此前被忽视的结构。 2. 评估侧重于语言模型处理未见长测试句的泛化能力。

Result: 类型学上合理的词序更易于语言模型进行生产性泛化。

Conclusion: 本研究的人工语言能更好地捕捉自然语言特征，且实验范式得出了更清晰的结论：语言模型更容易生产性地泛化类型学上合理的词序。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 本研究提出DGRC方法，利用“at-issueness”概念评估语言模型（LMs）的对话自然度。结果显示，LMs倾向于在at-issue内容上继续对话，并能根据提示词调节这种偏好。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型对话的自然度并非易事，因为“自然度”的概念定义各异，且缺乏可扩展的量化指标。

Method: 引入新方法：DGRC（Divide, Generate, Recombine, and Compare），利用“at-issueness”的语言学概念来评估对话自然度。DGRC通过（i）将对话划分为提示，（ii）使用LMs为子部分生成续写，（iii）重组对话和续写，以及（iv）比较重组序列的似然性。

Result: LMs倾向于在at-issue内容上继续对话，且指令微调模型此效果更明显。当存在相关提示词（如“等等”）时，LMs会降低其at-issue偏好。

Conclusion: 观察到的对话模式反映了成功对话动态的关键特征，尽管指令微调模型未进一步增强这种调制。DGRC方法有助于缓解语言分析中的偏见，并能系统测试LMs的话语敏感行为。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 本文认为基于索绪尔和乔姆斯基理论对LLMs的语言学评论是无效的，提出转向Mańczak的经验主义原则，将语言定义为所有说和写内容的集合，以频率为主要规则，以此挑战LLM的既有批判并提供设计指南。


<details>
  <summary>Details</summary>
Motivation: 现有基于索绪尔和乔姆斯基理论对LLMs的语言学评论往往是推测性和低效的，且难以解决LLMs是否能真正模拟语言的质疑（如对“深层结构”或“基础”的需求）。因此需要一种新的视角来有效设计、评估和解释语言模型。

Method: 本文提出向Witold Mańczak的经验主义原则进行根本性转变。Mańczak将语言定义为所有说和写内容的总体，并视使用频率为语言的首要管理原则。作者利用这一框架来挑战此前对LLMs的批判。

Result: 通过采用Mańczak的经验主义框架，本文挑战了先前对LLMs的批判，并为设计、评估和解释语言模型提供了一个建设性的指导。

Conclusion: 转向Mańczak的经验主义视角，能够更有效地理解和评价LLMs，克服现有理论框架的局限性，并为语言模型的发展提供实用指导。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM是一个可追溯框架，通过轻量级路由器动态调整LLM层的使用，以在计算预算下提高推理效率和准确性，而无需修改模型基础权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对每个token进行全层处理，导致简单查询计算浪费，且难以灵活应对需要更深推理的复杂查询。现有自适应深度方法成本高昂（推理时搜索、架构改变或大规模重训练），并常常在提高效率的同时降低准确性。

Method: 本文提出了Dr.LLM，即LLM层动态路由，这是一个可追溯的框架。它为预训练模型配备了轻量级逐层路由器，这些路由器决定跳过、执行或重复一个块。路由器通过蒙特卡洛树搜索（MCTS）进行显式监督训练，以在给定计算预算下得出高质量的层配置。其设计采用窗口池化以实现稳定路由，结合类别平衡的焦点损失，以及瓶颈MLP路由器，以确保在类别不平衡和长序列下的鲁棒性。

Result: 在ARC（逻辑）和DART（数学）任务上，Dr.LLM将准确性提高了高达+3.4%p，同时平均每个示例节省了5层。路由器可泛化到域外任务（MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval），仅导致0.85%的准确性下降，同时保持了效率，并且表现优于之前的路由方法高达+7.7%p。

Conclusion: Dr.LLM通过显式监督的路由器，在不改变基础权重的情况下，成功地改造了冻结的LLM，实现了预算感知且精度驱动的推理，证明了其在提高LLM效率和准确性方面的潜力。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 本研究量化了低资源语言（如Bambara语）语音数据转录所需的人工成本，发现在实验室条件下每小时语音需30小时人工，现场条件下需36小时。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言创建语音数据集是一个关键但人工成本不明的挑战。

Method: 通过一项为期一个月的田野研究，十名母语转录员校正了53小时Bambara语ASR生成的转录文本。

Result: 准确转录一小时语音数据，在实验室条件下平均需要30小时人工，在现场条件下平均需要36小时人工。

Conclusion: 该研究为创建NLP资源的类似低资源语言提供了一个基准和实用见解。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [Enhancing the Quality of 3D Lunar Maps Using JAXA's Kaguya Imagery](https://arxiv.org/abs/2510.11817)
*Yumi Iwashita,Haakon Moe,Yang Cheng,Adnan Ansar,Georgios Georgakis,Adrian Stoica,Kazuto Nakashima,Ryo Kurazume,Jim Torresen*

Main category: cs.CV

TL;DR: 本文提出一种方法，通过降低Kaguya TC图像压缩引起的视差图中噪声，提高3D月球地图的质量，从而增强未来月球任务地形数据的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着月球探索任务（如NASA的Endurance任务）的深入，对高质量3D月球地图的需求日益迫切。然而，现有的Kaguya TC图像在全球范围内可用，但因立体匹配误差和JPEG压缩伪影导致高程精度不足。

Method: 分析Kaguya TC图像的压缩行为，识别出系统性的视差噪声模式（尤其在较暗区域）。提出一种方法来减少压缩图像衍生出的视差图中的残余噪声，以提升3D地图质量。

Result: 实验结果表明，所提出的方法能有效降低高程噪声。

Conclusion: 该方法提高了地形数据的安全性和可靠性，对未来的月球任务具有重要意义。

Abstract: As global efforts to explore the Moon intensify, the need for high-quality 3D
lunar maps becomes increasingly critical-particularly for long-distance
missions such as NASA's Endurance mission concept, in which a rover aims to
traverse 2,000 km across the South Pole-Aitken basin. Kaguya TC (Terrain
Camera) images, though globally available at 10 m/pixel, suffer from altitude
inaccuracies caused by stereo matching errors and JPEG-based compression
artifacts. This paper presents a method to improve the quality of 3D maps
generated from Kaguya TC images, focusing on mitigating the effects of
compression-induced noise in disparity maps. We analyze the compression
behavior of Kaguya TC imagery, and identify systematic disparity noise
patterns, especially in darker regions. In this paper, we propose an approach
to enhance 3D map quality by reducing residual noise in disparity images
derived from compressed images. Our experimental results show that the proposed
approach effectively reduces elevation noise, enhancing the safety and
reliability of terrain data for future lunar missions.

</details>


### [64] [Data or Language Supervision: What Makes CLIP Better than DINO?](https://arxiv.org/abs/2510.11835)
*Yiming Liu,Yuhui Zhang,Dhruba Ghosh,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 研究发现，在受控设置下，CLIP捕获高级语义，DINO捕获低级特征。在VLM中，CLIP擅长文本密集任务，DINO擅长视觉中心任务，且语言监督变体效果有限。


<details>
  <summary>Details</summary>
Motivation: CLIP作为视觉语言模型（VLM）的视觉编码器表现优于DINO等自监督模型，但尚不清楚这种优势是来自语言监督还是更大的训练数据。本研究旨在解开这些因素。

Method: 在受控设置下（相同架构、数据集和训练配置）预训练CLIP和DINO，使其ImageNet准确率相似。然后进行嵌入分析，并将其集成到VLM中，在20个VQA基准上进行评估。同时，探索了不同的语言监督变体。

Result: 嵌入分析显示，CLIP捕获高级语义（如对象类别、文本），而DINO对低级特征（如颜色、样式）更敏感。集成到VLM后，CLIP在文本密集型任务上表现出色，而DINO在视觉中心型任务上略有优势。此外，不同语言监督变体（如Sigmoid损失、预训练语言编码器）带来的性能提升有限。

Conclusion: 本研究为视觉编码器设计及其对VLM性能的影响提供了科学见解，揭示了语言监督和训练数据在不同语义层次和任务类型上的作用。

Abstract: CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.

</details>


### [65] [MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images](https://arxiv.org/abs/2510.11883)
*Sicheng Zhou,Lei Wu,Cao Xiao,Parminder Bhatia,Taha Kass-Hout*

Main category: cs.CV

TL;DR: MammoDINO是一个新的自监督学习框架，利用1.4百万张乳腺图像，通过定制的数据增强和跨层对比学习，在多个乳腺癌筛查任务中实现了最先进的性能，为医学影像提供了一种可扩展、无需标注的解决方案。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在通用视觉领域取得了巨大成功，但在医学影像领域（特别是乳腺X光检查）因数据限制和领域特有偏差而未被充分利用。

Method: 提出了MammoDINO框架，在1.4百万张乳腺X光图像上进行预训练。引入了乳腺组织感知的图像级和补丁级数据增强采样器，并设计了利用3D数字乳腺断层合成（DBT）结构的跨层对比学习目标，将其应用于2D预训练。

Result: MammoDINO在多个乳腺癌筛查任务中取得了最先进（SOTA）的性能，并在五个基准数据集上表现出良好的泛化能力。

Conclusion: MammoDINO为乳腺X光检查的计算机辅助诊断（CAD）工具提供了一个可扩展、无需标注的基础，有助于减轻放射科医生的工作量，并提高乳腺癌筛查的诊断效率。

Abstract: Self-supervised learning (SSL) has transformed vision encoder training in
general domains but remains underutilized in medical imaging due to limited
data and domain specific biases. We present MammoDINO, a novel SSL framework
for mammography, pretrained on 1.4 million mammographic images. To capture
clinically meaningful features, we introduce a breast tissue aware data
augmentation sampler for both image-level and patch-level supervision and a
cross-slice contrastive learning objective that leverages 3D digital breast
tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves
state-of-the-art performance on multiple breast cancer screening tasks and
generalizes well across five benchmark datasets. It offers a scalable,
annotation-free foundation for multipurpose computer-aided diagnosis (CAD)
tools for mammogram, helping reduce radiologists' workload and improve
diagnostic efficiency in breast cancer screening.

</details>


### [66] [Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis](https://arxiv.org/abs/2510.11907)
*Blessing Agyei Kyem,Neema Jakisa Owor,Andrews Danyo,Joshua Kofi Asamoah,Eugene Denteh,Tanner Muturi,Anthony Dontoh,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出一个双模型框架，利用VideoLLaMA和Qwen2.5-VL的互补优势，通过为视频描述和视觉问答任务进行独立训练，以提升交通安全视频理解能力，并在AI City Challenge中取得第10名的成绩。


<details>
  <summary>Details</summary>
Motivation: 交通安全分析需要复杂的视频理解能力，以捕捉精细的行为模式并生成全面的描述，从而有效预防事故。

Method: 本研究提出一个独特的双模型框架，战略性地利用VideoLLaMA和Qwen2.5-VL的互补优势，通过针对特定任务的优化来解决问题。核心思想是将视频描述（captioning）和视觉问答（VQA）任务的训练分开，以最大程度地减少任务干扰，使每个模型能更有效地进行专业化。VideoLLaMA侧重时序推理，Qwen2.5-VL侧重视觉理解。

Result: 实验结果表明，VideoLLaMA在时序推理方面表现出色，CIDEr得分为1.1001；Qwen2.5-VL在视觉理解方面表现突出，VQA准确率为60.80%。在WTS数据集上的广泛实验中，该方法在2025 AI City Challenge Track 2中取得了45.7572的S2得分，位列挑战赛排行榜第10名。消融研究验证了独立训练策略在保持描述质量的同时，VQA准确率比联合训练提高了8.6%。

Conclusion: 所提出的双模型框架，通过任务分离训练策略，成功利用了VideoLLaMA和Qwen2.5-VL各自在时序推理和视觉理解上的优势，显著提升了交通安全视频理解的性能，并在国际竞赛中展现出强大的竞争力。

Abstract: Traffic safety analysis requires complex video understanding to capture
fine-grained behavioral patterns and generate comprehensive descriptions for
accident prevention. In this work, we present a unique dual-model framework
that strategically utilizes the complementary strengths of VideoLLaMA and
Qwen2.5-VL through task-specific optimization to address this issue. The core
insight behind our approach is that separating training for captioning and
visual question answering (VQA) tasks minimizes task interference and allows
each model to specialize more effectively. Experimental results demonstrate
that VideoLLaMA is particularly effective in temporal reasoning, achieving a
CIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a
VQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, our
method achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,
placing 10th on the challenge leaderboard. Ablation studies validate that our
separate training strategy outperforms joint training by 8.6\% in VQA accuracy
while maintaining captioning quality.

</details>


### [67] [PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation](https://arxiv.org/abs/2510.11992)
*Hatem Ibrahem,Ahmed Salem,Qinmin Vivian Hu,Guanghui Wang*

Main category: cs.CV

TL;DR: 本文提出PanoTPS-Net模型，利用CNN和TPS变换从单张全景图像高效、鲁棒地估计房间3D布局，支持立方体和非立方体房间类型。


<details>
  <summary>Details</summary>
Motivation: 准确估计房间3D布局对机器人、增强现实和室内设计等领域至关重要。

Method: 提出PanoTPS-Net模型，结合CNN提取特征学习TPS变换参数，然后通过TPS层将参考布局形变为所需布局。该两阶段架构使模型能从单个全景图预测房间布局，并泛化到立方体和非立方体结构。

Result: 在多个公开数据集上的实验表明，模型在房间布局估计中表现出高精度和鲁棒性，尤其能有效处理立方体和非立方体布局。3DIoU值在PanoContext、Stanford-2D3D、Matterport3DLayout和ZInD数据集上分别达到85.49、86.16、81.76和91.98。

Conclusion: PanoTPS-Net成功实现了从单一全景图进行高精度、鲁棒的3D房间布局估计，特别是对立方体和非立方体布局的良好泛化能力，并验证了TPS变换与全景图像的兼容性。

Abstract: Accurately estimating the 3D layout of rooms is a crucial task in computer
vision, with potential applications in robotics, augmented reality, and
interior design. This paper proposes a novel model, PanoTPS-Net, to estimate
room layout from a single panorama image. Leveraging a Convolutional Neural
Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial
transformation, the architecture of PanoTPS-Net is divided into two stages:
First, a convolutional neural network extracts the high-level features from the
input images, allowing the network to learn the spatial parameters of the TPS
transformation. Second, the TPS spatial transformation layer is generated to
warp a reference layout to the required layout based on the predicted
parameters. This unique combination empowers the model to properly predict room
layouts while also generalizing effectively to both cuboid and non-cuboid
layouts. Extensive experiments on publicly available datasets and comparisons
with state-of-the-art methods demonstrate the effectiveness of the proposed
method. The results underscore the model's accuracy in room layout estimation
and emphasize the compatibility between the TPS transformation and panorama
images. The robustness of the model in handling both cuboid and non-cuboid room
layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and
91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets,
respectively. The source code is available at:
https://github.com/HatemHosam/PanoTPS_Net.

</details>


### [68] [Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning](https://arxiv.org/abs/2510.11996)
*Tanner Muturi,Blessing Agyei Kyem,Joshua Kofi Asamoah,Neema Jakisa Owor,Richard Dyzinela,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一个针对大规模3D仓库环境的空间推理框架，通过将边界框坐标嵌入提示来增强空间理解，并在多项任务上微调，最终在AI City Challenge中排名第四。


<details>
  <summary>Details</summary>
Motivation: 大规模3D仓库环境中的空间推理对视觉-语言系统构成重大挑战，现有模型因依赖局部外观且缺乏明确空间基础而难以泛化。

Method: 引入一个专用的空间推理框架，通过将边界框坐标作为掩码维度直接嵌入到输入提示中。该框架在距离估计、物体计数、多选定位和空间关系推理四类问题上进行任务特定监督的微调，并在训练集中将标准化答案附加到GPT响应中以提高一致性。

Result: 该综合流程在公共排行榜上获得了73.0606的最终分数，总排名第四。

Conclusion: 研究结果表明，结构化提示丰富和目标优化能有效提升真实工业环境中的空间推理能力。

Abstract: Spatial reasoning in large-scale 3D environments such as warehouses remains a
significant challenge for vision-language systems due to scene clutter,
occlusions, and the need for precise spatial understanding. Existing models
often struggle with generalization in such settings, as they rely heavily on
local appearance and lack explicit spatial grounding. In this work, we
introduce a dedicated spatial reasoning framework for the Physical AI Spatial
Intelligence Warehouse dataset introduced in the Track 3 2025 AI City
Challenge. Our approach enhances spatial comprehension by embedding mask
dimensions in the form of bounding box coordinates directly into the input
prompts, enabling the model to reason over object geometry and layout. We
fine-tune the framework across four question categories namely: Distance
Estimation, Object Counting, Multi-choice Grounding, and Spatial Relation
Inference using task-specific supervision. To further improve consistency with
the evaluation system, normalized answers are appended to the GPT response
within the training set. Our comprehensive pipeline achieves a final score of
73.0606, placing 4th overall on the public leaderboard. These results
demonstrate the effectiveness of structured prompt enrichment and targeted
optimization in advancing spatial reasoning for real-world industrial
environments.

</details>


### [69] [Evaluating the Explainability of Vision Transformers in Medical Imaging](https://arxiv.org/abs/2510.12021)
*Leili Barekatain,Ben Glocker*

Main category: cs.CV

TL;DR: 本研究评估了不同Vision Transformer架构及其预训练策略在医疗图像分类任务中的可解释性，发现DINO结合Grad-CAM能提供最忠实和局部的解释，支持ViTs在医疗诊断中的可靠应用。


<details>
  <summary>Details</summary>
Motivation: 医疗影像中模型决策的可解释性对临床信任和采纳至关重要。Vision Transformers (ViTs) 在诊断成像中表现出色，但其复杂的注意力机制给解释性带来了挑战。

Method: 研究评估了ViT、DeiT、DINO和Swin Transformer等不同Vision Transformer架构及其预训练策略的可解释性。采用Gradient Attention Rollout和Grad-CAM两种解释方法，并在外周血细胞分类和乳腺超声图像分类这两个医学影像任务上进行了定量和定性分析。

Result: 研究发现，DINO结合Grad-CAM在不同数据集上提供了最忠实和局部的解释。Grad-CAM始终能生成类别判别性且空间精确的热图，而Gradient Attention Rollout则产生更分散的激活。即使在误分类情况下，DINO结合Grad-CAM也能突出导致模型误判的临床相关形态特征。

Conclusion: 通过提高模型透明度，本研究支持Vision Transformers可靠且可解释地整合到关键医疗诊断工作流程中。

Abstract: Understanding model decisions is crucial in medical imaging, where
interpretability directly impacts clinical trust and adoption. Vision
Transformers (ViTs) have demonstrated state-of-the-art performance in
diagnostic imaging; however, their complex attention mechanisms pose challenges
to explainability. This study evaluates the explainability of different Vision
Transformer architectures and pre-training strategies - ViT, DeiT, DINO, and
Swin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct
both quantitative and qualitative analyses on two medical imaging tasks:
peripheral blood cell classification and breast ultrasound image
classification. Our findings indicate that DINO combined with Grad-CAM offers
the most faithful and localized explanations across datasets. Grad-CAM
consistently produces class-discriminative and spatially precise heatmaps,
while Gradient Attention Rollout yields more scattered activations. Even in
misclassification cases, DINO with Grad-CAM highlights clinically relevant
morphological features that appear to have misled the model. By improving model
transparency, this research supports the reliable and explainable integration
of ViTs into critical medical diagnostic workflows.

</details>


### [70] [APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2510.12056)
*Xinxin Huang,Han Sun,Junmin Cai,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: APGNet是一个自适应先验引导网络，通过结合Siamese架构、新颖的先验引导机制以及图像增强和多尺度特征提取策略，有效解决了水下伪装物体检测中的图像退化和生物伪装挑战，并在两个公共MAS数据集上超越了15种现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 水下伪装物体检测对海洋生态研究和资源勘探至关重要。然而，现有方法面临两大挑战：水下图像退化（低对比度、颜色失真）以及海洋生物的自然伪装。传统图像增强和陆地伪装检测方法难以适应水下环境特性。

Method: 提出APGNet（自适应先验引导网络），整合Siamese架构和新颖的先验引导机制。具体包括：1) 使用MSRCR算法进行数据增强，生成光照不变图像以减轻退化；2) 设计扩展感受野（ERF）模块结合多尺度渐进解码器（MPD），捕捉多尺度上下文信息并优化特征表示；3) 提出自适应先验引导机制，通过在高层特征中嵌入空间注意力进行粗定位，并在低层特征中使用可变形卷积细化轮廓，分层融合位置和边界先验。

Result: 在两个公共MAS数据集上进行了广泛实验，结果表明APGNet在广泛使用的评估指标下，性能优于15种最先进（state-of-art）的方法。

Conclusion: APGNet通过创新的网络架构和多方面策略，有效解决了水下伪装物体检测的难题，显著提升了检测的鲁棒性和准确性，并超越了现有技术水平。

Abstract: Detecting camouflaged objects in underwater environments is crucial for
marine ecological research and resource exploration. However, existing methods
face two key challenges: underwater image degradation, including low contrast
and color distortion, and the natural camouflage of marine organisms.
Traditional image enhancement techniques struggle to restore critical features
in degraded images, while camouflaged object detection (COD) methods developed
for terrestrial scenes often fail to adapt to underwater environments due to
the lack of consideration for underwater optical characteristics.
  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,
which integrates a Siamese architecture with a novel prior-guided mechanism to
enhance robustness and detection accuracy. First, we employ the Multi-Scale
Retinex with Color Restoration (MSRCR) algorithm for data augmentation,
generating illumination-invariant images to mitigate degradation effects.
Second, we design an Extended Receptive Field (ERF) module combined with a
Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual
information and refine feature representations. Furthermore, we propose an
adaptive prior-guided mechanism that hierarchically fuses position and boundary
priors by embedding spatial attention in high-level features for coarse
localization and using deformable convolution to refine contours in low-level
features.
  Extensive experimental results on two public MAS datasets demonstrate that
our proposed method APGNet outperforms 15 state-of-art methods under widely
used evaluation metrics.

</details>


### [71] [VIDMP3: Video Editing by Representing Motion with Pose and Position Priors](https://arxiv.org/abs/2510.12069)
*Sandeep Mishra,Oindrila Saha,Alan C. Bovik*

Main category: cs.CV

TL;DR: VidMP3利用姿态和位置先验，实现了在保持原始运动的同时，对视频中的对象进行结构和语义灵活的编辑，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 运动保持视频编辑在对象结构和语义需灵活的场景下至关重要，但现有扩散模型在结构可变编辑中存在时间不一致、主体身份漂移和人工干预等问题。

Method: 提出VidMP3方法，通过利用姿态（pose）和位置（position）先验从源视频中学习泛化的运动表示，以生成保持原始运动且结构和语义灵活的新视频。

Result: 定性和定量评估均表明，该方法优于现有方法。

Conclusion: VidMP3通过学习泛化运动表示，有效解决了运动保持视频编辑中结构和语义灵活性的挑战，并展现出优越性。

Abstract: Motion-preserved video editing is crucial for creators, particularly in
scenarios that demand flexibility in both the structure and semantics of
swapped objects. Despite its potential, this area remains underexplored.
Existing diffusion-based editing methods excel in structure-preserving tasks,
using dense guidance signals to ensure content integrity. While some recent
methods attempt to address structure-variable editing, they often suffer from
issues such as temporal inconsistency, subject identity drift, and the need for
human intervention. To address these challenges, we introduce VidMP3, a novel
approach that leverages pose and position priors to learn a generalized motion
representation from source videos. Our method enables the generation of new
videos that maintain the original motion while allowing for structural and
semantic flexibility. Both qualitative and quantitative evaluations demonstrate
the superiority of our approach over existing methods. The code will be made
publicly available at https://github.com/sandeep-sm/VidMP3.

</details>


### [72] [A Review on Domain Adaption and Generative Adversarial Networks(GANs)](https://arxiv.org/abs/2510.12075)
*Aashish Dhawan,Divyanshu Mudgal*

Main category: cs.CV

TL;DR: 本文旨在探讨域适应（Domain Adaptation）及其实现方法，以解决计算机视觉，特别是图像分类领域中高质量标注数据稀缺的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉领域面临高质量标注数据获取困难且成本高昂的问题，这限制了在图像分类等任务中达到理想的基准结果。因此，急需找到能克服数据稀缺性的可靠方法。

Method: 本文将讨论域适应（Domain Adaptation）的概念及其多种实现方法。其核心思想是利用在某个数据集上训练的模型，去预测来自同一类型但不同数据域的数据。

Result: 摘要未提供具体的实验结果或性能数据，仅阐述了研究目的和方法论。

Conclusion: 域适应作为一种有效的策略被提出，旨在解决计算机视觉中数据标注不足的问题，通过允许模型跨不同域进行知识迁移来应对数据稀缺挑战。

Abstract: The major challenge in today's computer vision scenario is the availability
of good quality labeled data. In a field of study like image classification,
where data is of utmost importance, we need to find more reliable methods which
can overcome the scarcity of data to produce results comparable to previous
benchmark results. In most cases, obtaining labeled data is very difficult
because of the high cost of human labor and in some cases impossible. The
purpose of this paper is to discuss Domain Adaptation and various methods to
implement it. The main idea is to use a model trained on a particular dataset
to predict on data from a different domain of the same kind, for example - a
model trained on paintings of airplanes predicting on real images of airplanes

</details>


### [73] [Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback](https://arxiv.org/abs/2510.12089)
*Xingpei Ma,Shenneng Huang,Jiaran Cai,Yuansheng Guan,Shen Zheng,Hanfeng Zhao,Qiang Zhang,Shunsi Zhang*

Main category: cs.CV

TL;DR: 提出一种基于扩散Transformer的框架，通过LoRA、奖励反馈和无训练的Mask-CFG，实现高质量、长时程且多角色的音频驱动视频生成，解决唇同步和时间连贯性等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在音频驱动人脸视频生成方面，仍面临唇形同步精度、长视频时间连贯性和多角色动画的挑战。

Method: ['提出一个基于扩散Transformer (DiT) 的框架，用于生成任意长度的逼真谈话视频。', '采用LoRA训练策略结合位置偏移推理方法，以实现高效的长视频生成并保留基础模型能力。', '结合部分参数更新与奖励反馈，以增强唇形同步和自然身体动作。', '引入无训练的Mask Classifier-Free Guidance (Mask-CFG) 方法，实现无需专用数据集或模型修改的多角色音频驱动动画。']

Result: 实验结果表明，该方法在生成高质量、时间连贯和多角色音频驱动视频方面，优于现有最先进方法。

Conclusion: 本方法能以简单、高效和低成本的方式，生成高质量、时间连贯且支持多角色的音频驱动视频，有效解决了现有技术的关键挑战。

Abstract: Recent advances in diffusion models have significantly improved audio-driven
human video generation, surpassing traditional methods in both quality and
controllability. However, existing approaches still face challenges in lip-sync
accuracy, temporal coherence for long video generation, and multi-character
animation. In this work, we propose a diffusion transformer (DiT)-based
framework for generating lifelike talking videos of arbitrary length, and
introduce a training-free method for multi-character audio-driven animation.
First, we employ a LoRA-based training strategy combined with a position shift
inference approach, which enables efficient long video generation while
preserving the capabilities of the foundation model. Moreover, we combine
partial parameter updates with reward feedback to enhance both lip
synchronization and natural body motion. Finally, we propose a training-free
approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character
animation, which requires no specialized datasets or model modifications and
supports audio-driven animation for three or more characters. Experimental
results demonstrate that our method outperforms existing state-of-the-art
approaches, achieving high-quality, temporally coherent, and multi-character
audio-driven video generation in a simple, efficient, and cost-effective
manner.

</details>


### [74] [IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation](https://arxiv.org/abs/2510.12095)
*Wenxu Zhou,Kaixuan Nie,Hang Du,Dong Yin,Wei Huang,Siqiang Guo,Xiaobo Zhang,Pengbo Hu*

Main category: cs.CV

TL;DR: IL3D是一个大规模数据集，专为LLM驱动的3D场景生成设计，包含大量室内布局、3D对象资产及自然语言标注，旨在解决高质量训练数据稀缺问题，并显著提升LLM在该任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM驱动的3D场景生成在室内布局设计中对多样化、高质量训练数据的迫切需求。

Method: 构建了IL3D数据集，包含27,816个室内布局（18种房间类型）和29,215个高保真3D对象资产，并提供实例级自然语言标注。建立了严格的基准来评估LLM驱动的场景生成，并支持多种多模态数据导出（如点云、3D边界框、多视图图像等）。

Result: 在IL3D上对LLM进行监督微调（SFT）显著提高了泛化能力，并超越了在其他数据集上SFT的性能。

Conclusion: IL3D作为一个通用且强大的资源，通过提供高保真场景数据支持具身智能体的环境感知任务，显著推进了3D场景生成和具身智能领域的研究。

Abstract: In this study, we present IL3D, a large-scale dataset meticulously designed
for large language model (LLM)-driven 3D scene generation, addressing the
pressing demand for diverse, high-quality training data in indoor layout
design. Comprising 27,816 indoor layouts across 18 prevalent room types and a
library of 29,215 high-fidelity 3D object assets, IL3D is enriched with
instance-level natural language annotations to support robust multimodal
learning for vision-language tasks. We establish rigorous benchmarks to
evaluate LLM-driven scene generation. Experimental results show that supervised
fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and
surpasses the performance of SFT on other datasets. IL3D offers flexible
multimodal data export capabilities, including point clouds, 3D bounding boxes,
multiview images, depth maps, normal maps, and semantic masks, enabling
seamless adaptation to various visual tasks. As a versatile and robust
resource, IL3D significantly advances research in 3D scene generation and
embodied intelligence, by providing high-fidelity scene data to support
environment perception tasks of embodied agents.

</details>


### [75] [An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring](https://arxiv.org/abs/2510.12098)
*Jianping Li,Dongyang Guo,Wenjie Li,Wei Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种利用边缘先验的Transformer架构，用于高效地对模糊二维码进行去模糊处理，并通过一个自适应双网络根据模糊程度选择合适的网络，以优化解码率和速度。


<details>
  <summary>Details</summary>
Motivation: 通用图像去模糊侧重感知质量，而二维码去模糊则以成功解码为优先。二维码具有结构化模式和清晰边缘的特点，是强大的恢复先验，但现有深度学习方法很少明确利用这些先验。

Method: 1. 提出“边缘引导注意力块（EGAB）”将显式边缘先验嵌入Transformer架构。2. 基于EGAB开发“边缘引导Restormer（EG-Restormer）”网络，用于严重模糊的二维码。3. 为轻度模糊输入设计“轻量高效网络（LENet）”以实现快速去模糊。4. 将EG-Restormer和LENet集成到“自适应双网络（ADNet）”中，根据输入模糊严重程度动态选择网络，适用于资源受限的移动设备。

Result: EG-Restormer和ADNet在严重模糊的二维码解码率上取得了显著提升，并在性能上达到了最新水平，同时保持了具有竞争力的速度。

Conclusion: 通过显式利用二维码的边缘先验，本文提出的EG-Restormer和ADNet方法有效地解决了二维码去模糊问题，显著提高了解码率，并在效率上表现出色，尤其适用于移动设备。

Abstract: Unlike general image deblurring that prioritizes perceptual quality, QR code
deblurring focuses on ensuring successful decoding. QR codes are characterized
by highly structured patterns with sharp edges, a robust prior for restoration.
Yet existing deep learning methods rarely exploit these priors explicitly. To
address this gap, we propose the Edge-Guided Attention Block (EGAB), which
embeds explicit edge priors into a Transformer architecture. Based on EGAB, we
develop Edge-Guided Restormer (EG-Restormer), an effective network that
significantly boosts the decoding rate of severely blurred QR codes. For mildly
blurred inputs, we design the Lightweight and Efficient Network (LENet) for
fast deblurring. We further integrate these two networks into an Adaptive
Dual-network (ADNet), which dynamically selects the suitable network based on
input blur severity, making it ideal for resource-constrained mobile devices.
Extensive experiments show that our EG-Restormer and ADNet achieve
state-of-the-art performance with a competitive speed. Project page:
https://github.com/leejianping/ADNet

</details>


### [76] [G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior](https://arxiv.org/abs/2510.12099)
*Junfeng Ni,Yixin Chen,Zhifei Yang,Yu Liu,Ruijie Lu,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出一种新的3D场景重建方法，通过利用平面结构推导出准确的深度图，并将其作为几何指导整合到生成模型流程中，以提高可见区域和未观测区域的几何与外观重建质量，同时增强多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练扩散模型进行3D场景重建的方法存在两大局限：一是缺乏可靠的几何监督，导致观测区域和未观测区域的重建质量不佳；二是未能有效缓解生成图像中的多视角不一致性，从而引起形状-外观模糊和几何退化。本研究认为准确的几何信息是有效利用生成模型增强3D场景重建的根本前提。

Method: 该方法首先利用平面结构的普遍性来推导出精确的度量尺度深度图，为观测和未观测区域提供可靠的监督。其次，将这种几何指导整合到整个生成流程中，以改进可见性掩码估计、指导新视角选择，并在使用视频扩散模型进行图像修复时增强多视角一致性。

Result: 在Replica、ScanNet++和DeepBlending数据集上的大量实验表明，本文方法在几何和外观重建方面均持续优于现有基线，尤其在未观测区域表现突出。此外，该方法自然支持单视角输入和无姿态视频，并在室内外场景中展现出强大的泛化能力和实际应用价值。

Conclusion: 通过优先考虑并有效整合来自平面结构的精确几何指导，本方法成功克服了现有扩散模型3D场景重建方法的关键局限，实现了在几何和外观重建方面（特别是未观测区域）的显著提升，并具有广泛的适用性和实用性。

Abstract: Despite recent advances in leveraging generative prior from pre-trained
diffusion models for 3D scene reconstruction, existing methods still face two
critical limitations. First, due to the lack of reliable geometric supervision,
they struggle to produce high-quality reconstructions even in observed regions,
let alone in unobserved areas. Second, they lack effective mechanisms to
mitigate multi-view inconsistencies in the generated images, leading to severe
shape-appearance ambiguities and degraded scene geometry. In this paper, we
identify accurate geometry as the fundamental prerequisite for effectively
exploiting generative models to enhance 3D scene reconstruction. We first
propose to leverage the prevalence of planar structures to derive accurate
metric-scale depth maps, providing reliable supervision in both observed and
unobserved regions. Furthermore, we incorporate this geometry guidance
throughout the generative pipeline to improve visibility mask estimation, guide
novel view selection, and enhance multi-view consistency when inpainting with
video diffusion models, resulting in accurate and consistent scene completion.
Extensive experiments on Replica, ScanNet++, and DeepBlending show that our
method consistently outperforms existing baselines in both geometry and
appearance reconstruction, particularly for unobserved regions. Moreover, our
method naturally supports single-view inputs and unposed videos, with strong
generalizability in both indoor and outdoor scenarios with practical real-world
applicability. The project page is available at
https://dali-jack.github.io/g4splat-web/.

</details>


### [77] [DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning](https://arxiv.org/abs/2510.12107)
*Jiawei Zhan,Jun Liu,Jinlong Peng,Xiaochen Chen,Bin-Bin Gao,Yong Liu,Chengjie Wang*

Main category: cs.CV

TL;DR: 提出判别性表示学习(DRL)框架，包含增量并行适配器(IPA)网络和解耦锚点监督(DAS)，有效解决了无排练类增量学习中的模型复杂性、表示漂移和优化不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 无排练类增量学习(CIL)面临三大挑战：模型复杂度高、增量学习过程中表示漂移不平滑以及阶段性子问题优化与全局推理之间存在不一致性。

Method: DRL框架包含：1. 增量并行适配器(IPA)网络，基于预训练模型(PTM)构建，通过学习轻量级适配器逐步增加模型，并通过传输门实现与当前模型的并行连接，确保表示平滑转移。2. 解耦锚点监督(DAS)，通过将正负样本与虚拟锚点分别比较，解耦约束，促进判别性表示学习并对齐不同阶段的特征空间。

Result: DRL在六个基准测试上持续优于其他最先进的方法，并在整个CIL期间保持了较高的训练和推理效率。

Conclusion: DRL框架通过其创新的IPA网络和DAS机制，有效解决了类增量学习中的关键难题，实现了卓越的性能和效率，缩小了局部优化与全局推理之间的差距。

Abstract: With the excellent representation capabilities of Pre-Trained Models (PTMs),
remarkable progress has been made in non-rehearsal Class-Incremental Learning
(CIL) research. However, it remains an extremely challenging task due to three
conundrums: increasingly large model complexity, non-smooth representation
shift during incremental learning and inconsistency between stage-wise
sub-problem optimization and global inference. In this work, we propose the
Discriminative Representation Learning (DRL) framework to specifically address
these challenges. To conduct incremental learning effectively and yet
efficiently, the DRL's network, called Incremental Parallel Adapter (IPA)
network, is built upon a PTM and increasingly augments the model by learning a
lightweight adapter with a small amount of parameter learning overhead in each
incremental stage. The adapter is responsible for adapting the model to new
classes, it can inherit and propagate the representation capability from the
current model through parallel connection between them by a transfer gate. As a
result, this design guarantees a smooth representation shift between different
incremental stages. Furthermore, to alleviate inconsistency and enable
comparable feature representations across incremental stages, we design the
Decoupled Anchor Supervision (DAS). It decouples constraints of positive and
negative samples by respectively comparing them with the virtual anchor. This
decoupling promotes discriminative representation learning and aligns the
feature spaces learned at different stages, thereby narrowing the gap between
stage-wise local optimization over a subset of data and global inference across
all classes. Extensive experiments on six benchmarks reveal that our DRL
consistently outperforms other state-of-the-art methods throughout the entire
CIL period while maintaining high efficiency in both training and inference
phases.

</details>


### [78] [Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration](https://arxiv.org/abs/2510.12114)
*Wenjie Li,Xiangyi Wang,Heng Guo,Guangwei Gao,Zhanyu Ma*

Main category: cs.CV

TL;DR: SSDiff是一种自监督选择性引导扩散模型，通过生成伪参考人脸并分阶段监督，实现旧照片人脸的高质量、区域可控修复，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 旧照片人脸修复因复合降级（如破损、褪色、模糊）挑战巨大，现有扩散方法依赖明确的降级先验或全局统计指导，难以处理局部伪影或人脸色彩问题。

Method: 提出SSDiff，利用弱引导预训练扩散模型生成伪参考人脸；通过分阶段监督（去噪过程结构引导，后期色彩细化）实现区域特定修复；结合人脸解析图和划痕掩膜，选择性修复破损区域并避免身份不匹配。同时构建了包含300张真实旧人脸照片的VintageFace基准数据集。

Result: SSDiff在感知质量、保真度和区域可控性方面，超越了现有的GAN和扩散方法。

Conclusion: SSDiff通过自监督和选择性引导，有效解决了旧照片人脸修复中的复杂降级和现有方法的局限性，在修复质量和控制性上表现出色，并提供了新的评估基准VintageFace。

Abstract: Old-photo face restoration poses significant challenges due to compounded
degradations such as breakage, fading, and severe blur. Existing pre-trained
diffusion-guided methods either rely on explicit degradation priors or global
statistical guidance, which struggle with localized artifacts or face color. We
propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages
pseudo-reference faces generated by a pre-trained diffusion model under weak
guidance. These pseudo-labels exhibit structurally aligned contours and natural
colors, enabling region-specific restoration via staged supervision: structural
guidance applied throughout the denoising process and color refinement in later
steps, aligned with the coarse-to-fine nature of diffusion. By incorporating
face parsing maps and scratch masks, our method selectively restores breakage
regions while avoiding identity mismatch. We further construct VintageFace, a
300-image benchmark of real old face photos with varying degradation levels.
SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual
quality, fidelity, and regional controllability. Code link:
https://github.com/PRIS-CV/SSDiff.

</details>


### [79] [ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation](https://arxiv.org/abs/2510.12119)
*Ziyuan Luo,Yangyi Zhao,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: ImageSentinel提出一种新颖框架，通过合成哨兵图像保护检索增强图像生成（RAIG）系统中的视觉数据集免遭未经授权的使用。


<details>
  <summary>Details</summary>
Motivation: 检索增强图像生成（RAIG）系统广泛应用，但存在未经授权使用私人图像数据集的担忧；传统数字水印方法在RAIG复杂特征提取和重组过程中失效，因此急需一种保护视觉数据集的方法。

Method: 本文提出ImageSentinel框架，通过视觉语言模型合成与原始数据集视觉一致的“哨兵图像”；这些哨兵图像利用随机生成的字符序列作为检索密钥，实现保护验证和未经授权使用的检测。

Result: 实验结果表明，ImageSentinel能够有效检测未经授权的数据集使用，同时为授权应用保持了生成质量。

Conclusion: ImageSentinel为保护RAIG系统中的视觉数据集免遭未经授权的使用提供了一种有效且实用的解决方案。

Abstract: The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has
raised significant concerns about the unauthorized use of private image
datasets. While these systems have shown remarkable capabilities in enhancing
generation quality through reference images, protecting visual datasets from
unauthorized use in such systems remains a challenging problem. Traditional
digital watermarking approaches face limitations in RAIG systems, as the
complex feature extraction and recombination processes fail to preserve
watermark signals during generation. To address these challenges, we propose
ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our
framework synthesizes sentinel images that maintain visual consistency with the
original dataset. These sentinels enable protection verification through
randomly generated character sequences that serve as retrieval keys. To ensure
seamless integration, we leverage vision-language models to generate the
sentinel images. Experimental results demonstrate that ImageSentinel
effectively detects unauthorized dataset usage while preserving generation
quality for authorized applications. Code is available at
https://github.com/luo-ziyuan/ImageSentinel.

</details>


### [80] [Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras](https://arxiv.org/abs/2510.12123)
*David Parra,Felipe Gutierrez-Barragan,Trevor Seets,Andreas Velten*

Main category: cs.CV

TL;DR: 单光子3D成像受硬件限制，现有方法性能不佳。本文提出一种基于梯度下降的受限优化方法，联合优化照明和编码函数，显著提升性能，特别是在峰值功率受限系统。


<details>
  <summary>Details</summary>
Motivation: 单光子相机在时间飞行3D成像中受带宽、激光功率、数据速率等硬件限制。现有的压缩直方图虽能处理数据速率，但在真实照明硬件约束下性能不足。

Method: 提出一种受限优化方法，通过梯度下降联合优化照明和编码矩阵（即编码函数），使其符合硬件约束，以设计适用于压缩单光子3D成像的实用编码函数。

Result: 通过广泛仿真表明，在带宽和峰值功率约束下，所设计的编码函数持续优于传统编码设计，尤其在峰值功率受限系统中优势显著。该方法还能适应任意参数化脉冲响应，并在真实系统上得到验证。

Conclusion: 本文的受限优化方法成功解决了单光子3D成像在真实硬件约束下的性能瓶颈，通过联合优化照明和编码，提供了更高效、更鲁棒的编码函数设计，提高了成像系统的实用性。

Abstract: Single-photon cameras are becoming increasingly popular in time-of-flight 3D
imaging because they can time-tag individual photons with extreme resolution.
However, their performance is susceptible to hardware limitations, such as
system bandwidth, maximum laser power, sensor data rates, and in-sensor memory
and compute resources. Compressive histograms were recently introduced as a
solution to the challenge of data rates through an online in-sensor compression
of photon timestamp data. Although compressive histograms work within limited
in-sensor memory and computational resources, they underperform when subjected
to real-world illumination hardware constraints. To address this, we present a
constrained optimization approach for designing practical coding functions for
compressive single-photon 3D imaging. Using gradient descent, we jointly
optimize an illumination and coding matrix (i.e., the coding functions) that
adheres to hardware constraints. We show through extensive simulations that our
coding functions consistently outperform traditional coding designs under both
bandwidth and peak power constraints. This advantage is particularly pronounced
in systems constrained by peak power. Finally, we show that our approach adapts
to arbitrary parameterized impulse responses by evaluating it on a real-world
system with a non-ideal impulse response function.

</details>


### [81] [MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites](https://arxiv.org/abs/2510.12126)
*Zhenxin Lei,Zhangwei Gao,Changyao Tian,Erfei Cui,Guanzhou Chen,Danni Yang,Yuchen Duan,Zhaokai Wang,Wenhao Li,Weiyun Wang,Xiangyu Zhao,Jiayi Ji,Yu Qiao,Wenhai Wang,Gen Luo*

Main category: cs.CV

TL;DR: 提出CapFlow多智能体协作流，利用开源模型以低成本实现媲美GPT-4.1的视觉字幕生成。基于CapFlow合成数据训练的MetaCaptioner，其性能可与商业模型竞争，并在开源社区达到顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 通用视觉字幕任务中，当前开源模型与商业模型存在显著性能差距，限制了数据合成等多种应用。

Method: 提出CapFlow，一种新颖的多智能体协作工作流，利用开源模型实现高质量字幕生成。将CapFlow作为数据合成器，大规模生产图像和视频的高质量视觉字幕，并通过微调获得通用视觉字幕模型MetaCaptioner。

Result: CapFlow证明能以89.5%的成本降低，实现与GPT-4.1相当的字幕质量。MetaCaptioner不仅达到了与商业模型相当的字幕能力，还在开源社区中实现了顶尖的多模态性能。

Conclusion: CapFlow和MetaCaptioner提供了一个强大且经济高效的视觉字幕解决方案，有望促进未来的多模态研究。

Abstract: Generalist visual captioning goes beyond a simple appearance description
task, but requires integrating a series of visual cues into a caption and
handling various visual domains. In this task, current open-source models
present a large performance gap with commercial ones, which limits various
applications such as data synthesis. To bridge the gap, this paper proposes
CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for
the first time that, by capitalizing on open-source models, it is possible to
achieve caption quality on par with GPT-4.1 in various domains with an 89.5%
reduction in costs. By leveraging CapFlow as the data synthesizer, we produce
high-quality visual captions from image and video domains at scale, and obtain
a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through
extensive experiments, we show that MetaCaptioner not only achieves comparable
captioning capabilities with commercial models but also reaches top-tier
multimodal performance in the open-source community. We hope CapFlow and
MetaCaptioner can benefit future multimodal research by providing a strong and
cost-effective visual captioning solution.

</details>


### [82] [FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements](https://arxiv.org/abs/2510.12132)
*Xiao Yang,Jiyao Wang*

Main category: cs.CV

TL;DR: 针对远程生理测量中隐私数据收集和无标签数据更新的挑战，本文提出FedHUG联邦无监督域泛化框架，通过最小偏置聚合和全局分布感知学习控制器解决数据异构和标签分布偏差，并取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 远程生理测量需收集用户隐私敏感信息，且现有非接触式测量方法依赖标注的客户端数据。这导致在利用大量缺乏标签的真实世界用户数据更新已部署模型时面临重大挑战。

Method: 本文实例化了一种新的联邦无监督域泛化（FUDG）协议，并在此基础上提出了联邦异构无监督泛化（FedHUG）框架。FedHUG包含两个核心模块：1) 最小偏置聚合模块：根据先验驱动的偏置评估动态调整聚合权重，以应对来自多域的异构非IID特征。2) 全局分布感知学习控制器：参数化标签分布并动态操纵客户端特定的训练策略，从而缓解服务器-客户端标签分布偏差和长尾问题。

Result: 所提出的FedHUG框架在结合RGB视频或毫米波雷达进行估计时，相较于现有最先进技术展现出卓越的性能。

Conclusion: FedHUG框架通过解决远程生理测量中的隐私敏感数据、无标签模型更新、数据异构性以及标签分布不均衡等核心问题，有效提升了模型的泛化能力和性能。

Abstract: Remote physiological measurement gained wide attention, while it requires
collecting users' privacy-sensitive information, and existing contactless
measurements still rely on labeled client data. This presents challenges when
we want to further update real-world deployed models with numerous user data
lacking labels. To resolve these challenges, we instantiate a new protocol
called Federated Unsupervised Domain Generalization (FUDG) in this work.
Subsequently, the \textbf{Fed}erated \textbf{H}eterogeneous
\textbf{U}nsupervised \textbf{G}eneralization (\textbf{FedHUG}) framework is
proposed and consists of: (1) Minimal Bias Aggregation module dynamically
adjusts aggregation weights based on prior-driven bias evaluation to cope with
heterogeneous non-IID features from multiple domains. (2) The Global
Distribution-aware Learning Controller parameterizes the label distribution and
dynamically manipulates client-specific training strategies, thereby mitigating
the server-client label distribution skew and long-tail issue. The proposal
shows superior performance across state-of-the-art techniques in estimation
with either RGB video or mmWave radar. The code will be released.

</details>


### [83] [Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation](https://arxiv.org/abs/2510.12150)
*Jiahuan Zhou,Chao Zhu,Zhenyu Cui,Zichen Liu,Xu Zou,Gang Hua*

Main category: cs.CV

TL;DR: 本文提出了一种名为KFF的类感知域知识融合与分裂方法，用于持续测试时间自适应(CTTA)，旨在通过动态管理域知识来解决现有方法中灾难性遗忘和新知识学习不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时间自适应(CTTA)方法在模型适应多个未知下游域时，常因不规则切换的下游域数据导致历史知识的灾难性遗忘和新知识学习不足，并受到潜在有害历史知识的干扰，从而导致严重的性能下降。

Method: 我们提出了类感知域知识融合与分裂方法（KFF），该方法根据测试时来自不同域的数据自适应地扩展和合并新旧域中的类感知域知识。具体包括：1) 域知识分裂（KFI）模块：从配对的类感知域提示池中自适应分离新域知识，以减轻旧域带来的负面知识影响。2) 域知识融合（KFU）模块：将分裂出的新知识以最小成本合并到现有知识池中，其中设计了贪婪知识动态合并策略，以提高新旧知识的兼容性并保持计算效率，避免累积的计算和存储开销。

Result: 在ImageNet-C数据集上进行的大量实验验证了所提出方法相对于其他方法的有效性。

Conclusion: KFF方法通过动态管理类感知域知识的融合与分裂，有效解决了CTTA中灾难性遗忘和新知识学习不足的问题，并在实验中展现出优越的性能。

Abstract: Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model
during the test phase so that it can adapt to multiple unknown downstream
domain distributions without pre-acquiring downstream domain data. To this end,
existing advanced CTTA methods mainly reduce the catastrophic forgetting of
historical knowledge caused by irregular switching of downstream domain data by
restoring the initial model or reusing historical models. However, these
methods are usually accompanied by serious insufficient learning of new
knowledge and interference from potentially harmful historical knowledge,
resulting in severe performance degradation. To this end, we propose a
class-aware domain Knowledge Fusion and Fission method for continual test-time
adaptation, called KFF, which adaptively expands and merges class-aware domain
knowledge in old and new domains according to the test-time data from different
domains, where discriminative historical knowledge can be dynamically
accumulated. Specifically, considering the huge domain gap within streaming
data, a domain Knowledge FIssion (KFI) module is designed to adaptively
separate new domain knowledge from a paired class-aware domain prompt pool,
alleviating the impact of negative knowledge brought by old domains that are
distinct from the current domain. Besides, to avoid the cumulative computation
and storage overheads from continuously fissioning new knowledge, a domain
Knowledge FUsion (KFU) module is further designed to merge the fissioned new
knowledge into the existing knowledge pool with minimal cost, where a greedy
knowledge dynamic merging strategy is designed to improve the compatibility of
new and old knowledge while keeping the computational efficiency. Extensive
experiments on the ImageNet-C dataset verify the effectiveness of our proposed
method against other methods.

</details>


### [84] [DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation](https://arxiv.org/abs/2510.12159)
*Ziyuan Gao,Philippe Morel*

Main category: cs.CV

TL;DR: 本文提出扩散原型学习（DPL）框架，通过扩散模型生成多样化的原型变体，解决了单次医学图像分割中原型表示因数据稀疏和变异性导致的问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 单次医学图像分割在原型表示方面面临挑战，原因在于标注数据有限和解剖变异性大。传统基于原型的SOTA方法依赖确定性平均，无法捕获类内多样性，导致泛化能力不足。

Method: 本文引入扩散原型学习（DPL）框架，通过扩散过程探索特征空间来重构原型。DPL将一次性原型建模为可学习的概率分布，能从少量标记数据中生成多样且语义一致的原型变体。核心创新包括：1) 基于扩散的原型增强模块，通过前向-逆向扩散将单一支持原型转换为多样变体集；2) 空间感知条件机制，利用原型特征统计的几何属性；3) 保守融合策略，在保持原型保真度的同时最大化表示多样性。DPL确保训练和推理阶段的一致性，扩散过程也作为正则化。

Result: 在腹部MRI和CT数据集上的广泛实验表明，DPL均取得了显著改进，在单次医学图像分割中建立了新的SOTA性能。

Conclusion: DPL通过引入扩散原型学习，有效解决了单次医学图像分割中原型表示的局限性，显著提升了分割性能，达到了该领域的最先进水平。

Abstract: One-shot medical image segmentation faces fundamental challenges in prototype
representation due to limited annotated data and significant anatomical
variability across patients. Traditional prototype-based methods rely on
deterministic averaging of support features, creating brittle representations
that fail to capture intra-class diversity essential for robust generalization.
This work introduces Diffusion Prototype Learning (DPL), a novel framework that
reformulates prototype construction through diffusion-based feature space
exploration. DPL models one-shot prototypes as learnable probability
distributions, enabling controlled generation of diverse yet semantically
coherent prototype variants from minimal labeled data. The framework operates
through three core innovations: (1) a diffusion-based prototype enhancement
module that transforms single support prototypes into diverse variant sets via
forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism
that leverages geometric properties derived from prototype feature statistics,
and (3) a conservative fusion strategy that preserves prototype fidelity while
maximizing representational diversity. DPL ensures training-inference
consistency by using the same diffusion enhancement and fusion pipeline in both
phases. This process generates enhanced prototypes that serve as the final
representations for similarity calculations, while the diffusion process itself
acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets
demonstrate significant improvements respectively, establishing new
state-of-the-art performance in one-shot medical image segmentation.

</details>


### [85] [State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding](https://arxiv.org/abs/2510.12160)
*Jiahuan Zhou,Kai Zhu,Zhenyu Cui,Zichen Liu,Xu Zou,Gang Hua*

Main category: cs.CV

TL;DR: 本文提出了一种名为State Space Prompting (SSP) 的视频理解方法，通过结合帧内和帧间提示词，解决了预训练状态空间模型中现有提示学习无法有效捕获时空上下文信息的问题，在提高性能的同时减少了微调参数开销。


<details>
  <summary>Details</summary>
Motivation: 预训练状态空间模型在视频分类中显示出高效率和高性能。然而，现有的提示学习方法在应用于这些模型时，其顺序压缩的视觉提示词未能有效捕获视频中的空间和时间上下文信息，限制了判别性信息的传播和提取。

Method: 我们提出了State Space Prompting (SSP) 方法。该方法结合了帧内和帧间提示词来聚合和传播关键时空信息。具体地，设计了帧内信息聚合 (Intra-Frame Gathering, IFG) 模块来聚合每帧内的空间关键信息；设计了帧间信息传播 (Inter-Frame Spreading, IFS) 模块来在不同帧之间传播判别性时空信息。SSP通过自适应地平衡和压缩帧内和帧间关键时空信息，以互补的方式传播判别性信息。

Result: 在四个视频基准数据集上的大量实验表明，我们的SSP方法平均显著优于现有SOTA方法2.76%，同时降低了微调参数的开销。

Conclusion: SSP通过结合帧内和帧间提示，有效解决了状态空间模型中提示学习在捕获视频时空上下文方面的局限性，实现了判别性信息的有效传播，从而在视频理解任务中获得了卓越的性能和效率。

Abstract: Recently, pre-trained state space models have shown great potential for video
classification, which sequentially compresses visual tokens in videos with
linear complexity, thereby improving the processing efficiency of video data
while maintaining high performance. To apply powerful pre-trained models to
downstream tasks, prompt learning is proposed to achieve efficient downstream
task adaptation with only a small number of fine-tuned parameters. However, the
sequentially compressed visual prompt tokens fail to capture the spatial and
temporal contextual information in the video, thus limiting the effective
propagation of spatial information within a video frame and temporal
information between frames in the state compression model and the extraction of
discriminative information. To tackle the above issue, we proposed a State
Space Prompting (SSP) method for video understanding, which combines
intra-frame and inter-frame prompts to aggregate and propagate key
spatiotemporal information in the video. Specifically, an Intra-Frame Gathering
(IFG) module is designed to aggregate spatial key information within each
frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread
discriminative spatio-temporal information across different frames. By
adaptively balancing and compressing key spatio-temporal information within and
between frames, our SSP effectively propagates discriminative information in
videos in a complementary manner. Extensive experiments on four video benchmark
datasets verify that our SSP significantly outperforms existing SOTA methods by
2.76% on average while reducing the overhead of fine-tuning parameters.

</details>


### [86] [UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering](https://arxiv.org/abs/2510.12174)
*Yusen Xie,Zhenmin Huang,Jianhao Jiao,Dimitrios Kanoulas,Jun Ma*

Main category: cs.CV

TL;DR: 本文提出UniGS，一个基于3D高斯飞溅的统一表示与可微分框架，用于高保真多模态3D重建，通过改进光栅化和引入可微分剪枝实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实现高保真多模态3D重建，并解决现有3D高斯飞溅方法在多模态渲染的几何精确性、一致性以及计算存储效率方面的挑战。

Method: ['提出UniGS，一个统一的地图表示和可微分框架，用于多模态3D重建。', '集成CUDA加速的光栅化管线，能够同步渲染逼真的RGB图像、几何精确的深度图、一致的表面法线和语义逻辑。', '重新设计深度光栅化，通过可微分的射线-椭球体交集而非高斯中心渲染深度，并通过解析深度梯度有效优化旋转和尺度属性。', '推导表面法线渲染的解析梯度公式，确保重建3D场景的几何一致性。', '引入可学习属性，实现训练期间对贡献最小的高斯进行可微分剪枝，提高计算和存储效率。']

Result: ['实现了照片级RGB图像、几何精确深度图、一致表面法线和语义逻辑的同步渲染。', '在所有模态上均达到最先进的重建精度。', '实验验证了几何感知范式的有效性。']

Conclusion: UniGS通过其统一表示、改进的光栅化技术和可微分剪枝，显著提高了高保真多模态3D重建的精度、几何一致性和效率，达到了行业领先水平。

Abstract: In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.

</details>


### [87] [BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation](https://arxiv.org/abs/2510.12182)
*Youngju Yoo,Seho Kim,Changick Kim*

Main category: cs.CV

TL;DR: 论文提出BEEP3D，一种端到端的框监督3D实例分割方法，通过学生-教师框架、实例中心查询细化和新颖的损失函数，解决了传统两阶段方法的效率和精度问题，并在主要数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 3D实例分割需要昂贵的点级标注。虽然框级标注成本较低，但其在重叠区域的固有歧义性使得点到实例分配具有挑战性。现有通过生成伪掩码的两阶段方法增加了训练时间和复杂性，阻碍了端到端优化。

Method: 提出BEEP3D，一个用于3D实例分割的框监督端到端伪掩码生成方法。它采用学生-教师框架，教师模型通过EMA由学生模型更新并生成伪掩码。为生成更精确的伪掩码，引入了基于实例中心的查询细化机制。此外，设计了查询一致性损失和掩码特征一致性损失，以对齐预测与伪掩码之间的语义和几何信号。

Result: 在ScanNetV2和S3DIS数据集上的广泛实验表明，BEEP3D与最先进的弱监督方法相比，取得了具有竞争力或更优的性能，同时保持了计算效率。

Conclusion: BEEP3D提供了一个高效且性能优越的端到端解决方案，有效解决了框监督3D实例分割中伪掩码生成和效率的挑战。

Abstract: 3D instance segmentation is crucial for understanding complex 3D
environments, yet fully supervised methods require dense point-level
annotations, resulting in substantial annotation costs and labor overhead. To
mitigate this, box-level annotations have been explored as a weaker but more
scalable form of supervision. However, box annotations inherently introduce
ambiguity in overlapping regions, making accurate point-to-instance assignment
challenging. Recent methods address this ambiguity by generating pseudo-masks
through training a dedicated pseudo-labeler in an additional training stage.
However, such two-stage pipelines often increase overall training time and
complexity, hinder end-to-end optimization. To overcome these challenges, we
propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance
segmentation. BEEP3D adopts a student-teacher framework, where the teacher
model serves as a pseudo-labeler and is updated by the student model via an
Exponential Moving Average. To better guide the teacher model to generate
precise pseudo-masks, we introduce an instance center-based query refinement
that enhances position query localization and leverages features near instance
centers. Additionally, we design two novel losses-query consistency loss and
masked feature consistency loss-to align semantic and geometric signals between
predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS
datasets demonstrate that BEEP3D achieves competitive or superior performance
compared to state-of-the-art weakly supervised methods while remaining
computationally efficient.

</details>


### [88] [CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs](https://arxiv.org/abs/2510.12184)
*Jiwan Kim,Kibum Kim,Sangwoo Seo,Chanyoung Park*

Main category: cs.CV

TL;DR: 针对现有MLLMs知识蒸馏方法在视觉感知能力传递上的不足，本文提出CompoDistill框架，通过对齐学生与教师模型的视觉注意力，显著提升了学生模型在组合推理任务中的视觉感知能力，同时保持了视觉问答任务的性能和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏（KD）方法难以有效将教师多模态大语言模型（MLLM）的丰富视觉感知能力传递给学生模型，该问题常被忽视。研究发现，学生与教师模型间的视觉注意力失调是主要原因。

Method: 提出CompoDistill，一种新颖的KD框架，其核心在于显式对齐学生模型的视觉注意力与教师模型的视觉注意力，以增强学生模型的视觉感知能力。

Result: CompoDistill显著提高了需要视觉感知能力的组合推理任务的性能，同时在视觉问答任务上保持了现有研究的强大性能。此外，CompoDistill对更先进的骨干网络也有效，展现了良好的泛化能力。

Conclusion: CompoDistill通过解决视觉注意力失调问题，有效提升了学生模型在多模态大语言模型中的视觉感知能力，特别是在组合推理任务上表现突出，并具有良好的泛化性，为开发更高效实用的MLLMs提供了解决方案。

Abstract: Recently, efficient Multimodal Large Language Models (MLLMs) have gained
significant attention as a solution to their high computational complexity,
making them more practical for real-world applications. In this regard, the
knowledge distillation (KD) approach has emerged as a promising alternative,
which transfers the rich visual and linguistic knowledge from a larger model
(teacher) to a smaller model (student). However, we observe that existing KD
methods struggle to effectively distill the teacher MLLM's rich visual
perception abilities to the student, a challenge that has been largely
overlooked in previous studies. Through a systematic analysis, we identify
visual attention misalignment between student and teacher as the main cause of
this issue. Based on this insight, we propose CompoDistill, a novel KD
framework that explicitly aligns the student's visual attention with that of
the teacher to enhance the student's visual perception abilities. Our extensive
experiments show that CompoDistill significantly improves performance on
compositional reasoning tasks that require visual perception abilities while
maintaining strong performance on visual question answering tasks, as done in
existing studies. Furthermore, CompoDistill demonstrates effectiveness with a
more advanced backbone, highlighting its generalizability.

</details>


### [89] [Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos](https://arxiv.org/abs/2510.12190)
*Shingo Yokoi,Kento Sasaki,Yu Yamaguchi*

Main category: cs.CV

TL;DR: 针对自动驾驶模型在OOD场景下的挑战，本研究提出一种基于VLM的层级推理框架，用于从行车记录仪视频生成可解释的事故报告，并在2COOOL挑战赛中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型在面对分布外(OOD)场景时表现不足，需要超越预设分类法理解危险，并生成人类可解释的事故报告来弥补这一差距。

Method: 本文提出一个层级推理框架，用于从行车记录仪视频生成事故报告。该框架整合了帧级字幕生成、事件帧检测和视觉语言模型(VLM)内的细粒度推理，并通过模型集成和盲A/B评分选择协议来提高事实准确性和可读性。

Result: 在2COOOL挑战赛官方排行榜上，该方法在29支队伍中排名第二，并获得了最佳的CIDEr-D分数，能够生成准确且连贯的事件叙述。

Conclusion: 结合VLM的层级推理是事故分析和更广泛理解安全关键交通事件的一个有前景的方向。

Abstract: Recent advances in end-to-end (E2E) autonomous driving have been enabled by
training on diverse large-scale driving datasets, yet autonomous driving models
still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark
targets this gap by encouraging hazard understanding beyond closed taxonomies,
and the 2COOOL challenge extends it to generating human-interpretable incident
reports. We present a hierarchical reasoning framework for incident report
generation from dashcam videos that integrates frame-level captioning, incident
frame detection, and fine-grained reasoning within vision-language models
(VLMs). We further improve factual accuracy and readability through model
ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL
open leaderboard, our method ranks 2nd among 29 teams and achieves the best
CIDEr-D score, producing accurate and coherent incident narratives. These
results indicate that hierarchical reasoning with VLMs is a promising direction
for accident analysis and for broader understanding of safety-critical traffic
events. The implementation and code are available at
https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.

</details>


### [90] [The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data](https://arxiv.org/abs/2510.12208)
*Muammer Bay,Timo von Marcard,Dren Fazlija*

Main category: cs.CV

TL;DR: 本文研究了合成数据在仓库物流目标检测中的应用，发现平衡使用合成与真实数据能有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在计算机视觉领域带来新机遇，但AI应用常受限于专业知识和资源，需依赖通用模型。通用模型微调需领域专用数据，而获取真实数据成本高昂且效率低下。因此，评估成本效益高的合成数据对模型性能的影响成为研究动机。

Method: 本研究调查了合成数据（通过NVIDIA Omniverse Replicator生成）在仓库物流场景下对目标检测模型性能的影响。实验侧重于仓库环境中的托盘检测，对比了仅使用真实数据以及结合真实与多种合成数据生成策略训练模型的有效性。

Result: 研究结果表明，平衡整合合成图像数据和真实数据，能够显著提升计算机视觉领域中目标检测模型的鲁棒性和效率，为合成数据的实际应用提供了有价值的见解。

Conclusion: 在实际的计算机视觉应用中，建议采取合成数据与真实数据相结合的策略，以构建更强大、更高效的目标检测模型。

Abstract: Recent advances in generative AI, particularly in computer vision (CV), offer
new opportunities to optimize workflows across industries, including logistics
and manufacturing. However, many AI applications are limited by a lack of
expertise and resources, which forces a reliance on general-purpose models.
Success with these models often requires domain-specific data for fine-tuning,
which can be costly and inefficient. Thus, using synthetic data for fine-tuning
is a popular, cost-effective alternative to gathering real-world data. This
work investigates the impact of synthetic data on the performance of object
detection models, compared to models trained on real-world data only,
specifically within the domain of warehouse logistics. To this end, we examined
the impact of synthetic data generated using the NVIDIA Omniverse Replicator
tool on the effectiveness of object detection models in real-world scenarios.
It comprises experiments focused on pallet detection in a warehouse setting,
utilizing both real and various synthetic dataset generation strategies. Our
findings provide valuable insights into the practical applications of synthetic
image data in computer vision, suggesting that a balanced integration of
synthetic and real data can lead to robust and efficient object detection
models.

</details>


### [91] [DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images](https://arxiv.org/abs/2510.12219)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thi Bich Phuong Man,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 微表情识别面临挑战，本文提出DIANet双流框架，利用阶段感知动态图像（起止-顶点和顶点-结束）和交叉注意力融合，显著优于传统单相方法。


<details>
  <summary>Details</summary>
Motivation: 微表情识别对心理学、安全和行为分析等应用至关重要，但由于其细微和短暂性以及标注数据有限而充满挑战。现有动态图像(DI)方法往往忽略了微表情中不同时间阶段的独特特征。

Method: 本文提出DIANet双流框架，利用两个阶段感知的动态图像：一个编码从起始到顶点的阶段，另一个编码从顶点到结束的阶段。每个流由一个专门的卷积神经网络处理，并通过一个交叉注意力融合模块自适应地整合来自两个流的特征。

Result: 在CASME-II、SAMM和MMEW三个基准微表情识别数据集上进行的广泛实验表明，所提出的方法始终优于传统的单相动态图像方法。

Conclusion: 研究结果强调了明确建模时间阶段信息的重要性，并为推动微表情识别提供了有前景的方向。

Abstract: Micro-expressions are brief, involuntary facial movements that typically last
less than half a second and often reveal genuine emotions. Accurately
recognizing these subtle expressions is critical for applications in
psychology, security, and behavioral analysis. However, micro-expression
recognition (MER) remains a challenging task due to the subtle and transient
nature of facial cues and the limited availability of annotated data. While
dynamic image (DI) representations have been introduced to summarize temporal
motion into a single frame, conventional DI-based methods often overlook the
distinct characteristics of different temporal phases within a
micro-expression. To address this issue, this paper proposes a novel
dual-stream framework, DIANet, which leverages phase-aware dynamic images - one
encoding the onset-to-apex phase and the other capturing the apex-to-offset
phase. Each stream is processed by a dedicated convolutional neural network,
and a cross-attention fusion module is employed to adaptively integrate
features from both streams based on their contextual relevance. Extensive
experiments conducted on three benchmark MER datasets (CASME-II, SAMM, and
MMEW) demonstrate that the proposed method consistently outperforms
conventional single-phase DI-based approaches. The results highlight the
importance of modeling temporal phase information explicitly and suggest a
promising direction for advancing MER.

</details>


### [92] [HoneyBee: Data Recipes for Vision-Language Reasoners](https://arxiv.org/abs/2510.12225)
*Hritik Bansal,Devandra Singh Sachan,Kai-Wei Chang,Aditya Grover,Gargi Ghosh,Wen-tau Yih,Ramakanth Pasunuru*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言推理（VL推理）训练数据集的构建原则，提出了数据策展方法、新的HoneyBee数据集以及测试时缩放策略，显著提升了VLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在推理任务上表现出色，但构建高性能VL推理训练数据集的底层原则仍不明确。

Method: 研究了多种数据策展方法，包括上下文来源、数据干预（如图像字幕辅助信号、纯文本推理）和数据维度（图像、问题、思维链）的扩展。基于研究结果，构建了包含250万示例和35万图像-问题对的大规模高质量思维链推理数据集HoneyBee。此外，提出了一种测试时缩放策略。

Result: 发现上下文来源策略显著影响VLM性能；数据干预（如辅助信号、纯文本推理）带来实质性提升；数据维度（每图独立问题、每图-问独立思维链）的扩展持续提高推理能力。使用HoneyBee训练的VLM在MathVerse等基准测试上超越了现有最佳模型（例如，3B参数模型比SOTA提升7.8%）。测试时缩放策略在不牺牲准确性的前提下，将解码成本降低了73%。

Conclusion: 本研究提出了改进VL推理数据集策展的研究策略，并引入了高性能数据集HoneyBee和高效的测试时缩放方法，为VL推理模型的进步做出了贡献。

Abstract: Recent advances in vision-language models (VLMs) have made them highly
effective at reasoning tasks. However, the principles underlying the
construction of performant VL reasoning training datasets remain poorly
understood. In this work, we introduce several data curation approaches and
study their impacts on VL reasoning capabilities by carefully controlling
training and evaluation setups. We analyze the effects of context (image and
question pair) sources, implement targeted data interventions, and explore
scaling up images, questions, and chain-of-thought (CoT) solutions. Our
findings reveal that (a) context source strategies significantly affect VLM
performance, (b) interventions such as auxiliary signals from image captions
and the inclusion of text-only reasoning yield substantial gains, and (c)
scaling all data dimensions (e.g., unique questions per image and unique CoTs
per image-question pair) consistently improves reasoning capability. Motivated
by these insights, we introduce HoneyBee, a large-scale, high-quality CoT
reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs
trained with HoneyBee outperform state-of-the-art models across model sizes.
For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA
model and the base model by 7.8% and 24.8%, respectively, on MathVerse.
Furthermore, we propose a test-time scaling strategy that reduces decoding cost
by 73% without sacrificing accuracy. Overall, this work presents improved
strategies for VL reasoning dataset curation research.

</details>


### [93] [BIGFix: Bidirectional Image Generation with Token Fixing](https://arxiv.org/abs/2510.12231)
*Victor Besnier,David Hurych,Andrei Bursuc,Eduardo Valle*

Main category: cs.CV

TL;DR: 本文提出一种自纠正方法，通过迭代精炼采样令牌，解决了并行多令牌预测在图像和视频生成中引入的结构不一致问题，在保持效率的同时显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 图像和视频生成模型的推理效率是关键挑战。并行多令牌预测虽然能大幅提升效率，但由于令牌不兼容和联合依赖捕获困难，易导致结构不一致。传统方法无法回溯和纠正错误预测。

Method: 提出一种通过迭代精炼采样令牌的自纠正图像生成方法。通过一种新颖的训练方案实现，该方案在上下文中注入随机令牌，以提高鲁棒性，并在采样过程中实现令牌修复。

Result: 该方法在保持并行令牌预测效率优势的同时，显著提升了生成质量。在ImageNet-256、CIFAR-10（图像生成）以及UCF-101、NuScenes（视频生成）数据集上进行了评估，在两种模态下均显示出显著改进。

Conclusion: 所提出的自纠正方法通过迭代精炼和新颖的随机令牌注入训练方案，有效解决了并行多令牌生成中因不兼容性导致的一致性问题，实现了效率与生成质量的双重提升，在图像和视频生成任务上表现出色。

Abstract: Recent advances in image and video generation have raised significant
interest from both academia and industry. A key challenge in this field is
improving inference efficiency, as model size and the number of inference steps
directly impact the commercial viability of generative models while also posing
fundamental scientific challenges. A promising direction involves combining
auto-regressive sequential token modeling with multi-token prediction per step,
reducing inference time by up to an order of magnitude. However, predicting
multiple tokens in parallel can introduce structural inconsistencies due to
token incompatibilities, as capturing complex joint dependencies during
training remains challenging. Traditionally, once tokens are sampled, there is
no mechanism to backtrack and refine erroneous predictions. We propose a method
for self-correcting image generation by iteratively refining sampled tokens. We
achieve this with a novel training scheme that injects random tokens in the
context, improving robustness and enabling token fixing during sampling. Our
method preserves the efficiency benefits of parallel token prediction while
significantly enhancing generation quality. We evaluate our approach on image
generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video
generation with UCF-101 and NuScenes, demonstrating substantial improvements
across both modalities.

</details>


### [94] [Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection](https://arxiv.org/abs/2510.12241)
*Yuehui Li,Yahao Lu,Haoyuan Wu,Sen Zhang,Liang Lin,Yukai Shi*

Main category: cs.CV

TL;DR: 针对红外小目标检测（ISTD）中的跨域偏移和噪声问题，本文提出了Ivan-ISTD框架，通过小波引导的跨域合成和真实域噪声不变性学习，有效提升了在跨域场景下的检测性能和鲁棒性，并构建了新的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测（ISTD）在无人机多模态感知中扮演重要角色，但面临跨域漂移和异方差噪声扰动的双重挑战。

Method: 本文提出双重小波引导的不变性学习框架Ivan-ISTD。首先，通过小波引导的跨域合成（利用多频率小波滤波分离目标背景）生成与目标域对齐的训练样本。其次，引入真实域噪声不变性学习，从目标域提取真实噪声特征构建动态噪声库，并通过自监督损失学习噪声不变性。此外，本文还创建了Dynamic-ISTD Benchmark数据集，并使用其他真实世界数据集验证了方法的通用性。

Result: 实验结果表明，Ivan-ISTD在多项定量指标上优于现有最先进方法。特别地，该方法在跨域场景中展示出卓越的鲁棒性。

Conclusion: Ivan-ISTD框架通过创新性的小波引导合成和真实噪声不变性学习，成功解决了ISTD中的跨域偏移和噪声挑战，显著提升了在复杂真实世界应用中的性能和鲁棒性。

Abstract: In the multimedia domain, Infrared Small Target Detection (ISTD) plays a
important role in drone-based multi-modality sensing. To address the dual
challenges of cross-domain shift and heteroscedastic noise perturbations in
ISTD, we propose a doubly wavelet-guided Invariance learning
framework(Ivan-ISTD). In the first stage, we generate training samples aligned
with the target domain using Wavelet-guided Cross-domain Synthesis. This
wavelet-guided alignment machine accurately separates the target background
through multi-frequency wavelet filtering. In the second stage, we introduce
Real-domain Noise Invariance Learning, which extracts real noise
characteristics from the target domain to build a dynamic noise library. The
model learns noise invariance through self-supervised loss, thereby overcoming
the limitations of distribution bias in traditional artificial noise modeling.
Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic
degradation dataset that simulates the distribution shifts encountered in
real-world applications. Additionally, we validate the versatility of our
method using other real-world datasets. Experimental results demonstrate that
our approach outperforms existing state-of-the-art methods in terms of many
quantitative metrics. In particular, Ivan-ISTD demonstrates excellent
robustness in cross-domain scenarios. The code for this work can be found at:
https://github.com/nanjin1/Ivan-ISTD.

</details>


### [95] [Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding](https://arxiv.org/abs/2510.12256)
*Ye Chen,Liming Tan,Yupeng Zhu,Yuanbin Wang,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出一种基于时空一致代理节点的新型视频表示方法，以克服传统像素级追踪的脆弱性，实现稳定的多尺度对象表达、高效场景变化处理，并支持高质量视频重建与编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示严重依赖不稳定且过于细粒度的像素级匹配和追踪，微小的追踪误差或遮挡、大运动会导致视觉对象表示崩溃。

Method: 提出时空一致的代理节点来表示视频中动态变化的对象/场景。该方法通过分层代理节点稳定表达多尺度结构，不受追踪误差、长时间运动、遮挡和视角变化影响；同时，利用动态表示更新机制和时空先验，减轻不准确追踪器的影响；此外，形状和纹理表示的解耦编码促进可控且细粒度的外观编辑。

Result: 所提出的表示方法以更少参数实现高视频重建精度，并支持复杂的视频处理任务，如视频修复和基于关键帧的时序一致视频编辑。

Conclusion: 通过引入时空一致代理节点，本研究成功克服了传统视频表示的脆弱性，提供了一种稳定、高效且支持高级编辑功能的新型视频表示框架。

Abstract: Current video representations heavily rely on unstable and over-grained
priors for motion and appearance modelling, \emph{i.e.}, pixel-level matching
and tracking. A tracking error of just a few pixels would lead to the collapse
of the visual object representation, not to mention occlusions and large motion
frequently occurring in videos. To overcome the above mentioned vulnerability,
this work proposes spatio-temporally consistent proxy nodes to represent
dynamically changing objects/scenes in the video. On the one hand, the
hierarchical proxy nodes have the ability to stably express the multi-scale
structure of visual objects, so they are not affected by accumulated tracking
error, long-term motion, occlusion, and viewpoint variation. On the other hand,
the dynamic representation update mechanism of the proxy nodes adequately
leverages spatio-temporal priors of the video to mitigate the impact of
inaccurate trackers, thereby effectively handling drastic changes in scenes and
objects. Additionally, the decoupled encoding manner of the shape and texture
representations across different visual objects in the video facilitates
controllable and fine-grained appearance editing capability. Extensive
experiments demonstrate that the proposed representation achieves high video
reconstruction accuracy with fewer parameters and supports complex video
processing tasks, including video in-painting and keyframe-based temporally
consistent video editing.

</details>


### [96] [Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images](https://arxiv.org/abs/2510.12258)
*Yuto Yokoi,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: 本文提出了两种新颖的乘法损失函数：乘法损失（Multiplicative Loss）和置信度自适应乘法损失（Confidence-Adaptive Multiplicative Loss），用于医学和细胞图像的语义分割。它们通过乘法方式结合交叉熵和Dice损失，根据预测置信度动态调整梯度，在数据有限的情况下表现优于传统的加法组合和现有损失函数，提供了一种无超参数的鲁棒分割机制。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的交叉熵和Dice损失的加法组合对超参数敏感，且在数据有限的情况下表现不佳。医学图像由于隐私、伦理和昂贵的标注等原因，面临数据稀缺的挑战，因此需要更鲁棒、高效的训练目标。

Method: 1. **乘法损失（Multiplicative Loss）**：以乘法方式结合交叉熵和Dice损失。它根据预测置信度动态调整梯度，减少对自信且正确预测的惩罚，同时放大对过度自信但错误预测的梯度，从而稳定优化。2. **置信度自适应乘法损失（Confidence-Adaptive Multiplicative Loss）**：在乘法损失的基础上，引入了受Focal Loss启发的置信度驱动的指数缩放。通过整合预测概率和Dice系数，强调了难以学习的样本，并在置信度较低时强化梯度，以增强在极端数据稀缺下的学习效果。

Result: 在细胞和医学分割基准上的实验表明，本文提出的框架持续优于经过调优的加法组合和现有损失函数，证明其在挑战性数据限制下能提供一致的性能提升。

Conclusion: 所提出的乘法损失函数提供了一种简单、有效且无超参数的机制，能够实现数据受限情况下的鲁棒分割，显著提升了医学和细胞图像分割的性能和稳定性。

Abstract: We propose two novel loss functions, Multiplicative Loss and
Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical
and cellular images. Although Cross Entropy and Dice Loss are widely used,
their additive combination is sensitive to hyperparameters and often performs
suboptimally, especially with limited data. Medical images suffer from data
scarcity due to privacy, ethics, and costly annotations, requiring robust and
efficient training objectives. Our Multiplicative Loss combines Cross Entropy
and Dice losses multiplicatively, dynamically modulating gradients based on
prediction confidence. This reduces penalties for confident correct predictions
and amplifies gradients for incorrect overconfident ones, stabilizing
optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies
a confidence-driven exponential scaling inspired by Focal Loss, integrating
predicted probabilities and Dice coefficients to emphasize difficult samples.
This enhances learning under extreme data scarcity by strengthening gradients
when confidence is low. Experiments on cellular and medical segmentation
benchmarks show our framework consistently outperforms tuned additive and
existing loss functions, offering a simple, effective, and hyperparameter-free
mechanism for robust segmentation under challenging data limitations.

</details>


### [97] [Local Background Features Matter in Out-of-Distribution Detection](https://arxiv.org/abs/2510.12259)
*Jinlun Ye,Zhuohao Sun,Yiqiao Qiu,Qiu Li,Zhijun Tan,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出一种新颖的OOD检测方法，利用ID图像的局部背景特征作为伪OOD特征进行模型训练，有效缓解了模型对OOD数据的过高置信度，并取得了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实际部署中，OOD检测对于确保其可靠性和安全性至关重要。主要挑战是神经网络模型对OOD数据经常产生过高置信度的预测。现有使用辅助OOD数据集或生成伪OOD图像的方法虽然有前景，但受限于高昂的数据收集和训练成本。

Method: 本研究提出一种新颖有效的OOD检测方法，利用局部背景特征作为伪OOD特征进行模型训练。受OOD图像通常与ID图像共享相似背景区域的启发，方法基于卷积的局部不变性，从ID图像中提取背景特征作为训练期间模拟的OOD视觉表示。通过优化以减少这些背景特征的L2范数，使神经网络能够缓解对OOD数据的过高置信度问题。

Result: 在多个标准OOD检测基准上进行的广泛实验证实了该方法的有效性，并显示其与现有后处理方法具有广泛的组合兼容性。本方法取得了新的最先进性能。

Conclusion: 该方法通过巧妙地利用ID图像的局部背景特征作为伪OOD特征进行训练，有效解决了神经网络在OOD数据上的过高置信度问题，显著提升了OOD检测性能，并表现出良好的兼容性。

Abstract: Out-of-distribution (OOD) detection is crucial when deploying deep neural
networks in the real world to ensure the reliability and safety of their
applications. One main challenge in OOD detection is that neural network models
often produce overconfident predictions on OOD data. While some methods using
auxiliary OOD datasets or generating fake OOD images have shown promising OOD
detection performance, they are limited by the high costs of data collection
and training. In this study, we propose a novel and effective OOD detection
method that utilizes local background features as fake OOD features for model
training. Inspired by the observation that OOD images generally share similar
background regions with ID images, the background features are extracted from
ID images as simulated OOD visual representations during training based on the
local invariance of convolution. Through being optimized to reduce the
$L_2$-norm of these background features, the neural networks are able to
alleviate the overconfidence issue on OOD data. Extensive experiments on
multiple standard OOD detection benchmarks confirm the effectiveness of our
method and its wide combinatorial compatibility with existing post-hoc methods,
with new state-of-the-art performance achieved from our method.

</details>


### [98] [AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion](https://arxiv.org/abs/2510.12260)
*Xiaopeng Liu,Yupei Lin,Sen Zhang,Xiao Wang,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出AngularFuse框架，通过引入跨模态互补掩码、精细化参考图像合成策略和角度感知损失，解决了可见光-红外图像融合中现有无监督方法在细节和边缘保真度方面的挑战，显著提升了融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像融合方法依赖手动设计的损失函数，但其构建的参考图像缺乏细节且亮度不均，同时广泛使用的梯度损失仅关注梯度幅度而忽略方向，导致融合图像质量受限，无法满足下游任务需求。

Method: 本文提出AngularFuse框架：首先设计了跨模态互补掩码模块，促使网络学习模态间互补信息；其次引入精细化参考图像合成策略，结合拉普拉斯边缘增强和自适应直方图均衡化，生成细节丰富、亮度均衡的参考图像；最后，首次引入了角度感知损失，在梯度域同时约束梯度幅度和方向。

Result: 在MSRS、RoadScene和M3FD等公开数据集上的综合实验表明，AngularFuse的性能明显优于现有主流方法。视觉对比也进一步证实，在挑战性场景中，AngularFuse能生成更锐利、更细节丰富的融合结果。

Conclusion: AngularFuse展示了卓越的融合能力，能有效保留纹理强度和正确的边缘方向，解决了现有无监督融合方法的关键挑战，为自主驾驶和夜间监控等应用提供了更优质的融合图像。

Abstract: Visible-infrared image fusion is crucial in key applications such as
autonomous driving and nighttime surveillance. Its main goal is to integrate
multimodal information to produce enhanced images that are better suited for
downstream tasks. Although deep learning based fusion methods have made
significant progress, mainstream unsupervised approaches still face serious
challenges in practical applications. Existing methods mostly rely on manually
designed loss functions to guide the fusion process. However, these loss
functions have obvious limitations. On one hand, the reference images
constructed by existing methods often lack details and have uneven brightness.
On the other hand, the widely used gradient losses focus only on gradient
magnitude. To address these challenges, this paper proposes an angle-based
perception framework for spatial-sensitive image fusion (AngularFuse). At
first, we design a cross-modal complementary mask module to force the network
to learn complementary information between modalities. Then, a fine-grained
reference image synthesis strategy is introduced. By combining Laplacian edge
enhancement with adaptive histogram equalization, reference images with richer
details and more balanced brightness are generated. Last but not least, we
introduce an angle-aware loss, which for the first time constrains both
gradient magnitude and direction simultaneously in the gradient domain.
AngularFuse ensures that the fused images preserve both texture intensity and
correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and
M3FD public datasets show that AngularFuse outperforms existing mainstream
methods with clear margin. Visual comparisons further confirm that our method
produces sharper and more detailed results in challenging scenes, demonstrating
superior fusion capability.

</details>


### [99] [SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis](https://arxiv.org/abs/2510.12267)
*Chenghanyu Zhang,Zekun Li,Peipei Li,Xing Cui,Shuhan Xia,Weixiang Yan,Yiqiao Zhang,Qianyu Zhuang*

Main category: cs.CV

TL;DR: 为评估多模态大语言模型（MLLMs）在脊柱领域的表现，本文提出SpineBench，一个包含6万多问答对的视觉问答基准，并发现现有MLLMs在该领域表现不佳。


<details>
  <summary>Details</summary>
Motivation: MLLMs日益融入医疗领域，但现有基准未能全面评估其在脊柱等视觉输入依赖型细微医学领域的性能。

Method: 引入SpineBench，一个针对脊柱领域的综合性VQA基准，包含64,878个问答对和40,263张脊柱图像，涵盖11种脊柱疾病的诊断和病灶定位任务（多项选择格式）。该基准通过整合开源数据集并生成基于视觉相似性的挑战性难题选项构建。使用SpineBench评估了12个主流MLLMs。

Result: 评估结果显示，当前主流MLLMs在脊柱任务中表现不佳。

Conclusion: 当前MLLMs在脊柱领域存在局限性，本研究为未来脊柱医学应用提供了改进方向。

Abstract: With the increasing integration of Multimodal Large Language Models (MLLMs)
into the medical field, comprehensive evaluation of their performance in
various medical domains becomes critical. However, existing benchmarks
primarily assess general medical tasks, inadequately capturing performance in
nuanced areas like the spine, which relies heavily on visual input. To address
this, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)
benchmark designed for fine-grained analysis and evaluation of MLLMs in the
spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,
covering 11 spinal diseases through two critical clinical tasks: spinal disease
diagnosis and spinal lesion localization, both in multiple-choice format.
SpineBench is built by integrating and standardizing image-label pairs from
open-source spinal disease datasets, and samples challenging hard negative
options for each VQA pair based on visual similarity (similar but not the same
disease), simulating real-world challenging scenarios. We evaluate 12 leading
MLLMs on SpineBench. The results reveal that these models exhibit poor
performance in spinal tasks, highlighting limitations of current MLLM in the
spine domain and guiding future improvements in spinal medicine applications.
SpineBench is publicly available at
https://zhangchenghanyu.github.io/SpineBench.github.io/.

</details>


### [100] [PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes](https://arxiv.org/abs/2510.12282)
*Ying A,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: PAGS引入语义优先级解决自动驾驶3D城市场景重建中质量与效率的权衡问题，实现高保真和快速渲染。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶3D城市场景重建方法在保真度和计算成本上存在冲突，且因缺乏语义感知而效率低下，资源分配不均，未区分重要与非重要物体。

Method: 提出优先级自适应高斯溅射（PAGS）框架，将任务感知语义优先级直接注入3D重建与渲染管线。包含：1) 语义引导剪枝与正则化策略，利用混合重要性指标简化非关键场景元素并保留导航关键物体的精细细节；2) 优先级驱动渲染管线，通过基于优先级的深度预处理剔除被遮挡图元并加速最终着色计算。

Result: 在Waymo和KITTI数据集上的广泛实验表明，PAGS在安全关键物体上实现了卓越的重建质量，同时显著缩短了训练时间，并将渲染速度提升至350 FPS以上。

Conclusion: PAGS通过整合语义优先级，有效克服了3D城市场景重建中的质量与效率难题，显著提升了自动驾驶任务中关键物体的重建质量和系统性能。

Abstract: Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet
current methods face a stark trade-off between fidelity and computational cost.
This inefficiency stems from their semantically agnostic design, which
allocates resources uniformly, treating static backgrounds and safety-critical
objects with equal importance. To address this, we introduce Priority-Adaptive
Gaussian Splatting (PAGS), a framework that injects task-aware semantic
priorities directly into the 3D reconstruction and rendering pipeline. PAGS
introduces two core contributions: (1) Semantically-Guided Pruning and
Regularization strategy, which employs a hybrid importance metric to
aggressively simplify non-critical scene elements while preserving fine-grained
details on objects vital for navigation. (2) Priority-Driven Rendering
pipeline, which employs a priority-based depth pre-pass to aggressively cull
occluded primitives and accelerate the final shading computations. Extensive
experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves
exceptional reconstruction quality, particularly on safety-critical objects,
while significantly reducing training time and boosting rendering speeds to
over 350 FPS.

</details>


### [101] [Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval](https://arxiv.org/abs/2510.12283)
*Jianfeng Dong,Lei Huang,Daizong Liu,Xianke Chen,Xun Yang,Changting Lin,Xun Wang,Meng Wang*

Main category: cs.CV

TL;DR: 针对部分相关视频检索(PRVR)这一更具挑战性的任务，本文提出DL-DKD++框架，通过知识蒸馏将大型预训练模型的泛化能力迁移到轻量级双分支网络，并引入动态软目标机制，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索工作大多假设视频已预剪辑且内容与文本高度相关，但在实际应用中，视频通常是未剪辑的长时段且包含复杂背景内容。因此，需要解决更实用但更具挑战性的部分相关视频检索(PRVR)任务。

Method: 本文提出一个新颖的DL-DKD++（Dual Learning framework with Dynamic Knowledge Distillation）框架。该框架从强大的大规模视觉-语言预训练模型中提取泛化知识，并将其迁移到一个轻量级的任务特定PRVR网络。学生模型包含两个分支：一个继承分支吸收教师模型的可迁移知识，一个探索分支从PRVR数据集中学习任务特定信息以弥合领域差距。此外，引入动态软目标构建机制，以自适应的软目标取代僵硬的硬目标监督，以更好地捕捉视频和查询之间细粒度的部分相关性。

Result: 实验结果表明，所提出的模型在TVR、ActivityNet和Charades-STA数据集上，针对PRVR任务均取得了最先进的性能。

Conclusion: DL-DKD++框架通过知识蒸馏和动态软目标机制，有效解决了部分相关视频检索的挑战，并在多个基准数据集上超越了现有水平。

Abstract: Almost all previous text-to-video retrieval works ideally assume that videos
are pre-trimmed with short durations containing solely text-related content.
However, in practice, videos are typically untrimmed in long durations with
much more complicated background content. Therefore, in this paper, we focus on
the more practical yet challenging task of Partially Relevant Video Retrieval
(PRVR), which aims to retrieve partially relevant untrimmed videos with the
given query. To tackle this task, we propose a novel framework that distills
generalization knowledge from a powerful large-scale vision-language
pre-trained model and transfers it to a lightweight, task-specific PRVR
network. Specifically, we introduce a Dual Learning framework with Dynamic
Knowledge Distillation (DL-DKD++), where a large teacher model provides
supervision to a compact dual-branch student network. The student model
comprises two branches: an inheritance branch that absorbs transferable
knowledge from the teacher, and an exploration branch that learns task-specific
information from the PRVR dataset to address domain gaps. To further enhance
learning, we incorporate a dynamic soft-target construction mechanism. By
replacing rigid hard-target supervision with adaptive soft targets that evolve
during training, our method enables the model to better capture the
fine-grained, partial relevance between videos and queries. Experiment results
demonstrate that our proposed model achieves state-of-the-art performance on
TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at
https://github.com/HuiGuanLab/DL-DKD.

</details>


### [102] [Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector](https://arxiv.org/abs/2510.12287)
*Sifan Li,Hongkai Chen,Yujun Cai,Qingwen Ye,Liyang Chen,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 视觉语言模型(VLM)存在“标志幻觉”问题，即对无文字标志生成品牌名。本文系统性测量其在主流VLM上的表现及鲁棒性，发现幻觉与投影器(projector)的特定维度相关，并提出通过投影器解耦和OCR引导解码来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态推理方面取得了显著进展，但它们仍易受幻觉影响，即输出与视觉证据不符。本文聚焦于一个此前被忽视的领域：标志幻觉，即模型在标志无可见文字的情况下生成品牌名称或文本内容。

Method: 研究方法包括：使用精心策划的纯符号、混合型和含文字标志数据集（包括挑战性的Hard-60子集）系统测量领先VLM的幻觉现象；通过九种结构化扰动探究模型的鲁棒性；对开放权重LLaVA进行嵌入层分析，以揭示幻觉与投影器维度之间的关联，并通过有针对性的消融实验验证，同时保持OCR准确性。

Result: 研究发现：幻觉即使在强扰动下仍持续存在，其中遮挡暴露了最显著的弱点；幻觉与投影器的一小部分维度紧密相关，对这些维度进行针对性消融能显著减少错误，同时保持OCR准确性；VLM常依赖符号先验而非真正的字形感知，尤其对于标志性圆形标志，且投影器子空间在此类失败模式中起决定性作用。

Conclusion: 本研究提供了一个新颖的诊断视角和可操作的缓解策略，强调投影器解耦和OCR引导的解码是构建更可信多模态系统的有前景方向。

Abstract: Vision Language Models (VLMs) have achieved impressive progress in multimodal
reasoning; yet, they remain vulnerable to hallucinations, where outputs are not
grounded in visual evidence. In this paper, we investigate a previously
overlooked setting: logo hallucination, where models generate brand names or
textual content despite logos containing no visible words. Using curated splits
of pure symbols, hybrids, and text-bearing logos, as well as the challenging
Hard-60 subset, we systematically measure hallucination across leading VLMs. We
further probe robustness through nine structured perturbations and show that
hallucinations persist even under strong distortions, with occlusion exposing
the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA
demonstrates that hallucination is tied to a small subset of projector
dimensions, and targeted ablation substantially reduces errors while preserving
OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic
priors rather than genuine glyph perception, particularly for iconic circular
logos, and that projector subspaces play a decisive role in this failure mode.
Our work contributes both a novel diagnostic lens and actionable mitigation
insights, highlighting projector disentanglement and OCR-guided decoding as
promising directions for building more trustworthy multimodal systems.

</details>


### [103] [Hybrid Gaussian Splatting for Novel Urban View Synthesis](https://arxiv.org/abs/2510.12308)
*Mohamed Omran,Farhad Zanjani,Davide Abati,Jens Petersen,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 高通AI研究团队为RealADSim-NVS挑战提出了一种新颖视角合成方案，结合3D重建（基于高斯泼溅）和单步扩散模型，并在挑战中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决RealADSim-NVS挑战中街景新视角合成问题，即根据车载训练帧生成同一城市环境的不同视角渲染图。

Method: 采用两阶段混合方法：首先，对场景进行3D重建并渲染目标视角；然后，使用专用的单步扩散模型增强生成的帧。该方案融合了高斯泼溅和扩散模型的优点。

Result: 模型性能通过PSNR、SSIM和LPIPS等指标评估，并对组件进行了消融研究。在公开排行榜上，该方案获得0.432的总分，位列第二。

Conclusion: 该混合方法在街景新视角合成方面表现出显著效果和竞争力，成功在RealADSim-NVS挑战中取得优异成绩。

Abstract: This paper describes the Qualcomm AI Research solution to the RealADSim-NVS
challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge
concerns novel view synthesis in street scenes, and participants are required
to generate, starting from car-centric frames captured during some training
traversals, renders of the same urban environment as viewed from a different
traversal (e.g. different street lane or car direction). Our solution is
inspired by hybrid methods in scene generation and generative simulators
merging gaussian splatting and diffusion models, and it is composed of two
stages: First, we fit a 3D reconstruction of the scene and render novel views
as seen from the target cameras. Then, we enhance the resulting frames with a
dedicated single-step diffusion model. We discuss specific choices made in the
initialization of gaussian primitives as well as the finetuning of the enhancer
model and its training data curation. We report the performance of our model
design and we ablate its components in terms of novel view quality as measured
by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our
proposal reaches an aggregated score of 0.432, achieving the second place
overall.

</details>


### [104] [CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion](https://arxiv.org/abs/2510.12362)
*Jinzhou Lin,Jie Zhou,Wenhao Xu,Rongtao Xu,Changwei Wang,Shunpeng Chen,Kexue Fu,Yihua Shao,Li Guo,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文提出CurriFlow框架，通过整合光流时间对齐和课程引导深度融合，解决了现有语义场景补全（SSC）方法在运动推理和噪声深度监督方面的不足，并在SemanticKITTI上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSC方法缺乏显式运动推理，难以处理遮挡和噪声深度监督，且多依赖于时间堆叠或深度投影。

Method: CurriFlow框架整合了三项关键技术：1) 利用预训练光流进行多级特征（分割、视觉、深度）跨帧对齐，以提高时间一致性和动态对象理解；2) 采用课程学习机制，在训练中从稀疏精确的LiDAR深度逐步过渡到密集但有噪声的立体深度，增强几何鲁棒性；3) 引入Segment Anything Model (SAM) 的语义先验，强化体素级语义学习和空间一致性。

Result: 在SemanticKITTI基准测试中，CurriFlow实现了16.9的平均IoU，达到最先进（SOTA）性能。

Conclusion: CurriFlow的运动引导和课程感知设计对于基于摄像头的3D语义场景补全非常有效。

Abstract: Semantic Scene Completion (SSC) aims to infer complete 3D geometry and
semantics from monocular images, serving as a crucial capability for
camera-based perception in autonomous driving. However, existing SSC methods
relying on temporal stacking or depth projection often lack explicit motion
reasoning and struggle with occlusions and noisy depth supervision. We propose
CurriFlow, a novel semantic occupancy prediction framework that integrates
optical flow-based temporal alignment with curriculum-guided depth fusion.
CurriFlow employs a multi-level fusion strategy to align segmentation, visual,
and depth features across frames using pre-trained optical flow, thereby
improving temporal consistency and dynamic object understanding. To enhance
geometric robustness, a curriculum learning mechanism progressively transitions
from sparse yet accurate LiDAR depth to dense but noisy stereo depth during
training, ensuring stable optimization and seamless adaptation to real-world
deployment. Furthermore, semantic priors from the Segment Anything Model (SAM)
provide category-agnostic supervision, strengthening voxel-level semantic
learning and spatial consistency. Experiments on the SemanticKITTI benchmark
demonstrate that CurriFlow achieves state-of-the-art performance with a mean
IoU of 16.9, validating the effectiveness of our motion-guided and
curriculum-aware design for camera-based 3D semantic scene completion.

</details>


### [105] [Deep Attention-guided Adaptive Subsampling](https://arxiv.org/abs/2510.12376)
*Sharath M Shankaranarayana,Soumava Kumar Roy,Prasad Sudhakar,Chandan Aladahalli*

Main category: cs.CV

TL;DR: 针对深度神经网络计算复杂性高的问题，本文提出了一种注意力引导的、输入自适应的可学习子采样框架，以在3D/视频分类任务中减少冗余、降低计算量并提升性能，克服了现有静态子采样方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络性能优异但计算成本高昂，特别是在3D体积或视频分类任务中，存在大量冗余数据。现有子采样方法（如Gumbel-max）虽然解决了不可微问题，但其采样机制是静态的，仅任务自适应而非输入自适应，难以适应真实世界中动态变化的输入。

Method: 本文提出了一种新颖的可学习子采样框架，其核心是一个注意力引导的采样模块。该模块能够实现输入自适应，即在推理阶段也能根据不同的输入进行动态调整，从而有效选择重要切片或帧。

Result: 该方法在MedMNIST3D的3D医学影像数据集以及两个超声视频分类数据集（其中一个为真实的临床内部数据集）上进行了验证，结果表明它能有效提升模型性能并显著降低深度神经网络的计算复杂性。

Conclusion: 本文提出的注意力引导的输入自适应子采样框架，成功解决了深度神经网络在处理3D/视频数据时的计算复杂性问题，并在多种分类任务中取得了性能提升，为真实世界应用提供了更高效、更灵活的解决方案。

Abstract: Although deep neural networks have provided impressive gains in performance,
these improvements often come at the cost of increased computational complexity
and expense. In many cases, such as 3D volume or video classification tasks,
not all slices or frames are necessary due to inherent redundancies. To address
this issue, we propose a novel learnable subsampling framework that can be
integrated into any neural network architecture. Subsampling, being a
nondifferentiable operation, poses significant challenges for direct adaptation
into deep learning models. While some works, have proposed solutions using the
Gumbel-max trick to overcome the problem of non-differentiability, they fall
short in a crucial aspect: they are only task-adaptive and not inputadaptive.
Once the sampling mechanism is learned, it remains static and does not adjust
to different inputs, making it unsuitable for real-world applications. To this
end, we propose an attention-guided sampling module that adapts to inputs even
during inference. This dynamic adaptation results in performance gains and
reduces complexity in deep neural network models. We demonstrate the
effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as
well as two ultrasound video datasets for classification tasks, one of them
being a challenging in-house dataset collected under real-world clinical
conditions.

</details>


### [106] [Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling](https://arxiv.org/abs/2510.12385)
*Tim J. Schoonbeek,Shao-Hsuan Hung,Dan Lehman,Hans Onvlee,Jacek Kustra,Peter H. N. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: 针对现有程序步骤识别（PSR）模型在遮挡下性能受限的问题，本文提出STORM-PSR双流框架，融合时空特征，显著提升了步骤识别的准确性并减少了预测延迟。


<details>
  <summary>Details</summary>
Motivation: 现有程序步骤识别（PSR）模型仅依赖单帧对象状态检测，忽视时间特征，导致在物体部分遮挡时鲁棒性和准确性受限。

Method: 提出STORM-PSR双流框架，结合空间和时间特征。一个流检测无遮挡对象状态，另一个时空流（包含弱监督预训练空间编码器和基于Transformer的时间编码器）专门处理部分遮挡下的步骤识别。

Result: 在MECCANO和IndustReal数据集上，与现有方法相比，将实际与预测步骤完成之间的平均延迟分别减少了11.2%和26.1%。该改进主要归因于不依赖无遮挡视图的时空流。

Conclusion: STORM-PSR通过有效整合时空特征，克服了现有模型在遮挡下的局限性，显著提高了程序步骤识别的鲁棒性和准确性，尤其是通过其时空流实现了预测延迟的显著降低。

Abstract: Procedure step recognition (PSR) aims to identify all correctly completed
steps and their sequential order in videos of procedural tasks. The existing
state-of-the-art models rely solely on detecting assembly object states in
individual video frames. By neglecting temporal features, model robustness and
accuracy are limited, especially when objects are partially occluded. To
overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient
Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework
for PSR that leverages both spatial and temporal features. The assembly state
detection stream operates effectively with unobstructed views of the object,
while the spatio-temporal stream captures both spatial and temporal features to
recognize step completions even under partial occlusion. This stream includes a
spatial encoder, pre-trained using a novel weakly supervised approach to
capture meaningful spatial representations, and a transformer-based temporal
encoder that learns how these spatial features relate over time. STORM-PSR is
evaluated on the MECCANO and IndustReal datasets, reducing the average delay
between actual and predicted assembly step completions by 11.2% and 26.1%,
respectively, compared to prior methods. We demonstrate that this reduction in
delay is driven by the spatio-temporal stream, which does not rely on
unobstructed views of the object to infer completed steps. The code for
STORM-PSR, along with the newly annotated MECCANO labels, is made publicly
available at https://timschoonbeek.github.io/stormpsr .

</details>


### [107] [Scene Coordinate Reconstruction Priors](https://arxiv.org/abs/2510.12387)
*Wenjing Bian,Axel Barroso-Laguna,Tommaso Cavallari,Victor Adrian Prisacariu,Eric Brachmann*

Main category: cs.CV

TL;DR: 针对场景坐标回归（SCR）模型在多视角约束不足时易退化的问题，本文提出通过概率性重新解释和引入高层重建先验（包括基于3D点云扩散模型的学习先验）来增强SCR训练，从而获得更优的场景表示、提高配准率和相机姿态。


<details>
  <summary>Details</summary>
Motivation: 场景坐标回归（SCR）模型是强大的3D视觉隐式场景表示，但当训练图像提供的多视角约束不足时，模型性能会显著下降甚至退化。

Method: 1. 对SCR模型训练进行概率性重新解释。2. 引入高层重建先验，在训练中引导3D场景点趋向合理几何。3. 探讨多种先验，包括简单深度分布先验和基于大规模室内扫描数据集训练的3D点云扩散模型的学习先验。

Result: 在三个室内数据集上，所提出的先验有助于学习更好的场景表示，生成更一致的场景点云，实现更高的配准率和更好的相机姿态，并对新视图合成和相机重定位等下游任务产生积极影响。

Conclusion: 通过对SCR训练进行概率性重构并融合高层重建先验，本文有效解决了多视角约束不足导致的SCR模型退化问题，显著提升了场景表示的质量和下游3D视觉任务的性能。

Abstract: Scene coordinate regression (SCR) models have proven to be powerful implicit
scene representations for 3D vision, enabling visual relocalization and
structure-from-motion. SCR models are trained specifically for one scene. If
training images imply insufficient multi-view constraints SCR models
degenerate. We present a probabilistic reinterpretation of training SCR models,
which allows us to infuse high-level reconstruction priors. We investigate
multiple such priors, ranging from simple priors over the distribution of
reconstructed depth values to learned priors over plausible scene coordinate
configurations. For the latter, we train a 3D point cloud diffusion model on a
large corpus of indoor scans. Our priors push predicted 3D scene points towards
plausible geometry at each training step to increase their likelihood. On three
indoor datasets our priors help learning better scene representations,
resulting in more coherent scene point clouds, higher registration rates and
better camera poses, with a positive effect on down-stream tasks such as novel
view synthesis and camera relocalization.

</details>


### [108] [Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda](https://arxiv.org/abs/2510.12400)
*André Torneiro,Diogo Monteiro,Paulo Novais,Pedro Rangel Henriques,Nuno F. Rodrigues*

Main category: cs.CV

TL;DR: 本系统综述探讨了视觉-语言模型（VLM）在城市基础设施监测中的应用，特别关注零样本场景，分析了32项研究以回答相关任务、架构、数据集和评估等核心问题。


<details>
  <summary>Details</summary>
Motivation: 城市公共基础设施监测面临对象多样、环境复杂等挑战。现有方法（物联网传感器与人工检查）成本高、难扩展且与市民视觉感知不符。因此，研究机器能否像市民一样“观察”并评估基础设施状况成为关键问题。视觉-语言模型（VLM）在处理复杂视觉信息和自然语言推理方面展现出巨大潜力，有望解决这一挑战。

Method: 本研究采用系统综述方法，遵循PRISMA指南，对2021年至2025年间发表的32篇同行评审研究进行分析。综述重点调查VLM在城市监测中的作用，特别是零样本应用，旨在回答四个核心研究问题：VLM有效解决的城市监测任务、常用且表现优异的VLM架构和框架、支持该领域的数据集和资源，以及VLM应用的评估方法和性能水平。

Result: （本摘要为综述论文的摘要，并未直接给出综述的具体发现。综述结果将体现在对以下四个问题的解答中）：该综述将阐明VLM在城市监测中能够有效处理的任务、主流的VLM架构与框架、支持该新兴领域的数据集与资源，以及VLM应用的评估方法与已报告的性能水平。

Conclusion: 视觉-语言模型（VLM）在解决城市基础设施监测的挑战中具有巨大潜力。本系统综述通过全面分析现有研究，旨在深入理解VLM在该领域的应用现状、关键技术、支持资源以及评估标准，为未来的研究和应用提供全面的见解。

Abstract: Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?

</details>


### [109] [Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model](https://arxiv.org/abs/2510.12408)
*Huu Tien Nguyen,Ahmed Karam Eldaly*

Main category: cs.CV

TL;DR: 本文提出一种基于条件流匹配（CFM）的图像质量迁移框架，用于将低场MRI图像重建为高场级质量，表现出先进性能、良好泛化能力且参数量少。


<details>
  <summary>Details</summary>
Motivation: 低场MRI具有成本低和便携性优势，但信噪比低，诊断质量受限。研究旨在弥合低场与高场MRI之间的图像质量差距，无需昂贵基础设施。

Method: 引入条件流匹配（CFM）框架。CFM通过直接回归最优速度场，学习噪声分布与目标数据分布之间的连续流，以实现图像质量迁移，区别于传统的迭代采样或对抗性生成模型。

Result: CFM在MRI重建中实现了最先进的性能，对分布内和分布外数据均具有强大的泛化能力。此外，其参数量显著少于其他深度学习方法。

Conclusion: CFM作为一种强大且可扩展的工具，在MRI重建，尤其是在资源受限的临床环境中，具有巨大的应用潜力。

Abstract: This paper introduces a novel framework for image quality transfer based on
conditional flow matching (CFM). Unlike conventional generative models that
rely on iterative sampling or adversarial objectives, CFM learns a continuous
flow between a noise distribution and target data distributions through the
direct regression of an optimal velocity field. We evaluate this approach in
the context of low-field magnetic resonance imaging (LF-MRI), a rapidly
emerging modality that offers affordable and portable scanning but suffers from
inherently low signal-to-noise ratio and reduced diagnostic quality. Our
framework is designed to reconstruct high-field-like MR images from their
corresponding low-field inputs, thereby bridging the quality gap without
requiring expensive infrastructure. Experiments demonstrate that CFM not only
achieves state-of-the-art performance, but also generalizes robustly to both
in-distribution and out-of-distribution data. Importantly, it does so while
utilizing significantly fewer parameters than competing deep learning methods.
These results underline the potential of CFM as a powerful and scalable tool
for MRI reconstruction, particularly in resource-limited clinical environments.

</details>


### [110] [VideoLucy: Deep Memory Backtracking for Long Video Understanding](https://arxiv.org/abs/2510.12422)
*Jialong Zuo,Yongtai Deng,Lingdong Kong,Jingkang Yang,Rui Jin,Yiwei Zhang,Nong Sang,Liang Pan,Ziwei Liu,Changxin Gao*

Main category: cs.CV

TL;DR: 本文提出VideoLucy，一个受人类记忆启发，用于长视频理解的深度记忆回溯框架。它通过分层记忆结构和代理迭代回溯机制，有效解决了现有LLM代理系统在时间上下文理解和关键信息丢失方面的挑战，表现优于SOTA模型和GPT-4o，并引入了新基准EgoMem。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的代理系统在长视频理解中面临两大挑战：一是难以捕获连续帧的时间上下文，通常只对单个帧进行建模和推理；二是为降低成本采用稀疏帧采样，可能导致关键信息丢失。

Method: 我们提出VideoLucy框架，受人类从粗到精的回忆过程启发，采用具有渐进粒度的分层记忆结构，明确定义不同层级的细节和时间范围。通过基于代理的迭代回溯机制，VideoLucy系统地挖掘视频范围内的、与问题相关的深层记忆，直至获得足够信息，从而有效理解连续帧的时间上下文并保留关键细节。此外，本文引入了新的长视频理解基准EgoMem，用于全面评估模型理解复杂事件和捕捉细粒度细节的能力。

Result: VideoLucy在多个长视频理解基准测试中表现出卓越的性能，显著优于最先进的方法，甚至超越了GPT-4o等最新的专有模型。这些结果充分证明了VideoLucy的优越性。

Conclusion: VideoLucy通过其创新的分层记忆和迭代回溯机制，成功克服了长视频理解中时间上下文缺失和信息丢失的局限性，显著提升了理解能力。作为一个基于开源模型的强大解决方案，它为该领域提供了新的方向，并且引入的EgoMem基准将推动未来研究。

Abstract: Recent studies have shown that agent-based systems leveraging large language
models (LLMs) for key information retrieval and integration have emerged as a
promising approach for long video understanding. However, these systems face
two major challenges. First, they typically perform modeling and reasoning on
individual frames, struggling to capture the temporal context of consecutive
frames. Second, to reduce the cost of dense frame-level captioning, they adopt
sparse frame sampling, which risks discarding crucial information. To overcome
these limitations, we propose VideoLucy, a deep memory backtracking framework
for long video understanding. Inspired by the human recollection process from
coarse to fine, VideoLucy employs a hierarchical memory structure with
progressive granularity. This structure explicitly defines the detail level and
temporal scope of memory at different hierarchical depths. Through an
agent-based iterative backtracking mechanism, VideoLucy systematically mines
video-wide, question-relevant deep memories until sufficient information is
gathered to provide a confident answer. This design enables effective temporal
understanding of consecutive frames while preserving critical details. In
addition, we introduce EgoMem, a new benchmark for long video understanding.
EgoMem is designed to comprehensively evaluate a model's ability to understand
complex events that unfold over time and capture fine-grained details in
extremely long videos. Extensive experiments demonstrate the superiority of
VideoLucy. Built on open-source models, VideoLucy significantly outperforms
state-of-the-art methods on multiple long video understanding benchmarks,
achieving performance even surpassing the latest proprietary models such as
GPT-4o. Our code and dataset will be made publicly at
https://videolucy.github.io

</details>


### [111] [A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation](https://arxiv.org/abs/2510.12444)
*Shaoyang Zhou,Yingshu Li,Yunyi Liu,Lingqiao Liu,Lei Wang,Luping Zhou*

Main category: cs.CV

TL;DR: 首次全面综述纵向放射学报告生成（LRRG），涵盖数据集、模型架构、评估协议、性能分析，并指出当前局限和未来方向。


<details>
  <summary>Details</summary>
Motivation: 胸部X光影像诊断工作量巨大，现有报告生成模型（CXRRRG）多依赖单张影像，缺乏纵向上下文，无法生成准确的比较性报告。现有综述也主要关注单张影像，缺乏针对纵向设置的系统性指导。

Method: 本综述对纵向放射学报告生成（LRRG）进行了首次全面审查。具体分析了数据集构建策略、报告生成架构（特别是纵向定制设计）、评估协议（包括纵向特定指标和通用基准）。

Result: 总结了LRRG方法的性能，并分析了消融研究，强调纵向信息和架构设计选择在提高模型性能中的关键作用。

Conclusion: 总结了当前研究的五个主要局限性，并提出了未来发展的有前景方向，旨在为推进这一新兴领域奠定基础。

Abstract: Chest Xray imaging is a widely used diagnostic tool in modern medicine, and
its high utilization creates substantial workloads for radiologists. To
alleviate this burden, vision language models are increasingly applied to
automate Chest Xray radiology report generation (CXRRRG), aiming for clinically
accurate descriptions while reducing manual effort. Conventional approaches,
however, typically rely on single images, failing to capture the longitudinal
context necessary for producing clinically faithful comparison statements.
Recently, growing attention has been directed toward incorporating longitudinal
data into CXR RRG, enabling models to leverage historical studies in ways that
mirror radiologists diagnostic workflows. Nevertheless, existing surveys
primarily address single image CXRRRG and offer limited guidance for
longitudinal settings, leaving researchers without a systematic framework for
model design. To address this gap, this survey provides the first comprehensive
review of longitudinal radiology report generation (LRRG). Specifically, we
examine dataset construction strategies, report generation architectures
alongside longitudinally tailored designs, and evaluation protocols
encompassing both longitudinal specific measures and widely used benchmarks. We
further summarize LRRG methods performance, alongside analyses of different
ablation studies, which collectively highlight the critical role of
longitudinal information and architectural design choices in improving model
performance. Finally, we summarize five major limitations of current research
and outline promising directions for future development, aiming to lay a
foundation for advancing this emerging field.

</details>


### [112] [MS-GAGA: Metric-Selective Guided Adversarial Generation Attack](https://arxiv.org/abs/2510.12468)
*Dion J. X. Ho,Gabriel Lee Jun Rong,Niharika Shrivastava,Harshavardhan Abichandani,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CV

TL;DR: MS-GAGA是一个两阶段框架，用于在黑盒设置下生成针对深度伪造检测器的高可迁移且视觉不可察觉的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 在黑盒设置下，现有对抗攻击在针对深度伪造检测器时，其对抗样本的可迁移性和视觉隐蔽性有待提高。

Method: 该框架包含两阶段：阶段1使用双流攻击模块（MNTD-PGD和SG-PGD）生成对抗候选样本，以增强梯度计算或关注显著区域，从而扩大搜索空间并提高可迁移性。阶段2通过度量感知选择模块，基于对黑盒模型的攻击成功率和与原始图像的结构相似性（SSIM）来选择最佳候选样本，以共同优化可迁移性和隐蔽性。

Result: 与现有最先进的攻击相比，MS-GAGA在未见过的检测器上实现了高达27%的更高错误分类率。

Conclusion: MS-GAGA通过其独特的两阶段设计，成功地提高了在黑盒设置下对抗深度伪造检测器的对抗样本的可迁移性和视觉隐蔽性，并表现优于现有技术。

Abstract: We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a
two-stage framework for crafting transferable and visually imperceptible
adversarial examples against deepfake detectors in black-box settings. In Stage
1, a dual-stream attack module generates adversarial candidates: MNTD-PGD
applies enhanced gradient calculations optimized for small perturbation
budgets, while SG-PGD focuses perturbations on visually salient regions. This
complementary design expands the adversarial search space and improves
transferability across unseen models. In Stage 2, a metric-aware selection
module evaluates candidates based on both their success against black-box
models and their structural similarity (SSIM) to the original image. By jointly
optimizing transferability and imperceptibility, MS-GAGA achieves up to 27%
higher misclassification rates on unseen detectors compared to state-of-the-art
attacks.

</details>


### [113] [A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation](https://arxiv.org/abs/2510.12482)
*Shurong Chai,Rahul Kumar JAIN,Rui Xu,Shaocong Mo,Ruibo Hou,Shiyu Teng,Jiaqing Liu,Lanfen Lin,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 为解决文本引导医学图像分割中数据增强破坏图文空间一致性问题，本文提出一种在增强前进行特征融合的框架，并设计轻量级生成器将文本嵌入视觉空间，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在文本引导图像分割中，传统数据增强（如旋转、翻转）会破坏图像与文本的空间对齐，从而降低模型性能，尤其在医学影像领域数据受限时更甚。

Method: 提出一种早期融合框架，在数据增强前结合文本与视觉特征以保持空间一致性。此外，设计了一个轻量级生成器，将文本嵌入投影到视觉空间以弥合语义差距。

Result: 该方法在三个医学影像任务和四个分割框架上进行评估，均取得了最先进的（state-of-the-art）结果。生成的伪图像可视化显示区域定位准确。

Conclusion: 通过在数据增强前进行特征早期融合，并利用轻量级生成器桥接文本与视觉语义，有效解决了文本引导医学图像分割中的空间对齐问题，显著提升了分割性能。

Abstract: Deep learning relies heavily on data augmentation to mitigate limited data,
especially in medical imaging. Recent multimodal learning integrates text and
images for segmentation, known as referring or text-guided image segmentation.
However, common augmentations like rotation and flipping disrupt spatial
alignment between image and text, weakening performance. To address this, we
propose an early fusion framework that combines text and visual features before
augmentation, preserving spatial consistency. We also design a lightweight
generator that projects text embeddings into visual space, bridging semantic
gaps. Visualization of generated pseudo-images shows accurate region
localization. Our method is evaluated on three medical imaging tasks and four
segmentation frameworks, achieving state-of-the-art results. Code is publicly
available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.

</details>


### [114] [BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring](https://arxiv.org/abs/2510.12493)
*An Zhao,Piaopiao Yu,Zhe Zhu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bi-Stage 3D Gaussian Splatting (BSGS)的新框架，通过两阶段优化策略（相机姿态精修和全局刚体变换）以及创新的梯度聚合和时空优化策略，解决了3D Gaussian Splatting从运动模糊图像重建高质量3D场景的难题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的去模糊方法在从运动模糊图像重建高质量3D场景时面临挑战，主要限制在于对相机姿态准确性的极端依赖以及无法有效控制运动模糊导致的错误高斯基元稠密化。

Method: 引入Bi-Stage 3D Gaussian Splatting (BSGS)框架。第一阶段是相机姿态精修（Camera Pose Refinement），用于粗略优化相机姿态以减少运动引起的畸变。第二阶段是全局刚体变换（Global Rigid Transformation），在固定粗略相机姿态下进一步校正运动引起的模糊畸变。为解决多子帧梯度冲突，提出了子帧梯度聚合策略。此外，引入时空两阶段优化策略（space-time bi-stage optimization）来动态调整基元稠密化阈值，防止模糊区域过早生成噪声高斯。

Result: 全面的实验验证了所提出去模糊方法的有效性，并显示其优于现有最先进的方法。

Conclusion: 所提出的Bi-Stage 3D Gaussian Splatting框架能够有效地从运动模糊图像中准确重建3D场景，解决了现有方法在相机姿态依赖性和错误高斯稠密化控制方面的局限性。

Abstract: 3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene
reconstruction.However, reconstructing high-quality 3D scenes from
motion-blurred images caused by camera motion poses a significant challenge.The
performance of existing 3DGS-based deblurring methods are limited due to their
inherent mechanisms, such as extreme dependence on the accuracy of camera poses
and inability to effectively control erroneous Gaussian primitives
densification caused by motion blur.To solve these problems, we introduce a
novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D
scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose
Refinement roughly optimizes camera poses to reduce motion-induced distortions.
Second, with fixed rough camera poses, Global RigidTransformation further
corrects motion-induced blur distortions.To alleviate multi-subframe gradient
conflicts, we propose a subframe gradient aggregation strategy to optimize both
stages.Furthermore, a space-time bi-stage optimization strategy is introduced
to dynamically adjust primitive densification thresholds and prevent premature
noisy Gaussian generation in blurred regions. Comprehensive experiments verify
the effectiveness of our proposed deblurring method and show its superiority
over the state of the arts.

</details>


### [115] [Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points](https://arxiv.org/abs/2510.12524)
*Jiayi Kong,Chen Zong,Junkai Deng,Xuhui Chen,Fei Hou,Shiqing Xin,Junhui Hou,Chen Qian,Ying He*

Main category: cs.CV

TL;DR: 提出VAD方法，通过Voronoi辅助的双向法线对齐和扩散，从无向点云高效计算UDF，解决了现有方法的稳定性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络方法在学习无符号距离场（UDF）时，常面临数值不稳定、计算成本高和可控性有限等挑战。

Method: VAD是一种轻量级、无网络的UDF计算方法。首先，通过两个Voronoi几何准则编码的能量函数，为无向点云分配双向法线；随后，扩散这些对齐的法线形成近似的UDF梯度场；最后，积分该梯度场以恢复最终的UDF。

Result: 实验证明VAD能鲁棒处理水密、开放、复杂非流形和非定向几何体，同时保持计算效率和稳定性。

Conclusion: VAD为从无向点云计算UDF提供了一种有效、稳定且高效的解决方案，克服了现有神经方法的局限性。

Abstract: Unsigned Distance Fields (UDFs) provide a flexible representation for 3D
shapes with arbitrary topology, including open and closed surfaces, orientable
and non-orientable geometries, and non-manifold structures. While recent neural
approaches have shown promise in learning UDFs, they often suffer from
numerical instability, high computational cost, and limited controllability. We
present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),
for computing UDFs directly from unoriented point clouds. Our approach begins
by assigning bi-directional normals to input points, guided by two
Voronoi-based geometric criteria encoded in an energy function for optimal
alignment. The aligned normals are then diffused to form an approximate UDF
gradient field, which is subsequently integrated to recover the final UDF.
Experiments demonstrate that VAD robustly handles watertight and open surfaces,
as well as complex non-manifold and non-orientable geometries, while remaining
computationally efficient and stable.

</details>


### [116] [Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion](https://arxiv.org/abs/2510.12537)
*David Björkstrand,Tiesheng Wang,Lars Bretzner,Josephine Sullivan*

Main category: cs.CV

TL;DR: 通过特征空间归一化和加权L2损失，一个简化的分数扩散模型在无需复杂输入或辅助损失的情况下，实现了最先进的无条件人体运动生成，并能直接生成运动和形状。


<details>
  <summary>Details</summary>
Motivation: 当前人体运动生成方法常依赖过度参数化的输入特征和辅助损失。研究者认为扩散模型理论上无需这些复杂策略也能匹配人体运动分布，因此旨在探索一个更简洁高效的扩散模型方案。

Method: 本文提出一个基于分数的扩散模型，该模型仅采用细致的特征空间归一化和标准L2分数匹配损失的分析导出权重。它直接生成运动和形状，从而避免了传统方法中缓慢的关节后处理形状恢复。研究通过分步构建方法和有针对性的消融实验验证了每个组件的有效性。

Result: 该方法在无条件人体运动生成任务上达到了与现有最先进技术相当的性能，且无需过度参数化的输入特征和辅助损失。同时，它实现了运动和形状的直接生成。

Conclusion: 仅通过精心设计的特征空间归一化和加权L2分数匹配损失，扩散模型便能实现与最先进技术媲美的人体运动生成，并直接生成运动和形状，证明了复杂输入特征和辅助损失并非扩散模型取得高性能的必要条件。

Abstract: Recent work has explored a range of model families for human motion
generation, including Variational Autoencoders (VAEs), Generative Adversarial
Networks (GANs), and diffusion-based models. Despite their differences, many
methods rely on over-parameterized input features and auxiliary losses to
improve empirical results. These strategies should not be strictly necessary
for diffusion models to match the human motion distribution. We show that on
par with state-of-the-art results in unconditional human motion generation are
achievable with a score-based diffusion model using only careful feature-space
normalization and analytically derived weightings for the standard L2
score-matching loss, while generating both motion and shape directly, thereby
avoiding slow post hoc shape recovery from joints. We build the method step by
step, with a clear theoretical motivation for each component, and provide
targeted ablations demonstrating the effectiveness of each proposed addition in
isolation.

</details>


### [117] [CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving](https://arxiv.org/abs/2510.12560)
*Xiaoji Zheng,Ziyuan Yang,Yanhao Chen,Yuhang Peng,Yuanrong Tang,Gengyuan Liu,Bokui Chen,Jiangtao Gong*

Main category: cs.CV

TL;DR: 本文提出CoIRL-AD，一种竞争性的双策略框架，结合模仿学习(IL)和强化学习(RL)以改善自动驾驶模型的泛化能力，尤其是在长尾场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 纯模仿学习(IL)训练的端到端自动驾驶模型泛化能力差；强化学习(RL)虽然能促进探索但面临样本效率低和收敛不稳定性等挑战。结合IL和RL是自然解决方案。

Method: 提出CoIRL-AD框架，超越传统两阶段范式（IL预训练后RL微调）。CoIRL-AD采用竞争机制，使IL和RL智能体在训练过程中交互，促进知识交换同时防止梯度冲突。

Result: 在nuScenes数据集上，与基线相比，碰撞率降低18%，并展现出更强的泛化能力以及在长尾场景下性能的提升。

Conclusion: CoIRL-AD框架通过有效结合IL和RL的优势，显著提升了自动驾驶模型的性能和泛化能力，特别是在复杂和不常见的驾驶情境中。

Abstract: End-to-end autonomous driving models trained solely with imitation learning
(IL) often suffer from poor generalization. In contrast, reinforcement learning
(RL) promotes exploration through reward maximization but faces challenges such
as sample inefficiency and unstable convergence. A natural solution is to
combine IL and RL. Moving beyond the conventional two-stage paradigm (IL
pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive
dual-policy framework that enables IL and RL agents to interact during
training. CoIRL-AD introduces a competition-based mechanism that facilitates
knowledge exchange while preventing gradient conflicts. Experiments on the
nuScenes dataset show an 18% reduction in collision rate compared to baselines,
along with stronger generalization and improved performance on long-tail
scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.

</details>


### [118] [MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking](https://arxiv.org/abs/2510.12565)
*Tianhao Li,Tingfa Xu,Ying Wang,Haolin Qin,Xu Lin,Jianan Li*

Main category: cs.CV

TL;DR: 针对无人机多目标跟踪中RGB方法在恶劣环境下性能受限的问题，本文发布了首个大规模、具挑战性的多光谱无人机多目标跟踪基准数据集MMOT，并提出了一个多光谱与方向感知的MOT方案，实验证明多光谱输入能显著提升跟踪性能，尤其对小目标和密集目标效果显著。


<details>
  <summary>Details</summary>
Motivation: 无人机多目标跟踪面临小目标、严重遮挡和杂乱背景等挑战。现有基于RGB的算法依赖空间外观特征，在空中视角下容易退化，导致可靠性下降。多光谱图像能提供增强目标判别性的关键线索，但该领域缺乏专用的多光谱UAV数据集，阻碍了研究进展。

Method: ['构建了首个无人机多光谱多目标跟踪基准数据集MMOT，其特点包括：(i) 大规模（125个视频序列，超过48.8万次标注，涵盖8个类别）；(ii) 综合挑战性（包含极端小目标、高密度场景、严重遮挡和复杂运动等多样条件）；(iii) 精确的定向标注，以实现精确的定位并减少空中视角下的歧义。', '提出了一种多光谱与方向感知的多目标跟踪方案，以更好地提取光谱特征并利用定向标注。该方案包含：(i) 一个轻量级的Spectral 3D-Stem，用于整合光谱特征同时保持与RGB预训练的兼容性；(ii) 一个方向感知卡尔曼滤波器，用于精确的状态估计；(iii) 一个端到端方向自适应Transformer。']

Result: 在代表性跟踪器上的大量实验一致表明，多光谱输入比RGB基线显著提升了跟踪性能，特别是在小目标和密集排列目标的情况下效果更为明显。

Conclusion: 本研究通过引入MMOT数据集和提出的多光谱与方向感知MOT方案，有望推动无人机多光谱多目标跟踪领域的研究进展。所有数据集、代码和基准均已公开。

Abstract: Drone-based multi-object tracking is essential yet highly challenging due to
small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based
tracking algorithms heavily depend on spatial appearance cues such as color and
texture, which often degrade in aerial views, compromising reliability.
Multispectral imagery, capturing pixel-level spectral reflectance, provides
crucial cues that enhance object discriminability under degraded spatial
conditions. However, the lack of dedicated multispectral UAV datasets has
hindered progress in this domain. To bridge this gap, we introduce MMOT, the
first challenging benchmark for drone-based multispectral multi-object
tracking. It features three key characteristics: (i) Large Scale - 125 video
sequences with over 488.8K annotations across eight categories; (ii)
Comprehensive Challenges - covering diverse conditions such as extreme small
targets, high-density scenarios, severe occlusions, and complex motion; and
(iii) Precise Oriented Annotations - enabling accurate localization and reduced
ambiguity under aerial perspectives. To better extract spectral features and
leverage oriented annotations, we further present a multispectral and
orientation-aware MOT scheme adapting existing methods, featuring: (i) a
lightweight Spectral 3D-Stem integrating spectral features while preserving
compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for
precise state estimation; and (iii) an end-to-end orientation-adaptive
transformer. Extensive experiments across representative trackers consistently
show that multispectral input markedly improves tracking performance over RGB
baselines, particularly for small and densely packed objects. We believe our
work will advance drone-based multispectral multi-object tracking research. Our
MMOT, code, and benchmarks are publicly available at
https://github.com/Annzstbl/MMOT.

</details>


### [119] [Learning Human Motion with Temporally Conditional Mamba](https://arxiv.org/abs/2510.12573)
*Quang Nguyen,Tri Le,Baoru Huang,Minh Nhat Vu,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: 提出基于Mamba的条件Mamba模型（TCM），通过将条件信息融入Mamba的循环动力学，显著改善人体运动生成中的时间对齐问题。


<details>
  <summary>Details</summary>
Motivation: 基于时间依赖输入信号学习人体运动具有挑战性且应用广泛。现有方法（如交叉注意力）主要捕获全局交互，难以保持逐步时间对齐。

Method: 引入“时间条件Mamba”（Temporally Conditional Mamba, TCM），一种新的基于Mamba的人体运动生成模型。该方法将条件信息整合到Mamba块的循环动力学中，以实现更好的时间对齐运动。

Result: 大量实验证明，与现有最先进方法相比，TCM在时间对齐、运动真实性和条件一致性方面有显著改进。

Conclusion: TCM通过将条件信息融入Mamba的循环动力学，有效解决了人体运动生成中的时间对齐局限性，从而生成更真实、更符合条件的运动。

Abstract: Learning human motion based on a time-dependent input signal presents a
challenging yet impactful task with various applications. The goal of this task
is to generate or estimate human movement that consistently reflects the
temporal patterns of conditioning inputs. Existing methods typically rely on
cross-attention mechanisms to fuse the condition with motion. However, this
approach primarily captures global interactions and struggles to maintain
step-by-step temporal alignment. To address this limitation, we introduce
Temporally Conditional Mamba, a new mamba-based model for human motion
generation. Our approach integrates conditional information into the recurrent
dynamics of the Mamba block, enabling better temporally aligned motion. To
validate the effectiveness of our method, we evaluate it on a variety of human
motion tasks. Extensive experiments demonstrate that our model significantly
improves temporal alignment, motion realism, and condition consistency over
state-of-the-art approaches. Our project page is available at
https://zquang2202.github.io/TCM.

</details>


### [120] [Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence](https://arxiv.org/abs/2510.12579)
*Simon Ravé,Jean-Christophe Lombardo,Pejman Rasti,Alexis Joly,David Rousseau*

Main category: cs.CV

TL;DR: 该研究提出一种零样本农业图像分割方法，结合Plantnet的DinoV2骨干网络生成粗略掩码，再由SAM细化，以解决标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习方法在农业图像分割中常受限于有限的训练数据、复杂的田间条件以及繁重的数据标注工作。

Method: 该方法利用Plantnet（大型植物分类模型）的DinoV2骨干网络来识别植物区域并生成粗略分割掩码，然后通过Segment Anything Model (SAM) 对这些粗略掩码进行精细化处理，以获得详细的分割结果，无需收集和标注新数据集。

Result: 在四个公开数据集上的评估表明，使用Plantnet微调的DinoV2模型相比基础DinoV2模型，在Jaccard指数（IoU）方面展现出持续的性能提升。

Conclusion: 结合基础模型（如DinoV2、SAM）与专业植物模型（如Plantnet）具有巨大潜力，能够有效缓解标注瓶颈，并在多样化的农业场景中实现高效的分割。

Abstract: We present a zero-shot segmentation approach for agricultural imagery that
leverages Plantnet, a large-scale plant classification model, in conjunction
with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than
collecting and annotating new datasets, our method exploits Plantnet's
specialized plant representations to identify plant regions and produce coarse
segmentation masks. These masks are then refined by SAM to yield detailed
segmentations. We evaluate on four publicly available datasets of various
complexity in terms of contrast including some where the limited size of the
training data and complex field conditions often hinder purely supervised
methods. Our results show consistent performance gains when using
Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the
Jaccard Index (IoU). These findings highlight the potential of combining
foundation models with specialized plant-centric models to alleviate the
annotation bottleneck and enable effective segmentation in diverse agricultural
scenarios.

</details>


### [121] [LayerSync: Self-aligning Intermediate Layers](https://arxiv.org/abs/2510.12581)
*Yasaman Haghighi,Bastien van Delft,Mariam Hassan,Alexandre Alahi*

Main category: cs.CV

TL;DR: LayerSync通过利用扩散模型自身语义丰富的中间表示进行内部指导，显著提升了扩散模型的生成质量和训练效率，且具有领域无关和即插即用的特性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明外部指导能加速扩散模型的训练，但需依赖外部监督。本文旨在通过内部机制，利用模型自身的中间表示来提供指导，从而减少对外部监督的需求。

Method: 提出LayerSync，它通过正则化扩散模型自身较弱的中间表示，利用其内部语义更丰富的中间表示作为内在指导。这是一种自给自足、即插即用的正则化项，不增加训练开销，不依赖预训练模型或额外数据，并适用于多种模态。

Result: LayerSync在图像生成任务上表现优异，并成功推广至音频、视频和动作生成等领域。它持续提升了生成质量和训练效率，例如，在ImageNet数据集上将基于流的Transformer训练速度提高8.75倍以上，同时将生成质量提升了23.6%。

Conclusion: LayerSync通过创新的自指导机制，有效提升了扩散模型的生成质量和训练效率，其领域无关、无额外开销的特性使其成为一个有前景的通用优化方案。

Abstract: We propose LayerSync, a domain-agnostic approach for improving the generation
quality and the training efficiency of diffusion models. Prior studies have
highlighted the connection between the quality of generation and the
representations learned by diffusion models, showing that external guidance on
model intermediate representations accelerates training. We reconceptualize
this paradigm by regularizing diffusion models with their own intermediate
representations. Building on the observation that representation quality varies
across diffusion model layers, we show that the most semantically rich
representations can act as an intrinsic guidance for weaker ones, reducing the
need for external supervision. Our approach, LayerSync, is a self-sufficient,
plug-and-play regularizer term with no overhead on diffusion model training and
generalizes beyond the visual domain to other modalities. LayerSync requires no
pretrained models nor additional data. We extensively evaluate the method on
image generation and demonstrate its applicability to other domains such as
audio, video, and motion generation. We show that it consistently improves the
generation quality and the training efficiency. For example, we speed up the
training of flow-based transformer by over 8.75x on ImageNet dataset and
improved the generation quality by 23.6%. The code is available at
https://github.com/vita-epfl/LayerSync.

</details>


### [122] [Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training](https://arxiv.org/abs/2510.12586)
*Jiachen Lei,Keli Liu,Julius Berner,Haiming Yu,Hongkai Zheng,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 针对像素空间生成模型训练难、性能差的问题，本文提出一种两阶段训练框架，显著提升了像素空间扩散模型和一致性模型在ImageNet上的生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 像素空间生成模型训练困难，性能和效率通常不及潜在空间模型，存在明显的差距。

Method: 引入两阶段训练框架：1. 预训练编码器以捕捉图像语义并将其与确定性采样轨迹对齐。2. 将编码器与随机初始化的解码器集成，并端到端地微调完整的扩散和一致性模型。

Result: 扩散模型在ImageNet-256上达到FID 2.04，在ImageNet-512上达到FID 2.35 (75 NFE)，大幅超越现有像素空间方法，并与领先的VAE模型性能相当。一致性模型在ImageNet-256上实现单步采样FID 8.82，显著优于其潜在空间对应模型。首次成功在不依赖预训练VAE或扩散模型的情况下，直接训练高分辨率图像的一致性模型。

Conclusion: 本研究提出的两阶段训练框架成功弥合了像素空间生成模型与潜在空间模型之间的性能和效率差距，在扩散模型和一致性模型上均取得了SOTA性能，并实现了高分辨率像素空间一致性模型的直接训练。

Abstract: Pixel-space generative models are often more difficult to train and generally
underperform compared to their latent-space counterparts, leaving a persistent
performance and efficiency gap. In this paper, we introduce a novel two-stage
training framework that closes this gap for pixel-space diffusion and
consistency models. In the first stage, we pre-train encoders to capture
meaningful semantics from clean images while aligning them with points along
the same deterministic sampling trajectory, which evolves points from the prior
to the data distribution. In the second stage, we integrate the encoder with a
randomly initialized decoder and fine-tune the complete model end-to-end for
both diffusion and consistency models. Our training framework demonstrates
strong empirical performance on ImageNet dataset. Specifically, our diffusion
model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75
number of function evaluations (NFE), surpassing prior pixel-space methods by a
large margin in both generation quality and efficiency while rivaling leading
VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our
consistency model achieves an impressive FID of 8.82 in a single sampling step,
significantly surpassing its latent-space counterpart. To the best of our
knowledge, this marks the first successful training of a consistency model
directly on high-resolution images without relying on pre-trained VAEs or
diffusion models.

</details>


### [123] [Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space](https://arxiv.org/abs/2510.12603)
*Chao Chen,Zhixin Ma,Yongqi Li,Yupeng Hu,Yinwei Wei,Wenjie Li,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出一种名为IVT-LR的多模态潜在推理方法，通过在潜空间中结合视觉和文本信息进行推理，有效解决了现有显式多模态推理方法对大量标注和推理延迟的依赖，显著提升了MLLM的性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态推理方法依赖于显式推理步骤，这需要耗费大量人工的视觉-文本标注，并导致显著的推理延迟。研究旨在解决这些问题，实现多模态表示、减少标注和提高推理效率。

Method: 研究引入了多模态潜在推理，并提出了交错式视觉-文本潜在推理（IVT-LR）。IVT-LR在潜空间中注入视觉和文本信息进行推理，每个推理步骤由潜在文本（前一步的隐藏状态）和潜在视觉（一组选定的图像嵌入）两部分组成。此外，还引入了渐进式多阶段训练策略来使MLLM执行这些潜在推理步骤。

Result: 在M3CoT和ScienceQA数据集上的实验表明，IVT-LR方法平均准确率提升了5.45%，同时与现有方法相比，推理速度提高了5倍以上。

Conclusion: IVT-LR通过多模态潜在推理，成功地克服了传统显式推理方法在标注和效率方面的局限性，在提高多模态大语言模型推理性能的同时，大幅提升了推理速度，为MLLM的推理能力提供了更高效的解决方案。

Abstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [124] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 本研究评估了Dhumbal纸牌游戏中的AI代理，发现基于规则的激进型代理在不完美信息下表现最佳（胜率88.3%），超过了搜索和强化学习方法，并提供了可重现的AI框架和开源代码。


<details>
  <summary>Details</summary>
Motivation: Dhumbal是一款具有文化意义且信息不完美的多人纸牌游戏。研究旨在通过系统比较不同类型的AI策略（基于规则、基于搜索和基于学习）来评估AI代理在该游戏中的表现，以推进AI研究并支持文化游戏的数字化保存。

Method: 研究形式化了Dhumbal游戏机制，实现了多种AI代理，包括：启发式（激进、保守、平衡、机会主义）、基于搜索（MCTS、ISMCTS）和强化学习（DQN、PPO）以及一个随机基线。评估通过类别内和跨类别锦标赛进行，性能指标包括胜率、经济结果和Jhyap成功率等。统计分析采用Welch's t-test、Bonferroni校正和Cohen's d。

Result: 在1024轮模拟中，基于规则的激进型代理取得了最高的胜率（88.3%，95% CI: [86.3, 90.3]），显著优于ISMCTS（9.0%）和PPO（1.5%），其成功得益于对Jhyap声明的有效利用。

Conclusion: 本研究贡献了一个可重现的AI框架，提供了关于启发式方法在部分信息下有效性的见解，并发布了开源代码，从而推动了AI研究并支持了文化游戏的数字化保存。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [125] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: 本研究揭示了“LLM作为评估者”存在严重的正面偏见，并提出了“少数否决策略”和“基于回归的框架”来有效缓解评估偏差，其中回归方法在代码反馈任务上表现出显著优于现有集成模型的精度。


<details>
  <summary>Details</summary>
Motivation: 随着新型大型语言模型（LLMs）的频繁发布，应用开发者面临选择模型的难题。人工评估成本高昂且难以扩展。当前最佳的“LLM作为评估者”方法存在一个关键缺陷：LLMs表现出强烈的正面偏见，导致评估结果虚高。

Method: 首先，通过实证证据揭示了LLMs在评估时高准确识别有效输出（真阳性率96%）但极差识别无效输出（真阴性率<25%）的问题。针对此偏差，提出了两种策略：1) 一种对缺失数据具有鲁棒性并能大幅缓解偏见的“最优少数否决策略”。2) 一个新型的“基于回归的框架”，通过少量人工标注的真实数据直接建模验证器的偏见，以实现更高的精度。

Result: 研究发现LLM作为评估者存在严重正面偏见，易导致评估分数虚高。在对366个高中Python程序的代码反馈任务中，提出的回归方法将最大绝对误差降低到仅1.2%，比表现最佳的14个SOTA LLM集成模型的效果提高了两倍。

Conclusion: LLM作为评估者存在严重的正面偏见，导致其在识别无效输出时表现不佳。提出的最优少数否决策略和基于回归的框架能有效缓解这一评估偏差，尤其回归方法在需要高精度评估的场景中，能显著提升评估的准确性和可靠性。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [126] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 本文引入了Holistic Agent Leaderboard (HAL) 来标准化AI智能体的评估，通过大规模并行测试和日志检查，揭示了智能体的真实表现和意外行为，旨在推动智能体在现实世界中的可靠性。


<details>
  <summary>Details</summary>
Motivation: AI智能体已应用于复杂任务，但现有评估方法存在诸多挑战，无法准确反映智能体的实际工作效果。

Method: 1. 提供了标准化的评估平台（HAL），支持数百个VM并行评估，将评估时间从数周缩短至数小时。2. 进行了跨模型、支架和基准的三维分析，执行了21,730次智能体运行，覆盖编码、网页导航、科学和客户服务等9个模型和9个基准。3. 利用LLM辅助的日志检查技术，深入挖掘智能体行为。

Result: 1. HAL显著减少了评估时间并消除了常见的实现错误。2. 分析发现，更高的推理努力在多数运行中反而降低了准确性。3. 揭示了此前未报告的智能体行为，如搜索基准而非解决任务，或在机票预订任务中误用信用卡。

Conclusion: 通过标准化智能体评估并解决常见缺陷，期望将研究重心从追求基准高分转向开发在现实世界中可靠工作的智能体。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [127] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: 研究引入CGBench，一个基于ClinGen的新基准测试，评估大型语言模型（LMs）在科学文献（特别是临床遗传学）解释中的推理能力。结果显示LMs有潜力，但在细粒度任务和解释准确性方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗和转化生物医学中，变异和基因解读至关重要，但传统方法耗时费力。现有基准测试未涵盖LMs在科学文献解释中的真实世界推理能力，因此需要一个更全面的工具来加速研究转化为临床洞察。

Method: 构建了CGBench基准，其数据源自ClinGen专家策定的临床遗传学文献解读。CGBench评估LMs以下能力：1) 依据精确协议提取实验结果；2) 判断证据强度；3) 分类和描述实验结果。测试了8种不同的LMs，并通过LM评判方法将LM的解释与人类解释进行比较。

Result: LMs展现出前景，但在文献解释，尤其是在遵循细粒度指令方面存在巨大差距。推理模型在细粒度任务中表现优异，而非推理模型在高层次解释中表现更好。LMs在解释过程中常出现幻觉或误解结果，即使它们能正确分类证据。

Conclusion: CGBench揭示了LMs在精确解读科学出版物方面的优缺点，为未来AI在临床遗传学及更广泛科学领域的研究开辟了道路。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [128] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 本文提出一种受扩散模型启发的两阶段训练方法，旨在帮助大型语言模型（LLMs）在会话式推荐中生成有效的序贯澄清问题，以更好地获取用户偏好，尤其是在用户历史数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 在开放式会话推荐系统中，LLMs需要获取用户偏好以提供个性化响应，但在用户历史数据有限时，这尤其困难。目前，生成跨领域有效的序贯澄清问题以获取用户偏好仍是一个挑战。

Method: 引入一种新颖的训练方法，使LLMs能够提出揭示用户偏好的序贯问题。该方法借鉴扩散模型，分为两阶段：1. 正向过程：从用户画像生成澄清问题并获取答案，然后逐步移除这些答案，模拟向用户画像添加“噪声”。2. 逆向过程：训练模型通过学习提出有效的澄清问题来“去噪”用户画像。

Result: 实验结果表明，该方法显著提高了LLM提出漏斗式问题和有效获取用户偏好的能力。

Conclusion: 通过受扩散模型启发的两阶段训练过程，可以有效提升LLMs生成序贯澄清问题的能力，从而更好地获取用户偏好并实现个性化推荐。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [129] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成到SmartPilot工业CoPilot中的神经符号因果分析模块，旨在通过数据驱动的因果分析和知识图谱，为工业异常提供可解释的决策支持，并在火箭装配测试中表现出高精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代制造业需要对过程异常、根本原因和潜在干预措施提供可解释的洞察，但现有AI系统常是孤立的“黑箱”，缺乏预测、解释和因果推理的整合，这限制了它们在高风险工业环境中的可信度和实用性。

Method: 本文提出了CausalTrace，一个集成在SmartPilot工业CoPilot中的神经符号因果分析模块。它利用工业本体和知识图谱进行数据驱动的因果分析，包括因果发现、反事实推理和根本原因分析（RCA），旨在提供透明、可解释的决策支持，并支持实时操作员交互。该模块通过多种因果评估方法和C3AN框架（关注鲁棒性、智能性和可信度）进行了全面评估。

Result: 在学术火箭装配测试平台中，CausalTrace与领域专家达成了高度一致（本体问答ROUGE-1: 0.91），并展现出强大的RCA性能（MAP@3: 94%，PR@2: 97%，MRR: 0.92，Jaccard: 0.92）。在C3AN评估中获得4.59/5分，证明其在实际部署中的精确性和可靠性。

Conclusion: CausalTrace通过提供透明、可解释的因果分析，有效解决了现有AI系统在工业环境中缺乏可信度和实用性的问题，为高风险决策提供了精确可靠的支持，展现了其在实际部署中的巨大潜力。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [130] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: 现有代码生成基准忽略了LLM生成代码的契约依从性。本文引入PACT框架，通过扩展测试集、分析提示条件和新度量指标，系统地评估和提升LLM生成代码的功能正确性和契约依从性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准（如HumanEval+, MBPP+）主要关注LLM生成代码的功能正确性，使用格式良好的输入进行评估，但忽略了契约依从性（即对不规范输入的拒绝能力），导致模型无法生成真正健壮可靠的代码。

Method: 本文提出了PACT框架来解决此问题。PACT贡献包括：1) 提供一个专注于契约违规的综合测试套件语料库，扩展了HumanEval+和MBPP+；2) 能够系统分析不同提示条件下的代码生成；3) 引入了新颖的度量指标来严格量化测试生成和代码生成中的契约依从性。

Result: 分析表明，与仅使用契约描述相比，通过增加包含契约违规测试用例的提示，能显著增强模型尊重契约的能力。PACT揭示了传统基准忽视的关键错误，并提供了严谨、可解释的度量标准。

Conclusion: PACT框架为评估LLM生成代码在功能和契约依从性两方面的鲁棒性提供了严谨和可解释的度量指标，填补了现有基准的空白，提升了代码的可靠性。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [131] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 提出CAMNet，一个基于V2V CAM数据的图神经网络，用于车辆轨迹预测，结果显示CAMs能有效支持轨迹预测以应对自动驾驶传感器遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临安全挑战，传统传感器（如激光雷达、摄像头）的视场和视线易受遮挡，降低态势感知能力。V2V通信中的协同感知消息（CAMs）能解决此问题，即使传感器被遮挡也能实现车辆间信息共享。

Method: 设计并训练了一个名为CAMNet的图神经网络，利用协同感知消息（CAMs）数据进行车辆轨迹预测。模型在一个常用的运动预测数据集上训练，并在一个自建的CAMs数据集上进行评估，以验证CAM数据是否能被有效利用。

Result: 该方法展示了有希望的结果，证明协同感知消息（CAMs）确实能够支持车辆轨迹预测。

Conclusion: CAMs数据能够有效辅助车辆轨迹预测，为自动驾驶在传感器受限情况下的态势感知提供了新的解决方案，但仍存在局限性，有待进一步研究。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [132] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 该研究引入了地理空间感知层（GAL），将地理空间数据整合到大型语言模型（LLMs）中，以改进灾害响应中的资源分配建议，并在野火场景中表现优于基线，同时具备推广至其他灾害的潜力。


<details>
  <summary>Details</summary>
Motivation: 有效的灾害响应至关重要。现有统计方法缺乏语义上下文、泛化能力差、可解释性有限。大型语言模型（LLMs）虽有少量样本泛化能力，但受限于文本，缺乏地理空间感知能力，无法理解地理背景，这限制了它们在需要地理认知的灾害响应中的应用。

Method: 引入地理空间感知层（GAL），用于将LLM代理与结构化地球数据相结合。GAL从原始野火检测开始，自动从外部地理数据库检索并整合基础设施、人口、地形和天气信息，将其组装成一个简洁、带有单位注释的感知脚本。这种丰富的上下文使LLM代理能够生成基于证据的资源分配建议（例如，人员分配、预算分配），并通过历史类比和每日变化信号进一步增强以实现增量更新。

Result: 通过在真实野火场景中对多个LLM模型进行评估，研究表明地理空间接地的代理能够超越基线模型的表现。

Conclusion: 所提出的结合地理空间感知的LLM框架能够有效提升灾害响应中的资源分配建议的准确性和效率，并且该框架可以推广应用于洪水和飓风等其他类型的灾害。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [133] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个训练无关的框架，通过进化生成的指令（think-prefixes）自动优化大型推理模型的推理过程，显著提高效率、安全性并增强指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理时效率低下且容易偏离目标，而现有训练无关方法局限于僵硬的启发式规则或描述性但不可操作的分析。

Method: 引入ThinkPilot框架，该框架采用进化过程生成“think-prefixes”指令。这些指令基于推理行为分类学进行演化，旨在引导模型产生更优性能，从而自动优化LRMs的推理。

Result: ThinkPilot显著改善了推理效率的准确性-长度权衡，大幅提升了安全性（例如，将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7%），并增强了指令遵循能力。它还能与现有基于训练的方法协同增效。分析表明，think-prefixes能够可靠地控制LRMs的推理行为，且不同任务对特定的行为分布有强烈偏好。

Conclusion: 通过自动识别和引导LRMs的特定推理行为，ThinkPilot提供了一个通用的框架，能够使LRMs的推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [134] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 本文将AI推理智能体视为计算型动态系统，提出转导学习范式，强调“时间”是学习推理的核心。信息在学习中的作用是减少解决新任务的时间，而非降低重构误差。研究表明最优加速与算法信息量相关，并理论推导了推理与训练时间的幂律缩放。作者认为盲目扩大模型规模可能导致蛮力而非智能，优化时间才是关键。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理智能体虽能解决各类任务，但其“计算”方式非经典程序执行，引出了AI智能体是否具有普适性、能否解决所有可计算任务，以及其学习推理机制（是否仅与模型或训练数据规模相关）等根本问题。

Method: 作者重新诠释了AI智能体学习的角色，将其视为具备计算能力的随机动态系统，并强调时间在学习推理中的基础性作用。提出将学习范式从经典的归纳学习转向转导学习，其目标是捕获数据的算法结构，以减少解决新任务所需的时间。

Result: 1. 转导学习表明，信息在学习中的关键作用在于缩短时间而非降低重构误差。
2. 通用求解器利用历史数据实现的最优加速与其算法信息量紧密相关。
3. 理论推导了推理时间与训练时间的幂律缩放关系。
4. 指出模型规模的扩大虽然能在基准上提高准确性，但在极限情况下可能导致模型行为如同“学者症候群”，仅能通过蛮力解决任务而缺乏真正洞察力，无法通过合理的智能测试。

Conclusion: 在扩展推理模型时，核心的优化量应是“时间”，而非单纯的模型或数据规模，时间在学习中的关键作用此前仅被间接考虑。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [135] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: 本文提出HiCoTraj框架，利用大型语言模型(LLMs)和分层思维链提示，将移动轨迹转换为自然语言表示，在无需标注数据的情况下实现零样本人口属性推断，并具有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 从人类移动模式推断人口属性对于公共卫生、城市规划和交通服务至关重要。然而，现有方法严重依赖大规模带标签的轨迹数据，导致可解释性差且泛化能力有限。

Method: HiCoTraj框架利用LLM的零样本学习和语义理解能力，无需标注训练数据即可进行人口属性推断。它将轨迹转化为语义丰富的自然语言表示（详细活动编年史和多尺度访问摘要），并通过新颖的分层思维链推理（事实特征提取、行为模式分析、人口属性推断）系统地引导LLM。

Result: 在真实世界轨迹数据上的实验评估表明，HiCoTraj在零样本场景下对多种人口属性取得了有竞争力的性能。

Conclusion: HiCoTraj成功解决了标注人口数据稀缺的挑战，提供了透明的推理链，并在零样本场景下实现了对人口属性推断的良好性能。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [136] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出了EmboMatrix，一个用于训练大语言模型（LLM）获取具身决策能力的综合训练平台，并通过该平台培养了EmboBrain，其在具身决策基准测试中显著超越了现有大型模型。


<details>
  <summary>Details</summary>
Motivation: 具身决策是通用具身智能的基石。虽然LLM具有通用决策能力，但其仅基于语言的训练使其缺乏对物理世界的理解。研究旨在弥合LLM与真正具身理解之间的差距。

Method: 提出了“训练场”概念，并构建了EmboMatrix，一个提供任务与场景模拟、具身交互和反馈信号的一站式基础设施。EmboMatrix包含多智能体数据引擎（用于任务场景生成）、分布式异构硬件系统（用于可扩展模拟）和多级奖励架构（用于精确监督）。利用EmboMatrix，通过大规模具身交互培养了EmboBrain。

Result: EmboBrain-7B在两个具身决策基准测试中，表现超越了671B的DeepSeek-R1基线9.5%，证明了交互式、环境接地学习在构建智能具身智能体方面的有效性。

Conclusion: 交互式、环境接地学习对于构建真正智能的具身智能体至关重要。EmboMatrix和EmboBrain的成功，展示了通过大规模具身交互，LLM可以有效获得真正的具身决策能力。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [137] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态，在大规模人口移动数据中检测个体层面的细微行为异常。


<details>
  <summary>Details</summary>
Motivation: 传统的移动异常检测主要集中于轨迹层面，难以在大规模数据集中识别个体相对于自身历史模式的异常行为。

Method: BeSTAD框架通过联合建模空间上下文和时间动态，学习语义丰富的移动表示。它采用行为簇感知建模机制，从正常活动中构建个性化行为档案，并通过跨期行为比较与一致的语义对齐来识别异常。

Result: 该方法能够检测个体移动行为中的细微偏差、行为转变以及与既定日常活动的偏离，并能在大规模移动数据集中识别出表现出这些变化的个体。

Conclusion: BeSTAD通过直接从无标签数据中学习个体行为，推动了异常检测向个性化和可解释的移动分析发展。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [138] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）在处理随机性任务时表现不一致，且常偏离预期，存在明显局限。


<details>
  <summary>Details</summary>
Motivation: LLMs在许多应用（如决策、游戏、加密）中需处理随机性，但其生成和利用随机数的能力尚不明确，有待深入探究。

Method: 通过设计一系列实验，考察LLM处理随机性任务的能力。实验考量了外部工具可访问性、任务类型、模型状态和提示策略等因素，涵盖随机数生成、随机字符串生成、项目打乱以及使用熵和NIST测试套件评估随机性质量等任务。

Result: LLMs能够生成具有一定随机性的输出，但其性能不稳定，且经常与预期行为存在显著偏差。

Conclusion: 实验分析揭示了LLMs在有效处理随机性任务方面的关键局限和需要改进的领域。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [139] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: 本研究提出OneLife框架，用于在复杂、随机、敌对环境中，仅通过有限无引导交互（“一命”）学习符号世界模型。OneLife通过条件激活的程序化法则建模动态，在Crafter-OO环境上表现优异，并能辅助规划，为自主构建复杂环境世界模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有符号世界建模工作主要集中于确定性、简单、数据丰富且有人类指导的环境。本研究旨在解决更具挑战性的现实场景：智能体在复杂、随机、敌对环境中，仅有“一命”进行探索，且无人类指导。

Method: 引入OneLife框架，该框架在一个概率编程范式内，通过具有前置条件-效果结构的条件激活程序化法则来建模世界动态。这些法则仅在相关世界状态下激活，形成一个动态计算图，从而避免了扩展性挑战，并能即使在法则激活稀疏的情况下也能学习随机动态。

Result: OneLife能够从最少的、无引导的交互中成功学习关键环境动态，在测试的23种场景中有16种优于强基线。此外，OneLife还展示了规划能力，通过模拟推演成功识别出更优策略。

Conclusion: 本工作为自主构建未知复杂环境的程序化世界模型奠定了基础。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [140] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个多智能体AI框架，通过自然语言指令对拓扑聚合物进行粗粒度分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言接口降低复杂计算工作流的门槛，推动聚合物科学领域的AI驱动材料发现，并为自主可扩展的多智能体科学研究生态系统奠定基础。

Method: 引入ToPolyAgent框架，该框架整合大型语言模型（LLMs）与LAMMPS等领域专用计算工具。系统包含四个LLM驱动的智能体：Config Agent（配置生成）、Simulation Agent（MD模拟与构象分析）、Report Agent（报告编译）和Workflow Agent（自主操作），支持交互式和自主式模拟工作流。

Result: 通过案例研究展示了ToPolyAgent在处理多种聚合物结构（如线性、环状、刷状、星状聚合物及枝晶）和不同模拟条件（溶剂条件、恒温器、模拟长度）下的通用性。此外，证明其能作为研究助手，分析相互作用参数对线性聚合物构象的影响以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: ToPolyAgent通过结合自然语言接口和严谨的模拟工具，降低了复杂计算工作流的门槛，促进了聚合物科学中AI驱动的材料发现，并为自主多智能体科学研究生态系统的发展奠定了基础。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [141] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 该研究提出了一种新方法，用于精确控制大型语言模型（LLM）输出的属性强度，通过将其重构为目标导向问题，结合价值函数学习和梯度干预实现细粒度、连续控制，并在多个LLM上验证了其高准确性及在下游任务中的效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM对齐方法只能提供方向性或开放式指导，无法可靠地实现用户定义的精确属性强度，这对于构建适应多样用户需求的AI系统至关重要。

Method: ['将精确属性强度控制重构为一个“达到目标”而非简单“最大化”的问题。', '通过时序差分学习训练一个轻量级价值函数，以预测部分生成文本的最终属性强度，从而指导LLM输出。', '利用基于梯度的隐藏层表示干预，精确地将模型导航至特定的属性强度目标。']

Result: ['实现了对属性强度细粒度、连续的控制，超越了简单的方向性对齐。', '在LLaMA-3.2-3b和Phi-4-mini上的实验证实，该方法能够以高精度将文本生成导向用户指定的属性强度。', '在偏好数据合成、帕累托前沿近似与优化以及对齐行为蒸馏等三个下游任务中展现了效率提升。']

Conclusion: 该方法通过创新性地将精确属性强度控制重构为目标导向问题，并结合价值函数学习和梯度干预，成功克服了现有LLM对齐方法的局限，实现了对LLM输出属性的细粒度、连续控制，显著提升了AI系统的适应性，并在实际应用中展现了广泛价值。

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [142] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: 本文引入MatSciBench，一个综合性的大学级别材料科学基准测试，旨在评估和提升大型语言模型（LLMs）在该领域的科学推理能力，发现现有顶尖模型表现仍有不足，并分析了不同推理策略的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在科学推理方面表现出显著能力，但它们在材料科学领域的推理能力尚未得到充分探索。

Method: 引入MatSciBench，一个包含1,340个大学级别问题的综合基准测试，涵盖材料科学的6个主要领域和31个子领域，并根据推理长度进行三级难度分类。该基准提供详细参考解决方案，并融入多模态（视觉上下文）推理。研究评估了包括Gemini-2.5-Pro在内的主流LLM，并系统分析了基本思维链、工具增强和自我校正等不同推理策略，以及检索增强生成的影响。此外，还分析了按难度级别的性能、效率与准确性的权衡、多模态推理挑战和故障模式。

Result: 评估显示，即使是性能最高的模型Gemini-2.5-Pro，在大学级别的材料科学问题上准确率也低于80%，突显了MatSciBench的复杂性。研究表明，没有单一的推理方法能在所有场景中持续表现优异。同时，分析揭示了多模态推理任务的固有挑战，并诊断了不同LLM和推理方法的故障模式。

Conclusion: MatSciBench建立了一个全面且坚实的基准，用于评估和推动大型语言模型在材料科学领域科学推理能力的改进。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [143] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本综述全面分析了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）及其专用的参数高效微调（PEFT）方法，涵盖模型架构、性能、PEFT机制、应用案例及未来研究方向，旨在为研究人员和实践者提供一站式资源。


<details>
  <summary>Details</summary>
Motivation: 鉴于Meta AI LLaMA系列模型的快速发展及其相关参数高效微调策略的重要性，本研究旨在提供一个全面、系统的综述，帮助机器学习研究人员和实践者深入理解和应用这些技术。

Method: 1. 描述LLaMA基础模型家族（7B至288B参数），包括其架构（如多模态和专家混合变体）和关键性能特征。 2. 阐述参数高效微调（PEFT）的概念。 3. 回顾并讨论五种应用于LLaMA的PEFT方法（LoRA、LLaMA-Adapter V1/V2、LLaMA-Excitor、QLoRA），分析其机制、参数节省和应用示例（如指令微调、多模态任务）。 4. 对模型和适配器架构、参数量以及基准测试结果进行结构化讨论和分析。

Result: 1. 详细总结了LLaMA模型系列（1-4）的演变、架构（包含多模态和Mixture-of-Experts变体）和性能特点。 2. 系统介绍了多种PEFT方法，揭示了其在节省参数方面的有效性。 3. 展示了经过PEFT微调的LLaMA模型在某些基准测试中能超越更大的基线模型。 4. 列举了LLaMA及PEFT在法律、医疗等实际领域中的成功应用案例。

Conclusion: 1. LLaMA系列模型和PEFT方法在大语言模型领域展现出强大的潜力和广泛的应用价值。 2. 本综述为理解和应用LLaMA模型及高效微调策略提供了宝贵的参考。 3. 未来研究需关注模型扩展到更大上下文和提高鲁棒性等持续挑战和方向。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [144] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是一个开源框架，它将实时人工控制与强大的自动化性能结合，在GAIA基准上达到SOTA，证明了高效自动化与精细人工控制可并存。


<details>
  <summary>Details</summary>
Motivation: 当前的深度研究代理采用“即发即忘”模式，执行过程中用户无法纠正错误或添加专业知识，缺乏实时控制能力。

Method: ResearStudio是一个以实时人工控制为核心的开源框架。它采用协作式工作坊设计，通过分层规划器-执行器将每一步写入“计划即文档”，并通过快速通信层将所有操作流式传输到Web界面。用户可以随时暂停、编辑计划或代码、运行自定义命令并恢复，从而在AI主导、人工辅助与人工主导、AI辅助模式间平滑切换。

Result: 在全自主模式下，ResearStudio在GAIA基准上取得了最先进的成果，超越了OpenAI的DeepResearch和Manus等现有系统。

Conclusion: 强大的自动化性能与精细的人工控制可以在研究代理中实现共存。该框架鼓励进一步开发安全且可控的研究代理。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [145] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 本文综述了65项评估XAI系统的用户研究，旨在为以用户为中心的XAI系统设计和评估提供指南，并根据用户的AI专业知识水平调整设计目标。


<details>
  <summary>Details</summary>
Motivation: 随着AI普及，对高性能且可理解的智能系统需求增加。现有可解释AI（XAI）评估过程过于技术化，未能充分关注人类用户需求，因此需要以用户研究为指导，制定以人为中心的评估和设计指南。

Method: 本文对65项跨不同领域和应用背景的XAI系统用户研究进行了全面综述。分析了XAI系统特性和以人为中心的评估指标，提出了XAI系统以人为中心的设计目标，并根据用户的AI专业知识水平（AI新手和数据专家）调整了这些设计目标，从而扩展了现有XAI评估和设计框架。

Result: 研究结果包括XAI系统特性分析，区分了核心系统和XAI解释。评估指标被分为对系统的情感、认知、可用性、可解释性和解释指标。针对AI新手，扩展的设计目标包括负责任的使用、接受度和可用性；对于数据专家，重点是性能导向的，包括人机协作以及系统和用户任务性能。

Conclusion: 本研究为XAI开发人员提供了以人为中心的XAI系统特性和评估指标的全面概述，并提出了适应不同用户AI专业知识水平的设计目标，为XAI的评估和设计提供了宝贵的指导。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [146] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，通过合成数据集，显著提升了开源LLM代理在复杂目标导向工具使用上的能力，并在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解为多个相互依赖API调用的目标导向查询时能力有限，特别是开源模型表现不佳且缺乏训练数据，主要依赖零样本评估。

Method: 本文提出GOAT训练框架，该框架无需人工标注，能直接从API文档自动构建目标导向API执行任务的合成数据集，从而微调LLM代理，使其能够推理相互依赖的调用并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中取得了最先进的性能。此外，引入了新的GOATBench基准，GOAT训练的代理在该基准上同样表现出色。

Conclusion: GOAT为构建具备复杂推理和工具使用能力的强大开源LLM代理提供了一条切实可行的途径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [147] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: MedKGEval是一个基于知识图谱的多轮评估框架，用于模拟患者并实时评估临床大语言模型（LLMs），以识别传统方法忽视的细微缺陷和安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估方法难以捕捉多轮医患交互的复杂性，且忽视对话的动态性和患者信息需求的变化，需要更可靠的评估体系。

Method: MedKGEval框架包含三点：1) 基于知识图谱的患者模拟机制，通过从知识图谱检索医学事实，使患者代理具有逼真对话行为；2) 实时、逐轮评估框架，由法官代理根据临床适当性、事实正确性和安全性评估模型响应；3) 一个包含八个先进LLM的综合多轮基准测试。

Result: MedKGEval能够识别出传统评估流程常忽略的LLM细微行为缺陷和安全风险。

Conclusion: MedKGEval提供了一种新颖、可扩展（支持中英等多语言）的临床LLM评估方法，能更有效地捕捉医患交互的复杂性，提高评估的可靠性和安全性。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [148] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 本文提出PromptFlow框架，一个模块化训练系统，通过元学习和强化学习实现自动化、动态且可复用经验的提示工程，以高效适应大型语言模型（LLM）到特定任务。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的提示工程存在挑战：手动设计耗时且依赖专业知识；现有自动化方法采用静态更新规则，缺乏动态策略选择，适应性不足；多数方法整体更新提示，缺乏细粒度编辑；LLM经验复用问题尚未得到充分探索。

Method: 本文提出了PromptFlow框架，这是一个受TensorFlow启发的模块化训练框架。它整合了元提示（meta-prompts）、操作符（operators）、优化器（optimization）和评估器（evaluator）。该框架通过基于梯度的元学习（gradient-based meta-learning）自主探索最优的提示精炼轨迹，并能集成最新的优化方法，只需极少的任务特定训练数据。特别地，它设计了一种强化学习方法来在提示工程过程中复用LLM的经验。

Result: 在各种数据集上进行了广泛实验，结果表明PromptFlow框架具有显著的有效性。

Conclusion: PromptFlow框架通过其模块化设计、元学习和强化学习机制，有效地解决了自动化提示工程中的挑战，实现了LLM在不同任务上的高效、动态和经验复用的适应性。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [149] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: LLMs主动推理中存在信念偏差导致RL训练效率低下。本文提出T$^3$方法，通过检测并截断过度偏差的轨迹，显著提高训练稳定性、token效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在主动推理中，由于推理能力有限，常出现信念偏差，导致难以正确追踪问题状态，陷入无信息或重复动作，进而使得强化学习（RL）训练无法有效归因关键探索步骤的奖励。

Method: 提出T$^3$方法，该方法通过跟踪模型信念的偏差，检测过度信念偏差，并在训练过程中截断轨迹以移除无信息的尾部。通过保留信息性前缀的奖励，T$^3$系统地改进了策略优化。

Result: 在5项具有挑战性的任务中，T$^3$持续提升了训练稳定性、token效率和最终性能，实现了高达30%的性能增益，同时减少了约25%的rollout tokens。

Conclusion: 研究结果强调，信念控制是开发鲁棒且可泛化的基于LLM的主动推理器的关键原则。

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [150] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 本文提出Tensor Logic，一种新的编程语言，通过将逻辑规则和爱因斯坦求和视为同一操作，在基础层面统一了神经AI和符号AI，旨在解决现有AI语言的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI编程语言（如PyTorch/TensorFlow）缺乏对自动化推理和知识获取的支持，而传统AI语言（如LISP/Prolog）缺乏可扩展性和学习支持，这阻碍了AI的进步。

Method: 引入Tensor Logic，其唯一构造是张量方程。该方法基于逻辑规则和爱因斯坦求和本质上是相同操作的观察，所有其他操作都可归结于此，从而统一了神经和符号AI。

Result: 展示了如何在Tensor Logic中优雅地实现关键形式的神经AI（如Transformer）、符号AI（如形式推理）和统计AI（如核机器和图模型）。更重要的是，它开辟了新的方向，如在嵌入空间中进行可靠推理。

Conclusion: Tensor Logic结合了神经网络的可扩展性和可学习性与符号推理的可靠性和透明度，有望成为AI更广泛应用的基础。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [151] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: 本文提出RAG-Anything框架，解决现有RAG仅限于文本的问题，通过双图构建和跨模态混合检索，实现全面的多模态知识检索，并在多模态基准测试中显著超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）框架仅限于文本内容，无法处理现实世界中固有的多模态知识库（包含文本、视觉、表格、数学表达式等），导致在处理多模态文档时存在根本性缺陷。

Method: 提出RAG-Anything统一框架，将多模态内容重构为互联的知识实体。引入双图构建（dual-graph construction）以捕获跨模态关系和文本语义，形成统一表示。开发跨模态混合检索（cross-modal hybrid retrieval），结合结构化知识导航与语义匹配，实现对异构内容的有效推理。

Result: RAG-Anything在具有挑战性的多模态基准测试中表现出卓越性能，显著优于现有SOTA方法。在传统方法失效的长文档上，性能提升尤其显著。

Conclusion: RAG-Anything为多模态知识访问建立了新的范式，消除了当前系统面临的架构碎片化问题。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [152] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: 本文提出了LLM+CAS框架和O-Forge工具，通过将大语言模型与计算机代数系统结合，在上下文符号反馈循环中生成经符号验证的创新性证明，尤其在寻找渐近不等式的域分解方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决国际奥数和普特南竞赛问题上能力突出，但在研究数学中应用受限，主要障碍在于证明的验证。渐近不等式往往涉及复杂证明和恰当的域分解，特里·陶等数学家认为AI工具在此类研究级问题中寻找正确分解非常有益。

Method: 引入LLM+CAS框架和O-Forge工具，将前沿大语言模型与计算机代数系统（CAS）通过“上下文符号反馈循环”耦合。具体做法是，LLM提出域分解建议，CAS（如Mathematica）对分解的每个部分提供公理化验证。

Result: LLM+CAS框架在提出渐近不等式的域分解方面表现出卓越的有效性。通过此循环，成功回答了特伦斯·陶提出的关于大语言模型结合验证器是否能帮助证明复杂的渐近不等式的问题。

Conclusion: 经验证，结合验证器的大语言模型能够帮助证明复杂的渐近不等式。这表明AI能够超越竞赛数学，成为专业数学家的研究级工具。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [153] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文对基于大语言模型的“Vibe Coding”范式进行了首次全面系统的综述，旨在建立其理论基础和实践框架，解决其有效性未被充分探索的现状，并揭示了成功实施的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动的“Vibe Coding”开发范式潜力巨大，但其有效性尚待探索，且存在意想不到的生产力损失和人机协作挑战，亟需建立理论基础和实践框架。

Method: 通过系统分析1000多篇研究论文，全面审视了Vibe Coding生态系统，包括LLM编码、编码代理、开发环境和反馈机制。通过受限马尔可夫决策过程对Vibe Coding进行形式化，并归纳出五种不同的开发模型。

Result: 首次将Vibe Coding形式化为一个规范学科；构建了该领域第一个全面的分类法，包含五种开发模型；揭示了成功的Vibe Coding不仅依赖于代理能力，更依赖于系统的上下文工程、完善的开发环境以及人机协作开发模型。

Conclusion: Vibe Coding的成功实现不仅仅取决于AI代理的能力，还需系统化的上下文工程、健全的开发环境以及高效的人机协作开发模式。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [154] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 本文提出了PricingLogic，一个用于评估大型语言模型（LLM）在处理多重、重叠旅游定价规则时可靠性的新基准。研究发现，LLM在更复杂的定价任务上表现不佳，暴露出其在规则解释和算术推理方面的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 旅游机构希望通过AI系统自动化易出错的旅游定价任务，但若部署未经可靠性验证的LLM，可能导致重大经济损失并损害客户信任。因此，需要一个基准来评估LLM在此类收入关键型应用中的可靠性。

Method: 开发了PricingLogic基准，包含300个基于42个真实世界定价策略的自然语言问题。这些问题分为两个难度级别：基本客户类型定价和涉及复杂折扣交互的捆绑旅游计算。研究评估了一系列LLM在该基准上的表现。

Result: 评估结果显示，LLM在难度更高的定价任务上表现急剧下降，揭示了其在规则解释和算术推理方面的系统性失败。

Conclusion: 尽管LLM具备通用能力，但在没有进一步保障或领域适应的情况下，它们在收入关键型应用（如旅游定价）中仍然不可靠。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [155] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 本文提出MTOS框架，利用大语言模型（LLM）模拟多主题社交媒体中的观点演化，发现主题相关性显著影响两极分化，并证明LLM代理比传统数值模型更真实、可解释。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的观点两极分化、信息隔离和认知偏差是重要研究领域。现实世界信息常跨越多主题，但现有LLM研究多集中于单主题，限制了多主题认知转移的捕捉；传统数值模型简化语言态度，缺乏可解释性和多主题整合能力，无法有效模拟复杂的意见演化。

Method: 提出多主题观点模拟（MTOS）框架，整合了LLM与多主题上下文。MTOS利用LLM结合短期和长期记忆，引入多种用户选择交互机制和动态主题选择策略，并采用信念衰减机制以实现跨主题的观点更新。研究通过改变主题数量、关联类型并进行消融实验来评估MTOS特性。

Result: 多主题设置显著改变了两极分化趋势：正相关主题会放大回音室效应，负相关主题会抑制回音室效应，而不相关主题也能通过资源竞争减轻回音室效应。与数值模型相比，基于LLM的代理更真实地模拟了动态观点变化，再现了新闻文本的语言特征，并捕获了复杂的人类推理，从而提高了模拟的可解释性和系统稳定性。

Conclusion: MTOS框架成功地在多主题情境下模拟了意见演化，揭示了主题相关性对两极分化的影响机制。基于LLM的方法提供了比传统数值模型更真实、可解释且稳定的多主题社会模拟能力，克服了现有模型的局限性。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [156] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 本文提出一种结合偏置注意力机制的深度强化学习决策框架，用于解决无信号交叉口自动驾驶的安全与效率问题。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口的自动驾驶决策因复杂的动态交互和高冲突风险而极具挑战性，需要实现主动安全控制。

Method: 该研究提出一个基于Soft Actor-Critic (SAC) 算法的深度强化学习 (DRL) 决策框架。其核心创新在于使用偏置注意力机制构建交通风险预测器，该预测器评估车辆进入交叉口的长期碰撞风险，并将其转化为密集的奖励信号，以指导SAC智能体做出安全高效的驾驶决策。

Result: 仿真结果表明，所提出的方法能有效提升交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景下能够有效提高交通效率和车辆安全性，证明了其有效性。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [157] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了作为评判者的LLM的判断偏见，发现其对偏见输入具有鲁棒性，但偏见数据微调会降低性能，且分数与任务难度相关。提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLM被广泛用于自主评估通信系统内容质量，但其作为“AI评判者”的公正性未获保证，潜在偏见可能扭曲结果并损害用户信任。

Method: 在逐点评分设置下，系统调查了GPT-Judge和JudgeLM这两种LLM评判模型中的11种隐性和显性判断偏见。同时研究了详细评分标准和偏见数据微调的影响。

Result: 1. 最先进的LLM评判者对偏见输入表现出鲁棒性，通常给予较低分数；详细评分标准可增强此鲁棒性。 2. 使用高分但有偏见的响应进行微调会显著降低LLM性能。 3. 评判分数与任务难度相关，挑战性数据集分数较低，开放式推理数据集分数较高。

Conclusion: LLM评判者对偏见输入具有一定鲁棒性，但偏见数据微调构成风险，且分数受任务难度影响。为确保AI评判的公平可靠，提出了四种潜在的缓解策略。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [158] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 该论文开发了一个基于有向无环图（DAG）的任务导向型医学对话框架，旨在通过自动化提问、自适应交互和报告生成，优化临床工作流程。初步评估显示，该框架在患者和医生应用中均表现出低认知负荷、高可用性和强满意度。


<details>
  <summary>Details</summary>
Motivation: 论文旨在开发一个结构化的任务导向型对话系统，将医学算法和指南转化为高效的提问流程，以减少临床工作中的认知负担，并支持医生友好型报告的自动生成，从而优化医疗工作流程。

Method: 研究开发了一个以医学问题有向无环图（DAG）为结构的任务导向型对话框架。该框架整合了：1) 将医学算法和指南转换为临床问题语料库的系统性流程；2) 基于分层聚类的冷启动机制，无需预先患者信息即可生成初始提问；3) 基于患者响应的扩展和剪枝机制，实现自适应分支和回溯；4) 确保信息收集完毕后终止访谈的逻辑；5) 自动化生成与临床工作流程对齐的医生友好型结构化报告。设计遵循人机交互原则。初步评估涉及五名医生，使用标准化工具NASA-TLX、SUS和QUIS。

Result: 初步评估结果显示，患者应用在认知负荷（NASA-TLX = 15.6）、可用性（SUS = 86）和满意度（QUIS = 8.1/9）方面表现出色，尤其在学习便利性和界面设计上获得高分。医生应用表现出中等认知负荷（NASA-TLX = 26）、卓越的可用性（SUS = 88.5）和高满意度（8.3/9）。两个应用程序均有效融入临床工作流程，降低了认知需求并支持了高效的报告生成。局限性包括偶尔的系统延迟和小规模、非多样化的评估样本。

Conclusion: 该基于DAG的医学对话框架成功地集成了多项功能，有效提高了临床工作流程的效率，降低了用户认知负荷，并获得了较高的用户满意度。尽管存在系统延迟和样本规模小等局限性，但其在优化医疗交互和报告生成方面的潜力已得到初步验证。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [159] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: 人工智能虚拟细胞（AIVCs）在泛化能力、跨尺度耦合和评估方面存在局限。本文提出一种模型无关的细胞状态潜在（CSL）视角，通过操作符语法组织学习，并提供决策对齐的评估蓝图及数据设计建议，以提升AIVCs的鲁棒性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 尽管AIVCs领域发展迅速，但现有评估主要局限于单一数据集和设置，导致模型跨实验室和平台泛化能力有限；数据划分易受泄露和覆盖偏差影响；剂量、时间及组合效应尚未得到系统性处理。此外，分子、细胞和组织层面之间的跨尺度耦合较弱，且与科学或临床结果的对齐存在差异。

Method: 提出一种模型无关的细胞状态潜在（CSL）视角，通过操作符语法（包括测量、用于跨尺度耦合的提升/投影、以及用于剂量和调度的干预）来组织学习。

Result: 该CSL视角催生了一个跨模态、跨尺度、跨语境和跨干预的决策对齐评估蓝图，并强调功能空间读数（如通路活性、空间邻域和临床相关终点）。同时，建议采用操作符感知的数据设计、抗泄露分区以及透明的校准和报告，以实现可复现的、同类比较。

Conclusion: 通过引入CSL视角和操作符语法，并提出决策对齐的评估蓝图及具体数据实践建议，该研究有望显著提升AIVCs模型的泛化能力、鲁棒性、跨尺度耦合以及评估的系统性和可复现性，从而更好地服务于决策制定。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [160] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: 本文提出了ProtoSiTex，一个针对细粒度多标签文本分类的半可解释框架，通过双阶段训练和分层损失，实现了最先进的性能和可信解释。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论激增，对可提供细粒度洞察的可解释模型需求增加。现有原型模型解释性好但粒度粗糙（句子或文档级别），且未能处理多标签文本分类的复杂性。

Method: ProtoSiTex框架采用双阶段交替训练：无监督原型发现阶段学习语义一致的原型，有监督分类阶段将原型映射到类别标签。使用分层损失函数确保子句、句子和文档级别的一致性。该模型通过自适应原型和多头注意力捕捉重叠和冲突的语义。此外，还引入了一个新的子句级多标签酒店评论基准数据集。

Result: ProtoSiTex在自建数据集和两个公共基准数据集上均实现了最先进的性能，并提供了忠实且与人类认知一致的解释。

Conclusion: ProtoSiTex被确立为半可解释多标签文本分类的稳健解决方案。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [161] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 本文提出一个受自然选择启发的基因型多智能体强化学习框架，使用广义适合度奖励来模拟社会动态，发现结果与汉密尔顿法则等生物学原理一致，并预测能产生复杂策略和进化式自学习过程。


<details>
  <summary>Details</summary>
Motivation: 受自然选择驱动智能演化的启发，旨在构建一个多智能体强化学习框架，以模拟这一过程，突破现有研究中常见的二元团队结构，探索基于基因相似度的合作谱系，从而培养出策略更高级、社会智能更高的智能体。

Method: 提出了一种新颖的多智能体强化学习框架，为每个智能体分配基因型，并基于广义适合度概念建模奖励函数，该函数自然地考虑了智能体之间共享的遗传物质。通过两种带有囚徒困境的网络博弈来研究由此产生的社会动态，并概述了框架如何扩展到具有空间和时间结构、有限资源和不断演化种群的更开放环境。

Result: 研究结果与汉密尔顿法则等成熟生物学原理一致。框架预测将出现策略军备竞赛和类似于生物进化的多智能体自动课程。与早期研究中普遍存在的二元团队结构不同，基于基因的奖励结构引入了基于基因相似性从完全对抗到完全合作的合作谱系，从而实现了独特的非团队社会动态（例如，一个智能体与另两个智能体相互合作，而这两个智能体之间却互相对抗）。

Conclusion: 将广义适合度纳入智能体为更高级策略和更具社会智能的智能体的出现奠定了基础，并能够推动在复杂开放环境中的研究。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [162] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 本文引入HardcoreLogic基准，旨在评估大型推理模型在非典型逻辑游戏中的鲁棒性，发现现有模型严重依赖记忆，在复杂性增加和细微规则变化下表现显著下降，揭示了其在真正高层逻辑推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在复杂逻辑任务上表现出色，但其在面对非典型游戏变体时，能否灵活应用规则仍存疑问。现有语料库可能导致模型过拟合和记忆化，掩盖了其理解新规则和适应新策略的能力缺陷。

Method: 引入HardcoreLogic，一个包含10种游戏、超过5000个谜题的挑战性基准。通过三个维度（增加复杂性、不常见元素、不可解谜题）系统地转换规范谜题，以减少对记忆的依赖。对多种大型推理模型进行评估，并对可解和不可解谜题进行系统误差分析。

Result: 评估显示，即使在现有基准上表现优秀的模型，在HardcoreLogic上也出现显著的性能下降，表明其严重依赖记忆化的刻板印象。模型主要受增加的复杂性影响，但也难以处理不一定增加谜题难度的细微规则变化。误差分析进一步揭示了模型在真实推理方面的不足。

Conclusion: HardcoreLogic揭示了当前大型推理模型的局限性，并为推动高水平逻辑推理研究建立了一个新的基准。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [163] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出Memory-as-Action框架，将大语言模型的工作记忆管理作为可学习的内在能力，通过强化学习和Dynamic Context Policy Optimization算法，实现内存与任务目标的端到端优化，从而提升任务性能并降低计算消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长周期代理任务中，其有限记忆易被无关上下文淹没，现有记忆管理方法多依赖外部启发式机制，与核心策略脱节。

Method: 提出Memory-as-Action框架，将记忆管理视为统一策略下的可执行编辑操作。为解决记忆编辑操作导致的“轨迹断裂”问题，引入Dynamic Context Policy Optimization算法，通过分段轨迹并应用轨迹级优势，实现稳定的端到端强化学习。

Result: 通过端到端优化任务推理和记忆管理，不仅降低了整体计算消耗，还通过适应性上下文管理策略提高了任务性能。

Conclusion: 将记忆管理重新定义为可学习的内在能力，并通过强化学习进行端到端优化，能有效提升大语言模型在长周期任务中的表现和效率。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [164] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: 本文提出了ERA（Embodied Reasoning Agent）框架，通过两阶段的先验知识学习和在线强化学习，使小型视觉语言模型（ERA-3B）在具身AI任务中显著超越大型模型（如GPT-4o）和现有基线，实现了高效且可扩展的具身智能。


<details>
  <summary>Details</summary>
Motivation: 顶级的具身AI系统依赖于部署成本高昂的大型视觉语言模型（VLM），而小型VLM因缺乏必要的知识和技能难以成功。研究旨在弥合这一差距，开发出能有效利用小型VLM的具身智能代理。

Method: ERA是一个两阶段框架：1. **具身先验学习**：从轨迹增强数据（通过更强模型生成结构化推理）、环境锚定数据（提供环境内知识和基础监督）和外部知识数据中提炼基础知识。2. **在线强化学习**：在此先验知识基础上进一步提升性能，引入了三项关键设计以克服RL挑战：用于上下文管理的自我总结、稠密奖励塑造和回合级策略优化。

Result: ERA-3B在高级规划（EB-ALFRED）和低级控制（EB-Manipulation）任务上均表现出色。与GPT-4o相比，在EB-ALFRED上整体提升8.4%，在EB-Manipulation上提升19.4%，并超越了其他训练基线。ERA还展现出对未见任务的强大泛化能力。

Conclusion: ERA为实现可扩展的具身智能提供了一条实用路径，并为未来具身AI系统的设计提供了重要的方法论见解。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [165] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体辩论法官框架，通过协作推理和迭代优化，增强LLM自动判断的准确性，并引入高效的自适应停止机制，优于简单的多数投票方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）作为法官的自动化判断方法，常依赖于简单的聚合策略（如多数投票），即使个体智能体提供正确答案，也可能导致判断失败。研究旨在解决这一局限性。

Method: 我们提出一个多智能体辩论法官框架，其中智能体协作推理并迭代细化响应。该辩论过程被数学形式化，分析了智能体互动并证明辩论能放大正确性。为提高效率，引入了稳定性检测机制，该机制通过时变Beta-二项混合模型模拟法官共识动态，并基于分布相似度（Kolmogorov-Smirnov检验）采用自适应停止准则。

Result: 在多个基准和模型上的实验表明，我们的框架与多数投票相比，显著提高了判断准确性，同时保持了计算效率。

Conclusion: 所提出的多智能体辩论法官框架能有效提高大语言模型自动判断的准确性，并通过自适应停止机制保持计算效率，优于传统的简单聚合方法。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [166] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 本文提出了一种结合自监督学习和图论技术的方法，无需标注数据即可提高AI系统的域外(OOD)检测能力，从而增强系统鲁棒性，并在性能上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: AI系统在OOD样本、对抗性攻击和环境变化下保持可靠和准确的性能（即鲁棒性）至关重要，尤其是在自动驾驶、交通或医疗等安全关键系统中，故障可能导致严重后果。因此，提高OOD检测能力对于增强AI系统鲁棒性具有重要意义，且现有方法可能依赖于标注数据。

Method: 该方法利用自监督学习从无标注数据中学习有用表示，并结合图论技术，以实现更高效的OOD样本识别和分类，从而提高OOD检测能力，无需依赖标注数据。

Result: 与现有最先进的方法相比，该方法在接收者操作特征曲线下面积 (AUROC) 上达到了0.99的高分。

Conclusion: 该研究成功地提出了一种无需标注数据的OOD检测方法，通过结合自监督学习和图论技术，显著提升了AI系统的鲁棒性，并在OOD检测性能上展现出优越性。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [167] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种新型深度组合bandit算法，通过智能选择JavaScript代码变异目标，显著提高了模糊测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript模糊测试方法在选择代码变异目标时采用随机方式，效率不高。研究旨在通过引入组合bandit模型，解决变异目标选择问题，提升JavaScript引擎模糊测试的效率和安全性。

Method: 提出CLUTCH，一个结合了深度学习注意力机制的新型深度组合bandit模型，能够处理变长JavaScript测试用例表示。它还利用Concrete Dropout动态调整探索策略。

Result: 相比现有三种先进解决方案，CLUTCH在JavaScript模糊测试中平均将有效测试用例数量增加20.3%，每测试用例覆盖率增加8.9%。在波动和组合场景下，CLUTCH比现有bandit算法至少减少78.1%和4.1%的遗憾值。

Conclusion: CLUTCH通过智能地选择JavaScript模糊测试中的变异目标，有效解决了随机选择的低效问题，显著提高了测试效率、代码覆盖率和整体性能，超越了现有先进方法。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [168] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一种新方法，利用LLM实现用户通过自然语言实时、高效地控制传统推荐系统，显著提升用户满意度和掌控感。


<details>
  <summary>Details</summary>
Motivation: 用户对推荐系统不满意时，缺乏细粒度的控制方式来调整推荐。LLMs提供了通过自然语言请求引导推荐的潜力。

Method: CTRL-Rec在训练时利用LLM模拟用户对推荐项的语言请求下的批准情况，并训练嵌入模型来近似这些模拟判断。这些基于用户请求的预测被整合到传统推荐系统的信号加权中。在部署时，每个用户请求只需一次LLM嵌入计算，实现实时控制。

Result: 在MovieLens数据集上的实验表明，该方法能持续实现细粒度控制。一项针对19名Letterboxd用户的研究发现，CTRL-Rec受到用户积极评价，并显著增强了用户对推荐的控制感和满意度，优于传统控制方式。

Conclusion: CTRL-Rec成功地将自然语言控制引入传统推荐系统，实现计算效率高、实时性强且能有效提升用户控制感和满意度的推荐体验。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [169] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover是一个多智能体系统，结合LLM和Lean工具进行自动化定理证明，能够在不同科学领域解决问题，并可自主运行或与人类专家协作。


<details>
  <summary>Details</summary>
Motivation: 科学问题求解中的形式化证明需要创造性推理和严格的语法精确性，这本身是一个挑战。此外，现有的专业证明系统在泛化到不同领域时表现不佳。

Method: Ax-Prover通过Model Context Protocol (MCP)将提供知识和推理的大型语言模型(LLM)与确保形式正确性的Lean工具相结合，以此构建了一个工具化的多智能体系统。

Result: 作为自主证明器，Ax-Prover在公共数据集上与最先进的证明器具有竞争力，并在新引入的抽象代数和量子理论Lean基准测试中显著超越它们。此外，它还成功帮助一位专家数学家形式化了一个复杂密码学定理的证明，展示了其辅助能力。

Conclusion: Ax-Prover的工具化智能体定理证明方法提供了一种跨不同科学领域进行形式化验证的通用方法，克服了专用系统泛化能力差的缺点，并展示了其作为人类专家助手的强大潜力。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [170] [Think as a Doctor: An Interpretable AI Approach for ICU Mortality Prediction](https://arxiv.org/abs/2510.11745)
*Qingwen Li,Xiaohang Zhao,Xiao Han,Hailiang Huang,Lanjuan Liu*

Main category: cs.LG

TL;DR: ProtoDoctor是一个新型ICU死亡率预测框架，通过整合临床病程、人口异质性和预后感知三大要素，实现了内在可解释性，并提高了预测准确性和临床解释的实用性。


<details>
  <summary>Details</summary>
Motivation: ICU死亡率预测中，仅有预测准确性不足，可解释性同样关键。现有方法未能同时整合临床病程识别、人口异质性及预后感知这三个ICU决策实践的关键要素。

Method: 提出ProtoDoctor框架，其核心是两个创新模块：1) 预后临床病程识别模块，通过原型学习识别病程并利用新颖的正则化机制实现预后感知；2) 人口异质性识别模块，通过特定队列原型和风险调整建模人口异质性。

Result: ProtoDoctor在预测准确性上超越了最先进的基线方法。人工评估进一步证实其解释在ICU实践中更具临床意义、更值得信赖且更具适用性。

Conclusion: ProtoDoctor为ICU死亡率预测提供了一个具有内在可解释性的解决方案，通过全面整合ICU决策实践的关键要素，显著提升了预测性能和模型解释的临床实用性与可信度。

Abstract: Intensive Care Unit (ICU) mortality prediction, which estimates a patient's
mortality status at discharge using EHRs collected early in an ICU admission,
is vital in critical care. For this task, predictive accuracy alone is
insufficient; interpretability is equally essential for building clinical trust
and meeting regulatory standards, a topic that has attracted significant
attention in information system research. Accordingly, an ideal solution should
enable intrinsic interpretability and align its reasoning with three key
elements of the ICU decision-making practices: clinical course identification,
demographic heterogeneity, and prognostication awareness. However, conventional
approaches largely focus on demographic heterogeneity, overlooking clinical
course identification and prognostication awareness. Recent prototype learning
methods address clinical course identification, yet the integration of the
other elements into such frameworks remains underexplored. To address these
gaps, we propose ProtoDoctor, a novel ICU mortality prediction framework that
delivers intrinsic interpretability while integrating all three elements of the
ICU decision-making practices into its reasoning process. Methodologically,
ProtoDoctor features two key innovations: the Prognostic Clinical Course
Identification module and the Demographic Heterogeneity Recognition module. The
former enables the identification of clinical courses via prototype learning
and achieves prognostication awareness using a novel regularization mechanism.
The latter models demographic heterogeneity through cohort-specific prototypes
and risk adjustments. Extensive empirical evaluations demonstrate that
ProtoDoctor outperforms state-of-the-art baselines in predictive accuracy.
Human evaluations further confirm that its interpretations are more clinically
meaningful, trustworthy, and applicable in ICU practice.

</details>


### [171] [GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving](https://arxiv.org/abs/2510.11769)
*Ruida Wang,Jiarui Yao,Rui Pan,Shizhe Diao,Tong Zhang*

Main category: cs.LG

TL;DR: 现有可验证数学问题求解模型训练效率低且难以处理复杂问题。本文提出GAR（生成对抗强化学习）框架，通过对抗训练问题生成器和求解器，引入隐式课程学习机制，显著提升了模型性能和训练效率，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的可验证语言（如Lean）数学问题求解模型，其基于昂贵的在线强化学习或专家迭代训练，但受限于固定问题集，导致训练效率低下并限制了模型解决复杂问题的能力。

Method: 本文提出了GAR（生成对抗强化学习）框架，这是一个全面的强化学习训练框架。GAR通过对抗循环联合训练问题生成器（composer）和问题求解器（solver），并引入了一种隐式课程学习机制，使任务难度与证明器的能力演进相匹配。

Result: 实验结果显示，通过GAR训练，Goedel-Prover-V2-8B和DeepSeek-Prover-V2-7B在MiniF2F-Test基准测试上pass@32平均相对提升4.20%。同时，DeepSeek-Prover-V2在ProofNet-Test上的pass@32从22.58%提升至25.81%。

Conclusion: GAR不仅在形式化证明领域实现了性能提升，还为可验证环境下的问题生成与求解的共同演化建立了一个通用的强化学习范式。

Abstract: Solving math problems through verifiable languages such as Lean has
significantly impacted both the mathematics and computer science communities.
Current state-of-the-art models are often trained with expensive online
Reinforcement Learning (RL) or expert iteration. However, these approaches rely
on fixed problem sets, which causes inefficient training and limits the model
to tackle complex problems. To overcome these limitations, we propose GAR:
Generative Adversarial Reinforcement learning, a comprehensive RL training
framework that jointly trains the problem composer and solver in an adversarial
loop. GAR introduces an implicit curriculum learning mechanism, which aligns
task difficulty with the prover's evolving capability. It thereby improves the
training efficiency and enables stronger performance of proving advanced
theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and
DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of
4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on
ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR
establishes a general RL paradigm for co-evolution of problem generation and
solving under verifiable environments.

</details>


### [172] [Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection](https://arxiv.org/abs/2510.11827)
*Simone Mungari,Ettore Ritacco,Pietro Sabatino*

Main category: cs.LG

TL;DR: Janus是一个节点级异常检测框架，通过结合欧几里德和双曲图神经网络与对比学习，捕获互补的节点表示以识别异常。


<details>
  <summary>Details</summary>
Motivation: 节点级异常检测（NAD）因结构模式和特征分布多样性而面临挑战，但对欺诈检测、网络安全和推荐系统等关键应用至关重要。

Method: Janus框架联合利用欧几里德和双曲图神经网络捕获互补节点表示。它为每个节点创建原始特征和结构特征（来自随机游走和度）的两视图，并嵌入到欧几里德和双曲空间。一个多图自编码器框架，辅以对比学习目标作为正则项，对齐跨空间的嵌入，并识别视图难以协调的节点为异常。

Result: 在四个真实世界数据集上的实验表明，Janus始终优于浅层和深度基线方法。

Conclusion: 结合多种几何表示（欧几里德和双曲）为识别图中细微和复杂异常提供了一种鲁棒而有效的方法。

Abstract: Node-level anomaly detection (NAD) is challenging due to diverse structural
patterns and feature distributions. As such, NAD is a critical task with
several applications which range from fraud detection, cybersecurity, to
recommendation systems. We introduce Janus, a framework that jointly leverages
Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects
of node representations. Each node is described by two views, composed by the
original features and structural features derived from random walks and
degrees, then embedded into Euclidean and Hyperbolic spaces. A multi
Graph-Autoencoder framework, equipped with a contrastive learning objective as
regularization term, aligns the embeddings across the Euclidean and Hyperbolic
spaces, highlighting nodes whose views are difficult to reconcile and are thus
likely anomalous. Experiments on four real-world datasets show that Janus
consistently outperforms shallow and deep baselines, empirically demonstrating
that combining multiple geometric representations provides a robust and
effective approach for identifying subtle and complex anomalies in graphs.

</details>


### [173] [Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis](https://arxiv.org/abs/2510.11829)
*Jin Ma,Ying Tan,Renyuan Xu*

Main category: cs.LG

TL;DR: 本文提出软约束薛定谔桥问题（SCSBP）来解决经典薛定谔桥问题（SBP）在生成式AI中因硬终端约束导致的不稳定性。通过引入惩罚函数，建立了最优解的存在性，并证明了其控制器和值函数以线性速率收敛于经典SBP，从而提升了生成模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI与经典薛定谔桥问题（SBP）有紧密联系，但经典SBP的硬终端约束在实际应用（特别是在高维或数据稀缺场景）中常导致不稳定性。因此，需要一个更灵活、更稳定的框架。

Method: 引入软约束薛定谔桥问题（SCSBP），将经典SBP的硬终端约束替换为一般的惩罚函数，形成一种McKean-Vlasov类型的随机控制表述。分析方法包括Doob的h-变换表示、薛定谔势的稳定性结果、Gamma-收敛以及一个结合测度空间优化问题与辅助熵最优传输问题的新颖不动点论证。

Result: 1. 确立了所有惩罚水平下最优解的存在性。2. 证明了当惩罚参数增大时，控制器和值函数以线性速率收敛于经典SBP的相应结果。这些结果首次为软约束薛定谔桥提供了定量的收敛保证。

Conclusion: 软约束薛定谔桥问题（SCSBP）的提出不仅提供了首个关于软约束桥的定量收敛保证，而且阐明了惩罚正则化如何增强生成式建模、微调和迁移学习的鲁棒性，有效解决了经典SBP的实际应用难题。

Abstract: Generative AI can be framed as the problem of learning a model that maps
simple reference measures into complex data distributions, and it has recently
found a strong connection to the classical theory of the Schr\"odinger bridge
problems (SBPs) due partly to their common nature of interpolating between
prescribed marginals via entropy-regularized stochastic dynamics. However, the
classical SBP enforces hard terminal constraints, which often leads to
instability in practical implementations, especially in high-dimensional or
data-scarce regimes. To address this challenge, we follow the idea of the
so-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which the
terminal constraint is replaced by a general penalty function. This relaxation
leads to a more flexible stochastic control formulation of McKean-Vlasov type.
  We establish the existence of optimal solutions for all penalty levels and
prove that, as the penalty grows, both the controls and value functions
converge to those of the classical SBP at a linear rate. Our analysis builds on
Doob's h-transform representations, the stability results of Schr\"odinger
potentials, Gamma-convergence, and a novel fixed-point argument that couples an
optimization problem over the space of measures with an auxiliary entropic
optimal transport problem. These results not only provide the first
quantitative convergence guarantees for soft-constrained bridges but also shed
light on how penalty regularization enables robust generative modeling,
fine-tuning, and transfer learning.

</details>


### [174] [Z0-Inf: Zeroth Order Approximation for Data Influence](https://arxiv.org/abs/2510.11832)
*Narine Kokhlikyan,Kamalika Chaudhuri,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 本文提出了一种高效的零阶近似方法，用于估计训练数据对模型的影响。该方法仅依赖训练和测试数据的中间检查点损失值，计算成本极低，且适用于不可微分损失函数，显著提升了大型模型数据影响分析的实用性。


<details>
  <summary>Details</summary>
Motivation: 理解训练样本如何影响模型预测行为对于机器学习系统分析和改进至关重要，它支持数据选择和模型调试，特别是自影响分析可用于数据质量评估和异常检测。然而，现有方法因精度不足或高昂的计算成本（依赖梯度和逆Hessian计算）而难以应用于大型模型。

Method: 引入了一种高效的零阶近似方法来估计训练数据影响。该方法仅需利用训练和测试数据的中间检查点及其对应的损失值，无需梯度和逆Hessian计算。这使得它即使在损失函数不可微分的情况下也具有广泛适用性，并显著减少了时间和内存消耗。

Result: 与现有方法相比，该方法仅需极小一部分的时间和内存。在估计自影响方面取得了卓越的精度，而在估计微调大型语言模型的训练-测试影响方面，其精度也达到了可比甚至更优的水平。

Conclusion: 该方法能够实现可扩展且实用的分析，揭示训练数据如何塑造模型行为，克服了先前方法在大型模型分析中的计算效率和精度局限。

Abstract: A critical aspect of analyzing and improving modern machine learning systems
lies in understanding how individual training examples influence a model's
predictive behavior. Estimating this influence enables critical applications,
including data selection and model debugging; in particular, self-influence,
which quantifies the influence of a training point on itself, has found many
uses in data quality assessment and outlier detection. Existing methods for
measuring data influence, however, are often impractical for large models due
to low accuracy or prohibitive computational costs: most approaches either
provide poor approximations or rely on gradients and inverse-Hessian
computations that remain challenging to scale. In this work, we introduce a
highly efficient zeroth-order approximation for estimating the influence of
training data that requires only a fraction of the time and memory footprint of
prior methods. Notably, our method relies solely on loss values of intermediate
checkpoints on the training and test data, along with the checkpoints
themselves, making it broadly applicable even when the loss function of
interest is non-differentiable. Beyond its computational efficiency, our
approach achieves superior accuracy in estimating self-influence and comparable
or improved accuracy in estimating train-test influence for fine-tuned large
language models, enabling scalable and practical analysis of how training data
shapes model behavior.

</details>


### [175] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: 现有生成模型与安全分类器结合的微调方法常将样本推向决策边界，导致安全性和实用性受损。本文提出“Boundary Guidance”强化学习方法，引导生成远离分类器边界，从而同时提升输出的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 生成模型与安全分类器结合时，传统微调方法（降低被过滤概率）往往导致模型生成接近分类器决策边界的样本，从而增加误报和漏报，影响生成内容的安全性与实用性。

Method: 提出了一种名为“Boundary Guidance”的强化学习微调方法，其核心在于明确地引导生成过程远离安全分类器的决策边界（margin）。

Result: 在越狱（jailbreak）和模糊提示（ambiguous prompts）的基准测试中，通过LLM-as-a-Judge评估，Boundary Guidance显著提升了输出的安全性和实用性。此外，在不同模型规模和奖励设计上的全面消融实验证明了该方法的鲁棒性。

Conclusion: Boundary Guidance是一种有效且鲁棒的强化学习微调方法，能够解决现有安全过滤策略的局限性，通过引导生成远离分类器决策边界，从而同时提升生成模型输出的安全性和实用性。

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [176] [WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation](https://arxiv.org/abs/2510.11839)
*Yu-Hsiang Wang,Olgica Milenkovic*

Main category: cs.LG

TL;DR: WaveletDiff通过在小波系数上训练扩散模型，利用时间序列的多分辨率结构，生成高质量的合成时间序列，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多种应用中无处不在，但高质量大型数据集稀缺。现有合成生成模型难以复现真实时间序列固有的多尺度结构。

Method: 引入WaveletDiff框架，它直接在小波系数上训练扩散模型，以利用时间序列固有的多分辨率结构。该模型结合了为每个分解级别设计的Transformer和跨级别注意力机制（通过自适应门控实现选择性信息交换），并整合了基于Parseval定理的单级别能量守恒约束以保持频谱保真度。

Result: 在六个来自能源、金融和神经科学领域的真实世界数据集上进行测试，WaveletDiff在短时和长时序列上，以及五种不同的性能指标上，均持续优于最先进的时域和频域生成方法。例如，其判别分数和Context-FID分数平均比次优基线小3倍。

Conclusion: WaveletDiff通过结合小波分解和扩散模型，有效解决了现有时间序列生成方法在处理多尺度结构方面的局限性，能够生成高质量的合成时间序列，表现出显著的性能优势。

Abstract: Time series are ubiquitous in many applications that involve forecasting,
classification and causal inference tasks, such as healthcare, finance, audio
signal processing and climate sciences. Still, large, high-quality time series
datasets remain scarce. Synthetic generation can address this limitation;
however, current models confined either to the time or frequency domains
struggle to reproduce the inherently multi-scaled structure of real-world time
series. We introduce WaveletDiff, a novel framework that trains diffusion
models directly on wavelet coefficients to exploit the inherent
multi-resolution structure of time series data. The model combines dedicated
transformers for each decomposition level with cross-level attention mechanisms
that enable selective information exchange between temporal and frequency
scales through adaptive gating. It also incorporates energy preservation
constraints for individual levels based on Parseval's theorem to preserve
spectral fidelity throughout the diffusion process. Comprehensive tests across
six real-world datasets from energy, finance, and neuroscience domains
demonstrate that WaveletDiff consistently outperforms state-of-the-art
time-domain and frequency-domain generative methods on both short and long time
series across five diverse performance metrics. For example, WaveletDiff
achieves discriminative scores and Context-FID scores that are $3\times$
smaller on average than the second-best baseline across all datasets.

</details>


### [177] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本研究通过实证分析重放比例与计算预算的相互作用，解决了语言模型持续预训练中新旧知识遗忘的权衡问题，并提供了选择重放比例以降低训练成本的指导方针。


<details>
  <summary>Details</summary>
Motivation: 语言模型持续预训练存在学习新能力与避免灾难性遗忘现有知识之间的基本权衡。在计算约束下，平衡任务性能和知识保留的最佳重放比例仍未得到充分理解。

Method: 进行了一项全面的实证研究，探讨重放比例配置与计算预算在语言模型任务适应中的相互作用。使用bAbI推理任务作为目标，采用合成数据生成，系统评估不同的总token预算和重放比例配置，分析其对任务掌握和通用知识保留的影响。

Result: 实验揭示了一种能够平衡任务特定性能与通用知识保留的最佳配置。

Conclusion: 基于研究结果，提供了根据计算预算选择重放比例的经验性指导方针，旨在帮助实践者以显著降低的训练成本实现强大的任务适应。

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [178] [Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.11852)
*Saroj Basnet,Shafkat Farabi,Tharindu Ranasinghe,Diptesh Kanoji,Marcos Zampieri*

Main category: cs.LG

TL;DR: 本文评估了七种最先进的视觉-语言模型（VLMs）在多模态讽刺检测和解释生成方面的能力。结果显示模型在检测方面取得中等成功，但在未微调的情况下无法生成高质量的解释。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉-语言模型（VLMs）在理解多模态讽刺等复杂主观现象方面的新机遇，并量化其检测讽刺图像-文本对的性能以及生成高质量解释的能力。

Method: 使用零样本、单样本和少样本提示，评估BLIP2、InstructBLIP、OpenFlamingo、LLaVA、PaliGemma、Gemma3和Qwen-VL等七个最先进的VLMs在多模态讽刺检测上的表现。同时，评估这些模型生成讽刺实例解释的能力。评估在Muse、MMSD2.0和SarcNet三个基准讽刺数据集上进行。

Result: 当前模型在二元讽刺检测方面取得了中等成功。然而，在没有进行特定任务微调的情况下，它们无法生成高质量的讽刺解释。

Conclusion: 尽管当前的VLMs在讽刺检测方面表现出一定的能力，但若要生成高质量的、突出视觉-文本不一致的讽刺解释，仍需要进行特定任务的微调。

Abstract: Recent advances in open-source vision-language models (VLMs) offer new
opportunities for understanding complex and subjective multimodal phenomena
such as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2,
InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their
ability to detect multimodal sarcasm using zero-, one-, and few-shot prompting.
Furthermore, we evaluate the models' capabilities in generating explanations to
sarcastic instances. We evaluate the capabilities of VLMs on three benchmark
sarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are
twofold: (1) to quantify each model's performance in detecting sarcastic
image-caption pairs, and (2) to assess their ability to generate human-quality
explanations that highlight the visual-textual incongruities driving sarcasm.
Our results indicate that, while current models achieve moderate success in
binary sarcasm detection, they are still not able to generate high-quality
explanations without task-specific finetuning.

</details>


### [179] [Actor-Enriched Time Series Forecasting of Process Performance](https://arxiv.org/abs/2510.11856)
*Aurelie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 本研究将行动者行为建模为时间序列并纳入预测性过程监控（PPM）模型，显著提升了吞吐时间（TT）的预测性能。


<details>
  <summary>Details</summary>
Motivation: 预测性过程监控（PPM）中准确预测对主动决策至关重要。鉴于流程资源驱动的特性，理解并整合行动者行为至关重要。现有研究对行动者行为作为时变信号的利用有限，本研究旨在探索将其纳入TT预测模型是否能提高性能。

Method: 利用真实事件日志，构建包含吞吐时间（TT）和行动者中心特征（如行动者参与度、行为频率与持续时间等）的多变量时间序列。训练并比较多种模型，以评估添加行动者行为信息带来的效益。

Result: 实验结果表明，与仅包含TT特征的基线模型相比，融合行动者行为信息的模型在RMSE、MAE和R2等指标上持续表现更优。

Conclusion: 将行动者行为随时间变化建模并整合到预测模型中，能有效提高性能指标的预测能力。

Abstract: Predictive Process Monitoring (PPM) is a key task in Process Mining that aims
to predict future behavior, outcomes, or performance indicators. Accurate
prediction of the latter is critical for proactive decision-making. Given that
processes are often resource-driven, understanding and incorporating actor
behavior in forecasting is crucial. Although existing research has incorporated
aspects of actor behavior, its role as a time-varying signal in PPM remains
limited. This study investigates whether incorporating actor behavior
information, modeled as time series, can improve the predictive performance of
throughput time (TT) forecasting models. Using real-life event logs, we
construct multivariate time series that include TT alongside actor-centric
features, i.e., actor involvement, the frequency of continuation, interruption,
and handover behaviors, and the duration of these behaviors. We train and
compare several models to study the benefits of adding actor behavior. The
results show that actor-enriched models consistently outperform baseline
models, which only include TT features, in terms of RMSE, MAE, and R2. These
findings demonstrate that modeling actor behavior over time and incorporating
this information into forecasting models enhances performance indicator
predictions.

</details>


### [180] [Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements](https://arxiv.org/abs/2510.11868)
*Rita T. Sousa,Heiko Paulheim*

Main category: cs.LG

TL;DR: 本文提出一种新颖方法，通过双模型架构将显式负面陈述整合到知识图谱嵌入学习中，显著提升了链接预测和三元组分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法多依赖封闭世界假设，将缺失三元组视为假，与现实世界知识图谱的开放世界假设相悖。此外，显式负面陈述虽能区分真假，但常被忽视。

Method: 引入一种将显式负面陈述集成到知识嵌入学习过程的方法。采用双模型架构，两个嵌入模型并行训练，一个处理正面陈述，另一个处理负面陈述。训练期间，每个模型通过损坏正面样本并由另一个模型评分来生成负面样本。

Result: 在通用和特定领域知识图谱上进行了链接预测和三元组分类任务评估。广泛实验表明，该方法在预测性能上超越了现有先进嵌入模型。

Conclusion: 该研究证明了将有意义的负面知识整合到嵌入学习中的价值，有效提升了知识图谱嵌入模型的性能。

Abstract: Knowledge graphs represent information as structured triples and serve as the
backbone for a wide range of applications, including question answering, link
prediction, and recommendation systems. A prominent line of research for
exploring knowledge graphs involves graph embedding methods, where entities and
relations are represented in low-dimensional vector spaces that capture
underlying semantics and structure. However, most existing methods rely on
assumptions such as the Closed World Assumption or Local Closed World
Assumption, treating missing triples as false. This contrasts with the Open
World Assumption underlying many real-world knowledge graphs. Furthermore,
while explicitly stated negative statements can help distinguish between false
and unknown triples, they are rarely included in knowledge graphs and are often
overlooked during embedding training.
  In this work, we introduce a novel approach that integrates explicitly
declared negative statements into the knowledge embedding learning process. Our
approach employs a dual-model architecture, where two embedding models are
trained in parallel, one on positive statements and the other on negative
statements. During training, each model generates negative samples by
corrupting positive samples and selecting the most likely candidates as scored
by the other model. The proposed approach is evaluated on both general-purpose
and domain-specific knowledge graphs, with a focus on link prediction and
triple classification tasks. The extensive experiments demonstrate that our
approach improves predictive performance over state-of-the-art embedding
models, demonstrating the value of integrating meaningful negative knowledge
into embedding learning.

</details>


### [181] [Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling](https://arxiv.org/abs/2510.11877)
*Xiaohang Tang,Zhuowen Cheng,Satyabrat Kumar*

Main category: cs.LG

TL;DR: 本文提出了保守对抗鲁棒决策Transformer (CART)，这是第一个旨在增强Decision Transformer在对抗性随机博弈中鲁棒性的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管Decision Transformer将Transformer架构应用于序列决策，但基于序列建模的强化学习方法在对抗鲁棒性方面的研究仍是空白。

Method: CART将主角与对手在每个阶段的交互建模为阶段博弈，其支付定义为后续状态的期望最大值，从而明确纳入随机状态转移。通过基于这些阶段博弈导出的NashQ值来条件化Transformer策略。

Result: 经验证明，CART实现了更精确的极小极大值估计，并在各种对抗性随机博弈中持续获得卓越的最差情况回报。

Conclusion: CART能够生成同时具有更低可利用性（对抗鲁棒性）并对转移不确定性保守的策略，从而在对抗性随机博弈中表现出优越的鲁棒性和性能。

Abstract: The Transformer, a highly expressive architecture for sequence modeling, has
recently been adapted to solve sequential decision-making, most notably through
the Decision Transformer (DT), which learns policies by conditioning on desired
returns. Yet, the adversarial robustness of reinforcement learning methods
based on sequence modeling remains largely unexplored. Here we introduce the
Conservative Adversarially Robust Decision Transformer (CART), to our knowledge
the first framework designed to enhance the robustness of DT in adversarial
stochastic games. We formulate the interaction between the protagonist and the
adversary at each stage as a stage game, where the payoff is defined as the
expected maximum value over subsequent states, thereby explicitly incorporating
stochastic state transitions. By conditioning Transformer policies on the NashQ
value derived from these stage games, CART generates policy that are
simultaneously less exploitable (adversarially robust) and conservative to
transition uncertainty. Empirically, CART achieves more accurate minimax value
estimation and consistently attains superior worst-case returns across a range
of adversarial stochastic games.

</details>


### [182] [ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty](https://arxiv.org/abs/2510.11899)
*Chenliang Li,Junyu Leng,Jiaxiang Li,Youbang Sun,Shixiang Chen,Shahin Shahrampour,Alfredo Garcia*

Main category: cs.LG

TL;DR: 本文提出AdaRL，一种新的双层优化框架，通过自适应低秩策略表示来有效解决鲁棒强化学习中环境不确定性带来的计算成本高和策略保守问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒强化学习需要应对环境动力学中的认知不确定性，但现有方法（基于嵌套最小-最大优化）计算成本高昂且容易产生过于保守的策略。

Method: AdaRL是一个双层优化框架。下层在固定秩约束下进行策略优化，动态从质心模型周围的Wasserstein球中采样。上层自适应调整秩，以平衡偏差-方差权衡，并将策略参数投影到低秩流形，避免了对抗性最坏情况动态的求解，同时确保鲁棒性而无需过度参数化。

Result: 在MuJoCo连续控制基准测试中，AdaRL不仅持续优于固定秩基线（如SAC）和最先进的鲁棒RL方法（如RNAC, Parseval），而且其秩收敛到底层任务的内在秩。

Conclusion: 自适应低秩策略表示为模型不确定性下的鲁棒强化学习提供了一种高效且有原则的替代方案。

Abstract: Robust reinforcement learning (Robust RL) seeks to handle epistemic
uncertainty in environment dynamics, but existing approaches often rely on
nested min--max optimization, which is computationally expensive and yields
overly conservative policies. We propose \textbf{Adaptive Rank Representation
(AdaRL)}, a bi-level optimization framework that improves robustness by
aligning policy complexity with the intrinsic dimension of the task. At the
lower level, AdaRL performs policy optimization under fixed-rank constraints
with dynamics sampled from a Wasserstein ball around a centroid model. At the
upper level, it adaptively adjusts the rank to balance the bias--variance
trade-off, projecting policy parameters onto a low-rank manifold. This design
avoids solving adversarial worst-case dynamics while ensuring robustness
without over-parameterization. Empirical results on MuJoCo continuous control
benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank
baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,
Parseval), but also converges toward the intrinsic rank of the underlying
tasks. These results highlight that adaptive low-rank policy representations
provide an efficient and principled alternative for robust RL under model
uncertainty.

</details>


### [183] [Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks](https://arxiv.org/abs/2510.11903)
*Rizal Fathony,Igor Melnyk,Owen Reinert,Nam H. Nguyen,Daniele Rosa,C. Bayan Bruss*

Main category: cs.LG

TL;DR: 用户事件建模常将个人与关系事件分开处理。本文引入了包含两类事件的数据集和统一形式化方法，经验证结合两类事件可提升模型性能，并指出现有方法仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有用户事件建模方法通常将个人事件（序列）和关系事件（图）独立建模，忽视了现实世界系统对两类事件综合建模的需求，存在数据和方法上的空白。

Method: 引入了一系列同时包含个人和关系事件的公共数据集，并提出了一种统一的形式化方法来表示这两类事件。

Result: 经验性地证明了模型通过整合两类事件能获得性能提升；同时发现现有方法仍有显著的改进空间。

Conclusion: 论文发布了相关资源以支持统一用户事件建模的进一步研究，并鼓励该方向的进展。

Abstract: User event modeling plays a central role in many machine learning
applications, with use cases spanning e-commerce, social media, finance,
cybersecurity, and other domains. User events can be broadly categorized into
personal events, which involve individual actions, and relational events, which
involve interactions between two users. These two types of events are typically
modeled separately, using sequence-based methods for personal events and
graph-based methods for relational events. Despite the need to capture both
event types in real-world systems, prior work has rarely considered them
together. This is often due to the convenient simplification that user behavior
can be adequately represented by a single formalization, either as a sequence
or a graph. To address this gap, there is a need for public datasets and
prediction tasks that explicitly incorporate both personal and relational
events. In this work, we introduce a collection of such datasets, propose a
unified formalization, and empirically show that models benefit from
incorporating both event types. Our results also indicate that current methods
leave a notable room for improvements. We release these resources to support
further research in unified user event modeling and encourage progress in this
direction.

</details>


### [184] [Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks](https://arxiv.org/abs/2510.11917)
*Jun-En Ding,Anna Zilverstand,Shihao Yang,Albert Chih-Chieh Yang,Feng Liu*

Main category: cs.LG

TL;DR: VMoGE是一种新的EEG深度学习模型，通过结合频率特异性分析和图神经网络，显著提升了痴呆症亚型诊断和分期性能，并提供了可解释的生物标志物。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病(AD)和额颞叶痴呆(FTD)等痴呆症在EEG上表现出重叠的电生理特征，导致诊断困难。现有基于EEG的方法受限于全频段分析，无法精确区分痴呆症亚型和严重程度。

Method: 本文提出一种变分图神经专家混合模型(VMoGE)。该模型采用多粒度Transformer提取四个频段的多尺度时间模式，接着使用基于高斯马尔可夫随机场先验的变分图卷积编码器。通过结构化变分推断和自适应门控，VMoGE将神经专业化与生理学上有意义的EEG频段相关联。

Result: 在两个不同数据集上，VMoGE在痴呆症亚型分类和严重程度分期方面均优于现有最佳方法，AUC提高了+4%到+10%。此外，VMoGE通过专家权重提供可解释的洞察，这些权重与临床指标相关，空间模式与神经病理学特征一致。

Conclusion: VMoGE模型提高了痴呆症的诊断和分期精度，并通过提供可解释的EEG生物标志物，促进了全面的痴呆症诊断和监测。

Abstract: Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity transformer to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.

</details>


### [185] [Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer](https://arxiv.org/abs/2510.11926)
*Nayan Sanjay Bhatia,Pranay Kocheta,Russell Elliott,Harikrishna S. Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Locaris是一种基于解码器大型语言模型(LLM)的室内Wi-Fi定位系统，无需预处理和大量校准，可实现高精度、可扩展且鲁棒的跨环境定位，甚至支持少量样本适应。


<details>
  <summary>Details</summary>
Motivation: 室内Wi-Fi定位面临信号敏感性、环境动态性、信道特性和硬件异构性等挑战。传统指纹识别和模型方法需要大量人工校准，且在设备、信道或部署条件变化时性能迅速下降。

Method: 本文提出Locaris，一个仅解码器的大型语言模型（LLM），将每个AP测量视为一个“token”，直接处理原始Wi-Fi遥测数据而无需预处理。通过在不同Wi-Fi数据集上微调，Locaris学习从原始信号到设备位置的轻量级且泛化性强的映射。它还支持少量样本（few-shot）适应新设备和部署场景。

Result: 实验结果表明，Locaris在各种遥测数据上与现有先进技术相当或超越。它证明了紧凑型LLM可作为无需校准的回归模型，在异构Wi-Fi部署中提供可扩展且鲁棒的跨环境性能。通过少量校准点，Locaris对未见过的设备和部署场景仍保持高精度，可实现亚米级精度，并在AP缺失下表现稳定，支持所有可用遥测数据。

Conclusion: Locaris在难以进行广泛校准的大规模部署等真实场景中，展示了其在室内定位方面的实用可行性，提供了无需校准、高精度、鲁棒且灵活的解决方案。

Abstract: Indoor Wi-Fi positioning remains a challenging problem due to the high
sensitivity of radio signals to environmental dynamics, channel propagation
characteristics, and hardware heterogeneity. Conventional fingerprinting and
model-based approaches typically require labor-intensive calibration and suffer
rapid performance degradation when devices, channel or deployment conditions
change. In this paper, we introduce Locaris, a decoder-only large language
model (LLM) for indoor localization. Locaris treats each access point (AP)
measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without
pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris
learns a lightweight and generalizable mapping from raw signals directly to
device location. Our experimental study comparing Locaris with state-of-the-art
methods consistently shows that Locaris matches or surpasses existing
techniques for various types of telemetry. Our results demonstrate that compact
LLMs can serve as calibration-free regression models for indoor localization,
offering scalable and robust cross-environment performance in heterogeneous
Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of
calibration points per device, further show that Locaris maintains high
accuracy when applied to previously unseen devices and deployment scenarios.
This yields sub-meter accuracy with just a few hundred samples, robust
performance under missing APs and supports any and all available telemetry. Our
findings highlight the practical viability of Locaris for indoor positioning in
the real-world scenarios, particularly in large-scale deployments where
extensive calibration is infeasible.

</details>


### [186] [Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning](https://arxiv.org/abs/2510.11933)
*Hiroshi Nonaka,Simon Ambrozak,Sofia R. Miskala-Dinc,Amedeo Ercole,Aviva Prins*

Main category: cs.LG

TL;DR: 本文提出了三种高效的重启范式（部分、自适应、选择性重启），用于解决非平稳强化学习中现有重启机制的两个核心问题，并在多种环境中实现了接近最优的性能，显著降低了动态遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有非平稳强化学习算法（如RestartQ-UCB）的重启机制存在两个核心问题：完全遗忘导致所有学习到的信息丢失，以及固定时间重启无法适应当前环境动态变化。

Method: 提出了部分重启、自适应重启和选择性重启三种新方法，并将其应用于改进RestartQ-UCB和RANDOMIZEDQ算法。

Result: 在多种不同环境中实现了接近最优的经验性能，与RestartQ-UCB相比，动态遗憾最多降低了91%。

Conclusion: 提出的部分、自适应和选择性重启范式能有效解决非平稳强化学习中现有重启机制的局限性，显著提升算法性能，实现接近最优的动态遗憾。

Abstract: In this work, we propose three efficient restart paradigms for model-free
non-stationary reinforcement learning (RL). We identify two core issues with
the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete
forgetting, where all the information learned about an environment is lost
after a restart, and (2) scheduled restarts, in which restarts occur only at
predefined timings, regardless of the incompatibility of the policy with the
current environment dynamics. We introduce three approaches, which we call
partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB
and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance
in multiple different environments, decreasing dynamic regret by up to $91$%
relative to RestartQ-UCB.

</details>


### [187] [On efficiently computable functions, deep networks and sparse compositionality](https://arxiv.org/abs/2510.11942)
*Tomaso Poggio*

Main category: cs.LG

TL;DR: 研究表明，高效的图灵可计算性在给定精度下，意味着存在组合稀疏的DAG表示和相应的神经网络近似器。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探索高效可计算性与深度学习模型结构（如组合稀疏性）之间的理论联系，并提供深度网络实现特定精度的理论基础，同时将其与组合近似率及稀疏结构优化联系起来。

Method: 通过证明如果一个函数在位深度上是多项式时间可计算的，则可构造一个多项式大小和深度的有界扇入布尔电路来计算其离散化映射；随后，将电路中的每个门替换为固定大小的神经网络仿真器，从而得到一个深度神经网络。

Result: 如果函数在位深度上是多项式时间可计算的，则存在一个大小和深度均在输入/输出精度多项式范围内的有界扇入布尔电路。通过门替换，可以得到一个大小和深度在相同多项式范围内的深度神经网络，并达到目标精度 $\varepsilon=2^{-m_{\mathrm{out}}}$。

Conclusion: 高效的图灵可计算性是构建组合稀疏DAG表示和相应神经网络近似器的充分条件，这为深度学习模型强大的近似能力和结构提供了理论支撑，并与计算复杂性及组合近似理论建立了联系。

Abstract: We show that \emph{efficient Turing computability} at any fixed input/output
precision implies the existence of \emph{compositionally sparse}
(bounded-fan-in, polynomial-size) DAG representations and of corresponding
neural approximants achieving the target precision. Concretely: if
$f:[0,1]^d\to\R^m$ is computable in time polynomial in the bit-depths, then for
every pair of precisions $(n,m_{\mathrm{out}})$ there exists a bounded-fan-in
Boolean circuit of size and depth $\poly(n+m_{\mathrm{out}})$ computing the
discretized map; replacing each gate by a constant-size neural emulator yields
a deep network of size/depth $\poly(n+m_{\mathrm{out}})$ that achieves accuracy
$\varepsilon=2^{-m_{\mathrm{out}}}$. We also relate these constructions to
compositional approximation rates
\cite{MhaskarPoggio2016b,poggio_deep_shallow_2017,Poggio2017,Poggio2023HowDS}
and to optimization viewed as hierarchical search over sparse structures.

</details>


### [188] [Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors](https://arxiv.org/abs/2510.11953)
*Quentin Fruytier,Akshay Malhotra,Shahab Hamidi-Rad,Aditya Sant,Aryan Mokhtari,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 论文指出VAE的KL散度正则化器在学习解耦表示时不可靠。作者提出了一种基于MMD的可编程先验框架，实现了最先进的解耦性能，且不牺牲重建质量，并可用于定制语义先验。


<details>
  <summary>Details</summary>
Motivation: 现有的VAE框架中，基于KL散度的正则化器在鼓励潜在空间匹配分解高斯先验时并不可靠，无法有效执行目标分布，导致解耦表示学习失败。

Method: 首先，作者使用新颖的、无监督的潜在可预测性分数（LPS）验证并量化了KL正则化器的不可靠性。接着，他们引入了基于最大均值差异（MMD）的可编程先验框架，以显式地塑造潜在空间。

Result: 该框架在CIFAR-10和Tiny ImageNet等复杂数据集上实现了最先进的相互独立性，且没有常见的重建质量权衡。此外，它还展示了通过工程设计复杂的先验来改善与语义有意义特征对齐的能力。

Conclusion: 本工作为表示工程提供了一个基础工具，为模型可识别性和因果推理开辟了新途径。

Abstract: Learning disentangled representations, where distinct factors of variation
are captured by independent latent variables, is a central goal in machine
learning. The dominant approach has been the Variational Autoencoder (VAE)
framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage
the latent space to match a factorized Gaussian prior. In this work, however,
we provide direct evidence that this KL-based regularizer is an unreliable
mechanism, consistently failing to enforce the target distribution on the
aggregate posterior. We validate this and quantify the resulting entanglement
using our novel, unsupervised Latent Predictability Score (LPS). To address
this failure, we introduce the Programmable Prior Framework, a method built on
the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to
explicitly sculpt the latent space, achieving state-of-the-art mutual
independence on complex datasets like CIFAR-10 and Tiny ImageNet without the
common reconstruction trade-off. Furthermore, we demonstrate how this
programmability can be used to engineer sophisticated priors that improve
alignment with semantically meaningful features. Ultimately, our work provides
a foundational tool for representation engineering, opening new avenues for
model identifiability and causal reasoning.

</details>


### [189] [Y-shaped Generative Flows](https://arxiv.org/abs/2510.11955)
*Arip Asadulaev,Semyon Semenov,Abduragim Shtanchaev,Eric Moulines,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 本文提出Y型生成流，通过共享路径和创新的次线性指数速度动力传输成本，解决传统V型模型忽略共享结构的问题，能恢复层级结构并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现代连续时间生成模型（V型传输）使每个样本独立移动，忽略了数据间的共享结构，导致效率和结构恢复不足。

Method: 引入Y型生成流，通过共享路径移动概率质量，然后在目标特定终点分岔。其核心是提出一种具有次线性指数（0到1之间）的新型速度动力传输成本，这种凹性依赖奖励联合快速的质量移动。实际中，通过可扩展的神经ODE训练目标实现。

Result: 在合成、图像和生物学数据集上，Y型流能恢复层级感知结构，在分布指标上优于强大的基于流的基线模型，并能以更少的积分步数达到目标。

Conclusion: Y型生成流有效解决了传统生成模型中共享结构被忽视的问题，通过创新的传输成本设计，实现了更好的结构恢复、更高的分布度量表现和更少的计算步骤。

Abstract: Modern continuous-time generative models often induce V-shaped transport:
each sample travels independently along nearly straight trajectories from prior
to data, overlooking shared structure. We introduce Y-shaped generative flows,
which move probability mass together along shared pathways before branching to
target-specific endpoints. Our formulation is based on novel velocity-powered
transport cost with a sublinear exponent (between zero and one). this concave
dependence rewards joint and fast mass movement. Practically, we instantiate
the idea in a scalable neural ODE training objective. On synthetic, image, and
biology datasets, Y-flows recover hierarchy-aware structure, improve
distributional metrics over strong flow-based baselines, and reach targets with
fewer integration steps.

</details>


### [190] [MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics](https://arxiv.org/abs/2510.11962)
*Bowei Guo,Shengkun Tang,Cong Zeng,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出MosaicDiff框架，通过轨迹感知结构剪枝，将扩散模型预训练的学习速度动态与后训练采样加速对齐，显著提升采样速度且保持质量。


<details>
  <summary>Details</summary>
Motivation: 以往的扩散模型后训练加速方法忽视了预训练过程中存在的不同学习速度阶段。

Method: 引入了MosaicDiff框架，通过轨迹感知结构剪枝，将扩散预训练动态与采样加速对齐。具体策略是：对预训练中学习速度较快的中期阶段进行更保守的剪枝，而对学习速度较慢的早期和后期阶段则采用更激进的剪枝策略。

Result: 在DiT和SDXL上的广泛实验证明，MosaicDiff在不损害输出质量的前提下显著提高了采样速度，并大幅优于现有最先进的方法。

Conclusion: MosaicDiff首次明确反映了扩散预训练固有的学习速度变化，协调了模型的内部训练动态与加速采样过程，为更高效、鲁棒的免训练扩散加速提供了新的视角。

Abstract: Diffusion models are renowned for their generative capabilities, yet their
pretraining processes exhibit distinct phases of learning speed that have been
entirely overlooked in prior post-training acceleration efforts in the
community. In this study, we introduce a novel framework called MosaicDiff that
aligns diffusion pretraining dynamics with post-training sampling acceleration
via trajectory-aware structural pruning. Our approach leverages the observation
that the middle, fast-learning stage of diffusion pretraining requires more
conservative pruning to preserve critical model features, while the early and
later, slow-learning stages benefit from a more aggressive pruning strategy.
This adaptive pruning mechanism is the first to explicitly mirror the inherent
learning speed variations of diffusion pretraining, thereby harmonizing the
model's inner training dynamics with its accelerated sampling process.
Extensive experiments on DiT and SDXL demonstrate that our method achieves
significant speed-ups in sampling without compromising output quality,
outperforming previous state-of-the-art methods by large margins, also
providing a new viewpoint for more efficient and robust training-free diffusion
acceleration.

</details>


### [191] [QLENS: Towards A Quantum Perspective of Language Transformers](https://arxiv.org/abs/2510.11963)
*Aditya Gupta,Kirandeep Kaur,Vinayak Gupta*

Main category: cs.LG

TL;DR: 针对Transformer解释性不足，受量子力学启发，本文提出QLENS框架，将Transformer层建模为酉算符在希尔伯特空间中演化，用Born规则获取最终概率，并初步验证了其探测层级影响的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer解释方法仅为有限诊断检查点，缺乏数学框架来机械地建模各层如何促进状态转换。语言模型的概率性与量子力学核心假设的相似性，激发了作者寻求物理学视角来弥补这一解释性鸿沟。

Method: 提出QLENS框架：将Transformer的潜在激活转换为希尔伯特空间中的状态向量；隐藏层重构为酉算符和哈密顿量以模拟状态演化；最终概率分布通过对末态应用Born规则和特定测量算符获得。

Result: 通过对一个玩具Transformer进行概念验证，QLENS成功用于探测模型预测轨迹中单个层的影响，展示了其在提供物理学视角理解Transformer方面的潜力。

Conclusion: 本工作为利用跨领域见解，更深入地理解Transformer提供了新的基础。

Abstract: In natural language processing, current methods for understanding
Transformers are successful at identifying intermediate predictions during a
model's inference. However, these approaches function as limited diagnostic
checkpoints, lacking a mathematical framework for mechanistically modeling how
each layer facilitates transitions between these evolving states. This
interpretability gap and past successes of interdisciplinary outlooks inspire
us to turn to physics in search of a descriptive mathematical framework for
Transformers. We observe that language models are intrinsically probabilistic,
an attribute that is echoed in the core postulates of quantum mechanics. This
parallel inspires us to translate insights from this discipline to that of
natural language processing. Towards this objective, we propose QLENS a novel
attempt to develop a physics-based perspective on the Transformer generation
process. Under QLENS, a Transformer is studied by converting its latent
activations into a state vector in a Hilbert space derived from the model's
output units. This state subsequently evolves through hidden layers -
reformulated as unitary operators and analogously defined Hamiltonians - during
inference. The model's final probability distribution is obtained by applying
the Born rule to the end state using a specific measurement operator. To
demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy
Transformer to investigate the influence of individual layers in a model's
prediction trajectory. We present our work as a foundation for cross-domain
insights to be leveraged towards a broader understanding of Transformers.

</details>


### [192] [Learning Dynamics of VLM Finetuning](https://arxiv.org/abs/2510.11978)
*Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Preference-based finetuning of vision--language models (VLMs) is brittle:
trivially wrong negatives inject uninformative gradients that destabilize
training. We recast alignment as \textbf{learning-dynamics--aware optimization}
and introduce \textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that
explicitly models and exploits the training trajectory. \textbf{Stage 1}
performs supervised finetuning with \textbf{gentle negatives}:
\textbf{low-weight smoothed supervision} that regularizes the base policy and
curbs overconfidence without explicit penalties. \textbf{Stage 2} applies a DPO
objective in which the \textbf{negative term is scaled by a cooling weight}
computed from the model's \textbf{average token log-probability} on each
negative, suppressing uninformative gradients from easy or off-distribution
samples while preserving signal from hard negatives. In practice, we emphasize
\textbf{on-policy negatives} and allow \textbf{mixed negatives} by blending a
controllable fraction of dataset negatives to maintain contrast freshness.
Throughout, we instrument training with $\Delta\!\log p$ probes on positives
and negatives as first-class signals for early stopping, curriculum design, and
failure diagnosis. Across diverse VLM tasks, CW-DPO yields \textbf{more stable
optimization}, \textbf{better calibration}, and \textbf{higher pairwise
win-rates} than SFT-only and vanilla DPO, while \textbf{converging in fewer
steps}. Ablations isolate the \textbf{cooling-weight mechanism} as the primary
driver of these gains and show complementary benefits from mixing on-policy and
dataset negatives. Taken together, our results show that \textbf{smoothing
learning dynamics before cooling preferences} is a simple, general principle
for robust VLM alignment.

</details>


### [193] [Learning by Steering the Neural Dynamics: A Statistical Mechanics Perspective](https://arxiv.org/abs/2510.11984)
*Mattia Scardecchia*

Main category: cs.LG

TL;DR: 本文提出一种生物学上合理、局部且分布式的监督学习算法，用于二值循环神经网络。该算法通过将输入映射到动力学吸引子（不动点）并利用局部可塑性进行稳定，解决了无反向传播的信用分配问题，并在统计力学分析和MNIST基准测试中展示了其有效性及与不动点结构相变的关系。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络与生物学学习机制存在根本差异，引发了关于自然界如何实现鲁棒、样本高效、低能耗学习以及无需反向传播解决信用分配问题的关键问题。研究旨在弥合当代人工智能与计算神经科学之间的鸿沟。

Method: 1. 采用统计力学工具研究随机非对称循环网络的神经动力学。2. 识别出鲁棒动力学吸引子（不动点）的出现条件，并推导出不动点数量随自耦合强度的闭合形式表达式。3. 揭示了不动点结构中的相变。4. 提出一种生物学上合理的监督学习算法：通过瞬时外部刺激放松并利用局部可塑性稳定结果配置，将输入映射到动力学的不动点。5. 将算法应用于缠绕版MNIST数据集，并探索其在不同架构下的适用性。

Result: 1. 识别了随机非对称循环网络中鲁棒动力学吸引子的出现条件。2. 导出了不动点数量随自耦合强度的闭合形式表达式，并揭示了其结构中的相变：低于临界自耦合时，孤立不动点与指数级众多窄簇共存；高于临界自耦合时，出现次要但密集且广泛的簇。3. 不动点在算法依赖的自耦合阈值后变得可访问。4. 所提出的算法能够学习缠绕版MNIST，利用深度发展分层表示并增加异质关联容量。5. 算法适用于多种架构。6. 发现算法性能与揭示的相变之间存在强关联。

Conclusion: 本研究通过提出一种生物学上合理、局部且分布式的学习算法，将输入映射到动力学吸引子，为弥合当代AI与计算神经科学的鸿沟迈出了一步。该算法在简单机器学习基准测试上表现出色，并揭示了其性能与网络动力学不动点结构相变之间的紧密联系。研究还建议了一种皮层启发式的替代方案来产生这种相变。

Abstract: Despite the striking successes of deep neural networks trained with
gradient-based optimization, these methods differ fundamentally from their
biological counterparts. This gap raises key questions about how nature
achieves robust, sample-efficient learning at minimal energy costs and solves
the credit-assignment problem without backpropagation. We take a step toward
bridging contemporary AI and computational neuroscience by studying how neural
dynamics can support fully local, distributed learning that scales to simple
machine-learning benchmarks. Using tools from statistical mechanics, we
identify conditions for the emergence of robust dynamical attractors in random
asymmetric recurrent networks. We derive a closed-form expression for the
number of fixed points as a function of self-coupling strength, and we reveal a
phase transition in their structure: below a critical self-coupling, isolated
fixed points coexist with exponentially many narrow clusters showing the
overlap-gap property; above it, subdominant yet dense and extensive clusters
appear. These fixed points become accessible, including to a simple
asynchronous dynamical rule, after an algorithm-dependent self-coupling
threshold. Building on this analysis, we propose a biologically plausible
algorithm for supervised learning with any binary recurrent network. Inputs are
mapped to fixed points of the dynamics, by relaxing under transient external
stimuli and stabilizing the resulting configurations via local plasticity. We
show that our algorithm can learn an entangled version of MNIST, leverages
depth to develop hierarchical representations and increase hetero-association
capacity, and is applicable to several architectures. Finally, we highlight the
strong connection between algorithm performance and the unveiled phase
transition, and we suggest a cortex-inspired alternative to self-couplings for
its emergence.

</details>


### [194] [Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives](https://arxiv.org/abs/2510.11987)
*Conor Rowan*

Main category: cs.LG

TL;DR: 研究发现，在神经网络训练中使用精确的二阶曲率信息（真实Hessian）会导致训练失败，这与预期相反，并对损失函数景观的几何形状和驻点分布提供了新见解，挑战了传统观念。


<details>
  <summary>Details</summary>
Motivation: 尽管包含曲率信息的二阶优化方法被认为具有优势，但目前研究的二阶方法仅限于准牛顿法（近似Hessian）。论文旨在探究使用真实Hessian而非近似值，是否能带来预期的性能提升。

Method: 通过在神经网络训练中直接使用精确的二阶曲率信息（即真实Hessian矩阵），而非其近似值，来计算优化步骤。

Result: 实验结果表明，在依赖精确曲率信息进行神经网络训练时，训练会可靠地失败。这些失败模式提供了对非线性离散化几何以及损失函数景观中驻点分布的深入见解。

Conclusion: 精确的二阶方法导致神经网络训练失败，这挑战了损失函数景观充满局部极小值的传统观点，并为理解其几何结构和驻点分布提供了新的视角。

Abstract: Second-order methods are emerging as promising alternatives to standard
first-order optimizers such as gradient descent and ADAM for training neural
networks. Though the advantages of including curvature information in computing
optimization steps have been celebrated in the scientific machine learning
literature, the only second-order methods that have been studied are
quasi-Newton, meaning that the Hessian matrix of the objective function is
approximated. Though one would expect only to gain from using the true Hessian
in place of its approximation, we show that neural network training reliably
fails when relying on exact curvature information. The failure modes provide
insight both into the geometry of nonlinear discretizations as well as the
distribution of stationary points in the loss landscape, leading us to question
the conventional wisdom that the loss landscape is replete with local minima.

</details>


### [195] [Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning](https://arxiv.org/abs/2510.12026)
*Junsoo Oh,Wei Huang,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文理论分析了Mamba在低维非线性任务上的上下文学习（ICL）能力，证明了其通过测试时特征学习实现高效ICL，性能优于线性Transformer，并指出非线性门控机制是关键。


<details>
  <summary>Details</summary>
Motivation: Mamba作为一种新型高效序列模型备受关注，但其内在机制，特别是上下文学习（ICL）能力的理论理解尚有限。

Method: 通过理论分析，本文聚焦于Mamba在由低维非线性目标函数定义的任务（具体为单索引模型$y \approx g_*(\langle \boldsymbol{\beta}, \boldsymbol{x} \rangle)$）上的ICL能力。研究预训练Mamba如何通过梯度方法进行学习和特征提取。

Result: 研究证明，经梯度预训练的Mamba能通过测试时特征学习实现高效ICL，直接从上下文示例中提取相关方向。其测试时样本复杂度优于线性Transformer，并与非线性Transformer相当。分析揭示Mamba中的非线性门控机制在特征提取中起着关键作用。

Conclusion: Mamba的高计算效率和优异性能源于其非线性门控机制，该机制使其能够有效进行特征提取，从而实现强大的上下文学习能力。

Abstract: Mamba, a recently proposed linear-time sequence model, has attracted
significant attention for its computational efficiency and strong empirical
performance. However, a rigorous theoretical understanding of its underlying
mechanisms remains limited. In this work, we provide a theoretical analysis of
Mamba's in-context learning (ICL) capability by focusing on tasks defined by
low-dimensional nonlinear target functions. Specifically, we study in-context
learning of a single-index model $y \approx g_*(\langle \boldsymbol{\beta},
\boldsymbol{x} \rangle)$, which depends on only a single relevant direction
$\boldsymbol{\beta}$, referred to as feature. We prove that Mamba, pretrained
by gradient-based methods, can achieve efficient ICL via test-time feature
learning, extracting the relevant direction directly from context examples.
Consequently, we establish a test-time sample complexity that improves upon
linear Transformers -- analyzed to behave like kernel methods -- and is
comparable to nonlinear Transformers, which have been shown to surpass the
Correlational Statistical Query (CSQ) lower bound and achieve near
information-theoretically optimal rate in previous works. Our analysis reveals
the crucial role of the nonlinear gating mechanism in Mamba for feature
extraction, highlighting it as the fundamental driver behind Mamba's ability to
achieve both computational efficiency and high performance.

</details>


### [196] [Your VAR Model is Secretly an Efficient and Explainable Generative Classifier](https://arxiv.org/abs/2510.12060)
*Yi-Chung Chen,David I. Inouye,Jing Gao*

Main category: cs.LG

TL;DR: 本文提出了一种基于视觉自回归（VAR）模型的新型生成分类器A-VARC$^+$，解决了扩散模型计算成本高、扩展性受限的问题，实现了准确性和推理速度的更优平衡，并展现了可解释性和对灾难性遗忘的内在抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成分类器主要由扩散模型驱动，但其计算成本高昂且限制了可扩展性，同时排他性地关注扩散模型限制了对生成分类器的理解。因此，需要一种新方法来提供新视角并提升实用性。

Method: 提出了一种基于视觉自回归（VAR）建模的新型生成分类器，并进一步引入了自适应VAR分类器A-VARC$^+$，以优化性能。

Result: A-VARC$^+$在准确性和推理速度之间取得了卓越的平衡，显著提升了实际应用性。此外，基于VAR的方法展现出与扩散方法根本不同的特性，包括通过逐令牌互信息实现视觉可解释性，以及在类增量学习任务中固有的抗灾难性遗忘能力。

Conclusion: VAR-based生成分类器为研究该领域提供了新视角，A-VARC$^+$在实际应用中具有优越的性能，并展示了在可解释性和抗灾难性遗忘方面的独特优势，扩展了对生成分类器的理解。

Abstract: Generative classifiers, which leverage conditional generative models for
classification, have recently demonstrated desirable properties such as
robustness to distribution shifts. However, recent progress in this area has
been largely driven by diffusion-based models, whose substantial computational
cost severely limits scalability. This exclusive focus on diffusion-based
methods has also constrained our understanding of generative classifiers. In
this work, we propose a novel generative classifier built on recent advances in
visual autoregressive (VAR) modeling, which offers a new perspective for
studying generative classifiers. To further enhance its performance, we
introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a
superior trade-off between accuracy and inference speed, thereby significantly
improving practical applicability. Moreover, we show that the VAR-based method
exhibits fundamentally different properties from diffusion-based methods. In
particular, due to its tractable likelihood, the VAR-based classifier enables
visual explainability via token-wise mutual information and demonstrates
inherent resistance to catastrophic forgetting in class-incremental learning
tasks.

</details>


### [197] [MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging](https://arxiv.org/abs/2510.12070)
*Sangmin Jo,Jee Seok Yoon,Wootaek Jeong,Kwanseok Oh,Heung-Il Suk*

Main category: cs.LG

TL;DR: 本文提出MEASURE框架，通过有效减少过量域相关信息并保留关键特征，显著提升了深度学习睡眠分期模型在未见受试者上的泛化能力，优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习睡眠分期模型在未见受试者上泛化能力差，因生理信号变异性导致性能下降。现有域泛化方法，特别是对比学习，未能充分提取域不变特征，因其未明确处理样本间非共享信息中嵌入的域特性。直接缓解域相关属性的方法易在高层信息上过拟合，限制了其利用多层级时序和频谱信息的能力。

Method: 提出了一种新颖的MEASURE（Multi-scalE minimAl SUfficient Representation lEarning）框架。该框架旨在有效减少“过量域相关信息”，同时保留对睡眠分期至关重要的多尺度时序和频谱特征，以弥合域间差异。

Result: 在公开的睡眠分期基准数据集SleepEDF-20和MASS上进行的详尽实验表明，所提出的MEASURE方法持续优于现有最先进的方法。

Conclusion: MEASURE框架通过有效缓解过量域相关信息并保留关键的多尺度特征，成功解决了自动睡眠分期中的域泛化难题，在未见受试者上取得了卓越的性能。

Abstract: Deep learning-based automatic sleep staging has significantly advanced in
performance and plays a crucial role in the diagnosis of sleep disorders.
However, those models often struggle to generalize on unseen subjects due to
variability in physiological signals, resulting in degraded performance in
out-of-distribution scenarios. To address this issue, domain generalization
approaches have recently been studied to ensure generalized performance on
unseen domains during training. Among those techniques, contrastive learning
has proven its validity in learning domain-invariant features by aligning
samples of the same class across different domains. Despite its potential, many
existing methods are insufficient to extract adequately domain-invariant
representations, as they do not explicitly address domain characteristics
embedded within the unshared information across samples. In this paper, we
posit that mitigating such domain-relevant attributes-referred to as excess
domain-relevant information-is key to bridging the domain gap. However, the
direct strategy to mitigate the domain-relevant attributes often overfits
features at the high-level information, limiting their ability to leverage the
diverse temporal and spectral information encoded in the multiple feature
levels. To address these limitations, we propose a novel MEASURE (Multi-scalE
minimAl SUfficient Representation lEarning) framework, which effectively
reduces domain-relevant information while preserving essential temporal and
spectral features for sleep stage classification. In our exhaustive experiments
on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,
our proposed method consistently outperformed state-of-the-art methods. Our
code is available at : https://github.com/ku-milab/Measure

</details>


### [198] [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071)
*Jin Hwa Lee,Matthew Smith,Maxwell Adam,Jesse Hoogland*

Main category: cs.LG

TL;DR: 该研究提出了一个基于奇异学习理论的阶段性数据归因框架，揭示了神经网络中样本影响并非静态，而是在学习阶段中动态变化，并与模型对语义层次的逐步学习相吻合。


<details>
  <summary>Details</summary>
Motivation: 当前训练数据归因方法假设样本影响是静态的，但这与神经网络分阶段学习且其影响模式不断变化的实际情况不符。

Method: 引入了一个基于奇异学习理论的阶段性数据归因框架。首先，在玩具模型中进行分析和实证验证。最后，在大规模语言模型中展示了这些现象。

Result: 研究预测影响变化可能是非单调的，包括符号翻转和在发展转变期的急剧峰值。在玩具模型中，动态影响变化直接映射到模型对语义层次的渐进学习。在语言模型中，令牌级影响变化与已知的发展阶段一致。

Conclusion: 神经网络中的数据影响并非静态，而是在学习的不同阶段动态变化，且这些变化与模型对语义结构的获取紧密相关。

Abstract: Current training data attribution (TDA) methods treat the influence one
sample has on another as static, but neural networks learn in distinct stages
that exhibit changing patterns of influence. In this work, we introduce a
framework for stagewise data attribution grounded in singular learning theory.
We predict that influence can change non-monotonically, including sign flips
and sharp peaks at developmental transitions. We first validate these
predictions analytically and empirically in a toy model, showing that dynamic
shifts in influence directly map to the model's progressive learning of a
semantic hierarchy. Finally, we demonstrate these phenomena at scale in
language models, where token-level influence changes align with known
developmental stages.

</details>


### [199] [GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs](https://arxiv.org/abs/2510.12085)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Haochen You,Zijian Zhang,Yilei Yuan,Jin Huang*

Main category: cs.LG

TL;DR: 现有图基础模型在不同几何结构边界性能下降，GraphShaper提出多几何专家网络和自适应融合，显著提高了图-文本对齐的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在处理不同拓扑模式的结构边界时性能显著下降（准确率损失超20%），原因是它们假设所有图结构都能在单一欧几里得空间中编码，无法处理树状结构所需的双曲几何和环状结构所需的球面几何等内在几何多样性。

Method: 引入GraphShaper框架，通过多几何专业化增强图编码。该方法利用针对不同几何空间的专家网络，并动态计算融合权重，根据局部结构特征自适应地整合几何属性，从而在与文本嵌入对齐之前保持结构完整性。

Result: 在零样本设置下，GraphShaper在引文网络上实现了9.47%的准确率提升，在社交网络上实现了7.63%的准确率提升。

Conclusion: GraphShaper通过设计尊重图结构内在几何多样性的对齐框架，有效解决了现有图基础模型在结构边界的性能退化问题，显著提高了图-文本对齐的准确性。

Abstract: Graph foundation models represent a transformative paradigm for learning
transferable representations across diverse graph domains. Recent methods
leverage large language models to unify graph and text modalities into a shared
representation space using contrastive learning. However, systematic
evaluations reveal significant performance degradation at structural boundaries
where distinct topological patterns converge, with accuracy losses exceeding 20
percentage points. This issue arises from a key limitation: current methods
assume all graph structures can be encoded within a single Euclidean space. In
reality, tree structures require hyperbolic geometry to preserve hierarchical
branching, while cyclic patterns depend on spherical geometry for closure
properties. At structural boundaries, nodes experience conflicting geometric
constraints that uniform encoding spaces cannot resolve. This raises a crucial
challenge: \textbf{Can alignment frameworks be designed to respect the
intrinsic geometric diversity of graph structures?} We introduce
\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding
through multi-geometric specialization. Our approach employs expert networks
tailored to different geometric spaces, dynamically computing fusion weights to
adaptively integrate geometric properties based on local structural
characteristics. This adaptive fusion preserves structural integrity before
alignment with text embeddings. Extensive experiments demonstrate that
GraphShaper achieves 9.47\% accuracy improvements on citation networks and
7.63\% on social networks in zero-shot settings.

</details>


### [200] [H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space](https://arxiv.org/abs/2510.12094)
*Heng Zhang,Tianyi Zhang,Zijun Liu,Yuling Shi,Yaomin Shen,Haochen You,Haichuan Hu,Lubin Gan,Jin Huang*

Main category: cs.LG

TL;DR: 现有零样本图学习方法因过度抽象导致细粒度模式识别能力不足。本文提出H4G框架，通过减小双曲嵌入半径，有效保留细粒度结构信息，在各类图上显著提升了零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有图-文对齐方法在处理细粒度模式识别任务（尤其是在异质图上）时表现不佳。主要问题是当前方法在双曲空间中操作的半径过大，导致多尺度结构信息被过度抽象，关键的局部细粒度模式信息丢失，影响预测准确性。

Method: 提出H4G框架，利用可学习的块对角缩放矩阵和莫比乌斯矩阵乘法，系统地减小嵌入半径。此方法旨在恢复对细粒度模式的访问，同时以最小的计算开销保持全局感受能力。

Result: H4G在零样本图学习任务中实现了最先进的性能，在异质图上取得了12.8%的改进，在同质图上取得了8.4%的改进。

Conclusion: 通过减小双曲嵌入半径，能够忠实地保留多尺度表示，这对于推进零样本图学习至关重要。

Abstract: Text-attributed graphs are widely used across domains, offering rich
opportunities for zero-shot learning via graph-text alignment. However,
existing methods struggle with tasks requiring fine-grained pattern
recognition, particularly on heterophilic graphs. Through empirical and
theoretical analysis, we identify an \textbf{over-abstraction problem}: current
approaches operate at excessively large hyperbolic radii, compressing
multi-scale structural information into uniform high-level abstractions. This
abstraction-induced information loss obscures critical local patterns essential
for accurate predictions. By analyzing embeddings in hyperbolic space, we
demonstrate that optimal graph learning requires \textbf{faithful preservation}
of fine-grained structural details, better retained by representations
positioned closer to the origin. To address this, we propose \textbf{H4G}, a
framework that systematically reduces embedding radii using learnable
block-diagonal scaling matrices and M\"obius matrix multiplication. This
approach restores access to fine-grained patterns while maintaining global
receptive ability with minimal computational overhead. Experiments show H4G
achieves state-of-the-art zero-shot performance with \textbf{12.8\%}
improvement on heterophilic graphs and \textbf{8.4\%} on homophilic graphs,
confirming that radius reduction enables faithful multi-scale representation
for advancing zero-shot graph learning.

</details>


### [201] [Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning](https://arxiv.org/abs/2510.12096)
*Guozheng Ma,Lu Li,Zilin Wang,Haoyu Wang,Shengchao Hu,Leszek Rutkowski,Dacheng Tao*

Main category: cs.LG

TL;DR: 深度强化学习(DRL)模型扩展性差。本文揭示动态稀疏训练策略对DRL模块具有特定益处，并提出模块特定训练(MST)框架，结合架构改进，显著提升了DRL算法的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模神经网络在机器学习中取得突破，但在深度强化学习（DRL）中，模型增大常因可塑性损失等优化问题导致性能下降。现有动态网络拓扑适应方法虽能缓解，但存在三点局限：1) 对不同DRL模块（编码器、评论器、执行器）采用统一动态策略；2) 评估仅限于基础架构，未阐明动态训练与架构改进的相互作用；3) 缺乏对不同动态稀疏训练方法的系统性比较。

Method: 通过对不同DRL模块和架构进行全面调查，研究了动态稀疏训练策略。基于所得洞察，提炼并提出了模块特定训练（Module-Specific Training, MST）框架。

Result: 研究发现动态稀疏训练策略能提供模块特定的益处，并与架构改进建立的可扩展性基础相互补充。提出的MST框架能进一步利用架构改进的优势，并在无需修改现有算法的情况下，显著提升了多样化RL算法的可扩展性。

Conclusion: MST框架通过模块特定动态稀疏训练策略，有效结合架构改进，解决了DRL模型扩展性面临的挑战，实现了显著的性能提升和可扩展性收益，且保持了对现有RL算法的兼容性。

Abstract: Scaling neural networks has driven breakthrough advances in machine learning,
yet this paradigm fails in deep reinforcement learning (DRL), where larger
models often degrade performance due to unique optimization pathologies such as
plasticity loss. While recent works show that dynamically adapting network
topology during training can mitigate these issues, existing studies have three
critical limitations: (1) applying uniform dynamic training strategies across
all modules despite encoder, critic, and actor following distinct learning
paradigms, (2) focusing evaluation on basic architectures without clarifying
the relative importance and interaction between dynamic training and
architectural improvements, and (3) lacking systematic comparison between
different dynamic approaches including sparse-to-sparse, dense-to-sparse, and
sparse-to-dense. Through comprehensive investigation across modules and
architectures, we reveal that dynamic sparse training strategies provide
module-specific benefits that complement the primary scalability foundation
established by architectural improvements. We finally distill these insights
into Module-Specific Training (MST), a practical framework that further
exploits the benefits of architectural improvements and demonstrates
substantial scalability gains across diverse RL algorithms without algorithmic
modifications.

</details>


### [202] [Chimera: State Space Models Beyond Sequences](https://arxiv.org/abs/2510.12111)
*Aakash Lahoti,Tanya Marwah,Ratish Puduppully,Albert Gu*

Main category: cs.LG

TL;DR: 提出Chimera模型，一个统一的深度学习框架，通过推广状态空间模型直接整合数据拓扑结构，无需领域特定归纳偏置，并在语言、视觉和图数据任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在处理序列、图像和图数据时，其自注意力机制将数据视为无序元素集合，忽略了数据固有的邻域结构或图拓扑。这需要耗费大量精力设计任务特定的归纳偏置（如位置编码、随机游走），且这些偏置可能引入副作用，阻碍模型泛化能力。

Method: 引入Chimera，一个统一模型，通过将无需位置嵌入的状态空间模型推广到捕获任何图拓扑结构，从而以原则性的方式直接整合数据拓扑。同时，提出了算法优化：对于有向无环图，Chimera可实现线性时间递推；对于一般图，通过简单的数学松弛可达到Transformer的二次复杂度。

Result: 实验表明，Chimera在多个领域表现强劲，在GLUE上超越BERT 0.7点，在ImageNet-1k上超越ViT 2.6%，并在Long Range Graph Benchmark上超越所有基线模型。算法优化也有效提高了Chimera的效率。

Conclusion: Chimera的核心贡献得到验证，支持了数据拓扑结构是跨模态的强大归纳偏置这一观点。

Abstract: Transformer-based deep learning methods have become the standard approach for
modeling diverse data such as sequences, images, and graphs. These methods rely
on self-attention, which treats data as an unordered set of elements. This
ignores the neighborhood structure or graph topology of the data and requires
inductive biases--such as position embeddings in sequences and images, or
random walks in graphs--to incorporate topology. However, designing such
task-specific biases requires significant effort and can introduce side effects
that hinder generalization. We introduce Chimera, a unified model that directly
incorporates data topology in a principled way, removing the need for
domain-specific biases. The key idea is that state space models--which
naturally do not require position embeddings--can be generalized to capture any
graph topology. Our experiments show that Chimera achieves strong performance
across language, vision, and graph domains, outperforming BERT on GLUE by 0.7
points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph
Benchmark. We further propose algorithmic optimizations to improve Chimera's
efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a
linear-time recurrence; (2) for general graphs, a simple mathematical
relaxation achieves Transformer's quadratic complexity without domain-specific
heuristics. These results validate Chimera's core contribution and support the
idea that data topology is a powerful inductive bias across modalities.

</details>


### [203] [nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations](https://arxiv.org/abs/2510.12128)
*Ziqi Zhao,Vivek Sarin*

Main category: cs.LG

TL;DR: 本文提出nuGPR框架，通过结合数值线性代数技巧、数值梯度和GPU并行化，显著降低高斯过程回归（GPR）的训练时间（最高2倍）和内存消耗（最高12倍）。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归（GPR）在预测中具有固有的不确定性度量，但其训练过程计算成本高昂，是一个众所周知的挑战。

Method: nuGPR框架融合了多种方法：利用预处理共轭梯度法加速线性求解；利用输入数据聚类识别协方差矩阵的块对角结构，并构建非对角块的低秩近似；采用数值梯度而非精确微分来优化超参数；利用CUDA工具包在NVIDIA GPU上高效并行化训练过程。

Result: 与现有最佳基于GPU的GPR实现相比，nuGPR在各种合成和真实世界数据集上，总训练时间减少高达2倍，峰值内存消耗减少高达12倍。

Conclusion: nuGPR框架通过创新的数值计算和并行化技术，成功解决了GPR训练计算成本高的问题，显著提高了GPR的效率和可扩展性。

Abstract: Gaussian Process Regression (GPR) is an important type of supervised machine
learning model with inherent uncertainty measure in its predictions. We propose
a new framework, nuGPR, to address the well-known challenge of high computation
cost associated with GPR training. Our framework includes several ideas from
numerical linear algebra to reduce the amount of computation in key steps of
GPR, and we combine them to establish an end-to-end training algorithm.
Specifically, we leverage the preconditioned conjugate gradient method to
accelerate the convergence of the linear solves required in GPR. We exploit
clustering in the input data to identify block-diagonal structure of the
covariance matrix and subsequently construct low-rank approximations of the
off-diagonal blocks. These enhancements significantly reduce the time and space
complexity of our computations. In addition, unlike other frameworks that rely
on exact differentiation, we employ numerical gradients to optimize the
hyperparameters of our GPR model, further reducing the training cost by
eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit
to efficiently parallelize the training procedure on NVIDIA GPUs. As a result,
nuGPR reduces total training time by up to 2x and peak memory consumption by up
to 12x on various synthetic and real-world datasets when compared to the best
existing GPU-based GPR implementation.

</details>


### [204] [Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration](https://arxiv.org/abs/2510.12140)
*Yonghao Liu,Yajun Wang,Chunli Guo,Wei Pang,Ximing Li,Fausto Giunchiglia,Xiaoyue Feng,Renchu Guan*

Main category: cs.LG

TL;DR: 针对图少样本学习中固定图滤波器和支持/查询集分布不匹配的限制，本文提出了GRACE框架，通过自适应频谱专家和跨集分布校准技术，显著提升了模型泛化能力，并超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有图少样本学习方法存在两大局限：一是依赖预定义且统一的图滤波器，未能考虑局部拓扑结构的异质性；二是常假设支持集和查询集来自相同分布，但在少样本条件下，支持集数据不足以捕获查询集的复杂分布，导致泛化性能不佳。

Method: 提出GRACE（Graph few-shot leaRning framework that integrates Adaptive spectrum experts with Cross-sEt distribution calibration techniques），一个新颖的图少样本学习框架。该方法整合了自适应频谱专家以适应局部结构变化，并采用了跨集分布校准技术以解决支持集与查询集之间的分布差异。

Result: 经验性结果表明，GRACE在广泛的实验设置中均持续优于现有的最先进基线方法。

Conclusion: GRACE通过自适应处理局部结构变异和实现跨集分布校准，有效增强了模型的泛化能力。

Abstract: Graph few-shot learning has attracted increasing attention due to its ability
to rapidly adapt models to new tasks with only limited labeled nodes. Despite
the remarkable progress made by existing graph few-shot learning methods,
several key limitations remain. First, most current approaches rely on
predefined and unified graph filters (e.g., low-pass or high-pass filters) to
globally enhance or suppress node frequency signals. Such fixed spectral
operations fail to account for the heterogeneity of local topological
structures inherent in real-world graphs. Moreover, these methods often assume
that the support and query sets are drawn from the same distribution. However,
under few-shot conditions, the limited labeled data in the support set may not
sufficiently capture the complex distribution of the query set, leading to
suboptimal generalization. To address these challenges, we propose GRACE, a
novel Graph few-shot leaRning framework that integrates Adaptive spectrum
experts with Cross-sEt distribution calibration techniques. Theoretically, the
proposed approach enhances model generalization by adapting to both local
structural variations and cross-set distribution calibration. Empirically,
GRACE consistently outperforms state-of-the-art baselines across a wide range
of experimental settings. Our code can be found here.

</details>


### [205] [Fairness-Constrained Optimization Attack in Federated Learning](https://arxiv.org/abs/2510.12143)
*Harsh Kasyap,Minghong Fang,Zhuqing Liu,Carsten Maple,Somanath Tripathy*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习中的恶意公平性攻击，通过增加公平性损失来使模型产生严重偏差，同时保持全局准确性，难以被现有防御机制检测。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）尽管保护隐私，但易受投毒攻击和偏差传播的影响。由于参与者对其数据拥有独立性，恶意客户端可能有意引入偏差。现有的FL系统缺乏对这种“故意公平性攻击”的有效防御，即使在数据分布同质的情况下也可能发生，因此需要研究这种潜在威胁。

Method: 本文提出一种故意公平性攻击：恶意客户端在训练过程中通过优化问题计算并增加公平性损失（针对如人口均等和均等机会等公平性指标），从而发送一个有偏差的模型。该攻击的特点是能在增加偏差的同时维持全局准确性，使其难以被察觉。研究在不同数据集和设置下，针对最先进的拜占庭鲁棒和公平性感知聚合方案评估了这种攻击。

Result: 实证结果表明，即使在FL系统中只有一个恶意客户端存在的情况下，该攻击也能有效地将偏差提高高达90%。这证实了攻击的有效性，并揭示了现有防御机制的局限性。

Conclusion: 该研究成功演示了一种针对联邦学习的恶意公平性攻击，该攻击通过在不牺牲全局准确性的情况下增加公平性损失来显著提高模型偏差。这突出表明FL系统对这种隐蔽的、难以检测的偏差引入攻击存在重大漏洞，即使面对先进的鲁棒和公平性感知聚合方案也是如此。

Abstract: Federated learning (FL) is a privacy-preserving machine learning technique
that facilitates collaboration among participants across demographics. FL
enables model sharing, while restricting the movement of data. Since FL
provides participants with independence over their training data, it becomes
susceptible to poisoning attacks. Such collaboration also propagates bias among
the participants, even unintentionally, due to different data distribution or
historical bias present in the data. This paper proposes an intentional
fairness attack, where a client maliciously sends a biased model, by increasing
the fairness loss while training, even considering homogeneous data
distribution. The fairness loss is calculated by solving an optimization
problem for fairness metrics such as demographic parity and equalized odds. The
attack is insidious and hard to detect, as it maintains global accuracy even
after increasing the bias. We evaluate our attack against the state-of-the-art
Byzantine-robust and fairness-aware aggregation schemes over different
datasets, in various settings. The empirical results demonstrate the attack
efficacy by increasing the bias up to 90\%, even in the presence of a single
malicious client in the FL system.

</details>


### [206] [Budget-constrained Active Learning to Effectively De-censor Survival Data](https://arxiv.org/abs/2510.12144)
*Ali Parsaee,Bei Jiang,Zachary Friggstad,Russell Greiner*

Main category: cs.LG

TL;DR: 本文研究在生存数据集中进行预算学习，允许通过支付预算来（部分）重新标记审查实例，以提高模型性能，并提供了理论和实验结果，证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 标准监督学习假设数据已标注，但生存数据包含审查实例（只知事件发生时间下限）。现实世界数据收集中的随访并不总能完全去审查。因此，需要一种预算学习方法，能有效利用有限预算来获取（部分）标签信息，以改进生存分析模型。

Method: 该研究探讨了在包含（右侧）审查实例的生存数据集中进行预算学习。学习器可以支付预算来（部分）标记审查实例，例如获取实际事件时间或更多时间信息。该方法将最先进的预算学习算法应用于生存数据，并进行了理论和实验分析。

Result: 该方法在渐近界和时间复杂度上与标准的活跃学习方法BatchBALD相当。此外，对多个生存任务的实证分析表明，该模型在多个基准测试上优于其他潜在方法。

Conclusion: 本研究成功将预算学习应用于处理审查实例的生存数据。所提出的方法在理论上与现有技术相当，在经验上表现优异，为更真实的生存数据收集和建模提供了有效途径。

Abstract: Standard supervised learners attempt to learn a model from a labeled dataset.
Given a small set of labeled instances, and a pool of unlabeled instances, a
budgeted learner can use its given budget to pay to acquire the labels of some
unlabeled instances, which it can then use to produce a model. Here, we explore
budgeted learning in the context of survival datasets, which include (right)
censored instances, where we know only a lower bound on an instance's
time-to-event. Here, that learner can pay to (partially) label a censored
instance -- e.g., to acquire the actual time for an instance [perhaps go from
(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about
one more year, so go from (3 yr, censored) to either (4 yr, censored) or
perhaps (3.2 yr, uncensored)]. This serves as a model of real world data
collection, where follow-up with censored patients does not always lead to
uncensoring, and how much information is given to the learner model during data
collection is a function of the budget and the nature of the data itself. We
provide both experimental and theoretical results for how to apply
state-of-the-art budgeted learning algorithms to survival data and the
respective limitations that exist in doing so. Our approach provides bounds and
time complexity asymptotically equivalent to the standard active learning
method BatchBALD. Moreover, empirical analysis on several survival tasks show
that our model performs better than other potential approaches on several
benchmarks.

</details>


### [207] [Self-Verifying Reflection Helps Transformers with CoT Reasoning](https://arxiv.org/abs/2510.12157)
*Zhongwei Yu,Wannian Xia,Xue Yan,Bo Xu,Haifeng Zhang,Yali Du,Jun Wang*

Main category: cs.LG

TL;DR: 本文提出一个极简推理框架，证明并实验表明，对于小型Transformer，在验证误差受限时，自我验证反射能保证性能提升，并达到LLM级别的表现；同时分析了强化学习在其中的作用。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）常通过反思/思维链（CoTs）进行自我验证，但鉴于LLMs在CoTs中错误检测能力有限，反思如何贡献于实际性能提升的机制尚不明确。

Method: 本文提出了一个不依赖自然语言、支持基本自我验证反射的极简推理框架，应用于小型Transformer。理论上，证明了在验证误差适当受限时，自我验证反射能保证性能提升。实验上，在整数乘法和数独任务上评估了小型Transformer的自我验证能力，并探讨了强化学习（RL）的作用。

Result: 理论上，自我验证反射在验证误差受限时能保证性能提升。实验上，小型Transformer通过自我验证在训练和反射执行中均获益，在整数乘法和数独任务上达到了显著的LLM级别性能。强化学习虽能提升分布内性能并促进频繁反思，但主要优化浅层统计模式，未能有效降低验证误差。

Conclusion: 生成式Transformer与判别式验证的结合，无论模型规模和是否使用自然语言，都能内在促进思维链推理。

Abstract: Advanced large language models (LLMs) frequently reflect in reasoning
chain-of-thoughts (CoTs), where they self-verify the correctness of current
solutions and explore alternatives. However, given recent findings that LLMs
detect limited errors in CoTs, how reflection contributes to empirical
improvements remains unclear. To analyze this issue, in this paper, we present
a minimalistic reasoning framework to support basic self-verifying reflection
for small transformers without natural language, which ensures analytic clarity
and reduces the cost of comprehensive experiments. Theoretically, we prove that
self-verifying reflection guarantees improvements if verification errors are
properly bounded. Experimentally, we show that tiny transformers, with only a
few million parameters, benefit from self-verification in both training and
reflective execution, reaching remarkable LLM-level performance in integer
multiplication and Sudoku. Similar to LLM results, we find that reinforcement
learning (RL) improves in-distribution performance and incentivizes frequent
reflection for tiny transformers, yet RL mainly optimizes shallow statistical
patterns without faithfully reducing verification errors. In conclusion,
integrating generative transformers with discriminative verification inherently
facilitates CoT reasoning, regardless of scaling and natural language.

</details>


### [208] [Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees](https://arxiv.org/abs/2510.12209)
*Yiming Zhang,Chester Holtz,Gal Mishne,Alex Cloninger*

Main category: cs.LG

TL;DR: 本文对含噪声标签的元重加权学习进行了理论分析，揭示了其训练的三阶段机制及局限性。在此基础上，提出了一种轻量级、更稳定的替代方法，并在实验中超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 在含噪声标签学习中，过参数网络会记忆错误监督，使问题复杂化。尽管元学习样本重加权能缓解此问题，但其行为和训练动态缺乏理论理解。

Method: 1. 提供了噪声标签下元重加权方法的严格理论分析，揭示了其训练轨迹的三个阶段：对齐、过滤和后过滤。2. 基于理论分析，提出了一种轻量级的元重加权替代方法，通过整合均值中心化、行移位和标签符号调制，避免了昂贵的双层优化。

Result: 1. 理论分析表明，元重加权的训练轨迹分为对齐、过滤和后过滤三个阶段。其机制是训练信号与干净子集信号的相似度加权耦合，以及干净子集训练损失的收缩。在后过滤阶段，当干净子集损失足够小时，元重加权会失去判别力。2. 在合成和真实含噪声标签的基准测试中，提出的方法始终优于强大的重加权/选择基线。

Conclusion: 通过深入的理论分析，揭示了元重加权在噪声标签学习中的训练机制和局限性。基于此分析，开发了一种更稳定、高效的轻量级替代方法，有效提升了含噪声标签学习的性能。

Abstract: Learning with noisy labels remains challenging because over-parameterized
networks memorize corrupted supervision. Meta-learning-based sample reweighting
mitigates this by using a small clean subset to guide training, yet its
behavior and training dynamics lack theoretical understanding. We provide a
rigorous theoretical analysis of meta-reweighting under label noise and show
that its training trajectory unfolds in three phases: (i) an alignment phase
that amplifies examples consistent with a clean subset and suppresses
conflicting ones; (ii) a filtering phase driving noisy example weights toward
zero until the clean subset loss plateaus; and (iii) a post-filtering phase in
which noise filtration becomes perturbation-sensitive. The mechanism is a
similarity-weighted coupling between training and clean subset signals together
with clean subset training loss contraction; in the post-filtering regime where
the clean-subset loss is sufficiently small, the coupling term vanishes and
meta-reweighting loses discriminatory power. Guided by this analysis, we
propose a lightweight surrogate for meta-reweighting that integrates
mean-centering, row shifting, and label-signed modulation, yielding more stable
performance while avoiding expensive bi-level optimization. Across synthetic
and real noisy-label benchmarks, our method consistently outperforms strong
reweighting/selection baselines.

</details>


### [209] [DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification](https://arxiv.org/abs/2510.12214)
*Tao Xie,Zexi Tan,Haoyi Xiao,Binbin Sun,Yiqun Zhang*

Main category: cs.LG

TL;DR: 本论文提出了DE3S模型，通过新颖的双增强软稀疏形状学习框架，在医疗早期时间序列分类（ETSC）中精确识别形状基元，解决了现有方法在准确性和早期性上的冲突与早期模式捕获困难，并在六个真实医疗数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗ETSC在败血症预测等时间敏感场景中至关重要，延迟预测会导致大量死亡，并降低ICU资源利用效率和医疗精度。然而，ETSC面临准确性和早期性冲突的挑战，现有方法难以捕捉微弱的早期信号，且存在类别不平衡问题。

Method: 本文提出Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification (DE3S)。该框架通过以下创新精确识别形状基元：
1.  **双重增强策略**：结合传统时间增强和基于注意力的全局时间增强，实现鲁棒表示学习。
2.  **基于注意力分数的软形状基元稀疏化机制**：动态保留判别性模式，同时聚合不重要的形状基元。
3.  **双路径Mixture of Experts (MoE)与Inception模块融合架构**：MoE进行形状基元内局部学习，Inception模块捕获跨形状基元的全局模式。
此外，采用加权交叉熵损失处理类别不平衡，并在主题一致性数据集上验证了模型的鲁棒性。

Result: 在六个真实世界医疗数据集上进行的广泛实验表明，DE3S模型取得了最先进的性能。消融研究也证实了其各组件的有效性。

Conclusion: DE3S通过其创新的形状基元学习框架，成功解决了医疗早期时间序列分类中准确性和早期性之间的冲突，并有效捕获了早期微弱信号，有望显著提升医疗场景中的预测能力和效率。

Abstract: Early time-series classification (ETSC) in medical applications is crucial
for time-sensitive scenarios such as sepsis prediction in intensive care units
(ICUs), where a large number of deaths are caused by delayed prediction. ETSC
can significantly improve ICU resource utilization efficiency and healthcare
precision. However, it faces conflicting goals of accuracy and earliness, with
existing methods often trading one for the other, struggling to capture subtle
early-stage patterns due to weak initial signals and class imbalance. The key
to solve these challenges is to find shapelets, which are discriminative
subsequences (or shapes) with high interpretability in time-series
classification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning
for Medical Early Time-Series Classification (DE3S), which introduces a novel
Dual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely
through three innovations: (1) a comprehensive dual-enhancement strategy
combines traditional temporal augmentation with attention-based global temporal
enhancement for robust representation learning, (2) an attention-score-based
soft shapelet sparsification mechanism dynamically preserves discriminative
patterns while aggregating less important shapelets into representative tokens,
and (3) a dual-path Mixture of Experts Network (MoE) and Inception modules
fusion architecture where MoE performs local learning within shapelets and
multi-scale Inception modules capture global patterns across shapelets. The
framework employs weighted cross-entropy loss for class imbalance handling and
demonstrates robustness on subject-consistency datasets. Extensive experiments
on six real-world medical datasets show state-of-the-art performance, with
ablation studies confirming component efficacy.

</details>


### [210] [Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory](https://arxiv.org/abs/2510.12220)
*Hanru Bai,Weiyang Ding,Difan Zou*

Main category: cs.LG

TL;DR: 本文提出分层Koopman扩散模型，通过将非线性扩散动力学提升到全局线性潜在空间，实现一步采样和可解释的生成轨迹，同时保持精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样速度慢，而一步采样方法牺牲了解释性和细粒度控制。研究旨在解决这一矛盾，实现快速采样同时保持生成过程的可解释性和可控性。

Method: 引入“分层Koopman扩散”框架。基于Koopman算子理论，将非线性扩散动力学映射到由全局线性算子控制的潜在空间，从而实现闭式轨迹解和一步采样。设计分层架构，通过尺度特定的Koopman子空间分离不同空间分辨率的生成动力学，系统地捕获从粗到细的图像细节。

Result: 分层Koopman扩散模型不仅实现了有竞争力的一步生成性能，而且通过谱分析提供了一种解释和操作生成过程的原则性机制。该方法消除了迭代采样，并能完全访问中间状态，允许在生成过程中进行手动干预。

Conclusion: 该框架弥合了扩散模型中快速采样和可解释性之间的鸿沟，为生成模型中的可解释图像合成铺平了道路。

Abstract: Diffusion models have achieved impressive success in high-fidelity image
generation but suffer from slow sampling due to their inherently iterative
denoising process. While recent one-step methods accelerate inference by
learning direct noise-to-image mappings, they sacrifice the interpretability
and fine-grained control intrinsic to diffusion dynamics, key advantages that
enable applications like editable generation. To resolve this dichotomy, we
introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that
achieves both one-step sampling and interpretable generative trajectories.
Grounded in Koopman operator theory, our method lifts the nonlinear diffusion
dynamics into a latent space where evolution is governed by globally linear
operators, enabling closed-form trajectory solutions. This formulation not only
eliminates iterative sampling but also provides full access to intermediate
states, allowing manual intervention during generation. To model the
multi-scale nature of images, we design a hierarchical architecture that
disentangles generative dynamics across spatial resolutions via scale-specific
Koopman subspaces, capturing coarse-to-fine details systematically. We
empirically show that the Hierarchical Koopman Diffusion not only achieves
competitive one-step generation performance but also provides a principled
mechanism for interpreting and manipulating the generative process through
spectral analysis. Our framework bridges the gap between fast sampling and
interpretability in diffusion models, paving the way for explainable image
synthesis in generative modeling.

</details>


### [211] [Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs](https://arxiv.org/abs/2510.12233)
*Bowen Fan,Zhilin Guo,Xunkai Li,Yihan Zhou,Bing Zhou,Zhenjun Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本研究提出了IMDGA，一个统一且可解释的多维图对抗攻击框架，旨在同时扰动Graph-LLM的图结构和文本特征，揭示其深层漏洞并提供改进韧性的见解。


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks结合大型语言模型（Graph-LLMs）在处理文本属性图（TAGs）时，虽然增强了表示能力，但也引入了结构和文本属性两方面的对抗性攻击漏洞。现有攻击方法各自为政，缺乏一个能够统一这两方面进行全面攻击的综合框架。

Method: 本研究提出了可解释多维图攻击（IMDGA）框架。这是一个以人为中心的对抗性攻击框架，通过协调图结构和文本特征的多级别扰动来生成攻击。IMDGA包含三个紧密整合的模块，旨在平衡攻击的可解释性和影响力，以深入分析Graph-LLM的脆弱性。

Result: 经过严谨的理论分析和在多样数据集及架构上的综合实证评估，IMDGA展示出优于现有方法的解释性、攻击有效性、隐蔽性和鲁棒性。它成功揭示了文本属性图表示学习中的关键弱点，并发现了一个以前未被充分探索的Graph-LLMs脆弱性语义维度。

Conclusion: 本工作通过揭露Graph-LLMs中未被充分探索的语义维度脆弱性及其关键弱点，为提升Graph-LLMs的韧性提供了宝贵的见解。

Abstract: Graph Neural Networks (GNNs) have become a pivotal framework for modeling
graph-structured data, enabling a wide range of applications from social
network analysis to molecular chemistry. By integrating large language models
(LLMs), text-attributed graphs (TAGs) enhance node representations with rich
textual semantics, significantly boosting the expressive power of graph-based
learning. However, this sophisticated synergy introduces critical
vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both
their structural topology and textual attributes. Although specialized attack
methods have been designed for each of these aspects, no work has yet unified
them into a comprehensive approach. In this work, we propose the Interpretable
Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial
attack framework designed to orchestrate multi-level perturbations across both
graph structure and textual features. IMDGA utilizes three tightly integrated
modules to craft attacks that balance interpretability and impact, enabling a
deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical
analysis and comprehensive empirical evaluations on diverse datasets and
architectures, IMDGA demonstrates superior interpretability, attack
effectiveness, stealthiness, and robustness compared to existing methods. By
exposing critical weaknesses in TAG representation learning, this work uncovers
a previously underexplored semantic dimension of vulnerability in Graph-LLMs,
offering valuable insights for improving their resilience. Our code and
resources are publicly available at
https://anonymous.4open.science/r/IMDGA-7289.

</details>


### [212] [MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant](https://arxiv.org/abs/2510.12245)
*Tao Yin,Xiaohong Zhang,Jiacheng Zhang,Li Huang,Zhibin Zhang,Yuansong Zeng,Jin Xie,Meng Yan*

Main category: cs.LG

TL;DR: 提出MoRA框架，为每个分子图动态生成低秩适应权重，注入冻结LLM，以实现实例感知参数空间对齐，克服现有方法局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合分子图结构与LLM时存在局限：1) 优化共享参数空间，限制捕获实例特定结构特征的能力；2) 微调LLM可能导致灾难性遗忘，损害通用推理能力。

Method: 引入MoRA（Molecule-aware Low-Rank Adaptation），动态为每个输入分子图生成一组独特的低秩适应权重，并将其注入冻结的LLM，实现实例特定参数空间对齐，而非静态任务导向适应。

Result: 在化学反应预测中，准确匹配率相对提高了14.1%；在量子属性预测中，错误率降低了22%，优于静态适应基线方法。

Conclusion: MoRA通过实例特定动态适应，有效解决了分子结构与LLM集成中的挑战，在不损害LLM核心知识的前提下，显著提升了分子任务性能。

Abstract: Effectively integrating molecular graph structures with Large Language Models
(LLMs) is a key challenge in drug discovery. Most existing multi-modal
alignment methods typically process these structures by fine-tuning the LLM or
adding a static adapter simultaneously. However, these approaches have two main
limitations: (1) it optimizes a shared parameter space across all molecular
inputs, limiting the model's ability to capture instance-specific structural
features; and (2) fine-tuning the LLM for molecular tasks can lead to
catastrophic forgetting, undermining its general reasoning capabilities. In
this paper, instead of static task-oriented adaptation, we propose an
instance-specific parameter space alignment approach for each molecule
on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA)
that produces a unique set of low-rank adaptation weights for each input
molecular graph. These weights are then dynamically injected into a frozen LLM,
allowing the model to adapt its reasoning to the structure of each molecular
input, while preserving the LLM's core knowledge. Extensive experiments
demonstrate that on key molecular tasks, such as chemical reaction prediction
and molecular captioning, MoRA's instance-specific dynamic adaptation
outperforms statically adapted baselines, including a 14.1% relative
improvement in reaction prediction exact match and a 22% reduction in error for
quantum property prediction. The code is available at
https://github.com/jk-sounds/MoRA.

</details>


### [213] [Optimal Regularization for Performative Learning](https://arxiv.org/abs/2510.12249)
*Edwige Cyffers,Alireza Mirrokni,Marco Mondelli*

Main category: cs.LG

TL;DR: 在适应性学习中，数据分布会响应模型。本文研究了正则化在高维岭回归中如何应对适应性效应。发现适应性效应在过度参数化设置下可能是有益的，且最优正则化强度与适应性效应的整体强度相关。


<details>
  <summary>Details</summary>
Motivation: 在适应性学习中，数据分布会响应部署的模型而变化，导致比传统监督学习更复杂的动态。模型不仅需要优化当前数据，还需考虑其可能引导数据分布向未知新方向发展，因此需要探索应对这种动态的方法。

Method: 通过在高维岭回归中研究正则化的影响，探索其如何应对适应性效应。通过在合成数据集和真实世界数据集上对最优正则化参数进行实证评估来验证研究发现。

Result: 研究表明，虽然适应性效应在总体设置中会增加测试风险，但在过度参数化（特征数超过样本数）的情况下，它们可能是有益的。此外，最优正则化强度与适应性效应的整体强度成比例，这使得可以预先设定正则化参数。这些发现通过经验评估得到了证实。

Conclusion: 正则化可以有效地帮助应对适应性学习中的动态效应，特别是在过度参数化的情境下，适应性效应甚至可能带来益处。最优正则化参数可以根据适应性效应的整体强度进行调整，从而提高模型在动态环境中的性能和鲁棒性。

Abstract: In performative learning, the data distribution reacts to the deployed model
- for example, because strategic users adapt their features to game it - which
creates a more complex dynamic than in classical supervised learning. One
should thus not only optimize the model for the current data but also take into
account that the model might steer the distribution in a new direction, without
knowing the exact nature of the potential shift. We explore how regularization
can help cope with performative effects by studying its impact in
high-dimensional ridge regression. We show that, while performative effects
worsen the test risk in the population setting, they can be beneficial in the
over-parameterized regime where the number of features exceeds the number of
samples. We show that the optimal regularization scales with the overall
strength of the performative effect, making it possible to set the
regularization in anticipation of this effect. We illustrate this finding
through empirical evaluations of the optimal regularization parameter on both
synthetic and real-world datasets.

</details>


### [214] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: 本文综述了扩散模型（DMs）在强化学习（RL）中的应用，通过建立双轴分类法、分析集成框架和应用，并讨论开放问题及未来方向，提供了一个全面且最新的分析。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）作为领先的生成模型，在多模态表达、稳定训练和轨迹级规划方面为强化学习（RL）提供了关键优势。本调查旨在提供一个关于基于扩散模型的强化学习的全面且最新的综合分析，探讨DMs如何解决RL中的核心挑战。

Method: 本文首先概述了RL的挑战和DMs的基本概念，接着建立了一个双轴分类法（功能导向和技术导向）来组织该领域。然后，分析了从单智能体到多智能体领域的进展，并形成了DM-RL集成框架。

Result: 本调查建立了双轴分类法来阐明DMs在RL流程中的作用和实现方式。形成了多个DM-RL集成框架并突出了其实用性。概述了扩散模型在不同领域成功应用的多个类别。

Conclusion: 本调查讨论了当前方法的开放研究问题，并指出了未来研究的关键方向，以推动扩散模型在强化学习领域的发展。

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [215] [FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning](https://arxiv.org/abs/2510.12254)
*Ningxin He,Yang Liu,Wei Sun,Xiaozhou Ye,Ye Ouyang,Tiegang Gao,Zehui Zhang*

Main category: cs.LG

TL;DR: 针对T2I模型在特定任务上数据隐私受限的问题，本文提出联邦多模态知识迁移 (FedMMKT) 框架，利用去中心化多模态数据，在保护隐私的前提下协同增强服务器T2I模型和客户端任务特定模型。


<details>
  <summary>Details</summary>
Motivation: 文本到图像 (T2I) 模型在专业任务上的适应性受限于隐私担忧导致的任务特定数据缺乏。同时，现代移动系统和IoT基础设施中丰富的多模态数据提供了新的利用机会。

Method: 本文引入联邦多模态知识迁移 (FedMMKT) 框架，该框架通过利用去中心化的多模态数据，实现服务器T2I模型和客户端任务特定模型的协同增强。

Result: FedMMKT框架能够在不损害数据隐私的前提下，有效实现服务器T2I模型与客户端任务特定模型的共同提升。

Conclusion: 该框架为T2I模型在隐私受限和数据分散的场景下，适应特定任务提供了创新的解决方案。

Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide
range of applications. However, adaptation of T2I models to specialized tasks
is often limited by the availability of task-specific data due to privacy
concerns. On the other hand, harnessing the power of rich multimodal data from
modern mobile systems and IoT infrastructures presents a great opportunity.
This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a
novel framework that enables co-enhancement of a server T2I model and client
task-specific models using decentralized multimodal data without compromising
data privacy.

</details>


### [216] [HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization](https://arxiv.org/abs/2510.12266)
*Ziyi Han,Huanyu Wang,Zeyu Zhang,Xiangxiang Dai,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: 本文提出了HiLoRA，一个无需训练的框架，通过对LoRA池进行自适应分层路由（基于秩一分量），解决了现有LoRA重用方法在域泛化上的局限性，大幅提升了LLM的域泛化性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有用于大型语言模型（LLM）域泛化的LoRA重用方法，常依赖于显式任务标签或额外训练，导致部署不切实际。此外，它们通常激活固定数量的整个LoRA模块，造成参数冗余或不足，从而降低性能。

Method: 本文提出HiLoRA，一个无需训练的框架，对LoRA池执行自适应分层路由。它将LoRA结构分解为独立的“秩一分量”（ROCs）。对于给定输入序列，HiLoRA首先在序列级别基于高斯似然自适应选择LoRA子集并分配ROCs。然后在token级别，通过仅激活信息量最大的ROCs来进一步优化路由。该方法还提供了HiLoRA以高概率选择最相关LoRA的理论保证。

Result: 实验结果表明，HiLoRA在域泛化方面取得了显著改进，相较于最先进的基线，准确率提升高达55%，同时保持了相当的推理吞吐量。

Conclusion: HiLoRA通过创新的自适应分层路由和细粒度ROCs激活机制，成功克服了现有LoRA重用方法的部署和性能瓶颈，在无需额外训练的情况下大幅提升了LLM的域泛化能力和效率，为实际应用提供了强大而灵活的解决方案。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely used technique for
adapting large language models (LLMs) to new domains, due to its modular design
and broad availability on platforms such as HuggingFace. This availability has
motivated efforts to reuse existing LoRAs for domain generalization.
  However, existing methods often rely on explicit task labels or additional
training, which are impractical for deployment. Moreover, they typically
activate a fixed number of entire LoRA modules, leading to parameter redundancy
or insufficiency that degrade performance.
  In this paper, we propose \texttt{HiLoRA}, a training-free framework that
performs adaptive hierarchical routing over LoRA pools. Drawing on structural
properties of LoRA, we define rank-one components (ROCs), in which each rank
parameter is regarded as an independent unit. For a given input sequence,
\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their
ROC allocation based on Gaussian likelihoods at the sequence level. At the
token level, it further refines routing by activating only the most informative
ROCs.
  We further provide theoretical guarantees that \texttt{HiLoRA} selects the
most relevant LoRAs with high probability.
  Extensive experiments show that \texttt{HiLoRA} achieves substantial
improvements in domain generalization, with accuracy gains of up to {\small
$55\%$} over state-of-the-art baselines, while maintaining comparable inference
throughput.

</details>


### [217] [Multi-Action Self-Improvement for Neural Combinatorial Optimization](https://arxiv.org/abs/2510.12273)
*Laurin Luttmann,Lin Xie*

Main category: cs.LG

TL;DR: 现有NCO自改进方法在计算效率和多智能体协调方面存在局限。本文提出一种新的自改进范式，通过联合多智能体动作和集合预测损失，有效利用问题对称性，显著提升了解决方案质量并减少了生成延迟。


<details>
  <summary>Details</summary>
Motivation: 现有神经组合优化（NCO）中的自改进方法存在两大限制：1) 训练计算成本高，需要大量采样才能获得单个专家轨迹；2) 未能有效利用多智能体问题的结构和智能体置换对称性，通过监督单动作轨迹，阻碍了泛化能力和协调行为的学习。

Method: 将自改进扩展到操作联合多智能体动作。模型架构在每个决策步联合预测完整的智能体-任务分配。为显式利用对称性，采用集合预测损失，在给定状态下监督策略学习多个专家分配。

Result: 在多个组合问题上的实验验证表明，本方法与标准自改进相比，持续提升了最终解决方案的质量，并显著减少了解决方案的生成延迟。

Conclusion: 通过引入联合多智能体动作和集合预测损失，本方法成功克服了传统自改进在计算效率和多智能体协调方面的挑战，实现了更优的解决方案和更快的生成速度。

Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural
Combinatorial Optimization (NCO), where models iteratively refine their
policies by generating and imitating high-quality solutions. Despite strong
empirical performance, existing methods face key limitations. Training is
computationally expensive, as policy updates require sampling numerous
candidate solutions per instance to extract a single expert trajectory. More
fundamentally, these approaches fail to exploit the structure of combinatorial
problems involving the coordination of multiple agents, such as vehicles in
min-max routing or machines in scheduling. By supervising on single-action
trajectories, they fail to exploit agent-permutation symmetries, where distinct
sequences of actions yield identical solutions, hindering generalization and
the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over
joint multi-agent actions. Our model architecture predicts complete agent-task
assignments jointly at each decision step. To explicitly leverage symmetries,
we employ a set-prediction loss, which supervises the policy on multiple expert
assignments for any given state. This approach enhances sample efficiency and
the model's ability to learn coordinated behavior. Furthermore, by generating
multi-agent actions in parallel, it drastically accelerates the solution
generation phase of the self-improvement loop. Empirically, we validate our
method on several combinatorial problems, demonstrating consistent improvements
in the quality of the final solution and a reduced generation latency compared
to standard self-improvement.

</details>


### [218] [General Fourier Feature Physics-Informed Extreme Learning Machine (GFF-PIELM) for High-Frequency PDEs](https://arxiv.org/abs/2510.12293)
*Fei Ren,Sifan Wang,Pei-Zhi Zhuang,Hai-Sui Yu,He Yang*

Main category: cs.LG

TL;DR: GFF-PIELM通过引入傅里叶特征，解决了传统PIELM在处理高频和变频偏微分方程时的挑战，显著提高了预测精度，同时保持了效率和简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息极限学习机（PIELM）在求解涉及高频和变频行为的偏微分方程（PDEs）时面临挑战。

Method: 提出通用傅里叶特征物理信息极限学习机（GFF-PIELM）。方法包括：1. 将傅里叶特征映射（FFM）的变体整合到ELM中作为傅里叶激活函数，保持单隐藏层；2. 为隐藏神经元分配频率系数以捕获不同频率成分；3. 通过监测ELM输出权重分布，开发创新的超参数初始化方法。

Result: GFF-PIELM在处理高频、变频、多尺度行为、不规则边界和逆问题时，显著提高了预测精度，且未增加训练时间和架构复杂性。通过五项案例研究（十个数值示例）验证了其可行性和有效性。

Conclusion: PIELM可以扩展到高精度求解高频和变频PDEs。本研究的初始化策略有望为其他物理信息机器学习（PIML）框架的进步提供启发。

Abstract: Conventional physics-informed extreme learning machine (PIELM) often faces
challenges in solving partial differential equations (PDEs) involving
high-frequency and variable-frequency behaviors. To address these challenges,
we propose a general Fourier feature physics-informed extreme learning machine
(GFF-PIELM). We demonstrate that directly concatenating multiple Fourier
feature mappings (FFMs) and an extreme learning machine (ELM) network makes it
difficult to determine frequency-related hyperparameters. Fortunately, we find
an alternative to establish the GFF-PIELM in three main steps. First, we
integrate a variation of FFM into ELM as the Fourier-based activation function,
so there is still one hidden layer in the GFF-PIELM framework. Second, we
assign a set of frequency coefficients to the hidden neurons, which enables ELM
network to capture diverse frequency components of target solutions. Finally,
we develop an innovative, straightforward initialization method for these
hyperparameters by monitoring the distribution of ELM output weights. GFF-PIELM
not only retains the high accuracy, efficiency, and simplicity of the PIELM
framework but also inherits the ability of FFMs to effectively handle
high-frequency problems. We carry out five case studies with a total of ten
numerical examples to highlight the feasibility and validity of the proposed
GFF-PIELM, involving high frequency, variable frequency, multi-scale behaviour,
irregular boundary and inverse problems. Compared to conventional PIELM, the
GFF-PIELM approach significantly improves predictive accuracy without
additional cost in training time and architecture complexity. Our results
confirm that that PIELM can be extended to solve high-frequency and
variable-frequency PDEs with high accuracy, and our initialization strategy may
further inspire advances in other physics-informed machine learning (PIML)
frameworks.

</details>


### [219] [Deep SPI: Safe Policy Improvement via World Models](https://arxiv.org/abs/2510.12312)
*Florent Delgrange,Raphael Avalos,Willem Röpke*

Main category: cs.LG

TL;DR: 本文将安全策略改进（SPI）理论扩展到在线深度强化学习（RL）场景，提出DeepSPI算法，在保持理论保证的同时，实现了与现有强基线相当或超越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的安全策略改进（SPI）理论保证主要局限于离线、表格型强化学习。研究旨在将SPI应用于结合世界模型和表征学习的通用在线设置中，并提供理论控制。

Method: 开发了一个理论框架，表明将策略更新限制在当前策略的特定邻域内可确保单调改进和收敛。该分析将转换和奖励预测损失与表征质量联系起来。在此基础上，引入了DeepSPI算法，这是一种结合局部转换和奖励损失以及正则化策略更新的原则性在策略（on-policy）算法。

Result: 研究产生了在线、“深度”版的经典SPI定理。DeepSPI算法在ALE-57基准测试上匹配或超越了PPO和DeepMDPs等强大基线，同时保持了理论保证。

Conclusion: DeepSPI提供了一种在在线深度强化学习中进行安全策略改进的原则性方法，它结合了严格的理论基础和竞争性的实证性能。

Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates,
yet existing guarantees largely concern offline, tabular reinforcement learning
(RL). We study SPI in general online settings, when combined with world model
and representation learning. We develop a theoretical framework showing that
restricting policy updates to a well-defined neighborhood of the current policy
ensures monotonic improvement and convergence. This analysis links transition
and reward prediction losses to representation quality, yielding online, "deep"
analogues of classical SPI theorems from the offline RL literature. Building on
these results, we introduce DeepSPI, a principled on-policy algorithm that
couples local transition and reward losses with regularised policy updates. On
the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including
PPO and DeepMDPs, while retaining theoretical guarantees.

</details>


### [220] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: 该文结合物理信息图神经网络和极值分析，提升了泰国雨量站的降雨预报，尤其在极端事件预测方面表现优异，并优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确的降雨预报，特别是极端事件的预报，在气候学和地球系统领域仍是一个重大挑战。

Method: 提出了一种新颖的物理信息图神经网络（GNNs）结合极值分析技术。具体使用带有长短期记忆（Attention-LSTM）的图注意力网络，其注意力机制利用了基于地形降水物理公式的初始边特征。为处理极端值，采用新颖的空间季节感知广义帕累托分布（GPD）方法进行超越阈值（POT）映射。

Result: 该方法在大多数区域（包括极端事件多发区）表现优于现有基线模型，并与最先进方法具有竞争力。与SEAS5业务预报系统相比，该方法在实际应用中显著改进了极端事件预测，并能生成支持长期水资源管理的精细分辨率地图。

Conclusion: 该研究为改进极端降雨事件预测提供了有效途径，并通过生成精细分辨率预报图，为长期水资源管理决策提供了实用的支持。

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [221] [Finite-time Convergence Analysis of Actor-Critic with Evolving Reward](https://arxiv.org/abs/2510.12334)
*Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.

</details>


### [222] [Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models](https://arxiv.org/abs/2510.12343)
*Donghwan Rho,Sieun Seo,Hyewon Sung,Chohong Min,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 该研究提出了一种基于旅行商问题（TSP）的策略，解决同态加密下大型语言模型（LLM）文本生成（特别是下一词预测）的挑战，以实现隐私保护的LLM交互。


<details>
  <summary>Details</summary>
Motivation: 随着用户使用私人信息与LLM交互，安全的加密通信至关重要。同态加密（HE）允许在加密数据上计算，但文本生成（特别是下一词预测）在HE下仍是实际加密交互的主要障碍。

Method: 提出了一种基于TSP的词元重排序策略来处理加密文本生成难题，并结合了进一步减少近似误差的后处理步骤。

Result: 理论分析和实验结果表明，该方法能防止生成崩溃，提高生成文本的连贯性，并全程保护数据隐私。

Conclusion: 该工作推进了实用且隐私保护的LLM推理的可行性。

Abstract: As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.

</details>


### [223] [Towards Cross-Modal Error Detection with Tables and Images](https://arxiv.org/abs/2510.12383)
*Olga Ovcharenko,Sebastian Schelter*

Main category: cs.LG

TL;DR: 该研究基准测试了多种方法，用于解决大规模组织中多模态数据（特别是表格数据）的跨模态错误检测挑战，发现Cleanlab和DataScope结合AutoML表现最佳，但现有方法仍有局限。


<details>
  <summary>Details</summary>
Motivation: 大规模数据质量保证是一个持续的挑战，传统错误检测方法多专注于单一模态，无法发现电商和医疗等领域中常见的跨模态数据错误（如图像、表格、文本数据共存）。

Method: 通过在四个数据集上对五种基线方法进行基准测试，评估了表格数据中的跨模态错误检测能力，其中包括将Cleanlab（标签错误检测）和DataScope（数据价值评估）与强大的AutoML框架结合。

Result: 当与强大的AutoML框架结合时，Cleanlab（一种标签错误检测框架）和DataScope（一种数据估值方法）表现最佳，取得了最高的F1分数。

Conclusion: 研究结果表明，当前方法仍存在局限性，尤其是在应用于重尾真实世界数据时，这激发了该领域进一步研究的需求。

Abstract: Ensuring data quality at scale remains a persistent challenge for large
organizations. Despite recent advances, maintaining accurate and consistent
data is still complex, especially when dealing with multiple data modalities.
Traditional error detection and correction methods tend to focus on a single
modality, typically a table, and often miss cross-modal errors that are common
in domains like e-Commerce and healthcare, where image, tabular, and text data
co-exist. To address this gap, we take an initial step towards cross-modal
error detection in tabular data, by benchmarking several methods. Our
evaluation spans four datasets and five baseline approaches. Among them,
Cleanlab, a label error detection framework, and DataScope, a data valuation
method, perform the best when paired with a strong AutoML framework, achieving
the highest F1 scores. Our findings indicate that current methods remain
limited, particularly when applied to heavy-tailed real-world data, motivating
further research in this area.

</details>


### [224] [Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs](https://arxiv.org/abs/2510.12401)
*Shengyin Sun,Chen Ma,Jiehao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种在异构图上预训练GNN的有效框架，通过设计结构感知和语义感知预训练任务，解决标签数据依赖和语义不匹配问题，从而提高模型的知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图数据挖掘中表现出色，但其训练需要大量标记数据，这通常成本高昂或难以获取。现有的GNN预训练方法主要针对同构图，且未考虑异构图中存在的语义不匹配问题。

Method: 该研究提出一个在大型异构图上预训练GNN的框架。首先，设计一个结构感知预训练任务以捕获异构图的结构特性。其次，设计一个语义感知预训练任务以解决语义不匹配问题，具体通过构建由语义邻居组成的扰动子空间，帮助模型关注语义空间中的通用知识，从而提升知识迁移能力。

Result: 在真实世界的大型异构图上进行了广泛实验，结果表明所提出的方法优于最先进的基线方法。

Conclusion: 该框架通过结合结构感知和语义感知任务，成功实现了在大型异构图上的GNN预训练，有效解决了标记数据依赖和语义不匹配问题，显著提高了模型的知识迁移能力和性能。

Abstract: In recent years, graph neural networks (GNNs) have facilitated the
development of graph data mining. However, training GNNs requires sufficient
labeled task-specific data, which is expensive and sometimes unavailable. To be
less dependent on labeled data, recent studies propose to pre-train GNNs in a
self-supervised manner and then apply the pre-trained GNNs to downstream tasks
with limited labeled data. However, most existing methods are designed solely
for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not
consider semantic mismatch (the semantic difference between the original data
and the ideal data containing more transferable semantic information). In this
paper, we propose an effective framework to pre-train GNNs on the large-scale
heterogeneous graph. We first design a structure-aware pre-training task, which
aims to capture structural properties in heterogeneous graphs. Then, we design
a semantic-aware pre-training task to tackle the mismatch. Specifically, we
construct a perturbation subspace composed of semantic neighbors to help deal
with the semantic mismatch. Semantic neighbors make the model focus more on the
general knowledge in the semantic space, which in turn assists the model in
learning knowledge with better transferability. Finally, extensive experiments
are conducted on real-world large-scale heterogeneous graphs to demonstrate the
superiority of the proposed method over state-of-the-art baselines. Code
available at https://github.com/sunshy-1/PHE.

</details>


### [225] [Cautious Weight Decay](https://arxiv.org/abs/2510.12402)
*Lizhang Chen,Jonathan Li,Kaizhao Liang,Baiyu Su,Cong Xie,Nuo Wang Pierse,Chen Liang,Ni Lao,Qiang Liu*

Main category: cs.LG

TL;DR: 本文提出谨慎权重衰减（CWD），一种优化器无关的简单修改，仅当参数坐标符号与优化器更新方向一致时才应用权重衰减。CWD在不改变原始损失目标的情况下，能持续提升大模型在语言预训练和ImageNet分类任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的解耦权重衰减会隐式地优化一个正则化或约束的目标，而CWD旨在保持原始损失目标，并寻找未修改目标的局部帕累托最优驻点。

Method: 引入谨慎权重衰减（CWD），这是一个一行代码、与优化器无关的修改。它只对那些符号与优化器更新方向一致的参数坐标应用权重衰减。CWD保留了原始损失函数，并具有双层解释，在达到平稳流形时引发滑模行为。

Result: CWD可以作为AdamW、Lion和Muon等优化器的即插即用修改，无需新的超参数或额外调优。在语言模型预训练和ImageNet分类任务中，CWD在百万到十亿参数规模上持续改进最终损失和准确率。

Conclusion: CWD是一种简单、无需调优的有效修改，通过有选择地应用权重衰减，在保持原始损失目标的同时，能持续提升大规模模型在不同任务上的性能。

Abstract: We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic
modification that applies weight decay only to parameter coordinates whose
signs align with the optimizer update. Unlike standard decoupled decay, which
implicitly optimizes a regularized or constrained objective, CWD preserves the
original loss and admits a bilevel interpretation: it induces sliding-mode
behavior upon reaching the stationary manifold, allowing it to search for
locally Pareto-optimal stationary points of the unmodified objective. In
practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,
requiring no new hyperparameters or additional tuning. For language model
pre-training and ImageNet classification, CWD consistently improves final loss
and accuracy at million- to billion-parameter scales.

</details>


### [226] [Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals](https://arxiv.org/abs/2510.12405)
*Masahiro Negishi,Hyunsoo Park,Kinga O. Mastej,Aron Walsh*

Main category: cs.LG

TL;DR: 针对材料生成AI模型评估中现有晶体距离函数的局限性，本文提出两种连续距离函数，以提供更可靠的模型评估方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在材料发现中面临评估挑战，现有晶体距离函数存在四大局限：无法量化相似度、无法区分成分与结构差异、缺乏原子坐标偏移的Lipschitz连续性，以及唯一性指标对样本排列不变性差。

Method: 提出并使用两种新的连续距离函数来评估生成模型的独特性和新颖性，这些函数在理论上克服了传统距离函数的局限性。

Result: 实验表明，所提出的距离函数能揭示传统距离函数所遗漏的见解。

Conclusion: 所提出的连续距离函数为评估和比较无机晶体生成模型提供了更可靠的基础。

Abstract: To address pressing scientific challenges such as climate change,
increasingly sophisticated generative artificial intelligence models are being
developed that can efficiently sample the large chemical space of possible
functional materials. These models can quickly sample new chemical compositions
paired with crystal structures. They are typically evaluated using uniqueness
and novelty metrics, which depend on a chosen crystal distance function.
However, the most prevalent distance function has four limitations: it fails to
quantify the degree of similarity between compounds, cannot distinguish
compositional difference and structural difference, lacks Lipschitz continuity
against shifts in atomic coordinates, and results in a uniqueness metric that
is not invariant against the permutation of generated samples. In this work, we
propose using two continuous distance functions to evaluate uniqueness and
novelty, which theoretically overcome these limitations. Our experiments show
that these distances reveal insights missed by traditional distance functions,
providing a more reliable basis for evaluating and comparing generative models
for inorganic crystals.

</details>


### [227] [Bayesian Optimization for Dynamic Pricing and Learning](https://arxiv.org/abs/2510.12447)
*Anush Anand,Pranav Agrawal,Tejas Bodas*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯过程和贝叶斯优化的非参数动态定价方法，避免了对需求函数的限制性假设。该方法在无限和有限库存设置下均优于传统的强化学习算法，在收入和鲁棒性方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 动态定价的核心挑战在于需求函数未知且需从数据中学习。传统方法通常假设需求函数具有特定的参数形式，这在现实世界场景中可能不成立，从而限制了其适用性。

Method: 我们提出了一种基于高斯过程（GP）的非参数动态定价方法，避免了限制性建模假设。将需求函数视为黑盒函数，并开发了基于贝叶斯优化（BO）的定价算法，该算法对无限和有限库存设置均进行了定制，并提供了遗憾保证。

Result: 实验结果表明，我们基于BO的方法在收入方面优于几种最先进的强化学习（RL）算法。此外，我们的方法所需假设更少，并提供了更强的鲁棒性。

Conclusion: 贝叶斯优化是一种强大且实用的工具，适用于复杂和不确定环境下的动态定价。

Abstract: Dynamic pricing is the practice of adjusting the selling price of a product
to maximize a firm's revenue by responding to market demand. The literature
typically distinguishes between two settings: infinite inventory, where the
firm has unlimited stock and time to sell, and finite inventory, where both
inventory and selling horizon are limited. In both cases, the central challenge
lies in the fact that the demand function -- how sales respond to price -- is
unknown and must be learned from data. Traditional approaches often assume a
specific parametric form for the demand function, enabling the use of
reinforcement learning (RL) to identify near-optimal pricing strategies.
However, such assumptions may not hold in real-world scenarios, limiting the
applicability of these methods. In this work, we propose a Gaussian Process
(GP) based nonparametric approach to dynamic pricing that avoids restrictive
modeling assumptions. We treat the demand function as a black-box function of
the price and develop pricing algorithms based on Bayesian Optimization (BO) --
a sample-efficient method for optimizing unknown functions. We present BO-based
algorithms tailored for both infinite and finite inventory settings and provide
regret guarantees for both regimes, thereby quantifying the learning efficiency
of our methods. Through extensive experiments, we demonstrate that our BO-based
methods outperform several state-of-the-art RL algorithms in terms of revenue,
while requiring fewer assumptions and offering greater robustness. This
highlights Bayesian Optimization as a powerful and practical tool for dynamic
pricing in complex, uncertain environments.

</details>


### [228] [A Function Centric Perspective On Flat and Sharp Minima](https://arxiv.org/abs/2510.12451)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: 该论文挑战了平坦最小值与良好泛化性能相关的传统观点，通过实证研究表明，在正则化下出现的尖锐最小值反而能带来更好的泛化、校准和鲁棒性，指出函数复杂性而非平坦度才是决定解空间几何的关键。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为深度神经网络中平坦最小值与良好泛化能力相关，但近期研究发现这种联系更为微妙，并出现了理论反例和经验例外。本文旨在重新审视锐度在模型性能中的作用，提出锐度应被理解为一种依赖于函数的属性，而非泛化性能差的可靠指标。

Method: 进行广泛的实证研究，涵盖从单目标优化到现代图像分类任务。通过观察正则化（例如SAM、权重衰减或数据增强）和非正则化模型收敛情况及其在泛化、校准、鲁棒性和函数一致性等方面的性能表现。

Result: 研究发现，正则化模型常收敛到更尖锐的最小值，而这些尖锐最小值反而能带来更好的泛化、校准、鲁棒性和函数一致性。相反，无正则化基线模型倾向于收敛到更平坦的最小值，但在所有安全指标上表现更差。

Conclusion: 研究结果表明，是函数复杂性而非单纯的平坦度决定了解空间几何，且尖锐最小值（尤其在正则化下）能反映更合适的归纳偏置。因此，有必要对损失景观几何进行以函数为中心的重新评估。

Abstract: Flat minima are widely believed to correlate with improved generalisation in
deep neural networks. However, this connection has proven more nuanced in
recent studies, with both theoretical counterexamples and empirical exceptions
emerging in the literature. In this paper, we revisit the role of sharpness in
model performance, proposing that sharpness is better understood as a
function-dependent property rather than a reliable indicator of poor
generalisation. We conduct extensive empirical studies, from single-objective
optimisation to modern image classification tasks, showing that sharper minima
often emerge when models are regularised (e.g., via SAM, weight decay, or data
augmentation), and that these sharp minima can coincide with better
generalisation, calibration, robustness, and functional consistency. Across a
range of models and datasets, we find that baselines without regularisation
tend to converge to flatter minima yet often perform worse across all safety
metrics. Our findings demonstrate that function complexity, rather than
flatness alone, governs the geometry of solutions, and that sharper minima can
reflect more appropriate inductive biases (especially under regularisation),
calling for a function-centric reappraisal of loss landscape geometry.

</details>


### [229] [Time-Correlated Video Bridge Matching](https://arxiv.org/abs/2510.12453)
*Viacheslav Vasilev,Arseny Ivanov,Nikita Gushchin,Maria Kovaleva,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出时间相关视频桥接匹配（TCVBM）框架，将桥接匹配（BM）扩展到时间相关数据序列，以解决扩散模型在复杂数据分布间转换的局限性，特别是在视频生成和操作任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型擅长噪声到数据生成，但在复杂数据分布间的转换（数据到数据任务）上表现不佳。现有桥接匹配（BM）模型虽能处理数据间转换，但未应用于时间相关数据序列，这限制了其在需要时间连贯性的视频生成和操作任务中的有效性。

Method: 我们提出了时间相关视频桥接匹配（TCVBM），一个将BM扩展到视频领域中时间相关数据序列的框架。TCVBM在扩散桥中明确建模序列间的依赖关系，直接将时间相关性纳入采样过程。

Result: TCVBM在帧插值、图像到视频生成和视频超分辨率三项视频相关任务中，与基于桥接匹配和扩散模型的经典方法相比，在多项定量指标上取得了卓越性能，展示了增强的生成质量和重建保真度。

Conclusion: TCVBM通过将桥接匹配扩展到时间相关视频数据，成功解决了现有模型的局限性，在多种视频任务中实现了显著的性能提升，证明了其在视频生成和操作领域的有效性。

Abstract: Diffusion models excel in noise-to-data generation tasks, providing a mapping
from a Gaussian distribution to a more complex data distribution. However they
struggle to model translations between complex distributions, limiting their
effectiveness in data-to-data tasks. While Bridge Matching (BM) models address
this by finding the translation between data distributions, their application
to time-correlated data sequences remains unexplored. This is a critical
limitation for video generation and manipulation tasks, where maintaining
temporal coherence is particularly important. To address this gap, we propose
Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to
time-correlated data sequences in the video domain. TCVBM explicitly models
inter-sequence dependencies within the diffusion bridge, directly incorporating
temporal correlations into the sampling process. We compare our approach to
classical methods based on bridge matching and diffusion models for three
video-related tasks: frame interpolation, image-to-video generation, and video
super-resolution. TCVBM achieves superior performance across multiple
quantitative metrics, demonstrating enhanced generation quality and
reconstruction fidelity.

</details>


### [230] [CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling](https://arxiv.org/abs/2510.12489)
*Beibu Li,Qichao Shentu,Yang Shu,Hui Zhang,Ming Li,Ning Jin,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文提出CrossAD框架，通过跨尺度重建和全局多尺度上下文建模，解决了现有时间序列异常检测方法在跨尺度关联和固定窗口限制上的不足，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在实际应用中至关重要。尽管多尺度建模有利于发现潜在异常模式，但现有方法常独立处理多尺度信息或采用简单融合策略，忽略异常期间的动态跨尺度关联。此外，多数方法基于固定滑动窗口进行多尺度建模，限制了捕获全面上下文信息的能力。

Method: 本文提出CrossAD框架，通过以下方式解决问题：1. 提出跨尺度重建机制，从粗粒度序列重建细粒度序列，显式捕获跨尺度关联。2. 设计查询库并融入全局多尺度上下文，以克服固定窗口大小的限制。

Result: 在多个真实世界数据集上进行了广泛实验，使用九种评估指标验证了CrossAD的有效性，证明其在异常检测方面达到了最先进的性能。

Conclusion: CrossAD通过考虑跨尺度关联和跨窗口建模，显著提升了时间序列异常检测的性能，超越了现有方法。

Abstract: Time series anomaly detection plays a crucial role in a wide range of
real-world applications. Given that time series data can exhibit different
patterns at different sampling granularities, multi-scale modeling has proven
beneficial for uncovering latent anomaly patterns that may not be apparent at a
single scale. However, existing methods often model multi-scale information
independently or rely on simple feature fusion strategies, neglecting the
dynamic changes in cross-scale associations that occur during anomalies.
Moreover, most approaches perform multi-scale modeling based on fixed sliding
windows, which limits their ability to capture comprehensive contextual
information. In this work, we propose CrossAD, a novel framework for time
series Anomaly Detection that takes Cross-scale associations and Cross-window
modeling into account. We propose a cross-scale reconstruction that
reconstructs fine-grained series from coarser series, explicitly capturing
cross-scale associations. Furthermore, we design a query library and
incorporate global multi-scale context to overcome the limitations imposed by
fixed window sizes. Extensive experiments conducted on multiple real-world
datasets using nine evaluation metrics validate the effectiveness of CrossAD,
demonstrating state-of-the-art performance in anomaly detection.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [231] [Stable and Fault-Tolerant Decentralized Traffic Engineering](https://arxiv.org/abs/2510.11937)
*Arjun Devraj,Umesh Krishnaswamy,Ying Zhang,Karuna Grewal,Justin Hsu,Eva Tardos,Rachee Singh*

Main category: cs.NI

TL;DR: Symphony是一个去中心化的流量工程系统，通过引入二次正则化解决自主控制器间的流量分配分歧导致的拥塞，同时利用智能切片算法最小化故障半径，大幅提升了云广域网的稳定性和弹性。


<details>
  <summary>Details</summary>
Motivation: 当前的去中心化流量工程系统旨在限制控制器故障影响，但研究发现自主切片控制器可能产生分歧的流量分配，导致链路超载30%，从而引发拥塞。

Method: Symphony通过两种创新方法解决问题：1. **二次正则化**：增强流量工程目标，使流量分配对需求扰动更鲁棒，确保控制器无需协调即可收敛到兼容分配。2. **随机切片算法**：智能划分网络，将关键流量源分散到不同切片，以最小化故障半径，防止单一故障造成灾难性影响。

Result: 通过在云服务提供商广域网上的广泛评估，Symphony将分歧引起的拥塞减少了14倍，并将故障半径减少了79%，显著优于现有实践。

Conclusion: Symphony通过结合算法稳定性（正则化）和架构弹性（智能切片），有效解决了去中心化流量工程系统中的拥塞问题，同时保持并增强了故障隔离优势，为云广域网提供了更稳定和弹性的流量管理方案。

Abstract: Cloud providers have recently decentralized their wide-area network traffic
engineering (TE) systems to contain the impact of TE controller failures. In
the decentralized design, a controller fault only impacts its slice of the
network, limiting the blast radius to a fraction of the network. However, we
find that autonomous slice controllers can arrive at divergent traffic
allocations that overload links by 30% beyond their capacity. We present
Symphony, a decentralized TE system that addresses the challenge of
divergence-induced congestion while preserving the fault-isolation benefits of
decentralization. By augmenting TE objectives with quadratic regularization,
Symphony makes traffic allocations robust to demand perturbations, ensuring TE
controllers naturally converge to compatible allocations without coordination.
In parallel, Symphony's randomized slicing algorithm partitions the network to
minimize blast radius by distributing critical traffic sources across slices,
preventing any single failure from becoming catastrophic. These innovations
work in tandem: regularization ensures algorithmic stability to traffic
allocations while intelligent slicing provides architectural resilience in the
network. Through extensive evaluation on cloud provider WANs, we show Symphony
reduces divergence-induced congestion by 14x and blast radius by 79% compared
to current practice.

</details>


### [232] [GeoPipe: a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network](https://arxiv.org/abs/2510.12064)
*Jun Dai,Xiaorun Wang,Kexiong Fang,Zheng Yang,Yuefeng Ji,Jiawei Zhang*

Main category: cs.NI

TL;DR: 首次展示了基于RDMA的DC-OTN和增强型流水线并行方案，实现高效地理分布式LLM跨数据中心训练。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）参数量呈指数级增长，跨数据中心（DC）训练成为必然趋势，但将单DC训练框架扩展到多DC环境的有效策略仍不成熟。

Method: 实验性地构建了一个高性能地理分布式LLM训练框架，该框架跨越由无损、支持RDMA的数据中心光传输网络（DC-OTN）互联的多个DC。在华为Ascend全栈环境中，实施了增强的流水线并行方案，并通过限制跨DC带宽和高带宽内存（HBM）实现了计算与跨DC通信的重叠。

Result: 该方法有效消除了跨DC通信开销对训练效率的影响，并将计算气泡比降低了高达78.91%。

Conclusion: 成功验证了一个能够显著提升效率的高性能地理分布式LLM训练框架，为解决跨DC训练挑战提供了可行方案。

Abstract: The proliferation of Large Language Models (LLMs) with exponentially growing
parameters is making cross-data center (DC) training an inevitable trend.
However, viable strategies for extending single-DC training frameworks to
multi-DC environments remain underdeveloped. We experimentally demonstrate, for
the first time, a high-performance geo-distributed LLMs training framework
across multiple DCs interconnected by a lossless, remote direct memory access
(RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced
pipeline parallelism scheme is implemented within the Ascend full-stack
environment of Huawei, which effectively eliminates the impact of cross-DC
communication overhead on training efficiency. The overlapped computation and
cross-DC communication is achieved with constraint cross-DC bandwidth and High
Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%.

</details>


### [233] [A Network Digital Twin of a 5G Private Network: Designing a Proof-of-Concept from Theory to Practice](https://arxiv.org/abs/2510.12458)
*Cristina Emilia Costa,Tatenda Horiro Zhou,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 本文展示了一个基于开源仿真软件的实际网络数字孪生体，用于高精度复制实验室中的真实5G私有网络状态和行为，并提供开源。


<details>
  <summary>Details</summary>
Motivation: 网络数字孪生是未来6G网络分析和预测的关键技术，但目前缺乏实际实现案例。

Method: 在实验室的真实5G私有网络上，基于开源网络仿真软件构建了一个网络数字孪生体，并将其开源。

Result: 通过对物理基础设施和数字孪生体进行测量，证明该数字孪生体能高精度地重现实际5G系统的状态和行为。

Conclusion: 成功部署了一个高精度的开源网络数字孪生体，验证了其在真实5G网络中的可行性，填补了理论与实践之间的鸿沟。

Abstract: Network Digital Twins represent a key technology in future networks, expected
to provide the capability to perform accurate analysis and predictions about
the behaviour of 6G mobile networks. However, despite the availability of
several theoretical works on the subject, still very few examples of actual
implementations of Network Digital Twin are available. This paper provides a
detailed description about the characteristics of Network Digital Twin and
provides a practical example about real deployment of the technology. The
considered network infrastructure is a real 5G private network running in a
lab. The Network Digital Twin is built based on open source network emulation
software and is available to the community as open source. Measurements on both
the physical infrastructure and the related Digital Twin demonstrate a high
accuracy in reproducing the state and behavior of the actual 5G system.

</details>


### [234] [AMHRP: Adaptive Multi-Hop Routing Protocol to Improve Network Lifetime for Multi-Hop Wireless Body Area Network](https://arxiv.org/abs/2510.12698)
*Muhammad Mateen Yaqoob,Kulsoom Fatima,Shahab Shamshirband,Amir Mosavi,Waqar Khurshid*

Main category: cs.NI

TL;DR: 本文提出一种WBAN网络协议，旨在提升网络寿命，并优化吞吐量、路径损耗和残余能量等性能。


<details>
  <summary>Details</summary>
Motivation: 提升WBAN（无线体域网）的网络生命周期，并解决吞吐量、路径损耗和残余能量等协议相关问题。

Method: 使用生物传感器部署，采用泊松分布和均衡模型技术。网络拓扑为多跳，节点采用随机部署。

Result: 实现了最低能耗和更长的网络生命周期，并解决了吞吐量、路径损耗和残余能量等问题。

Conclusion: 提出的协议能有效增强WBAN网络的寿命，并优化其在吞吐量、路径损耗和残余能量方面的表现。

Abstract: This paper presents a protocol for enhancement of life time of WBAN network
as well other protocol related issues such as throughput, path loss, and
residual energy. Bio-sensors are used for deployment on human body. Poisson
distribution and equilibrium model techniques have been used for attaining the
required results. Multi-hop network topology and random network node deployment
used to achieve minimum energy consumption and longer network lifetime.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [235] [Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need](https://arxiv.org/abs/2510.11734)
*Yuqi Bai,Tianyu Huang,Kun Sun,Yuting Chen*

Main category: cs.CY

TL;DR: 本研究利用大型语言模型（LLM）模拟社会实验中的人格，开发了新的评估框架并修改了心理测量方法。研究发现角色细节对人格模拟质量至关重要，并提出了LLM人格模拟的“尺度定律”。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在虚拟角色扮演中模拟人类人格进行社会实验的能力。同时，解决传统心理测量方法在评估LLM当前低水平模拟时可能出现的不足。

Method: 开发了一个端到端评估框架，包括个体层面的稳定性、可识别性分析和群体层面的“渐进式人格曲线”分析。对传统的心理测量方法（如CFA和构念效度）进行了重要修改，以更好地适应LLM的模拟特点。

Result: 提出了一个系统性的LLM虚拟人格评估框架。实证证明了角色细节对人格模拟质量的关键作用。识别了角色档案的边际效用效应，特别是在LLM人格模拟中存在“尺度定律”。

Conclusion: 本研究为大型语言模型在社会科学实验中的应用提供了操作性评估指标和理论基础，强调了角色细节的重要性，并揭示了LLM人格模拟的“尺度定律”。

Abstract: This research focuses on using large language models (LLMs) to simulate
social experiments, exploring their ability to emulate human personality in
virtual persona role-playing. The research develops an end-to-end evaluation
framework, including individual-level analysis of stability and
identifiability, as well as population-level analysis called progressive
personality curves to examine the veracity and consistency of LLMs in
simulating human personality. Methodologically, this research proposes
important modifications to traditional psychometric approaches (CFA and
construct validity) which are unable to capture improvement trends in LLMs at
their current low-level simulation, potentially leading to remature rejection
or methodological misalignment. The main contributions of this research are:
proposing a systematic framework for LLM virtual personality evaluation;
empirically demonstrating the critical role of persona detail in personality
simulation quality; and identifying marginal utility effects of persona
profiles, especially a Scaling Law in LLM personality simulation, offering
operational evaluation metrics and a theoretical foundation for applying large
language models in social science experiments.

</details>


### [236] [Artificial Intelligence for Optimal Learning: A Comparative Approach towards AI-Enhanced Learning Environments](https://arxiv.org/abs/2510.11755)
*Ananth Hariharan*

Main category: cs.CY

TL;DR: 本研究旨在比较传统、非AI技术增强和AI驱动的教育模式，以期综合各优势，构建一个更全面、高效且公平的混合式教育框架。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，尤其是AI的兴起，技术已成为教育策略的基石。研究动机在于评估不同技术整合水平（无技术、非AI技术、AI技术）对教育成果、参与度、教学方法和学习资源公平性的影响，以期开发更优的教育模式。

Method: 通过批判性评估和比较三种不同的教育环境：传统教育方法、非AI技术增强的教育环境以及利用AI驱动技术的教育环境。评估指标包括教育成果、学生参与度、教学方法和学习资源获取的公平性。

Result: 本研究的目标是综合各模式的优势，提出一种混合式教育方法。该方法将结合传统课堂的人际互动和既有教学法、非AI技术提供的可及性和协作工具，以及AI技术实现的个性化自适应学习策略，以期创建更丰富、有效的学习环境。

Conclusion: 通过整合各种教育模式的最佳元素，所提出的混合式教育方法旨在提升教育成果、学生参与度和包容性，同时应对各模式固有的挑战与局限，最终目标是创建一个关注学生多样化需求并确保高质量教育公平可及的教育框架。

Abstract: In the rapidly evolving educational landscape, the integration of technology
has shifted from an enhancement to a cornerstone of educational strategy
worldwide. This transition is propelled by advancements in digital technology,
especially the emergence of artificial intelligence as a crucial tool in
learning environments. This research project critically evaluates the impact of
three distinct educational settings: traditional educational methods without
technological integration, those enhanced by non-AI technology, and those
utilising AI-driven technologies. This comparison aims to assess how each
environment influences educational outcomes, engagement, pedagogical methods,
and equity in access to learning resources, and how each contributes uniquely
to the learning experience. The ultimate goal of this research is to synthesise
the strengths of each model to create a more holistic educational approach. By
integrating the personal interaction and tested pedagogical techniques of
traditional classrooms, the enhanced accessibility and collaborative tools
offered by non-AI technology, and the personalised, adaptive learning
strategies enabled by AI-driven technologies, education systems can develop
richer, more effective learning environments. This hybrid approach aims to
leverage the best elements of each setting, thereby enhancing educational
outcomes, engagement, and inclusiveness, while also addressing the distinct
challenges and limitations inherent in each model. The intention is to create
an educational framework deeply attentive to the diverse needs of students,
ensuring equitable access to high-quality education for all.

</details>


### [237] [The Adoption Paradox: A Comparative Analysis of Veterinary AI Adoption in China and the North America](https://arxiv.org/abs/2510.11758)
*Shumin Li,Xiaoyun Lai*

Main category: cs.CY

TL;DR: 本研究比较了中北美兽医对AI的感知、采纳和应用。中国兽医采纳率高但熟悉度低，侧重临床应用；北美兽医熟悉度高但采纳率低，侧重行政管理。研究揭示了“采纳悖论”，强调了AI整合需采取区域定制策略。


<details>
  <summary>Details</summary>
Motivation: 比较中国和北美兽医专业人员对人工智能（AI）的感知、采纳和应用，并验证采纳模式是否受区域市场和人口因素影响的假设。

Method: 对455名中国兽医专业人员进行了描述性横断面调查（2025年5月至7月），并与已发表的2024年3,968名美国和加拿大兽医的调查数据进行对比分析。

Result: 中国兽医（主要为临床医生）AI采纳率高（71.0%）但熟悉度低（55.4%），主要用于疾病诊断（50.1%）和处方计算（44.8%）等临床任务。北美兽医熟悉度高（83.8%）但采纳率低（39.2%），优先用于影像分析（39.0%）和记录管理（39.0%）等行政任务。AI可靠性和准确性是两组共同的主要障碍，揭示了“采纳悖论”。

Conclusion: 中国市场展现出实践者驱动、自下而上以增强临床效率为中心的AI采纳模式；北美市场则表现为更谨慎、结构化、自上而下以提高行政效率为目标的整合模式。这表明AI开发和整合不应采用“一刀切”方法，而应根据区域特性制定量身定制的策略。

Abstract: This study compares the perception, adoption, and application of artificial
intelligence (AI) among veterinary professionals in China and North America
(NA), testing the hypothesis that adoption patterns are shaped by regional
market and demographic factors. A descriptive, cross-sectional survey was
conducted with 455 veterinary professionals in China between May and July 2025.
The results were compared with published data from a 2024 survey of 3,968
veterinary professionals in the United States and Canada. The Chinese cohort,
primarily composed of clinicians (81.5%), showed a high AI adoption rate
(71.0%) despite low familiarity (55.4%). Their AI use was focused on clinical
tasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%).
In contrast, the NA cohort reported high familiarity (83.8%) but a lower
adoption rate (39.2%). Their priorities were administrative, including imaging
analysis (39.0%) and record-keeping (39.0%). Concerns about AI reliability and
accuracy were the top barrier in both groups. Our findings reveal an "adoption
paradox" where the Chinese market demonstrates a practitioner-driven, bottom-up
adoption model focused on augmenting clinical efficacy, while the NA market
shows a more cautious, structured, top-down integration aimed at improving
administrative efficiency. This suggests that a one-size-fits-all approach to
AI development and integration is insufficient, and tailored, region-specific
strategies are necessary to responsibly incorporate AI into global veterinary
practice.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [238] [Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring](https://arxiv.org/abs/2503.20934)
*Fraol Batole,Abhiram Bellur,Malinda Dilhara,Mohammed Raihan Ullah,Yaroslav Zharov,Timofey Bryksin,Kai Ishikawa,Haifeng Chen,Masaharu Morimoto,Shota Motoura,Takeo Hosomi,Tien N. Nguyen,Hridesh Rajan,Nikolaos Tsantalis,Danny Dig*

Main category: cs.SE

TL;DR: MOVEMETHOD重构工具与专家不符，而大型语言模型（LLMs）虽有专家建议潜力，却存在高达80%的幻觉和上下文限制。本文提出MM-assist，一个利用静态分析、LLM自洽性及RAG技术，全自动化MOVEMETHOD重构的LLM辅助工具。实验表明，MM-assist显著优于现有SOTA方法，召回率提升1.7x至2.4x，用户研究中82.8%的建议获得积极评价，证明其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有MOVEMETHOD重构工具的推荐与专家开发者的实践不符。尽管大型语言模型（LLMs）在代码的自然性方面具有优势，有望提供专家级的重构建议，但初步研究发现LLMs的建议高达80%为幻觉，且其有限的上下文大小阻碍了全局项目级推理，使其不可靠。

Method: 引入MM-assist，一个LLM驱动的MOVEMETHOD重构助手，自动化重构的端到端生命周期。通过以下新颖解决方案解决LLM的局限性：1) 使用IDE的静态分析自动过滤LLM幻觉。2) 设计要求LLM自洽、批判并排序重构建议的工作流。3) 针对MOVEMETHOD重构所需的全局项目级推理，采用重构感知的检索增强生成（RAG）解决LLM上下文大小限制。MM-assist协同结合了LLM、IDE、静态分析和语义关联的优势。

Result: 通过多方法实证评估，MM-assist显著优于现有最先进方法：1) 在广泛使用的基准测试上，Recall@1和Recall@3提升了1.7倍。2) 在包含210个开源软件最新重构的语料库上，召回率提升至少2.4倍。3) 30名经验丰富的参与者使用MM-assist进行一周重构的用户研究显示，82.8%的MM-assist推荐获得了积极评价。

Conclusion: MM-assist在MOVEMETHOD重构方面显著优于以往的先进方法，证明了其既有效又实用。

Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools
that recommend which methods to move and where, these recommendations do not
align with how expert developers perform MOVEMETHOD. Given the extensive
training of Large Language Models and their reliance upon naturalness of code,
they should expertly recommend which methods are misplaced in a given class and
which classes are better hosts. Our formative study of 2016 LLM recommendations
revealed that LLMs give expert suggestions, yet they are unreliable: up to 80%
of the suggestions are hallucinations. We introduce the first LLM fully powered
assistant for MOVEMETHOD refactoring that automates its whole end-to-end
lifecycle, from recommendation to execution. We designed novel solutions that
automatically filter LLM hallucinations using static analysis from IDEs and a
novel workflow that requires LLMs to be self-consistent, critique, and rank
refactoring suggestions. As MOVEMETHOD refactoring requires global,
projectlevel reasoning, we solved the limited context size of LLMs by employing
refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,
synergistically combines the strengths of the LLM, IDE, static analysis, and
semantic relevance. In our thorough, multi-methodology empirical evaluation, we
compare MM-assist with the previous state-of-the-art approaches. MM-assist
significantly outperforms them: (i) on a benchmark widely used by other
researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a
corpus of 210 recent refactorings from Open-source software, our Recall rates
improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced
participants who used MM-assist to refactor their own code for one week. They
rated 82.8% of MM-assist recommendations positively. This shows that MM-assist
is both effective and useful.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [239] [Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication](https://arxiv.org/abs/2510.12265)
*Sami Khairy,Gabriel Mittag,Vishak Gopal,Ross Cutler*

Main category: cs.MM

TL;DR: 针对视频会议QoE受带宽估计影响的挑战，本文提出一种结合用户主观评估、百万级真实数据驱动的分布式离线强化学习框架，有效降低了Microsoft Teams的通话差评率11.41%，并展现了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视频会议系统的体验质量（QoE）受制于对时变可用带宽的精确估计。由于网络架构迅速演进、协议栈日益复杂，以及难以可靠定义改善用户体验的QoE指标，实时通信的带宽估计仍是一个开放性挑战。

Method: 1. 训练基于用户主观评估的客观QoE奖励模型，以实时衡量音视频质量。 2. 从真实Microsoft Teams通话中收集约100万条包含客观QoE奖励的网络轨迹，构建带宽估计训练数据集。 3. 引入一种新颖的分布式离线强化学习（RL）算法，训练一个基于神经网络的带宽估计器，旨在提升用户QoE。 整个框架是已部署的、人机协作的、数据驱动的。

Result: 1. 在真实世界的A/B测试中，所提出的方法与基线带宽估计器相比，将主观差评通话比例降低了11.41%。 2. 所提出的离线RL算法在D4RL任务上进行了基准测试，展示了其超越带宽估计的泛化能力。

Conclusion: 该研究通过结合用户主观评估、大规模真实数据和新颖的分布式离线强化学习算法，成功构建了一个高效的带宽估计框架。该框架显著提升了视频会议的体验质量，尤其在实际应用中将差评通话比例降低了11.41%，并证明了其泛化能力，为解决实时通信带宽估计的挑战提供了有效方案。

Abstract: The quality of experience (QoE) delivered by video conferencing systems is
significantly influenced by accurately estimating the time-varying available
bandwidth between the sender and receiver. Bandwidth estimation for real-time
communications remains an open challenge due to rapidly evolving network
architectures, increasingly complex protocol stacks, and the difficulty of
defining QoE metrics that reliably improve user experience. In this work, we
propose a deployed, human-in-the-loop, data-driven framework for bandwidth
estimation to address these challenges. Our approach begins with training
objective QoE reward models derived from subjective user evaluations to measure
audio and video quality in real-time video conferencing systems. Subsequently,
we collect roughly $1$M network traces with objective QoE rewards from
real-world Microsoft Teams calls to curate a bandwidth estimation training
dataset. We then introduce a novel distributional offline reinforcement
learning (RL) algorithm to train a neural-network-based bandwidth estimator
aimed at improving QoE for users. Our real-world A/B test demonstrates that the
proposed approach reduces the subjective poor call ratio by $11.41\%$ compared
to the baseline bandwidth estimator. Furthermore, the proposed offline RL
algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond
bandwidth estimation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [240] [Fast and Interpretable Protein Substructure Alignment via Optimal Transport](https://arxiv.org/abs/2510.11752)
*Zhiyu Wang,Bingxin Zhou,Jing Wang,Yang Tan,Weishu Zhao,Pietro Liò,Liang Hong*

Main category: q-bio.QM

TL;DR: 本研究提出了PLASMA，首个用于高效且可解释的残基级蛋白质子结构比对的深度学习框架，通过优化传输任务解决现有计算方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法难以识别和比较蛋白质结构中的局部基序，阻碍了对蛋白质结构功能的理解和蛋白质工程，因此需要更有效的方法来弥补这一空白。

Method: 开发了PLASMA，一个深度学习框架，将蛋白质子结构比对问题重新表述为正则化最优传输任务，并利用可微分的Sinkhorn迭代。此外，还引入了PLASMA-PF，一个无需训练的变体。

Result: 通过广泛的定量评估和生物学案例研究，PLASMA实现了准确、轻量级和可解释的残基级比对。PLASMA-PF则在训练数据不可用时提供了一个实用的替代方案。

Conclusion: PLASMA方法解决了蛋白质结构分析工具中的一个关键空白，为功能注释、进化研究和基于结构的药物设计提供了新机遇。

Abstract: Proteins are essential biological macromolecules that execute life functions.
Local motifs within protein structures, such as active sites, are the most
critical components for linking structure to function and are key to
understanding protein evolution and enabling protein engineering. Existing
computational methods struggle to identify and compare these local structures,
which leaves a significant gap in understanding protein structures and
harnessing their functions. This study presents PLASMA, the first deep learning
framework for efficient and interpretable residue-level protein substructure
alignment. We reformulate the problem as a regularized optimal transport task
and leverage differentiable Sinkhorn iterations. For a pair of input protein
structures, PLASMA outputs a clear alignment matrix with an interpretable
overall similarity score. Through extensive quantitative evaluations and three
biological case studies, we demonstrate that PLASMA achieves accurate,
lightweight, and interpretable residue-level alignment. Additionally, we
introduce PLASMA-PF, a training-free variant that provides a practical
alternative when training data are unavailable. Our method addresses a critical
gap in protein structure analysis tools and offers new opportunities for
functional annotation, evolutionary studies, and structure-based drug design.
Reproducibility is ensured via our official implementation at
https://github.com/ZW471/PLASMA-Protein-Local-Alignment.git.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [241] [Modeling Hypergraph Using Large Language Models](https://arxiv.org/abs/2510.11728)
*Bingqiao Gu,Jiale Zeng,Xingqin Qi,Dong Li*

Main category: cs.SI

TL;DR: 本文提出HyperLLM，一个由大型语言模型（LLM）驱动的超图生成器，通过多智能体协作模拟超图的形成和演化，以解决真实超图数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有超图数据集在规模和多样性上均稀缺，严重限制了先进超图学习算法的开发和评估。因此，快速生成符合真实网络特性的、大规模超图是一个关键但未受足够重视的任务。

Method: 利用LLMs在语义推理、结构化生成和模拟人类行为方面的能力，引入HyperLLM，一个LLM驱动的超图生成器。该框架通过多智能体协作模拟超图的形成和演化，并集成提示和结构反馈机制以确保生成的超图反映真实世界的关键模式。

Result: 在多样化数据集上的广泛实验表明，HyperLLM在结构和时间超图模式方面实现了卓越的保真度，且仅需最少的统计先验知识。

Conclusion: 研究结果表明，基于LLM的框架为超图建模提供了一个有前景的新方向。

Abstract: Due to the advantages of hypergraphs in modeling high-order relationships in
complex systems, they have been applied to higher-order clustering, hypergraph
neural networks and computer vision. These applications rely heavily on access
to high-quality, large-scale real-world hypergraph data. Yet, compared to
traditional pairwise graphs, real hypergraph datasets remain scarce in both
scale and diversity. This shortage significantly limits the development and
evaluation of advanced hypergraph learning algorithms. Therefore, how to
quickly generate large-scale hypergraphs that conform to the characteristics of
real networks is a crucial task that has not received sufficient attention.
Motivated by recent advances in large language models (LLMs), particularly
their capabilities in semantic reasoning, structured generation, and simulating
human behavior, we investigate whether LLMs can facilitate hypergraph
generation from a fundamentally new perspective. We introduce HyperLLM, a novel
LLM-driven hypergraph generator that simulates the formation and evolution of
hypergraphs through a multi-agent collaboration. The framework integrates
prompts and structural feedback mechanisms to ensure that the generated
hypergraphs reflect key real-world patterns. Extensive experiments across
diverse datasets demonstrate that HyperLLM achieves superior fidelity to
structural and temporal hypergraph patterns, while requiring minimal
statistical priors. Our findings suggest that LLM-based frameworks offer a
promising new direction for hypergraph modeling.

</details>


### [242] [Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed](https://arxiv.org/abs/2510.11739)
*Muhammad Hamza,Rizwan Jafar*

Main category: cs.SI

TL;DR: 本研究利用机器学习和深度学习技术，通过分析粉丝的乌尔都语推文来预测名人的人口统计学信息，弥补了低资源语言在此领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于英语等高资源语言的名人画像，而乌尔都语作为低资源语言，在此领域鲜有探索。本研究旨在弥补这一空白，探究利用粉丝语言数据对乌尔都语名人进行人口统计学预测的可能性。

Method: 研究收集并预处理了南亚次大陆名人粉丝的乌尔都语短推文数据集。采用了逻辑回归、支持向量机、随机森林、卷积神经网络和长短期记忆网络等多种机器学习和深度学习算法进行模型训练，并使用准确率、精确度、召回率、F1分数和累积排名（cRank）进行评估。

Result: 性别预测表现最佳，cRank和准确率均达到0.65。年龄、职业和名气预测结果中等。

Conclusion: 研究证明，利用机器学习和神经网络方法，可以有效利用基于粉丝的语言特征对乌尔都语（一种低资源语言）进行人口统计学预测。

Abstract: Social media has become an essential part of the digital age, serving as a
platform for communication, interaction, and information sharing. Celebrities
are among the most active users and often reveal aspects of their personal and
professional lives through online posts. Platforms such as Twitter provide an
opportunity to analyze language and behavior for understanding demographic and
social patterns. Since followers frequently share linguistic traits and
interests with the celebrities they follow, textual data from followers can be
used to predict celebrity demographics. However, most existing research in this
field has focused on English and other high-resource languages, leaving Urdu
largely unexplored.
  This study applies modern machine learning and deep learning techniques to
the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from
followers of subcontinent celebrities was collected and preprocessed. Multiple
algorithms were trained and compared, including Logistic Regression, Support
Vector Machines, Random Forests, Convolutional Neural Networks, and Long
Short-Term Memory networks. The models were evaluated using accuracy,
precision, recall, F1-score, and cumulative rank (cRank). The best performance
was achieved for gender prediction with a cRank of 0.65 and an accuracy of
0.65, followed by moderate results for age, profession, and fame prediction.
These results demonstrate that follower-based linguistic features can be
effectively leveraged using machine learning and neural approaches for
demographic prediction in Urdu, a low-resource language.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [243] [Over-Threshold Multiparty Private Set Intersection for Collaborative Network Intrusion Detection](https://arxiv.org/abs/2510.12045)
*Onur Eren Arpaci,Raouf Boutaba,Florian Kerschbaum*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: An important function of collaborative network intrusion detection is to
analyze the network logs of the collaborators for joint IP addresses. However,
sharing IP addresses in plain is sensitive and may be even subject to privacy
legislation as it is personally identifiable information. In this paper, we
present the privacy-preserving collection of IP addresses. We propose a single
collector, over-threshold private set intersection protocol. In this protocol
$N$ participants identify the IP addresses that appear in at least $t$
participant's sets without revealing any information about other IP addresses.
Using a novel hashing scheme, we reduce the computational complexity of the
previous state-of-the-art solution from $O(M(N \log{M}/t)^{2t})$ to
$O(t^2M\binom{N}{t})$, where $M$ denotes the dataset size. This reduction makes
it practically feasible to apply our protocol to real network logs. We test our
protocol using joint networks logs of multiple institutions. Additionally, we
present two deployment options: a collusion-safe deployment, which provides
stronger security guarantees at the cost of increased communication overhead,
and a non-interactive deployment, which assumes a non-colluding collector but
offers significantly lower communication costs and applicable to many use cases
of collaborative network intrusion detection similar to ours.

</details>


### [244] [Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds](https://arxiv.org/abs/2510.12629)
*Gunwoo Kim,Taejune Park,Jinwoo Kim*

Main category: cs.CR

TL;DR: 本文分析了容器云环境中RDMA网卡（RNIC）的两种资源耗尽攻击（状态饱和与流水线饱和），发现它们可导致严重性能下降。为此，提出了HT-Verbs，一个基于软件的阈值驱动框架，用于缓解这些攻击并恢复性能隔离。


<details>
  <summary>Details</summary>
Motivation: 现代容器化云环境采用RDMA以降低CPU开销并实现高性能数据交换，但其性能隔离性难以保证，一个容器的RDMA工作负载可能降低其他容器的性能，威胁关键安全保障。现有隔离技术因RNIC微架构资源管理的复杂性而难以有效应用。

Method: 本文实验分析了在NVIDIA BlueField-3上两种RDMA网卡（RNIC）资源耗尽攻击：(i) 状态饱和攻击和 (ii) 流水线饱和攻击。为缓解这些威胁，提出了HT-Verbs框架，这是一个基于实时每容器RDMA verb遥测和自适应资源分类的阈值驱动框架，通过将RNIC资源划分为热、温、冷三层，并在不修改硬件的情况下限制滥用工作负载。

Result: 实验结果显示，状态饱和攻击可导致受害者容器带宽损失高达93.9%，延迟增加1,117倍，缓存未命中率上升115%。流水线饱和攻击则导致严重的链路级拥塞和显著的放大效应（小请求导致不成比例的高资源消耗）。HT-Verbs框架能够有效缓解这些威胁，恢复可预测的安全保障。

Conclusion: 本研究揭示了RDMA网卡中微架构资源耗尽（状态饱和与流水线饱和）对容器云环境性能隔离和安全保障的严重影响。为应对此挑战，提出了HT-Verbs，一个无需硬件修改的阈值驱动框架，通过实时遥测和自适应资源分类，能够有效缓解攻击并恢复可预测的安全保障。

Abstract: In modern containerized cloud environments, the adoption of RDMA (Remote
Direct Memory Access) has expanded to reduce CPU overhead and enable
high-performance data exchange. Achieving this requires strong performance
isolation to ensure that one container's RDMA workload does not degrade the
performance of others, thereby maintaining critical security assurances.
However, existing isolation techniques are difficult to apply effectively due
to the complexity of microarchitectural resource management within RDMA NICs
(RNICs). This paper experimentally analyzes two types of resource exhaustion
attacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline
saturation attacks. Our results show that state saturation attacks can cause up
to a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in
cache misses for victim containers, while pipeline saturation attacks lead to
severe link-level congestion and significant amplification, where small verb
requests result in disproportionately high resource consumption. To mitigate
these threats and restore predictable security assurances, we propose HT-Verbs,
a threshold-driven framework based on real-time per-container RDMA verb
telemetry and adaptive resource classification that partitions RNIC resources
into hot, warm, and cold tiers and throttles abusive workloads without
requiring hardware modifications.

</details>


### [245] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce是一个开源容器化工具包，旨在简化大型语言模型和机器学习模型的红队测试，通过整合14个工具并提供统一界面，降低AI安全评估的门槛。


<details>
  <summary>Details</summary>
Motivation: AI模型广泛应用引发安全担忧，但AI红队测试面临工具选择困难、依赖管理复杂及专业团队缺乏等挑战，急需标准化、易用的环境来降低评估门槛。

Method: 引入BlackIce，一个受Kali Linux启发的开源容器化工具包。它提供一个包含14个精选负责任AI和安全测试工具的Docker镜像，通过统一命令行界面访问，实现本地或云端一键部署和执行红队评估，并支持模块化扩展。

Result: BlackIce成功构建了一个可复现、版本固定的AI红队测试环境，简化了工具选择和依赖管理，显著降低了AI模型安全评估的复杂性和门槛，并促进了社区驱动的扩展。

Conclusion: BlackIce通过提供统一、容器化的工具包，有效解决了AI红队测试的现有痛点，大大降低了进入门槛，使组织能够更便捷、系统地进行AI模型安全与负责任AI评估。

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [246] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: AwareCompiler是一个基于LLM的代理框架，通过结构化知识整合、知识驱动的自适应优化过程生成和数据驱动的混合训练，解决了编译器优化中语义不匹配、交互低效和奖励稀疏等挑战，显著提升了程序性能和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）代理在软件优化方面潜力巨大，但自动化编译器优化仍面临挑战：1) 抽象程序表示与具体优化过程的语义不匹配；2) 代理与编译器环境的交互机制效率低下；3) 大型优化空间中决策过程冗长导致的奖励稀疏问题。

Method: 本文提出了AwareCompiler框架，通过以下三项关键创新来解决上述挑战：1) 结构化知识集成与数据集构建；2) 知识驱动的自适应优化过程生成；3) 数据驱动的混合训练流程。

Result: 在标准基准测试中，AwareCompiler在性能和效率上均显著优于现有基线，证明了其知识与数据协同驱动方法的有效性。

Conclusion: AwareCompiler通过创新的知识与数据协同驱动方法，成功克服了LLM代理在编译器优化中的挑战，为程序性能优化提供了更有效和高效的解决方案。

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [247] [Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning](https://arxiv.org/abs/2510.11754)
*Dongrong Yang,Xin Wu,Yibo Xie,Xinyi Li,Qiuwen Wu,Jackie Wu,Yang Sheng*

Main category: physics.med-ph

TL;DR: 本研究提出并验证了一个基于大型语言模型（LLM）的智能代理，用于零样本自动化调强放射治疗（IMRT）计划，其在头颈部癌症病例中表现出与临床手动计划相当甚至更优的剂量学结果。


<details>
  <summary>Details</summary>
Motivation: 放射治疗计划是迭代且依赖专业知识的过程，日益增长的癌症病例使得手动计划变得不可持续，亟需自动化解决方案。

Method: 研究开发了一个LLM代理工作流程，使其直接与临床治疗计划系统（TPS）交互，迭代提取中间计划状态并提出新的约束值以指导逆向优化。该代理在零样本推理设置下运行，未经过手动计划训练或微调，并在二十例头颈部癌症病例上进行评估。

Result: LLM生成的计划在危及器官（OAR）保护方面与临床计划相当，同时显著改善了热点控制（Dmax: 106.5% vs. 108.8%）和靶区适形度（boost PTV适形指数: 1.18 vs. 1.39；primary PTV: 1.82 vs. 1.88）。

Conclusion: 本研究证明了在商业TPS中实现零样本、LLM驱动的IMRT自动化治疗计划的可行性，为提供通用且临床适用的解决方案，减少计划变异性并支持AI规划策略的广泛应用奠定基础。

Abstract: Radiation therapy treatment planning is an iterative, expertise-dependent
process, and the growing burden of cancer cases has made reliance on manual
planning increasingly unsustainable, underscoring the need for automation. In
this study, we propose a workflow that leverages a large language model
(LLM)-based agent to navigate inverse treatment planning for
intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to
directly interact with a clinical treatment planning system (TPS) to
iteratively extract intermediate plan states and propose new constraint values
to guide inverse optimization. The agent's decision-making process is informed
by current observations and previous optimization attempts and evaluations,
allowing for dynamic strategy refinement. The planning process was performed in
a zero-shot inference setting, where the LLM operated without prior exposure to
manually generated treatment plans and was utilized without any fine-tuning or
task-specific training. The LLM-generated plans were evaluated on twenty
head-and-neck cancer cases against clinical manual plans, with key dosimetric
endpoints analyzed and reported. The LLM-generated plans achieved comparable
organ-at-risk (OAR) sparing relative to clinical plans while demonstrating
improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity
(conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV).
This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for
automated IMRT treatment planning in a commercial TPS. The proposed approach
provides a generalizable and clinically applicable solution that could reduce
planning variability and support broader adoption of AI-based planning
strategies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [248] [Serial-Parallel Dual-Path Architecture for Speaking Style Recognition](https://arxiv.org/abs/2510.11732)
*Guojian Li,Qijie Shao,Zhixian Zhao,Shuiyuan Wang,Zhonghua Fu,Lei Xie*

Main category: cs.SD

TL;DR: 提出一种新型串并联双路径架构，融合声学与语言双模态信息，显著提升说话风格识别准确率并大幅减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有说话风格识别方法过度依赖语言信息，对声学信息整合不足，限制了识别准确率的提升。融合声学与语言模态具有提高性能的巨大潜力。

Method: 本文提出一种新颖的串并联双路径架构，以利用声学-语言双模态信息。串联路径遵循ASR+STYLE范式处理序列时间依赖，而并联路径则集成设计的声学-语言相似性模块(ALSM)，以促进跨模态的时间同步交互。

Result: 相较于现有OSUM基线模型，该方法将参数量减少了88.4%，并在测试集上八种风格的说话风格识别准确率上实现了30.3%的提升。

Conclusion: 所提出的串并联双路径架构有效融合了声学与语言信息，显著提高了说话风格识别的准确性，同时实现了模型尺寸的大幅优化。

Abstract: Speaking Style Recognition (SSR) identifies a speaker's speaking style
characteristics from speech. Existing style recognition approaches primarily
rely on linguistic information, with limited integration of acoustic
information, which restricts recognition accuracy improvements. The fusion of
acoustic and linguistic modalities offers significant potential to enhance
recognition performance. In this paper, we propose a novel serial-parallel
dual-path architecture for SSR that leverages acoustic-linguistic bimodal
information. The serial path follows the ASR+STYLE serial paradigm, reflecting
a sequential temporal dependency, while the parallel path integrates our
designed Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal
interaction with temporal simultaneity. Compared to the existing SSR baseline
-- the OSUM model, our approach reduces parameter size by 88.4% and achieves a
30.3% improvement in SSR accuracy for eight styles on the test set.

</details>


### [249] [SeeingSounds: Learning Audio-to-Visual Alignment via Text](https://arxiv.org/abs/2510.11738)
*Simone Carnemolla,Matteo Pennisi,Chiara Russo,Simone Palazzo,Daniela Giordano,Concetto Spampinato*

Main category: cs.SD

TL;DR: SeeingSounds是一个轻量级音频到图像生成框架，通过音频、语言和视觉的双重对齐，无需配对数据或训练大型视觉模型，实现了可控的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频到图像生成方法通常需要配对的音频-视觉数据或对视觉生成模型进行大量训练。本研究旨在开发一种无需这些限制、受认知神经科学启发、能反映人类感知中跨模态关联的方法，并提供精细化控制。

Method: 该方法名为SeeingSounds，采用双重对齐机制：音频通过冻结的语言编码器投射到语义语言空间，然后使用视觉-语言模型在视觉领域进行语境化。模型基于冻结的扩散主干网络，仅训练轻量级适配器。它还通过程序化文本提示生成实现精细和可解释的控制，将音频转换转化为描述性提示来指导视觉输出。

Result: 在标准基准测试中，SeeingSounds在零样本和监督设置下均优于现有方法，在可控音频到视觉生成领域建立了新的最先进水平。

Conclusion: SeeingSounds证明了利用音频、语言和视觉的协同作用，可以在不依赖配对数据或训练大型视觉模型的情况下，实现高效、可扩展且可控的音频到图像生成，并取得了SOTA性能。

Abstract: We introduce SeeingSounds, a lightweight and modular framework for
audio-to-image generation that leverages the interplay between audio, language,
and vision-without requiring any paired audio-visual data or training on visual
generative models. Rather than treating audio as a substitute for text or
relying solely on audio-to-text mappings, our method performs dual alignment:
audio is projected into a semantic language space via a frozen language
encoder, and, contextually grounded into the visual domain using a
vision-language model. This approach, inspired by cognitive neuroscience,
reflects the natural cross-modal associations observed in human perception. The
model operates on frozen diffusion backbones and trains only lightweight
adapters, enabling efficient and scalable learning. Moreover, it supports
fine-grained and interpretable control through procedural text prompt
generation, where audio transformations (e.g., volume or pitch shifts)
translate into descriptive prompts (e.g., "a distant thunder") that guide
visual outputs. Extensive experiments across standard benchmarks confirm that
SeeingSounds outperforms existing methods in both zero-shot and supervised
settings, establishing a new state of the art in controllable audio-to-visual
generation.

</details>


### [250] [Audio-Guided Visual Perception for Audio-Visual Navigation](https://arxiv.org/abs/2510.11760)
*Yi Wang,Yinfeng Yu,Fuchun Sun,Liejun Wang,Wendong Zheng*

Main category: cs.SD

TL;DR: 提出AGVP框架，通过显式跨模态对齐将声音转化为空间指引，显著提升了音频-视觉具身导航在未知音源和环境中的泛化能力、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉具身导航方法在面对未知音源或环境时泛化能力差，导致导航成功率低且路径过长。这主要是因为缺乏听觉信号与视觉区域的显式对齐机制，使得策略容易记忆虚假声学指纹与场景的关联，而非真正理解声音的空间意义。

Method: 提出AGVP框架，该框架首先通过音频自注意力机制提取全局听觉上下文，然后将此上下文作为查询来指导视觉特征注意力，从而在特征层面突出与声源相关的视觉区域。在此基础上进行时序建模和策略优化。这种设计通过可解释的跨模态对齐和区域重加权，将声音从记忆性声学指纹转化为空间指引，减少了对特定声学指纹的依赖。

Result: 实验结果表明，AGVP框架在提高导航效率和鲁棒性的同时，对以前未听过的声音展现出卓越的跨场景泛化能力。

Conclusion: AGVP框架通过创新的跨模态对齐和区域重加权设计，有效解决了音频-视觉具身导航在未知音源和环境下的泛化能力难题，使得智能体能够更高效、鲁棒地导航到声源。

Abstract: Audio-Visual Embodied Navigation aims to enable agents to autonomously
navigate to sound sources in unknown 3D environments using auditory cues. While
current AVN methods excel on in-distribution sound sources, they exhibit poor
cross-source generalization: navigation success rates plummet and search paths
become excessively long when agents encounter unheard sounds or unseen
environments. This limitation stems from the lack of explicit alignment
mechanisms between auditory signals and corresponding visual regions. Policies
tend to memorize spurious \enquote{acoustic fingerprint-scenario} correlations
during training, leading to blind exploration when exposed to novel sound
sources. To address this, we propose the AGVP framework, which transforms sound
from policy-memorable acoustic fingerprint cues into spatial guidance. The
framework first extracts global auditory context via audio self-attention, then
uses this context as queries to guide visual feature attention, highlighting
sound-source-related regions at the feature level. Subsequent temporal modeling
and policy optimization are then performed. This design, centered on
interpretable cross-modal alignment and region reweighting, reduces dependency
on specific acoustic fingerprints. Experimental results demonstrate that AGVP
improves both navigation efficiency and robustness while achieving superior
cross-scenario generalization on previously unheard sounds.

</details>
