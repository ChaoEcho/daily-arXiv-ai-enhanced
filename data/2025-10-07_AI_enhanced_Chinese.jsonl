{"id": "2510.03438", "pdf": "https://arxiv.org/pdf/2510.03438", "abs": "https://arxiv.org/abs/2510.03438", "authors": ["Grace Ra Kim", "Duncan Eddy", "Vedant Srinivas", "Mykel J. Kochenderfer"], "title": "Scalable Ground Station Selection for Large LEO Constellations", "categories": ["cs.NI", "cs.AI", "cs.SY", "eess.SY"], "comment": "14 pages, 7 tables, 10 figures, submitted to IEEE Aeroconf 2026", "summary": "Effective ground station selection is critical for low Earth orbiting (LEO)\nsatellite constellations to minimize operational costs, maximize data downlink\nvolume, and reduce communication gaps between access windows. Traditional\nground station selection typically begins by choosing from a fixed set of\nlocations offered by Ground Station-as-a-Service (GSaaS) providers, which helps\nreduce the problem scope to optimizing locations over existing infrastructure.\nHowever, finding a globally optimal solution for stations using existing\nmixed-integer programming methods quickly becomes intractable at scale,\nespecially when considering multiple providers and large satellite\nconstellations. To address this issue, we introduce a scalable, hierarchical\nframework that decomposes the global selection problem into single-satellite,\nshort time-window subproblems. Optimal station choices from each subproblem are\nclustered to identify consistently high-value locations across all decomposed\ncases. Cluster-level sets are then matched back to the closest GSaaS candidate\nsites to produce a globally feasible solution. This approach enables scalable\ncoordination while maintaining near-optimal performance. We evaluate our\nmethod's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10\nstations), achieving solutions within 95% of the global IP optimum for all test\ncases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and\nPlanet's Flock (96) show that while exact IP solutions fail to scale, our\nframework continues to deliver high-quality site selections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u661f\u5ea7\u7684\u5730\u9762\u7ad9\u9009\u62e9\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u96be\u4ee5\u5904\u7406\u7684\u6311\u6218\u3002", "motivation": "\u6709\u6548\u7684\u5730\u9762\u7ad9\u9009\u62e9\u5bf9\u4e8eLEO\u536b\u661f\u661f\u5ea7\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u6700\u5c0f\u5316\u8fd0\u8425\u6210\u672c\u3001\u6700\u5927\u5316\u6570\u636e\u4e0b\u884c\u91cf\u5e76\u51cf\u5c11\u901a\u4fe1\u7a7a\u767d\u3002\u7136\u800c\uff0c\u4f20\u7edf\u57fa\u4e8e\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u548c\u6df7\u5408\u6574\u6570\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u3001\u591a\u4f9b\u5e94\u5546\u548c\u5927\u578b\u536b\u661f\u661f\u5ea7\u60c5\u5883\u4e0b\uff0c\u5f88\u5feb\u53d8\u5f97\u96be\u4ee5\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5206\u5c42\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u5168\u7403\u9009\u62e9\u95ee\u9898\u5206\u89e3\u4e3a\u5355\u9897\u536b\u661f\u3001\u77ed\u65f6\u95f4\u7a97\u53e3\u7684\u5b50\u95ee\u9898\u3002\u4ece\u6bcf\u4e2a\u5b50\u95ee\u9898\u4e2d\u9009\u62e9\u7684\u6700\u4f73\u7ad9\u70b9\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u8bc6\u522b\u6240\u6709\u5206\u89e3\u6848\u4f8b\u4e2d\u6301\u7eed\u9ad8\u4ef7\u503c\u7684\u4f4d\u7f6e\u3002\u7136\u540e\u5c06\u805a\u7c7b\u7ea7\u96c6\u5408\u4e0e\u6700\u63a5\u8fd1\u7684GSaaS\u5019\u9009\u7ad9\u70b9\u8fdb\u884c\u5339\u914d\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u4e2a\u5168\u5c40\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u5408\u6210Walker-Star\u6d4b\u8bd5\u6848\u4f8b\uff081-10\u9897\u536b\u661f\uff0c1-10\u4e2a\u5730\u9762\u7ad9\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u5c40IP\u6700\u4f18\u89e395%\u7684\u6027\u80fd\u3002\u5728Capella Space\uff085\u9897\u536b\u661f\uff09\u3001ICEYE\uff0840\u9897\uff09\u548cPlanet's Flock\uff0896\u9897\uff09\u7684\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0c\u867d\u7136\u7cbe\u786eIP\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6269\u5c55\uff0c\u4f46\u8be5\u6846\u67b6\u4ecd\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u7ad9\u70b9\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u534f\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21LEO\u536b\u661f\u5730\u9762\u7ad9\u9009\u62e9\u95ee\u9898\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2510.03491", "pdf": "https://arxiv.org/pdf/2510.03491", "abs": "https://arxiv.org/abs/2510.03491", "authors": ["Sarah-Michelle Hammer", "Stefan Schmid", "Rachee Singh", "Vamsi Addanki"], "title": "Short-circuiting Rings for Low-Latency AllReduce", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Efficient collective communication is critical for many distributed ML and\nHPC applications. In this context, it is widely believed that the Ring\nalgorithm for the AllReduce collective communication operation is optimal only\nfor large messages, while Recursive Doubling is preferable for small ones due\nto its logarithmic number of steps compared to the linear number for Ring. In\nthis paper, we challenge this long-held assumption and show that the Ring\nalgorithm can remain optimal even for short messages in ring-based GPU-to-GPU\ntopologies, once realistic propagation delays and link capacity constraints are\naccounted for. We find that the total propagation delay for both Ring and\nRecursive Doubling essentially sums to the same value, but the latter incurs\nsignificantly higher congestion due to longer hop counts, leading to increased\ncompletion times. This surprising result motivates our case for in-collective\nadaptive topologies, particularly in the context of emerging photonic\ninterconnects, which can break through the limitations of static topology\ndesigns at the collective communication granularity. We design a \\emph{simple\nand fast} heuristic for circuit-switching that enables Recursive Doubling to\nexploit dynamically reconfigurable photonic paths, carefully balancing\nreconfiguration delays, propagation latencies, and link congestion to minimize\noverall completion time. Our preliminary evaluations, using realistic\nreconfiguration delays, show that our circuit-switching schedules enable faster\ncompletion times for Recursive Doubling, even compared to Ring AllReduce on\nstatic ring topologies. We conclude by highlighting key challenges and future\nresearch directions for realizing practical, in-collective photonic switching.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86AllReduce\u4e2dRing\u7b97\u6cd5\u53ea\u9002\u7528\u4e8e\u5927\u6d88\u606f\u3001Recursive Doubling\u9002\u7528\u4e8e\u5c0f\u6d88\u606f\u7684\u666e\u904d\u8ba4\u77e5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8003\u8651\u5b9e\u9645\u5ef6\u8fdf\u548c\u62e5\u585e\u540e\uff0cRing\u5728\u73af\u5f62GPU\u62d3\u6251\u4e2d\u5bf9\u77ed\u6d88\u606f\u4e5f\u53ef\u80fd\u6700\u4f18\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u5229\u7528\u65b0\u5174\u5149\u4e92\u8fde\u7684\u81ea\u9002\u5e94\u62d3\u6251\uff0c\u901a\u8fc7\u7535\u8def\u4ea4\u6362\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f7fRecursive Doubling\u8d85\u8d8aRing\u7684\u6027\u80fd\u3002", "motivation": "\u5206\u5e03\u5f0fML\u548cHPC\u5e94\u7528\u4e2d\u9ad8\u6548\u7684\u96c6\u4f53\u901a\u4fe1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u89c2\u70b9\u8ba4\u4e3a\uff0cRing\u7b97\u6cd5\u4ec5\u5bf9\u5927\u6d88\u606f\u6700\u4f18\uff0c\u800cRecursive Doubling\u56e0\u5176\u5bf9\u6570\u7ea7\u6b65\u6570\u800c\u5bf9\u5c0f\u6d88\u606f\u66f4\u4f18\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u957f\u671f\u5047\u8bbe\uff0c\u63a2\u7a76\u5728\u771f\u5b9e\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u5bb9\u91cf\u9650\u5236\u4e0b\uff0cRing\u7b97\u6cd5\u5728\u73af\u5f62GPU\u62d3\u6251\u4e2d\u5bf9\u77ed\u6d88\u606f\u662f\u5426\u4f9d\u7136\u80fd\u4fdd\u6301\u6700\u4f18\uff0c\u5e76\u53d7\u6b64\u542f\u53d1\u63a2\u7d22\u81ea\u9002\u5e94\u62d3\u6251\uff08\u7279\u522b\u662f\u5149\u4e92\u8fde\uff09\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790Ring\u548cRecursive Doubling\u7b97\u6cd5\u5728\u8003\u8651\u5b9e\u9645\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u5bb9\u91cf\u9650\u5236\u4e0b\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e24\u8005\u5728\u73af\u5f62GPU-to-GPU\u62d3\u6251\u4e2d\u7684\u603b\u4f20\u64ad\u5ef6\u8fdf\u548c\u62e5\u585e\u7a0b\u5ea6\uff08\u57fa\u4e8e\u8df3\u6570\uff09\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u201c\u7b80\u5355\u5feb\u901f\u201d\u7684\u7535\u8def\u4ea4\u6362\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f7fRecursive Doubling\u80fd\u5229\u7528\u52a8\u6001\u53ef\u91cd\u6784\u7684\u5149\u5b50\u8def\u5f84\uff0c\u5e73\u8861\u91cd\u6784\u5ef6\u8fdf\u3001\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u62e5\u585e\u4ee5\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\u3002\u8fdb\u884c\u4e86\u521d\u6b65\u8bc4\u4f30\uff0c\u4f7f\u7528\u771f\u5b9e\u7684\u91cd\u6784\u5ef6\u8fdf\u6765\u6bd4\u8f83\u5176\u7535\u8def\u4ea4\u6362\u65b9\u6848\u4e0e\u9759\u6001\u73af\u5f62\u62d3\u6251\u4e0a\u7684Ring AllReduce\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8003\u8651\u5b9e\u9645\u4f20\u64ad\u5ef6\u8fdf\u548c\u94fe\u8def\u5bb9\u91cf\u9650\u5236\u540e\uff0cRing\u7b97\u6cd5\u5728\u73af\u5f62GPU-to-GPU\u62d3\u6251\u4e2d\u5bf9\u77ed\u6d88\u606f\u4e5f\u80fd\u4fdd\u6301\u6700\u4f18\u3002Ring\u548cRecursive Doubling\u7684\u603b\u4f20\u64ad\u5ef6\u8fdf\u57fa\u672c\u76f8\u540c\uff0c\u4f46\u540e\u8005\u56e0\u8df3\u6570\u66f4\u591a\u800c\u5bfc\u81f4\u66f4\u9ad8\u7684\u62e5\u585e\uff0c\u589e\u52a0\u4e86\u5b8c\u6210\u65f6\u95f4\u3002\u6240\u63d0\u51fa\u7684\u7535\u8def\u4ea4\u6362\u8c03\u5ea6\uff08\u7528\u4e8eRecursive Doubling\u5728\u53ef\u91cd\u6784\u5149\u5b50\u8def\u5f84\u4e0a\uff09\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u751a\u81f3\u4f18\u4e8e\u9759\u6001\u73af\u5f62\u62d3\u6251\u4e0a\u7684Ring AllReduce\u3002", "conclusion": "\u5728\u8003\u8651\u5b9e\u9645\u7f51\u7edc\u9650\u5236\u65f6\uff0cRing\u7b97\u6cd5\u5728\u9759\u6001\u73af\u5f62\u62d3\u6251\u4e2d\u5373\u4f7f\u5bf9\u77ed\u6d88\u606f\u4e5f\u53ef\u80fd\u6700\u4f18\u3002\u901a\u8fc7\u5229\u7528\u65b0\u5174\u7684\u5149\u5b50\u4e92\u8fde\u548c\u52a8\u6001\u91cd\u6784\uff0c\u81ea\u9002\u5e94\u62d3\u6251\u4e2d\u7684Recursive Doubling\u7b97\u6cd5\u80fd\u591f\u514b\u670d\u9759\u6001\u62d3\u6251\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u6027\u80fd\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u89e3\u51b3\u5b9e\u73b0\u5b9e\u7528\u7684\u96c6\u4f53\u5185\u90e8\u5149\u5b50\u4ea4\u6362\u6240\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.03524", "pdf": "https://arxiv.org/pdf/2510.03524", "abs": "https://arxiv.org/abs/2510.03524", "authors": ["Mohammad Reza Akbari", "Hamid Barati", "Ali Barati"], "title": "A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem", "categories": ["cs.NI"], "comment": null, "summary": "Fog computing integrates cloud and edge resources. According to an\nintelligent and decentralized method, this technology processes data generated\nby IoT sensors to seamlessly integrate physical and cyber environments.\nInternet of Things uses wireless and smart objects. They communicate with each\nother, monitor the environment, collect information, and respond to user\nrequests. These objects have limited energy resources since they use batteries\nto supply energy. Also, they cannot replace their batteries. As a result, the\nnetwork lifetime is limited and short. Thus, reducing energy consumption and\naccelerating the data transmission process are very important challenges in IoT\nnetworks to reduce the response time. In the data transmission process,\nselecting an appropriate cluster head node is very important because it can\nreduce the delay when sending data to the fog. In this paper, cluster head\nnodes are selected based on several important criteria such as distance,\nresidual energy, received signal strength, and link expiration time. Then,\nobjects send the processed data to the server hierarchically through a balanced\ntree. The simulation results show that the proposed method outperforms the\nenergy-efficient centroid-based routing protocol (EECRP) and the Emergency\nResponse IoT based on Global Information Decision (ERGID) in terms of packet\ndelivery rate, delay, response time, and network lifetime.", "AI": {"tldr": "\u9488\u5bf9\u7269\u8054\u7f51\u80fd\u8017\u548c\u7f51\u7edc\u5bff\u547d\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u51c6\u5219\u7684\u7c07\u5934\u9009\u62e9\u4e0e\u5e73\u8861\u6811\u5206\u5c42\u6570\u636e\u4f20\u8f93\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6570\u636e\u4f20\u8f93\u6548\u7387\u5e76\u5ef6\u957f\u7f51\u7edc\u5bff\u547d\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u6e90\u6709\u9650\u4e14\u7535\u6c60\u4e0d\u53ef\u66f4\u6362\uff0c\u5bfc\u81f4\u7f51\u7edc\u5bff\u547d\u77ed\u3002\u56e0\u6b64\uff0c\u964d\u4f4e\u80fd\u8017\u3001\u52a0\u901f\u6570\u636e\u4f20\u8f93\u5e76\u7f29\u77ed\u54cd\u5e94\u65f6\u95f4\u662f\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u91cd\u8981\u6311\u6218\uff0c\u7279\u522b\u9700\u8981\u4f18\u5316\u7c07\u5934\u9009\u62e9\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u3002", "method": "\u7c07\u5934\u8282\u70b9\u6839\u636e\u8ddd\u79bb\u3001\u5269\u4f59\u80fd\u91cf\u3001\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u548c\u94fe\u8def\u8fc7\u671f\u65f6\u95f4\u7b49\u591a\u4e2a\u91cd\u8981\u51c6\u5219\u8fdb\u884c\u9009\u62e9\u3002\u968f\u540e\uff0c\u5bf9\u8c61\u901a\u8fc7\u5e73\u8861\u6811\u4ee5\u5206\u5c42\u65b9\u5f0f\u5c06\u5904\u7406\u8fc7\u7684\u6570\u636e\u53d1\u9001\u5230\u670d\u52a1\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u636e\u5305\u6295\u9012\u7387\u3001\u5ef6\u8fdf\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u7f51\u7edc\u5bff\u547d\u65b9\u9762\u5747\u4f18\u4e8e\u80fd\u91cf\u9ad8\u6548\u8d28\u5fc3\u8def\u7531\u534f\u8bae\uff08EECRP\uff09\u548c\u57fa\u4e8e\u5168\u5c40\u4fe1\u606f\u51b3\u7b56\u7684\u5e94\u6025\u54cd\u5e94\u7269\u8054\u7f51\uff08ERGID\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u591a\u51c6\u5219\u7684\u7c07\u5934\u9009\u62e9\u548c\u5e73\u8861\u6811\u5206\u5c42\u6570\u636e\u4f20\u8f93\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u7269\u8054\u7f51\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5305\u62ec\u63d0\u5347\u6570\u636e\u5305\u6295\u9012\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u5ef6\u957f\u7f51\u7edc\u5bff\u547d\u3002"}}
{"id": "2510.03533", "pdf": "https://arxiv.org/pdf/2510.03533", "abs": "https://arxiv.org/abs/2510.03533", "authors": ["Mohammad Reza Akbari", "Hamid Barati", "Ali Barati"], "title": "An efficient grey theory-driven path selection for energy efficiency control in the Internet of Things using fog and cloud computing", "categories": ["cs.NI"], "comment": null, "summary": "Due to the big data exchange on the Internet of Things, proper routing and\nselecting the best routes for fast data transmission improve network\nperformance. There are major challenges, like high delay, when cloud computing\nis used. Therefore, one solution is to use other schemes, such as fog\ncomputing. In fog computing, all data is not sent to the cloud and the fog\nnodes close to objects are used for data processing. This reduces the network\ndelay. In this paper, we propose an overlapping clustering method called\nMFCT-IoT to select the best cluster head nodes to guarantee the fast data\ntransfer from objects to fog nodes. The selected cluster head nodes are\nresponsible for sending the collected data to the closest fog nodes in the\nnetwork edge. Upon receiving the data, the fog nodes process it, and if a\nresponse is ready, they respond immediately to the object. Otherwise, they\nmerge and transmit the data to the cloud servers, which are considered as the\nroot node of the proposed hierarchical tree. After processing, the merged data\nis sent to the object. We compare the proposed scheme with two schemes,\nincluding ERGID and EECRP. These schemes are evaluated based on various\ncriteria, including the response time, packet delivery ratio, end-to-end delay,\nnetwork lifetime, and energy consumption. The results indicate that the\nproposed method outperforms others in terms of all criteria.", "AI": {"tldr": "\u63d0\u51faMFCT-IoT\u91cd\u53e0\u805a\u7c7b\u65b9\u6cd5\uff0c\u4f18\u5316\u7269\u8054\u7f51\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6570\u636e\u4f20\u8f93\uff0c\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u7269\u8054\u7f51\u5927\u6570\u636e\u4ea4\u6362\u4e2d\uff0c\u57fa\u4e8e\u4e91\u8ba1\u7b97\u5b58\u5728\u9ad8\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u7f51\u7edc\u6027\u80fd\u3002\u96fe\u8ba1\u7b97\u4f5c\u4e3a\u4e00\u79cd\u9760\u8fd1\u5bf9\u8c61\u5904\u7406\u6570\u636e\u7684\u65b9\u6848\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMFCT-IoT\u7684\u91cd\u53e0\u805a\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u9009\u62e9\u6700\u4f73\u7c07\u5934\u8282\u70b9\uff0c\u4ee5\u786e\u4fdd\u7269\u8054\u7f51\u4e2d\u4ece\u5bf9\u8c61\u5230\u96fe\u8282\u70b9\u7684\u5feb\u901f\u6570\u636e\u4f20\u8f93\u3002\u9009\u5b9a\u7684\u7c07\u5934\u8d1f\u8d23\u5c06\u6570\u636e\u53d1\u9001\u5230\u6700\u8fd1\u7684\u96fe\u8282\u70b9\u8fdb\u884c\u5904\u7406\uff1b\u82e5\u65e0\u6cd5\u7acb\u5373\u54cd\u5e94\uff0c\u5219\u5408\u5e76\u5e76\u4f20\u8f93\u6570\u636e\u81f3\u4e91\u670d\u52a1\u5668\uff08\u4f5c\u4e3a\u5206\u5c42\u6811\u7684\u6839\u8282\u70b9\uff09\u3002\u8be5\u65b9\u6cd5\u4e0eERGID\u548cEECRP\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u54cd\u5e94\u65f6\u95f4\u3001\u6570\u636e\u5305\u6295\u9012\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u7f51\u7edc\u751f\u547d\u5468\u671f\u548c\u80fd\u8017\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MFCT-IoT\u65b9\u6cd5\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u6570\u636e\u5305\u6295\u9012\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u7f51\u7edc\u751f\u547d\u5468\u671f\u548c\u80fd\u8017\u7b49\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8eERGID\u548cEECRP\u65b9\u6848\u3002", "conclusion": "MFCT-IoT\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u7c07\u5934\u9009\u62e9\u548c\u6570\u636e\u4f20\u8f93\u673a\u5236\uff0c\u5728\u7269\u8054\u7f51\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u6709\u6548\u964d\u4f4e\u4e86\u7f51\u7edc\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u6574\u4f53\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2510.03315", "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text.", "AI": {"tldr": "\u7814\u7a76Transformer\u4e2d\u7a33\u5b9a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u901a\u8fc7\u6821\u51c6\u6587\u672c\u8fd1\u4f3c\u5176\u8f93\u51fa\uff0c\u5e76\u53d1\u73b0GPT2-Small\u4e2d\u54cd\u5e94\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u795e\u7ecf\u5143\u3002", "motivation": "\u5206\u6790Transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\u4e14\u5bf9\u5185\u5bb9\u4f9d\u8d56\u5f31\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u89e3\u91ca\u5b83\u4eec\u5bf9\u6587\u672c\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u8d21\u732e\u3002", "method": "\u7814\u7a76\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\u4e14\u5bf9\u5185\u5bb9\u4f9d\u8d56\u5f31\u7684\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51fa\u5176softmax\u5206\u6bcd\u5728\u56fa\u5b9atoken\u5206\u5e03\u4e0b\u7a33\u5b9a\u3002\u901a\u8fc7\u201c\u6821\u51c6\u6587\u672c\u201d\u91c7\u6837softmax\u5206\u6bcd\uff0c\u7ed3\u5408GPT2-Small\u7b2c\u4e00\u5c42\u591a\u4e2a\u7a33\u5b9a\u5934\u7684\u8f93\u51fa\uff0c\u5e76\u7528\u5468\u56f4\u6587\u672c\u7684\u7ebf\u6027\u6458\u8981\u8fd1\u4f3c\u5176\u7ec4\u5408\u8f93\u51fa\u3002", "result": "\u8be5\u8fd1\u4f3c\u65b9\u6cd5\u4ec5\u51ed\u6a21\u578b\u6743\u91cd\u548c\u4e00\u4efd\u6821\u51c6\u6587\u672c\uff0c\u5373\u53ef\u53d1\u73b0GPT2-Small\u7b2c\u4e00\u5c42\u4e2d\u6570\u767e\u4e2a\u54cd\u5e94\u5468\u56f4\u6587\u672c\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u795e\u7ecf\u5143\uff0c\u751a\u81f3\u5305\u62ec\u672a\u5728\u6821\u51c6\u6587\u672c\u4e0a\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u7279\u5b9a\u7a33\u5b9a\u6ce8\u610f\u529b\u5934\u5e76\u8fd1\u4f3c\u5176\u884c\u4e3a\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u548c\u53d1\u73b0Transformer\u6a21\u578b\u4e2d\u5bf9\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u654f\u611f\u7684\u795e\u7ecf\u5143\u3002"}}
{"id": "2510.03287", "pdf": "https://arxiv.org/pdf/2510.03287", "abs": "https://arxiv.org/abs/2510.03287", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Prateek Prasanna"], "title": "SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Accurate prediction of tumor trajectories under standard-of-care (SoC)\ntherapies remains a major unmet need in oncology. This capability is essential\nfor optimizing treatment planning and anticipating disease progression.\nConventional reaction-diffusion models are limited in scope, as they fail to\ncapture tumor dynamics under heterogeneous therapeutic paradigms. There is\nhence a critical need for computational frameworks that can realistically\nsimulate SoC interventions while accounting for inter-patient variability in\ngenomics, demographics, and treatment regimens. We introduce Standard-of-Care\nDigital Twin (SoC-DT), a differentiable framework that unifies\nreaction-diffusion tumor growth models, discrete SoC interventions (surgery,\nchemotherapy, radiotherapy) along with genomic and demographic personalization\nto predict post-treatment tumor structure on imaging. An implicit-explicit\nexponential time-differencing solver, IMEX-SoC, is also proposed, which ensures\nstability, positivity, and scalability in SoC treatment situations. Evaluated\non both synthetic data and real world glioma data, SoC-DT consistently\noutperforms classical PDE baselines and purely data-driven neural models in\npredicting tumor dynamics. By bridging mechanistic interpretability with modern\ndifferentiable solvers, SoC-DT establishes a principled foundation for\npatient-specific digital twins in oncology, enabling biologically consistent\ntumor dynamics estimation. Code will be made available upon acceptance.", "AI": {"tldr": "SoC-DT\u662f\u4e00\u4e2a\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u5e94\u6269\u6563\u6a21\u578b\u3001SoC\u5e72\u9884\u548c\u4e2a\u6027\u5316\u6570\u636e\uff0c\u7528\u4e8e\u7cbe\u786e\u9884\u6d4b\u80bf\u7624\u5728\u6807\u51c6\u6cbb\u7597\u4e0b\u7684\u8f68\u8ff9\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u80bf\u7624\u5b66\u4e2d\uff0c\u7cbe\u786e\u9884\u6d4b\u6807\u51c6\u6cbb\u7597\u4e0b\u80bf\u7624\u8f68\u8ff9\u662f\u4e00\u4e2a\u5c1a\u672a\u6ee1\u8db3\u7684\u91cd\u5927\u9700\u6c42\uff0c\u5bf9\u4e8e\u4f18\u5316\u6cbb\u7597\u8ba1\u5212\u548c\u9884\u6d4b\u75be\u75c5\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u53cd\u5e94\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u5f02\u8d28\u6027\u6cbb\u7597\u8303\u5f0f\u4e0b\u7684\u80bf\u7624\u52a8\u6001\uff0c\u56e0\u6b64\u4e9f\u9700\u80fd\u6a21\u62dfSoC\u5e72\u9884\u5e76\u8003\u8651\u60a3\u8005\u95f4\u53d8\u5f02\u6027\uff08\u57fa\u56e0\u7ec4\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u6cbb\u7597\u65b9\u6848\uff09\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86Standard-of-Care Digital Twin (SoC-DT)\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u4e2a\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u878d\u5408\u4e86\u53cd\u5e94\u6269\u6563\u80bf\u7624\u751f\u957f\u6a21\u578b\u3001\u79bb\u6563SoC\u5e72\u9884\uff08\u624b\u672f\u3001\u5316\u7597\u3001\u653e\u7597\uff09\u4ee5\u53ca\u57fa\u56e0\u7ec4\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e2a\u6027\u5316\uff0c\u4ee5\u9884\u6d4b\u6cbb\u7597\u540e\u7684\u5f71\u50cf\u80bf\u7624\u7ed3\u6784\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u9690\u5f0f-\u663e\u5f0f\u6307\u6570\u65f6\u95f4\u5dee\u5206\u6c42\u89e3\u5668IMEX-SoC\uff0c\u786e\u4fddSoC\u6cbb\u7597\u60c5\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u3001\u6b63\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "SoC-DT\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u5747\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u9884\u6d4b\u80bf\u7624\u52a8\u6001\u65b9\u9762\uff0c\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u57fa\u7ebf\u6a21\u578b\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "conclusion": "SoC-DT\u901a\u8fc7\u5c06\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e0e\u73b0\u4ee3\u53ef\u5fae\u5206\u6c42\u89e3\u5668\u76f8\u7ed3\u5408\uff0c\u4e3a\u80bf\u7624\u5b66\u4e2d\u60a3\u8005\u7279\u5f02\u6027\u6570\u5b57\u5b6a\u751f\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u751f\u7269\u5b66\u4e0a\u4e00\u81f4\u7684\u80bf\u7624\u52a8\u6001\u4f30\u8ba1\u3002"}}
{"id": "2510.03243", "pdf": "https://arxiv.org/pdf/2510.03243", "abs": "https://arxiv.org/abs/2510.03243", "authors": ["Yiheng Tao", "Yihe Zhang", "Matthew T. Dearing", "Xin Wang", "Yuping Fan", "Zhiling Lan"], "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low\nlatency and high throughput, particularly with the growing use of\nreasoning-capable LLMs. Traditional strategies like First-Come-First-Serve\n(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks\ndelay shorter ones queued behind them. In this paper, we introduce PARS, a\nprompt-aware LLM task scheduler that improves serving efficiency by\napproximating shortest-job-first (SJF) scheduling through pairwise ranking with\nmargin ranking loss. PARS focuses on impactful scheduling decisions and is\nseamlessly integrated into the state-of-the-art LLM serving system vLLM. It\neffectively predicts response-length-based task ordering, reducing latency with\nminimal overhead. Extensive experiments across multiple LLMs and real-world\ninference datasets show that PARS significantly improves performance, including\nfor reasoning workloads. Furthermore, our cross-model evaluations demonstrate\nthat the design generalizes well, enabling effective scheduling even when\npredictors are trained on different LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPARS\uff0c\u4e00\u4e2a\u63d0\u793a\u611f\u77e5\u7684LLM\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6210\u5bf9\u6392\u5e8f\u8fd1\u4f3c\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\uff08SJF\uff09\u8c03\u5ea6\uff0c\u96c6\u6210\u81f3vLLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u4efb\u52a1\u8c03\u5ea6\u7b56\u7565\uff08\u5982FCFS\uff09\u5b58\u5728\u961f\u5934\u963b\u585e\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u4efb\u52a1\u5ef6\u8fdf\u77ed\u4efb\u52a1\uff0c\u963b\u788d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\uff0c\u5c24\u5176\u5bf9\u4e8e\u63a8\u7406\u80fd\u529b\u5f3a\u7684LLM\u3002", "method": "\u5f15\u5165PARS\uff0c\u4e00\u4e2a\u63d0\u793a\u611f\u77e5\u7684LLM\u4efb\u52a1\u8c03\u5ea6\u5668\u3002\u5b83\u901a\u8fc7\u4f7f\u7528\u5e26\u8fb9\u9645\u6392\u5e8f\u635f\u5931\u7684\u6210\u5bf9\u6392\u5e8f\u6765\u8fd1\u4f3c\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\uff08SJF\uff09\u8c03\u5ea6\uff0c\u4ee5\u9884\u6d4b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u4efb\u52a1\u6392\u5e8f\u3002PARS\u4e13\u6ce8\u4e8e\u6709\u5f71\u54cd\u529b\u7684\u8c03\u5ea6\u51b3\u7b56\uff0c\u5e76\u65e0\u7f1d\u96c6\u6210\u5230\u5148\u8fdb\u7684LLM\u670d\u52a1\u7cfb\u7edfvLLM\u4e2d\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPARS\u663e\u8457\u63d0\u5347\u4e86\u591aLLM\u548c\u771f\u5b9e\u4e16\u754c\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u62ec\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5ef6\u8fdf\u4e14\u5f00\u9500\u6781\u5c0f\u3002\u8de8\u6a21\u578b\u8bc4\u4f30\u8bc1\u5b9e\u5176\u8bbe\u8ba1\u6cdb\u5316\u6027\u826f\u597d\uff0c\u5373\u4f7f\u9884\u6d4b\u5668\u5728\u4e0d\u540cLLM\u4e0a\u8bad\u7ec3\u4e5f\u80fd\u5b9e\u73b0\u6709\u6548\u8c03\u5ea6\u3002", "conclusion": "PARS\u901a\u8fc7\u521b\u65b0\u7684SJF\u8fd1\u4f3c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u8c03\u5ea6\u4e2d\u7684\u6548\u7387\u548c\u5ef6\u8fdf\u95ee\u9898\u3002\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540cLLM\u4e4b\u95f4\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLLM\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03285", "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents.", "AI": {"tldr": "\u9274\u4e8e\u73b0\u6709LLM\u6d4f\u89c8\u5668\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\u8fc7\u4e8e\u7406\u60f3\uff0c\u672c\u6587\u63d0\u51faWAREX\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u4e16\u754c\u4e0d\u7a33\u5b9a\u6027\u4e0b\u8bc4\u4f30\u4ee3\u7406\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u4ee3\u7406\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6d4f\u89c8\u5668LLM\u4ee3\u7406\u7684\u8bc4\u4f30\u57fa\u51c6\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e0d\u7a33\u5b9a\u6027\u3001\u7f51\u7ad9\u653b\u51fb\u548c\u52a8\u6001\u4fee\u6539\u7b49\u56e0\u7d20\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4ee3\u7406\u7684\u5b9e\u9645\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165WAREX\uff08Web Agent Reliability Evaluation on Existing Benchmarks\uff09\u6846\u67b6\uff0c\u65e8\u5728\u6d4b\u91cf\u8fd9\u4e9b\u4ee3\u7406\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982WebArena\u3001WebVoyager\u548cREAL\uff09\u4e2d\u9762\u4e34\u771f\u5b9e\u4e16\u754c\u6311\u6218\u65f6\u7684\u53ef\u9760\u6027\u3002", "result": "\u5c06WAREX\u5f15\u5165\u6d4b\u8bd5\u540e\uff0cLLM\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7406\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u7f51\u7edc\u4e0d\u7a33\u5b9a\u6027\u548c\u7f51\u7ad9\u52a8\u6001\u53d8\u5316\u65f6\uff0c\u8868\u73b0\u51fa\u6709\u9650\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.03714", "pdf": "https://arxiv.org/pdf/2510.03714", "abs": "https://arxiv.org/abs/2510.03714", "authors": ["Nalith Udugampola", "Xiaoyu Ai", "Binghao Li", "Henry Gong", "Aruna Seneviratne"], "title": "A Position- and Energy-Aware Routing Strategy for Subterranean LoRa Mesh Networks", "categories": ["cs.NI"], "comment": null, "summary": "Although LoRa is predominantly employed with the single-hop LoRaWAN protocol,\nrecent advancements have extended its application to multi-hop mesh topologies.\nDesigning efficient routing for LoRa mesh networks remains challenging due to\nLoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional\nprotocols for low-traffic, aboveground networks with strict duty cycle\nconstraints or uses flooding-based methods in subterranean environments.\nHowever, these approaches inefficiently utilize the limited available network\nbandwidth in these low-data-rate networks due to excessive control overhead,\nacknowledgments, and redundant retransmissions. In this paper, we introduce a\nnovel position- and energy-aware routing strategy tailored for subterranean\nLoRa mesh networks aimed at enhancing maximum throughput and power efficiency\nwhile also maintaining high packet delivery ratios. Our mechanism begins with a\nlightweight position learning phase, during which LoRa repeaters ascertain\ntheir relative positions and gather routing information. Afterwards, the\nnetwork becomes fully operational with adaptive routing, leveraging standby\nLoRa repeaters for recovery from packet collisions and losses, and energy-aware\nroute switching to balance battery depletion across repeaters. The simulation\nresults on a representative subterranean network demonstrate a 185% increase in\nmaximum throughput and a 75% reduction in energy consumption compared to a\npreviously optimized flooding-based approach for high traffic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5730\u4e0bLoRa\u7f51\u72b6\u7f51\u7edc\u7684\uff0c\u57fa\u4e8e\u4f4d\u7f6e\u548c\u80fd\u91cf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u5927\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "LoRa\u7f51\u72b6\u7f51\u7edc\u8def\u7531\u9762\u4e34\u4f4e\u6570\u636e\u901f\u7387\u548cALOHA MAC\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6539\u7f16\u4f20\u7edf\u534f\u8bae\u6216\u57fa\u4e8e\u6d2a\u6cdb\uff09\u7531\u4e8e\u63a7\u5236\u5f00\u9500\u3001\u786e\u8ba4\u548c\u5197\u4f59\u91cd\u4f20\uff0c\u5728\u4f4e\u6570\u636e\u901f\u7387\u7f51\u7edc\u4e2d\u4f4e\u6548\u5229\u7528\u6709\u9650\u5e26\u5bbd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u80fd\u63d0\u9ad8\u541e\u5410\u91cf\u3001\u80fd\u6548\u5e76\u4fdd\u6301\u9ad8\u6570\u636e\u5305\u6295\u9012\u7387\u7684\u5730\u4e0bLoRa\u7f51\u72b6\u7f51\u7edc\u8def\u7531\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u578b\u7684\u3001\u57fa\u4e8e\u4f4d\u7f6e\u548c\u80fd\u91cf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\u3002\u8be5\u673a\u5236\u5305\u62ec\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4f4d\u7f6e\u5b66\u4e60\u9636\u6bb5\uff0c\u4e2d\u7ee7\u5668\u5728\u6b64\u9636\u6bb5\u786e\u5b9a\u76f8\u5bf9\u4f4d\u7f6e\u5e76\u6536\u96c6\u8def\u7531\u4fe1\u606f\u3002\u968f\u540e\uff0c\u7f51\u7edc\u8fdb\u5165\u81ea\u9002\u5e94\u8def\u7531\u9636\u6bb5\uff0c\u5229\u7528\u5907\u7528LoRa\u4e2d\u7ee7\u5668\u4ece\u6570\u636e\u5305\u51b2\u7a81\u548c\u4e22\u5931\u4e2d\u6062\u590d\uff0c\u5e76\u91c7\u7528\u80fd\u91cf\u611f\u77e5\u8def\u7531\u5207\u6362\u6765\u5e73\u8861\u4e2d\u7ee7\u5668\u4e4b\u95f4\u7684\u7535\u6c60\u6d88\u8017\u3002", "result": "\u5728\u4ee3\u8868\u6027\u5730\u4e0b\u7f51\u7edc\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u5148\u524d\u4f18\u5316\u7684\u9ad8\u6d41\u91cf\u6d2a\u6cdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6700\u5927\u541e\u5410\u91cf\u63d0\u9ad8\u4e86185%\uff0c\u80fd\u8017\u964d\u4f4e\u4e8675%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6570\u636e\u5305\u6295\u9012\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f4d\u7f6e\u548c\u80fd\u91cf\u611f\u77e5\u8def\u7531\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5730\u4e0bLoRa\u7f51\u72b6\u7f51\u7edc\u7684\u541e\u5410\u91cf\u548c\u80fd\u6548\uff0c\u4e3a\u8be5\u7c7b\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u8def\u7531\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03323", "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "categories": ["cs.CL"], "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "\u63d0\u51faGraph-$S^3$\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3LLM\u68c0\u7d22\u5668\uff0c\u89e3\u51b3\u6587\u672c\u56fe\u95ee\u7b54\u4e2d\u7684\u56fe\u68c0\u7d22\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u548cF1\u5206\u6570\uff0c\u5c24\u5176\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e38\u4ee5\u6587\u672c\u56fe\u5f62\u5f0f\u5b58\u5728\uff0c\u5c06\u5176\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5bf9\u5b9e\u73b0\u590d\u6742\u56fe\u95ee\u7b54\u524d\u666f\u5e7f\u9614\u3002\u7136\u800c\uff0cLLM\u6587\u672c\u56fe\u95ee\u7b54\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u56fe\u68c0\u7d22\uff0c\u5373\u5982\u4f55\u4ece\u5927\u578b\u56fe\u4e2d\u68c0\u7d22\u51fa\u8db3\u591f\u4fe1\u606f\u91cf\u4e14\u7d27\u51d1\u7684\u5185\u5bb9\u4ee5\u9002\u5e94LLM\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u68c0\u7d22\u5668\u56e0\u4f9d\u8d56\u6d45\u5c42\u5d4c\u5165\u76f8\u4f3c\u6027\u6216\u4ea4\u4e92\u5f0f\u68c0\u7d22\u7b56\u7565\u4f46\u9700\u9ad8\u6602\u6807\u6ce8\u548c\u8bad\u7ec3\u6210\u672c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGraph-$S^3$\u6846\u67b6\uff0c\u4e00\u4e2a\u667a\u80fd\u4f53\u6587\u672c\u56fe\u63a8\u7406\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u5668\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u4e0d\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u5956\u52b1\uff08\u53ef\u80fd\u5bfc\u81f4\u7a00\u758f\u548c\u4e0d\u7a33\u5b9a\u7684\u8bad\u7ec3\u4fe1\u53f7\uff09\uff0c\u800c\u662f\u901a\u8fc7\u79bb\u7ebf\u63d0\u53d6\u7684\u9ec4\u91d1\u5b50\u56fe\u5bc6\u5207\u8bc4\u4f30\u68c0\u7d22\u5668\u7684\u6bcf\u4e00\u6b65\u3002\u4e3b\u8981\u6280\u672f\u5305\u62ec\uff1a1) \u7528\u4e8e\u63d0\u53d6\u9ec4\u91d1\u5b50\u56fe\u4ee5\u751f\u6210\u5956\u52b1\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff1b2) \u7528\u4e8e\u5b66\u4e60\u57fa\u4e8e\u5408\u6210\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u56fe\u63a2\u7d22\u7b56\u7565\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u4e09\u4e2a\u901a\u7528\u6570\u636e\u96c6\u4e0a\u4e0e\u4e03\u4e2a\u5f3a\u57fa\u7ebf\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u5e73\u5747\u63d0\u9ad88.1%\uff0c\u5728F1\u5206\u6570\u4e0a\u5e73\u5747\u63d0\u9ad89.7%\u3002\u5728\u66f4\u590d\u6742\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4f18\u52bf\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "Graph-$S^3$\u901a\u8fc7\u5f15\u5165\u5408\u6210\u9010\u6b65\u76d1\u7763\u7684LLM\u68c0\u7d22\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u56fe\u95ee\u7b54\u4e2d\u7684\u5173\u952e\u56fe\u68c0\u7d22\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u8df3\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.03292", "pdf": "https://arxiv.org/pdf/2510.03292", "abs": "https://arxiv.org/abs/2510.03292", "authors": ["Do\u011fanay Demir", "\u0130lknur Durgar Elkahlout"], "title": "Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data", "categories": ["cs.CV"], "comment": null, "summary": "In an era dominated by video content, understanding its structure and\ndynamics has become increasingly important. This paper presents a hybrid\nframework that combines a distributed multi-GPU inference system with an\ninteractive visualization platform for analyzing celebrity dynamics in video\nepisodes. The inference framework efficiently processes large volumes of video\ndata by leveraging optimized ONNX models, heterogeneous batch inference, and\nhigh-throughput parallelism, ensuring scalable generation of timestamped\nappearance records. These records are then transformed into a comprehensive\nsuite of visualizations, including appearance frequency charts, duration\nanalyses, pie charts, co-appearance matrices, network graphs, stacked area\ncharts, seasonal comparisons, and heatmaps. Together, these visualizations\nprovide multi-dimensional insights into video content, revealing patterns in\ncelebrity prominence, screen-time distribution, temporal dynamics,\nco-appearance relationships, and intensity across episodes and seasons. The\ninteractive nature of the system allows users to dynamically explore data,\nidentify key moments, and uncover evolving relationships between individuals.\nBy bridging distributed recognition with structured, visually-driven analytics,\nthis work enables new possibilities for entertainment analytics, content\ncreation strategies, and audience engagement studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u591aGPU\u63a8\u7406\u7cfb\u7edf\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u5206\u6790\u89c6\u9891\u8282\u76ee\u4e2d\u7684\u540d\u4eba\u52a8\u6001\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u6d1e\u5bdf\u4ee5\u652f\u6301\u5a31\u4e50\u5206\u6790\u548c\u5185\u5bb9\u7b56\u7565\u3002", "motivation": "\u5728\u89c6\u9891\u5185\u5bb9\u65e5\u76ca\u4e3b\u5bfc\u7684\u65f6\u4ee3\uff0c\u7406\u89e3\u5176\u7ed3\u6784\u548c\u52a8\u6001\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\uff1a1) \u4e00\u4e2a\u5206\u5e03\u5f0f\u591aGPU\u63a8\u7406\u7cfb\u7edf\uff0c\u5229\u7528\u4f18\u5316\u7684ONNX\u6a21\u578b\u3001\u5f02\u6784\u6279\u91cf\u63a8\u7406\u548c\u9ad8\u541e\u5410\u5e76\u884c\u6027\uff0c\u9ad8\u6548\u5904\u7406\u5927\u91cf\u89c6\u9891\u6570\u636e\u5e76\u751f\u6210\u5e26\u65f6\u95f4\u6233\u7684\u51fa\u573a\u8bb0\u5f55\uff1b2) \u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u5c06\u8fd9\u4e9b\u8bb0\u5f55\u8f6c\u5316\u4e3a\u591a\u6837\u5316\u7684\u56fe\u8868\uff0c\u5305\u62ec\u51fa\u573a\u9891\u7387\u56fe\u3001\u65f6\u957f\u5206\u6790\u3001\u997c\u56fe\u3001\u5171\u73b0\u77e9\u9635\u3001\u7f51\u7edc\u56fe\u3001\u5806\u53e0\u9762\u79ef\u56fe\u3001\u5b63\u8282\u6027\u6bd4\u8f83\u548c\u70ed\u529b\u56fe\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u89c6\u9891\u5185\u5bb9\u7684\u591a\u7ef4\u5ea6\u6d1e\u5bdf\uff0c\u63ed\u793a\u540d\u4eba\u7684\u7a81\u51fa\u7a0b\u5ea6\u3001\u5c4f\u5e55\u65f6\u95f4\u5206\u5e03\u3001\u65f6\u95f4\u52a8\u6001\u3001\u5171\u73b0\u5173\u7cfb\u4ee5\u53ca\u8de8\u5267\u96c6\u548c\u5b63\u8282\u7684\u5f3a\u5ea6\u6a21\u5f0f\u3002\u5176\u4ea4\u4e92\u6027\u4f7f\u7528\u6237\u80fd\u591f\u52a8\u6001\u63a2\u7d22\u6570\u636e\uff0c\u8bc6\u522b\u5173\u952e\u65f6\u523b\u5e76\u53d1\u73b0\u4e2a\u4f53\u4e4b\u95f4\u4e0d\u65ad\u6f14\u53d8\u7684\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5e03\u5f0f\u8bc6\u522b\u4e0e\u7ed3\u6784\u5316\u3001\u89c6\u89c9\u9a71\u52a8\u7684\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5a31\u4e50\u5206\u6790\u3001\u5185\u5bb9\u521b\u4f5c\u7b56\u7565\u548c\u89c2\u4f17\u53c2\u4e0e\u5ea6\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.03244", "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in", "AI": {"tldr": "VIFO\u662f\u4e00\u79cd\u8de8\u6a21\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u5b83\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u8f6c\u5316\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff08LVM\uff09\u63d0\u53d6\u8de8\u901a\u9053\u6a21\u5f0f\uff0c\u5e76\u4e0e\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u878d\u5408\uff0c\u4ece\u800c\u9ad8\u6548\u6355\u6349\u53d8\u91cf\u95f4\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5e38\u91c7\u7528\u901a\u9053\u72ec\u7acb\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u8de8\u901a\u9053\u4f9d\u8d56\uff1b\u5f53\u524d\u591a\u6a21\u6001\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528LVM\u89e3\u91ca\u65f6\u7a7a\u6570\u636e\u7684\u80fd\u529b\uff1b\u4ee5\u53ca\u5229\u7528\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u7684\u6f5c\u529b\u5c1a\u672a\u5b8c\u5168\u6316\u6398\u3002", "method": "VIFO\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6e32\u67d3\u6210\u56fe\u50cf\uff0c\u4f7f\u9884\u8bad\u7ec3\u7684LVM\u80fd\u591f\u63d0\u53d6\u590d\u6742\u7684\u8de8\u901a\u9053\u6a21\u5f0f\u3002\u8fd9\u4e9b\u89c6\u89c9\u7279\u5f81\u968f\u540e\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u6001\u7684\u8868\u793a\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\u3002\u6a21\u578b\u901a\u8fc7\u51bb\u7ed3LVM\uff0c\u4ec5\u8bad\u7ec3\u51767.45%\u7684\u53c2\u6570\u3002", "result": "VIFO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "VIFO\u4e3a\u6355\u83b7\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8de8\u53d8\u91cf\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03377", "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "categories": ["cs.AI"], "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method.", "AI": {"tldr": "\u9488\u5bf9\u5b58\u5728\u963b\u585e\u7ea6\u675f\u7684\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff08BHFS\uff09\uff0c\u672c\u6587\u65e8\u5728\u540c\u65f6\u6700\u5c0f\u5316\u5b8c\u5de5\u65f6\u95f4\uff08makespan\uff09\u548c\u603b\u80fd\u8017\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\u3001\u589e\u5f3a\u578bepsilon-\u7ea6\u675f\u65b9\u6cd5\u4ee5\u53caRefined Iterated Pareto Greedy (RIPG) \u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5168\u7403\u7ecf\u6d4e\u9762\u4e34\u975e\u53ef\u518d\u751f\u80fd\u6e90\u7a00\u7f3a\u3001\u5730\u7f18\u653f\u6cbb\u51b2\u7a81\u3001\u80fd\u6e90\u4ef7\u683c\u4e0a\u6da8\u53ca\u6c14\u5019\u53d8\u5316\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u80fd\u6e90\u89e3\u51b3\u65b9\u6848\u3002\u5236\u9020\u4e1a\u4f5c\u4e3a\u4e3b\u8981\u80fd\u8017\u90e8\u95e8\uff0c\u8feb\u5207\u9700\u8981\u901a\u8fc7\u8282\u80fd\u8c03\u5ea6\u7b49\u65b9\u6cd5\u5feb\u901f\u964d\u4f4e\u80fd\u8017\u3002", "method": "1. \u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u65b0\u9896\u7684\u591a\u76ee\u6807\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684epsilon-\u7ea6\u675f\u65b9\u6cd5\u4ee5\u5bfb\u627e\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u30023. \u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u591a\u76ee\u6807\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5Refined Iterated Pareto Greedy (RIPG) \u4ee5\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\u30024. \u4f7f\u7528\u5c0f\u3001\u4e2d\u3001\u5927\u89c4\u6a21\u5b9e\u4f8b\u5bf9\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4e24\u79cd\u73b0\u6709\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7814\u7a76\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u963b\u585e\u8c03\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u5728\u6700\u5c0f\u5316\u5b8c\u5de5\u65f6\u95f4\u7684\u540c\u65f6\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u5236\u9020\u4e1a\u7684\u8282\u80fd\u8fd0\u8425\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2510.03807", "pdf": "https://arxiv.org/pdf/2510.03807", "abs": "https://arxiv.org/abs/2510.03807", "authors": ["Vaskar Chakma", "Wooyeol Choi"], "title": "6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)\ntechnology face critical limitations in achieving real-time performance for\nmission-critical industrial applications. Existing 5G-enabled systems suffer\nfrom latencies exceeding 10ms, which are inadequate for applications requiring\nsub-millisecond response times, such as autonomous industrial control and\npredictive maintenance. This research aims to develop and validate a 6G-enabled\nDigital Twin framework that achieves ultra-low latency communication and\nreal-time synchronization between physical industrial assets and their digital\ncounterparts, specifically targeting bearing fault detection as a critical\nindustrial use case. The proposed framework integrates terahertz communications\n(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence\nwithin a five-layer architecture. Experimental validation was conducted using\nthe Case Western Reserve University (CWRU) bearing dataset, implementing\ncomprehensive feature extraction (15 time and frequency domain features) and\nRandom Forest classification algorithms. The system performance was evaluated\nagainst traditional WiFi-6 and 5G networks across multiple metrics, including\nclassification accuracy, end-to-end latency, and scalability. It achieved 97.7%\nfault classification accuracy with 0.8ms end-to-end latency, representing a\n15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)\nnetworks. The system demonstrated superior scalability with sub-linear\nprocessing time growth and maintained consistent performance across four\nbearing fault categories (normal, inner race, outer race, and ball faults) with\nmacro-averaged F1-scores exceeding 97%.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a6G\u8d4b\u80fd\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u592a\u8d6b\u5179\u901a\u4fe1\u548c\u8fb9\u7f18AI\uff0c\u5b9e\u73b0\u4e860.8ms\u8d85\u4f4e\u5ef6\u8fdf\u548c97.7%\u9ad8\u7cbe\u5ea6\u8f74\u627f\u6545\u969c\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e5G\u548cWiFi-6\u7f51\u7edc\u3002", "motivation": "\u5f53\u524d\u96c6\u6210\u4e86\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u8d5b\u535a\u7269\u7406\u7cfb\u7edf(CPS)\u5728\u4efb\u52a1\u5173\u952e\u578b\u5de5\u4e1a\u5e94\u7528\u4e2d\u9762\u4e34\u5b9e\u65f6\u6027\u80fd\u7684\u4e25\u5cfb\u9650\u5236\u3002\u73b0\u67095G\u7cfb\u7edf\u5ef6\u8fdf\u8d85\u8fc710ms\uff0c\u65e0\u6cd5\u6ee1\u8db3\u81ea\u4e3b\u5de5\u4e1a\u63a7\u5236\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u7b49\u9700\u8981\u4e9a\u6beb\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u5c42\u67b6\u6784\u76846G\u8d4b\u80fd\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u592a\u8d6b\u5179\u901a\u4fe1(0.1-1 THz)\u3001\u667a\u80fd\u53cd\u5c04\u9762\u548c\u8fb9\u7f18\u4eba\u5de5\u667a\u80fd\u3002\u901a\u8fc7\u4f7f\u7528Case Western Reserve University (CWRU)\u8f74\u627f\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u91c7\u7528\u4e8615\u4e2a\u65f6\u9891\u57df\u7279\u5f81\u63d0\u53d6\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u7b97\u6cd5\u3002\u7cfb\u7edf\u6027\u80fd\u4e0e\u4f20\u7edfWiFi-6\u548c5G\u7f51\u7edc\u5728\u5206\u7c7b\u51c6\u786e\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e8697.7%\u7684\u6545\u969c\u5206\u7c7b\u51c6\u786e\u7387\u548c0.8ms\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u4e0eWiFi-6\uff0812.5ms\uff09\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u6027\u80fd\u63d0\u5347\u4e8615.6\u500d\uff1b\u4e0e5G\uff084.2ms\uff09\u76f8\u6bd4\uff0c\u63d0\u5347\u4e865.25\u500d\u3002\u7cfb\u7edf\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\uff08\u5904\u7406\u65f6\u95f4\u5448\u4e9a\u7ebf\u6027\u589e\u957f\uff09\uff0c\u5e76\u5728\u56db\u79cd\u8f74\u627f\u6545\u969c\u7c7b\u522b\uff08\u6b63\u5e38\u3001\u5185\u5708\u3001\u5916\u5708\u548c\u6eda\u73e0\u6545\u969c\uff09\u4e0a\u5747\u4fdd\u6301\u4e86\u8d85\u8fc797%\u7684\u5b8f\u5e73\u5747F1\u5f97\u5206\u3002", "conclusion": "\u8be56G\u8d4b\u80fd\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5de5\u4e1a\u5e94\u7528\u4e2d\u8d85\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u548c\u5b9e\u65f6\u540c\u6b65\u7684\u6311\u6218\uff0c\u4e3a\u4efb\u52a1\u5173\u952e\u578b\u5de5\u4e1a\u63a7\u5236\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u53ef\u9760\u6027\u548c\u9ad8\u53ef\u6269\u5c55\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03384", "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6267\u884c\u4e3b\u89c2\u65e5\u5e38\u4efb\u52a1\u65f6\u6240\u5c55\u73b0\u7684\u9690\u6027\u4ef7\u503c\u89c2\uff0c\u901a\u5e38\u4e0e\u4eba\u7c7b\u53ca\u5176\u4ed6LLM\u4e0d\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff08\u7531LLM\u9a71\u52a8\uff09\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5bf9\u4e8e\u5b83\u4eec\u5728\u5b8c\u6210\u4e3b\u89c2\u65e5\u5e38\u4efb\u52a1\uff08\u5982\u63a8\u8350\u3001\u8ba1\u7b97\uff09\u65f6\u6240\u5c55\u73b0\u7684\u9690\u6027\u4ef7\u503c\u89c2\uff08\u5982\u73af\u4fdd\u4e3b\u4e49\u3001\u6148\u5584\u3001\u591a\u6837\u6027\uff09\u77e5\u4e4b\u751a\u5c11\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5c55\u73b0\u8fd9\u4e9b\u4ef7\u503c\u89c2\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u4eba\u7c7b\u7684\u6bd4\u8f83\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5ba1\u8ba1\u516d\u4e2a\u6d41\u884c\u7684LLM\u5b8c\u621030\u9879\u65e5\u5e38\u4efb\u52a1\u7684\u8868\u73b0\u6765\u56de\u7b54\u7814\u7a76\u95ee\u9898\u3002\u7814\u7a76\u56e2\u961f\u5c06LLM\u4e4b\u95f4\u4ee5\u53caLLM\u4e0e\u6765\u81ea\u7f8e\u56fd\u7684100\u540d\u4eba\u7c7b\u4f17\u5305\u5de5\u4f5c\u8005\u7684\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u5728\u6240\u5c55\u73b0\u7684\u9690\u6027\u4ef7\u503c\u89c2\u65b9\u9762\uff0c\u7ecf\u5e38\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\uff0c\u4e5f\u4e0e\u5176\u4ed6LLM\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "LLM\u5728\u6267\u884c\u4e3b\u89c2\u65e5\u5e38\u4efb\u52a1\u65f6\uff0c\u5176\u5185\u5728\u7684\u4ef7\u503c\u89c2\u5c55\u73b0\u51fa\u663e\u8457\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u8fd9\u8868\u660e\u5728AI\u52a9\u624b\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u5173\u6ce8\u5176\u4ef7\u503c\u89c2\u5bf9\u7528\u6237\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.03294", "pdf": "https://arxiv.org/pdf/2510.03294", "abs": "https://arxiv.org/abs/2510.03294", "authors": ["Saanvi Kataria"], "title": "Domain-Robust Marine Plastic Detection Using Vision Models", "categories": ["cs.CV"], "comment": "16 pages, 5 figures, 1 table", "summary": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u4e8e\u6c34\u4e0b\u5851\u6599\u68c0\u6d4b\u7684\u8de8\u57df\u9c81\u68d2\u6027\u6a21\u578b\uff0c\u53d1\u73b0\u8f7b\u91cf\u7ea7MobileNetV2\u6027\u80fd\u6700\u4f73\uff08F1 0.97\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5fae\u8c03\u6a21\u578b\u548c\u96f6\u6837\u672c\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u6d77\u6d0b\u5851\u6599\u6c61\u67d3\u6784\u6210\u4e25\u91cd\u73af\u5883\u5a01\u80c1\uff0c\u56e0\u6b64\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6c34\u4e0b\u788e\u7247\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u89c6\u89c9\u7cfb\u7edf\u5e38\u56e0\u57df\u504f\u79fb\u800c\u5728\u65b0\u56fe\u50cf\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728\u6807\u8bb0\u7684\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08MobileNetV2\u3001ResNet-18\u3001EfficientNet-B0\uff09\u548c\u89c6\u89c9Transformer\uff08DeiT-Tiny\u3001ViT-B16\uff09\uff0c\u7136\u540e\u5728\u4e00\u4e2a\u5e73\u8861\u7684\u8de8\u57df\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u6d4b\u8bd5\u96c6\u7531\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u542b\u5851\u6599\u56fe\u50cf\u548c\u8bad\u7ec3\u57df\u7684\u8d1f\u6837\u672c\u7ec4\u6210\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u4e24\u4e2a\u96f6\u6837\u672c\u6a21\u578b\uff1aCLIP ViT-L14\u548cGoogle\u7684Gemini 2.0 Flash\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8f7b\u91cf\u7ea7MobileNetV2\u5b9e\u73b0\u4e86\u6700\u5f3a\u7684\u8de8\u57df\u6027\u80fd\uff08F1 0.97\uff09\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u6a21\u578b\u3002\u6240\u6709\u5fae\u8c03\u6a21\u578b\u90fd\u8fbe\u5230\u4e86\u9ad8\u7cbe\u5ea6\uff08\u7ea699%\uff09\uff0c\u4f46\u5728\u53ec\u56de\u7387\u4e0a\u6709\u6240\u5dee\u5f02\u3002\u96f6\u6837\u672cCLIP\u5177\u6709\u8f83\u9ad8\u7684\u654f\u611f\u6027\uff08\u53ec\u56de\u7387\u7ea680%\uff09\uff0c\u4f46\u6613\u4ea7\u751f\u8bef\u62a5\uff08\u7cbe\u5ea6\u7ea656%\uff09\uff0c\u800cGemini\u5219\u8868\u73b0\u51fa\u76f8\u53cd\u7684\u7279\u5f81\uff08\u7cbe\u5ea6\u7ea699%\uff0c\u53ec\u56de\u7387\u7ea681%\uff09\u3002\u9519\u8bef\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u5e38\u5c06\u73ca\u745a\u7eb9\u7406\u3001\u60ac\u6d6e\u9897\u7c92\u548c\u955c\u9762\u7729\u5149\u8bef\u8bc6\u522b\u4e3a\u5851\u6599\u3002", "conclusion": "\u603b\u4f53\u800c\u8a00\uff0c\u7d27\u51d1\u578bCNN\u901a\u8fc7\u6709\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0\u8de8\u57df\u6c34\u4e0b\u68c0\u6d4b\u7684\u6cdb\u5316\uff0c\u800c\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5219\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.03245", "pdf": "https://arxiv.org/pdf/2510.03245", "abs": "https://arxiv.org/abs/2510.03245", "authors": ["Ali Yavari", "Alireza Mohamadi", "Elham Beydaghi", "Rainer A. Leitgeb"], "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of\nreal world noise and intentional perturbations remains a significant challenge.\nTo address this, attribution methods have been proposed, though their efficacy\nremains suboptimal and necessitates further refinement. In this paper, we\npropose a novel category of transferable adversarial attacks, called\ntransferable frequency-aware attacks, enabling frequency-aware exploration via\nboth high-and low-frequency components. Based on this type of attacks, we also\npropose a novel attribution method, named Frequency-Aware Model Parameter\nExplorer (FAMPE), which improves the explainability for DNNs. Relative to the\ncurrent state-of-the-art method AttEXplore, our FAMPE attains an average gain\nof 13.02% in Insertion Score, thereby outperforming existing approaches.\nThrough detailed ablation studies, we also investigate the role of both high-\nand low-frequency components in explainability.", "AI": {"tldr": "\u4e3a\u89e3\u51b3DNN\u5728\u566a\u58f0\u548c\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u8fc1\u79fb\u7684\u9891\u7387\u611f\u77e5\u5bf9\u6297\u653b\u51fb\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u578b\u5f52\u56e0\u65b9\u6cd5FAMPE\uff0c\u663e\u8457\u63d0\u5347\u4e86DNN\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728Insertion Score\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8613.02%\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u548c\u6709\u610f\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\uff08attribution methods\uff09\u7684\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u8fc1\u79fb\u9891\u7387\u611f\u77e5\u5bf9\u6297\u653b\u51fb\uff08transferable frequency-aware attacks\uff09\uff0c\u901a\u8fc7\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\u8fdb\u884c\u9891\u7387\u611f\u77e5\u63a2\u7d22\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFrequency-Aware Model Parameter Explorer (FAMPE) \u7684\u65b0\u578b\u5f52\u56e0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8DNN\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5AttEXplore\uff0cFAMPE\u5728\u63d2\u5165\u5206\u6570\uff08Insertion Score\uff09\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8613.02%\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\u5728\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "FAMPE\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u611f\u77e5\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2510.03399", "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u81ea\u8eab\u751f\u6210\u6587\u672c\u65b9\u9762\u666e\u904d\u5931\u8d25\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u504f\u89c1\uff0c\u5c3d\u7ba1\u5bf9\u81ea\u8eab\u5b58\u5728\u6709\u4e00\u5b9a\u8ba4\u77e5\uff0c\u4f46\u63a8\u7406\u5b58\u5728\u5c42\u7ea7\u504f\u89c1\u3002", "motivation": "\u81ea\u6211\u8bc6\u522b\u5bf9AI\u7cfb\u7edf\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u5173\u4e8e\u6a21\u578b\u662f\u5426\u5177\u5907\u81ea\u6211\u8bc6\u522b\u7684\u89e3\u91ca\u5b58\u5728\u77db\u76fe\uff0c\u4fc3\u4f7f\u672c\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u5143\u81ea\u6211\u8bc6\u522b\u548c\u7cbe\u786e\u6a21\u578b\u9884\u6d4b\u4e24\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8610\u4e2a\u4e3b\u6d41LLM\u8bc6\u522b\u81ea\u8eab\u53ca\u5176\u4ed6\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u9996\u6b21\u8bc4\u4f30\u4e86\u6a21\u578b\u5bf9\u81ea\u5df1\u548c\u4ed6\u4eba\u5b58\u5728\u7684\u8ba4\u77e5\u53ca\u5176\u9009\u62e9\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u81ea\u6211\u8bc6\u522b\u4e0a\u666e\u904d\u5931\u8d25\uff0c10\u4e2a\u6a21\u578b\u4e2d\u4ec5\u67094\u4e2a\u80fd\u8bc6\u522b\u81ea\u8eab\uff0c\u4e14\u8868\u73b0\u5f88\u5c11\u9ad8\u4e8e\u968f\u673a\u731c\u6d4b\u3002\u6a21\u578b\u8868\u73b0\u51fa\u5bf9\u9884\u6d4bGPT\u548cClaude\u5bb6\u65cf\u7684\u5f3a\u70c8\u504f\u89c1\u3002\u6a21\u578b\u5bf9\u81ea\u8eab\u53ca\u5176\u4ed6\u6a21\u578b\u7684\u5b58\u5728\u6709\u4e00\u5b9a\u8ba4\u77e5\uff0c\u4f46\u5176\u63a8\u7406\u63ed\u793a\u4e86\u5c42\u7ea7\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u5c06GPT\u3001Claude\uff08\u5076\u5c14Gemini\uff09\u89c6\u4e3a\u9876\u7ea7\u6a21\u578b\u5e76\u5c06\u5176\u4e0e\u9ad8\u8d28\u91cf\u6587\u672c\u5173\u8054\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9AI\u5b89\u5168\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1AI\u9002\u5f53\u7684\u81ea\u6211\u610f\u8bc6\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.03829", "pdf": "https://arxiv.org/pdf/2510.03829", "abs": "https://arxiv.org/abs/2510.03829", "authors": ["Andr\u00e9 Coelho", "Pedro Ribeiro", "Helder Fontes", "Rui Campos"], "title": "A4FN: an Agentic AI Architecture for Autonomous Flying Networks", "categories": ["cs.NI", "cs.AI"], "comment": "This paper has been accepted for presentation in the Auto ML for\n  Zero-Touch Network Management Workshop (WS04-01) at the IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "This position paper presents A4FN, an Agentic Artificial Intelligence (AI)\narchitecture for intent-driven automation in Flying Networks (FNs) using\nUnmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI\nand Large Language Models (LLMs) to enable real-time, context-aware network\ncontrol via a distributed agentic system. It comprises two components: the\nPerception Agent (PA), which semantically interprets multimodal input --\nincluding imagery, audio, and telemetry data -- from UAV-mounted sensors to\nderive Service Level Specifications (SLSs); and the Decision-and-Action Agent\n(DAA), which reconfigures the network based on inferred intents. A4FN embodies\nkey properties of Agentic AI, including autonomy, goal-driven reasoning, and\ncontinuous perception-action cycles. Designed for mission-critical,\ninfrastructure-limited scenarios such as disaster response, it supports\nadaptive reconfiguration, dynamic resource management, and interoperability\nwith emerging wireless technologies. The paper details the A4FN architecture,\nits core innovations, and open research challenges in multi-agent coordination\nand Agentic AI integration in next-generation FNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86A4FN\uff0c\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u548cLLM\u7684\u667a\u80fd\u4f53AI\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u98de\u884c\u7f51\u7edc\u4e2d\u5b9e\u73b0\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u548c\u5b9e\u65f6\u7f51\u7edc\u63a7\u5236\u3002", "motivation": "\u5728\u707e\u96be\u54cd\u5e94\u7b49\u4efb\u52a1\u5173\u952e\u3001\u57fa\u7840\u8bbe\u65bd\u53d7\u9650\u7684\u573a\u666f\u4e2d\uff0c\u9700\u8981\u4e3a\u98de\u884c\u7f51\u7edc\u63d0\u4f9b\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u3001\u5b9e\u65f6\u4e0a\u4e0b\u6587\u611f\u77e5\u7f51\u7edc\u63a7\u5236\uff0c\u4ee5\u53ca\u9002\u5e94\u6027\u91cd\u914d\u7f6e\u548c\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u80fd\u529b\u3002", "method": "A4FN\u901a\u8fc7\u4e00\u4e2a\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b0\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u611f\u77e5\u667a\u80fd\u4f53\uff08PA\uff09\uff0c\u8d1f\u8d23\u8bed\u4e49\u89e3\u91ca\u591a\u6a21\u6001\u8f93\u5165\uff08\u56fe\u50cf\u3001\u97f3\u9891\u3001\u9065\u6d4b\u6570\u636e\uff09\u4ee5\u63a8\u5bfc\u670d\u52a1\u7b49\u7ea7\u89c4\u8303\uff08SLS\uff09\uff1b\u51b3\u7b56-\u884c\u52a8\u667a\u80fd\u4f53\uff08DAA\uff09\uff0c\u6839\u636e\u63a8\u65ad\u7684\u610f\u56fe\u91cd\u65b0\u914d\u7f6e\u7f51\u7edc\u3002\u8be5\u67b6\u6784\u5177\u5907\u667a\u80fd\u4f53AI\u7684\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u9a71\u52a8\u63a8\u7406\u548c\u6301\u7eed\u611f\u77e5-\u884c\u52a8\u5faa\u73af\u7b49\u7279\u6027\u3002", "result": "A4FN\u67b6\u6784\u80fd\u591f\u652f\u6301\u81ea\u9002\u5e94\u91cd\u914d\u7f6e\u3001\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u5e76\u4e0e\u65b0\u5174\u65e0\u7ebf\u6280\u672f\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\uff0c\u9002\u7528\u4e8e\u4efb\u52a1\u5173\u952e\u578b\u548c\u57fa\u7840\u8bbe\u65bd\u53d7\u9650\u573a\u666f\u3002", "conclusion": "\u8bba\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86A4FN\u67b6\u6784\u53ca\u5176\u6838\u5fc3\u521b\u65b0\uff0c\u5e76\u6307\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u548c\u667a\u80fd\u4f53AI\u5728\u4e0b\u4e00\u4ee3\u98de\u884c\u7f51\u7edc\u4e2d\u96c6\u6210\u7b49\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002"}}
{"id": "2510.03439", "pdf": "https://arxiv.org/pdf/2510.03439", "abs": "https://arxiv.org/abs/2510.03439", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Morpheme Induction for Emergent Language", "categories": ["cs.CL", "I.2.7; I.6.m"], "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 16 pages, 4 figures", "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language\ncorpora of parallel utterances and meanings. It is a greedy algorithm that (1)\nweights morphemes based on mutual information between forms and meanings, (2)\nselects the highest-weighted pair, (3) removes it from the corpus, and (4)\nrepeats the process to induce further morphemes (i.e., Count, Select, Ablate,\nRepeat). The effectiveness of CSAR is first validated on procedurally generated\ndatasets and compared against baselines for related tasks. Second, we validate\nCSAR's performance on human language data to show that the algorithm makes\nreasonable predictions in adjacent domains. Finally, we analyze a handful of\nemergent languages, quantifying linguistic characteristics like degree of\nsynonymy and polysemy.", "AI": {"tldr": "CSAR\u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u65b0\u5174\u8bed\u8a00\u8bed\u6599\u4e2d\u5f52\u7eb3\u8bcd\u7d20\u7684\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f62\u5f0f\u4e0e\u610f\u4e49\u7684\u4e92\u4fe1\u606f\u52a0\u6743\u9009\u62e9\u8bcd\u7d20\uff0c\u5e76\u5728\u751f\u6210\u6570\u636e\u96c6\u548c\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4ece\u5e76\u884c\u8bed\u6599\uff08\u5305\u542b\u8bdd\u8bed\u548c\u610f\u4e49\uff09\u4e2d\u5f52\u7eb3\u65b0\u5174\u8bed\u8a00\u7684\u8bcd\u7d20\u3002", "method": "\u5f15\u5165CSAR\u7b97\u6cd5\uff0c\u4e00\u4e2a\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u5176\u6b65\u9aa4\u4e3a\uff1a1) \u57fa\u4e8e\u5f62\u5f0f\u548c\u610f\u4e49\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5bf9\u8bcd\u7d20\u8fdb\u884c\u52a0\u6743\uff1b2) \u9009\u62e9\u6743\u91cd\u6700\u9ad8\u7684\u8bcd\u7d20\u5bf9\uff1b3) \u5c06\u5176\u4ece\u8bed\u6599\u4e2d\u79fb\u9664\uff1b4) \u91cd\u590d\u6b64\u8fc7\u7a0b\uff08\u5373\u8ba1\u6570\u3001\u9009\u62e9\u3001\u6d88\u878d\u3001\u91cd\u590d\uff09\u3002", "result": "CSAR\u5728\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u76f8\u5173\u4efb\u52a1\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5728\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u5408\u7406\u7684\u9884\u6d4b\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u91cf\u5316\u5206\u6790\u4e86\u4e00\u4e9b\u65b0\u5174\u8bed\u8a00\u7684\u540c\u4e49\u6027\u548c\u591a\u4e49\u6027\u7b49\u8bed\u8a00\u7279\u5f81\u3002", "conclusion": "CSAR\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u4ece\u65b0\u5174\u8bed\u8a00\u4e2d\u5f52\u7eb3\u8bcd\u7d20\uff0c\u5e76\u5728\u90bb\u8fd1\u9886\u57df\u505a\u51fa\u5408\u7406\u9884\u6d4b\uff0c\u540c\u65f6\u4e3a\u5206\u6790\u65b0\u5174\u8bed\u8a00\u7684\u8bed\u8a00\u7279\u5f81\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2510.03295", "pdf": "https://arxiv.org/pdf/2510.03295", "abs": "https://arxiv.org/abs/2510.03295", "authors": ["Passant Elchafei", "Amany Fashwan"], "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "We present VLCAP, an Arabic image captioning framework that integrates\nCLIP-based visual label retrieval with multimodal text generation. Rather than\nrelying solely on end-to-end captioning, VLCAP grounds generation in\ninterpretable Arabic visual concepts extracted with three multilingual\nencoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label\nretrieval. A hybrid vocabulary is built from training captions and enriched\nwith about 21K general domain labels translated from the Visual Genome dataset,\ncovering objects, attributes, and scenes. The top-k retrieved labels are\ntransformed into fluent Arabic prompts and passed along with the original image\nto vision-language models. In the second stage, we tested Qwen-VL and Gemini\nPro Vision for caption generation, resulting in six encoder-decoder\nconfigurations. The results show that mCLIP + Gemini Pro Vision achieved the\nbest BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL\nobtained the highest LLM-judge score (36.33%). This interpretable pipeline\nenables culturally coherent and contextually accurate Arabic captions.", "AI": {"tldr": "VLCAP\u662f\u4e00\u4e2a\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8eCLIP\u7684\u89c6\u89c9\u6807\u7b7e\u68c0\u7d22\u548c\u591a\u6a21\u6001\u6587\u672c\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u6587\u5316\u4e0a\u4e00\u81f4\u7684\u5b57\u5e55\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u53ef\u80fd\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u53ca\u6587\u5316\u76f8\u5173\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u6982\u5ff5\u6765\u751f\u6210\u66f4\u5177\u6587\u5316\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u5b57\u5e55\u3002", "method": "VLCAP\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a1. \u4f7f\u7528mCLIP\u3001AraCLIP\u548cJina V4\u4e09\u79cd\u591a\u8bed\u8a00\u7f16\u7801\u5668\u8fdb\u884cCLIP-based\u89c6\u89c9\u6807\u7b7e\u68c0\u7d22\uff0c\u5229\u7528\u5305\u542b21K\u901a\u7528\u9886\u57df\u6807\u7b7e\u7684\u6df7\u5408\u8bcd\u6c47\u8868\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u963f\u62c9\u4f2f\u8bed\u89c6\u89c9\u6982\u5ff5\u30022. \u5c06\u68c0\u7d22\u5230\u7684top-k\u6807\u7b7e\u8f6c\u6362\u4e3a\u6d41\u7545\u7684\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\uff0c\u5e76\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u8d77\u8f93\u5165Qwen-VL\u6216Gemini Pro Vision\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b57\u5e55\u751f\u6210\u3002\u5171\u6d4b\u8bd5\u4e86\u516d\u79cd\u7f16\u7801\u5668-\u89e3\u7801\u5668\u914d\u7f6e\u3002", "result": "mCLIP + Gemini Pro Vision\u914d\u7f6e\u5728BLEU-1 (5.34%) \u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6 (60.01%) \u4e0a\u8868\u73b0\u6700\u4f73\u3002\u800cAraCLIP + Qwen-VL\u914d\u7f6e\u5728LLM-judge\u5206\u6570\u4e0a\u83b7\u5f97\u6700\u9ad8\u5206 (36.33%)\u3002", "conclusion": "VLCAP\u63d0\u4f9b\u7684\u53ef\u89e3\u91ca\u6027\u7ba1\u9053\u80fd\u591f\u751f\u6210\u6587\u5316\u4e0a\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u51c6\u786e\u7684\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u5b57\u5e55\u3002"}}
{"id": "2510.03246", "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale.", "AI": {"tldr": "STRUPRUNE\u662f\u4e00\u79cdADMM\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u526a\u679d\uff0c\u6027\u80fd\u5ab2\u7f8e\u5168\u5c40\u526a\u679d\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u81f3O($\\sqrt{N}$)\uff0c\u652f\u6301\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u526a\u679d\u9762\u4e34\u6311\u6218\uff1a\u5168\u5c40\u526a\u679d\u5185\u5b58\u5f00\u9500\u5927(O(N))\uff0c\u5c40\u90e8\u526a\u679d\u6027\u80fd\u4e0d\u4f73\u4e14\u5ffd\u89c6\u5c42\u95f4\u4f9d\u8d56\uff0c\u7ed3\u6784\u5316\u526a\u679d\u901a\u5e38\u4f9d\u8d56\u5168\u5c40\u526a\u679d\u4e14\u5728\u5c40\u90e8\u4f18\u5316\u4e0b\u6027\u80fd\u6613\u9000\u5316\u3002\u7814\u7a76\u76ee\u6807\u662f\u540c\u65f6\u5b9e\u73b0\u7ed3\u6784\u5316\u526a\u679d\u7684\u786c\u4ef6\u6548\u7387\u548c\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5206\u6cbb\u7b56\u7565\uff0c\u5c06\u5168\u5c40\u526a\u679d\u95ee\u9898\u5206\u89e3\u4e3a\u8de8\u6a21\u5757\u7684\u534f\u8c03\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u53ef\u9002\u5e94\u6709\u9650\u7684GPU\u5185\u5b58\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8eADMM\u7684STRUPRUNE\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u7a00\u758f\u6027\u96c6\u6210\u5230\u526a\u679d\u8fc7\u7a0b\u4e2d\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\u7684\u786c\u4ef6\u517c\u5bb9\u6027\u3002\u63a8\u5bfc\u4e86\u7ed3\u6784\u5316\u526a\u679d\u63a9\u7801\u7684\u95ed\u5f0f\u89e3\u6790\u89e3\uff0c\u63d0\u4f9b\u4e86\u5c42\u7ea7\u7a00\u758f\u6027\u5206\u914d\u89c4\u5219\uff1b\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u80fd\u91cf\u7684\u6e10\u8fd1\u6846\u67b6\uff0c\u751f\u6210softmax\u5f62\u5f0f\u7684\u5206\u914d\u65b9\u6848\uff0c\u7b80\u5316\u4f18\u5316\u5e76\u9002\u5e94\u5c42\u95f4\u91cd\u8981\u6027\u5dee\u5f02\u3002", "result": "STRUPRUNE\u5728\u56f0\u60d1\u5ea6\uff08perplexity\uff09\u4e0a\u4e0e\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u6027\u80fd\u6301\u5e73\uff0c\u540c\u65f6\u5c06\u5185\u5b58\u6210\u672c\u4eceO(N)\u964d\u4f4e\u5230O($\\sqrt{N}$)\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u9645\u90e8\u7f72\u5728\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u3002", "conclusion": "STRUPRUNE\u901a\u8fc7\u521b\u65b0\u7684\u5206\u6cbb\u7b56\u7565\u548cADMM\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u526a\u679d\u7684\u5185\u5b58\u9ad8\u6548\u548c\u9ad8\u6027\u80fd\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u526a\u679d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u548c\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2510.03418", "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance.", "AI": {"tldr": "\u4e3a\u89e3\u51b3RAG\u5728\u4f01\u4e1a\u5e94\u7528\u4e2d\u56e0\u8bc1\u636e\u77db\u76fe\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86ContraGen\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4f01\u4e1a\u6587\u6863\u7684\u77db\u76fe\u611f\u77e5\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6587\u6863\u5185\u53ca\u8de8\u6587\u6863\u4e00\u81f4\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u5728\u4f01\u4e1a\u5e94\u7528\u4e2d\uff0c\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u77db\u76fe\u4f1a\u5bfc\u81f4\u8f93\u51fa\u4e0d\u4e00\u81f4\u6216\u4e0d\u53ef\u4fe1\uff0c\u8fd9\u5728\u5408\u89c4\u3001\u6cbb\u7406\u548c\u95ee\u8d23\u5236\u81f3\u5173\u91cd\u8981\u7684\u4f01\u4e1a\u73af\u5883\u4e2d\u5c24\u5176\u4e25\u91cd\u3002\u73b0\u6709\u77db\u76fe\u68c0\u6d4b\u57fa\u51c6\u4ec5\u9650\u4e8e\u53e5\u5b50\u5c42\u9762\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u4f01\u4e1a\u6587\u6863\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51faContraGen\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5e26\u6709\u5d4c\u5165\u5f0f\u77db\u76fe\u7684\u5408\u6210\u4f01\u4e1a\u98ce\u683c\u6587\u6863\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6587\u6863\u5185\u90e8\u548c\u8de8\u6587\u6863\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u81ea\u52a8\u5316\u77db\u76fe\u6316\u6398\u4e0e\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5e76\u6db5\u76d6\u5efa\u6a21\u77db\u76fe\u7c7b\u578b\u5206\u7c7b\u3001\u53d7\u63a7\u521b\u5efa\u81ea\u77db\u76fe\u548c\u6210\u5bf9\u77db\u76fe\u3001\u5f00\u53d1\u77db\u76fe\u611f\u77e5\u68c0\u7d22\u8bc4\u4f30\u7ba1\u9053\u53ca\u5f15\u5165\u4eba\u5de5\u76d1\u7763\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u903c\u771f\u4f01\u4e1a\u6587\u6863\u3001\u8bc6\u522b\u5e76\u5206\u7c7b\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u77db\u76fe\u7c7b\u578b\u3001\u53d7\u63a7\u521b\u5efa\u4e0d\u540c\u77db\u76fe\u5f62\u5f0f\u3001\u4ee5\u53ca\u5305\u542b\u4eba\u5de5\u76d1\u7763\u7684\u77db\u76fe\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff08ContraGen\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u4f01\u4e1a\u4fe1\u606f\u67e5\u8be2\u5e94\u7528\u4e2d\u6784\u5efa\u66f4\u53ef\u4fe1\u3001\u66f4\u8d1f\u8d23\u4efb\u7684RAG\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u77db\u76fe\u68c0\u6d4b\u548c\u89e3\u51b3\u5bf9\u4e8e\u964d\u4f4e\u98ce\u9669\u3001\u786e\u4fdd\u5408\u89c4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04035", "pdf": "https://arxiv.org/pdf/2510.04035", "abs": "https://arxiv.org/abs/2510.04035", "authors": ["Almamoon Alauthman", "Abeer Al-Hyari"], "title": "Analysis of LTE/5G Network Performance Parameters in Smartphone Use Cases: A Study of Packet Loss, Delay, and Slice Types", "categories": ["cs.NI"], "comment": null, "summary": "The paper addresses optimizing two of the most important performance\nparameters, packet loss, and delay, in the critical path optimization of LTE\nand 5G networks using metaheuristic algorithms to play a vital role in the\nsmartphone user experience. In this context, nine metaheuristic algorithms,\nsuch as WOA, PSO, and ABC, have been studied for their effectiveness in various\nslices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that\nWOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO\nfollowed closely with a 30% packet loss reduction with a decrease of 6.1 ms in\ndelay. In most scenarios, ABC accomplished good results with a packet loss\nreduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results\nemphasize how selecting appropriate algorithms based on the intended network\nslice is crucial for optimizing resource utilization and network efficiency. It\nprovides a quantitative framework for assessing and improving the reliability\nand responsiveness of an LTE/5G network. It encourages more research in hybrid\noptimization techniques and real-time adaptation mechanisms for further\nimprovements", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316LTE\u548c5G\u7f51\u7edc\u4e2d\u7684\u4e22\u5305\u548c\u5ef6\u8fdf\u3002\u7ed3\u679c\u663e\u793aWOA\u8868\u73b0\u6700\u4f73\uff0c\u5176\u6b21\u662fPSO\u548cABC\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u7f51\u7edc\u5207\u7247\u9009\u62e9\u7b97\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u63d0\u5347\u667a\u80fd\u624b\u673a\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u4f18\u5316LTE\u548c5G\u7f51\u7edc\u5173\u952e\u8def\u5f84\u4e2d\u7684\u4e24\u4e2a\u91cd\u8981\u6027\u80fd\u53c2\u6570\uff1a\u4e22\u5305\u548c\u5ef6\u8fdf\u3002", "method": "\u7814\u7a76\u4e86\u5305\u62ecWOA\u3001PSO\u548cABC\u5728\u5185\u7684\u4e5d\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728eMBB\u3001URLLC\u548cmMTC\u7b49\u591a\u79cd\u7f51\u7edc\u5207\u7247\u4e2d\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u7684\u6709\u6548\u6027\u3002", "result": "WOA\u8868\u73b0\u6700\u4f73\uff0c\u4e22\u5305\u7387\u964d\u4f4e31%\uff0c\u5ef6\u8fdf\u51cf\u5c116.3\u6beb\u79d2\uff1bPSO\u7d27\u968f\u5176\u540e\uff0c\u4e22\u5305\u7387\u964d\u4f4e30%\uff0c\u5ef6\u8fdf\u51cf\u5c116.1\u6beb\u79d2\uff1bABC\u5728\u591a\u6570\u573a\u666f\uff0c\u7279\u522b\u662fmMTC\u4e2d\uff0c\u4e22\u5305\u7387\u964d\u4f4e29%\uff0c\u5ef6\u8fdf\u51cf\u5c116\u6beb\u79d2\u3002", "conclusion": "\u6839\u636e\u76ee\u6807\u7f51\u7edc\u5207\u7247\u9009\u62e9\u5408\u9002\u7684\u4f18\u5316\u7b97\u6cd5\u5bf9\u4e8e\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u548c\u7f51\u7edc\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u5e76\u9f13\u52b1\u672a\u6765\u5728\u6df7\u5408\u4f18\u5316\u6280\u672f\u548c\u5b9e\u65f6\u9002\u5e94\u673a\u5236\u65b9\u9762\u8fdb\u884c\u66f4\u591a\u7814\u7a76\u3002"}}
{"id": "2510.03458", "pdf": "https://arxiv.org/pdf/2510.03458", "abs": "https://arxiv.org/abs/2510.03458", "authors": ["Mengyao Xu", "Wenfei Zhou", "Yauhen Babakhin", "Gabriel Moreira", "Ronay Ak", "Radek Osmulski", "Bo Liu", "Even Oldridge", "Benedikt Schifferer"], "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video", "categories": ["cs.CL"], "comment": null, "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding\nmodel developed to handle the increasing complexity of real-world information\nneeds. While Retrieval-Augmented Generation (RAG) has significantly advanced\nlanguage models by incorporating external knowledge, existing text-based\nretrievers rely on clean, structured input and struggle with the visually and\nsemantically rich content found in real-world documents such as PDFs, slides,\nor videos. Recent work such as ColPali has shown that preserving document\nlayout using image-based representations can improve retrieval quality.\nBuilding on this, and inspired by the capabilities of recent multimodal models\nsuch as Qwen2.5-Omni, we extend retrieval beyond text and images to also\nsupport audio and video modalities. Omni-Embed-Nemotron enables both\ncross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)\nretrieval using a single model. We describe the architecture, training setup,\nand evaluation results of Omni-Embed-Nemotron, and demonstrate its\neffectiveness in text, image, and video retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Omni-Embed-Nemotron\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u68c0\u7d22\u5d4c\u5165\u6a21\u578b\uff0c\u65e8\u5728\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u9700\u6c42\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u8de8\u6a21\u6001\u53ca\u8054\u5408\u6a21\u6001\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u7cfb\u7edf\u5728RAG\u5e94\u7528\u4e2d\uff0c\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u6587\u6863\uff08\u5982PDF\u3001\u5e7b\u706f\u7247\u3001\u89c6\u9891\uff09\u4e2d\u89c6\u89c9\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u590d\u6742\u5185\u5bb9\uff0c\u56e0\u5176\u4f9d\u8d56\u4e8e\u5e72\u51c0\u3001\u7ed3\u6784\u5316\u7684\u8f93\u5165\u3002", "method": "\u501f\u9274ColPali\u901a\u8fc7\u56fe\u50cf\u8868\u793a\u4fdd\u7559\u6587\u6863\u5e03\u5c40\u7684\u601d\u8def\uff0c\u5e76\u53d7Qwen2.5-Omni\u7b49\u591a\u6a21\u6001\u6a21\u578b\u7684\u542f\u53d1\uff0c\u5c06\u68c0\u7d22\u80fd\u529b\u6269\u5c55\u81f3\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u5355\u4e00\u67b6\u6784\u5b9e\u73b0\u8de8\u6a21\u6001\uff08\u5982\u6587\u672c-\u89c6\u9891\uff09\u548c\u8054\u5408\u6a21\u6001\uff08\u5982\u6587\u672c-\u89c6\u9891+\u97f3\u9891\uff09\u68c0\u7d22\u3002", "result": "\u8bba\u6587\u63cf\u8ff0\u4e86Omni-Embed-Nemotron\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u8bbe\u7f6e\u53ca\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Omni-Embed-Nemotron\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u6210\u529f\u5904\u7406\u4e86\u771f\u5b9e\u4e16\u754c\u4e2d\u65e5\u76ca\u590d\u6742\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u68c0\u7d22\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2510.03297", "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility.", "AI": {"tldr": "\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08EfficientNet-B0\uff09\u548cVision Transformer\uff08ViT-Base\uff09\u5728\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u6570\u636e\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u548c\u6548\u7387\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002EfficientNet-B0\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u5e73\u8861\u6570\u636e\u4e0b\u3002", "motivation": "\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08EfficientNet-B0\uff09\u548cVision Transformer\uff08ViT-Base\uff09\u5728\u9065\u611f\u6570\u636e\u96c6SpaceNet\u4e0a\uff0c\u5728\u81ea\u7136\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u91cd\u91c7\u6837\u4e24\u79cd\u6807\u7b7e\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u548c\u90e8\u7f72\u6307\u6807\u3002", "method": "\u7814\u7a76\u8005\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e86EfficientNet-B0\u548cViT-Base\u6a21\u578b\u3002\u5b9e\u9a8c\u8bbe\u7f6e\u5305\u62ec\u4e24\u79cd\u6807\u7b7e\u5206\u5e03\uff1a\u4e00\u79cd\u662f\u81ea\u7136\u4e0d\u5e73\u8861\u7684\u4e94\u5206\u7c7b\uff0c\u53e6\u4e00\u79cd\u662f\u6bcf\u7c7b700\u5f20\u56fe\u50cf\u7684\u5e73\u8861\u91cd\u91c7\u6837\u3002\u6240\u6709\u6a21\u578b\u91c7\u7528\u76f8\u540c\u7684\u9884\u5904\u7406\uff08224x224\uff0cImageNet\u5f52\u4e00\u5316\uff09\uff0c\u8f7b\u91cf\u7ea7\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5728NVIDIA P100\u4e0a\u8bad\u7ec340\u4e2aepoch\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u7387\u3001macro-F1\u3001\u5e73\u8861\u51c6\u786e\u7387\u3001\u6bcf\u7c7b\u53ec\u56de\u7387\u4ee5\u53ca\u90e8\u7f72\u6307\u6807\uff08\u6a21\u578b\u5927\u5c0f\u548c\u5ef6\u8fdf\uff09\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0cEfficientNet-B0\u8fbe\u523093%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0cmacro-F1\u8868\u73b0\u5f3a\u52b2\u4e14\u5ef6\u8fdf\u66f4\u4f4e\uff1bViT-Base\u4e5f\u8fbe\u523093%\uff0c\u4f46\u53c2\u6570\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u66f4\u5927\u3002\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u4e24\u4e2a\u6a21\u578b\u8868\u73b0\u5747\u5f88\u51fa\u8272\uff1bEfficientNet-B0\u8fbe\u523099%\uff0cViT-Base\u4e5f\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8fd9\u8868\u660e\u6570\u636e\u5e73\u8861\u7f29\u5c0f\u4e86\u67b6\u6784\u5dee\u8ddd\uff0c\u4f46CNN\u5728\u6548\u7387\u4e0a\u4ecd\u4fdd\u6301\u4f18\u52bf\u3002", "conclusion": "\u6570\u636e\u5e73\u8861\u53ef\u4ee5\u7f29\u5c0f\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5c3d\u7ba1ViT-Base\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\uff0cEfficientNet-B0\u5728\u6548\u7387\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u4e14\u5728\u5e73\u8861\u6570\u636e\u4e0b\u80fd\u8fbe\u5230\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002\u7814\u7a76\u7ed3\u679c\u901a\u8fc7\u53d1\u5e03\u6e05\u5355\u3001\u65e5\u5fd7\u548c\u56fe\u50cf\u7ea7\u9884\u6d4b\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2510.03247", "pdf": "https://arxiv.org/pdf/2510.03247", "abs": "https://arxiv.org/abs/2510.03247", "authors": ["Jiancheng Zhang", "Yinglun Zhu"], "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in\ndata-hungry deep learning. However, existing AL algorithms focus almost\nexclusively on unimodal data, overlooking the substantial annotation burden in\nmultimodal learning. We introduce the first framework for multimodal active\nlearning with unaligned data, where the learner must actively acquire\ncross-modal alignments rather than labels on pre-aligned pairs. This setting\ncaptures the practical bottleneck in modern multimodal pipelines such as CLIP\nand SigLIP, where unimodal features are easy to obtain but high-quality\nalignment is costly. We develop a new algorithm that combines uncertainty and\ndiversity principles in a modality-aware design, achieves linear-time\nacquisition, and applies seamlessly to both pool-based and streaming-based\nsettings. Extensive experiments on benchmark datasets demonstrate that our\napproach consistently reduces multimodal annotation cost while preserving\nperformance; for instance, on the ColorSwap dataset it cuts annotation\nrequirements by up to $40\\%$ without loss in accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u672a\u5bf9\u9f50\u591a\u6a21\u6001\u6570\u636e\u7684\u6d3b\u8dc3\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5176\u7b97\u6cd5\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u539f\u5219\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u6d3b\u8dc3\u5b66\u4e60\uff08AL\uff09\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5de8\u5927\u7684\u6807\u6ce8\u8d1f\u62c5\uff0c\u5c24\u5176\u662f\u5728CLIP\u548cSigLIP\u7b49\u73b0\u4ee3\u591a\u6a21\u6001\u7ba1\u9053\u4e2d\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6210\u672c\u975e\u5e38\u9ad8\u3002", "method": "\u5f15\u5165\u4e86\u9996\u4e2a\u9488\u5bf9\u672a\u5bf9\u9f50\u6570\u636e\u7684\u591a\u6a21\u6001\u6d3b\u8dc3\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u4e2d\u5b66\u4e60\u5668\u4e3b\u52a8\u83b7\u53d6\u8de8\u6a21\u6001\u5bf9\u9f50\u800c\u975e\u9884\u5bf9\u9f50\u6570\u636e\u7684\u6807\u7b7e\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u539f\u5219\u7ed3\u5408\u5728\u6a21\u6001\u611f\u77e5\u8bbe\u8ba1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u91c7\u96c6\uff0c\u5e76\u65e0\u7f1d\u9002\u7528\u4e8e\u57fa\u4e8e\u6c60\u548c\u57fa\u4e8e\u6d41\u7684\u8bbe\u7f6e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6301\u7eed\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u6807\u6ce8\u6210\u672c\uff1b\u4f8b\u5982\uff0c\u5728ColorSwap\u6570\u636e\u96c6\u4e0a\uff0c\u5b83\u5c06\u6807\u6ce8\u9700\u6c42\u524a\u51cf\u4e86\u9ad8\u8fbe40%\uff0c\u4e14\u6ca1\u6709\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6d3b\u8dc3\u5b66\u4e60\u6846\u67b6\u53ca\u5176\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6807\u6ce8\u74f6\u9888\uff0c\u901a\u8fc7\u9ad8\u6548\u5730\u83b7\u53d6\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.03453", "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "categories": ["cs.AI"], "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures.", "AI": {"tldr": "\u8bc4\u4f30\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u5f0f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u7406\u8bba\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u901a\u8fc7\u5e7f\u6cdb\u7684\u89c6\u89d2\uff0c\u5bf9\u8fd9\u4e24\u7c7b\u67b6\u6784\u53ca\u5176\u7cfb\u7edf\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9a\u6027\u6bd4\u8f83\u3002", "motivation": "\u7406\u8bba\u8bc4\u4f30\u5bf9\u4e8e\u4efb\u4f55\u7406\u8bba\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u4e8e\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u5f0f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u7406\u8bba\u800c\u8a00\uff0c\u8bc4\u4f30\u5c24\u5176\u56f0\u96be\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u201c\u53cc\u91cd\u6311\u6218\u201d\u3002", "method": "\u91c7\u7528\u5e7f\u9614\u7684\u7406\u8bba\u8bc4\u4f30\u89c6\u89d2\uff0c\u5bf9\u9762\u5411\u201c\u6574\u4f53\u5fc3\u667a\u201d\u7684\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u5f0f\u67b6\u6784\u53ca\u5176\u5b8c\u6574\u7cfb\u7edf\u8fdb\u884c\u5e7f\u6cdb\u4f46\u5b9a\u6027\u7684\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\uff0c\u4ea7\u51fa\u4e86\u5bf9\u9762\u5411\u201c\u6574\u4f53\u5fc3\u667a\u201d\u7684\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u5f0f\u67b6\u6784\u53ca\u5176\u5b8c\u6574\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u4f9b\u5bf9\u8ba4\u77e5\u548c\u751f\u6210\u5f0f\u67b6\u6784\u7684\u5e7f\u6cdb\u5b9a\u6027\u6bd4\u8f83\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u67b6\u6784\u4e2d\u7406\u8bba\u8bc4\u4f30\u7684\u6311\u6218\u3002"}}
{"id": "2510.04052", "pdf": "https://arxiv.org/pdf/2510.04052", "abs": "https://arxiv.org/abs/2510.04052", "authors": ["Behrooz Farkiani", "Fan Liu", "Patrick Crowley"], "title": "The Door to Policy Portability might be an IP Overlay", "categories": ["cs.NI"], "comment": null, "summary": "Portable service mesh implementations enable layer 4 to layer 7 policy\nenforcement across diverse infrastructures, but they remain tied to\ninfrastructure-specific layer 3 network policies. Network policies enable\ncontrol over IP traffic flow regardless of whether traffic is authorized at the\napplication level. However, not all infrastructure supports enforcing them, and\nachieving consistent enforcement across heterogeneous environments is\nchallenging. For example, studies have shown that the majority of Kubernetes\nclusters do not enforce any network policies. We propose integrating network\npolicy enforcement with service meshes to protect data-plane traffic in a\nportable, infrastructure-agnostic way. This enables developers to define\nintegrated layer 3 to layer 7 policies and ensure they are enforced across any\ninfrastructure. Additionally, due to its portability, our approach can be used\noutside the service environment to enforce policies on end-user traffic and\nprovide an end-to-end secure extended overlay. Our solution builds an overlay\nlayer 3 network and enforces layer 3 policies by routing traffic through\nspecific policy enforcement points and utilizing authorization keys. We\nprototyped our idea using Kubernetes and Istio, and show that while it adds\nless than 1ms latency, it can implement complex policies comparable to\nKubernetes native network policies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06L3\u7f51\u7edc\u7b56\u7565\u96c6\u6210\u5230\u670d\u52a1\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53e0\u52a0\u7f51\u7edc\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u65e0\u5173\u7684L3-L7\u7b56\u7565\u5f3a\u5236\u6267\u884c\uff0c\u5e76\u5177\u6709\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684\u670d\u52a1\u7f51\u683c\u5728L4-L7\u7b56\u7565\u6267\u884c\u65b9\u9762\u5177\u5907\u53ef\u79fb\u690d\u6027\uff0c\u4f46\u5728L3\u7f51\u7edc\u7b56\u7565\u4e0a\u4ecd\u4f9d\u8d56\u4e8e\u57fa\u7840\u8bbe\u65bd\u3002\u5728\u5f02\u6784\u73af\u5883\u4e2d\u4e00\u81f4\u5730\u6267\u884cL3\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u8bb8\u591a\u57fa\u7840\u8bbe\u65bd\uff08\u5982Kubernetes\u96c6\u7fa4\uff09\u7f3a\u4e4fL3\u7b56\u7565\u5f3a\u5236\u6267\u884c\uff0c\u5bfc\u81f4\u6570\u636e\u5e73\u9762\u6d41\u91cf\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u5c06\u7f51\u7edc\u7b56\u7565\u5f3a\u5236\u6267\u884c\u4e0e\u670d\u52a1\u7f51\u683c\u96c6\u6210\uff0c\u4ee5\u5b9e\u73b0\u53ef\u79fb\u690d\u3001\u57fa\u7840\u8bbe\u65bd\u65e0\u5173\u7684\u6570\u636e\u5e73\u9762\u6d41\u91cf\u4fdd\u62a4\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u6784\u5efa\u4e00\u4e2a\u53e0\u52a0L3\u7f51\u7edc\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u7b56\u7565\u5f3a\u5236\u70b9\u8def\u7531\u6d41\u91cf\u5e76\u5229\u7528\u6388\u6743\u5bc6\u94a5\u6765\u6267\u884cL3\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0L3\u5230L7\u7684\u96c6\u6210\u7b56\u7565\u5b9a\u4e49\u548c\u6267\u884c\u3002", "result": "\u5728Kubernetes\u548cIstio\u4e0a\u8fdb\u884c\u4e86\u539f\u578b\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4ec5\u589e\u52a0\u4e0d\u52301\u6beb\u79d2\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u4e0eKubernetes\u539f\u751f\u7f51\u7edc\u7b56\u7565\u76f8\u5f53\u7684\u590d\u6742\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u670d\u52a1\u73af\u5883\u4e4b\u5916\uff0c\u5bf9\u7ec8\u7aef\u7528\u6237\u6d41\u91cf\u5f3a\u5236\u6267\u884c\u7b56\u7565\uff0c\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u5b89\u5168\u6269\u5c55\u53e0\u52a0\u3002", "conclusion": "\u901a\u8fc7\u5728\u670d\u52a1\u7f51\u683c\u4e2d\u96c6\u6210\u53ef\u79fb\u690d\u7684\u53e0\u52a0L3\u7f51\u7edc\u7b56\u7565\u5f3a\u5236\u6267\u884c\uff0c\u80fd\u591f\u5b9e\u73b0\u8de8\u4efb\u4f55\u57fa\u7840\u8bbe\u65bd\u7684\u96c6\u6210L3-L7\u7b56\u7565\u5b9a\u4e49\u548c\u4e00\u81f4\u6267\u884c\uff0c\u6709\u6548\u4fdd\u62a4\u6570\u636e\u5e73\u9762\u6d41\u91cf\uff0c\u4e14\u5f15\u5165\u7684\u6027\u80fd\u5f00\u9500\u6781\u4f4e\u3002"}}
{"id": "2510.03467", "pdf": "https://arxiv.org/pdf/2510.03467", "abs": "https://arxiv.org/abs/2510.03467", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Searching for the Most Human-like Emergent Language", "categories": ["cs.CL", "I.2.7; I.6.m"], "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 19 pages, 12 figures", "summary": "In this paper, we design a signalling game-based emergent communication\nenvironment to generate state-of-the-art emergent languages in terms of\nsimilarity to human language. This is done with hyperparameter optimization,\nusing XferBench as the objective function. XferBench quantifies the statistical\nsimilarity of emergent language to human language by measuring its suitability\nfor deep transfer learning to human language. Additionally, we demonstrate the\npredictive power of entropy on the transfer learning performance of emergent\nlanguage as well as corroborate previous results on the entropy-minimization\nproperties of emergent communication systems. Finally, we report\ngeneralizations regarding what hyperparameters produce more realistic emergent\nlanguages, that is, ones which transfer better to human language.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u57fa\u4e8e\u4fe1\u53f7\u535a\u5f08\u7684\u7a81\u73b0\u901a\u4fe1\u73af\u5883\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5229\u7528XferBench\u751f\u6210\u4e86\u4e0e\u4eba\u7c7b\u8bed\u8a00\u9ad8\u5ea6\u76f8\u4f3c\u7684\u7a81\u73b0\u8bed\u8a00\u3002\u7814\u7a76\u63ed\u793a\u4e86\u71b5\u5bf9\u7a81\u73b0\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u7684\u9884\u6d4b\u529b\uff0c\u5e76\u603b\u7ed3\u4e86\u751f\u6210\u66f4\u771f\u5b9e\u7a81\u73b0\u8bed\u8a00\u7684\u8d85\u53c2\u6570\u89c4\u5f8b\u3002", "motivation": "\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u7a81\u73b0\u901a\u4fe1\u73af\u5883\uff0c\u4ee5\u751f\u6210\u5728\u7edf\u8ba1\u5b66\u4e0a\u4e0e\u4eba\u7c7b\u8bed\u8a00\u66f4\u76f8\u4f3c\u7684\u7a81\u73b0\u8bed\u8a00\uff0c\u5e76\u7406\u89e3\u5f71\u54cd\u8fd9\u79cd\u76f8\u4f3c\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u53f7\u535a\u5f08\u7684\u7a81\u73b0\u901a\u4fe1\u73af\u5883\u3002\u4f7f\u7528\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u4ee5XferBench\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u3002XferBench\u901a\u8fc7\u6d4b\u91cf\u7a81\u73b0\u8bed\u8a00\u5bf9\u4eba\u7c7b\u8bed\u8a00\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u7684\u9002\u7528\u6027\u6765\u91cf\u5316\u5176\u7edf\u8ba1\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u71b5\u4e0e\u7a81\u73b0\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5728\u4e0e\u4eba\u7c7b\u8bed\u8a00\u76f8\u4f3c\u5ea6\u65b9\u9762\u8fbe\u5230SOTA\u6c34\u5e73\u7684\u7a81\u73b0\u8bed\u8a00\u3002\u8bc1\u660e\u4e86\u71b5\u5bf9\u7a81\u73b0\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u7684\u9884\u6d4b\u80fd\u529b\u3002\u540c\u65f6\uff0c\u4e5f\u5370\u8bc1\u4e86\u5148\u524d\u5173\u4e8e\u7a81\u73b0\u901a\u4fe1\u7cfb\u7edf\u71b5\u6700\u5c0f\u5316\u7279\u6027\u7684\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u8bc6\u522b\u5e76\u62a5\u544a\u4e86\u54ea\u4e9b\u8d85\u53c2\u6570\u80fd\u591f\u4ea7\u751f\u66f4\u771f\u5b9e\uff08\u5373\u5bf9\u4eba\u7c7b\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\u66f4\u597d\uff09\u7684\u7a81\u73b0\u8bed\u8a00\u7684\u6cdb\u5316\u89c4\u5f8b\u3002\u71b5\u5bf9\u7a81\u73b0\u8bed\u8a00\u7684\u8fc1\u79fb\u5b66\u4e60\u8868\u73b0\u5177\u6709\u9884\u6d4b\u4f5c\u7528\u3002"}}
{"id": "2510.03314", "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "AI": {"tldr": "\u672c\u8bba\u6587\u7efc\u8ff0\u4e86\u8fd1\u4e94\u5e74\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u610f\u56fe\u8bc6\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u4fdd\u62a4\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08\u5982\u884c\u4eba\u548c\u9a91\u81ea\u884c\u8f66\u8005\uff09\u7684\u5b89\u5168\u662f\u4e00\u4e2a\u5168\u7403\u6027\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u7840\u8bbe\u65bd\u63aa\u65bd\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u5f80\u5f80\u4e0d\u8db3\u3002\u4eba\u5de5\u667a\u80fd\u4e3a\u4e3b\u52a8\u548c\u60c5\u5883\u611f\u77e5\u7684VRU\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u73b0\u6709\u7efc\u8ff0\u4e3b\u8981\u4fa7\u91cd\u4e8e\u68c0\u6d4b\uff0c\u5bf9\u5176\u4ed6\u5173\u952e\u7684\u89c6\u89c9\u4efb\u52a1\u8986\u76d6\u6709\u9650\u3002", "method": "\u672c\u6587\u5bf9\u8fc7\u53bb\u4e94\u5e74\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728VRU\u5b89\u5168\u65b9\u9762\u7684\u8fdb\u5c55\u548c\u65b0\u5174\u8d8b\u52bf\u8fdb\u884c\u4e86\u6700\u65b0\u5ba1\u67e5\u3002\u7cfb\u7edf\u5730\u8003\u5bdf\u4e86\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3001\u8ddf\u8e2a\u4e0e\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u4e0e\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5ba1\u67e5\uff0c\u660e\u786e\u4e86\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3001\u8ddf\u8e2a\u4e0e\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u4e0e\u9884\u6d4b\u8fd9\u56db\u4e2a\u4efb\u52a1\u662fAI\u9a71\u52a8VRU\u4e3b\u52a8\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u7684\u652f\u67f1\u3002\u540c\u65f6\uff0c\u4ece\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u7684\u89d2\u5ea6\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u4e3b\u8981\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u4e0b\u4e00\u4ee3\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\u6027\u53c2\u8003\uff0c\u4ee5\u589e\u5f3aVRU\u5b89\u5168\uff0c\u5c06\u89c6\u89c9AI\u7684\u8fdb\u6b65\u4e0e\u5b9e\u9645\u90e8\u7f72\u8003\u8651\u76f8\u7ed3\u5408\u3002"}}
{"id": "2510.03248", "pdf": "https://arxiv.org/pdf/2510.03248", "abs": "https://arxiv.org/abs/2510.03248", "authors": ["Anusha Agarwal", "Dibakar Roy Sarkar", "Somdatta Goswami"], "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over\n69 million cases annually worldwide. Finite element (FE) models offer\nhigh-fidelity predictions of brain deformation but are computationally\nexpensive, requiring hours per simulation and limiting their clinical utility\nfor rapid decision-making. This study benchmarks state-of-the-art neural\noperator (NO) architectures for rapid, patient-specific prediction of brain\ndisplacement fields, aiming to enable real-time TBI modeling in clinical and\ntranslational settings. We formulated TBI modeling as an operator learning\nproblem, mapping subject-specific anatomical MRI, magnetic resonance\nelastography (MRE) stiffness maps, and demographic features to full-field 3D\nbrain displacement predictions. Four architectures - Fourier Neural Operator\n(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator\nNetwork (DeepONet) were trained and evaluated on 249 MRE datasets across\nphysiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest\naccuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale\nfeatures, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet\noffered the fastest inference (14.5 iterations/s) with a 7$\\times$\ncomputational speed-up over MG-FNO, suggesting utility for embedded or edge\ncomputing applications. All NOs reduced computation time from hours to\nmilliseconds without sacrificing anatomical realism. NOs provide an efficient,\nresolution-invariant approach for predicting brain deformation, opening the\ndoor to real-time, patient-specific TBI risk assessment, clinical triage\nsupport, and optimization of protective equipment. These results highlight the\npotential for NO-based digital twins of the human brain, enabling scalable,\non-demand biomechanical modeling in both clinical and population health\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08TBI\uff09\u4e2d\u8111\u4f4d\u79fb\u573a\u7684\u5feb\u901f\u3001\u60a3\u8005\u7279\u5f02\u6027\u9884\u6d4b\uff0c\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u81f3\u6beb\u79d2\u3002", "motivation": "\u521b\u4f24\u6027\u8111\u635f\u4f24\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u6709\u9650\u5143\uff08FE\uff09\u6a21\u578b\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u6602\u8d35\uff08\u6bcf\u6b21\u6a21\u62df\u9700\u6570\u5c0f\u65f6\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u4e0a\u8fdb\u884c\u5feb\u901f\u51b3\u7b56\u7684\u5b9e\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u8fdb\u884cTBI\u5efa\u6a21\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5c06TBI\u5efa\u6a21\u8868\u8ff0\u4e3a\u4e00\u4e2a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u5c06\u60a3\u8005\u7279\u5b9a\u7684MRI\u89e3\u5256\u56fe\u50cf\u3001\u78c1\u5171\u632f\u5f39\u6027\u6210\u50cf\uff08MRE\uff09\u786c\u5ea6\u56fe\u4ee5\u53ca\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u6620\u5c04\u5230\u5168\u573a3D\u8111\u4f4d\u79fb\u9884\u6d4b\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u795e\u7ecf\u7b97\u5b50\uff08NO\uff09\u67b6\u6784\uff1a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u3001\u5206\u89e3\u5f0fFNO\uff08F-FNO\uff09\u3001\u591a\u7f51\u683cFNO\uff08MG-FNO\uff09\u548c\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08DeepONet\uff09\u3002\u8fd9\u4e9b\u6a21\u578b\u5728249\u4e2aMRE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u751f\u7406\u76f8\u5173\u7684\u9891\u7387\uff0820-90 Hz\uff09\u3002", "result": "MG-FNO\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u6027\uff08MSE = 0.0023\uff0c94.3%\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff09\u5e76\u80fd\u4fdd\u7559\u7cbe\u7ec6\u5c3a\u5ea6\u7279\u5f81\u3002F-FNO\u6536\u655b\u901f\u5ea6\u6bd4\u6807\u51c6FNO\u5feb2\u500d\u3002DeepONet\u63d0\u4f9b\u4e86\u6700\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0814.5\u6b21\u8fed\u4ee3/\u79d2\uff09\uff0c\u6bd4MG-FNO\u5feb7\u500d\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u6216\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002\u6240\u6709\u795e\u7ecf\u7b97\u5b50\u90fd\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6beb\u79d2\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u89e3\u5256\u5b66\u771f\u5b9e\u6027\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\u4e3a\u9884\u6d4b\u8111\u53d8\u5f62\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u4e0d\u53d8\u7684\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u3001\u60a3\u8005\u7279\u5f02\u6027\u7684TBI\u98ce\u9669\u8bc4\u4f30\u3001\u4e34\u5e8a\u5206\u8bca\u652f\u6301\u548c\u9632\u62a4\u8bbe\u5907\u4f18\u5316\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8fd9\u8868\u660e\u57fa\u4e8e\u795e\u7ecf\u7b97\u5b50\u7684\u4eba\u8111\u6570\u5b57\u5b6a\u751f\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u5728\u4e34\u5e8a\u548c\u4eba\u7fa4\u5065\u5eb7\u80cc\u666f\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6309\u9700\u7684\u751f\u7269\u529b\u5b66\u5efa\u6a21\u3002"}}
{"id": "2510.03469", "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLMs\u5c06\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u8f6c\u6362\u4e3aKripke\u7ed3\u6784\u548cLTL\u8fdb\u884c\u6a21\u578b\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u5212\u4e0e\u9884\u671f\u884c\u4e3a\u7684\u5bf9\u9f50\u6027\u3002\u5b9e\u9a8c\u663e\u793aGPT-5\u5206\u7c7b\u6027\u80fd\u4f18\u5f02\uff08F1 96.3%\uff09\u3002", "motivation": "\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u4e0e\u5176\u9884\u671f\u884c\u4e3a\u4e4b\u95f4\u7684\u5bf9\u9f50\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u8f6c\u6362\u4e3aKripke\u7ed3\u6784\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u68c0\u6d4b\u3002\u5728PlanBench\u8ba1\u5212\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u7b80\u5316\u7248\u672c\u4e0a\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "GPT-5\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff08F1\u5206\u6570\u4e3a96.3%\uff09\uff0c\u5e76\u4e14\u51e0\u4e4e\u603b\u80fd\u751f\u6210\u8bed\u6cd5\u4e0a\u5b8c\u7f8e\u7684\u3001\u53ef\u4f5c\u4e3a\u4fdd\u8bc1\u7684\u6b63\u5f0f\u8868\u793a\u3002", "conclusion": "\u5f53\u524d\u6846\u67b6\u80fd\u751f\u6210\u8bed\u6cd5\u5b8c\u7f8e\u7684\u6b63\u5f0f\u8868\u793a\uff0c\u4f46\u8bed\u4e49\u5b8c\u7f8e\u7684\u6b63\u5f0f\u6a21\u578b\u5408\u6210\u4ecd\u662f\u672a\u6765\u63a2\u7d22\u65b9\u5411\u3002"}}
{"id": "2510.04183", "pdf": "https://arxiv.org/pdf/2510.04183", "abs": "https://arxiv.org/abs/2510.04183", "authors": ["Lucas Pacheco", "Torsten Braun", "Kaushik Chowdhury", "Denis Ros\u00e1rio", "Batool Salehi", "Eduardo Cerqueira"], "title": "Dynamic Adaptive Federated Learning for mmWave Sector Selection", "categories": ["cs.NI"], "comment": null, "summary": "Beamforming techniques use massive antenna arrays to formulate narrow\nLine-of-Sight signal sectors to address the increased signal attenuation in\nmillimeter Wave (mmWave). However, traditional sector selection schemes involve\nextensive searches for the highest signal-strength sector, introducing extra\nlatency and communication overhead. This paper introduces a dynamic layer-wise\nand clustering-based federated learning (FL) algorithm for beam sector\nselection in autonomous vehicle networks called enhanced Dynamic Adaptive FL\n(eDAFL). The algorithm detects and selects the most important layers of a\nmachine learning model for aggregation in the FL process, significantly\nreducing network overhead and failure risks. eDAFL also considers intra-cluster\nand inter-cluster approaches to reduce overfitting and increase the abstraction\nlevel. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating\nimproved model accuracy by approximately 6.76% compared to existing methods,\nwhile reducing inference time by 84.04% and model size by up to 52.20%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faeDAFL\uff0c\u4e00\u79cd\u52a8\u6001\u5206\u5c42\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6beb\u7c73\u6ce2\u675f\u6247\u533a\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7cbe\u5ea6\u3001\u63a8\u7406\u901f\u5ea6\u5e76\u51cf\u5c0f\u4e86\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u4f20\u7edf\u7684\u6beb\u7c73\u6ce2\u675f\u6247\u533a\u9009\u62e9\u65b9\u6848\u9700\u8981\u5927\u91cf\u641c\u7d22\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7f51\u7edc\u6548\u7387\u3002", "method": "\u63d0\u51faeDAFL\uff08\u589e\u5f3a\u578b\u52a8\u6001\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u52a8\u6001\u9009\u62e9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u6700\u91cd\u8981\u7684\u5c42\u8fdb\u884c\u805a\u5408\uff0c\u4ee5\u51cf\u5c11\u7f51\u7edc\u5f00\u9500\u548c\u6545\u969c\u98ce\u9669\uff0c\u5e76\u7ed3\u5408\u96c6\u7fa4\u5185\u548c\u96c6\u7fa4\u95f4\u65b9\u6cd5\u6765\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u9ad8\u62bd\u8c61\u7ea7\u522b\u3002", "result": "\u5728\u771f\u5b9e\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\uff0ceDAFL\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u6a21\u578b\u7cbe\u5ea6\u63d0\u5347\u7ea66.76%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1184.04%\uff0c\u6a21\u578b\u5927\u5c0f\u7f29\u5c0f\u9ad8\u8fbe52.20%\u3002", "conclusion": "eDAFL\u7b97\u6cd5\u901a\u8fc7\u52a8\u6001\u5206\u5c42\u548c\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6beb\u7c73\u6ce2\u675f\u6247\u533a\u9009\u62e9\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3001\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2510.03490", "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.", "AI": {"tldr": "\u5f15\u5165SEER\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u6587\u672c\u4e2d\u8868\u8fbe\u60c5\u611f\u7684\u5177\u4f53\u77ed\u8bed\uff08\u60c5\u611f\u8bc1\u636e\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4ec5\u4e3a\u6574\u4e2a\u53e5\u5b50\u5206\u914d\u6807\u7b7e\uff0c\u7f3a\u4e4f\u7c92\u5ea6\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u652f\u6301\u540c\u7406\u5fc3\u5bf9\u8bdd\u548c\u4e34\u5e8a\u652f\u6301\u7b49\u5e94\u7528\uff0c\u9700\u8981\u8bc6\u522b\u60c5\u611f\u662f\u5982\u4f55\u8868\u8fbe\u7684\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bc6\u522b\u60c5\u611f\u672c\u8eab\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u63a2\u7d22\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u3002", "method": "\u5f15\u5165SEER\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1a\u5355\u53e5\u5185\u60c5\u611f\u8bc1\u636e\u8bc6\u522b\u548c\u77ed\u7bc7\uff08\u4e94\u53e5\uff09\u60c5\u611f\u8bc1\u636e\u8bc6\u522b\u3002\u8be5\u57fa\u51c6\u5305\u542b1200\u4e2a\u771f\u5b9e\u4e16\u754c\u53e5\u5b50\u4e0a\u7684\u60c5\u611f\u548c\u60c5\u611f\u8bc1\u636e\u65b0\u6ce8\u91ca\u3002\u8bc4\u4f30\u4e8614\u4e2a\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u90e8\u5206\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u53e5\u8f93\u5165\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0c\u4f46\u5728\u8f83\u957f\u6587\u672c\u6bb5\u843d\u4e2d\uff0c\u5b83\u4eec\u7684\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u4e3b\u8981\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u60c5\u611f\u5173\u952e\u8bcd\u548c\u5728\u4e2d\u6027\u6587\u672c\u4e2d\u51fa\u73b0\u5047\u9633\u6027\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u5ea6\u7ea7\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8f83\u957f\u4e0a\u4e0b\u6587\u65f6\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u514b\u670d\u6a21\u578b\u5bf9\u5173\u952e\u8bcd\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u5728\u590d\u6742\u8bed\u5883\u4e2d\u8bc6\u522b\u60c5\u611f\u8868\u8fbe\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03316", "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\uff08EOFMs\uff09\u7684\u5185\u90e8\u8868\u793a\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bbe\u8ba1\u7684\u7f3a\u9677\u5e76\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\uff08EOFMs\uff09\u5728\u5904\u7406\u6d77\u91cf\u9065\u611f\u6570\u636e\u548c\u6267\u884c\u5730\u7403\u76d1\u6d4b\u4efb\u52a1\u65b9\u9762\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u8f93\u51fa\u5e38\u88ab\u7528\u4f5c\u9ad8\u7ef4\u6570\u636e\u7684\u5d4c\u5165\u3002\u7136\u800c\uff0c\u5927\u591a\u6570EOFMs\u4ec5\u5728\u5355\u4e00\u6a21\u6001\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u8de8\u4e0d\u540c\u6a21\u6001\u5e94\u7528\uff0c\u73b0\u6709\u5de5\u4f5c\u5c1a\u4e0d\u6e05\u695a\u591a\u6837\u5316\u7684\u4f20\u611f\u5668\u67b6\u6784\u5982\u4f55\u5f71\u54cdEOFMs\u7684\u5185\u90e8\u8868\u793a\u3002", "method": "\u672c\u6458\u8981\u672a\u660e\u786e\u63cf\u8ff0\u5177\u4f53\u7814\u7a76\u65b9\u6cd5\uff0c\u4f46\u6307\u51fa\u7814\u7a76\u901a\u8fc7\u5c55\u793aEOFMs\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u7684\u654f\u611f\u6027\u6765\u8fbe\u6210\u7ed3\u8bba\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cEOFMs\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u7406\u89e3EOFMs\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u7684\u654f\u611f\u6027\uff0c\u4e3a\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u6f5c\u5728\u7f3a\u9677\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\uff0c\u5e76\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u3001\u7528\u6237\u4ee5\u53ca\u6574\u4e2a\u9065\u611f\u79d1\u5b66\u793e\u533a\u6307\u660e\u4e86\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.03250", "pdf": "https://arxiv.org/pdf/2510.03250", "abs": "https://arxiv.org/abs/2510.03250", "authors": ["Lukas R\u00fcttgers", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "Light Differentiable Logic Gate Networks", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency\nat inference while sustaining competitive accuracy. But vanishing gradients,\ndiscretization errors, and high training cost impede scaling these networks.\nEven with dedicated parameter initialization schemes from subsequent works,\nincreasing depth still harms accuracy. We show that the root cause of these\nissues lies in the underlying parametrization of logic gate neurons themselves.\nTo overcome this issue, we propose a reparametrization that also shrinks the\nparameter size logarithmically in the number of inputs per gate. For binary\ninputs, this already reduces the model size by 4x, speeds up the backward pass\nby up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we\nshow that the accuracy on CIFAR-100 remains stable and sometimes superior to\nthe original parametrization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\uff08DLGNs\uff09\u56e0\u5e95\u5c42\u53c2\u6570\u5316\u5bfc\u81f4\u7684\u68af\u5ea6\u6d88\u5931\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u53ca\u6df1\u5ea6\u6269\u5c55\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u6a21\u578b\u5c0f\u578b\u5316\u3001\u8bad\u7ec3\u52a0\u901f\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\uff08DLGNs\uff09\u5728\u63a8\u7406\u6548\u7387\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u68af\u5ea6\u6d88\u5931\u3001\u79bb\u6563\u5316\u8bef\u5dee\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\uff0c\u4e14\u589e\u52a0\u6df1\u5ea6\u4f1a\u635f\u5bb3\u7cbe\u5ea6\u3002\u8fd9\u4e9b\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\u903b\u8f91\u95e8\u795e\u7ecf\u5143\u672c\u8eab\u7684\u5e95\u5c42\u53c2\u6570\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u73b0\u6709DLGNs\u7684\u4e0a\u8ff0\u95ee\u9898\uff0c\u8fd8\u80fd\u4ee5\u5bf9\u6570\u65b9\u5f0f\u7f29\u5c0f\u6bcf\u4e2a\u903b\u8f91\u95e8\u8f93\u5165\u7684\u53c2\u6570\u91cf\u3002", "result": "\u5bf9\u4e8e\u4e8c\u5143\u8f93\u5165\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c114\u500d\uff0c\u53cd\u5411\u4f20\u64ad\u901f\u5ea6\u63d0\u9ad8\u9ad8\u8fbe1.86\u500d\uff0c\u8bad\u7ec3\u6b65\u6570\u51cf\u5c118.5\u500d\u3002\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u7cbe\u5ea6\u4fdd\u6301\u7a33\u5b9a\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86DLGNs\u7684\u6269\u5c55\u6027\u969c\u788d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2510.03485", "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "categories": ["cs.AI", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165PolicyGuardBench\u57fa\u51c6\u548cPolicyGuard-4B\u6a21\u578b\uff0c\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u7f51\u7edc\u667a\u80fd\u4f53\u8f68\u8ff9\u7684\u7b56\u7565\u5408\u89c4\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u3001\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u9632\u62a4\u680f\u5728\u5c0f\u89c4\u6a21\u4e0b\u662f\u53ef\u884c\u7684\u3002", "motivation": "\u81ea\u4e3b\u7f51\u7edc\u667a\u80fd\u4f53\u9700\u9075\u5faa\u65e2\u5b9a\u7b56\u7565\u751f\u6210\u957f\u8f68\u8ff9\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u9c9c\u5c11\u7814\u7a76\u8fd9\u4e9b\u8f68\u8ff9\u662f\u5426\u7b26\u5408\u7b56\u7565\uff0c\u4ee5\u53ca\u7b56\u7565\u8fdd\u89c4\u5728\u4e0d\u540c\u9886\u57df\u548c\u5b50\u9886\u57df\u4e2d\u662f\u5426\u6301\u7eed\u5b58\u5728\u3002", "method": "\u5f15\u5165\u4e86PolicyGuardBench\uff0c\u4e00\u4e2a\u5305\u542b\u7ea66\u4e07\u4e2a\u793a\u4f8b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u7b56\u7565\u8fdd\u89c4\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u7b56\u7565\u751f\u6210\u3001\u5b50\u9886\u57df\u5185\u548c\u8de8\u5b50\u9886\u57df\u7684\u8fdd\u89c4\u914d\u5bf9\uff0c\u5e76\u5305\u542b\u5b8c\u6574\u8f68\u8ff9\u548c\u57fa\u4e8e\u524d\u7f00\u7684\u8fdd\u89c4\u68c0\u6d4b\u4efb\u52a1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7\u9632\u62a4\u680f\u6a21\u578bPolicyGuard-4B\u3002", "result": "PolicyGuard-4B\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002\u8be5\u6a21\u578b\u80fd\u591f\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u5e76\u5728\u672a\u89c1\u8bbe\u7f6e\u4e0a\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "PolicyGuardBench\u548cPolicyGuard-4B\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u7814\u7a76\u7f51\u7edc\u667a\u80fd\u4f53\u8f68\u8ff9\u7b56\u7565\u5408\u89c4\u6027\u7684\u9996\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u5e76\u8868\u660e\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u9632\u62a4\u680f\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2510.04346", "pdf": "https://arxiv.org/pdf/2510.04346", "abs": "https://arxiv.org/abs/2510.04346", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins", "categories": ["cs.NI", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "comment": "Code: https://github.com/nahshonmokua/LoRaWAN-Indoor-PL-parametrics", "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u73af\u5883\u611f\u77e5\u7684LoRaWAN\u5ba4\u5185\u8def\u5f84\u635f\u8017\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u73af\u5883\u534f\u53d8\u91cf\u548c\u591a\u9879\u5f0f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4f20\u64ad\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u7269\u8054\u7f51\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u7684\u5bf9\u6570\u8ddd\u79bb\u6a21\u578b\u548c\u5bf9\u6570\u6b63\u6001\u9634\u5f71\u8870\u843d\u5047\u8bbe\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u5ba4\u5185LoRaWAN\u4f20\u64ad\uff0c\u56e0\u4e3a\u5176\u53d7\u5230\u7ed3\u6784\u548c\u65f6\u53d8\u73af\u5883\u56e0\u7d20\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728240\u5e73\u65b9\u7c73\u7684\u516b\u697c\u529e\u516c\u5ba4\u8fdb\u884c\u4e3a\u671f12\u4e2a\u6708\u7684\u6d4b\u91cf\u6d3b\u52a8\uff0c\u5e76\u4f7f\u7528\u9632\u6cc4\u6f0f\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u5bf9\u6570\u8ddd\u79bb\u591a\u5899\u5747\u503c\u6a21\u578b\u4e2d\u52a0\u5165\u4e86\u73af\u5883\u534f\u53d8\u91cf\uff08\u76f8\u5bf9\u6e7f\u5ea6\u3001\u6e29\u5ea6\u3001\u4e8c\u6c27\u5316\u78b3\u3001\u9897\u7c92\u7269\u3001\u6c14\u538b\uff09\u548c\u4fe1\u566a\u6bd4\u3002\u6bd4\u8f83\u4e86\u591a\u79cd\u7ebf\u6027\u56de\u5f52\u3001\u8d1d\u53f6\u65af\u7ebf\u6027\u56de\u5f52\u548c\u9009\u62e9\u6027\u4e8c\u9636\u591a\u9879\u5f0f\u6a21\u578b\u3002\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u975e\u53c2\u6570\u65cf\uff08\u5982\u9ad8\u65af\u6df7\u5408\uff09\u5bf9\u9634\u5f71\u8870\u843d\u8fdb\u884c\u5256\u6790\uff0c\u5e76\u901a\u8fc7\u79fb\u52a8\u5757\u81ea\u4e3e\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4fdd\u7559\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u591a\u9879\u5f0f\u5747\u503c\u6a21\u578b\u5c06\u4ea4\u53c9\u9a8c\u8bc1\u7684RMSE\u4ece8.07 dB\u964d\u81f37.09 dB\uff0cR^2\u4ece0.81\u63d0\u9ad8\u52300.86\u3002\u6298\u53e0\u5916\u6b8b\u5dee\u4e3a\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u4e00\u4e2a\u4e09\u6210\u5206\u6df7\u5408\u6a21\u578b\u80fd\u6355\u83b7\u5176\u5c16\u9510\u6838\u5fc3\u548c\u8f7b\u5bbd\u5c3e\u90e8\u3002\u572899%\u7684\u6570\u636e\u5305\u4f20\u8f93\u7387\u4e0b\uff0c\u73af\u5883\u611f\u77e5\u591a\u9879\u5f0f\u6a21\u578b\u6240\u9700\u7684\u8870\u843d\u88d5\u5ea6\u4e3a25.7 dB\uff0c\u800c\u7ebf\u6027\u57fa\u7ebf\u6a21\u578b\u4e3a27.7\u81f327.9 dB\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u90e8\u7f72\u5c31\u7eea\u4e14\u5177\u6709\u6821\u51c6\u53ef\u9760\u6027\u63a7\u5236\u7684\u5de5\u4f5c\u6d41\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u7269\u8054\u7f51\u89c4\u5212\uff0c\u5e76\u7b26\u54086G\u76ee\u6807\uff0c\u901a\u8fc7\u9884\u6d4b\u7cbe\u5ea6\u8f6c\u5316\u4e3a\u53ef\u9760\u6027\u3002"}}
{"id": "2510.03502", "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats.", "AI": {"tldr": "ALHD\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\uff0c\u65e8\u5728\u533a\u5206\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u6db5\u76d6\u591a\u6d41\u6d3e\u548c\u65b9\u8a00\uff0c\u5305\u542b40\u591a\u4e07\u6837\u672c\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5fae\u8c03BERT\u6a21\u578b\u4f18\u4e8eLLM\u6a21\u578b\uff0c\u4f46\u8de8\u6d41\u6d3e\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u6311\u6218\uff0c\u5c24\u5176\u5728\u65b0\u95fb\u9886\u57df\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5168\u9762\u7684\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\u6765\u533a\u5206\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u8fd9\u5bf9\u4e8e\u5e94\u5bf9\u865a\u5047\u4fe1\u606f\u3001\u5b66\u672f\u4e0d\u7aef\u548c\u7f51\u7edc\u5a01\u80c1\u7b49\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165ALHD\u6570\u636e\u96c6\uff0c\u5305\u542b\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u8bc4\u8bba\u4e09\u79cd\u6d41\u6d3e\uff0c\u8986\u76d6\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u548c\u65b9\u8a00\uff0c\u62e5\u6709\u8d85\u8fc740\u4e07\u7531\u4e09\u4e2a\u9886\u5148LLM\u548c\u591a\u4e2a\u4eba\u7c7b\u6765\u6e90\u751f\u6210\u7684\u5e73\u8861\u6837\u672c\u3002\u63d0\u4f9b\u4e25\u683c\u9884\u5904\u7406\u3001\u4e30\u5bcc\u6807\u6ce8\u548c\u6807\u51c6\u5316\u5206\u5272\u3002\u5bf9\u4f20\u7edf\u5206\u7c7b\u5668\u3001BERT\u6a21\u578b\u548cLLM\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5fae\u8c03\u7684BERT\u6a21\u578b\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4e14\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u6a21\u578b\u5728\u8de8\u6d41\u6d3e\u6cdb\u5316\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b0\u95fb\u6587\u7ae0\u65f6\uff0c\u7531\u4e8eLLM\u751f\u6210\u7684\u6587\u672c\u4e0e\u4eba\u7c7b\u6587\u672c\u98ce\u683c\u76f8\u4f3c\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "conclusion": "ALHD\u4e3a\u963f\u62c9\u4f2f\u8bedLLM\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u51cf\u8f7b\u865a\u5047\u4fe1\u606f\u3001\u5b66\u672f\u4e0d\u7aef\u548c\u7f51\u7edc\u5a01\u80c1\u7684\u98ce\u9669\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u6a21\u578b\u5728\u8de8\u6d41\u6d3e\uff0c\u7279\u522b\u662f\u5728\u65b0\u95fb\u9886\u57df\uff0c\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03317", "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["G\u00fcnel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u4fee\u590d\u7684\u6270\u52a8\u89e3\u91ca\u6280\u672f\uff0c\u65e8\u5728\u63d0\u9ad8\u751f\u6001\u5b66\u89c6\u89c9\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\uff0c\u901a\u8fc7\u751f\u6210\u903c\u771f\u3001\u65e0\u4f2a\u5f71\u7684\u5c40\u90e8\u7f16\u8f91\u6765\u63ed\u793a\u6a21\u578b\u9884\u6d4b\u7684\u5173\u952e\u5f62\u6001\u7ebf\u7d22\u3002", "motivation": "\u751f\u6001\u76d1\u6d4b\u4e2d\u89c6\u89c9\u6a21\u578b\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u7684\u9884\u6d4b\u7ed3\u679c\u9650\u5236\u4e86\u7528\u6237\u4fe1\u4efb\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u56fe\u50cf\u4fee\u590d\u4e3a\u5bfc\u5411\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u89e3\u91ca\u6280\u672f\uff0c\u80fd\u751f\u6210\u903c\u771f\u3001\u63a9\u7801\u5c40\u90e8\u5316\u5e76\u4fdd\u7559\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u7f16\u8f91\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u66ff\u6362\u6216\u79fb\u9664\u76ee\u6807\u7269\u4f53\uff08\u5982\u7528\u51b0/\u6c34\u66ff\u6362\u6d77\u8c79\uff09\u548c\u66ff\u6362\u80cc\u666f\u4e24\u79cd\u5e72\u9884\u65b9\u5f0f\uff0c\u5728\u7ec6\u8c03\u540e\u7684YOLOv9\u6d77\u8c79\u68c0\u6d4b\u5668\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u4f7f\u7528Segment-Anything-Model\u7cbe\u70bc\u63a9\u7801\u3002\u89e3\u91ca\u7684\u6709\u6548\u6027\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u91cd\u65b0\u8bc4\u5206\uff08\u7ffb\u8f6c\u7387\u3001\u7f6e\u4fe1\u5ea6\u4e0b\u964d\uff09\u548c\u4e13\u5bb6\u5ba1\u67e5\u8bc4\u4f30\u3002", "result": "\u8be5\u6280\u672f\u751f\u6210\u7684\u89e3\u91ca\u80fd\u591f\u5b9a\u4f4d\u8bca\u65ad\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\u5e38\u89c1\u7684\u5220\u9664\u4f2a\u5f71\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u7ec6\u7c92\u5ea6\u5f62\u6001\u7ebf\u7d22\uff0c\u4fdd\u6301\u4e86\u56fe\u50cf\u7684\u5206\u5e03\u5185\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e0e\u9886\u57df\u76f8\u5173\u7684\u6df1\u523b\u89c1\u89e3\uff0c\u652f\u6301\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u751f\u6001\u5b66\u9886\u57df\u66f4\u503c\u5f97\u4fe1\u8d56\u5730\u90e8\u7f72AI\u6a21\u578b\u3002"}}
{"id": "2510.03251", "pdf": "https://arxiv.org/pdf/2510.03251", "abs": "https://arxiv.org/abs/2510.03251", "authors": ["Hanzhong Cao", "Wenbo Yan", "Ying Tan"], "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many methods aim to enhance time series forecasting by decomposing the series\nthrough intricate model structures and prior knowledge, yet they are inevitably\nlimited by computational complexity and the robustness of the assumptions. Our\nresearch uncovers that in the complex domain and higher-order hypercomplex\nspaces, the characteristic frequencies of time series naturally decrease.\nLeveraging this insight, we propose Numerion, a time series forecasting model\nbased on multiple hypercomplex spaces. Specifically, grounded in theoretical\nsupport, we generalize linear layers and activation functions to hypercomplex\nspaces of arbitrary power-of-two dimensions and introduce a novel\nReal-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.\nNumerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces\nof varying dimensions, naturally decomposing and independently modeling the\nseries, and adaptively fuses the latent patterns exhibited in different spaces\nthrough a dynamic fusion mechanism. Experiments validate the model`s\nperformance, achieving state-of-the-art results on multiple public datasets.\nVisualizations and quantitative analyses comprehensively demonstrate the\nability of multi-dimensional RHR-MLPs to naturally decompose time series and\nreveal the tendency of higher dimensional hypercomplex spaces to capture lower\nfrequency features.", "AI": {"tldr": "Numerion\u6a21\u578b\u5229\u7528\u591a\u7ef4\u8d85\u590d\u6570\u7a7a\u95f4\u548cRHR-MLP\u7ed3\u6784\uff0c\u901a\u8fc7\u81ea\u7136\u9891\u7387\u5206\u89e3\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5047\u8bbe\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8d85\u590d\u6570\u7a7a\u95f4\u4e2d\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\u9891\u7387\u4f1a\u81ea\u7136\u964d\u4f4e\uff0c\u8fd9\u4e3a\u65b0\u7684\u9884\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u542f\u53d1\u3002", "method": "\u63d0\u51faNumerion\u6a21\u578b\uff0c\u5b83\u5c06\u7ebf\u6027\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u63a8\u5e7f\u5230\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165Real-Hypercomplex-Real\u57df\u591a\u5c42\u611f\u77e5\u673a\uff08RHR-MLP\uff09\u67b6\u6784\u3002Numerion\u5229\u7528\u591a\u4e2aRHR-MLP\u5c06\u65f6\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u4e0d\u540c\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\u8fdb\u884c\u81ea\u7136\u5206\u89e3\u548c\u72ec\u7acb\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u5730\u878d\u5408\u6f5c\u5728\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\u3002\u53ef\u89c6\u5316\u548c\u5b9a\u91cf\u5206\u6790\u8bc1\u660e\u4e86\u591a\u7ef4RHR-MLP\u80fd\u81ea\u7136\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u4e14\u9ad8\u7ef4\u8d85\u590d\u6570\u7a7a\u95f4\u503e\u5411\u4e8e\u6355\u83b7\u4f4e\u9891\u7279\u5f81\u3002", "conclusion": "Numerion\u6a21\u578b\u901a\u8fc7\u5229\u7528\u8d85\u590d\u6570\u7a7a\u95f4\u81ea\u7136\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u64c5\u957f\u6355\u83b7\u4e0d\u540c\u9891\u7387\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9884\u6d4b\u6548\u679c\u3002"}}
{"id": "2510.03506", "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "categories": ["cs.AI"], "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation.", "AI": {"tldr": "OneFlow\u662f\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u53ef\u53d8\u957f\u5ea6\u548c\u5e76\u53d1\u7684\u56fe\u6587\u751f\u6210\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u514b\u670d\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u6587\u751f\u6210\u4e2d\u50f5\u5316\u7684\u56e0\u679c\u987a\u5e8f\u9650\u5236\uff0c\u5b9e\u73b0\u53ef\u53d8\u957f\u5ea6\u548c\u5e76\u53d1\u7684\u591a\u6a21\u6001\u751f\u6210\u3002", "method": "\u7ed3\u5408\u4e86\u7528\u4e8e\u79bb\u6563\u6587\u672ctoken\u7684\u57fa\u4e8e\u63d2\u5165\u7684Edit Flow\u548c\u7528\u4e8e\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u7684Flow Matching\uff0c\u5e76\u91c7\u7528\u4f18\u5148\u5185\u5bb9\u800c\u975e\u8bed\u6cd5\u7684\u5206\u5c42\u91c7\u6837\u673a\u5236\u3002", "result": "\u57281B\u52308B\u7684\u6a21\u578b\u89c4\u6a21\u4e0a\uff0cOneFlow\u5728\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u81ea\u56de\u5f52\u57fa\u7ebf\uff0c\u8bad\u7ec3FLOPs\u51cf\u5c11\u9ad8\u8fbe50%\u3002\u5b83\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5e76\u89e3\u9501\u4e86\u5e76\u53d1\u751f\u6210\u3001\u8fed\u4ee3\u7ec6\u5316\u548c\u7c7b\u63a8\u7406\u751f\u6210\u7b49\u65b0\u80fd\u529b\u3002", "conclusion": "OneFlow\u662f\u4e00\u6b3e\u521b\u65b0\u4e14\u9ad8\u6548\u7684\u975e\u81ea\u56de\u5f52\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u751f\u6210\u673a\u5236\uff0c\u5728\u6027\u80fd\u548c\u65b0\u80fd\u529b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2510.04516", "pdf": "https://arxiv.org/pdf/2510.04516", "abs": "https://arxiv.org/abs/2510.04516", "authors": ["Behrooz Farkiani", "Fan Liu", "Patrick Crowley"], "title": "Rethinking HTTP API Rate Limiting: A Client-Side Approach", "categories": ["cs.NI"], "comment": null, "summary": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65e0\u9700\u4e2d\u5fc3\u63a7\u5236\u7684\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u91cd\u8bd5\u673a\u5236ATB\u548cAATB\uff0c\u901a\u8fc7\u63a8\u65ad\u7cfb\u7edf\u62e5\u585e\u6765\u5b89\u6392HTTP API\u91cd\u8bd5\uff0c\u76f8\u8f83\u4e8e\u6307\u6570\u9000\u907f\uff0c\u53ef\u5c06HTTP 429\u9519\u8bef\u51cf\u5c11\u9ad8\u8fbe97.3%\u3002", "motivation": "\u73b0\u4ee3\u4e92\u8054\u7f51\u670d\u52a1\u4e2d\u7684HTTP API\u6d41\u91cf\u53d7\u5230\u914d\u989d\u9650\u5236\uff0c\u5f53\u5171\u4eab\u914d\u989d\u8d85\u9650\u65f6\uff0c\u5ba2\u6237\u7aef\u4f1a\u88ab\u9650\u6d41\u5e76\u91cd\u8bd5\u3002\u670d\u52a1\u5668\u7aef\u63a7\u5236\u5728\u5171\u4eab\u914d\u989d\u4e0b\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u5ba2\u6237\u7aef\u65e0\u6cd5\u611f\u77e5\u5176\u4ed6\u8d1f\u8f7d\uff1b\u800c\u666e\u904d\u91c7\u7528\u7684\u6307\u6570\u9000\u907f\u7b49\u7b80\u5355\u5ba2\u6237\u7aef\u7b56\u7565\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u91cd\u8bd5\u548c\u663e\u8457\u6210\u672c\u3002\u96c6\u4e2d\u534f\u8c03\u867d\u80fd\u89e3\u51b3\uff0c\u4f46\u6709\u5b9e\u9645\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u65e0\u9700\u4e2d\u5fc3\u63a7\u5236\u3001\u4ec5\u4f9d\u8d56\u6700\u5c11\u53cd\u9988\u7684\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u673a\u5236\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aATB\uff08\u901a\u8fc7Service Worker\u90e8\u7f72\u7684\u79bb\u7ebf\u65b9\u6cd5\uff09\u548cAATB\uff08\u5229\u7528\u805a\u5408\u9065\u6d4b\u6570\u636e\u589e\u5f3a\u91cd\u8bd5\u884c\u4e3a\uff09\u3002\u4e24\u79cd\u7b97\u6cd5\u5747\u901a\u8fc7\u63a8\u65ad\u7cfb\u7edf\u62e5\u585e\u6765\u8c03\u5ea6\u91cd\u8bd5\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u4e16\u754c\u6d41\u91cf\u548c\u6700\u591a100\u4e2a\u5ba2\u6237\u7aef\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u4eff\u771f\u6765\u8bc4\u4f30\u3002", "result": "\u4e0e\u6307\u6570\u9000\u907f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5c06HTTP 429\u9519\u8bef\u51cf\u5c11\u4e86\u9ad8\u8fbe97.3%\u3002\u5b8c\u6210\u65f6\u95f4\u7565\u6709\u589e\u52a0\uff0c\u4f46\u9519\u8bef\u7387\u7684\u663e\u8457\u964d\u4f4e\u5f25\u8865\u4e86\u8fd9\u4e00\u589e\u957f\u3002", "conclusion": "\u81ea\u9002\u5e94\u5ba2\u6237\u7aef\u91cd\u8bd5\u673a\u5236\uff08ATB\u548cAATB\uff09\u80fd\u591f\u6709\u6548\u7ba1\u7406\u5171\u4eabHTTP API\u914d\u989d\u4e0b\u7684\u6d41\u91cf\uff0c\u5927\u5e45\u51cf\u5c11429\u9519\u8bef\uff0c\u5e76\u5728\u8f83\u4f4e\u7684\u5b8c\u6210\u65f6\u95f4\u5f00\u9500\u4e0b\u4f18\u4e8e\u7b80\u5355\u7b56\u7565\u3002"}}
{"id": "2510.03519", "pdf": "https://arxiv.org/pdf/2510.03519", "abs": "https://arxiv.org/abs/2510.03519", "authors": ["Fangxu Yu", "Hongyu Zhao", "Tianyi Zhou"], "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Time series reasoning is crucial to decision-making in diverse domains,\nincluding finance, energy usage, traffic, weather, and scientific discovery.\nWhile existing time series foundation models (TSFMs) can capture low-level\ndynamic patterns and provide accurate forecasting, further analysis usually\nrequires additional background knowledge and sophisticated reasoning, which are\nlacking in most TSFMs but can be achieved through large language models (LLMs).\nOn the other hand, without expensive post-training, LLMs often struggle with\nthe numerical understanding of time series data. Although it is intuitive to\nintegrate the two types of models, developing effective training recipes that\nalign the two modalities for reasoning tasks is still an open challenge. To\nthis end, we propose TS-Reasoner that aligns the latent representations of\nTSFMs with the textual inputs of LLMs for downstream understanding/reasoning\ntasks. Specifically, we propose a simple yet effective method to curate\ndiverse, synthetic pairs of time series and textual captions for alignment\ntraining. We then develop a two-stage training recipe that applies instruction\nfinetuning after the alignment pretraining. Unlike existing works that train an\nLLM to take time series as inputs, we leverage a pretrained TSFM and freeze it\nduring training. Extensive experiments on several benchmarks demonstrate that\nTS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision\nLanguage Models (VLMs), and Time Series LLMs, but also achieves this with\nremarkable data efficiency, e.g., using less than half the training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTS-Reasoner\uff0c\u901a\u8fc7\u5bf9\u9f50\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u6f5c\u5728\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u672c\u8f93\u5165\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u4e0e\u63a8\u7406\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5bf9\u5404\u9886\u57df\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709TSFMs\u64c5\u957f\u9884\u6d4b\u4f46\u7f3a\u4e4f\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u800cLLMs\u867d\u5177\u63a8\u7406\u6f5c\u529b\u5374\u4e0d\u64c5\u957f\u6570\u503c\u7406\u89e3\u3002\u5c06\u8fd9\u4e24\u79cd\u6a21\u578b\u6709\u6548\u6574\u5408\u4ee5\u5e94\u5bf9\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u89e3\u51b3\u4e24\u8005\u6a21\u6001\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faTS-Reasoner\u6a21\u578b\uff0c\u65e8\u5728\u5bf9\u9f50TSFMs\u7684\u6f5c\u5728\u8868\u793a\u4e0eLLMs\u7684\u6587\u672c\u8f93\u5165\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u521b\u5efa\u591a\u6837\u5316\u7684\u65f6\u95f4\u5e8f\u5217\u53ca\u5176\u6587\u672c\u63cf\u8ff0\u7684\u5408\u6210\u5bf9\uff0c\u7528\u4e8e\u5bf9\u9f50\u8bad\u7ec3\u30022) \u5f00\u53d1\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5148\u8fdb\u884c\u5bf9\u9f50\u9884\u8bad\u7ec3\uff0c\u518d\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u30023) \u5229\u7528\u5e76\u51bb\u7ed3\u4e00\u4e2a\u9884\u8bad\u7ec3TSFM\uff0c\u800c\u975e\u8bad\u7ec3LLM\u76f4\u63a5\u63a5\u6536\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u3002", "result": "TS-Reasoner\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5e7f\u6cdb\u7684LLMs\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u65f6\u95f4\u5e8f\u5217LLMs\uff0c\u8fd8\u5728\u6570\u636e\u6548\u7387\u4e0a\u663e\u8457\u9886\u5148\uff0c\u4f8b\u5982\u4ec5\u4f7f\u7528\u4e0d\u5230\u4e00\u534a\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "TS-Reasoner\u6210\u529f\u5730\u5c06TSFMs\u7684\u6570\u503c\u7406\u89e3\u80fd\u529b\u4e0eLLMs\u7684\u63a8\u7406\u80fd\u529b\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5747\u5c55\u793a\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03318", "pdf": "https://arxiv.org/pdf/2510.03318", "abs": "https://arxiv.org/abs/2510.03318", "authors": ["Ahmed Kabil", "Ghada Khoriba", "Mina Yousef", "Essam A. Rashed"], "title": "Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications", "categories": ["cs.CV"], "comment": "Computers in Biology and Medicine (to appear)", "summary": "Medical Image Segmentation (MIS) stands as a cornerstone in medical image\nanalysis, playing a pivotal role in precise diagnostics, treatment planning,\nand monitoring of various medical conditions. This paper presents a\ncomprehensive and systematic survey of MIS methodologies, bridging the gap\nbetween traditional image processing techniques and modern deep learning\napproaches. The survey encompasses thresholding, edge detection, region-based\nsegmentation, clustering algorithms, and model-based techniques while also\ndelving into state-of-the-art deep learning architectures such as Convolutional\nNeural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely\nadopted U-Net and its variants. Moreover, integrating attention mechanisms,\nsemi-supervised learning, generative adversarial networks (GANs), and\nTransformer-based models is thoroughly explored. In addition to covering\nestablished methods, this survey highlights emerging trends, including hybrid\narchitectures, cross-modality learning, federated and distributed learning\nframeworks, and active learning strategies, which aim to address challenges\nsuch as limited labeled datasets, computational complexity, and model\ngeneralizability across diverse imaging modalities. Furthermore, a specialized\ncase study on lumbar spine segmentation is presented, offering insights into\nthe challenges and advancements in this relatively underexplored anatomical\nregion. Despite significant progress in the field, critical challenges persist,\nincluding dataset bias, domain adaptation, interpretability of deep learning\nmodels, and integration into real-world clinical workflows.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272(MIS)\u65b9\u6cd5\uff0c\u6db5\u76d6\u4ece\u4f20\u7edf\u6280\u672f\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\uff0c\u63a2\u8ba8\u4e86\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272(MIS)\u4f5c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6838\u5fc3\uff0c\u5bf9\u7cbe\u786e\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u75c5\u60c5\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5bf9\u4ece\u4f20\u7edf\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u5404\u79cdMIS\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\u548c\u603b\u7ed3\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0(survey)\u65b9\u6cd5\uff0c\u5206\u6790\u4e86MIS\u65b9\u6cd5\uff0c\u5305\u62ec\u9608\u503c\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u533a\u57df\u5206\u5272\u3001\u805a\u7c7b\u7b97\u6cd5\u548c\u6a21\u578b\u57fa\u6280\u672f\u7b49\u4f20\u7edf\u65b9\u6cd5\uff0c\u4ee5\u53caCNNs\u3001FCNs\u3001U-Net\u53ca\u5176\u53d8\u4f53\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001GANs\u548cTransformer\u7b49\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u6df7\u5408\u67b6\u6784\u3001\u8de8\u6a21\u6001\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u7b49\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u8170\u690e\u5206\u5272\u7684\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u8be5\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u4ece\u4f20\u7edf\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u5404\u79cd\u65b9\u6cd5\u3001\u6700\u65b0\u8fdb\u5c55\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u8bc6\u522b\u4e86\u5176\u6240\u5e94\u5bf9\u7684\u6311\u6218\uff08\u5982\u6807\u8bb0\u6570\u636e\u6709\u9650\u3001\u8ba1\u7b97\u590d\u6742\u6027\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff09\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u8170\u690e\u5206\u5272\u7684\u6848\u4f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u5177\u4f53\u89c1\u89e3\u3002", "conclusion": "\u5c3d\u7ba1\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u96c6\u504f\u5dee\u3001\u9886\u57df\u9002\u5e94\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u4e0e\u5b9e\u9645\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6574\u5408\u3002"}}
{"id": "2510.03252", "pdf": "https://arxiv.org/pdf/2510.03252", "abs": "https://arxiv.org/abs/2510.03252", "authors": ["Duc Kieu", "Kien Do", "Tuan Hoang", "Thao Minh Le", "Tung Kieu", "Dang Nguyen", "Thin Nguyen"], "title": "Universal Multi-Domain Translation via Diffusion Routers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multi-domain translation (MDT) aims to learn translations between multiple\ndomains, yet existing approaches either require fully aligned tuples or can\nonly handle domain pairs seen in training, limiting their practicality and\nexcluding many cross-domain mappings. We introduce universal MDT (UMDT), a\ngeneralization of MDT that seeks to translate between any pair of $K$ domains\nusing only $K-1$ paired datasets with a central domain. To tackle this problem,\nwe propose Diffusion Router (DR), a unified diffusion-based framework that\nmodels all central$\\leftrightarrow$non-central translations with a single noise\npredictor conditioned on the source and target domain labels. DR enables\nindirect non-central translations by routing through the central domain. We\nfurther introduce a novel scalable learning strategy with a variational-bound\nobjective and an efficient Tweedie refinement procedure to support direct\nnon-central mappings. Through evaluation on three large-scale UMDT benchmarks,\nDR achieves state-of-the-art results for both indirect and direct translations,\nwhile lowering sampling cost and unlocking novel tasks such as\nsketch$\\leftrightarrow$segmentation. These results establish DR as a scalable\nand versatile framework for universal translation across multiple domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffusion Router (DR)\u6846\u67b6\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u7528\u591a\u57df\u7ffb\u8bd1 (UMDT)\u3002DR\u901a\u8fc7\u4e2d\u5fc3\u57df\u8def\u7531\u5b9e\u73b0\u4efb\u610fK\u4e2a\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u4ec5\u9700K-1\u4e2a\u4e2d\u5fc3\u57df\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MDT\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2aUMDT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u57df\u7ffb\u8bd1 (MDT) \u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5b8c\u5168\u5bf9\u9f50\u7684\u5143\u7ec4\u6570\u636e\uff0c\u8981\u4e48\u53ea\u80fd\u5904\u7406\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u57df\u5bf9\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u5e76\u6392\u9664\u4e86\u8bb8\u591a\u8de8\u57df\u6620\u5c04\u3002\u56e0\u6b64\uff0c\u672c\u6587\u5f15\u5165\u4e86\u901a\u7528\u591a\u57df\u7ffb\u8bd1 (UMDT) \u4efb\u52a1\uff0c\u65e8\u5728\u4ec5\u4f7f\u7528K-1\u4e2a\u4e0e\u4e2d\u5fc3\u57df\u914d\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4efb\u610fK\u4e2a\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Diffusion Router (DR) \u6846\u67b6\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u5355\u4e00\u7684\u566a\u58f0\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u6e90\u57df\u548c\u76ee\u6807\u57df\u6807\u7b7e\uff0c\u6765\u5efa\u6a21\u6240\u6709\u4e2d\u5fc3\u57df\u4e0e\u975e\u4e2d\u5fc3\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\u3002DR\u901a\u8fc7\u8def\u7531\u4e2d\u5fc3\u57df\u5b9e\u73b0\u95f4\u63a5\u7684\u975e\u4e2d\u5fc3\u57df\u7ffb\u8bd1\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u53d8\u5206\u754c\u76ee\u6807\u548c\u9ad8\u6548\u7684Tweedie\u7ec6\u5316\u8fc7\u7a0b\uff0c\u4ee5\u652f\u6301\u76f4\u63a5\u7684\u975e\u4e2d\u5fc3\u57df\u6620\u5c04\u3002", "result": "DR\u5728\u4e09\u4e2a\u5927\u89c4\u6a21UMDT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5bf9\u95f4\u63a5\u548c\u76f4\u63a5\u7ffb\u8bd1\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb (state-of-the-art) \u7684\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u5b83\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\uff0c\u5e76\u89e3\u9501\u4e86\u8349\u56fe\u2194\u5206\u5272\u7b49\u65b0\u578b\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDR\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u591a\u529f\u80fd\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8de8\u591a\u4e2a\u57df\u7684\u901a\u7528\u7ffb\u8bd1\u4efb\u52a1\u3002"}}
{"id": "2510.03605", "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u5728\u4e0a\u4e0b\u6587\u6743\u91cd\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08test-time scaling\uff09\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff08\u5982\u751f\u6210\u957f\u601d\u7ef4\u94fe\uff09\u7684\u6761\u4ef6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u53ef\u5728\u56fa\u5b9a\u8bef\u5dee\u4e0b\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4f46\u82e5\u8bad\u7ec3\u6570\u636e\u6280\u80fd\u4e0d\u8db3\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\uff0c\u4e14\u5728\u591a\u6837\u5316\u3001\u76f8\u5173\u4e14\u56f0\u96be\u7684\u4efb\u52a1\u96c6\u4e0a\u8bad\u7ec3\u80fd\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08Test-time scaling\uff09\u5df2\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\uff08\u4f8b\u5982\u751f\u6210\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff09\uff0c\u4f46\u5173\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u4ea7\u751f\u957f\u601d\u7ef4\u94fe\u7684\u6761\u4ef6\u4ee5\u53ca\u4f55\u65f6\u8fd9\u4e9b\u957f\u601d\u7ef4\u94fe\u80fd\u6709\u6548\u63d0\u9ad8\u6027\u80fd\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7814\u7a76\u5728\u7528\u4e8e\u7ebf\u6027\u56de\u5f52\u7684\u4e0a\u4e0b\u6587\u6743\u91cd\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff0c\u6765\u5206\u6790\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u6027\u80fd\u3002\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5728\u5927\u578b\u975e\u7ebf\u6027Transformer\u67b6\u6784\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u7814\u7a76\u7ed3\u679c\u3002", "result": ["\u5728\u4efb\u4f55\u56fa\u5b9a\u6d4b\u8bd5\u8bef\u5dee\u4e0b\uff0c\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u5141\u8bb8\u51cf\u5c11\u8bad\u7ec3\u63d0\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\uff08\u4e0a\u4e0b\u6587\u957f\u5ea6\uff09\u3002", "\u5982\u679c\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u6240\u9700\u7684\u6280\u80fd\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u591f\u5145\u5206\uff0c\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u53cd\u800c\u4f1a\u635f\u5bb3\u6027\u80fd\u3002", "\u901a\u8fc7\u7279\u5f81\u534f\u65b9\u5dee\u77e9\u9635\u7684\u6700\u5c0f\u7279\u5f81\u503c\u6765\u8868\u5f81\u4efb\u52a1\u96be\u5ea6\uff0c\u7ed3\u679c\u8868\u660e\u5728\u591a\u6837\u5316\u3001\u76f8\u5173\u4e14\u56f0\u96be\u7684\u4efb\u52a1\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u4f7f\u6d4b\u8bd5\u65f6\u7f29\u653e\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002"], "conclusion": "\u6d4b\u8bd5\u65f6\u7f29\u653e\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u7279\u6027\u3002\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u53ef\u4ee5\u6743\u8861\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\uff1b\u7136\u800c\uff0c\u82e5\u8bad\u7ec3\u6570\u636e\u672a\u80fd\u5145\u5206\u6db5\u76d6\u76f8\u5173\u6280\u80fd\uff0c\u8fc7\u591a\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002\u4e3a\u6700\u5927\u5316\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u76ca\u5904\uff0c\u5173\u952e\u5728\u4e8e\u5728\u591a\u6837\u5316\u3001\u76f8\u5173\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002"}}
{"id": "2510.04620", "pdf": "https://arxiv.org/pdf/2510.04620", "abs": "https://arxiv.org/abs/2510.04620", "authors": ["Siu Kei Chung", "Francisco Carpio", "Andrei Navoichyk", "Siarhei Valasovich", "Jordan Moore", "Slobodan Sudaric-Hefner", "Daniel Baker", "Thomas Demoor", "Maurizio Binello", "Christian Kaul", "Kai Wawrzinek"], "title": "Impossible Cloud Network: A Decentralized Internet Infrastructure Layer", "categories": ["cs.NI"], "comment": null, "summary": "The internet faces a sovereignty crisis due to power concentration and data\ngrowth among a few hyperscalers, leading to centralization and loss of user\ncontrol. This consolidation risks censorship and creates single points of\nfailure. While Web3 offers decentralized solutions, they often sacrifice either\nscalability, decentralization, or security, which are key elements in the\nblockchain trilemma. These solutions also struggle with limited access to\nenterprise-grade hardware and frequently rely on centralized infrastructure.\nThe Impossible Cloud Network (ICN) addresses these issues by creating a\nmulti-tiered, decentralized infrastructure layer. ICN offers a composable\nservice layer, an enterprise-grade hardware resource layer, and a transparent,\npermissionless HyperNode network for performance enforcement. By strategically\ndecoupling and decentralizing each layer, ICN aims to provide an open,\nextensively scalable infrastructure that ensures digital sovereignty,\neliminates single points of trust, enables service programmability, and offers\na decoupled architecture for limitless possibilities in the future internet.", "AI": {"tldr": "\u4e92\u8054\u7f51\u9762\u4e34\u4e2d\u5fc3\u5316\u5371\u673a\uff0cWeb3\u65b9\u6848\u6709\u5c40\u9650\u6027\u3002ICN\u63d0\u51fa\u591a\u5c42\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u5b57\u4e3b\u6743\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u4e92\u8054\u7f51\u6743\u529b\u96c6\u4e2d\u5728\u5c11\u6570\u8d85\u5927\u89c4\u6a21\u516c\u53f8\uff0c\u5bfc\u81f4\u4e2d\u5fc3\u5316\u3001\u7528\u6237\u63a7\u5236\u6743\u4e27\u5931\u3001\u5ba1\u67e5\u98ce\u9669\u548c\u5355\u70b9\u6545\u969c\u3002\u73b0\u6709Web3\u65b9\u6848\u96be\u4ee5\u5e73\u8861\u53ef\u6269\u5c55\u6027\u3001\u53bb\u4e2d\u5fc3\u5316\u548c\u5b89\u5168\u6027\uff0c\u5e76\u4f9d\u8d56\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u63d0\u51fa\u201c\u4e0d\u53ef\u80fd\u4e91\u7f51\u7edc\uff08ICN\uff09\u201d\uff0c\u901a\u8fc7\u521b\u5efa\u591a\u5c42\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\u5c42\u89e3\u51b3\u95ee\u9898\uff0c\u5305\u62ec\u53ef\u7ec4\u5408\u670d\u52a1\u5c42\u3001\u4f01\u4e1a\u7ea7\u786c\u4ef6\u8d44\u6e90\u5c42\u548c\u7528\u4e8e\u6027\u80fd\u4fdd\u969c\u7684HyperNode\u7f51\u7edc\uff0c\u6218\u7565\u6027\u5730\u89e3\u8026\u5e76\u53bb\u4e2d\u5fc3\u5316\u5404\u5c42\u3002", "result": "ICN\u65e8\u5728\u63d0\u4f9b\u5f00\u653e\u3001\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u786e\u4fdd\u6570\u5b57\u4e3b\u6743\uff0c\u6d88\u9664\u4fe1\u4efb\u5355\u70b9\uff0c\u5b9e\u73b0\u670d\u52a1\u53ef\u7f16\u7a0b\u6027\uff0c\u5e76\u63d0\u4f9b\u89e3\u8026\u67b6\u6784\u4ee5\u5e94\u5bf9\u672a\u6765\u4e92\u8054\u7f51\u7684\u65e0\u9650\u53ef\u80fd\u3002", "conclusion": "ICN\u901a\u8fc7\u5176\u591a\u5c42\u3001\u89e3\u8026\u548c\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u4e3a\u89e3\u51b3\u5f53\u524d\u4e92\u8054\u7f51\u7684\u4e2d\u5fc3\u5316\u5371\u673a\u548cWeb3\u65b9\u6848\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u66f4\u5f00\u653e\u3001\u4e3b\u6743\u548c\u53ef\u6269\u5c55\u7684\u672a\u6765\u4e92\u8054\u7f51\u3002"}}
{"id": "2510.03521", "pdf": "https://arxiv.org/pdf/2510.03521", "abs": "https://arxiv.org/abs/2510.03521", "authors": ["Ali Elahi"], "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025", "summary": "In specialized domains, humans often compare new problems against similar\nexamples, highlight nuances, and draw conclusions instead of analyzing\ninformation in isolation. When applying reasoning in specialized contexts with\nLLMs on top of a RAG, the pipeline can capture contextually relevant\ninformation, but it is not designed to retrieve comparable cases or related\nproblems.\n  While RAG is effective at extracting factual information, its outputs in\nspecialized reasoning tasks often remain generic, reflecting broad facts rather\nthan context-specific insights. In finance, it results in generic risks that\nare true for the majority of companies. To address this limitation, we propose\na peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics\nsuch as ROUGE and BERTScore in comparison with human-generated equity research\nand risk.", "AI": {"tldr": "\u73b0\u6709RAG\u5728\u4e13\u4e1a\u9886\u57df\u63a8\u7406\u65f6\u8f93\u51fa\u6cdb\u6cdb\u4e14\u7f3a\u4e4f\u7279\u5b9a\u6d1e\u5bdf\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u201c\u540c\u4f34\u611f\u77e5\u5bf9\u6bd4\u63a8\u7406\u5c42\u201d\u53e0\u52a0\u5728RAG\u4e0a\uff0c\u4f7f\u5176\u80fd\u68c0\u7d22\u53ef\u6bd4\u6848\u4f8b\u5e76\u751f\u6210\u66f4\u5177\u4f53\u7684\u6d1e\u5bdf\uff0c\u5728\u6587\u672c\u751f\u6210\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebfRAG\u3002", "motivation": "\u73b0\u6709RAG-LLM\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u867d\u7136\u80fd\u6355\u83b7\u76f8\u5173\u4fe1\u606f\uff0c\u4f46\u65e0\u6cd5\u68c0\u7d22\u53ef\u6bd4\u6848\u4f8b\u6216\u76f8\u5173\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u7ed3\u679c\u8fc7\u4e8e\u901a\u7528\uff0c\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u7684\u7ec6\u81f4\u6d1e\u5bdf\uff08\u4f8b\u5982\uff0c\u91d1\u878d\u9886\u57df\u4e2d\u751f\u6210\u901a\u7528\u98ce\u9669\u800c\u975e\u516c\u53f8\u7279\u5b9a\u98ce\u9669\uff09\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u201c\u540c\u4f34\u611f\u77e5\u5bf9\u6bd4\u63a8\u7406\u5c42\uff08peer-aware comparative inference layer\uff09\u201d\uff0c\u5c06\u5176\u53e0\u52a0\u5728RAG\u4e4b\u4e0a\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u53ef\u6bd4\u6848\u4f8b\u7684\u63a8\u7406\u3002", "result": "\u8be5\u5bf9\u6bd4\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u6307\u6807\uff08\u5982ROUGE\u548cBERTScore\uff09\u4e0a\u4f18\u4e8e\u57fa\u7ebfRAG\uff0c\u5e76\u4e14\u5728\u4e0e\u4eba\u7c7b\u751f\u6210\u7684\u80a1\u7968\u7814\u7a76\u548c\u98ce\u9669\u5206\u6790\u8fdb\u884c\u6bd4\u8f83\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u540c\u4f34\u611f\u77e5\u5bf9\u6bd4\u63a8\u7406\u5c42\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aRAG\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u7279\u5f02\u6027\u548c\u6d1e\u5bdf\u529b\u7684\u7ed3\u679c\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4f20\u7edfRAG\u8f93\u51fa\u6cdb\u5316\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03328", "pdf": "https://arxiv.org/pdf/2510.03328", "abs": "https://arxiv.org/abs/2510.03328", "authors": ["Fiona Victoria Stanley Jothiraj", "Arunaggiri Pandian Karunanidhi", "Seth A. Eichmeyer"], "title": "DECOR: Deep Embedding Clustering with Orientation Robustness", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems.", "AI": {"tldr": "DECOR\u662f\u4e00\u79cd\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u5bfc\u4f53\u6676\u5706\u7f3a\u9677\u6a21\u5f0f\u8bc6\u522b\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u3001\u65e0\u6807\u7b7e\u3001\u4e0d\u5e73\u8861\u7684\u6570\u636e\uff0c\u5e76\u5bf9\u6676\u5706\u56fe\u7684\u65b9\u5411\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\uff0c\u65e9\u671f\u68c0\u6d4b\u6676\u5706\u7f3a\u9677\u5bf9\u4ea7\u54c1\u826f\u7387\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u539f\u59cb\u6676\u5706\u6570\u636e\u901a\u5e38\u590d\u6742\u3001\u65e0\u6807\u7b7e\u3001\u4e0d\u5e73\u8861\u4e14\u5355\u4e2a\u6676\u5706\u53ef\u80fd\u5305\u542b\u591a\u79cd\u7f3a\u9677\uff0c\u8fd9\u4f7f\u5f97\u5728\u4e0d\u5b8c\u7f8e\u6570\u636e\u6761\u4ef6\u4e0b\u8bbe\u8ba1\u53ef\u9760\u7684\u805a\u7c7b\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86DECOR\uff0c\u4e00\u4e2a\u5177\u6709\u65b9\u5411\u9c81\u68d2\u6027\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u6676\u5706\u56fe\u4e2d\u7684\u590d\u6742\u7f3a\u9677\u6a21\u5f0f\u805a\u7c7b\u6210\u4e00\u81f4\u7684\u7c07\uff0c\u5e76\u660e\u786e\u8003\u8651\u4e86\u6676\u5706\u56fe\u7684\u65b9\u5411\u53d8\u5316\uff0c\u786e\u4fdd\u7a7a\u95f4\u76f8\u4f3c\u7684\u7f3a\u9677\u65e0\u8bba\u65cb\u8f6c\u6216\u5bf9\u9f50\u5982\u4f55\u90fd\u80fd\u4e00\u81f4\u805a\u7c7b\u3002", "result": "\u5728\u5f00\u6e90MixedWM38\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cDECOR\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u5373\u53ef\u53d1\u73b0\u7c07\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDECOR\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u805a\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DECOR\u4e3a\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03253", "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5206\u5c42\u504f\u597d\u5b66\u4e60\uff08HPL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8f68\u8ff9\u3001\u6b65\u957f\u548c\u521b\u65b0\u7684\u7ec4\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u53cc\u5c42\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u79bb\u7ebf\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u5bf9LLM\u4ee3\u7406\u8fdb\u884c\u5bf9\u9f50\u65f6\uff0c\u9762\u4e34\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u8f68\u8ff9\u7ea7\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u96be\u4ee5\u7cbe\u786e\u5206\u914d\u4fe1\u7528\uff1b\u6b65\u957f\u7ea7\u4fe1\u53f7\u53c8\u8fc7\u4e8e\u77ed\u89c6\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u6b65\u884c\u4e3a\u7684\u4ef7\u503c\uff0c\u4ece\u800c\u9650\u5236\u4e86LLM\u4ee3\u7406\u89e3\u51b3\u590d\u6742\u3001\u957f\u5468\u671f\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u5206\u5c42\u504f\u597d\u5b66\u4e60\uff08HPL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u7ea7\u548c\u6b65\u957f\u7ea7DPO\uff0c\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u7ec4\u7ea7\u504f\u597d\u4f18\u5316\u3002HPL\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u52a8\u4f5c\u7ec4\uff0c\u5e76\u751f\u6210\u5bf9\u6bd4\u6027\u7684\u6b21\u4f18\u7ec4\u8fdb\u884c\u5b66\u4e60\u3002\u540c\u65f6\uff0c\u5f15\u5165\u53cc\u5c42\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7ec4\u957f\u5ea6\uff08\u5b50\u4efb\u52a1\u590d\u6742\u6027\uff09\u548c\u6837\u672c\u96be\u5ea6\uff08\u504f\u597d\u4e0e\u975e\u504f\u597d\u7ec4\u4e4b\u95f4\u7684\u5956\u52b1\u5dee\u8ddd\uff09\u7ec4\u7ec7\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u7b80\u5230\u7e41\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPL\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u5206\u6790\u8868\u660e\uff0c\u5206\u5c42DPO\u635f\u5931\u80fd\u6709\u6548\u6574\u5408\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\uff0c\u800c\u53cc\u5c42\u8bfe\u7a0b\u5bf9\u4e8e\u4ee3\u7406\u89e3\u51b3\u4ece\u7b80\u5355\u884c\u4e3a\u5230\u590d\u6742\u591a\u6b65\u5e8f\u5217\u7684\u5e7f\u6cdb\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "HPL\u901a\u8fc7\u5176\u72ec\u7279\u7684\u591a\u7c92\u5ea6\u504f\u597d\u5b66\u4e60\u548c\u53cc\u5c42\u8bfe\u7a0b\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86LLM\u4ee3\u7406\u5bf9\u9f50\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2510.03612", "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc\uff08CPS\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u64cd\u7eb5\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee3\u7406\u66f4\u6709\u6548\u7684\u504f\u597d\u64cd\u7eb5\uff0c\u4e14\u66f4\u96be\u4ee5\u88ab\u68c0\u6d4b\uff0c\u51f8\u663e\u4e86\u5bf9\u9c81\u68d2\u9632\u5fa1\u7684\u8feb\u5207\u9700\u6c42\u3002", "motivation": "\u57fa\u4e8eVLM\u7684\u7f51\u7edc\u4ee3\u7406\u5728\u5185\u5bb9\u63a8\u8350\u3001\u4ea7\u54c1\u6392\u540d\u7b49\u9ad8\u98ce\u9669\u9009\u62e9\u4efb\u52a1\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u901a\u8fc7\u5bf9\u6297\u6027\u5f39\u51fa\u7a97\u53e3\u3001\u56fe\u50cf\u6270\u52a8\u6216\u5185\u5bb9\u8c03\u6574\u8fdb\u884c\u7684\u504f\u597d\u64cd\u7eb5\u653b\u51fb\u3002\u7136\u800c\uff0c\u73b0\u6709\u5de5\u4f5c\u8981\u4e48\u5047\u8bbe\u5f3a\u5927\u7684\u767d\u76d2\u8bbf\u95ee\u6743\u9650\u548c\u6709\u9650\u7684\u5355\u6a21\u6001\u6270\u52a8\uff0c\u8981\u4e48\u4f7f\u7528\u4e0d\u5207\u5b9e\u9645\u7684\u8bbe\u7f6e\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u653b\u51fb\u8005\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc\uff08Cross-Modal Preference Steering, CPS\uff09\u201d\u65b9\u6cd5\uff0c\u9996\u6b21\u8bc1\u660e\u8054\u5408\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\u80fd\u4ea7\u751f\u66f4\u5f3a\u5927\u7684\u504f\u597d\u64cd\u7eb5\u3002CPS\u8054\u5408\u4f18\u5316\u9879\u76ee\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u96be\u4ee5\u5bdf\u89c9\u7684\u4fee\u6539\uff0c\u5229\u7528CLIP\u53ef\u8fc1\u79fb\u7684\u56fe\u50cf\u6270\u52a8\u548cRLHF\u8bf1\u5bfc\u7684\u8bed\u8a00\u504f\u5dee\u6765\u5f15\u5bfc\u4ee3\u7406\u51b3\u7b56\u3002\u4e0e\u5047\u8bbe\u68af\u5ea6\u8bbf\u95ee\u6216\u7f51\u9875\u63a7\u5236\u7684\u5148\u524d\u7814\u7a76\u4e0d\u540c\uff0cCPS\u91c7\u7528\u771f\u5b9e\u7684\u9ed1\u76d2\u5a01\u80c1\u8bbe\u7f6e\uff1a\u975e\u7279\u6743\u653b\u51fb\u8005\u53ea\u80fd\u7f16\u8f91\u81ea\u5df1\u7684\u5217\u8868\u56fe\u7247\u548c\u6587\u672c\u5143\u6570\u636e\uff0c\u5bf9\u4ee3\u7406\u6a21\u578b\u5185\u90e8\u673a\u5236\u4e00\u65e0\u6240\u77e5\u3002", "result": "\u7814\u7a76\u5728GPT-4.1\u3001Qwen-2.5VL\u548cPixtral-Large\u7b49\u6700\u5148\u8fdb\u7684\u4e13\u6709\u53ca\u5f00\u6e90VLM\u4ee3\u7406\u4e0a\uff0c\u901a\u8fc7\u7535\u5f71\u9009\u62e9\u548c\u7535\u5b50\u5546\u52a1\u4efb\u52a1\u8bc4\u4f30\u4e86CPS\u3002\u7ed3\u679c\u663e\u793a\uff0cCPS\u6bd4\u9886\u5148\u7684\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u3002\u4f8b\u5982\uff0cCPS\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e70%\u7684\u68c0\u6d4b\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\uff0c\u9274\u4e8e\u4ee3\u7406\u7cfb\u7edf\u5728\u793e\u4f1a\u4e2d\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u9632\u5fa1\u63aa\u65bd\u6765\u5bf9\u6297\u6b64\u7c7b\u8de8\u6a21\u6001\u504f\u597d\u64cd\u7eb5\u653b\u51fb\u3002"}}
{"id": "2510.04651", "pdf": "https://arxiv.org/pdf/2510.04651", "abs": "https://arxiv.org/abs/2510.04651", "authors": ["Adnan Aijaz", "Peizheng Li", "Sajida Gufran"], "title": "Satellite Direct-to-Device from Low Earth Orbit: Techno-Economic Analysis of a Global Non-Terrestrial Network", "categories": ["cs.NI"], "comment": "8 pages, 13 figures. This paper has been accepted for presentation at\n  the IEEE/IFIP Wireless and Mobile Networking Conference (WMNC) 2025", "summary": "Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D)\ntechnology are at the heart of the next-generation global connectivity which\npromises direct access to space-based broadband services for unmodified\n3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to\nevaluate the feasibility, cost-effectiveness, and profitability of these\nservices. By assessing the technological aspects as well as economic\nimplications, stakeholders can make informed decisions about investment,\ndevelopment, and deployment strategies. This paper presents a comprehensive\ntechno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D\nsystems. The framework integrates a global satellite constellation model, radio\npropagation aspects including atmospheric and rainfall attenuation models\ncompliant with ITU-R recommendations, 3GPP-compliant capacity calculations,\nrealistic global population data, and an all-encompassing cost model accounting\nfor both capital and operational expenses associated with space and ground\nsegments. Further, the framework evaluates three different architectural\noptions for realizing a global non-terrestrial network (NTN) for satellite D2D\nservices. With an emphasis on reproducibility, the framework has been\nimplemented through significant enhancements to an open-source tool. The\neconomic assessment reveals that global satellite D2D services can be provided\nat a monthly cost per subscriber which is comparable to terrestrial services\nwhile achieving a positive return on investment (ROI). Moreover, the results\nshow the potential of Open RAN technology for realizing cost-effective\nsatellite D2D services.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6280\u672f\u7ecf\u6d4e\u5206\u6790\u6846\u67b6\uff0c\u8bc4\u4f30LEO\u536b\u661f\u76f4\u8fde\u8bbe\u5907\uff08D2D\uff09\u670d\u52a1\u7684\u53ef\u884c\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u76c8\u5229\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u5176\u6bcf\u6708\u7528\u6237\u6210\u672c\u4e0e\u5730\u9762\u670d\u52a1\u76f8\u5f53\uff0c\u6295\u8d44\u56de\u62a5\u7387\u4e3a\u6b63\uff0c\u5e76\u5f3a\u8c03\u4e86Open RAN\u6280\u672f\u5728\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740LEO\u536b\u661f\u76f4\u8fde\u8bbe\u5907\uff08D2D\uff09\u6280\u672f\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u670d\u52a1\u7684\u53ef\u884c\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u76c8\u5229\u80fd\u529b\u5bf9\u4e8e\u5229\u76ca\u76f8\u5173\u8005\u505a\u51fa\u660e\u667a\u7684\u6295\u8d44\u3001\u5f00\u53d1\u548c\u90e8\u7f72\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u7684\u6280\u672f\u7ecf\u6d4e\u5206\u6790\uff08TEA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLEO\u7684\u536b\u661fD2D\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u5168\u7403\u536b\u661f\u661f\u5ea7\u6a21\u578b\u3001\u7b26\u5408ITU-R\u5efa\u8bae\u7684\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u578b\u30013GPP\u517c\u5bb9\u7684\u5bb9\u91cf\u8ba1\u7b97\u3001\u771f\u5b9e\u7684\u5168\u7403\u4eba\u53e3\u6570\u636e\uff0c\u4ee5\u53ca\u6db5\u76d6\u7a7a\u95f4\u548c\u5730\u9762\u6bb5\u8d44\u672c\u4e0e\u8fd0\u8425\u652f\u51fa\u7684\u5168\u9762\u6210\u672c\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5168\u7403\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u67b6\u6784\u9009\u9879\uff0c\u5e76\u57fa\u4e8e\u5f00\u6e90\u5de5\u5177\u8fdb\u884c\u4e86\u589e\u5f3a\u5b9e\u73b0\uff0c\u4ee5\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002", "result": "\u7ecf\u6d4e\u8bc4\u4f30\u8868\u660e\uff0c\u5168\u7403\u536b\u661fD2D\u670d\u52a1\u53ef\u4ee5\u4ee5\u4e0e\u5730\u9762\u670d\u52a1\u76f8\u5f53\u7684\u6bcf\u6708\u7528\u6237\u6210\u672c\u63d0\u4f9b\uff0c\u540c\u65f6\u5b9e\u73b0\u6b63\u7684\u6295\u8d44\u56de\u62a5\u7387\uff08ROI\uff09\u3002\u6b64\u5916\uff0c\u7ed3\u679c\u8fd8\u663e\u793aOpen RAN\u6280\u672f\u5728\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u536b\u661fD2D\u670d\u52a1\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "LEO\u536b\u661fD2D\u670d\u52a1\u5177\u5907\u4e0e\u73b0\u6709\u5730\u9762\u670d\u52a1\u76f8\u5ab2\u7f8e\u7684\u6210\u672c\u6548\u76ca\u548c\u76c8\u5229\u80fd\u529b\uff0c\u662f\u4e0b\u4e00\u4ee3\u5168\u7403\u8fde\u63a5\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14Open RAN\u6280\u672f\u6709\u671b\u8fdb\u4e00\u6b65\u964d\u4f4e\u6210\u672c\uff0c\u63a8\u52a8\u5176\u5e7f\u6cdb\u90e8\u7f72\u3002"}}
{"id": "2510.03527", "pdf": "https://arxiv.org/pdf/2510.03527", "abs": "https://arxiv.org/abs/2510.03527", "authors": ["Sayan Ghosh", "Shahzaib Saqib Warraich", "Dhruv Tarsadiya", "Gregory Yauney", "Swabha Swayamdipta"], "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs", "categories": ["cs.CL"], "comment": null, "summary": "Language models can be sampled multiple times to access the distribution\nunderlying their responses, but existing methods cannot efficiently synthesize\nrich epistemic signals across different long-form responses. We introduce\nConsensus Graphs (ConGrs), a flexible DAG-based data structure that represents\nshared information, as well as semantic variation in a set of sampled LM\nresponses to the same prompt. We construct ConGrs using a light-weight lexical\nsequence alignment algorithm from bioinformatics, supplemented by the targeted\nusage of a secondary LM judge. Further, we design task-dependent decoding\nmethods to synthesize a single, final response from our ConGr data structure.\nOur experiments show that synthesizing responses from ConGrs improves factual\nprecision on two biography generation tasks by up to 31% over an average\nresponse and reduces reliance on LM judges by more than 80% compared to other\nmethods. We also use ConGrs for three refusal-based tasks requiring abstention\non unanswerable queries and find that abstention rate is increased by up to\n56%. We apply our approach to the MATH and AIME reasoning tasks and find an\nimprovement over self-verification and majority vote baselines by up to 6\npoints of accuracy. We show that ConGrs provide a flexible method for capturing\nvariation in LM responses and using the epistemic signals provided by response\nvariation to synthesize more effective responses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5171\u8bc6\u56fe\uff08ConGrs\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eDAG\u7684\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u7efc\u5408\u8bed\u8a00\u6a21\u578b\u591a\u6b21\u91c7\u6837\u7684\u957f\u7bc7\u54cd\u5e94\u4e2d\u7684\u77e5\u8bc6\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5f03\u6743\u80fd\u529b\u548c\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u7efc\u5408\u8bed\u8a00\u6a21\u578b\u591a\u6b21\u91c7\u6837\u4ea7\u751f\u7684\u957f\u7bc7\u54cd\u5e94\u4e2d\u4e30\u5bcc\u7684\u77e5\u8bc6\u4fe1\u53f7\uff08epistemic signals\uff09\u3002", "method": "\u5f15\u5165\u5171\u8bc6\u56fe\uff08ConGrs\uff09\uff0c\u4e00\u79cd\u7075\u6d3b\u7684DAG\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u8868\u793a\u91c7\u6837LM\u54cd\u5e94\u4e2d\u7684\u5171\u4eab\u4fe1\u606f\u548c\u8bed\u4e49\u53d8\u5f02\u3002\u901a\u8fc7\u751f\u7269\u4fe1\u606f\u5b66\u7684\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u5e8f\u5217\u6bd4\u5bf9\u7b97\u6cd5\uff0c\u8f85\u4ee5\u8f85\u52a9LM\u5224\u65ad\u5668\u6784\u5efaConGrs\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u4eceConGr\u7ed3\u6784\u4e2d\u5408\u6210\u5355\u4e00\u7684\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4eceConGrs\u5408\u6210\u7684\u54cd\u5e94\u5728\u4e24\u9879\u4f20\u8bb0\u751f\u6210\u4efb\u52a1\u4e2d\u5c06\u4e8b\u5b9e\u51c6\u786e\u6027\u6bd4\u5e73\u5747\u54cd\u5e94\u63d0\u9ad8\u4e86\u9ad8\u8fbe31%\uff0c\u5e76\u5c06\u5bf9LM\u5224\u65ad\u5668\u7684\u4f9d\u8d56\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u3002\u5728\u4e09\u9879\u57fa\u4e8e\u62d2\u7edd\u7684\u4efb\u52a1\u4e2d\uff0c\u65e0\u6cd5\u56de\u7b54\u67e5\u8be2\u7684\u5f03\u6743\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe56%\u3002\u5728MATH\u548cAIME\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u6bd4\u81ea\u9a8c\u8bc1\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9ad8\u8fbe6\u4e2a\u51c6\u786e\u5ea6\u70b9\u3002", "conclusion": "ConGrs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u83b7LM\u54cd\u5e94\u4e2d\u7684\u53d8\u5f02\uff0c\u5e76\u5229\u7528\u54cd\u5e94\u53d8\u5f02\u63d0\u4f9b\u7684\u77e5\u8bc6\u4fe1\u53f7\u6765\u5408\u6210\u66f4\u6709\u6548\u7684\u54cd\u5e94\u3002"}}
{"id": "2510.03337", "pdf": "https://arxiv.org/pdf/2510.03337", "abs": "https://arxiv.org/abs/2510.03337", "authors": ["Andrey A. Lebedev", "Victor B. Kazantsev", "Sergey V. Stasenko"], "title": "Error correction in multiclass image classification of facial emotion on unbalanced samples", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "This paper considers the problem of error correction in multi-class\nclassification of face images on unbalanced samples. The study is based on the\nanalysis of a data frame containing images labeled by seven different emotional\nstates of people of different ages. Particular attention is paid to the problem\nof class imbalance, in which some emotions significantly prevail over others.\nTo solve the classification problem, a neural network model based on LSTM with\nan attention mechanism focusing on key areas of the face that are informative\nfor emotion recognition is used. As part of the experiments, the model is\ntrained on all possible configurations of subsets of six classes with\nsubsequent error correction for the seventh class, excluded at the training\nstage. The results show that correction is possible for all classes, although\nthe degree of success varies: some classes are better restored, others are\nworse. In addition, on the test sample, when correcting some classes, an\nincrease in key quality metrics for small classes was recorded, which indicates\nthe promise of the proposed approach in solving applied problems related to the\nsearch for rare events, for example, in anti-fraud systems. Thus, the proposed\nmethod can be effectively applied in facial expression analysis systems and in\ntasks requiring stable classification under skewed class distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u89e3\u51b3\u4e0d\u5e73\u8861\u4eba\u8138\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u9519\u8bef\u6821\u6b63\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5c0f\u7c7b\u522b\u4e0a\u663e\u793a\u51fa\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u5e73\u8861\u6837\u672c\u4e2d\u4eba\u8138\u56fe\u50cf\u591a\u7c7b\u522b\u5206\u7c7b\u7684\u9519\u8bef\u6821\u6b63\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u67d0\u4e9b\u60c5\u611f\u7c7b\u522b\u663e\u8457\u591a\u4e8e\u5176\u4ed6\u7c7b\u522b\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLSTM\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u673a\u5236\u4e13\u6ce8\u4e8e\u9762\u90e8\u5173\u952e\u533a\u57df\u3002\u5b9e\u9a8c\u901a\u8fc7\u5728\u516d\u4e2a\u7c7b\u522b\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u7136\u540e\u5bf9\u8bad\u7ec3\u9636\u6bb5\u6392\u9664\u7684\u7b2c\u4e03\u4e2a\u7c7b\u522b\u8fdb\u884c\u9519\u8bef\u6821\u6b63\u3002", "result": "\u6240\u6709\u7c7b\u522b\u5747\u53ef\u8fdb\u884c\u9519\u8bef\u6821\u6b63\uff0c\u4f46\u6210\u529f\u7a0b\u5ea6\u4e0d\u540c\u3002\u5173\u952e\u5728\u4e8e\uff0c\u5728\u6821\u6b63\u67d0\u4e9b\u7c7b\u522b\u65f6\uff0c\u5c0f\u7c7b\u522b\u7684\u5173\u952e\u8d28\u91cf\u6307\u6807\u6709\u6240\u63d0\u5347\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u5206\u6790\u7cfb\u7edf\u4ee5\u53ca\u9700\u8981\u5728\u503e\u659c\u7c7b\u522b\u5206\u5e03\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u5206\u7c7b\u7684\u4efb\u52a1\u3002"}}
{"id": "2510.03254", "pdf": "https://arxiv.org/pdf/2510.03254", "abs": "https://arxiv.org/abs/2510.03254", "authors": ["David Benfield", "Stefano Coniglio", "Phan Tu Vuong", "Alain Zemkoho"], "title": "Adversarial training with restricted data manipulation", "categories": ["cs.LG", "cs.CR"], "comment": "21 page, 5 figures", "summary": "Adversarial machine learning concerns situations in which learners face\nattacks from active adversaries. Such scenarios arise in applications such as\nspam email filtering, malware detection and fake image generation, where\nsecurity methods must be actively updated to keep up with the everimproving\ngeneration of malicious data. Pessimistic Bilevel optimisation has been shown\nto be an effective method of training resilient classifiers against such\nadversaries. By modelling these scenarios as a game between the learner and the\nadversary, we anticipate how the adversary will modify their data and then\ntrain a resilient classifier accordingly. However, since existing pessimistic\nbilevel approaches feature an unrestricted adversary, the model is vulnerable\nto becoming overly pessimistic and unrealistic. When finding the optimal\nsolution that defeats the classifier, it is possible that the adversary's data\nbecomes nonsensical and loses its intended nature. Such an adversary will not\nproperly reflect reality, and consequently, will lead to poor classifier\nperformance when implemented on real-world data. By constructing a constrained\npessimistic bilevel optimisation model, we restrict the adversary's movements\nand identify a solution that better reflects reality. We demonstrate through\nexperiments that this model performs, on average, better than the existing\napproach.", "AI": {"tldr": "\u9488\u5bf9\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u4e2d\u73b0\u6709\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\u56e0\u5bf9\u6297\u8005\u4e0d\u53d7\u9650\u5236\u800c\u5bfc\u81f4\u4e0d\u5207\u5b9e\u9645\u7684\u653b\u51fb\u53ca\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u7ea6\u675f\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u9650\u5236\u5bf9\u6297\u8005\u7684\u884c\u52a8\u4f7f\u5176\u653b\u51fb\u66f4\u7b26\u5408\u73b0\u5b9e\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u7531\u4e8e\u5047\u8bbe\u5bf9\u6297\u8005\u62e5\u6709\u65e0\u9650\u7684\u653b\u51fb\u80fd\u529b\uff0c\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u8fc7\u4e8e\u60b2\u89c2\u548c\u4e0d\u5207\u5b9e\u9645\u3002\u8fd9\u79cd\u6a21\u578b\u53ef\u80fd\u4f7f\u5bf9\u6297\u8005\u751f\u6210\u7684\u6570\u636e\u5931\u53bb\u5176\u539f\u59cb\u6027\u8d28\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u653b\u51fb\uff0c\u8fdb\u800c\u5bfc\u81f4\u8bad\u7ec3\u51fa\u7684\u5206\u7c7b\u5668\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u53d7\u7ea6\u675f\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u9650\u5236\u5bf9\u6297\u8005\u7684\u884c\u52a8\u548c\u653b\u51fb\u65b9\u5f0f\uff0c\u786e\u4fdd\u5bf9\u6297\u6027\u6570\u636e\u7684\u751f\u6210\u66f4\u7b26\u5408\u73b0\u5b9e\u60c5\u666f\uff0c\u4ece\u800c\u5bfb\u627e\u5230\u4e00\u4e2a\u66f4\u771f\u5b9e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u672c\u6587\u63d0\u51fa\u7684\u53d7\u7ea6\u675f\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u975e\u53d7\u9650\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5bf9\u6297\u8005\u884c\u52a8\u65bd\u52a0\u5408\u7406\u7ea6\u675f\uff0c\u6240\u63d0\u51fa\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u53cd\u6620\u73b0\u5b9e\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u5bf9\u771f\u5b9e\u4e16\u754c\u5bf9\u6297\u6027\u653b\u51fb\u66f4\u5177\u9c81\u68d2\u6027\u548c\u66f4\u6709\u6548\u80fd\u7684\u5206\u7c7b\u5668\u3002"}}
{"id": "2510.03632", "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "categories": ["cs.AI"], "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.", "AI": {"tldr": "MITS\u901a\u8fc7\u57fa\u4e8e\u70b9\u4e92\u4fe1\u606f\uff08PMI\uff09\u7684\u8bc4\u5206\u51fd\u6570\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6811\u641c\u7d22\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6307\u5bfc\u6846\u67b6\u3002", "motivation": "\u73b0\u6709LLMs\u6811\u641c\u7d22\u63a8\u7406\u65b9\u6cd5\uff08\u5982Tree-of-Thought, MCTS\uff09\u96be\u4ee5\u5373\u65f6\u53ef\u9760\u5730\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8d28\u91cf\uff0c\u4e14\u5e7f\u6cdb\u7684\u8def\u5f84\u63a2\u7d22\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e92\u4fe1\u606f\u6811\u641c\u7d22\uff08MITS\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5f15\u5165\u57fa\u4e8e\u70b9\u4e92\u4fe1\u606f\uff08PMI\uff09\u7684\u6709\u6548\u8bc4\u5206\u51fd\u6570\uff0c\u5b9e\u73b0\u63a8\u7406\u8def\u5f84\u7684\u9010\u6b65\u8bc4\u4f30\u548c\u901a\u8fc7\u675f\u641c\u7d22\u8fdb\u884c\u6811\u6269\u5c55\uff0c\u907f\u514d\u6602\u8d35\u7684\u9884\u4f30\u6a21\u62df\u3002MITS\u8fd8\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7ed9\u4e0d\u786e\u5b9a\u7684\u63a8\u7406\u6b65\u9aa4\u3002\u6700\u7ec8\u9884\u6d4b\u91c7\u7528\u7ed3\u5408PMI\u5206\u6570\u548c\u9884\u6d4b\u5171\u8bc6\u7684\u52a0\u6743\u6295\u7968\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5728\u591a\u6837\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\uff0cMITS\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MITS\u4e3aLLM\u63a8\u7406\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u5f3a\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2510.04731", "pdf": "https://arxiv.org/pdf/2510.04731", "abs": "https://arxiv.org/abs/2510.04731", "authors": ["Douglas Dziedzorm Agbeve", "Andrey Belogaev", "Chris Blondia", "Jeroen Famaey"], "title": "Evaluating UORA-Based Polling Mechanism for Latency-Sensitive Uplink Traffic in Wi-Fi Networks", "categories": ["cs.NI"], "comment": null, "summary": "IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple\nAccess (OFDMA), which enables simultaneous transmissions through centralized\nresource allocation. However, effective uplink scheduling requires the Access\nPoint (AP) to identify which stations (STAs) have data to transmit. This\ntypically necessitates polling for buffer status reports, a process that\nbecomes increasingly inefficient and unscalable with growing device density. In\nthis paper, we study how the Uplink OFDMA-based Random Access (UORA) feature\nimproves the scalability and delay experienced by latency-sensitive data\nstreams. We show that UORA enables efficient uplink scheduling while\nopportunistically identifying buffered traffic from unscheduled STAs, striking\na balance between coordination and scalability. Performance evaluation of\ndifferent polling strategies is done by means of simulation in ns-3. The\nresults indicate that UORA-based polling outperforms alternative schemes in\ndensely deployed network environments with heterogeneous uplink traffic\npatterns. Furthermore, under highly sparse and sporadic traffic conditions,\nUORA-based polling yields over 40% delay reduction compared to Scheduled Access\n(SA) OFDMA.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Wi-Fi 6\u4e2dUplink OFDMA-based Random Access (UORA) \u5982\u4f55\u89e3\u51b3\u4f20\u7edfOFDMA\u4e0a\u884c\u8c03\u5ea6\u56e0\u8f6e\u8be2\u6548\u7387\u4f4e\u4e0b\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7ns-3\u4eff\u771f\u8bc1\u660eUORA\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u6d41\u91cf\u73af\u5883\u4e0b\u5747\u80fd\u663e\u8457\u63d0\u9ad8\u4e0a\u884c\u8c03\u5ea6\u6548\u7387\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "IEEE 802.11ax (Wi-Fi 6) \u7684OFDMA\u4e0a\u884c\u8c03\u5ea6\u9700\u8981AP\u8f6e\u8be2\u7ad9\u53f0\uff08STA\uff09\u4ee5\u83b7\u53d6\u7f13\u51b2\u72b6\u6001\u62a5\u544a\uff0c\u4f46\u968f\u7740\u8bbe\u5907\u5bc6\u5ea6\u589e\u52a0\uff0c\u8fd9\u79cd\u8f6e\u8be2\u65b9\u5f0f\u53d8\u5f97\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u7279\u522b\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u6570\u636e\u6d41\u6784\u6210\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728ns-3\u4eff\u771f\u73af\u5883\u4e2d\u5bf9\u4e0d\u540c\u7684\u8f6e\u8be2\u7b56\u7565\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff0c\u6bd4\u8f83UORA\u4e0e\u5176\u4ed6\u65b9\u6848\u7684\u8868\u73b0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1a1) \u5728\u90e8\u7f72\u5bc6\u96c6\u7684\u3001\u5177\u6709\u5f02\u6784\u4e0a\u884c\u6d41\u91cf\u6a21\u5f0f\u7684\u7f51\u7edc\u73af\u5883\u4e2d\uff0c\u57fa\u4e8eUORA\u7684\u8f6e\u8be2\u65b9\u6848\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\u30022) \u5728\u9ad8\u5ea6\u7a00\u758f\u548c\u5076\u53d1\u7684\u6d41\u91cf\u6761\u4ef6\u4e0b\uff0c\u57fa\u4e8eUORA\u7684\u8f6e\u8be2\u65b9\u6848\u4e0e\u8c03\u5ea6\u8bbf\u95ee (SA) OFDMA \u76f8\u6bd4\uff0c\u53ef\u5c06\u5ef6\u8fdf\u964d\u4f4e40%\u4ee5\u4e0a\u3002", "conclusion": "UORA\u901a\u8fc7\u673a\u4f1a\u6027\u5730\u8bc6\u522b\u672a\u8c03\u5ea6\u7ad9\u53f0\u7684\u7f13\u51b2\u6d41\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e0a\u884c\u8c03\u5ea6\uff0c\u5e76\u5728\u534f\u8c03\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u663e\u8457\u6539\u5584\u4e86\u5ef6\u8fdf\u654f\u611f\u6570\u636e\u6d41\u7684\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u8868\u73b0\u3002"}}
{"id": "2510.03528", "pdf": "https://arxiv.org/pdf/2510.03528", "abs": "https://arxiv.org/abs/2510.03528", "authors": ["Ahmed Alajrami", "Xingwei Tan", "Nikolaos Aletras"], "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance", "categories": ["cs.CL"], "comment": null, "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities\nof large language models (LLMs), improving their usability in generating\nhelpful responses on various tasks. However, previous work has demonstrated\nthat they are sensitive to minor variations in instruction phrasing. In this\npaper, we explore whether introducing perturbations in instruction-tuning data\ncan enhance LLMs' resistance against noisy instructions. We focus on how\ninstruction-tuning with perturbations, such as removing stop words or shuffling\nwords, affects LLMs' performance on the original and perturbed versions of\nwidely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics\nand potential shifts in model behavior. Surprisingly, our results suggest that\ninstruction-tuning on perturbed instructions can, in some cases, improve\ndownstream performance. These findings highlight the importance of including\nperturbed instructions in instruction-tuning, which can make LLMs more\nresilient to noisy user inputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5728\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4e2d\u5f15\u5165\u6270\u52a8\uff08\u5982\u5220\u9664\u505c\u7528\u8bcd\u3001\u6253\u4e71\u8bcd\u5e8f\uff09\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u566a\u97f3\u6307\u4ee4\u7684\u9c81\u68d2\u6027\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6539\u5584\u4e0b\u6e38\u6027\u80fd\u3002", "motivation": "\u6307\u4ee4\u5fae\u8c03\u5bf9\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u5bf9\u6307\u4ee4\u63aa\u8f9e\u7684\u5fae\u5c0f\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002", "method": "\u63a2\u7d22\u5728\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4e2d\u5f15\u5165\u6270\u52a8\uff08\u5982\u5220\u9664\u505c\u7528\u8bcd\u3001\u6253\u4e71\u8bcd\u5e8f\uff09\u662f\u5426\u80fd\u589e\u5f3aLLMs\u5bf9\u566a\u97f3\u6307\u4ee4\u7684\u62b5\u6297\u529b\u3002\u901a\u8fc7\u8bc4\u4f30LLMs\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08MMLU, BBH, GSM8K\uff09\u7684\u539f\u59cb\u53ca\u6270\u52a8\u7248\u672c\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5b66\u4e60\u52a8\u6001\u548c\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6270\u52a8\u6307\u4ee4\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u6539\u5584LLMs\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u5728\u6307\u4ee4\u5fae\u8c03\u4e2d\u7eb3\u5165\u6270\u52a8\u6307\u4ee4\u975e\u5e38\u91cd\u8981\uff0c\u8fd9\u80fd\u4f7fLLMs\u5bf9\u5608\u6742\u7684\u7528\u6237\u8f93\u5165\u66f4\u5177\u5f39\u6027\u3002"}}
{"id": "2510.03341", "pdf": "https://arxiv.org/pdf/2510.03341", "abs": "https://arxiv.org/abs/2510.03341", "authors": ["Bozheng Li", "Miao Yang", "Zhenhan Chen", "Jiawang Cao", "Mushui Liu", "Yi Lu", "Yongliang Wu", "Bin Zhang", "Yangguang Ji", "Licheng Tang", "Jay Wu", "Wenbo Zhu"], "title": "OpusAnimation: Code-Based Dynamic Chart Generation", "categories": ["cs.CV"], "comment": "working in progress", "summary": "Dynamic Chart Generation (DCG) involves producing code-rendered animated\nvisualizations as charts. While recent advances in multi-modal large language\nmodels (MLLMs) have significantly improved their capability on static chart\ngeneration and comprehension, MLLMs' potential for handling dynamic chart\ngeneration and understanding remains underexplored. To bridge this research\ngap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first\nbenchmark evaluating MLLM's capability on dynamic chart generation tasks from\nthree dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and\nVideo-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with\nannotations covering instruction-code-video triplets and QA pairs for both code\nand video evaluation. Based on DCG-8K, we explored a two-stage training recipe,\nproposing Joint-Code-Visual Reward for group relative policy optimization to\nconstruct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking\nresult reveals shortcomings of existing MLLMs in the visual-to-chart task, and\nour model beats the best open-sourced MLLM with an average 8.31% performance\ngain across three tasks, and shows on par performance against proprietary\nmodels with only 3B parameters, proving the effectiveness of our training\nrecipe. Our code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\uff08DCG\uff09\u9886\u57df\u7814\u7a76\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2aDCG\u57fa\u51c6DCG-Bench\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DCG-8K\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u8054\u5408\u4ee3\u7801-\u89c6\u89c9\u5956\u52b1\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e13\u5bb6MLLM Qwen2.5-VL-DCG-3B\uff0c\u8be5\u6a21\u578b\u5728DCG\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u4e13\u6709\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9759\u6001\u56fe\u8868\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\uff08DCG\uff09\u548c\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86DCG-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u8bc4\u4f30MLLM\u52a8\u6001\u56fe\u8868\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u7b80\u5355\u6587\u672c\u5230\u56fe\u8868\u3001\u8be6\u7ec6\u6587\u672c\u5230\u56fe\u8868\u548c\u89c6\u9891\u5230\u56fe\u8868\u4e09\u4e2a\u7ef4\u5ea6\u3002\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684DCG-8K\u6570\u636e\u96c6\uff0c\u5305\u542b\u6307\u4ee4-\u4ee3\u7801-\u89c6\u9891\u4e09\u5143\u7ec4\u53ca\u95ee\u7b54\u5bf9\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u5f15\u5165\u8054\u5408\u4ee3\u7801-\u89c6\u89c9\u5956\u52b1\uff08Joint-Code-Visual Reward\uff09\u8fdb\u884c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u4ee5\u6784\u5efa\u7528\u4e8eDCG\u4efb\u52a1\u7684\u4e13\u5bb6MLLM Qwen2.5-VL-DCG-3B\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\u73b0\u6709MLLMs\u5728\u89c6\u9891\u5230\u56fe\u8868\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002\u63d0\u51fa\u7684Qwen2.5-VL-DCG-3B\u6a21\u578b\u5728\u4e09\u9879\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u6bd4\u6700\u4f73\u5f00\u6e90MLLM\u63d0\u9ad8\u4e868.31%\uff0c\u4e14\u5728\u4ec53B\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u4e13\u6709\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165DCG-Bench\u3001DCG-8K\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u586b\u8865\u4e86MLLMs\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u9886\u57df\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u53c2\u6570\u91cf\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.03255", "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165SciTS\u57fa\u51c6\u6d4b\u8bd5\u96c6\u548cTimeOmni\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u5b58\u5728\u7684\u6311\u6218\uff0c\u63d0\u5347\u5176\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u65f6\u95f4\u5e8f\u5217\u8fd9\u79cd\u57fa\u672c\u79d1\u5b66\u6570\u636e\u6a21\u6001\u7684\u5904\u7406\u65b9\u6cd5\u4e0d\u8db3\uff08\u5982\u7f16\u7801\u4e3a\u6587\u672c\u6216\u56fe\u50cf\uff09\uff0c\u96be\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u4e0e\u751f\u6210\u3002\u73b0\u6709\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8fc7\u4e8e\u4e13\u4e1a\u5316\uff0c\u5bf9\u975e\u5468\u671f\u3001\u5f02\u6784\u79d1\u5b66\u4fe1\u53f7\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u6784\u5efaSciTS\u57fa\u51c6\uff1a\u6db5\u76d612\u4e2a\u79d1\u5b66\u9886\u57df\u300143\u9879\u4efb\u52a1\uff0c\u5305\u542b\u8d85\u8fc75\u4e07\u4e2a\u5b9e\u4f8b\uff0c\u4fe1\u53f7\u957f\u5ea6\u4ece$10^0$\u5230$10^7$\uff0c\u9891\u7387\u9ad8\u8fbe10 MHz\u30022. \u5bf917\u79cd\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5305\u62ec\u7eaf\u6587\u672cLLMs\u3001\u591a\u6a21\u6001LLMs\u548c\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u30023. \u63d0\u51faTimeOmni\u6846\u67b6\uff1a\u4f7fLLMs\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u540c\u65f6\u517c\u5bb9\u901a\u7528LLM\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "1. \u901a\u7528LLMs\u6bd4\u4e13\u4e1a\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u30022. \u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u4f1a\u56e0\u5e8f\u5217\u8fc7\u957f\u6216\u6570\u503c\u7cbe\u5ea6\u635f\u5931\u800c\u9650\u5236LLMs\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u57fa\u51c6\u548c\u5efa\u6a21\u6846\u67b6\u7684\u7a7a\u767d\uff0c\u4e3aLLMs\u7406\u89e3\u548c\u751f\u6210\u590d\u6742\u7684\u65f6\u5e8f\u79d1\u5b66\u6570\u636e\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03680", "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "categories": ["cs.AI"], "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u201c<eos>\u201d\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u54cd\u5e94\u53d8\u77ed\u3002\u672c\u6587\u5c06\u5176\u5f52\u56e0\u4e8e\u201c<eos>\u201d\u7684\u53cc\u91cd\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u5f69\u8679\u586b\u5145\u201d\uff08Rainbow Padding\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u586b\u5145\u4ee4\u724c\u6765\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6307\u4ee4\u5fae\u8c03\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u5206\u914d\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u65f6\uff0c\u54cd\u5e94\u53cd\u800c\u53d8\u77ed\uff0c\u751a\u81f3\u9000\u5316\u4e3a\u201c<eos>\u201d\u4ee4\u724c\u6d41\uff0c\u5373\u201c<eos>\u201d\u6ea2\u51fa\u3002\u5c3d\u7ba1\u5728\u5b9e\u8df5\u4e2d\u5df2\u88ab\u6ce8\u610f\u5230\uff0c\u4f46\u8be5\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u5206\u6790\uff0c\u963b\u788d\u4e86dLLMs\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8ffd\u6eaf\u4e86\u201c<eos>\u201d\u6ea2\u51fa\u7684\u6839\u672c\u539f\u56e0\uff0c\u53d1\u73b0\u5176\u5728\u4e8e\u201c<eos>\u201d\u4f5c\u4e3a\u7ec8\u6b62\u7b26\u548c\u586b\u5145\u7b26\u7684\u53cc\u91cd\u4f5c\u7528\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u5f69\u8679\u586b\u5145\u201d\u65b9\u6cd5\uff0c\u7528\u4e00\u4e2a\u91cd\u590d\u5faa\u73af\u7684\u4e0d\u540c\u586b\u5145\u4ee4\u724c\u66ff\u6362\u91cd\u590d\u7684\u201c<eos>\u201d\u5360\u4f4d\u7b26\uff0c\u4ee5\u5206\u6563\u6982\u7387\u8d28\u91cf\uff0c\u6253\u7834\u201c<eos>\u201d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\uff0c\u5305\u62ec\u4f7f\u7528LoRA\u5728\u5c11\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u5355epoch\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u201c\u5f69\u8679\u586b\u5145\u201d\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u7684\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u3002\u53ea\u9700\u4e03\u4e2a\u586b\u5145\u4ee4\u724c\u5373\u53ef\u6709\u6548\u9632\u6b62\u65e9\u671f\u7ec8\u6b62\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u5728\u5c11\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u5355epoch\u5373\u53ef\u83b7\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u201c\u5f69\u8679\u586b\u5145\u201d\u662f\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6307\u4ee4\u5fae\u8c03\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u201c<eos>\u201d\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u663e\u8457\u63d0\u5347\u5176\u957f\u5ea6\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u8d28\u91cf\uff0c\u5177\u6709\u9ad8\u5ea6\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03601", "pdf": "https://arxiv.org/pdf/2510.03601", "abs": "https://arxiv.org/abs/2510.03601", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Kai-Chun Liu", "Yu Tsao"], "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SP", "I.2.6; C.2.4"], "comment": "15 pages, 7 figures, and published in IEEE Sensors Journal", "summary": "The rising aging population has increased the importance of fall detection\n(FD) systems as an assistive technology, where deep learning techniques are\nwidely applied to enhance accuracy. FD systems typically use edge devices (EDs)\nworn by individuals to collect real-time data, which are transmitted to a cloud\ncenter (CC) or processed locally. However, this architecture faces challenges\nsuch as a limited ED model size and data transmission latency to the CC. Mobile\nedge computing (MEC), which allows computations at MEC servers deployed between\nEDs and CC, has been explored to address these challenges. We propose a\nmultilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC\nsplits the architecture into stations, each with a neural network model. If\nfront-end equipment cannot detect falls reliably, data are transmitted to a\nstation with more robust back-end computing. The knowledge distillation (KD)\napproach was employed to improve front-end detection accuracy by allowing\nhigh-power back-end stations to provide additional learning experiences,\nenhancing precision while reducing latency and processing loads. Simulation\nresults demonstrate that the KD approach improved accuracy by 11.65% on the\nSisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also\nreduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on\nthe SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD\nsystem exhibits improved accuracy and reduced latency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MLMEC\uff09\u5e76\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u7684\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\uff0c\u6709\u6548\u63d0\u5347\u7cbe\u5ea6\u5e76\u663e\u8457\u964d\u4f4e\u6570\u636e\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u8001\u9f84\u5316\u4eba\u53e3\u589e\u52a0\uff0c\u8dcc\u5012\u68c0\u6d4b\uff08FD\uff09\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u73b0\u6709FD\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u8fb9\u7f18\u8bbe\u5907-\u4e91\u4e2d\u5fc3\u67b6\u6784\uff0c\u4f46\u9762\u4e34\u8fb9\u7f18\u8bbe\u5907\u6a21\u578b\u5927\u5c0f\u53d7\u9650\u548c\u6570\u636e\u4f20\u8f93\u5230\u4e91\u4e2d\u5fc3\u65f6\u7684\u5ef6\u8fdf\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MLMEC\uff09\u6846\u67b6\uff0c\u65e8\u5728\u5e73\u8861\u7cbe\u5ea6\u4e0e\u5ef6\u8fdf\u3002\u8be5\u6846\u67b6\u5c06\u7cfb\u7edf\u67b6\u6784\u5212\u5206\u4e3a\u591a\u4e2a\u7ad9\u70b9\uff0c\u6bcf\u4e2a\u7ad9\u70b9\u90e8\u7f72\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u5f53\u524d\u7aef\u8bbe\u5907\u65e0\u6cd5\u53ef\u9760\u68c0\u6d4b\u8dcc\u5012\u65f6\uff0c\u6570\u636e\u4f1a\u88ab\u4f20\u8f93\u5230\u5177\u6709\u66f4\u5f3a\u5927\u540e\u7aef\u8ba1\u7b97\u80fd\u529b\u7684\u7ad9\u70b9\u3002\u4e3a\u63d0\u9ad8\u524d\u7aef\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6587\u4e2d\u91c7\u7528\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u529f\u7387\u540e\u7aef\u7ad9\u70b9\u7684\u77e5\u8bc6\u7ecf\u9a8c\u6765\u589e\u5f3a\u524d\u7aef\u5b66\u4e60\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\u4f7fSisFall\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e8611.65%\uff0cFallAllD\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e862.78%\u3002\u4e0e\u6ca1\u6709KD\u7684MLMEC\u76f8\u6bd4\uff0c\u7ed3\u5408KD\u7684MLMEC\u5728FallAllD\u6570\u636e\u96c6\u4e0a\u5c06\u6570\u636e\u5ef6\u8fdf\u7387\u964d\u4f4e\u4e8654.15%\uff0c\u5728SisFall\u6570\u636e\u96c6\u4e0a\u964d\u4f4e\u4e8646.67%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MLMEC\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5728\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u5ef6\u8fdf\u3002"}}
{"id": "2510.03536", "pdf": "https://arxiv.org/pdf/2510.03536", "abs": "https://arxiv.org/abs/2510.03536", "authors": ["Zhaohan Meng", "Zaiqiao Meng", "Siwei Liu", "Iadh Ounis"], "title": "TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) perform strongly in static and single-turn\nmedical Question Answer (QA) benchmarks, yet such settings diverge from the\niterative information gathering process required in practical clinical\nconsultations. The MEDIQ framework addresses this mismatch by recasting the\ndiagnosis as an interactive dialogue between a patient and an expert system,\nbut the reliability of LLMs drops dramatically when forced to reason with\ndialogue logs, where clinical facts appear in sentences without clear links. To\nbridge this gap, we introduce TriMediQ, a triplet-structured approach that\nsummarises patient responses into triplets and integrates them into a Knowledge\nGraph (KG), enabling multi-hop reasoning. We introduce a frozen triplet\ngenerator that extracts clinically relevant triplets, using prompts designed to\nensure factual consistency. In parallel, a trainable projection module,\ncomprising a graph encoder and a projector, captures relational information\nfrom the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)\nthe projection module fine-tuning with all LLM weights frozen; and (ii) using\nthe fine-tuned module to guide multi-hop reasoning during inference. We\nevaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up\nto 10.4\\% improvement in accuracy over five baselines on the iMedQA dataset.\nThese results demonstrate that converting patient responses into structured\ntriplet-based graphs enables more accurate clinical reasoning in multi-turn\nsettings, providing a solution for the deployment of LLM-based medical\nassistants.", "AI": {"tldr": "TriMediQ\u662f\u4e00\u4e2a\u4e09\u5143\u7ec4\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u60a3\u8005\u53cd\u9988\u8f6c\u5316\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u7ed3\u6784\u5316\u4e09\u5143\u7ec4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u5f0f\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u65e5\u5fd7\u65f6\u53ef\u9760\u6027\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9759\u6001\u548c\u5355\u8f6e\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd9\u79cd\u8bbe\u7f6e\u4e0e\u5b9e\u9645\u4e34\u5e8a\u54a8\u8be2\u6240\u9700\u7684\u8fed\u4ee3\u4fe1\u606f\u6536\u96c6\u8fc7\u7a0b\u4e0d\u7b26\u3002MEDIQ\u6846\u67b6\u5c06\u8bca\u65ad\u91cd\u6784\u4e3a\u4e92\u52a8\u5bf9\u8bdd\uff0c\u4f46\u5f53LLMs\u88ab\u8feb\u5728\u7f3a\u4e4f\u660e\u786e\u5173\u8054\u7684\u5bf9\u8bdd\u65e5\u5fd7\u4e2d\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u5176\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\u3002", "method": "\u5f15\u5165TriMediQ\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u95ee\u9898\uff1a1. \u5c06\u60a3\u8005\u7684\u56de\u7b54\u603b\u7ed3\u4e3a\u4e09\u5143\u7ec4\u5e76\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u591a\u8df3\u63a8\u7406\u30022. \u4f7f\u7528\u4e00\u4e2a\u51bb\u7ed3\u7684\u4e09\u5143\u7ec4\u751f\u6210\u5668\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u4e09\u5143\u7ec4\uff0c\u786e\u4fdd\u4e8b\u5b9e\u4e00\u81f4\u6027\u30023. \u5305\u542b\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u6a21\u5757\uff08\u7531\u56fe\u7f16\u7801\u5668\u548c\u6295\u5f71\u5668\u7ec4\u6210\uff09\uff0c\u7528\u4e8e\u4eceKG\u4e2d\u6355\u83b7\u5173\u7cfb\u4fe1\u606f\u4ee5\u589e\u5f3a\u4e13\u5bb6\u63a8\u7406\u3002TriMediQ\u5206\u4e24\u6b65\u64cd\u4f5c\uff1a(i) \u5728LLM\u6240\u6709\u6743\u91cd\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u6295\u5f71\u6a21\u5757\uff1b(ii) \u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5fae\u8c03\u540e\u7684\u6a21\u5757\u6307\u5bfc\u591a\u8df3\u63a8\u7406\u3002", "result": "TriMediQ\u5728\u4e24\u4e2a\u4ea4\u4e92\u5f0f\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728iMedQA\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4e94\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5176\u51c6\u786e\u6027\u6700\u9ad8\u63d0\u5347\u4e8610.4%\u3002", "conclusion": "\u5c06\u60a3\u8005\u56de\u7b54\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684\u56fe\uff0c\u80fd\u591f\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e34\u5e8a\u63a8\u7406\uff0c\u4e3a\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u533b\u7597\u52a9\u624b\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03348", "pdf": "https://arxiv.org/pdf/2510.03348", "abs": "https://arxiv.org/abs/2510.03348", "authors": ["Vlardimir Yugay", "Duy-Kien Nguyen", "Theo Gevers", "Cees G. M. Snoek", "Martin R. Oswald"], "title": "Visual Odometry with Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Modern monocular visual odometry methods typically combine pre-trained deep\nlearning components with optimization modules, resulting in complex pipelines\nthat rely heavily on camera calibration and hyperparameter tuning, and often\nstruggle in unseen real-world scenarios. Recent large-scale 3D models trained\non massive amounts of multi-modal data have partially alleviated these\nchallenges, providing generalizable dense reconstruction and camera pose\nestimation. Still, they remain limited in handling long videos and providing\naccurate per-frame estimates, which are required for visual odometry. In this\nwork, we demonstrate that monocular visual odometry can be addressed\neffectively in an end-to-end manner, thereby eliminating the need for\nhandcrafted components such as bundle adjustment, feature matching, camera\ncalibration, or dense 3D reconstruction. We introduce VoT, short for Visual\nodometry Transformer, which processes sequences of monocular frames by\nextracting features and modeling global relationships through temporal and\nspatial attention. Unlike prior methods, VoT directly predicts camera motion\nwithout estimating dense geometry and relies solely on camera poses for\nsupervision. The framework is modular and flexible, allowing seamless\nintegration of various pre-trained encoders as feature extractors. Experimental\nresults demonstrate that VoT scales effectively with larger datasets, benefits\nsubstantially from stronger pre-trained backbones, generalizes across diverse\ncamera motions and calibration settings, and outperforms traditional methods\nwhile running more than 3 times faster. The code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVoT\uff0c\u4e00\u79cd\u7aef\u5230\u7aefTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u6ce8\u610f\u529b\u76f4\u63a5\u9884\u6d4b\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u76f8\u673a\u8fd0\u52a8\uff0c\u65e0\u9700\u4f20\u7edf\u624b\u5de5\u7ec4\u4ef6\u6216\u5bc6\u96c6\u51e0\u4f55\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u590d\u6742\u3001\u4f9d\u8d56\u76f8\u673a\u6807\u5b9a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u4e14\u5728\u672a\u77e5\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u5c3d\u7ba1\u5927\u578b3D\u6a21\u578b\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u548c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u6bcf\u5e27\u4f30\u8ba1\u65b9\u9762\u4ecd\u6709\u9650\u5236\uff0c\u800c\u8fd9\u4e9b\u662f\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6240\u5fc5\u9700\u7684\u3002", "method": "\u5f15\u5165VoT\uff08Visual odometry Transformer\uff09\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u63d0\u53d6\u7279\u5f81\u5e76\u5229\u7528\u65f6\u7a7a\u6ce8\u610f\u529b\u5efa\u6a21\u5168\u5c40\u5173\u7cfb\u6765\u5904\u7406\u5355\u76ee\u5e27\u5e8f\u5217\uff0c\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\uff0c\u4e0d\u4f30\u8ba1\u5bc6\u96c6\u51e0\u4f55\uff0c\u4ec5\u4f9d\u8d56\u76f8\u673a\u4f4d\u59ff\u8fdb\u884c\u76d1\u7763\u3002\u8be5\u6846\u67b6\u6a21\u5757\u5316\u4e14\u7075\u6d3b\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5404\u79cd\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVoT\u80fd\u6709\u6548\u6269\u5c55\u5230\u66f4\u5927\u6570\u636e\u96c6\uff0c\u53d7\u76ca\u4e8e\u66f4\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u76f8\u673a\u8fd0\u52a8\u548c\u6807\u5b9a\u8bbe\u7f6e\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb3\u500d\u4ee5\u4e0a\u3002", "conclusion": "VoT\u8bc1\u660e\u4e86\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u53ef\u4ee5\u88ab\u6709\u6548\u3001\u7aef\u5230\u7aef\u5730\u89e3\u51b3\uff0c\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\u5e76\u6d88\u9664\u5bf9\u4f20\u7edf\u590d\u6742\u7ec4\u4ef6\u6216\u5bc6\u96c63D\u91cd\u5efa\u7684\u9700\u6c42\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03257", "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTriple-BERT\uff0c\u4e00\u79cd\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(SARL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u89e3\u548cBERT\u7f51\u7edc\u5e94\u5bf9\u5927\u89c4\u6a21\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u7684\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u7f51\u7ea6\u8f66\u5e73\u53f0\u9762\u4e34\u590d\u6742\u7684\u5b9e\u65f6\u8ba2\u5355\u5339\u914d\u548c\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u5927\u89c4\u6a21\u7684\u53f8\u4e58\u6570\u91cf\u5bfc\u81f4\u89c2\u5bdf\u7a7a\u95f4\u5de8\u5927\uff0c\u4f7f\u5f97\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u72ec\u7acb\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u548c\u534f\u4f5c\u6027\uff0c\u800c\u96c6\u4e2d\u8bad\u7ec3\u53bb\u4e2d\u5fc3\u5316\u6267\u884c(CTDE)\u7684MARL\u65b9\u6cd5\u5219\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u3002", "method": "\u6211\u4eec\u63d0\u51faTriple-BERT\uff0c\u4e00\u79cd\u57fa\u4e8eTD3\u53d8\u4f53\u7684\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(SARL)\u65b9\u6cd5\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u8bbe\u8ba1\u3002\u4e3a\u5904\u7406\u5e9e\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5b83\u91c7\u7528\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u8054\u5408\u52a8\u4f5c\u6982\u7387\u5206\u89e3\u4e3a\u5355\u4e2a\u53f8\u673a\u7684\u52a8\u4f5c\u6982\u7387\u3002\u4e3a\u5e94\u5bf9\u5e7f\u6cdb\u7684\u89c2\u5bdf\u7a7a\u95f4\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578bBERT\u7f51\u7edc\uff0c\u901a\u8fc7\u53c2\u6570\u91cd\u7528\u7f13\u89e3\u53c2\u6570\u589e\u957f\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6355\u6349\u5927\u91cf\u53f8\u4e58\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "Triple-BERT\u5728\u66fc\u54c8\u987f\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347\u7ea611.95%\uff0c\u5176\u4e2d\u670d\u52a1\u8ba2\u5355\u91cf\u589e\u52a04.26%\uff0c\u63a5\u9a7e\u65f6\u95f4\u51cf\u5c1122.25%\u3002", "conclusion": "Triple-BERT\u901a\u8fc7\u5176\u65b0\u9896\u7684\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u548cBERT\u7f51\u7edc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u4e2d\u7684\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8c03\u5ea6\u6548\u7387\u3001\u670d\u52a1\u8ba2\u5355\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4e3a\u672a\u6765\u7f51\u7ea6\u8f66\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03696", "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u9488\u5bf9\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u7cfb\u7edf\u7684\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u76ee\u6807\u6210\u529f\u7387\uff08GSR\uff09\u548c\u5931\u8d25\u6839\u672c\u539f\u56e0\uff08RCOF\uff09\u5206\u7c7b\u6cd5\uff0c\u5e76\u7ed3\u5408\u6559\u5e08LLM\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9ad8\u6548\u7684\u8bc4\u4f30\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u4f01\u4e1a\u7ea7\u7cfb\u7edf\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u673a\u5668\u4eba\u8bc4\u4f30\u65b9\u6cd5\u591a\u505c\u7559\u5728\u8f6e\u6b21\u5c42\u9762\uff0c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u7528\u6237\u6574\u4f53\u76ee\u6807\uff08\u5982\u4fe1\u606f\u9700\u6c42\u6216\u4efb\u52a1\uff09\u662f\u5426\u8fbe\u6210\uff0c\u5bfc\u81f4\u591a\u8f6e\u5bf9\u8bdd\u8d28\u91cf\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u9762\u5411\u76ee\u6807\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u76ee\u6807\u6210\u529f\u7387\uff08GSR\uff09\u6765\u8861\u91cf\u76ee\u6807\u8fbe\u6210\u767e\u5206\u6bd4\uff1b2) \u5931\u8d25\u6839\u672c\u539f\u56e0\uff08RCOF\uff09\u5206\u7c7b\u6cd5\u6765\u8bc6\u522b\u6545\u969c\u539f\u56e0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7528\u6237\u76ee\u6807\u5206\u5272\u5bf9\u8bdd\uff0c\u5e76\u5229\u7528\u6240\u6709\u76f8\u5173\u8f6e\u6b21\u8bc4\u4f30\u6210\u529f\u3002\u8bc4\u4f30\u7cfb\u7edf\u7ed3\u5408\u4e86\u6559\u5e08LLM\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u76ee\u6807\u548c\u8d28\u91cf\u6807\u51c6\uff0cLLM\u901a\u8fc7\u201c\u601d\u8003\u4ee4\u724c\u201d\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7406\u7531\u3002", "result": "\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u4f01\u4e1a\u7ea7\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u7cfb\u7edfAIDA\uff0c\u89c2\u5bdf\u5230\u5176GSR\u5728\u516d\u4e2a\u6708\u5185\u4ece63%\u63d0\u5347\u81f379%\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8be6\u7ec6\u7684\u7f3a\u9677\u5206\u7c7b\u6cd5\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u80fd\u591f\u8bca\u65ad\u6574\u4f53\u6210\u529f\u3001\u8bc6\u522b\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u5e76\u6307\u5bfc\u7cfb\u7edf\u6539\u8fdb\u3002", "conclusion": "\u8be5\u901a\u7528\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9ad8\u6548\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u80fd\u6709\u6548\u8bca\u65ad\u7cfb\u7edf\u6027\u80fd\u3001\u8bc6\u522b\u6545\u969c\u6a21\u5f0f\u5e76\u4e3a\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2510.03891", "pdf": "https://arxiv.org/pdf/2510.03891", "abs": "https://arxiv.org/abs/2510.03891", "authors": ["Shawn Shuoshuo Chen", "Daiyaan Arfeen", "Minlan Yu", "Peter Steenkiste", "Srinivasan Seshan"], "title": "Toward Co-adapting Machine Learning Job Shape and Cluster Topology", "categories": ["cs.DC", "cs.NI"], "comment": null, "summary": "Allocating resources to distributed machine learning jobs in multi-tenant\ntorus-topology clusters must meet each job's specific placement and\ncommunication requirements, which are typically described using shapes. There\nis an inherent tension between minimizing network contention and maximizing\ncluster utilization when placing various-shaped jobs. While existing schedulers\ntypically optimize for one objective at the expense of the other, we\ndemonstrate that both can be achieved simultaneously.\n  Our proposed approach, RFold, adapts both job shapes and the underlying\ncluster topology at runtime. This is accomplished by combining two techniques:\n(1) identifying homomorphic job shapes that support the jobs communication\nneeds, and (2) reconfiguring the optical circuit switch-enabled topology to\nsupport more diverse job shapes. Preliminary evaluation performed on a\n4096-node torus cluster simulator indicates that RFold can improve absolute\ncluster utilization by 57% and reduce job completion time by up to 11x relative\nto existing methods", "AI": {"tldr": "RFold\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\uff0c\u5728\u73af\u9762\u96c6\u7fa4\u4e2d\u540c\u65f6\u5b9e\u73b0\u4e86\u673a\u5668\u5b66\u4e60\u4f5c\u4e1a\u8d44\u6e90\u5206\u914d\u4e2d\u7f51\u7edc\u4e89\u7528\u6700\u5c0f\u5316\u548c\u96c6\u7fa4\u5229\u7528\u7387\u6700\u5927\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u79df\u6237\u73af\u9762\u62d3\u6251\u96c6\u7fa4\u4e2d\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4f5c\u4e1a\u5206\u914d\u8d44\u6e90\u65f6\uff0c\u9700\u8981\u5728\u6700\u5c0f\u5316\u7f51\u7edc\u4e89\u7528\u548c\u6700\u5927\u5316\u96c6\u7fa4\u5229\u7528\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u73b0\u6709\u8c03\u5ea6\u5668\u96be\u4ee5\u517c\u987e\u8fd9\u4e24\u4e2a\u76ee\u6807\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RFold\u65b9\u6cd5\uff0c\u5728\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u5e95\u5c42\u96c6\u7fa4\u62d3\u6251\u3002\u8fd9\u901a\u8fc7\u4e24\u79cd\u6280\u672f\u5b9e\u73b0\uff1a1) \u8bc6\u522b\u652f\u6301\u4f5c\u4e1a\u901a\u4fe1\u9700\u6c42\u7684\u540c\u6001\u4f5c\u4e1a\u5f62\u72b6\uff1b2) \u91cd\u65b0\u914d\u7f6e\u5149\u8def\u4ea4\u6362\u673a(OCS)\u4f7f\u80fd\u7684\u62d3\u6251\uff0c\u4ee5\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u4f5c\u4e1a\u5f62\u72b6\u3002", "result": "\u57284096\u8282\u70b9\u73af\u9762\u96c6\u7fa4\u6a21\u62df\u5668\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0cRFold\u80fd\u5c06\u7edd\u5bf9\u96c6\u7fa4\u5229\u7528\u7387\u63d0\u9ad857%\uff0c\u5e76\u5c06\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u9ad8\u8fbe11\u500d\u3002", "conclusion": "RFold\u6210\u529f\u8bc1\u660e\u4e86\u901a\u8fc7\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\uff0c\u53ef\u4ee5\u5728\u591a\u79df\u6237\u73af\u9762\u96c6\u7fa4\u4e2d\u540c\u65f6\u5b9e\u73b0\u7f51\u7edc\u4e89\u7528\u6700\u5c0f\u5316\u548c\u96c6\u7fa4\u5229\u7528\u7387\u6700\u5927\u5316\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4f5c\u4e1a\u7684\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2510.03541", "pdf": "https://arxiv.org/pdf/2510.03541", "abs": "https://arxiv.org/abs/2510.03541", "authors": ["Andrew Halterman", "Katherine A. Keith"], "title": "What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification", "categories": ["cs.CL"], "comment": null, "summary": "Generative large language models (LLMs) are now used extensively for text\nclassification in computational social science (CSS). In this work, focus on\nthe steps before and after LLM prompting -- conceptualization of concepts to be\nclassified and using LLM predictions in downstream statistical inference --\nwhich we argue have been overlooked in much of LLM-era CSS. We claim LLMs can\ntempt analysts to skip the conceptualization step, creating conceptualization\nerrors that bias downstream estimates. Using simulations, we show that this\nconceptualization-induced bias cannot be corrected for solely by increasing LLM\naccuracy or post-hoc bias correction methods. We conclude by reminding CSS\nanalysts that conceptualization is still a first-order concern in the LLM-era\nand provide concrete advice on how to pursue low-cost, unbiased, low-variance\ndownstream estimates.", "AI": {"tldr": "LLM\u5728CSS\u6587\u672c\u5206\u7c7b\u4e2d\u6613\u4f7f\u5206\u6790\u5e08\u5ffd\u89c6\u6982\u5ff5\u5316\u6b65\u9aa4\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4f30\u8ba1\u504f\u5dee\u3002\u6a21\u62df\u663e\u793a\u8be5\u504f\u5dee\u65e0\u6cd5\u5355\u9760\u63d0\u9ad8LLM\u51c6\u786e\u6027\u6216\u4e8b\u540e\u6821\u6b63\u89e3\u51b3\u3002\u5f3a\u8c03\u6982\u5ff5\u5316\u5728LLM\u65f6\u4ee3\u4ecd\u662f\u9996\u8981\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66(CSS)\u6587\u672c\u5206\u7c7b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u524d\u7684\u6982\u5ff5\u5316\u548c\u5176\u540e\u7684\u7edf\u8ba1\u63a8\u65ad\u6b65\u9aa4\u5e38\u88ab\u5ffd\u89c6\u3002\u7814\u7a76\u65e8\u5728\u6307\u51faLLM\u53ef\u80fd\u8bf1\u5bfc\u5206\u6790\u5e08\u8df3\u8fc7\u6982\u5ff5\u5316\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0b\u6e38\u4f30\u8ba1\u4ea7\u751f\u504f\u5dee\u3002", "method": "\u91c7\u7528\u6a21\u62df\u65b9\u6cd5\u6765\u8bba\u8bc1\u6982\u5ff5\u5316\u9519\u8bef\u5982\u4f55\u4ea7\u751f\u504f\u5dee\uff0c\u5e76\u8bc4\u4f30\u901a\u8fc7\u63d0\u9ad8LLM\u51c6\u786e\u6027\u6216\u4e8b\u540e\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u5bf9\u8be5\u504f\u5dee\u7684\u4fee\u6b63\u6548\u679c\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u7531\u6982\u5ff5\u5316\u9519\u8bef\u5f15\u8d77\u7684\u504f\u5dee\u65e0\u6cd5\u4ec5\u901a\u8fc7\u63d0\u9ad8LLM\u7684\u51c6\u786e\u6027\u6216\u4f7f\u7528\u4e8b\u540e\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u6765\u6709\u6548\u7ea0\u6b63\u3002", "conclusion": "\u5728LLM\u65f6\u4ee3\uff0c\u6982\u5ff5\u5316\u5bf9\u4e8e\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5206\u6790\u5e08\u6765\u8bf4\u4ecd\u662f\u4e00\u4e2a\u81f3\u5173\u91cd\u8981\u7684\u95ee\u9898\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u83b7\u53d6\u4f4e\u6210\u672c\u3001\u65e0\u504f\u3001\u4f4e\u65b9\u5dee\u4e0b\u6e38\u4f30\u8ba1\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2510.03352", "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fa7\u4fe1\u606f\u7684\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u5e38\u5ffd\u7565\u4fa7\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u75c5\u6001\u8bbe\u7f6e\u4e0b\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u3002\u6b64\u5916\uff0c\u73b0\u6709\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u6613\u53d7\u201c\u5956\u52b1\u4f5c\u5f0a\u201d\uff08reward-hacking\uff09\u4f2a\u5f71\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u5229\u7528\u4fa7\u4fe1\u606f\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u4ee5\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u3002\u8be5\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u6a21\u578b\u91cd\u5efa\u6d41\u7a0b\u4e2d\uff0c\u5e76\u4f5c\u4e3a\u68af\u5ea6\u5f15\u5bfc\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u76d2\u5185\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u4ee5\u53ca\u591a\u79cd\u53bb\u6a21\u7cca\u4efb\u52a1\uff08\u8fd0\u52a8\u3001\u9ad8\u65af\u3001\u975e\u7ebf\u6027\u3001\u76f2\u53bb\u6a21\u7cca\uff09\u7b49\u591a\u4e2a\u9006\u95ee\u9898\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u56fe\u50cf\u91cd\u5efa\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5305\u62ec\u57fa\u4e8e\u5956\u52b1\u68af\u5ea6\u7684\u5f15\u5bfc\u7b97\u6cd5\u5728\u5185\u7684\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5229\u7528\u4fa7\u4fe1\u606f\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u5404\u79cd\u9006\u95ee\u9898\u4e2d\u91cd\u5efa\u8d28\u91cf\u548c\u53ef\u9760\u6027\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u68af\u5ea6\u5f15\u5bfc\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03258", "pdf": "https://arxiv.org/pdf/2510.03258", "abs": "https://arxiv.org/abs/2510.03258", "authors": ["Chang'an Yi", "Xiaohui Deng", "Shuaicheng Niu", "Yan Zhou"], "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "11pages,6 figures", "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to\nunknown test data with potential distribution shifts in an online manner. Many\nexisting TTA methods rely on entropy as a confidence metric to optimize the\nmodel. However, these approaches are sensitive to the predefined entropy\nthreshold, influencing which samples are chosen for model adaptation.\nConsequently, potentially reliable target samples are often overlooked and\nunderutilized. For instance, a sample's entropy might slightly exceed the\nthreshold initially, but fall below it after the model is updated. Such samples\ncan provide stable supervised information and offer a normal range of gradients\nto guide model adaptation. In this paper, we propose a general approach,\n\\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the\npreviously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}}\nsa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch\nnetwork to strike a balance between extracting domain-agnostic representations\nand achieving high performance on target data. Comprehensive experiments across\nmultiple architectures demonstrate that POEM consistently outperforms existing\nTTA methods in both challenging scenarios and real-world domain shifts, while\nremaining computationally efficient. The effectiveness of POEM is evaluated\nthrough extensive analyses and thorough ablation studies. Moreover, the core\nidea behind POEM can be employed as an augmentation strategy to boost the\nperformance of existing TTA approaches. The source code is publicly available\nat \\emph{https://github.com/ycarobot/POEM}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPOEM\uff0c\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u53ef\u9760\u6837\u672c\u548c\u4e00\u4e2a\u989d\u5916\u7684\u81ea\u9002\u5e94\u5206\u652f\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86TTA\u5728\u5404\u79cd\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684TTA\u65b9\u6cd5\u5e38\u4f9d\u8d56\u71b5\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6307\u6807\u8fdb\u884c\u6a21\u578b\u4f18\u5316\uff0c\u4f46\u5bf9\u9884\u8bbe\u7684\u71b5\u9608\u503c\u654f\u611f\uff0c\u5bfc\u81f4\u8bb8\u591a\u6f5c\u5728\u53ef\u9760\u7684\u6837\u672c\u88ab\u5ffd\u7565\u548c\u672a\u5145\u5206\u5229\u7528\u3002\u8fd9\u4e9b\u88ab\u5ffd\u89c6\u7684\u6837\u672c\u672c\u53ef\u4ee5\u63d0\u4f9b\u7a33\u5b9a\u7684\u76d1\u7763\u4fe1\u606f\u548c\u6709\u76ca\u7684\u68af\u5ea6\u6765\u6307\u5bfc\u6a21\u578b\u81ea\u9002\u5e94\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a**POEM**\uff08Promote TTA via eXploring the PreviOusly unexplorEd reliable saMples\uff09\u7684\u901a\u7528\u65b9\u6cd5\u3002POEM\u7684\u6838\u5fc3\u662f\u63a2\u7d22\u5e76\u5229\u7528\u90a3\u4e9b\u5148\u524d\u672a\u88ab\u53d1\u73b0\u7684\u53ef\u9760\u6837\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u989d\u5916\u7684**\u81ea\u9002\u5e94\u5206\u652f\u7f51\u7edc\uff08Adapt Branch network\uff09**\uff0c\u4ee5\u5e73\u8861\u63d0\u53d6\u9886\u57df\u65e0\u5173\u7684\u8868\u793a\u548c\u5728\u76ee\u6807\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002POEM\u7684\u6838\u5fc3\u601d\u60f3\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u5347\u73b0\u6709TTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "POEM\u5728\u591a\u79cd\u67b6\u6784\u548c\u5177\u6709\u6311\u6218\u6027\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u7684\u9886\u57df\u504f\u79fb\u573a\u666f\u4e0b\uff0c\u5747\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684TTA\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u5176\u6709\u6548\u6027\u901a\u8fc7\u5e7f\u6cdb\u7684\u5206\u6790\u548c\u5f7b\u5e95\u7684\u6d88\u878d\u7814\u7a76\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "POEM\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684TTA\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6709\u6548\u5229\u7528\u53ef\u9760\u6837\u672c\u89e3\u51b3\u4e86\u73b0\u6709\u71b5\u57faTTA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002POEM\u4e0d\u4ec5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u8fd8\u80fd\u4f5c\u4e3a\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u5176\u4ed6TTA\u65b9\u6cd5\u7684\u8868\u73b0\u3002"}}
{"id": "2510.03700", "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "categories": ["cs.AI"], "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faH-DDx\uff0c\u4e00\u4e2a\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u7ea0\u6b63\u4e86\u4f20\u7edf\u6241\u5e73\u5316\u6307\u6807\u4f4e\u4f30\u6a21\u578b\u6027\u80fd\u7684\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9274\u522b\u8bca\u65ad\u9886\u57df\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6241\u5e73\u5316\u6307\u6807\uff08\u5982Top-k\u51c6\u786e\u7387\uff09\uff0c\u8fd9\u4e9b\u6307\u6807\u65e0\u6cd5\u533a\u5206\u4e34\u5e8a\u76f8\u5173\u7684\u201c\u8fd1\u5931\u201d\u9519\u8bef\u548c\u8bca\u65ad\u4e0a\u76f8\u8ddd\u751a\u8fdc\u7684\u9519\u8bef\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u5f15\u5165\u4e86H-DDx\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u6d41\u7a0b\u5c06\u81ea\u7531\u6587\u672c\u8bca\u65ad\u6620\u5c04\u5230ICD-10\u7f16\u7801\uff0c\u5e76\u5e94\u7528\u5206\u5c42\u6307\u6807\u6765\u5956\u52b1\u4e0e\u771f\u5b9e\u8bca\u65ad\u5bc6\u5207\u76f8\u5173\u7684\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd522\u4e2a\u4e3b\u6d41\u6a21\u578b\u540e\u53d1\u73b0\uff0c\u4f20\u7edf\u6241\u5e73\u5316\u6307\u6807\u4f4e\u4f30\u4e86\u6027\u80fd\uff0cH-DDx\u63ed\u793a\u4e86\u6a21\u578b\uff08\u7279\u522b\u662f\u9886\u57df\u4e13\u7528\u5f00\u6e90\u6a21\u578b\uff09\u7684\u771f\u5b9e\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u63ed\u793a\u5206\u5c42\u9519\u8bef\u6a21\u5f0f\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u8868\u660eLLMs\u5373\u4f7f\u672a\u80fd\u7ed9\u51fa\u7cbe\u786e\u8bca\u65ad\uff0c\u4e5f\u5e38\u80fd\u8bc6\u522b\u66f4\u5e7f\u6cdb\u7684\u4e34\u5e8a\u80cc\u666f\u3002", "conclusion": "H-DDx\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u80fd\u529b\uff0c\u7ea0\u6b63\u4e86\u4f20\u7edf\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u7406\u89e3\u66f4\u5e7f\u6cdb\u4e34\u5e8a\u80cc\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04413", "pdf": "https://arxiv.org/pdf/2510.04413", "abs": "https://arxiv.org/abs/2510.04413", "authors": ["Muhammad Umar Farooq Qaisar", "Weijie Yuan", "Onur G\u00fcnl\u00fc", "Taneli Riihonen", "Yuanhao Cui", "Lin Zhang", "Nuria Gonzalez-Prelcic", "Marco Di Renzo", "Zhu Han"], "title": "The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems", "categories": ["eess.SP", "cs.NI"], "comment": "28 pages, 6 figures, and 5 tables", "summary": "The commencement of the sixth-generation (6G) wireless networks represents a\nfundamental shift in the integration of communication and sensing technologies\nto support next-generation applications. Integrated sensing and communication\n(ISAC) is a key concept in this evolution, enabling end-to-end support for both\ncommunication and sensing within a unified framework. It enhances spectrum\nefficiency, reduces latency, and supports diverse use cases, including smart\ncities, autonomous systems, and perceptive environments. This tutorial provides\na comprehensive overview of ISAC's role in 6G networks, beginning with its\nevolution since 5G and the technical drivers behind its adoption. Core\nprinciples and system variations of ISAC are introduced, followed by an\nin-depth discussion of the enabling technologies that facilitate its practical\ndeployment. The paper further analyzes current research directions to highlight\nkey challenges, open issues, and emerging trends. Design insights and\nrecommendations are also presented to support future development and\nimplementation. This work ultimately try to address three central questions:\nWhy is ISAC essential for 6G? What innovations does it bring? How will it shape\nthe future of wireless communication?", "AI": {"tldr": "\u672c\u6559\u7a0b\u5168\u9762\u6982\u8ff0\u4e86\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1 (ISAC) \u57286G\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u6f14\u8fdb\u3001\u6838\u5fc3\u539f\u7406\u3001\u4f7f\u80fd\u6280\u672f\u3001\u7814\u7a76\u6311\u6218\u4ee5\u53ca\u5bf9\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u7684\u5230\u6765\uff0c\u901a\u4fe1\u4e0e\u4f20\u611f\u7684\u878d\u5408\u6210\u4e3a\u652f\u6301\u4e0b\u4e00\u4ee3\u5e94\u7528\u7684\u57fa\u672c\u8f6c\u53d8\u3002ISAC\u662f\u5b9e\u73b0\u8fd9\u4e00\u878d\u5408\u7684\u5173\u952e\u6982\u5ff5\uff0c\u65e8\u5728\u63d0\u5347\u9891\u8c31\u6548\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u652f\u6301\u667a\u80fd\u57ce\u5e02\u3001\u81ea\u4e3b\u7cfb\u7edf\u7b49\u591a\u6837\u5316\u7528\u4f8b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u63a2\u8ba8\u5176\u57286G\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u672c\u6587\u91c7\u7528\u6559\u7a0b\u5f62\u5f0f\uff0c\u9996\u5148\u56de\u987e\u4e86ISAC\u81ea5G\u4ee5\u6765\u7684\u6f14\u8fdb\u548c\u6280\u672f\u9a71\u52a8\u56e0\u7d20\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86ISAC\u7684\u6838\u5fc3\u539f\u7406\u548c\u7cfb\u7edf\u53d8\u4f53\uff0c\u5e76\u6df1\u5165\u8ba8\u8bba\u4e86\u5176\u4f7f\u80fd\u6280\u672f\u3002\u6587\u7ae0\u8fd8\u5206\u6790\u4e86\u5f53\u524d\u7684\u7814\u7a76\u65b9\u5411\u3001\u5173\u952e\u6311\u6218\u3001\u5f00\u653e\u95ee\u9898\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u89c1\u89e3\u548c\u5efa\u8bae\u3002", "result": "\u672c\u6559\u7a0b\u63d0\u4f9b\u4e86ISAC\u57286G\u7f51\u7edc\u4e2d\u89d2\u8272\u7684\u5168\u9762\u6982\u8ff0\uff0c\u63ed\u793a\u4e86\u5176\u6280\u672f\u6f14\u8fdb\u3001\u6838\u5fc3\u539f\u5219\u548c\u5b9e\u73b0\u6280\u672f\u3002\u5b83\u660e\u786e\u4e86\u5f53\u524d\u7684\u7814\u7a76\u6311\u6218\u3001\u5f00\u653e\u95ee\u9898\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u548c\u5b9e\u65bd\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\u548c\u5efa\u8bae\uff0c\u65e8\u5728\u56de\u7b54ISAC\u4e3a\u4f55\u5bf96G\u81f3\u5173\u91cd\u8981\u3001\u5e26\u6765\u54ea\u4e9b\u521b\u65b0\u4ee5\u53ca\u5982\u4f55\u5851\u9020\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7684\u95ee\u9898\u3002", "conclusion": "ISAC\u662f6G\u7f51\u7edc\u4e0d\u53ef\u6216\u7f3a\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5b83\u901a\u8fc7\u96c6\u6210\u901a\u4fe1\u548c\u4f20\u611f\u529f\u80fd\u5e26\u6765\u663e\u8457\u521b\u65b0\uff0c\u5e76\u5c06\u5728\u6839\u672c\u4e0a\u5851\u9020\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7684\u9762\u8c8c\u548c\u80fd\u529b\u3002"}}
{"id": "2510.03553", "pdf": "https://arxiv.org/pdf/2510.03553", "abs": "https://arxiv.org/abs/2510.03553", "authors": ["Hasibur Rahman", "Hanan Salam"], "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) are increasingly implicated in\ninterpersonal and societal decision-making, their ability to navigate explicit\nconflicts between legitimately different cultural value systems remains largely\nunexamined. Existing benchmarks predominantly target cultural knowledge\n(CulturalBench), value prediction (WorldValuesBench), or single-axis bias\ndiagnostics (CDEval); none evaluate how LLMs adjudicate when multiple\nculturally grounded values directly clash. We address this gap with CCD-Bench,\na benchmark that assesses LLM decision-making under cross-cultural value\nconflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,\neach paired with ten anonymized response options corresponding to the ten GLOBE\ncultural clusters. These dilemmas are presented using a stratified Latin square\nto mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models\ndisproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe\n(12.4 percent), while options for Eastern Europe and the Middle East and North\nAfrica are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of\nrationales reference multiple GLOBE dimensions, this pluralism is superficial:\nmodels recombine Future Orientation and Performance Orientation, and rarely\nground choices in Assertiveness or Gender Egalitarianism (both under 3\npercent). Ordering effects are negligible (Cramer's V less than 0.10), and\nsymmetrized KL divergence shows clustering by developer lineage rather than\ngeography. These patterns suggest that current alignment pipelines promote a\nconsensus-oriented worldview that underserves scenarios demanding power\nnegotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts\nevaluation beyond isolated bias detection toward pluralistic decision making\nand highlights the need for alignment strategies that substantively engage\ndiverse worldviews.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8de8\u6587\u5316\u4ef7\u503c\u51b2\u7a81\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u503e\u5411\u4e8e\u504f\u7231\u5317\u6b27\u548c\u65e5\u8033\u66fc\u6b27\u6d32\u4ef7\u503c\u89c2\uff0c\u4e14\u5176\u591a\u5143\u4e3b\u4e49\u662f\u80a4\u6d45\u7684\u3002\u8fd9\u8868\u660e\u5f53\u524d\u7684\u5bf9\u9f50\u7b56\u7565\u672a\u80fd\u5145\u5206\u8003\u8651\u591a\u5143\u7684\u4e16\u754c\u89c2\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51b3\u7b56\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u5728\u5904\u7406\u5408\u6cd5\u6587\u5316\u4ef7\u503c\u7cfb\u7edf\u4e4b\u95f4\u663e\u6027\u51b2\u7a81\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6587\u5316\u77e5\u8bc6\u3001\u4ef7\u503c\u9884\u6d4b\u6216\u5355\u8f74\u504f\u89c1\u8bca\u65ad\uff0c\u7f3a\u4e4f\u8bc4\u4f30LLMs\u5982\u4f55\u88c1\u51b3\u591a\u79cd\u6587\u5316\u4ef7\u503c\u76f4\u63a5\u51b2\u7a81\u7684\u573a\u666f\u3002", "method": "\u5f15\u5165CCD-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u8de8\u6587\u5316\u4ef7\u503c\u51b2\u7a81\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b2182\u4e2a\u6db5\u76d6\u4e03\u4e2a\u9886\u57df\u7684\u5f00\u653e\u5f0f\u56f0\u5883\uff0c\u6bcf\u4e2a\u56f0\u5883\u914d\u6709\u5341\u4e2a\u533f\u540d\u54cd\u5e94\u9009\u9879\uff0c\u5bf9\u5e94\u5341\u4e2aGLOBE\u6587\u5316\u96c6\u7fa4\u3002\u91c7\u7528\u5206\u5c42\u62c9\u4e01\u65b9\u8bbe\u8ba1\u4ee5\u51cf\u8f7b\u6392\u5e8f\u6548\u5e94\u3002\u5171\u8bc4\u4f30\u4e8617\u4e2a\u975e\u63a8\u7406LLM\u3002", "result": "\u6a21\u578b\u8fc7\u5ea6\u504f\u597d\u5317\u6b27\uff08\u5e73\u574720.2%\uff09\u548c\u65e5\u8033\u66fc\u6b27\u6d32\uff0812.4%\uff09\u7684\u9009\u9879\uff0c\u800c\u4e1c\u6b27\u548c\u4e2d\u4e1c\u53ca\u5317\u975e\u7684\u9009\u9879\u5219\u4ee3\u8868\u6027\u4e0d\u8db3\uff085.6%\u81f35.8%\uff09\u3002\u5c3d\u7ba187.9%\u7684\u7406\u7531\u5f15\u7528\u4e86\u591a\u4e2aGLOBE\u7ef4\u5ea6\uff0c\u4f46\u8fd9\u79cd\u591a\u5143\u4e3b\u4e49\u662f\u80a4\u6d45\u7684\uff0c\u6a21\u578b\u4e3b\u8981\u91cd\u7ec4\u201c\u672a\u6765\u5bfc\u5411\u201d\u548c\u201c\u7ee9\u6548\u5bfc\u5411\u201d\uff0c\u5f88\u5c11\u57fa\u4e8e\u201c\u6b66\u65ad\u6027\u201d\u6216\u201c\u6027\u522b\u5e73\u7b49\u4e3b\u4e49\u201d\u505a\u51fa\u9009\u62e9\uff08\u5747\u4f4e\u4e8e3%\uff09\u3002\u6392\u5e8f\u6548\u5e94\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u6a21\u578b\u805a\u7c7b\u66f4\u591a\u662f\u57fa\u4e8e\u5f00\u53d1\u8005\u8c31\u7cfb\u800c\u975e\u5730\u7406\u533a\u57df\u3002", "conclusion": "\u5f53\u524dLLM\u7684\u5bf9\u9f50\u6d41\u7a0b\u53ef\u80fd\u4fc3\u8fdb\u4e86\u4e00\u79cd\u4ee5\u5171\u8bc6\u4e3a\u5bfc\u5411\u7684\u4e16\u754c\u89c2\uff0c\u672a\u80fd\u5145\u5206\u5904\u7406\u9700\u8981\u6743\u529b\u8c08\u5224\u3001\u57fa\u4e8e\u6743\u5229\u7684\u63a8\u7406\u6216\u6027\u522b\u610f\u8bc6\u5206\u6790\u7684\u573a\u666f\u3002CCD-Bench\u5c06\u8bc4\u4f30\u4ece\u5b64\u7acb\u7684\u504f\u89c1\u68c0\u6d4b\u8f6c\u5411\u591a\u5143\u51b3\u7b56\uff0c\u5f3a\u8c03\u9700\u8981\u5b9e\u8d28\u6027\u5730\u878d\u5408\u4e0d\u540c\u4e16\u754c\u89c2\u7684\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2510.03353", "pdf": "https://arxiv.org/pdf/2510.03353", "abs": "https://arxiv.org/abs/2510.03353", "authors": ["Larissa S. Gomes", "Gustavo P. Almeida", "Bryan U. Moreira", "Marco Quiroz", "Breno Xavier", "Lucas Soares", "Stephanie L. Bri\u00e3o", "Felipe G. Oliveira", "Paulo L. J. Drews-Jr"], "title": "Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications", "categories": ["cs.CV", "I.4.9; I.5.0; H.3.1; I.2.6"], "comment": "Published in the Conference on Graphics, Patterns and Images\n  (SIBGRAPI). This 4-page paper presents a timeline of publicly available\n  datasets up to the year 2025", "summary": "Sonar images are relevant for advancing underwater exploration, autonomous\nnavigation, and ecosystem monitoring. However, the progress depends on data\navailability. The scarcity of publicly available, well-annotated sonar image\ndatasets creates a significant bottleneck for the development of robust machine\nlearning models. This paper presents a comprehensive and concise review of the\ncurrent landscape of sonar image datasets, seeking not only to catalog existing\nresources but also to contextualize them, identify gaps, and provide a clear\nroadmap, serving as a base guide for researchers of any kind who wish to start\nor advance in the field of underwater acoustic data analysis. We mapped\npublicly accessible datasets across various sonar modalities, including Side\nScan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),\nMultibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar\n(DIDSON). An analysis was conducted on applications such as classification,\ndetection, segmentation, and 3D reconstruction. This work focuses on\nstate-of-the-art advancements, incorporating newly released datasets. The\nfindings are synthesized into a master table and a chronological timeline,\noffering a clear and accessible comparison of characteristics, sizes, and\nannotation details datasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u73b0\u6709\u58f0\u7eb3\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u516c\u5f00\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\uff0c\u4e3a\u6c34\u4e0b\u58f0\u5b66\u6570\u636e\u5206\u6790\u63d0\u4f9b\u5168\u9762\u7684\u8d44\u6e90\u76ee\u5f55\u548c\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "motivation": "\u516c\u5f00\u53ef\u7528\u7684\u3001\u6807\u6ce8\u826f\u597d\u7684\u58f0\u7eb3\u56fe\u50cf\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u6c34\u4e0b\u63a2\u7d22\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u9886\u57df\u4e2d\u7a33\u5065\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5bf9\u73b0\u6709\u58f0\u7eb3\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u68b3\u7406\u5e76\u5206\u7c7b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u591a\u6a21\u6001\u58f0\u7eb3\u6570\u636e\u96c6\uff08\u5305\u62ecSSS, FLS, SAS, MBES, DIDSON\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u548c\u4e09\u7ef4\u91cd\u5efa\u7b49\u5e94\u7528\u4e2d\u7684\u60c5\u51b5\u3002\u5de5\u4f5c\u7efc\u5408\u4e86\u6700\u65b0\u53d1\u5e03\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u7ed3\u679c\u6574\u7406\u6210\u4e3b\u8868\u548c\u65f6\u95f4\u7ebf\u3002", "result": "\u7ed8\u5236\u4e86\u8de8\u591a\u79cd\u58f0\u7eb3\u6a21\u6001\u7684\u516c\u5f00\u6570\u636e\u96c6\u56fe\u8c31\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u7ed3\u679c\u88ab\u6574\u5408\u4e3a\u4e00\u5f20\u4e3b\u8868\u548c\u65f6\u95f4\u8f74\uff0c\u6e05\u6670\u5730\u6bd4\u8f83\u4e86\u6570\u636e\u96c6\u7684\u7279\u5f81\u3001\u5927\u5c0f\u548c\u6807\u6ce8\u7ec6\u8282\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u6c34\u4e0b\u58f0\u5b66\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u5165\u95e8\u548c\u8fdb\u9636\u6307\u5357\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63a8\u52a8\u6c34\u4e0b\u63a2\u7d22\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.03259", "pdf": "https://arxiv.org/pdf/2510.03259", "abs": "https://arxiv.org/abs/2510.03259", "authors": ["Yoonjeon Kim", "Doohyuk Jang", "Eunho Yang"], "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMASA\uff08Meta-Awareness via Self-Alignment\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u4fe1\u53f7\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u57df\u5185\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u5143\u8ba4\u77e5\uff08\u5373\u201c\u77e5\u9053\u5982\u4f55\u601d\u8003\u201d\uff09\u80fd\u529b\uff0c\u8868\u73b0\u4e3a\u771f\u5b9e\u6267\u884c\u8def\u5f84\u4e0e\u9884\u6d4b\u5143\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u9519\u4f4d\u3002\u4f5c\u8005\u5047\u8bbe\uff0c\u5bf9\u9f50\u5143\u9884\u6d4b\u4e0e\u771f\u5b9e\u6267\u884c\u8def\u5f84\u5c06\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u5957\u540d\u4e3aMASA\uff08Meta-Awareness via Self-Alignment\uff09\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5916\u90e8\u8bad\u7ec3\u6e90\uff0c\u800c\u662f\u5229\u7528\u6a21\u578b\u81ea\u751f\u6210\u7684\u4fe1\u53f7\u6765\u8bad\u7ec3\u5143\u8ba4\u77e5\u3002\u4e3a\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0cMASA\u4f1a\u8fc7\u6ee4\u6389\u7410\u788e\u6216\u65e0\u6cd5\u89e3\u51b3\u7684\u96f6\u65b9\u5dee\u63d0\u793a\uff0c\u5e76\u622a\u65ad\u4e0d\u592a\u53ef\u80fd\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u7684\u5197\u957f\u6267\u884c\u8def\u5f84\u3002", "result": "MASA\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u57df\u5185\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002\u5177\u4f53\u8868\u73b0\u4e3a\uff1aGRPO\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u8d85\u8fc71.28\u500d\uff1bAIME25\u51c6\u786e\u7387\u63d0\u9ad819.3%\uff1b\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad86.2%\uff1bGPQA-Diamond\u57df\u5916\u6cdb\u5316\u80fd\u529b\u63d0\u53473.87%\uff1b\u8de8\u903b\u8f91\u3001\u79d1\u5b66\u548c\u7f16\u7a0b\u7b4913\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6574\u4f53\u51c6\u786e\u7387\u63d0\u53472.08%\u3002", "conclusion": "\u589e\u5f3a\u6a21\u578b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u53ef\u4ee5\u76f4\u63a5\u8f6c\u5316\u4e3a\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002\u901a\u8fc7MASA\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u5143\u8ba4\u77e5\u6307\u5bfc\u8fd8\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03727", "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u589e\u5f3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MFMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u548c\u5f15\u5165\u7ed3\u6784\u5316\u3001\u53ef\u63a7\u7684\u751f\u6210\u6846\u67b6\uff0c\u5f25\u5408MFMs\u4e0e\u6709\u6548\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MFMs\uff09\u7f3a\u4e4f\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u52a8\u529b\u5b66\u6a21\u62df\u3001\u65f6\u7a7a\u7406\u89e3\u3001\u751f\u6210\u7ed3\u679c\u63a7\u5236\u53ca\u591a\u65b9\u9762\u63a8\u7406\u7b49\u5173\u952e\u80fd\u529b\uff0c\u4f7f\u5176\u65e0\u6cd5\u4f5c\u4e3a\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u7684\u591a\u611f\u5b98\u4e16\u754c\u7406\u89e3\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "1. \u901a\u8fc7\u5224\u522b\u6027\u4efb\u52a1\u548c\u8d4b\u4e88\u7ed3\u6784\u5316\u63a8\u7406\u6280\u80fd\uff08\u5982\u56e0\u679c\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u601d\u7ef4\u3001\u65f6\u7a7a\u63a8\u7406\uff09\uff0c\u63d0\u5347MFMs\u7684\u63a8\u7406\u80fd\u529b\u30022. \u63a2\u7d22MFMs\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u6001\u4e0a\u7684\u751f\u6210\u80fd\u529b\uff0c\u5f15\u5165\u5229\u7528\u573a\u666f\u56fe\u3001\u591a\u6a21\u6001\u6761\u4ef6\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u548c\u53ef\u63a7\u751f\u6210\u30023. \u5c06\u6280\u672f\u6269\u5c55\u81f3\u53ef\u63a74D\u751f\u6210\uff0c\u4ee5\u5b9e\u73b0\u4ea4\u4e92\u3001\u53ef\u7f16\u8f91\u548c\u53ef\u53d8\u5f62\u7684\u5bf9\u8c61\u65f6\u7a7a\u5408\u6210\u3002", "result": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u589e\u5f3aMFMs\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u56e0\u679c\u3001\u53cd\u4e8b\u5b9e\u548c\u65f6\u7a7a\u63a8\u7406\uff1b\u5e76\u63a8\u52a8\u5176\u5728\u56fe\u50cf\u3001\u89c6\u9891\u548c4D\u6a21\u6001\u4e0a\u5b9e\u73b0\u7ed3\u6784\u5316\u548c\u53ef\u63a7\u7684\u751f\u6210\uff0c\u4ee5\u786e\u4fdd\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u548c\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\uff0c\u901a\u8fc7\u5168\u9762\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u5f00\u53d1\u5148\u8fdb\u7684\u7ed3\u6784\u5316\u3001\u53ef\u63a7\u751f\u6210\u6280\u672f\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u63a5\u8fd1\u4e16\u754c\u6a21\u578b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u6a21\u62df\u52a8\u6001\u7269\u7406\u8fc7\u7a0b\uff0c\u5e76\u751f\u6210\u7b26\u5408\u590d\u6742\u610f\u56fe\u7684\u89c6\u89c9\u5185\u5bb9\u3002"}}
{"id": "2510.04619", "pdf": "https://arxiv.org/pdf/2510.04619", "abs": "https://arxiv.org/abs/2510.04619", "authors": ["Ivan Homoliak", "Martin Pere\u0161\u00edni", "Marek Tama\u0161kovi\u010d", "Timotej Ponek", "Luk\u00e1\u0161 Hellebrandt", "Kamil Malinka"], "title": "PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "Proof-of-Stake (PoS) consensus protocols often face a trade-off between\nperformance and security. Protocols that pre-elect leaders for subsequent\nrounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the\nnetwork and compromise liveness. In this work, we present PoS-CoPOR, a\nsingle-chain PoS consensus protocol that mitigates this vulnerability by\nintegrating a native onion routing mechanism into the consensus protocol\nitself. PoS-CoPOR combines stake-weighted probabilistic leader election with an\nanonymization layer that conceals the network identity of the next block\nproposer. This approach prevents targeted DoS attacks on leaders before they\nproduce a block, thus enhancing network resilience. We implemented and\nevaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to\n110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The\nresults show that native anonymization can provide robust DoS resistance with\nonly a modest impact on performance, offering a solution to build secure and\nscalable PoS blockchains.", "AI": {"tldr": "PoS-CoPOR\u534f\u8bae\u901a\u8fc7\u96c6\u6210\u539f\u751f\u6d0b\u8471\u8def\u7531\u673a\u5236\uff0c\u89e3\u51b3\u4e86PoS\u534f\u8bae\u4e2d\u9884\u9009\u4e3e\u9886\u5bfc\u8005\u6613\u53d7DoS\u653b\u51fb\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\u548c\u5f3a\u5927\u7684\u6297DoS\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684PoS\u5171\u8bc6\u534f\u8bae\u5e38\u9762\u4e34\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7279\u522b\u662f\u9884\u9009\u4e3e\u9886\u5bfc\u8005\u7684\u673a\u5236\u5bb9\u6613\u906d\u53d7\u62d2\u7edd\u670d\u52a1\uff08DoS\uff09\u653b\u51fb\uff0c\u5bfc\u81f4\u7f51\u7edc\u4e2d\u65ad\u548c\u6d3b\u6027\u53d7\u635f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PoS-CoPOR\uff0c\u4e00\u4e2a\u5355\u94fePoS\u5171\u8bc6\u534f\u8bae\u3002\u5b83\u5c06\u6743\u76ca\u52a0\u6743\u7684\u6982\u7387\u9886\u5bfc\u8005\u9009\u4e3e\u4e0e\u539f\u751f\u7684\u6d0b\u8471\u8def\u7531\u533f\u540d\u5316\u5c42\u76f8\u7ed3\u5408\uff0c\u4ee5\u9690\u85cf\u4e0b\u4e00\u533a\u5757\u63d0\u8bae\u8005\u7684\u7f51\u7edc\u8eab\u4efd\uff0c\u4ece\u800c\u9632\u6b62\u5bf9\u9886\u5bfc\u8005\u7684\u76ee\u6807\u6027DoS\u653b\u51fb\u3002", "result": "PoS-CoPOR\u57286\u4e2a\u8282\u70b9\u73af\u5883\u4e0b\uff0c\u5373\u4f7f\u6709\u533f\u540d\u5316\u5c42\u7684\u5f00\u9500\uff0c\u4e5f\u80fd\u8fbe\u5230110 tx/s\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u539f\u751f\u533f\u540d\u5316\u53ef\u4ee5\u63d0\u4f9b\u5f3a\u5927\u7684DoS\u62b5\u6297\u80fd\u529b\uff0c\u540c\u65f6\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5c0f\uff0c\u4e3a\u6784\u5efa\u5b89\u5168\u4e14\u53ef\u6269\u5c55\u7684PoS\u533a\u5757\u94fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03561", "pdf": "https://arxiv.org/pdf/2510.03561", "abs": "https://arxiv.org/abs/2510.03561", "authors": ["Adam Filipek"], "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 13 figures", "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity ($O(L^2)$) with respect to sequence length $L$.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to\nthe number of interactions $N$. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.03356", "pdf": "https://arxiv.org/pdf/2510.03356", "abs": "https://arxiv.org/abs/2510.03356", "authors": ["Ziyang Chen", "Yuta Itoh", "Kaan Ak\u015fit"], "title": "Learned Display Radiance Fields with Lensless Cameras", "categories": ["cs.CV", "cs.ET"], "comment": null, "summary": "Calibrating displays is a basic and regular task that content creators must\nperform to maintain optimal visual experience, yet it remains a troublesome\nissue. Measuring display characteristics from different viewpoints often\nrequires specialized equipment and a dark room, making it inaccessible to most\nusers. To avoid specialized hardware requirements in display calibrations, our\nwork co-designs a lensless camera and an Implicit Neural Representation based\nalgorithm for capturing display characteristics from various viewpoints. More\nspecifically, our pipeline enables efficient reconstruction of light fields\nemitted from a display from a viewing cone of 46.6{\\deg} X 37.6{\\deg}. Our\nemerging pipeline paves the initial steps towards effortless display\ncalibration and characterization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u65e0\u900f\u955c\u76f8\u673a\u548c\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4e13\u4e1a\u786c\u4ef6\u5373\u53ef\u4ece\u591a\u89c6\u89d2\u6355\u83b7\u663e\u793a\u5668\u7279\u6027\uff0c\u4ee5\u7b80\u5316\u663e\u793a\u5668\u6821\u51c6\u4efb\u52a1\u3002", "motivation": "\u663e\u793a\u5668\u6821\u51c6\u5bf9\u5185\u5bb9\u521b\u4f5c\u8005\u800c\u8a00\u662f\u57fa\u672c\u4f46\u9ebb\u70e6\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u6d4b\u91cf\u663e\u793a\u5668\u591a\u89c6\u89d2\u7279\u6027\u9700\u4e13\u4e1a\u8bbe\u5907\u548c\u6697\u5ba4\uff0c\u5bfc\u81f4\u5927\u591a\u6570\u7528\u6237\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5171\u540c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u900f\u955c\u76f8\u673a\u548c\u4e00\u4e2a\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e0d\u540c\u89c6\u89d2\u6355\u83b7\u663e\u793a\u5668\u7279\u6027\u3002", "result": "\u8be5\u7ba1\u7ebf\u80fd\u591f\u9ad8\u6548\u91cd\u5efa\u663e\u793a\u5668\u53d1\u51fa\u7684\u5149\u573a\uff0c\u8986\u76d646.6\u00b0 x 37.6\u00b0\u7684\u89c2\u770b\u9525\u89d2\u3002", "conclusion": "\u8be5\u65b0\u5174\u7ba1\u7ebf\u4e3a\u5b9e\u73b0\u8f7b\u677e\u7684\u663e\u793a\u5668\u6821\u51c6\u548c\u7279\u6027\u63cf\u8ff0\u5960\u5b9a\u4e86\u521d\u6b65\u57fa\u7840\uff0c\u907f\u514d\u4e86\u5bf9\u4e13\u4e1a\u786c\u4ef6\u7684\u9700\u6c42\u3002"}}
{"id": "2510.03260", "pdf": "https://arxiv.org/pdf/2510.03260", "abs": "https://arxiv.org/abs/2510.03260", "authors": ["Juan Jose Herrera-Aranda", "Guillermo Gomez-Trenado", "Francisco Herrera", "Isaac Triguero"], "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 9 figures, code available at\n  https://kiedie.github.io/Semantic-Inductive-Attribute-Selection-for-Zero-Shot-Learning/", "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial\nIntelligence Systems, particularly in those that operate in open-world\nscenarios where systems must adapt to new tasks dynamically. Semantic spaces\nplay a pivotal role as they bridge seen and unseen classes, but whether\nhuman-annotated or generated by a machine learning model, they often contain\nnoisy, redundant, or irrelevant attributes that hinder performance. To address\nthis, we introduce a partitioning scheme that simulates unseen conditions in an\ninductive setting (which is the most challenging), allowing attribute relevance\nto be assessed without access to semantic information from unseen classes.\nWithin this framework, we study two complementary feature-selection strategies\nand assess their generalisation. The first adapts embedded feature selection to\nthe particular demands of ZSL, turning model-driven rankings into meaningful\nsemantic pruning; the second leverages evolutionary computation to directly\nexplore the space of attribute subsets more broadly. Experiments on five\nbenchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods\nconsistently improve accuracy on unseen classes by reducing redundancy, but in\ncomplementary ways: RFS is efficient and competitive though dependent on\ncritical hyperparameters, whereas GA is more costly yet explores the search\nspace more broadly and avoids such dependence. These results confirm that\nsemantic spaces are inherently redundant and highlight the proposed\npartitioning scheme as an effective tool to refine them under inductive\nconditions.", "AI": {"tldr": "\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\u5e38\u542b\u5197\u4f59\u5c5e\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5206\u533a\u65b9\u6848\u548c\u4e24\u79cd\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff08RFS\u548cGA\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f52\u7eb3\u5f0fZSL\u5728\u672a\u89c1\u7c7b\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u96f6\u6837\u672c\u5b66\u4e60\uff08ZSL\uff09\u662f\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u8303\u5f0f\uff0c\u5c24\u5176\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u9700\u52a8\u6001\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u8bed\u4e49\u7a7a\u95f4\u5728\u8fde\u63a5\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5305\u542b\u566a\u58f0\u3001\u5197\u4f59\u6216\u4e0d\u76f8\u5173\u5c5e\u6027\uff0c\u4ece\u800c\u963b\u788d\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5206\u533a\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u5f52\u7eb3\u5f0f\u8bbe\u7f6e\u4e2d\u6a21\u62df\u672a\u89c1\u6761\u4ef6\uff0c\u65e0\u9700\u8bbf\u95ee\u672a\u89c1\u7c7b\u7684\u8bed\u4e49\u4fe1\u606f\u5373\u53ef\u8bc4\u4f30\u5c5e\u6027\u76f8\u5173\u6027\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff1a1. \u81ea\u9002\u5e94\u7684\u5d4c\u5165\u5f0f\u7279\u5f81\u9009\u62e9\uff08RFS\uff09\uff0c\u5c06\u6a21\u578b\u9a71\u52a8\u7684\u6392\u540d\u8f6c\u5316\u4e3a\u6709\u610f\u4e49\u7684\u8bed\u4e49\u526a\u679d\uff1b2. \u8fdb\u5316\u8ba1\u7b97\uff08GA\uff09\uff0c\u76f4\u63a5\u66f4\u5e7f\u6cdb\u5730\u63a2\u7d22\u5c5e\u6027\u5b50\u96c6\u7a7a\u95f4\u3002", "result": "\u5728AWA2\u3001CUB\u3001SUN\u3001aPY\u3001FLO\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\uff0c\u6301\u7eed\u63d0\u9ad8\u672a\u89c1\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4e14\u65b9\u5f0f\u4e92\u8865\uff1aRFS\u9ad8\u6548\u4e14\u5177\u7ade\u4e89\u529b\u4f46\u4f9d\u8d56\u5173\u952e\u8d85\u53c2\u6570\uff0c\u800cGA\u6210\u672c\u66f4\u9ad8\u4f46\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u66f4\u5e7f\u4e14\u907f\u514d\u4e86\u8d85\u53c2\u6570\u4f9d\u8d56\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u8bed\u4e49\u7a7a\u95f4\u672c\u8d28\u4e0a\u662f\u5197\u4f59\u7684\uff0c\u5e76\u5f3a\u8c03\u6240\u63d0\u51fa\u7684\u5206\u533a\u65b9\u6848\u662f\u5728\u5f52\u7eb3\u6761\u4ef6\u4e0b\u7cbe\u70bc\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.03771", "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "categories": ["cs.AI"], "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOptAgent\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u5546\u67e5\u8be2\u91cd\u5199\u3002\u901a\u8fc7LLM\u4ee3\u7406\u4f5c\u4e3a\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\uff0cOptAgent\u80fd\u6709\u6548\u63d0\u5347\u67e5\u8be2\u8d28\u91cf\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "LLM\u7cfb\u7edf\u5728\u4e3b\u89c2\u4efb\u52a1\uff08\u5982\u7535\u5546\u67e5\u8be2\u91cd\u5199\uff09\u4e2d\u7684\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u5355\u4e00\u6b63\u786e\u7b54\u6848\u4e14\u96be\u4ee5\u7b97\u6cd5\u5316\u5224\u65ad\u7528\u6237\u610f\u56fe\u3002", "method": "OptAgent\u6846\u67b6\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u3002\u5b83\u4e0d\u4f9d\u8d56\u9759\u6001\u5956\u52b1\u6a21\u578b\u6216\u5355\u4e00LLM\u8bc4\u5224\uff0c\u800c\u662f\u4f7f\u7528\u591a\u4e2aLLM\u4ee3\u7406\uff08\u6a21\u62df\u8d2d\u7269\u987e\u5ba2\uff09\u4f5c\u4e3a\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u4e9b\u4ee3\u7406\u7684\u5e73\u5747\u5f97\u5206\u4f5c\u4e3a\u8fdb\u5316\u7b97\u6cd5\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u8fed\u4ee3\u4f18\u5316\u7528\u6237\u521d\u59cb\u67e5\u8be2\u3002", "result": "OptAgent\u57281000\u4e2a\u771f\u5b9e\u7535\u5546\u67e5\u8be2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u5bf9\u4e8e\u539f\u59cb\u7528\u6237\u67e5\u8be2\uff0c\u5e73\u5747\u63d0\u5347\u4e8621.98%\uff1b\u76f8\u5bf9\u4e8e\u201c\u6700\u4f73N\u4e2aLLM\u91cd\u5199\u201d\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u5347\u4e863.36%\u3002", "conclusion": "OptAgent\u901a\u8fc7\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u67e5\u8be2\u91cd\u5199\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u96be\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u90e8\u7f72\u7528\u6237\u5bf9\u9f50\u7684LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2510.04674", "pdf": "https://arxiv.org/pdf/2510.04674", "abs": "https://arxiv.org/abs/2510.04674", "authors": ["Lorenzo Pannacci", "Simone Fiorellino", "Mario Edoardo Pandolfo", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": "Proceedings of IEEE Globecom 2025 Workshops", "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks.", "AI": {"tldr": "\u73b0\u6709DeepJSCC\u5728\u591a\u5382\u5546\u573a\u666f\u4e2d\u56e0\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5339\u914d\u800c\u5931\u6548\uff0c\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u4fe1\u9053\u5747\u8861\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u3001\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6216\u96f6\u6837\u672c\u5747\u8861\u5668\u5bf9\u9f50\u5f02\u6784\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u56fe\u50cf\u91cd\u5efa\u4e2d\u7684\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u5f02\u6784AI\u65e0\u7ebf\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709DeepJSCC\u65b9\u6848\u5047\u8bbe\u6536\u53d1\u7aef\u5b58\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u5e76\u53ef\u534f\u540c\u8bad\u7ec3\uff0c\u8fd9\u5728\u591a\u5382\u5546\u90e8\u7f72\u4e2d\u4e0d\u9002\u7528\uff0c\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5339\u914d\u5e76\u5f15\u5165\u201c\u8bed\u4e49\u566a\u58f0\u201d\uff0c\u4ece\u800c\u964d\u4f4e\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u4fe1\u9053\u5747\u8861\u65b9\u6cd5\uff0c\u589e\u52a0\u4e00\u4e2a\u5904\u7406\u9636\u6bb5\u4ee5\u5bf9\u9f50\u7269\u7406\u548c\u8bed\u4e49\u7f3a\u9677\u4e0b\u7684\u5f02\u6784\u6f5c\u5728\u7a7a\u95f4\u3002\u7814\u7a76\u4e86\u4e09\u7c7b\u5bf9\u9f50\u5668\uff1a(i) \u7ebf\u6027\u6620\u5c04\uff1b(ii) \u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff1b(iii) \u96f6\u6837\u672cParseval-frame\u5747\u8861\u5668\u3002\u901a\u8fc7AWGN\u548c\u8870\u843d\u4fe1\u9053\u4e0a\u7684\u56fe\u50cf\u91cd\u5efa\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u91cf\u5316\u4e86\u4e0d\u540c\u5bf9\u9f50\u5668\u5728\u590d\u6742\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u4e3a\u5728\u5f02\u6784AI\u539f\u751f\u65e0\u7ebf\u7f51\u7edc\u4e2d\u90e8\u7f72DeepJSCC\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2510.03577", "pdf": "https://arxiv.org/pdf/2510.03577", "abs": "https://arxiv.org/abs/2510.03577", "authors": ["Ikram Belmadani", "Parisa Nazari Hashemi", "Thomas Sebbag", "Benoit Favre", "Guillaume Fortier", "Solen Quiniou", "Emmanuel Morin", "Richard Dufour"], "title": "LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction", "categories": ["cs.CL", "cs.IR"], "comment": "in French language", "summary": "This work presents our participation in the EvalLLM 2025 challenge on\nbiomedical Named Entity Recognition (NER) and health event extraction in French\n(few-shot setting). For NER, we propose three approaches combining large\nlanguage models (LLMs), annotation guidelines, synthetic data, and\npost-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating\nautomatic selection of 10 examples and a summary of the annotation guidelines\ninto the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic\ncorpus and then verified by an LLM in post-processing, and (3) the open LLM\nLLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event\nextraction uses the same ICL strategy with GPT-4.1, reusing the guideline\nsummary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for\nNER and 15.02% for event extraction, highlighting the importance of\nwell-crafted prompting to maximize performance in very low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4f5c\u8005\u5728EvalLLM 2025\u6311\u6218\u8d5b\uff08\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5065\u5eb7\u4e8b\u4ef6\u63d0\u53d6\uff0c\u5c11\u6837\u672c\u8bbe\u7f6e\uff09\u4e2d\u7684\u53c2\u4e0e\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u6307\u4ee4\u548c\u5408\u6210\u6570\u636e\u7684\u4e09\u79cdNER\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7528\u4e8e\u4e8b\u4ef6\u63d0\u53d6\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u3002GPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u5f3a\u8c03\u4e86\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u53c2\u4e0eEvalLLM 2025\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u89e3\u51b3\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5065\u5eb7\u4e8b\u4ef6\u63d0\u53d6\u5728\u5c11\u6837\u672c\uff08few-shot\uff09\u8bbe\u7f6e\u4e0b\u7684\u6311\u6218\u3002", "method": "\u5bf9\u4e8eNER\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a1) \u4f7f\u7528GPT-4.1\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\uff0c\u5305\u542b10\u4e2a\u81ea\u52a8\u9009\u62e9\u7684\u793a\u4f8b\u548c\u6ce8\u91ca\u6307\u5357\u6458\u8981\uff1b2) \u901a\u7528NER\u7cfb\u7edfGLiNER\uff0c\u5728\u5408\u6210\u8bed\u6599\u5e93\u4e0a\u5fae\u8c03\u5e76\u7ecfLLM\u540e\u5904\u7406\u9a8c\u8bc1\uff1b3) \u5f00\u6e90LLM LLaMA-3.1-8B-Instruct\uff0c\u5728\u540c\u4e00\u5408\u6210\u8bed\u6599\u5e93\u4e0a\u5fae\u8c03\u3002\u4e8b\u4ef6\u63d0\u53d6\u5219\u91c7\u7528\u4e0eGPT-4.1\u76f8\u540c\u7684ICL\u7b56\u7565\uff0c\u5e76\u91cd\u7528\u6307\u5357\u6458\u8981\u3002", "result": "GPT-4.1\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u597d\u3002\u5728NER\u4efb\u52a1\u4e2d\uff0cGPT-4.1\u8fbe\u5230\u4e8661.53%\u7684\u5b8fF1\u5206\u6570\uff1b\u5728\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e2d\uff0cGPT-4.1\u8fbe\u5230\u4e8615.02%\u7684\u5b8fF1\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cGPT-4.1\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6781\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff08prompt\uff09\u5bf9\u4e8e\u6700\u5927\u5316\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03361", "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6eaf\u6e90\u7f51\u7edc\u201d\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u7ed3\u679c\u76f4\u63a5\u5173\u8054\u5230\u8bad\u7ec3\u6837\u672c\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u3001\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u201c\u9ed1\u7bb1\u201d\u95ee\u9898\uff0c\u5982\u4e0d\u900f\u660e\u6027\u3001\u5e7b\u89c9\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u5185\u5d4c\u4e8e\u6a21\u578b\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u7684\u900f\u660e\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5e76\u80fd\u5c06\u529f\u52b3\u5f52\u56e0\u4e8e\u6570\u636e\u8d21\u732e\u8005\u3002", "method": "\u5f15\u5165\u6eaf\u6e90\u7f51\u7edc\uff08provenance networks\uff09\uff0c\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u6b63\u5e38\u64cd\u4f5c\u4e2d\uff0c\u5c06\u6bcf\u4e2a\u9884\u6d4b\u76f4\u63a5\u94fe\u63a5\u5230\u652f\u6301\u5b83\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u5d4c\u5165\u67b6\u6784\u4e2d\u3002\u5176\u5de5\u4f5c\u539f\u7406\u7c7b\u4f3c\u4e8e\u5b66\u4e60\u578bKNN\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u4e2d\u52a0\u6743\u7684\u5177\u4f53\u5b9e\u4f8b\u6765\u89e3\u91ca\u6bcf\u4e2a\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4e4b\u95f4\u6743\u8861\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff1b\u80fd\u9a8c\u8bc1\u8f93\u5165\u662f\u5426\u5305\u542b\u5728\u8bad\u7ec3\u96c6\u4e2d\uff1b\u6709\u52a9\u4e8e\u68c0\u6d4b\u9519\u8bef\u6807\u8bb0\u6216\u5f02\u5e38\u6570\u636e\u70b9\uff1b\u589e\u5f3a\u4e86\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u5f39\u6027\uff1b\u5e76\u652f\u6301\u8bc6\u522b\u5bf9\u65b0\u6570\u636e\u70b9\u751f\u6210\u6709\u8d21\u732e\u7684\u76f8\u4f3c\u8f93\u5165\u3002\u5b83\u63d0\u4f9b\u4e86\u4f20\u7edf\u6df1\u5ea6\u7f51\u7edc\u65e0\u6cd5\u63d0\u4f9b\u7684\u6a21\u578b\u884c\u4e3a\u6d1e\u5bdf\u3002", "conclusion": "\u6eaf\u6e90\u7f51\u7edc\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u4efb\u52a1\u548c\u53ef\u89e3\u91ca\u6027\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u6a21\u578b\u4e0d\u900f\u660e\u6027\u3001\u5e7b\u89c9\u548c\u6570\u636e\u8d21\u732e\u8005\u5f52\u56e0\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u900f\u660e\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u5c3d\u7ba1\u6709\u8ba1\u7b97\u6210\u672c\u548c\u6570\u636e\u96c6\u89c4\u6a21\u9650\u5236\uff0c\u4f46\u5b83\u4e3a\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u76ca\u7684\u8865\u5145\u65b9\u6cd5\u3002"}}
{"id": "2510.03261", "pdf": "https://arxiv.org/pdf/2510.03261", "abs": "https://arxiv.org/abs/2510.03261", "authors": ["C. Coelho", "M. Hohmann", "D. Fern\u00e1ndez", "L. Penter", "S. Ihlenfeldt", "O. Niggemann"], "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark", "categories": ["cs.LG", "cs.CE", "J.2; I.2"], "comment": null, "summary": "Thermal errors in machine tools significantly impact machining precision and\nproductivity. Traditional thermal error correction/compensation methods rely on\nmeasured temperature-deformation fields or on transfer functions. Most existing\ndata-driven compensation strategies employ neural networks (NNs) to directly\npredict thermal errors or specific compensation values. While effective, these\napproaches are tightly bound to particular error types, spatial locations, or\nmachine configurations, limiting their generality and adaptability. In this\nwork, we introduce a novel paradigm in which NNs are trained to predict\nhigh-fidelity temperature and heat flux fields within the machine tool. The\nproposed framework enables subsequent computation and correction of a wide\nrange of error types using modular, swappable downstream components. The NN is\ntrained using data obtained with the finite element method under varying\ninitial conditions and incorporates a correlation-based selection strategy that\nidentifies the most informative measurement points, minimising hardware\nrequirements during inference. We further benchmark state-of-the-art\ntime-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,\nLong-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal\nConvolutional Network, by training both specialised models, tailored for\nspecific initial conditions, and general models, capable of extrapolating to\nunseen scenarios. The results show accurate and low-cost prediction of\ntemperature and heat flux fields, laying the basis for enabling flexible and\ngeneralisable thermal error correction in machine tool environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u673a\u5e8a\u5185\u90e8\u9ad8\u7cbe\u5ea6\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\uff0c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u3001\u901a\u7528\u4e14\u6a21\u5757\u5316\u7684\u70ed\u8bef\u5dee\u4fee\u6b63\uff0c\u5e76\u4f18\u5316\u4e86\u4f20\u611f\u5668\u9009\u62e9\u3002", "motivation": "\u4f20\u7edf\u53ca\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u70ed\u8bef\u5dee\u4fee\u6b63\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u8bef\u5dee\u7c7b\u578b\u3001\u4f4d\u7f6e\u6216\u673a\u5668\u914d\u7f6e\uff0c\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u673a\u5e8a\u5185\u90e8\u7684\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\u3002\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u6d4b\u91cf\u70b9\u9009\u62e9\u7b56\u7565\u4ee5\u51cf\u5c11\u786c\u4ef6\u9700\u6c42\u3002\u5bf9\u5305\u62ecRNN\u3001GRU\u3001LSTM\u3001Bi-LSTM\u3001Transformer\u548cTCN\u5728\u5185\u7684\u591a\u79cd\u65f6\u95f4\u5e8f\u5217NN\u67b6\u6784\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bad\u7ec3\u4e86\u4e13\u7528\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u4ee5\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u548c\u672a\u77e5\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4e14\u4f4e\u6210\u672c\u5730\u9884\u6d4b\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\u3002\u5bf9\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5e8a\u73af\u5883\u4e2d\u7075\u6d3b\u3001\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u70ed\u8bef\u5dee\u4fee\u6b63\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u9884\u6d4b\u6e29\u5ea6/\u70ed\u901a\u91cf\u573a\u800c\u975e\u76f4\u63a5\u8bef\u5dee\u503c\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03777", "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGuidedSampling\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u8fc7\u7a0b\u7684\u63a2\u7d22\u548c\u751f\u6210\u9636\u6bb5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRepeated Sampling (RS)\u5728\u751f\u6210\u591a\u6837\u6027\u89e3\u51b3\u65b9\u6848\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u751f\u6210\u5019\u9009\u65b9\u6848\u7684\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u7684Repeated Sampling (RS)\u7b97\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728\u751f\u6210\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u5e38\u4f9d\u8d56\u76f8\u540c\u65b9\u6cd5\uff0c\u5bfc\u81f4\u6837\u672c\u5197\u4f59\u3002", "method": "\u63d0\u51faGuidedSampling\u63a8\u7406\u7b97\u6cd5\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u89e3\u8026\u4e3a\u63a2\u7d22\u548c\u751f\u6210\u9636\u6bb5\u3002\u63a2\u7d22\u9636\u6bb5\u8bc6\u522b\u89e3\u51b3\u95ee\u9898\u7684\u591a\u4e2a\u6982\u5ff5\uff0c\u751f\u6210\u9636\u6bb5\u5e94\u7528\u7279\u5b9a\u6982\u5ff5\u63d0\u4f9b\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002\u4f5c\u8005\u5b9a\u4e49\u4e86\u5176\u7406\u8bba\u754c\u9650\uff0c\u5e76\u8fdb\u884c\u4e86\u7ecf\u9a8c\u6027\u9a8c\u8bc1\u548c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eRS\u76f8\u6bd4\uff0cGuidedSampling\u5728pass@50\u4e0a\u5e73\u5747\u63d0\u5347\u57fa\u7840\u6a21\u578b\u6027\u80fd\u7ea621.6%\u3002\u4f7f\u7528GuidedSampling\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u5728pass@5\u4e0a\u5e73\u5747\u63d0\u5347\u7ea69.7%\u3002\u6b64\u5916\uff0cGuidedSampling\u5c06\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5e73\u5747\u6982\u5ff5\u6570\u4ece1.67\u589e\u52a0\u52303.03\uff0c\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u7684\u5019\u9009\u65b9\u6848\u3002", "conclusion": "GuidedSampling\u7b97\u6cd5\u901a\u8fc7\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\uff0c\u663e\u8457\u6539\u5584\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\u4e5f\u663e\u793a\u51fa\u4f18\u8d8a\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684Repeated Sampling\u65b9\u6cd5\u3002"}}
{"id": "2510.05068", "pdf": "https://arxiv.org/pdf/2510.05068", "abs": "https://arxiv.org/abs/2510.05068", "authors": ["Shreya Meel", "Sennur Ulukus"], "title": "Multi-Agent Distributed Optimization With Feasible Set Privacy", "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.NI", "eess.SP", "math.IT"], "comment": null, "summary": "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5206\u6563\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u7531\u9886\u5bfc\u8005\u5728\u4fdd\u6301\u4e2a\u4f53\u53ef\u884c\u96c6\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b66\u4e60\u6700\u4f18\u89e3\u96c6\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u5728\u540d\u4e49\u4fe1\u606f\u6cc4\u9732\u4e0b\u7684\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u5728\u73af\u5f62\u548c\u661f\u5f62\u7f51\u7edc\u62d3\u6251\u4e0b\u5206\u6790\u4e86\u5176\u901a\u4fe1\u6210\u672c\uff0c\u8bc1\u660e\u5176\u5728\u901a\u4fe1\u6548\u7387\u4e0a\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528PSI\u534f\u8bae\u3002", "motivation": "\u591a\u4e2a\u667a\u80fd\u4f53\u9700\u8981\u5171\u540c\u5b66\u4e60\u6700\u4f18\u89e3\u96c6\uff0c\u4f46\u5176\u5404\u81ea\u7684\u79c1\u6709\u53ef\u884c\u96c6\u4fe1\u606f\u4e0d\u5e0c\u671b\u5411\u5176\u4ed6\u667a\u80fd\u4f53\uff08\u7279\u522b\u662f\u9886\u5bfc\u8005\uff09\u6cc4\u9732\u8d85\u51fa\u5b66\u4e60\u89e3\u96c6\u6240\u5fc5\u9700\u7684\u201c\u540d\u4e49\u201d\u4fe1\u606f\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u901a\u8fc7PSI\u534f\u8bae\u5b66\u4e60\u8054\u5408\u53ef\u884c\u96c6\uff09\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u7684\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u5f00\u53d1\u4e86\u5728\u540d\u4e49\u4fe1\u606f\u6cc4\u9732\u4e0b\u83b7\u53d6\u89e3\u96c6\u7684\u53ef\u884c\u65b9\u6848\u3002\u5728\u73af\u5f62\u548c\u661f\u5f62\u4e24\u79cd\u901a\u4fe1\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6848\u7684\u901a\u4fe1\u6210\u672c\u8fdb\u884c\u4e86\u7279\u6027\u63cf\u8ff0\u3002\u7814\u7a76\u4e86\u6240\u63d0\u65b9\u6848\u4e0e\u9608\u503c\u79c1\u6709\u96c6\u5408\u4ea4\u96c6\uff08ThPSI\uff09\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u6240\u5f00\u53d1\u7684\u65b9\u6848\u5b9e\u73b0\u4e86\u540d\u4e49\u4fe1\u606f\u6cc4\u9732\u3002\u8bc1\u660e\u4e86\u5982\u679c\u9886\u5bfc\u8005\u9996\u5148\u901a\u8fc7\u73b0\u6709\u79c1\u6709\u96c6\u5408\u4ea4\u96c6\uff08PSI\uff09\u534f\u8bae\u5b66\u4e60\u8054\u5408\u53ef\u884c\u96c6\uff0c\u4fe1\u606f\u6cc4\u9732\u91cf\u5c06\u5927\u4e8e\u540d\u4e49\u503c\u3002\u5bf9\u4e8e\u76ee\u6807\u51fd\u6570$f$\u7684\u5404\u79cd\u968f\u673a\u5b9e\u73b0\uff0c\u6240\u63d0\u65b9\u6848\u4e0e\u901a\u8fc7PSI\u68c0\u7d22\u6574\u4e2a\u53ef\u884c\u96c6\u76f8\u6bd4\uff0c\u4ee5\u9ad8\u6982\u7387\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u5728\u5206\u6563\u5f0f\u7ea6\u675f\u4f18\u5316\u4e2d\uff0c\u4ee5\u540d\u4e49\u4fe1\u606f\u6cc4\u9732\u83b7\u53d6\u6700\u4f18\u89e3\u96c6\u7684\u6709\u6548\u65b9\u6848\u3002\u8fd9\u4e9b\u65b9\u6848\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5728\u7279\u5b9a\u7f51\u7edc\u8bbe\u7f6e\u548c\u76ee\u6807\u51fd\u6570\u6761\u4ef6\u4e0b\uff0c\u8868\u73b0\u51fa\u6bd4\u76f4\u63a5\u4f7f\u7528PSI\u534f\u8bae\u66f4\u4f18\u7684\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2510.03595", "pdf": "https://arxiv.org/pdf/2510.03595", "abs": "https://arxiv.org/abs/2510.03595", "authors": ["Haikang Deng", "Po-Nien Kung", "Nanyun Peng"], "title": "Decoupling Task-Solving and Output Formatting in LLM Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly adept at following instructions\ncontaining task descriptions to solve complex problems, such as mathematical\nreasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow\nmore complex, models often struggle to adhere to all instructions. This\ndifficulty is especially common when instructive prompts intertwine reasoning\ndirectives -- specifying what the model should solve -- with rigid formatting\nrequirements that dictate how the solution must be presented. The entanglement\ncreates competing goals for the model, suggesting that more explicit separation\nof these two aspects could lead to improved performance. To this front, we\nintroduce Deco-G, a decoding framework that explicitly decouples format\nadherence from task solving. Deco-G handles format compliance with a separate\ntractable probabilistic model (TPM), while prompts LLMs with only task\ninstructions. At each decoding step, Deco-G combines next token probabilities\nfrom the LLM with the TPM calculated format compliance likelihood to form the\noutput probability. To make this approach both practical and scalable for\nmodern instruction-tuned LLMs, we introduce three key innovations:\ninstruction-aware distillation, a flexible trie-building algorithm, and HMM\nstate pruning for computational efficiency. We demonstrate the effectiveness of\nDeco-G across a wide range of tasks with diverse format requirements, including\nmathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,\nour approach yields 1.0% to 6.0% relative gain over regular prompting practice\nwith guaranteed format compliance.", "AI": {"tldr": "\u4e00\u79cd\u540d\u4e3aDeco-G\u7684\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u683c\u5f0f\u9075\u5faa\u4e0e\u4efb\u52a1\u89e3\u51b3\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u4fdd\u8bc1\u4e86\u683c\u5f0f\u5408\u89c4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u65f6\uff0c\u96be\u4ee5\u540c\u65f6\u9075\u5faa\u63a8\u7406\u6307\u4ee4\u548c\u4e25\u683c\u7684\u683c\u5f0f\u8981\u6c42\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u4ea7\u751f\u76f8\u4e92\u7ade\u4e89\u7684\u76ee\u6807\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165Deco-G\u89e3\u7801\u6846\u67b6\uff0c\u660e\u786e\u89e3\u8026\u683c\u5f0f\u9075\u5faa\u4e0e\u4efb\u52a1\u89e3\u51b3\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u5355\u72ec\u7684\u53ef\u5904\u7406\u6982\u7387\u6a21\u578b\uff08TPM\uff09\u5904\u7406\u683c\u5f0f\u5408\u89c4\u6027\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ec5\u5904\u7406\u4efb\u52a1\u6307\u4ee4\u3002\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\uff0cDeco-G\u7ed3\u5408LLM\u7684\u4e0b\u4e00\u4e2a\u8bcd\u5143\u6982\u7387\u548cTPM\u8ba1\u7b97\u7684\u683c\u5f0f\u5408\u89c4\u6027\u4f3c\u7136\u6765\u5f62\u6210\u8f93\u51fa\u6982\u7387\u3002\u4e3a\u5b9e\u73b0\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u6307\u4ee4\u611f\u77e5\u84b8\u998f\u3001\u7075\u6d3b\u7684Trie\u6784\u5efa\u7b97\u6cd5\u548cHMM\u72b6\u6001\u526a\u679d\u3002", "result": "Deco-G\u5728\u6570\u5b66\u63a8\u7406\u3001LLM-as-a-judge\u548c\u4e8b\u4ef6\u8bba\u5143\u63d0\u53d6\u7b49\u591a\u79cd\u5177\u6709\u4e0d\u540c\u683c\u5f0f\u8981\u6c42\u7684\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u5e38\u89c4\u63d0\u793a\u5b9e\u8df5\u53d6\u5f97\u4e861.0%\u81f36.0%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4fdd\u8bc1\u4e86\u683c\u5f0f\u5408\u89c4\u6027\u3002", "conclusion": "Deco-G\u901a\u8fc7\u5c06\u683c\u5f0f adherence \u4e0e\u4efb\u52a1\u89e3\u51b3\u89e3\u8026\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u9075\u5faa\u6240\u6709\u6307\u4ee4\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u786e\u4fdd\u4e86\u683c\u5f0f\u5408\u89c4\u6027\u3002"}}
{"id": "2510.03363", "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b(UAD)\u9762\u4e34\u5339\u914d\u566a\u58f0\u548c\u5355\u6a21\u6001/\u591a\u6a21\u6001\u65b9\u6cd5\u5206\u79bb\u7684\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u4ee3\u4ef7\u8fc7\u6ee4(UCF)\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u540e\u5904\u7406\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u8fc7\u6ee4\u6a21\u5757\uff0c\u6d88\u9664\u5f02\u5e38\u4ee3\u4ef7\u4f53\u79ef\u4e2d\u7684\u5339\u914d\u566a\u58f0\u5e76\u7a81\u51fa\u7ec6\u5fae\u5f02\u5e38\u3002UCF\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cdUAD\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b(UAD)\u65b9\u6cd5\uff08\u65e0\u8bba\u662f\u57fa\u4e8e\u91cd\u5efa\u8fd8\u662f\u57fa\u4e8e\u5d4c\u5165\uff09\u5728\u751f\u6210\u5f02\u5e38\u56fe\u65f6\uff0c\u901a\u8fc7\u56fe\u50cf\u6216\u7279\u5f81\u5339\u914d\u6765\u5b8c\u6210\uff0c\u4f46\u901a\u5e38\u5ffd\u89c6\u4e86\u201c\u5339\u914d\u566a\u58f0\u201d\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u591a\u6a21\u6001UAD\uff08\u5982RGB-3D\u3001RGB-Text\uff09\u65e5\u76ca\u53d1\u5c55\uff0c\u4f46\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u4e4b\u95f4\u4ecd\u76f8\u5bf9\u5b64\u7acb\uff0c\u963b\u788d\u4e86\u5bf9\u5171\u540c\u6311\u6218\u7684\u5168\u9762\u7406\u89e3\u548c\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u4ee3\u4ef7\u8fc7\u6ee4(UCF)\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u7684\u540e\u5904\u7406\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u5bf9\u4efb\u4f55UAD\u6a21\u578b\u7684\u5f02\u5e38\u4ee3\u4ef7\u4f53\u79ef\u8fdb\u884c\u7ec6\u5316\u3002UCF\u7684\u6838\u5fc3\u601d\u60f3\u662f\u4ece\u5339\u914d\u89c6\u89d2\u7edf\u4e00\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001UAD\u3002\u5b83\u9996\u5148\u901a\u8fc7\u5c06\u6d4b\u8bd5\u6837\u672c\u4e0e\u6765\u81ea\u76f8\u540c\u6216\u4e0d\u540c\u6a21\u6001\u7684\u6b63\u5e38\u6837\u672c\u8fdb\u884c\u5339\u914d\u6765\u6784\u5efa\u4ee3\u4ef7\u4f53\u79ef\uff0c\u7136\u540e\u4f7f\u7528\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u8fc7\u6ee4\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u7ed3\u5408\u4e86\u6d4b\u8bd5\u6837\u672c\u7684\u591a\u5c42\u6ce8\u610f\u529b\u5f15\u5bfc\uff0c\u4ee5\u7f13\u89e3\u5339\u914d\u566a\u58f0\u5e76\u7a81\u51fa\u7ec6\u5fae\u5f02\u5e38\u3002", "result": "\u572822\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cUCF\u80fd\u6709\u6548\u63d0\u5347\u591a\u79cd\u73b0\u6709UAD\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5b83\u5728\u5355\u6a21\u6001(RGB)\u548c\u591a\u6a21\u6001(RGB-3D\u3001RGB-Text)UAD\u573a\u666f\u4e2d\u6301\u7eed\u53d6\u5f97\u65b0\u7684\u6700\u5148\u8fdb(SOTA)\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709UAD\u65b9\u6cd5\u4e2d\u5339\u914d\u566a\u58f0\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001UAD\u65b9\u6cd5\u4e4b\u95f4\u7684\u9694\u79bb\u95ee\u9898\u3002\u901a\u8fc7\u63d0\u51fa\u7684UCF\u8fd9\u4e00\u7edf\u4e00\u4e14\u901a\u7528\u7684\u540e\u5904\u7406\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cd\u8bbe\u7f6e\u4e0b\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.03262", "pdf": "https://arxiv.org/pdf/2510.03262", "abs": "https://arxiv.org/abs/2510.03262", "authors": ["Andi Zhang", "Xuan Ding", "Haofan Wang", "Steven McDonagh", "Samuel Kaski"], "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict\northogonality when combining sparse semantic vectors without extra time\ncomplexity. LoRA, a popular fine-tuning method for large models, typically\ntrains a module to represent a specific concept such as an object or a style.\nWhen multiple LoRAs are merged, for example to generate an object in a\nparticular style, their semantic vectors may interfere with each other. Our\nmethod guarantees, at the theoretical and runtime levels, that merged LoRAs\nremain orthogonal and thus free from direct interference. However, empirical\nanalysis reveals that such orthogonality does not lead to the semantic\ndisentanglement or compositionality highlighted in prior work on compositional\nadaptation. This finding suggests that inter-LoRA orthogonality alone may be\ninsufficient for achieving true semantic compositionality, prompting a\nre-examination of its role in adapter merging.", "AI": {"tldr": "\u63d0\u51fa\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\uff0c\u65e8\u5728\u786e\u4fddLoRA\u5408\u5e76\u65f6\u8bed\u4e49\u5411\u91cf\u7684\u6b63\u4ea4\u6027\uff0c\u4f46\u5b9e\u8bc1\u5206\u6790\u8868\u660e\u5176\u672a\u80fd\u5b9e\u73b0\u8bed\u4e49\u89e3\u8026\u6216\u7ec4\u5408\u6027\u3002", "motivation": "\u5f53\u5408\u5e76\u591a\u4e2aLoRA\u6a21\u5757\uff08\u4f8b\u5982\u751f\u6210\u7279\u5b9a\u98ce\u683c\u7684\u7269\u4f53\uff09\u65f6\uff0c\u5176\u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u76f8\u4e92\u5e72\u6270\uff0c\u5f71\u54cd\u7ec4\u5408\u6548\u679c\u3002", "method": "\u5f15\u5165\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\u673a\u5236\uff0c\u5728\u7406\u8bba\u548c\u8fd0\u884c\u65f6\u5c42\u9762\u5f3a\u5236\u5408\u5e76\u540e\u7684\u7a00\u758f\u8bed\u4e49\u5411\u91cf\u4fdd\u6301\u4e25\u683c\u6b63\u4ea4\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba1\u786e\u4fdd\u4e86LoRA\u4e4b\u95f4\u7684\u6b63\u4ea4\u6027\uff0c\u4f46\u8fd9\u5e76\u672a\u5b9e\u73b0\u5148\u524d\u5de5\u4f5c\u4e2d\u5f3a\u8c03\u7684\u8bed\u4e49\u89e3\u8026\u6216\u7ec4\u5408\u6027\u3002", "conclusion": "\u4ec5\u9760LoRA\u95f4\u7684\u6b63\u4ea4\u6027\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u4e49\u7ec4\u5408\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5176\u5728\u9002\u914d\u5668\u5408\u5e76\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.03845", "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u578b\u7b56\u7565\u7a7a\u95f4\u535a\u5f08\uff0c\u63d0\u51fa\u201c\u9690\u85cf\u535a\u5f08\u95ee\u9898\u201d\uff0c\u5e76\u901a\u8fc7\u7ec4\u5408\u540e\u6094\u6700\u5c0f\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u6700\u4f18\u540e\u6094\u754c\u9650\uff0c\u52a0\u901f\u6536\u655b\u5230\u9690\u85cf\u5b50\u535a\u5f08\u7684\u5747\u8861\u3002", "motivation": "\u89e3\u51b3AI\u5bf9\u9f50\u548c\u8bed\u8a00\u535a\u5f08\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5927\u578b\u7b56\u7565\u7a7a\u95f4\u4e2d\u73a9\u5bb6\u5b58\u5728\u672a\u77e5\u9ad8\u56de\u62a5\u7b56\u7565\u5b50\u96c6\u7684\u60c5\u51b5\uff0c\u65e8\u5728\u8bbe\u8ba1\u9ad8\u6548\u7684\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u6765\u53d1\u73b0\u5e76\u5229\u7528\u8fd9\u4e9b\u9690\u85cf\u7ed3\u6784\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540e\u6094\u6700\u5c0f\u5316\u6280\u672f\u7684\u7ec4\u5408\u65b9\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u5916\u90e8\u540e\u6094\u548c\u4ea4\u6362\u540e\u6094\u754c\u9650\uff1b\u786e\u4fdd\u4e86\u5728\u9690\u85cf\u5b50\u535a\u5f08\u4e2d\u5feb\u901f\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\uff1b\u901a\u8fc7\u5229\u7528\u9690\u85cf\u535a\u5f08\u7ed3\u6784\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u540e\u6094\u6700\u5c0f\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u53d1\u73b0\u548c\u5229\u7528\u9690\u85cf\u535a\u5f08\u7ed3\u6784\uff0c\u4ece\u800c\u5b9e\u73b0\u5b50\u535a\u5f08\u4e2d\u7684\u5747\u8861\u5e76\u4fdd\u6301\u6574\u4f53\u7406\u6027\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2510.03611", "pdf": "https://arxiv.org/pdf/2510.03611", "abs": "https://arxiv.org/abs/2510.03611", "authors": ["Raquib Bin Yousuf", "Aadyant Khatri", "Shengzhe Xu", "Mandar Sharma", "Naren Ramakrishnan"], "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2025 IEEE International Conference on Knowledge Graph (ICKG)", "summary": "Recently proposed evaluation benchmarks aim to characterize the effective\ncontext length and the forgetting tendencies of large language models (LLMs).\nHowever, these benchmarks often rely on simplistic 'needle in a haystack'\nretrieval or continuation tasks that may not accurately reflect the performance\nof these models in information-dense scenarios. Thus, rather than simple next\ntoken prediction, we argue for evaluating these models on more complex\nreasoning tasks that requires them to induce structured relational knowledge\nfrom the text - such as graphs from potentially noisy natural language content.\nWhile the input text can be viewed as generated in terms of a graph, its\nstructure is not made explicit and connections must be induced from distributed\ntextual cues, separated by long contexts and interspersed with irrelevant\ninformation. Our findings reveal that LLMs begin to exhibit memory drift and\ncontextual forgetting at much shorter effective lengths when tasked with this\nform of relational reasoning, compared to what existing benchmarks suggest.\nWith these findings, we offer recommendations for the optimal use of popular\nLLMs for complex reasoning tasks. We further show that even models specialized\nfor reasoning, such as OpenAI o1, remain vulnerable to early memory drift in\nthese settings. These results point to significant limitations in the models'\nability to abstract structured knowledge from unstructured input and highlight\nthe need for architectural adaptations to improve long-range reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u590d\u6742\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\uff08\u5982\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u56fe\u7ed3\u6784\uff09\uff0cLLMs\u7684\u8bb0\u5fc6\u6f02\u79fb\u548c\u9057\u5fd8\u53d1\u751f\u5f97\u6bd4\u73b0\u6709\u7b80\u5355\u57fa\u51c6\u6d4b\u8bd5\u6240\u663e\u793a\u7684\u66f4\u65e9\uff0c\u5373\u4f7f\u662f\u4e13\u95e8\u7684\u63a8\u7406\u6a21\u578b\u4e5f\u5982\u6b64\uff0c\u8fd9\u63ed\u793a\u4e86\u5176\u62bd\u8c61\u7ed3\u6784\u5316\u77e5\u8bc6\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\uff08\u5982\u201c\u5927\u6d77\u635e\u9488\u201d\u4efb\u52a1\uff09\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620LLM\u5728\u4fe1\u606f\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u4e5f\u672a\u80fd\u6709\u6548\u523b\u753b\u5176\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u9057\u5fd8\u503e\u5411\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e94\u901a\u8fc7\u66f4\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u6765\u8bc4\u4f30LLM\uff0c\u8fd9\u4e9b\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u4ece\u53ef\u80fd\u5608\u6742\u7684\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u4e2d\u5f52\u7eb3\u51fa\u7ed3\u6784\u5316\u5173\u7cfb\u77e5\u8bc6\uff08\u4f8b\u5982\u56fe\uff09\uff0c\u800c\u975e\u7b80\u5355\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u3002\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u6a21\u578b\u4ece\u957f\u4e0a\u4e0b\u6587\u548c\u65e0\u5173\u4fe1\u606f\u4e2d\u63a8\u65ad\u51fa\u5206\u6563\u7684\u6587\u672c\u7ebf\u7d22\u3002", "result": "LLM\u5728\u5904\u7406\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u65f6\uff0c\u5176\u8bb0\u5fc6\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u9057\u5fd8\u5728\u6bd4\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5efa\u8bae\u7684\u66f4\u77ed\u7684\u6709\u6548\u957f\u5ea6\u4e0b\u5c31\u5f00\u59cb\u51fa\u73b0\u3002\u5373\u4f7f\u662f\u50cfOpenAI o1\u8fd9\u6837\u4e13\u95e8\u7528\u4e8e\u63a8\u7406\u7684\u6a21\u578b\uff0c\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u4e5f\u5bb9\u6613\u8fc7\u65e9\u51fa\u73b0\u8bb0\u5fc6\u6f02\u79fb\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86LLM\u4ece\u975e\u7ed3\u6784\u5316\u8f93\u5165\u4e2d\u62bd\u8c61\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u8fdb\u884c\u67b6\u6784\u8c03\u6574\u4ee5\u6539\u8fdb\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u8fd8\u6839\u636e\u7814\u7a76\u7ed3\u679c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2dLLM\u7684\u6700\u4f73\u4f7f\u7528\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.03376", "pdf": "https://arxiv.org/pdf/2510.03376", "abs": "https://arxiv.org/abs/2510.03376", "authors": ["Sanjukta Ghosh"], "title": "Visual Language Model as a Judge for Object Detection in Industrial Diagrams", "categories": ["cs.CV", "eess.IV"], "comment": "Pre-review version submitted to IEEE ICASSP 2026", "summary": "Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are\nessential for the design, operation, and maintenance of industrial plants.\nConverting these diagrams into digital form is an important step toward\nbuilding digital twins and enabling intelligent industrial automation. A\ncentral challenge in this digitalization process is accurate object detection.\nAlthough recent advances have significantly improved object detection\nalgorithms, there remains a lack of methods to automatically evaluate the\nquality of their outputs. This paper addresses this gap by introducing a\nframework that employs Visual Language Models (VLMs) to assess object detection\nresults and guide their refinement. The approach exploits the multimodal\ncapabilities of VLMs to identify missing or inconsistent detections, thereby\nenabling automated quality assessment and improving overall detection\nperformance on complex industrial diagrams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u4e1a\u56fe\u7eb8\uff08\u5982P&ID\uff09\u4e0a\u7684\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\uff0c\u4ee5\u89e3\u51b3\u7f3a\u4e4f\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u56fe\u7eb8\u6570\u5b57\u5316\u5bf9\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u667a\u80fd\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7cbe\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u662f\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u867d\u6709\u8fdb\u6b65\uff0c\u5374\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\u5176\u8f93\u51fa\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc4\u4f30\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u5e76\u6307\u5bfc\u5176\u6539\u8fdb\u7684\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528VLM\u7684\u591a\u6a21\u6001\u80fd\u529b\u6765\u8bc6\u522b\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u7684\u68c0\u6d4b\u3002", "result": "\u5b9e\u73b0\u4e86\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7VLM\u5b9e\u73b0\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u548c\u6307\u5bfc\u68c0\u6d4b\u4f18\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u590d\u6742\u5de5\u4e1a\u56fe\u7eb8\u4e0a\u7684\u6574\u4f53\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.03263", "pdf": "https://arxiv.org/pdf/2510.03263", "abs": "https://arxiv.org/abs/2510.03263", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The impressive capability of modern text-to-image models to generate\nrealistic visuals has come with a serious drawback: they can be misused to\ncreate harmful, deceptive or unlawful content. This has accelerated the push\nfor machine unlearning. This new field seeks to selectively remove specific\nknowledge from a model's training data without causing a drop in its overall\nperformance. However, it turns out that actually forgetting a given concept is\nan extremely difficult task. Models exposed to attacks using adversarial\nprompts show the ability to generate so-called unlearned concepts, which can be\nnot only harmful but also illegal. In this paper, we present considerations\nregarding the ability of models to forget and recall knowledge, introducing the\nMemory Self-Regeneration task. Furthermore, we present MemoRa strategy, which\nwe consider to be a regenerative approach supporting the effective recovery of\npreviously lost knowledge. Moreover, we propose that robustness in knowledge\nretrieval is a crucial yet underexplored evaluation measure for developing more\nrobust and effective unlearning techniques. Finally, we demonstrate that\nforgetting occurs in two distinct ways: short-term, where concepts can be\nquickly recalled, and long-term, where recovery is more challenging.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u6587\u751f\u56fe\u6a21\u578b\u4e2d\u673a\u5668\u9057\u5fd8\u7684\u6311\u6218\uff0c\u5f15\u5165\u201c\u8bb0\u5fc6\u81ea\u518d\u751f\u201d\u4efb\u52a1\u548cMemoRa\u7b56\u7565\u6765\u5206\u6790\u6a21\u578b\u9057\u5fd8\u4e0e\u77e5\u8bc6\u6062\u590d\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u9057\u5fd8\u5b58\u5728\u77ed\u65f6\u548c\u957f\u65f6\u4e24\u79cd\u5f62\u5f0f\u3002", "motivation": "\u73b0\u4ee3\u6587\u751f\u56fe\u6a21\u578b\u867d\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u88ab\u6ee5\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u50ac\u751f\u4e86\u673a\u5668\u9057\u5fd8\u9700\u6c42\u3002\u7136\u800c\uff0c\u5f53\u524d\u9057\u5fd8\u6280\u672f\u96be\u4ee5\u5f7b\u5e95\u79fb\u9664\u7279\u5b9a\u77e5\u8bc6\uff0c\u6a21\u578b\u4ecd\u80fd\u901a\u8fc7\u5bf9\u6297\u6027\u63d0\u793a\u53ec\u56de\u201c\u5df2\u9057\u5fd8\u201d\u6982\u5ff5\uff0c\u8868\u660e\u9057\u5fd8\u8fc7\u7a0b\u590d\u6742\u4e14\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u3002", "method": ["\u63d0\u51fa\u201c\u8bb0\u5fc6\u81ea\u518d\u751f\uff08Memory Self-Regeneration\uff09\u201d\u4efb\u52a1\uff0c\u7528\u4e8e\u7814\u7a76\u6a21\u578b\u9057\u5fd8\u548c\u56de\u5fc6\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "\u63d0\u51faMemoRa\u7b56\u7565\uff0c\u8fd9\u662f\u4e00\u79cd\u652f\u6301\u6709\u6548\u6062\u590d\u5148\u524d\u9057\u5931\u77e5\u8bc6\u7684\u518d\u751f\u65b9\u6cd5\u3002", "\u5efa\u8bae\u5c06\u77e5\u8bc6\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u4f5c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u6709\u6548\u9057\u5fd8\u6280\u672f\u7684\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u8bc4\u4f30\u6307\u6807\u3002"], "result": ["\u9057\u5fd8\u4ee5\u4e24\u79cd\u622a\u7136\u4e0d\u540c\u7684\u65b9\u5f0f\u53d1\u751f\uff1a\u77ed\u65f6\u9057\u5fd8\uff08\u6982\u5ff5\u53ef\u5feb\u901f\u53ec\u56de\uff09\u548c\u957f\u65f6\u9057\u5fd8\uff08\u6062\u590d\u66f4\u5177\u6311\u6218\u6027\uff09\u3002", "\u901a\u8fc7\u63d0\u51fa\u7684\u4efb\u52a1\u548c\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u77e5\u8bc6\u9057\u5fd8\u548c\u6062\u590d\u7684\u590d\u6742\u6027\u3002"], "conclusion": "\u673a\u5668\u9057\u5fd8\u662f\u590d\u6742\u4e14\u5145\u6ee1\u6311\u6218\u7684\u4efb\u52a1\uff0c\u6a21\u578b\u5373\u4f7f\u201c\u9057\u5fd8\u201d\u4e86\u77e5\u8bc6\u4e5f\u53ef\u80fd\u901a\u8fc7\u7279\u5b9a\u673a\u5236\u6062\u590d\u3002\u77e5\u8bc6\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u662f\u8bc4\u4f30\u9057\u5fd8\u6280\u672f\u6709\u6548\u6027\u7684\u5173\u952e\u6307\u6807\u3002\u533a\u5206\u77ed\u65f6\u548c\u957f\u65f6\u9057\u5fd8\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5e76\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u6709\u6548\u7684\u673a\u5668\u9057\u5fd8\u6280\u672f\u3002"}}
{"id": "2510.03847", "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "categories": ["cs.AI", "cs.LG"], "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLMs) \u5728\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65f6\uff0c\u7ed3\u5408\u5f15\u5bfc\u89e3\u7801\u548c\u9a8c\u8bc1\u5668\u53ef\u663e\u8457\u964d\u4f4e\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u5e76\u5e38\u80fd\u8d85\u8d8a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs)\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u53d7\u9650\u4e8e\u6a21\u5f0f\u548cAPI\u7684\u7cbe\u786e\u4efb\u52a1\u65f6\u53ef\u80fd\u5b58\u5728\u7684\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\u548c\u80fd\u8017\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u7efc\u5408\u5206\u6790\u4e86\u591a\u79cd\u5f00\u653e\u548c\u4e13\u6709SLM\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u4ee3\u8bc4\u4f30\u57fa\u51c6\u3001\u670d\u52a1\u5806\u6808\u548c\u5f15\u5bfc\u89e3\u7801\u5e93\u7ed3\u5408\u3002\u7814\u7a76\u63d0\u51faSLM\u9ed8\u8ba4\u3001LLM\u5907\u7528\u7cfb\u7edf\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8def\u7531\u548c\u9a8c\u8bc1\u5668\u7ea7\u8054\u3002\u63d0\u51fa\u4e86\u5b9e\u9645\u751f\u4ea7\u6307\u6807\uff08\u5982\u6bcf\u6b21\u6210\u529f\u4efb\u52a1\u6210\u672c\u3001\u6a21\u5f0f\u6709\u6548\u6027\uff09\u3002\u5e94\u7528\u5f15\u5bfc\u89e3\u7801\u3001\u4e25\u683cJSON Schema\u8f93\u51fa\u548c\u9a8c\u8bc1\u5668\u4f18\u5148\u7684\u5de5\u5177\u6267\u884c\uff0c\u5e76\u63d0\u4f9b\u5305\u62ecschema\u4f18\u5148\u63d0\u793a\u3001\u7c7b\u578b\u5b89\u5168\u51fd\u6570\u6ce8\u518c\u548c\u8f7b\u91cf\u7ea7\u9002\u914d\uff08LoRA/QLoRA\uff09\u7684\u8bbe\u8ba1\u6a21\u5f0f\u3002", "result": "\u5f15\u5bfc\u89e3\u7801\u3001\u4e25\u683cJSON Schema\u8f93\u51fa\u548c\u9a8c\u8bc1\u5668\u4f18\u5148\u7684\u5de5\u5177\u6267\u884c\u663e\u8457\u7f29\u5c0f\u4e86SLM\u4e0eLLM\u7684\u80fd\u529b\u5dee\u8ddd\u3002SLM\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548cRAG\u4efb\u52a1\u4e0a\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8aLLM\uff0c\u540c\u65f6\u5b9e\u73b010-100\u500d\u7684\u4ee3\u5e01\u6210\u672c\u964d\u4f4e\uff0c\u5e76\u663e\u8457\u6539\u5584\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "conclusion": "SLMs\u901a\u8fc7\u6070\u5f53\u7684\u5de5\u7a0b\u8bbe\u8ba1\uff08\u5982\u5f15\u5bfc\u89e3\u7801\u548c\u7ed3\u6784\u5316\u8f93\u51fa\uff09\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u6784\u5efa\u5feb\u901f\u3001\u5ec9\u4ef7\u4e14\u53ef\u9760\u4ee3\u7406\u7cfb\u7edf\u7684\u9996\u9009\uff0c\u4ec5\u5728\u5f00\u653e\u57df\u63a8\u7406\u548c\u957f\u5468\u671f\u89c4\u5212\u7b49\u5c11\u6570\u60c5\u51b5\u4e0b\u624d\u9700\u8981LLM\u4f5c\u4e3a\u8865\u5145\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4ee3\u7406\u5806\u6808\u7684\u5b9e\u7528\u84dd\u56fe\u3002"}}
{"id": "2510.03639", "pdf": "https://arxiv.org/pdf/2510.03639", "abs": "https://arxiv.org/abs/2510.03639", "authors": ["Liming Wang", "Junrui Ni", "Kai-Wei Chang", "Saurabhchand Bhati", "David Harwath", "Mark Hasegawa-Johnson", "James R. Glass"], "title": "Towards Unsupervised Speech Recognition at the Syllable-Level", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training speech recognizers with unpaired speech and text -- known as\nunsupervised speech recognition (UASR) -- is a crucial step toward extending\nASR to low-resource languages in the long-tail distribution and enabling\nmultimodal learning from non-parallel data. However, existing approaches based\non phones often rely on costly resources such as grapheme-to-phoneme converters\n(G2Ps) and struggle to generalize to languages with ambiguous phoneme\nboundaries due to training instability. In this paper, we address both\nchallenges by introducing a syllable-level UASR framework based on masked\nlanguage modeling, which avoids the need for G2P and the instability of\nGAN-based methods. Our approach achieves up to a 40\\% relative reduction in\ncharacter error rate (CER) on LibriSpeech and generalizes effectively to\nMandarin, a language that has remained particularly difficult for prior\nmethods. Code will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u7684\u97f3\u8282\u7ea7\u65e0\u76d1\u7763\u8bed\u97f3\u8bc6\u522b\uff08UASR\uff09\u6846\u67b6\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9G2P\u7684\u4f9d\u8d56\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728LibriSpeech\u4e0a\u663e\u8457\u964d\u4f4e\u4e86CER\uff0c\u5e76\u6709\u6548\u6cdb\u5316\u5230\u666e\u901a\u8bdd\u3002", "motivation": "\u5c06ASR\u6269\u5c55\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u975e\u5e76\u884c\u6570\u636e\u7684\u591a\u6a21\u6001\u5b66\u4e60\u662f\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u97f3\u7d20\u7684UASR\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684G2P\u8d44\u6e90\uff0c\u4e14\u56e0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u96be\u4ee5\u6cdb\u5316\u5230\u97f3\u7d20\u8fb9\u754c\u6a21\u7cca\u7684\u8bed\u8a00\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u7684\u97f3\u8282\u7ea7UASR\u6846\u67b6\uff0c\u4ee5\u907f\u514d\u5bf9G2P\u7684\u9700\u6c42\u548c\u57fa\u4e8eGAN\u65b9\u6cd5\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5728LibriSpeech\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe40%\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u76f8\u5bf9\u964d\u4f4e\uff1b\u6709\u6548\u6cdb\u5316\u5230\u5bf9\u73b0\u6709\u65b9\u6cd5\u800c\u8a00\u7279\u522b\u56f0\u96be\u7684\u666e\u901a\u8bdd\u3002", "conclusion": "\u8be5\u97f3\u8282\u7ea7UASR\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709UASR\u65b9\u6cd5\u7684\u8d44\u6e90\u4f9d\u8d56\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u591a\u8bed\u8a00\uff08\u5305\u62ec\u82f1\u8bed\u548c\u666e\u901a\u8bdd\uff09\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03441", "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165SpatialViLT\uff0c\u4e00\u4e2a\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u56fe\u30013D\u5750\u6807\u7b49\u7a7a\u95f4\u7279\u5f81\u7684\u589e\u5f3a\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86VLM\u57283D\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u6311\u6218\uff0c\u5e76\u5728VSR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u57283D\u573a\u666f\u548c\u590d\u6742\u7269\u4f53\u914d\u7f6e\u7684\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f15\u5165SpatialViLT\uff0c\u4e00\u4e2a\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6574\u5408\u6df1\u5ea6\u56fe\u30013D\u5750\u6807\u548c\u8fb9\u7f18\u56fe\u7b49\u7a7a\u95f4\u7279\u5f81\u7684\u589e\u5f3a\u578bVLM\u3002\u63d0\u51fa\u4e24\u79cd\u53d8\u4f53\uff1aSpatialViLT\uff08\u5173\u6ce8\u5b8c\u6574\u7269\u4f53\u533a\u57df\uff09\u548cMaskedSpatialViLT\uff08\u5173\u6ce8\u8499\u7248\u7269\u4f53\u533a\u57df\uff09\u3002SpatialEnsemble\u7ed3\u5408\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728Visual Spatial Reasoning (VSR) \u6570\u636e\u96c6\u4e0a\uff0c\u5728\u65b9\u5411\u3001\u62d3\u6251\u548c\u90bb\u8fd1\u5173\u7cfb\u7b49\u7a7a\u95f4\u63a8\u7406\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5176\u4e2dSpatialEnsemble\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5728\u589e\u5f3aAI\u7cfb\u7edf\u7684\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5bf9\u9ad8\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u548c\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u5177\u6709\u5173\u952e\u610f\u4e49\u3002"}}
{"id": "2510.03264", "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u63a8\u7406\u6570\u636e\u5728LLM\u9884\u8bad\u7ec3\u9636\u6bb5\u65e9\u671f\u5f15\u5165\u5bf9\u5176\u63a8\u7406\u80fd\u529b\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff08\u5e73\u5747\u63d0\u534719%\uff09\uff0c\u4e14\u6548\u679c\u4f18\u4e8e\u540e\u671f\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u63d0\u51fa\u4e86\u9884\u8bad\u7ec3\u9700\u591a\u6837\u6027\u3001\u5fae\u8c03\u9700\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u4e0d\u5bf9\u79f0\u5206\u914d\u539f\u5219\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e3b\u8981\u4f9d\u8d56\u540e\u8bad\u7ec3\uff0c\u4f46\u63a8\u7406\u6570\u636e\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u540e\u8bad\u7ec3\u7684\u76f8\u4e92\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u6765\u63a2\u7a76\u65e9\u671f\u5f15\u5165\u63a8\u7406\u6570\u636e\u662f\u5426\u66f4\u4f18\u6216\u6709\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u7684\u63a8\u7406\u6570\u636e\u5728LLM\u8bad\u7ec3\u7684\u4e0d\u540c\u9636\u6bb5\uff08\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\uff09\u5f15\u5165\u65f6\uff0c\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u65e9\u671f\u5c06\u63a8\u7406\u6570\u636e\u878d\u5165\u9884\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff08\u5e73\u5747\u63d0\u534719%\uff09\uff0c\u5176\u5960\u5b9a\u7684\u57fa\u7840\u80fd\u529b\u662f\u540e\u671f\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65e0\u6cd5\u5b8c\u5168\u590d\u5236\u7684\u3002\u6570\u636e\u5206\u914d\u5b58\u5728\u4e0d\u5bf9\u79f0\u539f\u5219\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u6700\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u5e7f\u6cdb\u591a\u6837\u6027\uff08\u5e73\u5747\u63d0\u534711%\uff09\uff0c\u800cSFT\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u4e3a\u654f\u611f\uff08\u5e73\u5747\u63d0\u534715%\uff09\u3002\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u5177\u6709\u6f5c\u5728\u6548\u5e94\uff0c\u4ec5\u5728SFT\u540e\u6fc0\u6d3b\uff1b\u76f2\u76ee\u6269\u589eSFT\u6570\u636e\u53cd\u800c\u53ef\u80fd\u6709\u5bb3\uff0c\u62b5\u6d88\u65e9\u671f\u63a8\u7406\u6ce8\u5165\u7684\u76ca\u5904\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u8bed\u8a00\u5efa\u6a21\u4e0e\u63a8\u7406\u7684\u4f20\u7edf\u5206\u79bb\uff0c\u4e3a\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u6218\u7565\u6027\u5730\u5206\u914d\u6570\u636e\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684LLM\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2510.03851", "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "categories": ["cs.AI"], "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).", "AI": {"tldr": "\u73b0\u6709LLM\u5728\u7cfb\u7edf\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7f3a\u4e4f\u521b\u9020\u6027\uff0c\u672c\u6587\u63d0\u51faMetaMuse\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u539f\u5219\u663e\u8457\u63d0\u5347\u4e86\u7f13\u5b58\u66ff\u6362\u548c\u5728\u7ebf\u88c5\u7bb1\u7b49\u5173\u952e\u7cfb\u7edf\u95ee\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u7cfb\u7edf\u7b97\u6cd5\u8bbe\u8ba1\u56e0\u89e3\u7a7a\u95f4\u4e0d\u8fde\u7eed\u800c\u5145\u6ee1\u6311\u6218\uff0c\u5bfc\u81f4\u5de5\u7a0b\u5e08\u5e38\u4f9d\u8d56\u901a\u7528\u542f\u53d1\u5f0f\u7b97\u6cd5\u727a\u7272\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709LLM\u503e\u5411\u4e8e\u901a\u7528\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5b9e\u73b0\u6240\u9700\u7684\u521b\u9020\u6027\u7a81\u7834\u3002", "method": "\u5f15\u5165MetaMuse\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u610f\u6784\u601d\uff0c\u57fa\u4e8e\u4e09\u4e2a\u81ea\u53cd\u601d\u539f\u5219\uff1a(1) \u5728\u53ef\u8861\u91cf\u7684\u6027\u80fd\u7a7a\u95f4\u800c\u975e\u62bd\u8c61\u6982\u5ff5\u7a7a\u95f4\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u591a\u6837\u6027\u548c\u6548\u7528\uff1b(2) \u901a\u8fc7\u5916\u90e8\u523a\u6fc0\u800c\u975e\u5185\u90e8\u968f\u673a\u6027\u5f15\u5bfc\u6784\u601d\uff1b(3) \u4f7f\u7528\u822a\u8def\u70b9\u63a8\u7406\u800c\u975e\u81ea\u7531\u5f62\u5f0f\u601d\u7ef4\u94fe\u6784\u5efa\u53ef\u6267\u884c\u65b9\u6848\u3002", "result": "MetaMuse\u80fd\u4e3a\u5168\u7403\u4e91\u63d0\u4f9b\u5546\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u751f\u6210\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff1a\u7f13\u5b58\u66ff\u6362\uff08\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u964d\u4f4e\u9ad8\u8fbe35.76%\uff09\u548c\u5728\u7ebf\u88c5\u7bb1\uff08\u88c5\u7bb1\u4f7f\u7528\u7387\u964d\u4f4e\u9ad8\u8fbe30.93%\uff09\u3002", "conclusion": "MetaMuse\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u7cfb\u7edf\u7b97\u6cd5\u751f\u6210\u4e2d\u521b\u9020\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u663e\u8457\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03663", "pdf": "https://arxiv.org/pdf/2510.03663", "abs": "https://arxiv.org/abs/2510.03663", "authors": ["Xiangyu Peng", "Cab Qin", "Zeyuan Chen", "Ran Xu", "Caiming Xiong", "Chien-Sheng Wu"], "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for\napplying large language models (LLMs) and agents to real-world knowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduce\nUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-world PDF pages across eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanning factual retrieval, comparison, summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication. UniDoc-Bench supports\napples-to-apples comparison across four paradigms: (1) text-only, (2)\nimage-only, (3) multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardized candidate pools,\nprompts, and evaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and how visual context\ncomplements textual evidence, uncovers systematic failure modes, and offers\nactionable guidance for developing more robust MM-RAG pipelines.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UniDoc-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u7684MM-RAG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e7\u4e07\u4efdPDF\u9875\u9762\u6784\u5efa\uff0c\u6db5\u76d6\u591a\u79cd\u67e5\u8be2\u7c7b\u578b\u548cRAG\u8303\u5f0f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u878d\u5408RAG\u7cfb\u7edf\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8054\u5408\u68c0\u7d22\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5d4c\u5165\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u66f4\u7a33\u5065\u7684MM-RAG\u7ba1\u9053\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709MM-RAG\u8bc4\u4f30\u65b9\u6cd5\u96f6\u6563\u4e14\u7b80\u5316\uff0c\u672a\u80fd\u6355\u83b7\u6587\u6863\u4e2d\u5fc3\u7684\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u7528\u4f8b\uff0c\u8fd9\u963b\u788d\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u77e5\u8bc6\u5e93\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86UniDoc-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8e7\u4e07\u4efd\u771f\u5b9e\u4e16\u754cPDF\u9875\u9762\uff08\u6db5\u76d68\u4e2a\u9886\u57df\uff09\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u3002\u901a\u8fc7\u7ba1\u9053\u4ece\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u4e2d\u63d0\u53d6\u5e76\u94fe\u63a5\u8bc1\u636e\uff0c\u751f\u62101600\u4e2a\u6db5\u76d6\u4e8b\u5b9e\u68c0\u7d22\u3001\u6bd4\u8f83\u3001\u6458\u8981\u548c\u903b\u8f91\u63a8\u7406\u7684\u591a\u6a21\u6001QA\u5bf9\u300220%\u7684QA\u5bf9\u7ecf\u8fc7\u591a\u4eba\u6807\u6ce8\u548c\u4e13\u5bb6\u88c1\u51b3\u9a8c\u8bc1\uff0c\u786e\u4fdd\u53ef\u9760\u6027\u3002\u8be5\u57fa\u51c6\u5728\u4e00\u4e2a\u7edf\u4e00\u534f\u8bae\u4e0b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5019\u9009\u6c60\u3001\u63d0\u793a\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u652f\u6301\u5bf9\u7eaf\u6587\u672c\u3001\u7eaf\u56fe\u50cf\u3001\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u878d\u5408\u4ee5\u53ca\u591a\u6a21\u6001\u8054\u5408\u68c0\u7d22\u56db\u79cd\u8303\u5f0f\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u878d\u5408RAG\u7cfb\u7edf\u6301\u7eed\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u57fa\u4e8e\u8054\u5408\u591a\u6a21\u6001\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\u3002\u8fd9\u8868\u660e\u5355\u72ec\u7684\u6587\u672c\u6216\u56fe\u50cf\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u4efb\u52a1\uff0c\u4e14\u5f53\u524d\u7684\u591a\u6a21\u6001\u5d4c\u5165\u4ecd\u4e0d\u5b8c\u5584\u3002\u6b64\u5916\uff0c\u5206\u6790\u63ed\u793a\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u8865\u5145\u6587\u672c\u8bc1\u636e\uff0c\u53d1\u73b0\u4e86\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684MM-RAG\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u5bf9\u4e8e\u6587\u6863\u4e2d\u5fc3\u7684MM-RAG\u4efb\u52a1\uff0c\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u878d\u5408\u662f\u76ee\u524d\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8054\u5408\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6848\u3002\u5f53\u524d\u7684\u591a\u6a21\u6001\u5d4c\u5165\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002UniDoc-Bench\u4f5c\u4e3a\u65b0\u7684\u57fa\u51c6\uff0c\u7ed3\u5408\u6df1\u5165\u5206\u6790\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u7684MM-RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u5bdf\u548c\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.03452", "pdf": "https://arxiv.org/pdf/2510.03452", "abs": "https://arxiv.org/abs/2510.03452", "authors": ["Allison Davis", "Yezhi Shen", "Xiaoyu Ji", "Fengqing Zhu"], "title": "Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks", "categories": ["cs.CV"], "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Structured illumination (SI) enhances image resolution and contrast by\nprojecting patterned light onto a sample. In two-phase optical-sectioning SI\n(OS-SI), reduced acquisition time introduces residual artifacts that\nconventional denoising struggles to suppress. Deep learning offers an\nalternative to traditional methods; however, supervised training is limited by\nthe lack of clean, optically sectioned ground-truth data. We investigate\nencoder-decoder networks for artifact reduction in two-phase OS-SI, using\nsynthetic training pairs formed by applying real artifact fields to synthetic\nimages. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on\nthe synthetic data, then evaluated on real OS-SI images. Both networks improve\nimage clarity, with each excelling against different artifact types. These\nresults demonstrate that synthetic training enables supervised denoising of\nOS-SI images and highlight the potential of encoder-decoder networks to\nstreamline reconstruction workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff08DAE\u548cU-Net\uff09\u5bf9\u4e24\u76f8\u5149\u5207\u7ed3\u6784\u5149\u7167\uff08OS-SI\uff09\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\u8fdb\u884c\u53bb\u566a\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u56fe\u50cf\u6e05\u6670\u5ea6\u3002", "motivation": "\u4e24\u76f8OS-SI\u56e0\u91c7\u96c6\u65f6\u95f4\u7f29\u77ed\u4f1a\u4ea7\u751f\u6b8b\u4f59\u4f2a\u5f71\uff0c\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u6291\u5236\u3002\u6df1\u5ea6\u5b66\u4e60\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u76d1\u7763\u8bad\u7ec3\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5e72\u51c0\u7684\u5149\u5207\u771f\u5b9e\u6570\u636e\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u5c06\u771f\u5b9e\u4f2a\u5f71\u573a\u5e94\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u6765\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u4e86\u4e00\u4e2a\u975e\u5bf9\u79f0\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\u548c\u4e00\u4e2aU-Net\u7f51\u7edc\uff0c\u968f\u540e\u5728\u771f\u5b9eOS-SI\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e24\u79cd\u7f51\u7edc\u90fd\u63d0\u5347\u4e86\u56fe\u50cf\u6e05\u6670\u5ea6\uff0c\u4e14\u5404\u81ea\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u4f2a\u5f71\u6291\u5236\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u8bad\u7ec3\u80fd\u591f\u5b9e\u73b0OS-SI\u56fe\u50cf\u7684\u76d1\u7763\u53bb\u566a\uff0c\u5e76\u7a81\u51fa\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u5728\u7b80\u5316\u91cd\u5efa\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03265", "pdf": "https://arxiv.org/pdf/2510.03265", "abs": "https://arxiv.org/abs/2510.03265", "authors": ["Bowei Tian", "Yexiao He", "Wanghao Ye", "Ziyao Wang", "Meng Liu", "Ang Li"], "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large-scale foundation models demonstrate strong performance across language,\nvision, and reasoning tasks. However, how they internally structure and\nstabilize concepts remains elusive. Inspired by causal inference, we introduce\nthe MindCraft framework built upon Concept Trees. By applying spectral\ndecomposition at each layer and linking principal directions into branching\nConcept Paths, Concept Trees reconstruct the hierarchical emergence of\nconcepts, revealing exactly when they diverge from shared representations into\nlinearly separable subspaces. Empirical evaluations across diverse scenarios\nacross disciplines, including medical diagnosis, physics reasoning, and\npolitical decision-making, show that Concept Trees recover semantic\nhierarchies, disentangle latent concepts, and can be widely applied across\nmultiple domains. The Concept Tree establishes a widely applicable and powerful\nframework that enables in-depth analysis of conceptual representations in deep\nmodels, marking a significant step forward in the foundation of interpretable\nAI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMindCraft\u6846\u67b6\u548c\u6982\u5ff5\u6811\uff0c\u901a\u8fc7\u5bf9\u5927\u6a21\u578b\u6bcf\u5c42\u8fdb\u884c\u8c31\u5206\u89e3\uff0c\u63ed\u793a\u6982\u5ff5\u5982\u4f55\u4ece\u5171\u4eab\u8868\u793a\u4e2d\u5206\u5316\uff0c\u5f62\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6a21\u578b\u5185\u90e8\u6982\u5ff5\u8868\u793a\u7684\u6df1\u5ea6\u89e3\u91ca\u3002", "motivation": "\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5982\u4f55\u6784\u5efa\u548c\u7a33\u5b9a\u6982\u5ff5\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4ecd\u662f\u4e00\u4e2a\u6709\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u53d7\u56e0\u679c\u63a8\u65ad\u542f\u53d1\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u57fa\u4e8e\u6982\u5ff5\u6811\u7684MindCraft\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6a21\u578b\u6bcf\u4e00\u5c42\u5e94\u7528\u8c31\u5206\u89e3\uff0c\u5e76\u5c06\u4e3b\u65b9\u5411\u8fde\u63a5\u6210\u5206\u652f\u7684\u6982\u5ff5\u8def\u5f84\uff0c\u4ee5\u91cd\u5efa\u6982\u5ff5\u7684\u5c42\u6b21\u5316\u5f62\u6210\u8fc7\u7a0b\uff0c\u63ed\u793a\u5b83\u4eec\u4f55\u65f6\u4ece\u5171\u4eab\u8868\u793a\u4e2d\u5206\u5316\u4e3a\u7ebf\u6027\u53ef\u5206\u79bb\u7684\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u533b\u5b66\u8bca\u65ad\u3001\u7269\u7406\u63a8\u7406\u548c\u653f\u6cbb\u51b3\u7b56\u7b49\u591a\u4e2a\u5b66\u79d1\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6982\u5ff5\u6811\u80fd\u591f\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u3001\u89e3\u8026\u6f5c\u5728\u6982\u5ff5\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u8de8\u9886\u57df\u9002\u7528\u6027\u3002", "conclusion": "\u6982\u5ff5\u6811\u5efa\u7acb\u4e86\u4e00\u4e2a\u666e\u904d\u9002\u7528\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u8868\u793a\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u6807\u5fd7\u7740\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u7814\u7a76\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.03859", "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u7269\u8054\u7f51\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u667a\u80fd\u7535\u7f51\u548c\u533b\u7597\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u5173\u952e\u7269\u8054\u7f51\u7cfb\u7edf\u9700\u5feb\u901f\u53d1\u73b0\u5f02\u5e38\u4ee5\u786e\u4fdd\u5b89\u5168\u5e73\u7a33\u8fd0\u884c\uff0c\u4f46\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5728\u65e5\u76ca\u590d\u6742\u7684\uff08\u5982\u667a\u80fd\u533b\u7597\u3001\u80fd\u6e90\u7535\u7f51\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\uff09\u3001\u52a8\u6001\u3001\u9ad8\u7ef4\u3001\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u6301\u7eed\u6f14\u8fdb\u7684\u7cfb\u7edf\u4e2d\u9762\u4e34\u5c40\u9650\uff0c\u4e9f\u9700\u81ea\u9002\u5e94\u3001\u667a\u80fd\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5\uff0c\u8f85\u4ee5XAI\u4ee3\u7406\u6765\u589e\u5f3a\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u3001\u907f\u514d\u9010\u65f6\u95f4\u6b65\u5904\u7406\u7ec6\u8282\uff0c\u5e76\u4f7f\u7528\u5177\u6709\u8bed\u4e49\u7684\u5185\u5b58\u7f13\u51b2\u533a\u6765\u53d1\u73b0\u9690\u85cf\u6a21\u5f0f\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5728\u5b9e\u9645\u667a\u80fd\u7535\u7f51\u548c\u533b\u7597\u573a\u666f\u4eff\u771f\u4e2d\uff0c\u5c06\u6b64LLM\u589e\u5f3a\u6a21\u578b\u4e0e\u4f20\u7edf\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u8bef\u62a5\u7387\u3001\u7ed3\u679c\u53ef\u8bfb\u6027\u548c\u7cfb\u7edf\u54cd\u5e94\u901f\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5LLM\u589e\u5f3a\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u7269\u8054\u7f51\u672a\u6765\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u548c\u6f5c\u529b\u3002"}}
{"id": "2510.03683", "pdf": "https://arxiv.org/pdf/2510.03683", "abs": "https://arxiv.org/abs/2510.03683", "authors": ["Nisar Hussain", "Amna Qasim", "Gull Mehak", "Muhammad Zain", "Momina Hafeez", "Grigori Sidorov"], "title": "Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text", "categories": ["cs.CL"], "comment": "25 pages, 22 figures", "summary": "The use of derogatory terms in languages that employ code mixing, such as\nRoman Urdu, presents challenges for Natural Language Processing systems due to\nunstated grammar, inconsistent spelling, and a scarcity of labeled data. In\nthis work, we propose a QLoRA based fine tuning framework to improve offensive\nlanguage detection in Roman Urdu-English text. We translated the Roman\nUrdu-English code mixed dataset into English using Google Translate to leverage\nEnglish LLMs, while acknowledging that this translation reduces direct\nengagement with code mixing features. Our focus is on classification\nperformance using English translated low resource inputs. We fine tuned several\ntransformers and large language models, including Meta LLaMA 3 8B, Mistral 7B\nv0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient\nadaptation. Models were trained and evaluated on a manually annotated Roman\nUrdu dataset for offensive vs non offensive content. Of all tested models, the\nhighest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral\n7B at 89.66, surpassing traditional transformer baselines. These results\ndemonstrate the efficacy of QLoRA in fine tuning high performing models for low\nresource environments such as code mixed offensive language detection, and\nconfirm the potential of LLMs for this task. This work advances a scalable\napproach to Roman Urdu moderation and paves the way for future multilingual\noffensive detection systems based on LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eQLoRA\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed-\u82f1\u8bed\u6df7\u5408\u4ee3\u7801\u4e2d\u7684\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u3002\u901a\u8fc7\u5c06\u6570\u636e\u96c6\u7ffb\u8bd1\u6210\u82f1\u8bed\uff0c\u5229\u7528\u82f1\u8bedLLM\uff0cMeta LLaMA 3 8B\u8fbe\u5230\u4e8691.45%\u7684F1\u5206\u6570\uff0c\u8bc1\u5b9e\u4e86QLoRA\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u5fae\u8c03LLM\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u7b49\u6df7\u5408\u4ee3\u7801\u8bed\u8a00\u4e2d\uff0c\u8d2c\u4e49\u8bcd\u7684\u4f7f\u7528\u7ed9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u5e26\u6765\u4e86\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u8bed\u6cd5\u4e0d\u660e\u786e\u3001\u62fc\u5199\u4e0d\u4e00\u81f4\u4ee5\u53ca\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eQLoRA\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed-\u82f1\u8bed\u6587\u672c\u4e2d\u7684\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u3002\u6570\u636e\u96c6\u901a\u8fc7Google Translate\u7ffb\u8bd1\u6210\u82f1\u8bed\uff0c\u4ee5\u5229\u7528\u73b0\u6709\u7684\u82f1\u8bedLLM\u3002\u7814\u7a76\u5bf9Meta LLaMA 3 8B\u3001Mistral 7B v0.1\u3001LLaMA 2 7B\u3001ModernBERT\u548cRoBERTa\u7b49\u591a\u4e2aTransformer\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528QLoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u5192\u72af\u6027\u4e0e\u975e\u5192\u72af\u6027\u5185\u5bb9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\uff0cMeta LLaMA 3 8B\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u657091.45\uff0c\u7d27\u968f\u5176\u540e\u7684\u662fMistral 7B\uff0cF1\u5206\u6570\u4e3a89.66\uff0c\u4e24\u8005\u5747\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684Transformer\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eQLoRA\u5728\u4f4e\u8d44\u6e90\u73af\u5883\uff08\u5982\u6df7\u5408\u4ee3\u7801\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\uff09\u4e2d\u5fae\u8c03\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u8ba4\u4e86LLM\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002\u8be5\u5de5\u4f5c\u4e3a\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u5ba1\u6838\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00\u5192\u72af\u6027\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03455", "pdf": "https://arxiv.org/pdf/2510.03455", "abs": "https://arxiv.org/abs/2510.03455", "authors": ["Sejuti Majumder", "Saarthak Kapse", "Moinak Bhattacharya", "Xuan Xu", "Alisa Yurovsky", "Prateek Prasanna"], "title": "PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology", "categories": ["cs.CV"], "comment": null, "summary": "Integrating histopathology with spatial transcriptomics (ST) provides a\npowerful opportunity to link tissue morphology with molecular function. Yet\nmost existing multimodal approaches rely on a small set of highly variable\ngenes, which limits predictive scope and overlooks the coordinated biological\nprograms that shape tissue phenotypes. We present PEaRL (Pathway Enhanced\nRepresentation Learning), a multimodal framework that represents\ntranscriptomics through pathway activation scores computed with ssGSEA. By\nencoding biologically coherent pathway signals with a transformer and aligning\nthem with histology features via contrastive learning, PEaRL reduces\ndimensionality, improves interpretability, and strengthens cross-modal\ncorrespondence. Across three cancer ST datasets (breast, skin, and lymph node),\nPEaRL consistently outperforms SOTA methods, yielding higher accuracy for both\ngene- and pathway-level expression prediction (up to 58.9 percent and 20.4\npercent increase in Pearson correlation coefficient compared to SOTA). These\nresults demonstrate that grounding transcriptomic representation in pathways\nproduces more biologically faithful and interpretable multimodal models,\nadvancing computational pathology beyond gene-level embeddings.", "AI": {"tldr": "PEaRL\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u8def\u6fc0\u6d3b\u5206\u6570\u8868\u793a\u8f6c\u5f55\u7ec4\u5b66\u5e76\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5b66\u5bf9\u9f50\uff0c\u5728\u764c\u75c7\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7ed3\u5408\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u7684\u65b9\u6cd5\u591a\u4f9d\u8d56\u5c11\u6570\u9ad8\u53d8\u57fa\u56e0\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u8303\u56f4\uff0c\u4e14\u5ffd\u7565\u4e86\u7ec4\u7ec7\u8868\u578b\u80cc\u540e\u7684\u534f\u540c\u751f\u7269\u5b66\u7a0b\u5e8f\u3002", "method": "\u63d0\u51faPEaRL\uff08Pathway Enhanced Representation Learning\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7ssGSEA\u8ba1\u7b97\u901a\u8def\u6fc0\u6d3b\u5206\u6570\u6765\u8868\u793a\u8f6c\u5f55\u7ec4\u5b66\uff0c\u5e76\u4f7f\u7528Transformer\u7f16\u7801\u901a\u8def\u4fe1\u53f7\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u8fd9\u4e9b\u901a\u8def\u4fe1\u53f7\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5b66\u7279\u5f81\u5bf9\u9f50\uff0c\u4ee5\u964d\u4f4e\u7ef4\u5ea6\u3001\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u5e76\u52a0\u5f3a\u8de8\u6a21\u6001\u5bf9\u5e94\u3002", "result": "\u5728\u4e09\u4e2a\u764c\u75c7\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\uff08\u4e73\u817a\u764c\u3001\u76ae\u80a4\u764c\u3001\u6dcb\u5df4\u7ed3\u764c\uff09\u4e0a\uff0cPEaRL\u5728\u57fa\u56e0\u548c\u901a\u8def\u6c34\u5e73\u7684\u8868\u8fbe\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u5747\u6301\u7eed\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5206\u522b\u6700\u9ad8\u63d0\u534758.9%\u548c20.4%\u3002", "conclusion": "\u5c06\u8f6c\u5f55\u7ec4\u5b66\u8868\u793a\u4e0e\u751f\u7269\u901a\u8def\u76f8\u7ed3\u5408\uff0c\u80fd\u4ea7\u751f\u66f4\u7b26\u5408\u751f\u7269\u5b66\u539f\u7406\u4e14\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u63a8\u52a8\u8ba1\u7b97\u75c5\u7406\u5b66\u8d85\u8d8a\u57fa\u56e0\u6c34\u5e73\u7684\u5d4c\u5165\u3002"}}
{"id": "2510.03266", "pdf": "https://arxiv.org/pdf/2510.03266", "abs": "https://arxiv.org/abs/2510.03266", "authors": ["Bharat Sharma", "Jitendra Kumar"], "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "categories": ["cs.LG", "stat.ME", "stat.OT"], "comment": null, "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics,\nnecessitating robust methods for detecting and analyzing anomalous behavior in\nplant productivity. This study presents a novel application of variational\nautoencoders (VAE) for identifying extreme events in gross primary productivity\n(GPP) from Community Earth System Model version 2 simulations across four AR6\nregions in the Continental United States. We compare VAE-based anomaly\ndetection with traditional singular spectral analysis (SSA) methods across\nthree time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.\nThe VAE architecture employs three dense layers and a latent space with an\ninput sequence length of 12 months, trained on a normalized GPP time series to\nreconstruct the GPP and identifying anomalies based on reconstruction errors.\nExtreme events are defined using 5th percentile thresholds applied to both VAE\nand SSA anomalies. Results demonstrate strong regional agreement between VAE\nand SSA methods in spatial patterns of extreme event frequencies, despite VAE\nproducing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA\nacross regions and periods). Both methods reveal increasing magnitudes and\nfrequencies of negative carbon cycle extremes toward 2050-80, particularly in\nWestern and Central North America. The VAE approach shows comparable\nperformance to established SSA techniques, while offering computational\nadvantages and enhanced capability for capturing non-linear temporal\ndependencies in carbon cycle variability. Unlike SSA, the VAE method does not\nrequire one to define the periodicity of the signals in the data; it discovers\nthem from the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u65b0\u6027\u5730\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u68c0\u6d4b\u9646\u5730\u521d\u7ea7\u751f\u4ea7\u529b\uff08GPP\uff09\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4e0e\u5947\u5f02\u8c31\u5206\u6790\uff08SSA\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u8bc6\u522b\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u9884\u6d4b\u672a\u6765\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u5c06\u589e\u52a0\u3002VAE\u5728\u5904\u7406\u975e\u7ebf\u6027\u4f9d\u8d56\u65b9\u9762\u5c55\u73b0\u51fa\u8ba1\u7b97\u4f18\u52bf\u548c\u66f4\u5f3a\u80fd\u529b\u3002", "motivation": "\u6c14\u5019\u5f02\u5e38\u5bf9\u9646\u5730\u78b3\u5faa\u73af\u52a8\u6001\u5f71\u54cd\u663e\u8457\uff0c\u56e0\u6b64\u6025\u9700\u5f00\u53d1\u7a33\u5065\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u690d\u7269\u751f\u4ea7\u529b\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5e94\u7528\u4e8e\u793e\u7fa4\u5730\u7403\u7cfb\u7edf\u6a21\u578b\uff08CESM2\uff09\u6a21\u62df\u7684\u7f8e\u56fd\u5927\u9646\u56db\u4e2aAR6\u533a\u57df\u7684GPP\u6570\u636e\uff0c\u4ee5\u8bc6\u522b\u6781\u7aef\u4e8b\u4ef6\u3002VAE\u67b6\u6784\u5305\u542b\u4e09\u4e2a\u5bc6\u96c6\u5c42\u548c\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a12\u4e2a\u6708\uff0c\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u8bc6\u522b\u5f02\u5e38\u3002\u7814\u7a76\u5c06VAE\u4e0e\u4f20\u7edf\u5947\u5f02\u8c31\u5206\u6790\uff08SSA\uff09\u65b9\u6cd5\u57281850-80\u30011950-80\u548cSSP585\u60c5\u666f\u4e0b\u76842050-80\u4e09\u4e2a\u65f6\u95f4\u6bb5\u8fdb\u884c\u6bd4\u8f83\u3002\u6781\u7aef\u4e8b\u4ef6\u5b9a\u4e49\u4e3a\u5f02\u5e38\u76845%\u9608\u503c\u3002", "result": "VAE\u4e0eSSA\u65b9\u6cd5\u5728\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u533a\u57df\u4e00\u81f4\u6027\uff0c\u5c3d\u7ba1VAE\u4ea7\u751f\u7684\u9608\u503c\u66f4\u9ad8\uff08VAE\u4e3a179-756 GgC\uff0cSSA\u4e3a100-784 GgC\uff09\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u663e\u793a\uff0c\u52302050-80\u5e74\uff0c\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u7684\u5f3a\u5ea6\u548c\u9891\u7387\u5c06\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u5317\u7f8e\u897f\u90e8\u548c\u4e2d\u90e8\u3002", "conclusion": "VAE\u65b9\u6cd5\u8868\u73b0\u51fa\u4e0eSSA\u6280\u672f\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6355\u83b7\u78b3\u5faa\u73af\u53d8\u5f02\u4e2d\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u80fd\u529b\u3002\u4e0eSSA\u4e0d\u540c\uff0cVAE\u65b9\u6cd5\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u6570\u636e\u4e2d\u4fe1\u53f7\u7684\u5468\u671f\u6027\uff0c\u800c\u662f\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u81ea\u52a8\u53d1\u73b0\u5b83\u4eec\u3002"}}
{"id": "2510.03863", "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "categories": ["cs.AI", "cs.CR"], "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI.", "AI": {"tldr": "\u63d0\u51faSpatial CAPTCHA\uff0c\u5229\u7528\u4eba\u7c7b\u4e0eMLLM\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u5dee\u5f02\uff0c\u6709\u6548\u5bf9\u6297\u5148\u8fdbAI\uff0c\u4eba\u7c7b\u8868\u73b0\u8fdc\u8d85MLLM\u3002", "motivation": "\u73b0\u6709CAPTCHA\u56e0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8fdb\u6b65\u800c\u5931\u6548\uff0c\u65e0\u6cd5\u6709\u6548\u9632\u5fa1\u81ea\u52a8\u5316\u6ee5\u7528\u3002", "method": "\u63d0\u51faSpatial CAPTCHA\uff0c\u5229\u7528\u4eba\u7c7b\u548cMLLM\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u3002\u751f\u6210\u52a8\u6001\u95ee\u9898\uff0c\u8981\u6c42\u51e0\u4f55\u63a8\u7406\u3001\u900f\u89c6\u3001\u906e\u6321\u5904\u7406\u548c\u5fc3\u7406\u65cb\u8f6c\u3002\u7cfb\u7edf\u91c7\u7528\u7a0b\u5e8f\u5316\u751f\u6210\u7ba1\u9053\uff0c\u5177\u5907\u96be\u5ea6\u63a7\u5236\u3001\u81ea\u52a8\u9a8c\u8bc1\u548c\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5728Spatial-CAPTCHA-Bench\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u4eba\u7c7b\u8868\u73b0\u8fdc\u8d8510\u4e2aSOTA MLLMs\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec531.0%\u3002\u4e0eGoogle reCAPTCHA\u5bf9\u6bd4\uff0c\u8bc1\u5b9e\u4e86\u5176\u4f5c\u4e3a\u5b89\u5168\u673a\u5236\u548cAI\u7a7a\u95f4\u63a8\u7406\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "Spatial CAPTCHA\u662f\u4e00\u79cd\u6709\u6548\u7684\u4eba\u673a\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u62b5\u5fa1\u73b0\u4ee3AI\uff0c\u5e76\u53ef\u4f5c\u4e3a\u8bc4\u4f30AI\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03687", "pdf": "https://arxiv.org/pdf/2510.03687", "abs": "https://arxiv.org/abs/2510.03687", "authors": ["Yue Huang", "Yanyuan Chen", "Dexuan Xu", "Weihua Yue", "Huamin Zhang", "Meikang Qiu", "Yu Huang"], "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical problem solving demands expert knowledge and intricate reasoning.\nRecent studies of large language models (LLMs) attempt to ease this complexity\nby introducing external knowledge verification through retrieval-augmented\ngeneration or by training on reasoning datasets. However, these approaches\nsuffer from drawbacks such as retrieval overhead and high annotation costs, and\nthey heavily rely on substituted external assistants to reach limited\nperformance in medical field. In this paper, we introduce MedReflect, a\ngeneralizable framework designed to inspire LLMs with a physician-like\nreflective thinking mode. MedReflect generates a single-pass reflection chain\nthat includes initial hypothesis generation, self-questioning, self-answering\nand decision refinement. This self-verified and self-reflective nature releases\nlarge language model's latent capability in medical problem-solving without\nexternal retrieval or heavy annotation. We demonstrate that MedReflect enables\ncost-efficient medical dataset construction: with merely 2,000 randomly sampled\ntraining examples and a light fine-tuning, this approach achieves notable\nabsolute accuracy improvements across a series of medical benchmarks while\ncutting annotation requirements. Our results provide evidence that LLMs can\nlearn to solve specialized medical problems via self-reflection and\nself-improve, reducing reliance on external supervision and extensive\ntask-specific fine-tuning data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMedReflect\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u533b\u751f\u53cd\u601d\u5f0f\u601d\u7ef4\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u9700\u5916\u90e8\u8f85\u52a9\u6216\u5927\u91cf\u6807\u6ce8\u5373\u53ef\u9ad8\u6548\u89e3\u51b3\u533b\u5b66\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u95ee\u9898\u89e3\u51b3\u590d\u6742\uff0c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65b9\u6cd5\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u3001\u63a8\u7406\u6570\u636e\u96c6\u8bad\u7ec3\uff09\u5b58\u5728\u68c0\u7d22\u5f00\u9500\u5927\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u6027\u80fd\u53d7\u9650\u7b49\u7f3a\u70b9\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u8f85\u52a9\u3002", "method": "MedReflect\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5355\u901a\u9053\u53cd\u601d\u94fe\uff0c\u5305\u62ec\u521d\u59cb\u5047\u8bbe\u751f\u6210\u3001\u81ea\u6211\u63d0\u95ee\u3001\u81ea\u6211\u56de\u7b54\u548c\u51b3\u7b56\u5b8c\u5584\uff0c\u5b9e\u73b0LLM\u7684\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u53cd\u601d\uff0c\u91ca\u653e\u5176\u5728\u533b\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u5927\u91cf\u6807\u6ce8\u3002", "result": "MedReflect\u4ec5\u75282000\u4e2a\u968f\u673a\u91c7\u6837\u7684\u8bad\u7ec3\u793a\u4f8b\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u5373\u53ef\u5b9e\u73b0\u6210\u672c\u9ad8\u6548\u7684\u533b\u5b66\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5e76\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u7684\u7edd\u5bf9\u51c6\u786e\u6027\u63d0\u5347\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6807\u6ce8\u9700\u6c42\u3002", "conclusion": "LLMs\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u6539\u8fdb\u6765\u89e3\u51b3\u4e13\u4e1a\u7684\u533b\u5b66\u95ee\u9898\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u548c\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.03483", "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52", "AI": {"tldr": "DuPLUS\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u548c\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u5206\u5272\u548c\u9884\u540e\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u3001\u7f3a\u4e4f\u9884\u540e\u80fd\u529b\u3001\u5bf9\u533b\u5b66\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u4ee5\u53ca\u201c\u901a\u7528\u201d\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5355\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002", "method": "DuPLUS\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u5b9e\u73b0\u5bf9\u5206\u6790\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u5b83\u91c7\u7528\u4e86\u4e00\u4e2a\u7531\u72ec\u7279\u7684\u53cc\u63d0\u793a\u673a\u5236\u9a71\u52a8\u7684\u3001\u6587\u672c\u63a7\u5236\u7684\u5206\u5c42\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u7684\u6269\u5c55\u6027\u3002\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u548c\u6a21\u6001\u7684\u5feb\u901f\u9002\u5e94\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u53ef\u65e0\u7f1d\u96c6\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u8fdb\u884c\u9884\u540e\u9884\u6d4b\u3002", "result": "\u5728\u5206\u5272\u4efb\u52a1\u4e0a\uff0cDuPLUS\u80fd\u591f\u6cdb\u5316\u5230\u4e09\u79cd\u6210\u50cf\u6a21\u6001\u3001\u5341\u4e2a\u89e3\u5256\u5b66\u4e0a\u4e0d\u540c\u7684\u533b\u5b66\u6570\u636e\u96c6\uff08\u6db5\u76d630\u591a\u79cd\u5668\u5b98\u548c\u80bf\u7624\u7c7b\u578b\uff09\uff0c\u5e76\u572810\u4e2a\u6570\u636e\u96c6\u4e2d\u67098\u4e2a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u548c\u901a\u7528\u6a21\u578b\u3002\u5728\u5934\u9888\u764c\u6570\u636e\u96c6\u4e0a\uff0cDuPLUS\u901a\u8fc7\u96c6\u6210EHR\u6570\u636e\u8fdb\u884c\u9884\u540e\u9884\u6d4b\uff0c\u83b7\u5f97\u4e860.69\u7684\u4e00\u81f4\u6027\u6307\u6570\uff08CI\uff09\u3002", "conclusion": "DuPLUS\u901a\u8fc7\u5176\u591a\u6a21\u6001\u5206\u6790\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u3001\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5feb\u901f\u9002\u5e94\u6027\uff0c\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03267", "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM.", "AI": {"tldr": "PT$^2$-LLM\uff1a\u4e00\u79cd\u9488\u5bf9LLMs\u7684\u8bad\u7ec3\u540e\u4e09\u503c\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u91cf\u5316\u548c\u7ed3\u6784\u91cd\u6392\u5e8f\uff0c\u5728\u964d\u4f4e\u5185\u5b58\u548c\u52a0\u901f\u63a8\u7406\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5de8\u5927\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u963b\u788d\u4e86\u90e8\u7f72\u3002\u5c3d\u7ba1\u4e09\u503c\u5316\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u538b\u7f29\u6280\u672f\uff0c\u4f46\u5728\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u8bbe\u7f6e\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u8bad\u7ec3\u53c2\u6570\u4f18\u5316\u3001\u5f02\u5e38\u503c\u548c\u6743\u91cd\u5206\u6563\u7b49\u95ee\u9898\uff0c\u5176\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u5f00\u53d1\u3002", "method": "\u63d0\u51faPT$^2$-LLM\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u975e\u5bf9\u79f0\u4e09\u503c\u91cf\u5316\u5668\uff0c\u914d\u5907\u4e24\u9636\u6bb5\u7ec6\u5316\uff1a1) \u8fed\u4ee3\u4e09\u503c\u62df\u5408\uff08ITF\uff09\uff0c\u7528\u4e8e\u6784\u5efa\u6700\u4f18\u4e09\u503c\u7f51\u683c\u548c\u7075\u6d3b\u820d\u5165\u4ee5\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\uff1b2) \u6fc0\u6d3b\u611f\u77e5\u7f51\u683c\u5bf9\u9f50\uff08AGA\uff09\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u7f51\u683c\u4ee5\u5339\u914d\u5168\u7cbe\u5ea6\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u7ed3\u6784\u76f8\u4f3c\u6027\u91cd\u6392\u5e8f\uff08SSR\uff09\u7b56\u7565\u6765\u7b80\u5316\u91cf\u5316\u5e76\u7f13\u89e3\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "PT$^2$-LLM\u5728\u4e0e\u6700\u5148\u8fdb\u76842\u6bd4\u7279PTQ\u65b9\u6cd5\u76f8\u6bd4\u65f6\uff0c\u80fd\u4ee5\u66f4\u4f4e\u7684\u5185\u5b58\u6210\u672c\u63d0\u4f9b\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u52a0\u901f\u9884\u586b\u5145\u548c\u89e3\u7801\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "PT$^2$-LLM\u901a\u8fc7\u521b\u65b0\u7684\u91cf\u5316\u5668\u548c\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLMs\u8bad\u7ec3\u540e\u4e09\u503c\u5316\u9762\u4e34\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u52a0\u901f\u63a8\u7406\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.03886", "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "categories": ["cs.AI"], "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface.", "AI": {"tldr": "\u9488\u5bf9MM-DiTs\u5728\u751f\u6210\u7f55\u89c1\u6982\u5ff5\u65f6\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8054\u5408\u6ce8\u610f\u529b\u5757\u524d\u6269\u5927\u6587\u672c\u5d4c\u5165\u7684\u8868\u793a\u57df\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u751f\u6210\u7f55\u89c1\u8bed\u4e49\u7684\u80fd\u529b\uff0c\u5e76\u6cdb\u5316\u81f3\u591a\u79cd\u6587\u672c\u5230\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u591a\u6a21\u6001\u6269\u6563Transformer (MM-DiTs) \u5728\u5904\u7406\u7528\u6237\u63d0\u51fa\u7684\u65b0\u9896\u6216\u7f55\u89c1\u63d0\u793a\u65f6\u8868\u73b0\u4e0d\u6b20\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6982\u5ff5\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u901a\u5e38\u7a00\u7f3a\uff0c\u672a\u80fd\u7559\u4e0b\u8db3\u591f\u5f3a\u7684\u5370\u8bb0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u6b65\u9aa4\u3001\u6570\u636e\u3001\u53bb\u566a\u65f6\u95f4\u4f18\u5316\u6216\u5bf9\u5916\u90e8\u6a21\u5757\u7684\u4f9d\u8d56\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff0c\u5728MM-DiT\u7684\u8054\u5408\u6ce8\u610f\u529b\u5757\u4e4b\u524d\uff0c\u901a\u8fc7\u589e\u52a0\u6587\u672ctoken\u5d4c\u5165\u7684\u65b9\u5dee\u6765\u6570\u5b66\u6027\u5730\u6269\u5927\u5176\u8868\u793a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u4f7fMM-DiT\u7684\u8f93\u51fa\u4e2d\u6e05\u6670\u5730\u6d6e\u73b0\u7f55\u89c1\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u7814\u7a76\u7ed3\u679c\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u6587\u672c\u5230\u89c6\u9891\u548c\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u7b49\u591a\u79cd\u6587\u672c\u5230\u89c6\u89c9\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4fc3\u4f7f\u751f\u6210\u6a21\u578b\u80fd\u591f\u63ed\u793a\u7528\u6237\u610f\u56fe\u7684\u3001\u539f\u672c\u9690\u85cf\u4f46\u5df2\u51c6\u5907\u597d\u6d6e\u73b0\u7684\u8bed\u4e49\u3002"}}
{"id": "2510.03748", "pdf": "https://arxiv.org/pdf/2510.03748", "abs": "https://arxiv.org/abs/2510.03748", "authors": ["Ramtin Kakavand", "Ebrahim Ansari"], "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have consistently demonstrated strong\nperformance in machine translation, especially when guided by high-quality\nprompts. Few-shot prompting is an effective technique to improve translation\nquality; however, most existing example selection methods focus solely on\nquery-to-example similarity and do not account for the quality of the examples.\nIn this work, we propose TreePrompt, a novel example selection approach that\nlearns LLM preferences to identify high-quality, contextually relevant examples\nwithin a tree-structured framework. To further explore the balance between\nsimilarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)\nand Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -\nEnglish-Persian (MIZAN) and English-German (WMT19) - show that integrating\nTreePrompt with AFSP or Random selection leads to improved translation\nperformance.", "AI": {"tldr": "\u63d0\u51faTreePrompt\uff0c\u4e00\u79cd\u65b0\u7684\u5c11\u6837\u672c\u63d0\u793a\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u504f\u597d\uff0c\u5728\u6811\u7ed3\u6784\u4e2d\u8bc6\u522b\u9ad8\u8d28\u91cf\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u793a\u4f8b\uff0c\u4ee5\u6539\u8fdb\u673a\u5668\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u63d0\u793a\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u4e0e\u793a\u4f8b\u7684\u76f8\u4f3c\u6027\uff0c\u4f46\u672a\u80fd\u5145\u5206\u8003\u8651\u793a\u4f8b\u672c\u8eab\u7684\u8d28\u91cf\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u5728\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TreePrompt\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u3002\u5b83\u5728\u4e00\u4e2a\u6811\u5f62\u6846\u67b6\u5185\uff0c\u901a\u8fc7\u5b66\u4e60LLM\u7684\u504f\u597d\u6765\u8bc6\u522b\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u793a\u4f8b\u3002\u4e3a\u63a2\u7d22\u76f8\u4f3c\u6027\u548c\u8d28\u91cf\u7684\u5e73\u8861\uff0c\u5c06TreePrompt\u4e0eK-Nearest Neighbors (K-NN)\u548cAdaptive Few-Shot Prompting (AFSP)\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u82f1-\u6ce2\u65af\u8bed\uff08MIZAN\uff09\u548c\u82f1-\u5fb7\u8bed\uff08WMT19\uff09\u4e24\u79cd\u8bed\u8a00\u5bf9\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5728\u82f1-\u6ce2\u65af\u8bed\uff08MIZAN\uff09\u548c\u82f1-\u5fb7\u8bed\uff08WMT19\uff09\u4e24\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5c06TreePrompt\u4e0eAFSP\u6216\u968f\u673a\u9009\u62e9\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\uff0c\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "TreePrompt\u901a\u8fc7\u5f15\u5165\u5bf9\u793a\u4f8b\u8d28\u91cf\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u8003\u8651\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u793a\u4f8b\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7b56\u7565\uff0c\u5c24\u5176\u5728\u4e0eAFSP\u7ed3\u5408\u65f6\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2510.03501", "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u79fb\u52a8\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u7a0b\u5316\u5e76\u884cYOLOv10\u68c0\u6d4b\u548cMobileSAM\u5206\u5272\uff0c\u5b9e\u73b0\u91ce\u5916\u52a8\u7269\u7684\u5b9e\u65f6\u3001\u9ad8\u6548\u68c0\u6d4b\u4e0e\u5206\u5272\uff0c\u5e76\u5728\u6fd2\u5371\u7269\u79cd\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u91ce\u5916\u73af\u5883\u4e2d\u52a8\u7269\u7684\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u5206\u5272\u5bf9\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u7269\u79cd\u4f2a\u88c5\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u975e\u4fb5\u5165\u5f0f\u76d1\u6d4b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u79fb\u52a8\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u7ebf\u7a0b\u5316\u68c0\u6d4b\u6a21\u578b\uff08TDM\uff09\uff0c\u5e76\u884c\u6267\u884c\u57fa\u4e8eYOLOv10\u7684\u68c0\u6d4b\u548c\u57fa\u4e8eMobileSAM\u7684\u8f7b\u91cf\u7ea7\u5206\u5272\uff0c\u901a\u8fc7\u7ebf\u7a0b\u5316\u964d\u4f4e\u5ef6\u8fdf\u4ee5\u63d0\u9ad8\u5b9e\u65f6\u6027\u80fd\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b4\u4e07\u5f20\u5e26\u6ce8\u91ca\u56fe\u50cf\u7684Houbara Bustard\u6570\u636e\u96c6\u3002", "result": "\u5728Houbara Bustard\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578bmAP50\u8fbe\u52300.9627\uff0cmAP75\u8fbe\u52300.7731\uff0cmAP95\u8fbe\u52300.7178\uff0cMobileSAM\u7684mIoU\u4e3a0.7421\u3002YOLOv10\u6bcf\u5e27\u8fd0\u884c\u65f6\u95f4\u4e3a43.7\u6beb\u79d2\uff0c\u8bc1\u5b9e\u4e86\u5b9e\u65f6\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7ebf\u7a0b\u5316\u5e76\u884c\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u91ce\u5916\u52a8\u7269\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u5272\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03268", "pdf": "https://arxiv.org/pdf/2510.03268", "abs": "https://arxiv.org/abs/2510.03268", "authors": ["Lingjie Yi", "Raphael Douady", "Chao Chen"], "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal contrastive learning (MCL) aims to embed data from different\nmodalities in a shared embedding space. However, empirical evidence shows that\nrepresentations from different modalities occupy completely separate regions of\nembedding space, a phenomenon referred to as the modality gap. Moreover,\nexperimental findings on how the size of the modality gap influences downstream\nperformance are inconsistent. These observations raise two key questions: (1)\nWhat causes the modality gap? (2) How does it affect downstream tasks? To\naddress these questions, this paper introduces the first theoretical framework\nfor analyzing the convergent optimal representations of MCL and the modality\nalignment when training is optimized. Specifically, we prove that without any\nconstraint or under the cone constraint, the modality gap converges to zero.\nUnder the subspace constraint (i.e., representations of two modalities fall\ninto two distinct hyperplanes due to dimension collapse), the modality gap\nconverges to the smallest angle between the two hyperplanes. This result\nidentifies \\emph{dimension collapse} as the fundamental origin of the modality\ngap. Furthermore, our theorems demonstrate that paired samples cannot be\nperfectly aligned under the subspace constraint. The modality gap influences\ndownstream performance by affecting the alignment between sample pairs. We\nprove that, in this case, perfect alignment between two modalities can still be\nachieved via two ways: hyperplane rotation and shared space projection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u5206\u6790\u4e86\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u6307\u51fa\u7ef4\u5ea6\u574d\u584c\u662f\u5176\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u6d88\u9664\u8be5\u95f4\u9699\u7684\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65e8\u5728\u5c06\u4e0d\u540c\u6a21\u6001\u6570\u636e\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\uff0c\u4f46\u5b58\u5728\u6a21\u6001\u95f4\u9699\u73b0\u8c61\uff08\u4e0d\u540c\u6a21\u6001\u8868\u793a\u5360\u636e\u72ec\u7acb\u533a\u57df\uff09\uff0c\u4e14\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u6a21\u6001\u95f4\u9699\u7684\u6210\u56e0\u53ca\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u9996\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u7684\u6536\u655b\u6700\u4f18\u8868\u793a\u548c\u6a21\u6001\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65e0\u7ea6\u675f\u6216\u9525\u5f62\u7ea6\u675f\u4e0b\u6a21\u6001\u95f4\u9699\u6536\u655b\u4e3a\u96f6\uff1b\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\uff08\u5373\u7ef4\u5ea6\u574d\u584c\u5bfc\u81f4\u8868\u793a\u843d\u5165\u4e0d\u540c\u8d85\u5e73\u9762\uff09\u6a21\u6001\u95f4\u9699\u6536\u655b\u4e3a\u4e24\u8d85\u5e73\u9762\u95f4\u7684\u6700\u5c0f\u89d2\u5ea6\uff0c\u4ece\u800c\u8bc6\u522b\u51fa\u201c\u7ef4\u5ea6\u574d\u584c\u201d\u662f\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\u3002\u6a21\u6001\u95f4\u9699\u901a\u8fc7\u5f71\u54cd\u6837\u672c\u5bf9\u9f50\u6765\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\uff0c\u53ef\u901a\u8fc7\u8d85\u5e73\u9762\u65cb\u8f6c\u548c\u5171\u4eab\u7a7a\u95f4\u6295\u5f71\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u5b8c\u7f8e\u5bf9\u9f50\u3002", "conclusion": "\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\u7ef4\u5ea6\u574d\u584c\uff0c\u5b83\u901a\u8fc7\u5f71\u54cd\u6837\u672c\u5bf9\u9f50\u6765\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u5b9e\u73b0\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.03892", "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6e38\u620f\u5316\u53ef\u89e3\u91caAI (XAI) \u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u6d88\u8d39\u8005\u5728\u5496\u5561\u9886\u57df\u505a\u51fa\u7b26\u5408\u9053\u5fb7\u7684\u51b3\u7b56\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u5eb7\u5fb7\u4e3b\u4e49\u89c4\u5219\u68c0\u67e5\u548c\u529f\u5229\u4e3b\u4e49\u591a\u6807\u51c6\u8bc4\u5206\u3002", "motivation": "\u65e8\u5728\u5e2e\u52a9\u5177\u6709\u9053\u5fb7\u610f\u8bc6\u7684\u6d88\u8d39\u8005\u5728\u590d\u6742\u7684\u5496\u5561\u5e02\u573a\u4e2d\u505a\u51fa\u660e\u667a\u3001\u7b26\u5408\u9053\u5fb7\u7684\u51b3\u7b56\uff0c\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684AI\u6d1e\u5bdf\u6765\u63ed\u793a\u4ea7\u54c1\u80cc\u540e\u7684\u4f26\u7406\u5f71\u54cd\u548c\u6743\u8861\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u4e24\u4e2a\u7b26\u53f7\u5f15\u64ce\uff1a\u4e00\u4e2a\u5eb7\u5fb7\u4e3b\u4e49\u6a21\u5757\u6807\u8bb0\u8fdd\u53cd\u9053\u5fb7\u89c4\u5219\u7684\u884c\u4e3a\uff08\u5982\u7ae5\u5de5\u3001\u6bc1\u6797\u98ce\u9669\uff09\uff0c\u4e00\u4e2a\u529f\u5229\u4e3b\u4e49\u6a21\u5757\u901a\u8fc7\u6807\u51c6\u5316\u5c5e\u6027\uff08\u4ef7\u683c\u3001\u78b3\u8db3\u8ff9\u3001\u6c34\u8d44\u6e90\u3001\u900f\u660e\u5ea6\u3001\u519c\u6c11\u6536\u5165\u7b49\uff09\u7684\u591a\u6807\u51c6\u805a\u5408\u6765\u8bc4\u4f30\u9009\u9879\u3002\u4e00\u4e2a\u5143\u89e3\u91ca\u5668\u4f1a\u7a81\u51fa\u5eb7\u5fb7\u4e3b\u4e49\u4e0e\u529f\u5229\u4e3b\u4e49\u7684\uff08\u4e0d\uff09\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u798f\u5229\u635f\u5931\u8f83\u5c0f\u65f6\uff0c\u63a8\u8350\u9053\u5fb7\u4e0a\u201c\u5e72\u51c0\u201d\u7684\u8fd1\u4f3c\u5e73\u4ef7\u9009\u9879\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u529f\u80fd\u6027\u7684\u6e38\u620f\u5316XAI\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5b9e\u65f6\u9053\u5fb7\u89e3\u91ca\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u7ed3\u6784\u5316\u914d\u7f6e\uff08\u5c5e\u6027\u6a21\u5f0f\u3001\u8ba4\u8bc1\u56fe\u3001\u6743\u91cd\u3001\u89c4\u5219\u96c6\uff09\u3001\u7528\u4e8e\u53ef\u5ba1\u8ba1\u6027\u7684\u7b56\u7565\u8ffd\u8e2a\u4ee5\u53ca\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e38\u620f\u5316\u3001\u53ef\u89e3\u91ca\u7684AI\uff0c\u7ed3\u5408\u4f26\u7406\u89c4\u5219\u548c\u6548\u7528\u8bc4\u4f30\u6765\u6307\u5bfc\u5496\u5561\u9886\u57df\u7684\u9053\u5fb7\u6d88\u8d39\u51b3\u7b56\uff0c\u5e76\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03758", "pdf": "https://arxiv.org/pdf/2510.03758", "abs": "https://arxiv.org/abs/2510.03758", "authors": ["Ilias Tougui", "Mehdi Zakroum", "Mounir Ghogho"], "title": "Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Parkinson's Disease (PD) affects over 10 million people worldwide, with\nspeech impairments in up to 89% of patients. Current speech-based detection\nsystems analyze entire utterances, potentially overlooking the diagnostic value\nof specific phonetic elements. We developed a granularity-aware approach for\nmultilingual PD detection using an automated pipeline that extracts\ntime-aligned phonemes, syllables, and words from recordings. Using Italian,\nSpanish, and English datasets, we implemented a bidirectional LSTM with\nmulti-head attention to compare diagnostic performance across the different\ngranularity levels. Phoneme-level analysis achieved superior performance with\nAUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates\nenhanced diagnostic capability for cross-linguistic PD detection. Importantly,\nattention analysis revealed that the most informative speech features align\nwith those used in established clinical protocols: sustained vowels (/a/, /e/,\n/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)\nat syllable level, and /pataka/ sequences at word level. Source code will be\navailable at https://github.com/jetliqs/clearpd.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u7c92\u5ea6\u611f\u77e5\u7684\u5e15\u91d1\u68ee\u75c5\u8bed\u97f3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u97f3\u7d20\u7ea7\u5206\u6790\u8868\u73b0\u6700\u4f73\uff08AUROC 93.78%\uff09\uff0c\u4e14\u5173\u952e\u4fe1\u606f\u7279\u5f81\u4e0e\u4e34\u5e8a\u534f\u8bae\u76f8\u7b26\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u8bed\u97f3\u969c\u788d\u666e\u904d\uff0c\u4f46\u73b0\u6709\u8bed\u97f3\u68c0\u6d4b\u7cfb\u7edf\u5e38\u5ffd\u7565\u7279\u5b9a\u8bed\u97f3\u5143\u7d20\u7684\u8bca\u65ad\u4ef7\u503c\uff0c\u4ec5\u5206\u6790\u5b8c\u6574\u8bed\u53e5\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4ece\u5f55\u97f3\u4e2d\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u7684\u97f3\u7d20\u3001\u97f3\u8282\u548c\u5355\u8bcd\u7279\u5f81\u3002\u91c7\u7528\u53cc\u5411LSTM\u4e0e\u591a\u5934\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5728\u610f\u5927\u5229\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e0d\u540c\u7c92\u5ea6\u5206\u6790\u7684\u8bca\u65ad\u6027\u80fd\u3002", "result": "\u97f3\u7d20\u7ea7\u5206\u6790\u8868\u73b0\u6700\u4f73\uff0cAUROC\u4e3a93.78% \u00b1 2.34%\uff0c\u51c6\u786e\u7387\u4e3a92.17% \u00b1 2.43%\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\uff0c\u6700\u5177\u4fe1\u606f\u91cf\u7684\u8bed\u97f3\u7279\u5f81\uff08\u5982\u97f3\u7d20\u7ea7\u7684\u6301\u7eed\u5143\u97f3\u3001\u97f3\u8282\u7ea7\u7684\u4ea4\u66ff\u8fd0\u52a8\u97f3\u8282\uff09\u4e0e\u73b0\u6709\u4e34\u5e8a\u534f\u8bae\u4e00\u81f4\u3002", "conclusion": "\u7c92\u5ea6\u611f\u77e5\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u97f3\u7d20\u7ea7\u5206\u6790\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u5e15\u91d1\u68ee\u75c5\u8bed\u97f3\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63ed\u793a\u4e86\u4e0e\u4e34\u5e8a\u5b9e\u8df5\u76f8\u7b26\u7684\u5173\u952e\u8bca\u65ad\u7279\u5f81\u3002"}}
{"id": "2510.03511", "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. S\u00e1nchez", "Sharvaree Vadgama", "Georg B\u00f6kman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Platonic Transformer\uff0c\u901a\u8fc7\u5f15\u5165\u67cf\u62c9\u56fe\u56fa\u4f53\u5bf9\u79f0\u7fa4\u53c2\u8003\u7cfb\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u548c\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u8d4b\u4e88Transformer\u51e0\u4f55\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u5728\u591a\u9886\u57df\u53d6\u5f97\u7ade\u4e89\u529b\u3002", "motivation": "\u901a\u7528Transformer\u7f3a\u4e4f\u5bf9\u51e0\u4f55\u5bf9\u79f0\u6027\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u800c\u73b0\u6709\u7b49\u53d8\u65b9\u6cd5\u5e38\u4ee5\u727a\u7272Transformer\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u4e3a\u4ee3\u4ef7\uff0c\u8bbe\u8ba1\u590d\u6742\u4e14\u8ba1\u7b97\u5bc6\u96c6\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u4e0e\u7b49\u53d8\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u5f15\u5165Platonic Transformer\uff0c\u901a\u8fc7\u5b9a\u4e49\u76f8\u5bf9\u4e8e\u67cf\u62c9\u56fe\u56fa\u4f53\u5bf9\u79f0\u7fa4\u53c2\u8003\u7cfb\u7684\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e00\u79cd\u539f\u5219\u6027\u7684\u6743\u91cd\u5171\u4eab\u65b9\u6848\u3002\u8be5\u6ce8\u610f\u529b\u5f62\u5f0f\u4e0a\u7b49\u4ef7\u4e8e\u52a8\u6001\u7fa4\u5377\u79ef\uff0c\u6a21\u578b\u5b66\u4e60\u81ea\u9002\u5e94\u51e0\u4f55\u6ee4\u6ce2\u5668\uff0c\u5e76\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u5377\u79ef\u53d8\u4f53\u3002", "result": "Platonic Transformer\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u5e73\u79fb\u548c\u67cf\u62c9\u56fe\u5bf9\u79f0\u6027\u7684\u7ec4\u5408\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6807\u51c6Transformer\u7684\u67b6\u6784\u548c\u8ba1\u7b97\u6210\u672c\u3002\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CIFAR-10\uff09\u30013D\u70b9\u4e91\uff08ScanObjectNN\uff09\u548c\u5206\u5b50\u6027\u8d28\u9884\u6d4b\uff08QM9, OMol25\uff09\u7b49\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e0d\u589e\u52a0\u989d\u5916\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Platonic Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3Transformer\u51e0\u4f55\u5bf9\u79f0\u6027\u4e0d\u8db3\u7684\u65b9\u6cd5\uff0c\u5b83\u5728\u4fdd\u6301\u6548\u7387\u548c\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u5f3a\u5927\u7684\u51e0\u4f55\u7ea6\u675f\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2510.03269", "pdf": "https://arxiv.org/pdf/2510.03269", "abs": "https://arxiv.org/abs/2510.03269", "authors": ["Wendi Li", "Changdae Oh", "Yixuan Li"], "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Optimistic exploration is central to improving sample efficiency in\nreinforcement learning with human feedback, yet existing exploratory bonus\nmethods to incentivize exploration often fail to realize optimism. We provide a\ntheoretical analysis showing that current formulations, under KL or\n$\\alpha$-divergence regularization, unintentionally bias exploration toward\nhigh-probability regions of the reference model, thereby reinforcing\nconservative behavior instead of promoting discovery of uncertain regions. To\naddress this pitfall, we introduce the General Exploratory Bonus (GEB), a novel\ntheoretical framework that provably satisfies the optimism principle. GEB\ncounteracts divergence-induced bias via reference-dependent reward regulation\nand unifies prior heuristic bonuses as special cases, while extending naturally\nacross the full $\\alpha$-divergence family. Empirically, GEB consistently\noutperforms baselines on alignment tasks across multiple divergence settings\nand large language model backbones. These results demonstrate that GEB offers\nboth a principled and practical solution for optimistic exploration in RLHF.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.03969", "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QRLLM\uff0c\u4e00\u4e2a\u9488\u5bf9LLM\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u707e\u96be\u6027\u98ce\u9669\u7684\u8ba4\u8bc1\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u5bf9\u8bdd\u6982\u7387\u5206\u5e03\u5e76\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u4e2d\u9ad8\u8fbe70%\u7684\u663e\u8457\u98ce\u9669\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bf9\u8bdd\u4e2d\u53ef\u80fd\u4ea7\u751f\u707e\u96be\u6027\u54cd\u5e94\uff0c\u5bf9\u516c\u5171\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u4f9d\u8d56\u56fa\u5b9a\u653b\u51fb\u63d0\u793a\u3001\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u5e7f\u9614\u7684\u591a\u8f6e\u5bf9\u8bdd\u7a7a\u95f4\uff0c\u672a\u80fd\u5145\u5206\u63ed\u793a\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "QRLLM\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u6709\u539f\u5219\u7684LLM\u591a\u8f6e\u5bf9\u8bdd\u707e\u96be\u6027\u98ce\u9669\u8ba4\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u7edf\u8ba1\u4fdd\u8bc1\u9650\u5236LLM\u4ea7\u751f\u707e\u96be\u6027\u54cd\u5e94\u7684\u6982\u7387\u3002\u8be5\u65b9\u6cd5\u5c06\u591a\u8f6e\u5bf9\u8bdd\u5efa\u6a21\u4e3a\u67e5\u8be2\u5e8f\u5217\u7684\u6982\u7387\u5206\u5e03\uff08\u901a\u8fc7\u67e5\u8be2\u56fe\u4e0a\u7684\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u8868\u793a\uff0c\u56fe\u8fb9\u7f16\u7801\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\uff0c\u5e76\u4f7f\u7528\u7f6e\u4fe1\u533a\u95f4\u91cf\u5316\u707e\u96be\u6027\u98ce\u9669\u3002\u5b9a\u4e49\u4e86\u51e0\u79cd\u7ecf\u6d4e\u5b9e\u7528\u7684\u5206\u5e03\uff1a\u968f\u673a\u8282\u70b9\u3001\u56fe\u8def\u5f84\u548c\u5e26\u62d2\u7edd\u7684\u81ea\u9002\u5e94\u5206\u5e03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u5206\u5e03\u80fd\u63ed\u793a\u524d\u6cbf\u6a21\u578b\u4e2d\u663e\u8457\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u5176\u4e2d\u6700\u5dee\u6a21\u578b\u7684\u8ba4\u8bc1\u4e0b\u9650\u9ad8\u8fbe70%\u3002", "conclusion": "\u524d\u6cbfLLM\u8feb\u5207\u9700\u8981\u6539\u8fdb\u5176\u5b89\u5168\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2510.03762", "pdf": "https://arxiv.org/pdf/2510.03762", "abs": "https://arxiv.org/abs/2510.03762", "authors": ["Deshan Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "title": "Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs", "categories": ["cs.CL"], "comment": "Paper accepted at GlobalNLP 2025: Workshop on beyond English: Natural\n  Language Processing for All Languages in an Era of Large Language Models\" 9\n  pages, 3 figures, 2 Tables", "summary": "Recent advances in Large Language Models (LLMs) have significantly reshaped\nthe landscape of Natural Language Processing (NLP). Among the various prompting\ntechniques, few-shot prompting has gained considerable attention for its\npracticality and effectiveness. This study investigates how few-shot prompting\nstrategies impact the Word Sense Disambiguation (WSD) task, particularly\nfocusing on the biases introduced by imbalanced sample distributions. We use\nthe GLOSSGPT prompting method, an advanced approach for English WSD, to test\nits effectiveness across five languages: English, German, Spanish, French, and\nItalian. Our results show that imbalanced few-shot examples can cause incorrect\nsense predictions in multilingual languages, but this issue does not appear in\nEnglish. To assess model behavior, we evaluate both the GPT-4o and\nLLaMA-3.1-70B models and the results highlight the sensitivity of multilingual\nWSD to sample distribution in few-shot settings, emphasizing the need for\nbalanced and representative prompting strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e2d\uff0c\u4e0d\u5e73\u8861\u7684\u6837\u672c\u5206\u5e03\u4f1a\u5bfc\u81f4\u591a\u8bed\u8a00\u8bcd\u4e49\u6d88\u6b67\uff08WSD\uff09\u4efb\u52a1\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u4f46\u5728\u82f1\u6587WSD\u4e2d\u672a\u51fa\u73b0\u6b64\u95ee\u9898\uff0c\u8868\u660e\u591a\u8bed\u8a00WSD\u5bf9\u6837\u672c\u5206\u5e03\u654f\u611f\u3002", "motivation": "\u63a2\u7a76\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8bcd\u4e49\u6d88\u6b67\uff08WSD\uff09\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5173\u6ce8\u7531\u4e0d\u5e73\u8861\u6837\u672c\u5206\u5e03\u5f15\u5165\u7684\u504f\u5dee\u3002", "method": "\u91c7\u7528GLOSSGPT\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e94\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u5176\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86GPT-4o\u548cLLaMA-3.1-70B\u6a21\u578b\u3002", "result": "\u4e0d\u5e73\u8861\u7684\u5c11\u6837\u672c\u793a\u4f8b\u4f1a\u5bfc\u81f4\u591a\u8bed\u8a00\uff08\u975e\u82f1\u8bed\uff09\u8bcd\u4e49\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u4f46\u8be5\u95ee\u9898\u672a\u5728\u82f1\u8bed\u4e2d\u51fa\u73b0\u3002\u7ed3\u679c\u51f8\u663e\u4e86\u591a\u8bed\u8a00WSD\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bf9\u6837\u672c\u5206\u5e03\u7684\u9ad8\u5ea6\u654f\u611f\u6027\u3002", "conclusion": "\u591a\u8bed\u8a00WSD\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bf9\u6837\u672c\u5206\u5e03\u8868\u73b0\u51fa\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5e73\u8861\u4e14\u5177\u6709\u4ee3\u8868\u6027\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2510.03540", "pdf": "https://arxiv.org/pdf/2510.03540", "abs": "https://arxiv.org/abs/2510.03540", "authors": ["Manuel Schwonberg", "Hanno Gottschalk"], "title": "Domain Generalization for Semantic Segmentation: A Survey", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025W", "summary": "The generalization of deep neural networks to unknown domains is a major\nchallenge despite their tremendous progress in recent years. For this reason,\nthe dynamic area of domain generalization (DG) has emerged. In contrast to\nunsupervised domain adaptation, there is no access to or knowledge about the\ntarget domains, and DG methods aim to generalize across multiple different\nunseen target domains. Domain generalization is particularly relevant for the\ntask semantic segmentation which is used in several areas such as biomedicine\nor automated driving. This survey provides a comprehensive overview of the\nrapidly evolving topic of domain generalized semantic segmentation. We cluster\nand review existing approaches and identify the paradigm shift towards\nfoundation-model-based domain generalization. Finally, we provide an extensive\nperformance comparison of all approaches, which highlights the significant\ninfluence of foundation models on domain generalization. This survey seeks to\nadvance domain generalization research and inspire scientists to explore new\nresearch directions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9762\u5411\u8bed\u4e49\u5206\u5272\u7684\u57df\u6cdb\u5316\uff08DG\uff09\u9886\u57df\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u57fa\u7840\u6a21\u578b\u5bf9DG\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u5176\u6cdb\u5316\u5230\u672a\u77e5\u9886\u57df\u7684\u80fd\u529b\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002\u57df\u6cdb\u5316\uff08DG\uff09\u9886\u57df\u5e94\u8fd0\u800c\u751f\uff0c\u65e8\u5728\u4f7f\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u3001\u672a\u89c1\u7684\u57df\uff0c\u5c24\u5176\u5bf9\u4e8e\u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u4e00\u9879\u7efc\u5408\u6027\u8c03\u67e5\uff0c\u5bf9\u73b0\u6709\u9762\u5411\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u7efc\u8ff0\uff0c\u5e76\u8bc6\u522b\u51fa\u5411\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u57df\u6cdb\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u6240\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u7840\u6a21\u578b\u5bf9\u57df\u6cdb\u5316\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5bf9\u57df\u6cdb\u5316\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002\u672c\u8c03\u67e5\u65e8\u5728\u63a8\u52a8\u57df\u6cdb\u5316\u7814\u7a76\uff0c\u5e76\u542f\u53d1\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.03270", "pdf": "https://arxiv.org/pdf/2510.03270", "abs": "https://arxiv.org/abs/2510.03270", "authors": ["Haolin Chen", "Shiyu Wang", "Can Qin", "Bo Pang", "Zuxin Liu", "Jielin Qiu", "Jianguo Zhang", "Yingbo Zhou", "Zeyuan Chen", "Ran Xu", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "CoDA: Coding LM via Diffusion Adaptation", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": null, "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a1.7B\u53c2\u6570\u7684\u5f00\u6e90\u6269\u6563\u7f16\u7801\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u3001\u4e2d\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u90e8\u52067B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u5907\u81ea\u56de\u5f52\u7f16\u7801\u5668\u6240\u7f3a\u4e4f\u7684\u53cc\u5411\u4e0a\u4e0b\u6587\u548c\u586b\u5145\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u8fc7\u4e8e\u5e9e\u5927\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u5f15\u5165CoDA\uff0c\u4e00\u4e2a1.7B\u53c2\u6570\u7684\u6269\u6563\u7f16\u7801\u5668\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6269\u6563\u9884\u8bad\u7ec3\u3001\u4ee5\u4ee3\u7801\u4e3a\u4e2d\u5fc3\u7684\u4e2d\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u5e76\u91c7\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u91c7\u6837\u6765\u786e\u4fdd\u63a8\u7406\u5ef6\u8fdf\u5177\u6709\u7ade\u4e89\u529b\u3002", "result": "\u5728Humaneval\u3001MBPP\u548cEvalPlus\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoDA-1.7B-Instruct\u7684\u6027\u80fd\u4e0e\u9ad8\u8fbe7B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u76f8\u5f53\u6216\u8d85\u8d8a\u3002", "conclusion": "CoDA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6027\u80fd\u7684\u6269\u6563\u7f16\u7801\u52a9\u624b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ee5\u52a0\u901f\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.04009", "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u521b\u9020\u529b\u8bc4\u4f30\u6846\u67b6\u5206\u6563\u4e14\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faC^2-Eval\uff0c\u4e00\u4e2a\u7edf\u4e00\u8bc4\u4f30FMs\u6536\u655b\u6027\u548c\u53d1\u6563\u6027\u521b\u9020\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u57fa\u4e8eU-O-S\u7406\u8bba\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\uff0c\u63ed\u793a\u4e86FMs\u521b\u9020\u80fd\u529b\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u5d1b\u8d77\u4f7f\u5f97\u521b\u9020\u529b\u6210\u4e3a\u8861\u91cf\u673a\u5668\u667a\u80fd\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5f15\u5165C^2-Eval\u57fa\u51c6\uff0c\u65e8\u5728\u7edf\u4e00\u8bc4\u4f30FMs\u7684\u521b\u9020\u529b\u3002\u8be5\u57fa\u51c6\u533a\u5206\u6536\u655b\u6027\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u548c\u53d1\u6563\u6027\uff08\u5982\u6545\u4e8b\u521b\u4f5c\uff09\u521b\u9020\u529b\uff0c\u5e76\u91c7\u7528\u6e90\u4e8e\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u201c\u6709\u7528\u6027\u3001\u539f\u521b\u6027\u548c\u60ca\u559c\u5ea6\u201d\uff08U-O-S\uff09\u7ec6\u7c92\u5ea6\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u521b\u9020\u529b\u65b9\u9762\u7684\u6743\u8861\u3002\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dFMs\u5728\u5b9e\u73b0\u521b\u9020\u6027\u673a\u5668\u601d\u7ef4\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "conclusion": "C^2-Eval\u88ab\u8bc1\u660e\u662f\u5ba1\u67e5\u521b\u610fAI\u53d1\u5c55\u683c\u5c40\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u7406\u89e3FMs\u7684\u521b\u9020\u80fd\u529b\u63d0\u4f9b\u4e86\u6e05\u6670\u89c6\u89d2\u3002"}}
{"id": "2510.03781", "pdf": "https://arxiv.org/pdf/2510.03781", "abs": "https://arxiv.org/abs/2510.03781", "authors": ["Majid Asgari-Bidhendi", "Muhammad Amin Ghaseminia", "Alireza Shahbazi", "Sayyed Ali Hossayni", "Najmeh Torabian", "Behrouz Minaei-Bidgoli"], "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "This paper presents the development of Rezwan, a large-scale AI-assisted\nHadith corpus comprising over 1.2M narrations, extracted and structured through\na fully automated pipeline. Building on digital repositories such as Maktabat\nAhl al-Bayt, the pipeline employs Large Language Models (LLMs) for\nsegmentation, chain--text separation, validation, and multi-layer enrichment.\nEach narration is enhanced with machine translation into twelve languages,\nintelligent diacritization, abstractive summarization, thematic tagging, and\ncross-text semantic analysis. This multi-step process transforms raw text into\na richly annotated research-ready infrastructure for digital humanities and\nIslamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled\nnarrations, assessed by six domain experts. Results show near-human accuracy in\nstructured tasks such as chain--text separation (9.33/10) and summarization\n(9.33/10), while highlighting ongoing challenges in diacritization and semantic\nsimilarity detection. Comparative analysis against the manually curated Noor\nCorpus demonstrates the superiority of Najm in both scale and quality, with a\nmean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis\nconfirms the economic feasibility of the AI approach: tasks requiring over\n229,000 hours of expert labor were completed within months at a fraction of the\ncost. The work introduces a new paradigm in religious text processing by\nshowing how AI can augment human expertise, enabling large-scale, multilingual,\nand semantically enriched access to Islamic heritage.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aRezwan\u7684\u5927\u89c4\u6a21AI\u8f85\u52a9\u5723\u8bad\u8bed\u6599\u5e93\u7684\u5f00\u53d1\uff0c\u8be5\u8bed\u6599\u5e93\u5305\u542b\u8d85\u8fc7120\u4e07\u7bc7\u53d9\u8ff0\uff0c\u901a\u8fc7\u5168\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d6\u548c\u7ed3\u6784\u5316\uff0c\u5229\u7528LLMs\u8fdb\u884c\u591a\u5c42\u5bcc\u96c6\u3002\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u5176\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u8fd1\u4eba\u51c6\u786e\u5ea6\uff0c\u5e76\u5728\u89c4\u6a21\u548c\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u8bed\u6599\u5e93\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u5b97\u6559\u6587\u672c\u8d44\u6e90\u7684\u5904\u7406\u6548\u7387\u548c\u4e30\u5bcc\u5ea6\u4e0d\u8db3\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u4eba\u5de5\u52b3\u52a8\uff0c\u8017\u65f6\u8017\u529b\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528AI\u5f00\u53d1\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u5723\u8bad\u8bed\u6599\u5e93\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u548c\u4f0a\u65af\u5170\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u3001\u7814\u7a76\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\uff0c\u4eceMaktabat Ahl al-Bayt\u7b49\u6570\u5b57\u5b58\u50a8\u5e93\u4e2d\u63d0\u53d6\u5185\u5bb9\u3002\u8be5\u6d41\u6c34\u7ebf\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6587\u672c\u5206\u5272\u3001\u94fe-\u6587\u672c\u5206\u79bb\u3001\u9a8c\u8bc1\u548c\u591a\u5c42\u5bcc\u96c6\u3002\u5bcc\u96c6\u529f\u80fd\u5305\u62ec\u5c06\u6bcf\u7bc7\u53d9\u8ff0\u673a\u5668\u7ffb\u8bd1\u6210\u5341\u4e8c\u79cd\u8bed\u8a00\u3001\u667a\u80fd\u52a0\u6ce8\u3001\u62bd\u8c61\u603b\u7ed3\u3001\u4e3b\u9898\u6807\u7b7e\u4ee5\u53ca\u8de8\u6587\u672c\u8bed\u4e49\u5206\u6790\u3002", "result": "\u5bf91,213\u7bc7\u968f\u673a\u62bd\u6837\u53d9\u8ff0\u7684\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u94fe-\u6587\u672c\u5206\u79bb\u548c\u603b\u7ed3\u7b49\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u8fd1\u4eba\u51c6\u786e\u5ea6\uff089.33/10\uff09\u3002\u5c3d\u7ba1\u5728\u52a0\u6ce8\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u68c0\u6d4b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u4e0e\u4eba\u5de5\u6574\u7406\u7684Noor\u8bed\u6599\u5e93\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\uff08Najm\uff09\u5728\u89c4\u6a21\u548c\u8d28\u91cf\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\uff08\u5e73\u5747\u603b\u52068.46/10 vs 3.66/10\uff09\u3002\u6210\u672c\u5206\u6790\u8bc1\u5b9e\u4e86AI\u65b9\u6cd5\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\uff0c\u4ec5\u7528\u6570\u6708\u65f6\u95f4\u3001\u4ee5\u6781\u5c0f\u6210\u672c\u5b8c\u6210\u4e86\u9700\u8981\u8d85\u8fc7229,000\u5c0f\u65f6\u4e13\u5bb6\u52b3\u52a8\u7684\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c55\u793aAI\u5982\u4f55\u589e\u5f3a\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5f15\u5165\u4e86\u5b97\u6559\u6587\u672c\u5904\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u4f0a\u65af\u5170\u9057\u4ea7\u7684\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u8bbf\u95ee\u3002"}}
{"id": "2510.03543", "pdf": "https://arxiv.org/pdf/2510.03543", "abs": "https://arxiv.org/abs/2510.03543", "authors": ["Evandros Kaklamanos", "Kristjana Kristinsdottir", "Jonathan Huang", "Dustin Carlson", "Rajesh Keswani", "John Pandolfino", "Mozziyar Etemadi"], "title": "From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy", "categories": ["cs.CV"], "comment": null, "summary": "Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and\ncolonoscopy play a critical role in diagnosing and managing gastrointestinal\n(GI) disorders. However, the documentation burden associated with these\nprocedures place significant strain on gastroenterologists, contributing to\ninefficiencies in clinical workflows and physician burnout. To address this\nchallenge, we propose a novel automated report generation model that leverages\na transformer-based vision encoder and text decoder within a two-stage training\nframework. In the first stage, both components are pre-trained on image/text\ncaption pairs to capture generalized vision-language features, followed by\nfine-tuning on images/report pairs to generate clinically meaningful findings.\nOur approach not only streamlines the documentation process but also holds\npromise for reducing physician workload and improving patient care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u5185\u7aa5\u955c\u68c0\u67e5\u62a5\u544a\u7684\u81ea\u52a8\u751f\u6210\uff0c\u4ee5\u51cf\u8f7b\u533b\u751f\u6587\u6863\u8d1f\u62c5\u3002", "motivation": "\u80c3\u80a0\u75c5\u533b\u751f\u5728\u5185\u7aa5\u955c\u68c0\u67e5\uff08\u5982EGD\u548c\u7ed3\u80a0\u955c\u68c0\u67e5\uff09\u4e2d\u9762\u4e34\u7e41\u91cd\u7684\u6587\u6863\u5de5\u4f5c\uff0c\u5bfc\u81f4\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u548c\u533b\u751f\u5026\u6020\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6587\u672c\u89e3\u7801\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u5728\u56fe\u50cf/\u6587\u672c\u6807\u9898\u5bf9\u4e0a\u9884\u8bad\u7ec3\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5728\u56fe\u50cf/\u62a5\u544a\u5bf9\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u751f\u6210\u4e34\u5e8a\u53d1\u73b0\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u671b\u7b80\u5316\u6587\u6863\u6d41\u7a0b\uff0c\u51cf\u8f7b\u533b\u751f\u5de5\u4f5c\u91cf\uff0c\u5e76\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5185\u7aa5\u955c\u68c0\u67e5\u7684\u6587\u6863\u8d1f\u62c5\uff0c\u5177\u6709\u63d0\u9ad8\u4e34\u5e8a\u6548\u7387\u548c\u60a3\u8005\u62a4\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03271", "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "categories": ["cs.LG", "cs.AI"], "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora.", "AI": {"tldr": "\u9488\u5bf9LLM\u51b3\u7b56\u8fb9\u754c\u96be\u4ee5\u6784\u5efa\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u201c\u51b3\u7b56\u52bf\u80fd\u9762\u201d(DPS)\u53ca\u5176\u8fd1\u4f3c\u7b97\u6cd5K-DPS\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86LLM\u51b3\u7b56\u8fb9\u754c\u7684\u8fd1\u4f3c\u6784\u5efa\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u8bef\u5dee\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u5206\u6790LLM\u51b3\u7b56\u8fb9\u754c\u5bf9\u4e8e\u63ed\u793a\u6a21\u578b\u6838\u5fc3\u5c5e\u6027\u548c\u89e3\u91ca\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8eLLM\u5e9e\u5927\u7684\u8bcd\u6c47\u5e8f\u5217\u7a7a\u95f4\u548c\u81ea\u56de\u5f52\u7279\u6027\uff0c\u76ee\u524d\u5c1a\u65e0\u6cd5\u5728\u8ba1\u7b97\u4e0a\u53ef\u884c\u5730\u6784\u5efa\u4e3b\u6d41LLM\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u201c\u51b3\u7b56\u52bf\u80fd\u9762\u201d\uff08DPS\uff09\u8fd9\u4e00\u65b0\u6982\u5ff5\u6765\u5206\u6790LLM\u51b3\u7b56\u8fb9\u754c\uff0c\u5176\u5b9a\u4e49\u57fa\u4e8e\u533a\u5206\u4e0d\u540c\u91c7\u6837\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u3002\u7406\u8bba\u8bc1\u660eDPS\u4e2d\u7684\u96f6\u9ad8\u5ea6\u7b49\u9ad8\u7ebf\u7b49\u4ef7\u4e8e\u51b3\u7b56\u8fb9\u754c\u3002\u57fa\u4e8eDPS\uff0c\u5f00\u53d1\u4e86K-DPS\u8fd1\u4f3c\u6784\u5efa\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ec5\u9700K\u6b21\u6709\u9650\u5e8f\u5217\u91c7\u6837\u5373\u53ef\u8fd1\u4f3cLLM\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u8bc1\u660e\u4e86DPS\u7684\u96f6\u9ad8\u5ea6\u7b49\u9ad8\u7ebf\u4e0e\u51b3\u7b56\u8fb9\u754c\u7b49\u4ef7\u3002K-DPS\u7b97\u6cd5\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u8bef\u5dee\u8fd1\u4f3cLLM\u51b3\u7b56\u8fb9\u754c\u3002\u8bba\u6587\u63a8\u5bfc\u4e86K-DPS\u4e0e\u7406\u60f3DPS\u4e4b\u95f4\u7edd\u5bf9\u8bef\u5dee\u3001\u671f\u671b\u8bef\u5dee\u548c\u8bef\u5dee\u96c6\u4e2d\u5ea6\u7684\u4e0a\u9650\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u8bef\u5dee\u53ef\u4e0e\u91c7\u6837\u6b21\u6570\u8fdb\u884c\u6743\u8861\u3002\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u901a\u8fc7DPS\u548cK-DPS\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u4e0a\u53ef\u884c\u7684LLM\u51b3\u7b56\u8fb9\u754c\u8fd1\u4f3c\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002\u8fd9\u4e3a\u7406\u89e3LLM\u7684\u51b3\u7b56\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u5e76\u83b7\u5f97\u4e86\u5168\u9762\u7684\u7406\u8bba\u652f\u6301\u548c\u7ecf\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2510.04017", "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff08Zephyrus\uff09\u53ca\u5176\u4ee3\u7801\u73af\u5883\uff08ZephyrusWorld\uff09\uff0c\u65e8\u5728\u5f25\u5408\u5929\u6c14\u57fa\u7840\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e4b\u95f4\u7684\u80fd\u529b\u9e3f\u6c9f\uff0c\u4f7fLLM\u80fd\u591f\u901a\u8fc7\u4ee3\u7801\u5de5\u5177\u4e0e\u5929\u6c14\u6570\u636e\u4ea4\u4e92\uff0c\u5e76\u5728\u65b0\u57fa\u51c6ZephyrusBench\u4e0a\u8d85\u8d8a\u7eaf\u6587\u672c\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5929\u6c14\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u800cLLMs\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4ea4\u4e92\u5f0f\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\uff1a1) ZephyrusWorld\uff0c\u4e00\u4e2a\u57fa\u4e8ePython\u4ee3\u7801\u7684\u73af\u5883\uff0c\u63d0\u4f9bWeatherBench 2\u6570\u636e\u96c6\u63a5\u53e3\u3001\u81ea\u7136\u8bed\u8a00\u5730\u7406\u67e5\u8be2\u3001\u5929\u6c14\u9884\u62a5\u548c\u6c14\u5019\u6a21\u62df\u5de5\u5177\uff1b2) Zephyrus\uff0c\u4e00\u4e2a\u591a\u8f6eLLM\u5929\u6c14\u4ee3\u7406\uff0c\u901a\u8fc7\u5bf9\u8bdd\u53cd\u9988\u8fed\u4ee3\u5206\u6790\u6570\u636e\uff1b3) ZephyrusBench\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u5305\u542b\u4ece\u57fa\u672c\u67e5\u8be2\u5230\u9ad8\u7ea7\u9884\u62a5\u3001\u6781\u7aef\u4e8b\u4ef6\u68c0\u6d4b\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7b49\u591a\u6837\u5316\u5929\u6c14\u4efb\u52a1\u3002", "result": "Zephyrus\u4ee3\u7406\u5728ZephyrusBench\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\uff0c\u6b63\u786e\u7387\u6700\u9ad8\u63d0\u5347\u4e8635\u4e2a\u767e\u5206\u70b9\u3002\u4f46\u5728\u66f4\u56f0\u96be\u7684\u4efb\u52a1\u4e0a\uff0cZephyrus\u4e0e\u7eaf\u6587\u672c\u57fa\u7ebf\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4ee3\u7406\u6846\u67b6\u6709\u6548\u5730\u589e\u5f3a\u4e86LLM\u5728\u5929\u6c14\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u4ea4\u4e92\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5c3d\u7ba1\u5728\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u4e5f\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.03799", "pdf": "https://arxiv.org/pdf/2510.03799", "abs": "https://arxiv.org/abs/2510.03799", "authors": ["Hadi Asghari", "Sami Nenno"], "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Peer-reviewed and presented at Advances in Interpretable Machine\n  Learning and Artificial Intelligence (AIMLAI) Workshop at ECML/PKDD 2024", "summary": "This paper explores the ability of large language models to generate and\nrecognize deep cognitive frames, particularly in socio-political contexts. We\ndemonstrate that LLMs are highly fluent in generating texts that evoke specific\nframes and can recognize these frames in zero-shot settings. Inspired by\nmechanistic interpretability research, we investigate the location of the\n`strict father' and `nurturing parent' frames within the model's hidden\nrepresentation, identifying singular dimensions that correlate strongly with\ntheir presence. Our findings contribute to understanding how LLMs capture and\nexpress meaningful human concepts.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u80fd\u591f\u751f\u6210\u548c\u8bc6\u522b\u6df1\u5c42\u8ba4\u77e5\u6846\u67b6\uff08\u5982\u201c\u4e25\u7236\u201d\u548c\u201c\u6148\u6bcd\u201d\uff09\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6846\u67b6\u5728\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e2d\u5bf9\u5e94\u7279\u5b9a\u7684\u7ef4\u5ea6\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u793e\u4f1a\u653f\u6cbb\u8bed\u5883\u4e2d\u751f\u6210\u548c\u8bc6\u522b\u6df1\u5c42\u8ba4\u77e5\u6846\u67b6\u7684\u80fd\u529b\uff0c\u5e76\u7406\u89e3LLMs\u5982\u4f55\u6355\u6349\u548c\u8868\u8fbe\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u6982\u5ff5\u3002", "method": ["\u6f14\u793aLLMs\u751f\u6210\u5524\u8d77\u7279\u5b9a\u6846\u67b6\u6587\u672c\u7684\u6d41\u7545\u6027\u53ca\u5176\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u8fd9\u4e9b\u6846\u67b6\u7684\u80fd\u529b\u3002", "\u53d7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u542f\u53d1\uff0c\u8c03\u67e5\u201c\u4e25\u7236\u201d\u548c\u201c\u6148\u6bcd\u201d\u7b49\u6846\u67b6\u5728\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e2d\u7684\u4f4d\u7f6e\u3002", "\u8bc6\u522b\u4e0e\u8fd9\u4e9b\u6846\u67b6\u5b58\u5728\u5f3a\u70c8\u76f8\u5173\u7684\u5947\u5f02\u7ef4\u5ea6\u3002"], "result": ["LLMs\u5728\u751f\u6210\u5524\u8d77\u7279\u5b9a\u6846\u67b6\u7684\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u5ea6\u6d41\u7545\u6027\u3002", "LLMs\u80fd\u591f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u8fd9\u4e9b\u6846\u67b6\u3002", "\u5728\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\u4e2d\uff0c\u8bc6\u522b\u51fa\u4e0e\u201c\u4e25\u7236\u201d\u548c\u201c\u6148\u6bcd\u201d\u6846\u67b6\u5b58\u5728\u5f3a\u70c8\u76f8\u5173\u7684\u5947\u5f02\u7ef4\u5ea6\u3002"], "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u7406\u89e3LLMs\u5982\u4f55\u6355\u6349\u548c\u8868\u8fbe\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u6982\u5ff5\u3002"}}
{"id": "2510.03545", "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "categories": ["cs.CV", "cs.RO"], "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u5668\uff0c\u80fd\u5c062D\u624b\u7ed8\u8349\u56fe\u8f6c\u6362\u4e3a3D\u98de\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u5728\u6df1\u5ea6\u56fe\u50cf\u4e0a\u76842D\u624b\u7ed8\u8349\u56fe\uff0c\u4e3a\u65e0\u4eba\u673a\u751f\u62103D\u98de\u884c\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u76f4\u89c2\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u3002", "method": "\u63d0\u51faSketchPlan\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1aSketchAdapter\u5c06\u624b\u7ed8\u8349\u56fe\u6620\u5c04\u52302D\u6295\u5f71\u8def\u5f84\uff1bDiffPath\uff08\u6269\u6563\u6a21\u578b\uff09\u6839\u636e2D\u6295\u5f71\u548c\u6df1\u5ea6\u56fe\u63a8\u65ad3D\u8f68\u8ff9\u3002\u6a21\u578b\u901a\u8fc732k\u5408\u6210\u98de\u884c\u8def\u5f84\uff08\u81ea\u52a8\u6807\u6ce82D\u6295\u5f71\uff09\u548c872\u6761\u771f\u5b9e\u4eba\u5de5\u6807\u6ce82D\u8349\u56fe\u76843D\u98de\u884c\u8def\u5f84\u6df7\u5408\u8bad\u7ec3\u3002", "result": "SketchPlan\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u6d4b\u8bd5\u4e2d\uff0c\u4f4e/\u4e2d\u5ea6\u6742\u4e71\u73af\u5883\u6210\u529f\u7387\u4e3a100%\uff0c\u9ad8\u6742\u4e71\u73af\u5883\u4e3a40%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u6bd4\u5173\u952e\u6d88\u878d\u6a21\u578b\u9ad820-60%\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SketchPlan\u4f5c\u4e3a\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u80fd\u6709\u6548\u89e3\u91ca2D\u624b\u7ed8\u8349\u56fe\u5e76\u751f\u62103D\u65e0\u4eba\u673a\u98de\u884c\u8def\u5f84\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u65b9\u5f0f\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20\u3002"}}
{"id": "2510.03272", "pdf": "https://arxiv.org/pdf/2510.03272", "abs": "https://arxiv.org/abs/2510.03272", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized artificial intelligence, yet\na principled theoretical understanding of its internal mechanisms remains\nelusive. This paper introduces a novel analytical framework that\nreconceptualizes the Transformer's discrete, layered structure as a continuous\nspatiotemporal dynamical system governed by a master Partial Differential\nEquation (PDE). Within this paradigm, we map core architectural components to\ndistinct mathematical operators: self-attention as a non-local interaction, the\nfeed-forward network as a local reaction, and, critically, residual connections\nand layer normalization as indispensable stabilization mechanisms. We do not\npropose a new model, but rather employ the PDE system as a theoretical probe to\nanalyze the mathematical necessity of these components. By comparing a standard\nTransformer with a PDE simulator that lacks explicit stabilizers, our\nexperiments provide compelling empirical evidence for our central thesis. We\ndemonstrate that without residual connections, the system suffers from\ncatastrophic representational drift, while the absence of layer normalization\nleads to unstable, explosive training dynamics. Our findings reveal that these\nseemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers\nrequired to tame an otherwise powerful but inherently unstable continuous\nsystem. This work offers a first-principles explanation for the Transformer's\ndesign and establishes a new paradigm for analyzing deep neural networks\nthrough the lens of continuous dynamics.", "AI": {"tldr": "\u672c\u6587\u5c06Transformer\u5efa\u6a21\u4e3a\u8fde\u7eed\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u52a8\u529b\u7cfb\u7edf\uff0c\u7406\u8bba\u6027\u5730\u89e3\u91ca\u4e86\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u4f5c\u4e3a\u5fc5\u8981\u6570\u5b66\u7a33\u5b9a\u5668\u7684\u4f5c\u7528\uff0c\u907f\u514d\u4e86\u8868\u5f81\u6f02\u79fb\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u53d6\u5f97\u9769\u547d\u6027\u6210\u529f\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u4e0d\u6e05\u6670\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u5206\u6790\u6846\u67b6\uff0c\u5c06Transformer\u7684\u79bb\u6563\u5206\u5c42\u7ed3\u6784\u91cd\u6784\u4e3a\u7531\u4e3b\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\u3002\u5c06\u81ea\u6ce8\u610f\u529b\u3001\u524d\u9988\u7f51\u7edc\u3001\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u5206\u522b\u6620\u5c04\u4e3a\u975e\u5c40\u90e8\u4ea4\u4e92\u3001\u5c40\u90e8\u53cd\u5e94\u548c\u7a33\u5b9a\u673a\u5236\u3002\u901a\u8fc7\u5c06\u6807\u51c6Transformer\u4e0e\u7f3a\u4e4f\u663e\u5f0f\u7a33\u5b9a\u5668\u7684PDE\u6a21\u62df\u5668\u8fdb\u884c\u6bd4\u8f83\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7f3a\u4e4f\u6b8b\u5dee\u8fde\u63a5\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u8868\u5f81\u6f02\u79fb\uff1b\u800c\u7f3a\u5c11\u5c42\u5f52\u4e00\u5316\u5219\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3001\u7206\u70b8\u6027\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u5e76\u975e\u542f\u53d1\u5f0f\u6280\u5de7\uff0c\u800c\u662f\u9a6f\u670d\u5f3a\u5927\u4f46\u672c\u8d28\u4e0d\u7a33\u5b9a\u7684\u8fde\u7eed\u7cfb\u7edf\u6240\u9700\u7684\u6839\u672c\u6570\u5b66\u7a33\u5b9a\u5668\u3002\u6b64\u5de5\u4f5c\u4e3aTransformer\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7b2c\u4e00\u6027\u539f\u7406\u7684\u89e3\u91ca\uff0c\u5e76\u5efa\u7acb\u4e86\u901a\u8fc7\u8fde\u7eed\u52a8\u529b\u5b66\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.04023", "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u9996\u6b21\u5168\u9762\u5206\u7c7b\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\uff0c\u5206\u6790\u4e8645\u4e2a\u7cfb\u7edf\u5728\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u5404\u9636\u6bb5\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u53ca\u672a\u6765\u53d1\u5c55\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u50ac\u751f\u4e86\u4e00\u7c7b\u65b0\u7684AI\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u591a\u4e2a\u9636\u6bb5\u3002\u672c\u7814\u7a76\u65e8\u5728\u9996\u6b21\u5bf9\u8fd9\u4e9b\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\u8fdb\u884c\u5168\u9762\u3001\u4e0e\u751f\u547d\u5468\u671f\u5bf9\u9f50\u7684\u5206\u7c7b\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u5206\u6790\u548c\u7406\u89e3\u5b83\u4eec\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e8645\u4e2a\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u516d\u4e2a\u9636\u6bb5\u3002\u6b64\u5916\uff0c\u8fd8\u6839\u636e\u63a8\u7406\u89c4\u5212\u98ce\u683c\u3001\u6a21\u6001\u6574\u5408\u3001\u5de5\u5177\u7f16\u6392\u6df1\u5ea6\u3001\u5b66\u4e60\u4e0e\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u53ca\u4fe1\u4efb\u4e0e\u5b89\u5168\u673a\u5236\u7b49\u4e94\u4e2a\u8bbe\u8ba1\u7ef4\u5ea6\u5bf9\u6bcf\u4e2a\u667a\u80fd\u4f53\u8fdb\u884c\u4e86\u6807\u6ce8\u3002\u7814\u7a76\u8fd8\u5bf9\u667a\u80fd\u4f53\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u6279\u5224\u6027\u7efc\u5408\uff0c\u5f3a\u8c03\u4e86\u5404\u9636\u6bb5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u56de\u987e\u4e86\u65b0\u5174\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d8b\u52bf\uff1a\u5927\u591a\u6570\u7cfb\u7edf\u4fa7\u91cd\u4e8e\u63a2\u7d22\u6027\u5206\u6790\u3001\u53ef\u89c6\u5316\u548c\u5efa\u6a21\uff0c\u800c\u5ffd\u89c6\u4e86\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u548c\u76d1\u63a7\uff1b\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u662f\u672a\u89e3\u51b3\u7684\u6311\u6218\uff1b\u8d85\u8fc790%\u7684\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u548c\u5b89\u5168\u673a\u5236\u3002", "conclusion": "\u5f53\u524d\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\u5728\u5bf9\u9f50\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cbb\u7406\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\u65b9\u9762\u5b58\u5728\u5f00\u653e\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u5e94\u7740\u529b\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u503c\u5f97\u4fe1\u8d56\u3001\u4f4e\u5ef6\u8fdf\u3001\u900f\u660e\u4e14\u666e\u53ca\u7684\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\u3002"}}
{"id": "2510.03805", "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5197\u957f\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faStep Pruner (SP)\uff0c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u957f\u611f\u77e5\u5956\u52b1\u548c\u52a8\u6001\u505c\u6b62\u673a\u5236\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u6b65\u9aa4\u548c\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u56e0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u800c\u8fc7\u5ea6\u5197\u957f\u3002\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u60e9\u7f5a\u751f\u6210\u7684\u4ee4\u724c\u6765\u4fc3\u8fdb\u7b80\u6d01\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u4ee4\u724c\u51cf\u5c11\u4e0d\u603b\u610f\u5473\u7740\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\uff0c\u4e14\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u4e22\u5f03\u63a8\u7406\u6b65\u9aa4\u6765\u89c4\u907f\u4ee4\u724c\u60e9\u7f5a\uff08\u4f5c\u5f0a\u884c\u4e3a\uff09\u3002", "method": "\u672c\u6587\u5f15\u5165Step Pruner (SP)\uff0c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u5f15\u5bfcLRMs\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u3002SP\u4f7f\u7528\u4ee5\u4e0b\u673a\u5236\uff1a1. **\u6b65\u957f\u611f\u77e5\u5956\u52b1\u51fd\u6570**\uff1a\u4f18\u5148\u8003\u8651\u6b63\u786e\u6027\uff0c\u60e9\u7f5a\u5197\u4f59\u6b65\u9aa4\uff0c\u5e76\u5bf9\u9519\u8bef\u54cd\u5e94\u4e0d\u7ed9\u4e88\u5956\u52b1\uff0c\u4ee5\u9632\u6b62\u5f3a\u5316\u9519\u8bef\u63a8\u7406\u30022. **\u52a8\u6001\u505c\u6b62\u673a\u5236**\uff1a\u5f53\u4efb\u4f55\u8f93\u51fa\u6b65\u9aa4\u7684\u957f\u5ea6\u8d85\u8fc7\u9884\u8bbe\u4e0a\u9650\u65f6\uff0c\u505c\u6b62\u66f4\u65b0\uff0c\u4ee5\u9632\u6b62\u6a21\u578b\u901a\u8fc7\u5408\u5e76\u6b65\u9aa4\u6765\u4f5c\u5f0a\u3002", "result": "SP\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u54cd\u5e94\u957f\u5ea6\u3002\u4f8b\u5982\uff0c\u5728AIME24\u6570\u636e\u96c6\u4e0a\uff0c\u4ee4\u724c\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8669.7%\u3002", "conclusion": "Step Pruner (SP) \u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5197\u957f\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u6b65\u9aa4\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03548", "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios.", "AI": {"tldr": "AI\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u5b58\u5728\u5b9e\u65f6\u6362\u8138\uff08\u6728\u5076\u64cd\u63a7\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6548\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u751f\u7269\u8bc6\u522b\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u5206\u6790\u4f20\u8f93\u7684\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u4fe1\u606f\u4e2d\u7684\u8eab\u4efd\u7ebf\u7d22\uff0c\u5b9e\u65f6\u68c0\u6d4b\u975e\u6cd5\u8eab\u4efd\u4e92\u6362\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "AI\u9a71\u52a8\u7684\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u4fe1\u606f\u4ee5\u51cf\u5c11\u5e26\u5bbd\uff0c\u4f46\u653b\u51fb\u8005\u53ef\u5229\u7528\u6b64\u6f5c\u5728\u4fe1\u606f\u5b9e\u65f6\u52ab\u6301\u53d7\u5bb3\u8005\u5f62\u8c61\uff08\u6728\u5076\u64cd\u63a7\uff09\u3002\u7531\u4e8e\u89c6\u9891\u5b8c\u5168\u5408\u6210\uff0c\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e0d\u4f9d\u8d56\u91cd\u5efaRGB\u89c6\u9891\u7684\u751f\u7269\u8bc6\u522b\u6cc4\u9732\u9632\u5fa1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u4e2a\u59ff\u6001-\u6761\u4ef6\u5316\u3001\u5927\u95f4\u9694\u5bf9\u6bd4\u7f16\u7801\u5668\uff0c\u4ece\u4f20\u8f93\u7684\u6f5c\u5728\u4fe1\u606f\u4e2d\u5206\u79bb\u51fa\u6301\u4e45\u7684\u8eab\u4efd\u7ebf\u7d22\uff0c\u5e76\u6d88\u9664\u77ac\u6001\u7684\u59ff\u6001\u548c\u8868\u60c5\u3002\u901a\u8fc7\u5bf9\u89e3\u8026\u540e\u7684\u5d4c\u5165\u8fdb\u884c\u7b80\u5355\u7684\u4f59\u5f26\u6d4b\u8bd5\uff0c\u5373\u53ef\u5b9e\u65f6\u6807\u8bb0\u975e\u6cd5\u8eab\u4efd\u4e92\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2atalking-head\u751f\u6210\u6a21\u578b\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6728\u5076\u64cd\u63a7\u9632\u5fa1\u65b9\u6848\uff0c\u80fd\u5b9e\u65f6\u64cd\u4f5c\uff0c\u5e76\u5bf9\u5206\u5e03\u5916\u573a\u666f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u9996\u4e2a\u65e0\u9700\u91cd\u5efaRGB\u89c6\u9891\u7684\u751f\u7269\u8bc6\u522b\u6cc4\u9732\u9632\u5fa1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u9a71\u52a8\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u6362\u8138\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u3001\u5b9e\u65f6\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.03273", "pdf": "https://arxiv.org/pdf/2510.03273", "abs": "https://arxiv.org/abs/2510.03273", "authors": ["Chenhao Ye", "Ming Tang"], "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Backpropagation (BP), while foundational to deep learning, imposes two\ncritical scalability bottlenecks: update locking, where network modules remain\nidle until the entire backward pass completes, and high memory consumption due\nto storing activations for gradient computation. To address these limitations,\nwe introduce Synergistic Information Distillation (SID), a novel training\nframework that reframes deep learning as a cascade of local cooperative\nrefinement problems. In SID, a deep network is structured as a pipeline of\nmodules, each imposed with a local objective to refine a probabilistic belief\nabout the ground-truth target. This objective balances fidelity to the target\nwith consistency to the belief from its preceding module. By decoupling the\nbackward dependencies between modules, SID enables parallel training and hence\neliminates update locking and drastically reduces memory requirements.\nMeanwhile, this design preserves the standard feed-forward inference pass,\nmaking SID a versatile drop-in replacement for BP. We provide a theoretical\nfoundation, proving that SID guarantees monotonic performance improvement with\nnetwork depth. Empirically, SID consistently matches or surpasses the\nclassification accuracy of BP, exhibiting superior scalability and pronounced\nrobustness to label noise.Code is available at:\nhttps://github.com/ychAlbert/sid-bp", "AI": {"tldr": "SID\u662f\u4e00\u4e2a\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u5757\u4f9d\u8d56\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86BP\u7684\u66f4\u65b0\u9501\u5b9a\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0eBP\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u53cd\u5411\u4f20\u64ad(BP)\u5b58\u5728\u66f4\u65b0\u9501\u5b9a\uff08\u6a21\u5757\u9700\u7b49\u5f85\u6574\u4e2a\u53cd\u5411\u4f20\u64ad\u5b8c\u6210\uff09\u548c\u9ad8\u5185\u5b58\u6d88\u8017\uff08\u9700\u5b58\u50a8\u6fc0\u6d3b\u7528\u4e8e\u68af\u5ea6\u8ba1\u7b97\uff09\u4e24\u5927\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u534f\u540c\u4fe1\u606f\u84b8\u998f(SID)\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u7f51\u7edc\u6784\u5efa\u4e3a\u6a21\u5757\u7ba1\u9053\uff0c\u6bcf\u4e2a\u6a21\u5757\u901a\u8fc7\u5c40\u90e8\u76ee\u6807\uff08\u5e73\u8861\u76ee\u6807\u4fdd\u771f\u5ea6\u548c\u4e0e\u524d\u5e8f\u6a21\u5757\u4fe1\u5ff5\u4e00\u81f4\u6027\uff09\u7ec6\u5316\u6982\u7387\u4fe1\u5ff5\uff0c\u4ece\u800c\u89e3\u8026\u53cd\u5411\u4f9d\u8d56\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3\u3002", "result": "SID\u6d88\u9664\u4e86\u66f4\u65b0\u9501\u5b9a\uff0c\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u4fdd\u6301\u6807\u51c6\u524d\u5411\u63a8\u7406\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6027\u80fd\u968f\u6df1\u5ea6\u5355\u8c03\u63d0\u5347\u3002\u7ecf\u9a8c\u4e0a\uff0c\u5206\u7c7b\u7cbe\u5ea6\u4e0eBP\u6301\u5e73\u6216\u8d85\u8d8a\uff0c\u5177\u6709\u66f4\u4f18\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SID\u4f5c\u4e3aBP\u7684\u901a\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86BP\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\u548c\u5185\u5b58\u4f18\u5316\uff0c\u5e76\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04033", "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "categories": ["cs.AI"], "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u4e34\u5e8aAI\u7cfb\u7edf\u7f3a\u4e4f\u6807\u51c6\u5316\u4e8b\u4ef6\u8bb0\u5f55\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faMedLog\u534f\u8bae\uff0c\u7528\u4e8e\u8bb0\u5f55\u4e34\u5e8aAI\u6a21\u578b\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u5b9e\u73b0\u6301\u7eed\u76d1\u63a7\u3001\u5ba1\u8ba1\u548c\u6539\u8fdb\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u7cfb\u7edf\u6709syslog\u8bb0\u5f55\u5173\u952e\u4e8b\u4ef6\uff0c\u4f46\u533b\u7597\u4fdd\u5065\u9886\u57df\u5feb\u901f\u589e\u957f\u7684\u4e34\u5e8aAI\u7f3a\u4e4f\u7c7b\u4f3c\u6807\u51c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u8861\u91cf\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u3001\u68c0\u6d4b\u4e0d\u826f\u4e8b\u4ef6\u6216\u7ea0\u6b63\u504f\u5dee\u3002", "method": "\u5f15\u5165MedLog\u534f\u8bae\uff0c\u5728AI\u6a21\u578b\u88ab\u8c03\u7528\u65f6\u521b\u5efa\u8bb0\u5f55\u3002\u6bcf\u6761\u8bb0\u5f55\u5305\u542b\u4e5d\u4e2a\u6838\u5fc3\u5b57\u6bb5\uff08header, model, user, target, inputs, artifacts, outputs, outcomes, feedback\uff09\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u98ce\u9669\u7684\u62bd\u6837\u3001\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u4fdd\u7559\u7b56\u7565\u548c\u5199\u56de\u7f13\u5b58\u3002", "result": "MedLog\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u4e00\u81f4\u7684\u6a21\u578b\u6d3b\u52a8\u8bb0\u5f55\u65b9\u5f0f\uff0c\u80fd\u591f\u4fc3\u8fdb\u65b0\u6570\u636e\u5e93\u548c\u8f6f\u4ef6\u7684\u5f00\u53d1\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u533b\u7597AI\u7684\u6301\u7eed\u76d1\u63a7\u3001\u5ba1\u8ba1\u548c\u8fed\u4ee3\u6539\u8fdb\u3002", "conclusion": "MedLog\u4e3a\u533b\u7597AI\u7684\u6301\u7eed\u76d1\u7ba1\u3001\u6539\u8fdb\u4ee5\u53ca\u65b0\u578b\u6570\u5b57\u6d41\u884c\u75c5\u5b66\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u4e34\u5e8aAI\u900f\u660e\u5ea6\u548c\u53ef\u89c1\u6027\u4e0d\u8db3\u7684\u6311\u6218\u3002"}}
{"id": "2510.03808", "pdf": "https://arxiv.org/pdf/2510.03808", "abs": "https://arxiv.org/abs/2510.03808", "authors": ["Mehedi Hasan Emon"], "title": "Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches", "categories": ["cs.CL"], "comment": null, "summary": "This research explores the annotation of rhetorical relations in discourse\nusing the INCEpTION tool and compares manual annotation with automatic\napproaches based on large language models. The study focuses on sports reports\n(specifically cricket news) and evaluates the performance of BERT, DistilBERT,\nand Logistic Regression models in classifying rhetorical relations such as\nelaboration, contrast, background, and cause-effect. The results show that\nDistilBERT achieved the highest accuracy, highlighting its potential for\nefficient discourse relation prediction. This work contributes to the growing\nintersection of discourse parsing and transformer-based NLP. (This paper was\nconducted as part of an academic requirement under the supervision of Prof. Dr.\nRalf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:\nRhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,\nNLP.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528INCEpTION\u5de5\u5177\u63a2\u7d22\u8bed\u7bc7\u4fee\u8f9e\u5173\u7cfb\u6807\u6ce8\uff0c\u5e76\u6bd4\u8f83\u4e86\u4eba\u5de5\u6807\u6ce8\u4e0e\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u65b9\u6cd5\uff0c\u5728\u4f53\u80b2\u62a5\u9053\u4e2d\u53d1\u73b0DistilBERT\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u8bed\u7bc7\u4e2d\u4fee\u8f9e\u5173\u7cfb\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4eba\u5de5\u6807\u6ce8\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528INCEpTION\u5de5\u5177\u8fdb\u884c\u4fee\u8f9e\u5173\u7cfb\u6807\u6ce8\u3002\u5728\u4f53\u80b2\u62a5\u9053\uff08\u677f\u7403\u65b0\u95fb\uff09\u4e0a\uff0c\u5bf9\u6bd4\u4e86\u4eba\u5de5\u6807\u6ce8\u4e0e\u57fa\u4e8eBERT\u3001DistilBERT\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7684\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "DistilBERT\u6a21\u578b\u5728\u5206\u7c7b\u4fee\u8f9e\u5173\u7cfb\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DistilBERT\u5728\u9ad8\u6548\u8bed\u7bc7\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u8be5\u5de5\u4f5c\u4e3a\u8bed\u7bc7\u5206\u6790\u4e0e\u57fa\u4e8eTransformer\u7684NLP\u4ea4\u53c9\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.03550", "pdf": "https://arxiv.org/pdf/2510.03550", "abs": "https://arxiv.org/abs/2510.03550", "authors": ["Junbao Zhou", "Yuan Zhou", "Kesen Zhao", "Qingshan Xu", "Beier Zhu", "Richang Hong", "Hanwang Zhang"], "title": "Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!", "categories": ["cs.CV"], "comment": null, "summary": "Achieving streaming, fine-grained control over the outputs of autoregressive\nvideo diffusion models remains challenging, making it difficult to ensure that\nthey consistently align with user expectations. To bridge this gap, we propose\n\\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new\ntask that enables users to modify generated videos \\emph{anytime} on\n\\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and\nSG-I2V, REVEL unifies drag-style video manipulation as editing and animating\nvideo frames with both supporting user-specified translation, deformation, and\nrotation effects, making drag operations versatile. In resolving REVEL, we\nobserve: \\emph{i}) drag-induced perturbations accumulate in latent space,\ncausing severe latent distribution drift that halts the drag process;\n\\emph{ii}) streaming drag is easily disturbed by context frames, thereby\nyielding visually unnatural outcomes. We thus propose a training-free approach,\n\\textbf{DragStream}, comprising: \\emph{i}) an adaptive distribution\nself-rectification strategy that leverages neighboring frames' statistics to\neffectively constrain the drift of latent embeddings; \\emph{ii}) a\nspatial-frequency selective optimization mechanism, allowing the model to fully\nexploit contextual information while mitigating its interference via\nselectively propagating visual cues along generation. Our method can be\nseamlessly integrated into existing autoregressive video diffusion models, and\nextensive experiments firmly demonstrate the effectiveness of our DragStream.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faREVEL\u4efb\u52a1\uff0c\u65e8\u5728\u5b9e\u73b0\u5bf9\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6d41\u5f0f\u3001\u7cbe\u7ec6\u5316\u7684\u62d6\u62fd\u4ea4\u4e92\u63a7\u5236\u3002\u9488\u5bf9\u6f5c\u5728\u7a7a\u95f4\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u5e72\u6270\u4e24\u5927\u6311\u6218\uff0c\u63d0\u51fa\u65e0\u8bad\u7ec3\u65b9\u6cd5DragStream\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5e03\u81ea\u6821\u6b63\u548c\u7a7a\u9891\u9009\u62e9\u6027\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u5bf9\u751f\u6210\u89c6\u9891\u7684\u64cd\u63a7\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u6d41\u5f0f\u3001\u7cbe\u7ec6\u5316\u7684\u8f93\u51fa\u63a7\u5236\uff0c\u4f7f\u5176\u96be\u4ee5\u6301\u7eed\u7b26\u5408\u7528\u6237\u9884\u671f\u3002\u5177\u4f53\u6311\u6218\u5728\u4e8e\uff1a1) \u62d6\u62fd\u64cd\u4f5c\u5f15\u8d77\u7684\u6270\u52a8\u5728\u6f5c\u5728\u7a7a\u95f4\u79ef\u7d2f\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6f5c\u5728\u5206\u5e03\u6f02\u79fb\uff0c\u8fdb\u800c\u4e2d\u65ad\u62d6\u62fd\u8fc7\u7a0b\uff1b2) \u6d41\u5f0f\u62d6\u62fd\u6613\u53d7\u4e0a\u4e0b\u6587\u5e27\u5e72\u6270\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0a\u4e0d\u81ea\u7136\u7684\u7ed3\u679c\u3002", "method": "1. \u63d0\u51fa\u4e86\u65b0\u4efb\u52a1REVEL (stReaming drag-oriEnted interactiVe vidEo manipuLation)\uff0c\u7edf\u4e00\u4e86\u62d6\u62fd\u5f0f\u89c6\u9891\u64cd\u4f5c\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5e73\u79fb\u3001\u53d8\u5f62\u548c\u65cb\u8f6c\u7b49\u6548\u679c\u3002\n2. \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u65b9\u6cd5DragStream\u6765\u89e3\u51b3REVEL\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\uff1ai) \u4e00\u79cd\u81ea\u9002\u5e94\u5206\u5e03\u81ea\u6821\u6b63\u7b56\u7565\uff0c\u5229\u7528\u76f8\u90bb\u5e27\u7684\u7edf\u8ba1\u4fe1\u606f\u6709\u6548\u7ea6\u675f\u6f5c\u5728\u5d4c\u5165\u7684\u6f02\u79fb\uff1bii) \u4e00\u79cd\u7a7a\u9891\u9009\u62e9\u6027\u4f18\u5316\u673a\u5236\uff0c\u5728\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4f20\u64ad\u89c6\u89c9\u7ebf\u7d22\u6765\u51cf\u8f7b\u5176\u5e72\u6270\u3002", "result": "DragStream\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6709\u529b\u5730\u8bc1\u660e\u4e86DragStream\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165REVEL\u4efb\u52a1\u548c\u65e0\u8bad\u7ec3\u7684DragStream\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u6d41\u5f0f\u3001\u7cbe\u7ec6\u5316\u63a7\u5236\u65b9\u9762\u7684\u6311\u6218\uff0c\u6709\u6548\u6291\u5236\u4e86\u6f5c\u5728\u5206\u5e03\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u5e72\u6270\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u3001\u7b26\u5408\u7528\u6237\u671f\u671b\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u64cd\u4f5c\u3002"}}
{"id": "2510.03274", "pdf": "https://arxiv.org/pdf/2510.03274", "abs": "https://arxiv.org/abs/2510.03274", "authors": ["Tianao Zhang", "Zhiteng Li", "Xianglong Yan", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion large language models (dLLMs), which offer bidirectional context\nand flexible masked-denoising generation, are emerging as a compelling\nalternative to autoregressive (AR) LLMs. However, like AR LLMs, their model\nsizes continue to grow, motivating weight compression for deployment. Although\npost-training quantization (PTQ) is effective for AR LLMs, directly\ntransferring it to dLLMs at 2-bit leads to unsatisfactory performance. To\ntackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework\ntailored to dLLMs. Since masked-denoising activations in dLLMs differ from the\nfully visible signals assumed by standard PTQ methods, we introduce Masked\nCalibration Simulation (MCS) to align calibration with the timestep-dependent\nmasking, which yields more reliable calibrations. Moreover, we propose a\nData-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight\nrepresentations via an optimization algorithm. It performs iterative\napproximation guided by our simulated calibration data. In addition, under a\nstrict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a\nsensitivity-based precision allocation scheme that adaptively assigns bit width\nacross channel groups. When restricted to 2-bit precision, Quant-dLLM\nconsistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer\nPTQ methods on dLLMs. The code and models will be available at:\nhttps://github.com/ZTA2785/Quant-dLLM.", "AI": {"tldr": "Quant-dLLM\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u8bbe\u8ba1\u76842\u6bd4\u7279\u8d85\u4f4e\u7cbe\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a9\u7801\u6821\u51c6\u6a21\u62df\u3001\u6570\u636e\u611f\u77e5\u4efb\u610f\u987a\u5e8f\u91cf\u5316\u5668\u548c\u81ea\u9002\u5e94\u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PTQ\u65b9\u6cd5\u5728dLLMs\u4e0a\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u4f5c\u4e3a\u81ea\u56de\u5f52\uff08AR\uff09LLMs\u7684\u66ff\u4ee3\u65b9\u6848\u6b63\u5728\u5174\u8d77\uff0c\u4f46\u5176\u6a21\u578b\u89c4\u6a21\u6301\u7eed\u589e\u957f\uff0c\u9700\u8981\u8fdb\u884c\u6743\u91cd\u538b\u7f29\u4ee5\u90e8\u7f72\u3002\u7136\u800c\uff0c\u9002\u7528\u4e8eAR LLMs\u7684\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8edLLMs\u76842\u6bd4\u7279\u91cf\u5316\u65f6\uff0c\u6027\u80fd\u4e0d\u5c3d\u5982\u4eba\u610f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Quant-dLLM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u90e8\u5206\uff1a1) \u63a9\u7801\u6821\u51c6\u6a21\u62df\uff08MCS\uff09\uff0c\u4ee5\u4f7f\u6821\u51c6\u4e0edLLMs\u7684\u65f6\u95f4\u6b65\u4f9d\u8d56\u63a9\u7801\u751f\u6210\u5bf9\u9f50\uff0c\u83b7\u5f97\u66f4\u53ef\u9760\u7684\u6821\u51c6\u6570\u636e\uff1b2) \u6570\u636e\u611f\u77e5\u4efb\u610f\u987a\u5e8f\u91cf\u5316\u5668\uff08DAQ\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u548c\u6a21\u62df\u6821\u51c6\u6570\u636e\uff0c\u5b66\u4e60\u8d85\u4f4e\u6bd4\u7279\u6743\u91cd\u8868\u793a\uff1b3) \u81ea\u9002\u5e94\u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\uff08ABMP\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u654f\u611f\u5ea6\u7684\u7cbe\u5ea6\u5206\u914d\u65b9\u6848\uff0c\u5728\u4e25\u683c\u76842\u6bd4\u7279\u9884\u7b97\u4e0b\u81ea\u9002\u5e94\u5206\u914d\u901a\u9053\u7ec4\u7684\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "\u57282\u6bd4\u7279\u7cbe\u5ea6\u9650\u5236\u4e0b\uff0cQuant-dLLM\u5728dLLMs\u4e0a\u6301\u7eed\u53d6\u5f97\u4e86\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09AR-transfer PTQ\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Quant-dLLM\u662f\u4e00\u4e2a\u4e3adLLMs\u91cf\u8eab\u5b9a\u5236\u7684\u8d85\u4f4e\u6bd4\u7279PTQ\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6821\u51c6\u548c\u91cf\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86dLLMs\u57282\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u6027\u80fd\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u3002"}}
{"id": "2510.04040", "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86FaithCoT-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u94fe\u5f0f\u601d\u7ef4(CoT)\u89e3\u91ca\u4e0d\u5fe0\u5b9e\u6027\u7684\u5b9e\u4f8b\u7ea7\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "LLMs\u7684CoT\u89e3\u91ca\u5728\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5176\u5bf9\u5e95\u5c42\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u6027\u53d7\u5230\u8d28\u7591\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u673a\u5236\u7ea7\u5206\u6790\uff0c\u7f3a\u4e4f\u5b9e\u7528\u65b9\u6cd5\u6765\u5224\u65ad\u7279\u5b9aCoT\u8f68\u8ff9\u662f\u5426\u5fe0\u5b9e\u4e8e\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86FaithCoT-Bench\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u5b9e\u4f8b\u7ea7CoT\u4e0d\u5fe0\u5b9e\u6027\u68c0\u6d4b\u57fa\u51c6\u3002\u8be5\u6846\u67b6\u5c06\u4e0d\u5fe0\u5b9e\u6027\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u5224\u522b\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86FINE-CoT\uff0c\u4e00\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u56db\u79cdLLM\u5728\u56db\u4e2a\u9886\u57df\u751f\u6210\u76841000\u591a\u6761CoT\u8f68\u8ff9\uff0c\u5176\u4e2d\u5305\u62ec300\u591a\u4e2a\u5e26\u6709\u7ec6\u7c92\u5ea6\u539f\u56e0\u548c\u6b65\u9aa4\u7ea7\u8bc1\u636e\u7684\u4e0d\u5fe0\u5b9e\u5b9e\u4f8b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7cfb\u7edf\u8bc4\u4f30\u4e8611\u79cd\u4ee3\u8868\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6db5\u76d6\u53cd\u4e8b\u5b9e\u3001\u57fa\u4e8e\u5bf9\u6570\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8303\u5f0f\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u7ecf\u9a8c\u6027\u89c1\u89e3\uff0c\u9610\u660e\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u548c\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u4e2d\uff0c\u4e0d\u5fe0\u5b9e\u6027\u68c0\u6d4b\u9762\u4e34\u66f4\u5927\u7684\u6311\u6218\u3002", "conclusion": "FaithCoT-Bench\u5efa\u7acb\u4e86\u9996\u4e2a\u5168\u9762\u7684\u5b9e\u4f8b\u7ea7CoT\u5fe0\u5b9e\u6027\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76LLMs\u4e2d\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.03898", "pdf": "https://arxiv.org/pdf/2510.03898", "abs": "https://arxiv.org/abs/2510.03898", "authors": ["Nusrat Jahan Lia", "Shubhashis Roy Dipta", "Abdullah Khan Zehady", "Naymul Islam", "Madhusodan Chakraborty", "Abdullah Al Wasif"], "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles", "categories": ["cs.CL"], "comment": null, "summary": "Detecting media bias is crucial, specifically in the South Asian region.\nDespite this, annotated datasets and computational studies for Bangla political\nbias research remain scarce. Crucially because, political stance detection in\nBangla news requires understanding of linguistic cues, cultural context, subtle\nbiases, rhetorical strategies, code-switching, implicit sentiment, and\nsocio-political background. To address this, we introduce the first benchmark\ndataset of 200 politically significant and highly debated Bangla news articles,\nlabeled for government-leaning, government-critique, and neutral stances,\nalongside diagnostic analyses for evaluating large language models (LLMs). Our\ncomprehensive evaluation of 28 proprietary and open-source LLMs shows strong\nperformance in detecting government-critique content (F1 up to 0.83) but\nsubstantial difficulty with neutral articles (F1 as low as 0.00). Models also\ntend to over-predict government-leaning stances, often misinterpreting\nambiguous narratives. This dataset and its associated diagnostics provide a\nfoundation for advancing stance detection in Bangla media research and offer\ninsights for improving LLM performance in low-resource languages.", "AI": {"tldr": "\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u7acb\u573a\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3002\u672c\u6587\u6784\u5efa\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u7acb\u573a\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f3028\u4e2aLLM\uff0c\u53d1\u73b0LLM\u5728\u8bc6\u522b\u653f\u5e9c\u6279\u8bc4\u5185\u5bb9\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e2d\u7acb\u5185\u5bb9\u548c\u653f\u5e9c\u503e\u5411\u5185\u5bb9\u4e0a\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u5357\u4e9a\u5730\u533a\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u504f\u89c1\u7814\u7a76\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u7814\u7a76\u3002\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u7acb\u573a\u68c0\u6d4b\u9700\u7406\u89e3\u590d\u6742\u7684\u8bed\u8a00\u3001\u6587\u5316\u548c\u793e\u4f1a\u653f\u6cbb\u80cc\u666f\u3002", "method": "\u5f15\u5165\u9996\u4e2a\u5305\u542b200\u7bc7\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u65b0\u95fb\u6587\u7ae0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e3a\u201c\u653f\u5e9c\u503e\u5411\u201d\u3001\u201c\u653f\u5e9c\u6279\u8bc4\u201d\u548c\u201c\u4e2d\u7acb\u201d\u7acb\u573a\u3002\u5e76\u5bf928\u4e2a\u4e13\u6709\u548c\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8bca\u65ad\u6027\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u68c0\u6d4b\u201c\u653f\u5e9c\u6279\u8bc4\u201d\u5185\u5bb9\u4e0a\u8868\u73b0\u51fa\u8272\uff08F1\u9ad8\u8fbe0.83\uff09\uff0c\u4f46\u5728\u8bc6\u522b\u201c\u4e2d\u7acb\u201d\u6587\u7ae0\u4e0a\u5b58\u5728\u663e\u8457\u56f0\u96be\uff08F1\u4f4e\u81f30.00\uff09\u3002\u6a21\u578b\u8fd8\u503e\u5411\u4e8e\u8fc7\u5ea6\u9884\u6d4b\u201c\u653f\u5e9c\u503e\u5411\u201d\u7acb\u573a\uff0c\u5e38\u8bef\u89e3\u6a21\u7cca\u53d9\u8ff0\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ca\u5176\u8bca\u65ad\u5206\u6790\u4e3a\u5b5f\u52a0\u62c9\u8bed\u5a92\u4f53\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u4e3a\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2dLLM\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.03555", "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GAS-MIL\u6846\u67b6\uff0c\u4e00\u4e2a\u7075\u6d3b\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\uff0c\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u7b80\u5316\u4e86\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u867d\u5f3a\u5927\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u7684\u9002\u914d\u548c\u57fa\u51c6\u6d4b\u8bd5\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u5c24\u5176\u8003\u8651\u5230\u6a21\u578b\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) \u6846\u67b6\u3002GAS-MIL\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u6765\u81ea\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u7279\u5f81\u9009\u62e9\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u4ee5\u4fdd\u7559\u5176\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u5728\u4e09\u4e2a\u764c\u75c7\u6570\u636e\u96c6\uff08\u524d\u5217\u817a\u764cPANDA\u3001\u5375\u5de2\u764cUBC-OCEAN\u548c\u4e73\u817a\u764cTCGA-BrCa\uff09\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGAS-MIL\u7684\u8868\u73b0\u6301\u7eed\u4f18\u4e8e\u6216\u4e0e\u5355\u4e2a\u57fa\u7840\u6a21\u578b\u53ca\u73b0\u6709MIL\u65b9\u6cd5\u6301\u5e73\uff0c\u5145\u5206\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GAS-MIL\u901a\u8fc7\u5b9e\u73b0\u5f02\u6784\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u6574\u5408\uff0c\u7b80\u5316\u4e86\u75c5\u7406\u5b66\u4e2d\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u591a\u6a21\u6001\u548c\u7cbe\u51c6\u80bf\u7624\u5b66\u5e94\u7528\u5960\u5b9a\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.03275", "pdf": "https://arxiv.org/pdf/2510.03275", "abs": "https://arxiv.org/abs/2510.03275", "authors": ["Junhao Xia", "Ming Zhao", "Limin Xiao", "Xiujun Zhang"], "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large language models (LLMs) face significant computational and memory\nchallenges, making extremely low-bit quantization crucial for their efficient\ndeployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for\n1-bit LLMs of any size, a novel framework that enables extremely low-bit\nquantization of LLMs while preserving their linguistic reasoning capabilities.\nA distinctive feature of SDQ-LLM is the continuous adjustability of the\nOver-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM\nconstraints by selecting fractional OSR (e.g. 2.5 times) for an optimal\ntrade-off between model size and accuracy. SDQ-LLM uses upsampling combined\nwith Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding\nhigh-precision parameters into 1-bit or 1.58-bit representations, replacing the\nmultiplication operations within linear layers with addition. This approach\nsignificantly enhances inference efficiency under extremely low-bit\nquantization. To further reduce the loss of quantization precision, we\nincorporate Hadamard-based weight smoothing prior to quantization, improving\nthe stability and robustness of the weight representations. Furthermore, to\nfully leverage the continuity of the OSR and reduce precision loss, recognizing\nthe correlation between quantization sensitivity and weight variance, we\npropose a fine-grained, layer- and linear-wise OSR allocation strategy,\nMultiOSR. This strategy distributes OSR both across layers and within each\nlayer, based on weight variance and parameter scale. Finally, extensive\nexperiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a\nmore efficient and high-precision performance even under highly aggressive\nlow-OSR settings. Our code is available at\nhttps://github.com/Dreamlittlecat/LLM-Quant-Factory.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SDQ-LLM\u6846\u67b6\uff0c\u5229\u7528Sigma-Delta\u91cf\u5316\u5b9e\u73b01\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u8fde\u7eed\u53ef\u8c03\u7684\u8fc7\u91c7\u6837\u7387\uff08OSR\uff09\u3001\u54c8\u8fbe\u739b\u5e73\u6ed1\u548c\u591a\u5c42\u6b21OSR\u5206\u914d\u7b56\u7565\uff0c\u4ece\u800c\u5728\u6781\u7aef\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6548\u548c\u9ad8\u7cbe\u5ea6\u6027\u80fd\uff0c\u4ee5\u5e94\u5bf9LLM\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u56e0\u6b64\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u5bf9\u5176\u9ad8\u6548\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "SDQ-LLM\u6846\u67b6\u91c7\u7528Sigma-Delta\u91cf\u5316\u5668\u7ed3\u5408\u4e0a\u91c7\u6837\u6280\u672f\uff0c\u5c06LLM\u6743\u91cd\u4e8c\u503c\u5316\u6216\u4e09\u503c\u5316\uff081\u6bd4\u7279\u62161.58\u6bd4\u7279\uff09\uff0c\u7528\u52a0\u6cd5\u4ee3\u66ff\u7ebf\u6027\u5c42\u4e2d\u7684\u4e58\u6cd5\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002\u5b83\u5f15\u5165\u4e86\u8fde\u7eed\u53ef\u8c03\u7684\u8fc7\u91c7\u6837\u7387\uff08OSR\uff09\uff0c\u4ee5\u52a8\u6001\u9002\u5e94\u5185\u5b58\u6216VRAM\u9650\u5236\uff0c\u5b9e\u73b0\u6a21\u578b\u5927\u5c0f\u548c\u7cbe\u5ea6\u7684\u6700\u4f73\u6743\u8861\u3002\u4e3a\u51cf\u5c11\u91cf\u5316\u7cbe\u5ea6\u635f\u5931\uff0c\u91cf\u5316\u524d\u878d\u5165\u57fa\u4e8e\u54c8\u8fbe\u739b\u7684\u6743\u91cd\u5e73\u6ed1\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u91cf\u5316\u654f\u611f\u5ea6\u4e0e\u6743\u91cd\u65b9\u5dee\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u3001\u57fa\u4e8e\u5c42\u548c\u7ebf\u6027\u5c42\u5185\u90e8\u5206\u914d\u7684OSR\u7b56\u7565\uff08MultiOSR\uff09\uff0c\u6839\u636e\u6743\u91cd\u65b9\u5dee\u548c\u53c2\u6570\u89c4\u6a21\u5206\u5e03OSR\u3002", "result": "\u5728OPT\u548cLLaMA\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6781\u5177\u653b\u51fb\u6027\u7684\u4f4eOSR\u8bbe\u7f6e\u4e0b\uff0cSDQ-LLM\u4e5f\u80fd\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u9ad8\u7cbe\u5ea6\u7684\u6027\u80fd\u3002", "conclusion": "SDQ-LLM\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u91cf\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u65b9\u9762\u7684\u6311\u6218\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4fdd\u6301\u5f3a\u5927\u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548c\u9ad8\u6548\u7684\u90e8\u7f72\u6027\u80fd\u3002"}}
{"id": "2510.04048", "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "categories": ["cs.AI"], "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u6709\u53ef\u53d8\u6295\u7968\u9608\u503c\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u786e\u5b9a\u65f6\u201c\u5f03\u6743\u201d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u7684\u53ef\u4fe1\u5ea6\uff0c\u9002\u7528\u4e8e\u9ad8\u8981\u6c42\u573a\u666f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7f3a\u4e4f\u91cf\u5316\u5176\u56de\u7b54\u4e0d\u786e\u5b9a\u6027\u7684\u4fbf\u6377\u53ef\u9760\u65b9\u6cd5\uff0c\u5bfc\u81f4\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u96be\u4ee5\u83b7\u5f97\u4fe1\u4efb\u3002", "method": "\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u96c6\u6210\uff08ensembling\uff09\u65b9\u6cd5\uff0c\u5f15\u5165\u53ef\u53d8\u6295\u7968\u9608\u503c\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u96c6\u6210\u6a21\u578b\u5728\u4e3b\u5bfc\u54cd\u5e94\u672a\u8fbe\u5230\u9884\u8bbe\u9608\u503c\u65f6\u201c\u5f03\u6743\u201d\u4e0d\u63d0\u4f9b\u7b54\u6848\uff0c\u4ece\u800c\u63d0\u9ad8\u5269\u4f59\u7b54\u6848\u7684\u53ef\u4fe1\u5ea6\u3002\u7814\u7a76\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5728\u7b97\u672f\u95ee\u9898\u89e3\u51b3\u548c\u4e34\u5e8a\u7b14\u8bb0\u95ee\u7b54\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e24\u4e2a\u5b9e\u9a8c\u9886\u57df\u4e2d\uff0c\u4f7f\u7528\u9ad8\u5ea6\u9650\u5236\u6027\u7684\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u7b54\u6848\u7684\u53ef\u4fe1\u5ea6\uff0c\u540c\u65f6\u4ec5\u5bfc\u81f4\u76f8\u5bf9\u9002\u5ea6\u7684\u56de\u7b54\u7387\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "conclusion": "\u8be5\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u5ea6\u786e\u5b9a\u6027\u4f46\u4e0d\u9700\u8981\u6bcf\u4e2a\u95ee\u9898\u90fd\u83b7\u5f97\u81ea\u52a8\u5316\u7b54\u6848\u7684\u5e94\u7528\u573a\u666f\uff0c\u4f8b\u5982\u533b\u7597\u4fdd\u5065\u548c\u6570\u636e\u6807\u6ce8\u3002"}}
{"id": "2510.03913", "pdf": "https://arxiv.org/pdf/2510.03913", "abs": "https://arxiv.org/abs/2510.03913", "authors": ["Mohammad Amin Abbasi", "Hassan Naderi"], "title": "PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian", "categories": ["cs.CL"], "comment": null, "summary": "This study presents PsychoLexTherapy, a framework for simulating\npsychotherapeutic reasoning in Persian using small language models (SLMs). The\nframework tackles the challenge of developing culturally grounded,\ntherapeutically coherent dialogue systems with structured memory for multi-turn\ninteractions in underrepresented languages. To ensure privacy and feasibility,\nPsychoLexTherapy is optimized for on-device deployment, enabling use without\nexternal servers. Development followed a three-stage process: (i) assessing\nSLMs psychological knowledge with PsychoLexEval; (ii) designing and\nimplementing the reasoning-oriented PsychoLexTherapy framework; and (iii)\nconstructing two evaluation datasets-PsychoLexQuery (real Persian user\nquestions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark\nagainst multiple baselines. Experiments compared simple prompting, multi-agent\ndebate, and structured therapeutic reasoning paths. Results showed that\ndeliberate model selection balanced accuracy, efficiency, and privacy. On\nPsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic\nLLM-as-a-judge evaluation and was ranked highest by human evaluators in a\nsingle-turn preference study. In multi-turn tests with PsychoLexDialogue, the\nlong-term memory module proved essential: while naive history concatenation\ncaused incoherence and information loss, the full framework achieved the\nhighest ratings in empathy, coherence, cultural fit, and personalization.\nOverall, PsychoLexTherapy establishes a practical, privacy-preserving, and\nculturally aligned foundation for Persian psychotherapy simulation,\ncontributing novel datasets, a reproducible evaluation pipeline, and empirical\ninsights into structured memory for therapeutic reasoning.", "AI": {"tldr": "PsychoLexTherapy\u662f\u4e00\u4e2a\u4e3a\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5f3a\u8c03\u9690\u79c1\u4fdd\u62a4\u3001\u6587\u5316\u9002\u5e94\u6027\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u53ef\u5728\u8bbe\u5907\u7aef\u90e8\u7f72\u3002", "motivation": "\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u4e2d\uff0c\u5f00\u53d1\u5177\u6709\u6587\u5316\u6839\u57fa\u3001\u6cbb\u7597\u8fde\u8d2f\u6027\u3001\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u4e14\u6ce8\u91cd\u9690\u79c1\u7684\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PsychoLexTherapy\u6846\u67b6\uff0c\u4f18\u5316\u4ee5\u5b9e\u73b0\u8bbe\u5907\u7aef\u90e8\u7f72\u3002\u5f00\u53d1\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a\u8bc4\u4f30\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLM)\u7684\u5fc3\u7406\u77e5\u8bc6\u3001\u8bbe\u8ba1\u548c\u5b9e\u73b0\u9762\u5411\u63a8\u7406\u7684PsychoLexTherapy\u6846\u67b6\u3001\u6784\u5efa\u4e24\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff08PsychoLexQuery\u548cPsychoLexDialogue\uff09\u5e76\u4e0e\u591a\u79cd\u57fa\u7ebf\uff08\u7b80\u5355\u63d0\u793a\u3001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u3001\u7ed3\u6784\u5316\u6cbb\u7597\u63a8\u7406\u8def\u5f84\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPsychoLexTherapy\u5728\u5355\u8f6e\u8bc4\u4f30\uff08PsychoLexQuery\uff09\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u5e76\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u83b7\u5f97\u6700\u9ad8\u6392\u540d\u3002\u5728\u591a\u8f6e\u8bc4\u4f30\uff08PsychoLexDialogue\uff09\u4e2d\uff0c\u5176\u957f\u65f6\u8bb0\u5fc6\u6a21\u5757\u81f3\u5173\u91cd\u8981\uff0c\u5b8c\u6574\u6846\u67b6\u5728\u540c\u7406\u5fc3\u3001\u8fde\u8d2f\u6027\u3001\u6587\u5316\u5951\u5408\u5ea6\u548c\u4e2a\u6027\u5316\u65b9\u9762\u83b7\u5f97\u4e86\u6700\u9ad8\u8bc4\u4ef7\uff0c\u800c\u7b80\u5355\u7684\u5386\u53f2\u62fc\u63a5\u4f1a\u5bfc\u81f4\u4e0d\u8fde\u8d2f\u548c\u4fe1\u606f\u4e22\u5931\u3002", "conclusion": "PsychoLexTherapy\u4e3a\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u6a21\u62df\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u4fdd\u62a4\u9690\u79c1\u4e14\u6587\u5316\u5951\u5408\u7684\u57fa\u7840\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u53ca\u5173\u4e8e\u6cbb\u7597\u63a8\u7406\u4e2d\u7ed3\u6784\u5316\u8bb0\u5fc6\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2510.03558", "pdf": "https://arxiv.org/pdf/2510.03558", "abs": "https://arxiv.org/abs/2510.03558", "authors": ["Shen Chang", "Renran Tian", "Nicole Adams", "Nan Kong"], "title": "Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid", "categories": ["cs.CV"], "comment": null, "summary": "Rapid naloxone delivery via drones offers a promising solution for responding\nto opioid overdose emergencies (OOEs), by extending lifesaving interventions to\nmedically untrained bystanders before emergency medical services (EMS) arrive.\nRecognizing the critical role of bystander situational awareness (SA) in\nhuman-autonomy teaming (HAT), we address a key research gap in real-time SA\nassessment by introducing the Drone-Assisted Naloxone Delivery Simulation\nDataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,\nwhere college students without medical training act as bystanders tasked with\nadministering intranasal naloxone to a mock overdose victim. Leveraging this\ndataset, we propose a video-based real-time SA assessment framework that\nutilizes graph embeddings and transformer models to assess bystander SA in real\ntime. Our approach integrates visual perception and comprehension cues--such as\ngeometric, kinematic, and interaction graph features--and achieves\nhigh-performance SA prediction. It also demonstrates strong temporal\nsegmentation accuracy, outperforming the FINCH baseline by 9% in Mean over\nFrames (MoF) and 5% in Intersection over Union (IoU). This work supports the\ndevelopment of adaptive drone systems capable of guiding bystanders\neffectively, ultimately improving emergency response outcomes and saving lives.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u65e0\u4eba\u673a\u8f85\u52a9\u7eb3\u6d1b\u916e\u9012\u9001\u6a21\u62df\u6570\u636e\u96c6\uff08DANDSD\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u5b9e\u65f6\u6001\u52bf\u611f\u77e5\uff08SA\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u56fe\u5d4c\u5165\u548cTransformer\u6a21\u578b\uff0c\u8bc4\u4f30\u975e\u533b\u7597\u4e13\u4e1a\u65c1\u89c2\u8005\u5728\u6a21\u62df\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u7d27\u6025\u60c5\u51b5\u4e0b\u7684SA\uff0c\u4ee5\u652f\u6301\u5f00\u53d1\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "motivation": "\u5728\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u65e0\u4eba\u673a\u5feb\u901f\u9012\u9001\u7eb3\u6d1b\u916e\u4e3a\u975e\u533b\u7597\u4e13\u4e1a\u65c1\u89c2\u8005\u63d0\u4f9b\u4e86\u633d\u6551\u751f\u547d\u5e72\u9884\u7684\u673a\u4f1a\u3002\u7136\u800c\uff0c\u5728\u4eba\u673a\u534f\u4f5c\uff08HAT\uff09\u4e2d\uff0c\u65c1\u89c2\u8005\u7684\u6001\u52bf\u611f\u77e5\uff08SA\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9e\u65f6\u7684SA\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u9996\u5148\u521b\u5efa\u4e86DANDSD\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u6a21\u62df\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u573a\u666f\u4e2d\uff0c\u6355\u83b7\u4e86\u5927\u5b66\u751f\u65c1\u89c2\u8005\uff08\u65e0\u533b\u7597\u57f9\u8bad\uff09\u5411\u6a21\u62df\u53d7\u5bb3\u8005\u65bd\u7528\u9f3b\u5185\u7eb3\u6d1b\u916e\u65f6\u7684\u4eba\u673a\u534f\u4f5c\u6570\u636e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u5b9e\u65f6SA\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u56fe\u5d4c\u5165\u548cTransformer\u6a21\u578b\uff0c\u6574\u5408\u4e86\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u548c\u4ea4\u4e92\u56fe\u7279\u5f81\u7b49\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u7ebf\u7d22\u6765\u8bc4\u4f30\u65c1\u89c2\u8005\u7684SA\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684SA\u9884\u6d4b\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u65f6\u95f4\u5206\u5272\u51c6\u786e\u6027\u3002\u5728\u5e73\u5747\u5e27\u7387\uff08MoF\uff09\u4e0a\uff0c\u5176\u6027\u80fd\u6bd4FINCH\u57fa\u7ebf\u9ad8\u51fa9%\uff1b\u5728\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4e0a\uff0c\u9ad8\u51fa5%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6307\u5bfc\u65c1\u89c2\u8005\u7684\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u4ece\u800c\u6709\u671b\u6539\u5584\u7d27\u6025\u54cd\u5e94\u7ed3\u679c\u5e76\u633d\u6551\u751f\u547d\u3002"}}
{"id": "2510.03276", "pdf": "https://arxiv.org/pdf/2510.03276", "abs": "https://arxiv.org/abs/2510.03276", "authors": ["Qian Chen", "Linxin Yang", "Akang Wang", "Xiaodong Luo", "Yin Zhang"], "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The combination of linear transformations and non-linear activation functions\nforms the foundation of most modern deep neural networks, enabling them to\napproximate highly complex functions. This paper explores the introduction of\nquadratic transformations to further increase nonlinearity in neural networks,\nwith the aim of enhancing the performance of existing architectures. To reduce\nparameter complexity and computational complexity, we propose a lightweight\nquadratic enhancer that uses low-rankness, weight sharing, and sparsification\ntechniques. For a fixed architecture, the proposed approach introduces\nquadratic interactions between features at every layer, while only adding\nnegligible amounts of additional model parameters and forward computations. We\nconduct a set of proof-of-concept experiments for the proposed method across\nthree tasks: image classification, text classification, and fine-tuning\nlarge-language models. In all tasks, the proposed approach demonstrates clear\nand substantial performance gains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u4ee5\u589e\u52a0\u975e\u7ebf\u6027\uff0c\u4ece\u800c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u4f9d\u8d56\u7ebf\u6027\u53d8\u6362\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u3002\u4e3a\u8fdb\u4e00\u6b65\u589e\u52a0\u975e\u7ebf\u6027\u5e76\u589e\u5f3a\u73b0\u6709\u67b6\u6784\u7684\u6027\u80fd\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u5229\u7528\u4f4e\u79e9\u6027\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\uff0c\u5728\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u5f15\u5165\u7279\u5f81\u95f4\u7684\u4e8c\u6b21\u4ea4\u4e92\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u53ef\u5ffd\u7565\u4e0d\u8ba1\u7684\u6a21\u578b\u53c2\u6570\u548c\u524d\u5411\u8ba1\u7b97\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e09\u9879\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5747\u5c55\u73b0\u51fa\u660e\u663e\u4e14\u5b9e\u8d28\u6027\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u4e8c\u6b21\u53d8\u6362\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u4ece\u800c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2510.04051", "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "categories": ["cs.AI"], "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences.", "AI": {"tldr": "\u73b0\u6709LLM\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u57fa\u4e8eIRT\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u63d0\u51faLEGO-IRT\u6846\u67b6\uff0c\u901a\u8fc7\u652f\u6301\u8fde\u7eed\u5206\u6570\u548c\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86LLM\u8bc4\u4f30\u7684\u6570\u636e\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u6f5c\u5728\u80fd\u529b\u3002", "motivation": "LLM\u7684\u5168\u9762\u57fa\u51c6\u8bc4\u4f30\u8ba1\u7b97\u548c\u8d22\u52a1\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u57fa\u4e8eIRT\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1a\u4ec5\u9650\u4e8e\u4e8c\u5143\u6307\u6807\uff0c\u65e0\u6cd5\u5904\u7406\u751f\u6210\u4efb\u52a1\u7684\u8fde\u7eed\u5206\u6570\uff1b\u4e14\u4ec5\u5728\u5355\u4e00\u57fa\u51c6\u4e0a\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u8de8\u6307\u6807\u6216\u57fa\u51c6\u7684\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "\u5f15\u5165LEGO-IRT\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6570\u636e\u9ad8\u6548LLM\u8bc4\u4f30\u6846\u67b6\u3002\u5176\u8bbe\u8ba1\u539f\u751f\u652f\u6301\u4e8c\u5143\u548c\u8fde\u7eed\u8bc4\u4f30\u6307\u6807\u3002\u540c\u65f6\uff0c\u5f15\u5165\u56e0\u5b50\u5316\u67b6\u6784\u4ee5\u663e\u5f0f\u5efa\u6a21\u548c\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u4f30\u8ba1\u5206\u89e3\u4e3a\u901a\u7528\u7ec4\u4ef6\u548c\u7ed3\u6784\u7279\u5b9a\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u572870\u4e2aLLM\u548c5\u4e2a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLEGO-IRT\u4ec5\u4f7f\u75283%\u7684\u8bc4\u4f30\u9879\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7684\u80fd\u529b\u4f30\u8ba1\u3002\u7ed3\u5408\u7ed3\u6784\u77e5\u8bc6\u53ef\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe10%\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u4f30\u8ba1\u7684\u6f5c\u5728\u80fd\u529b\u53ef\u80fd\u4e0e\u4eba\u7c7b\u504f\u597d\u66f4\u5bc6\u5207\u5730\u5bf9\u9f50\u3002", "conclusion": "LEGO-IRT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u51c6\u786e\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709IRT\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u5206\u6570\u548c\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u80fd\u529b\u4f30\u8ba1\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03997", "pdf": "https://arxiv.org/pdf/2510.03997", "abs": "https://arxiv.org/abs/2510.03997", "authors": ["Junjie Luo", "Rui Han", "Arshana Welivita", "Zeleikun Di", "Jingfu Wu", "Xuzhe Zhi", "Ritu Agarwal", "Gordon Gao"], "title": "Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how patients perceive their physicians is essential to\nimproving trust, communication, and satisfaction. We present a large language\nmodel (LLM)-based pipeline that infers Big Five personality traits and five\npatient-oriented subjective judgments. The analysis encompasses 4.1 million\npatient reviews of 226,999 U.S. physicians from an initial pool of one million.\nWe validate the method through multi-model comparison and human expert\nbenchmarking, achieving strong agreement between human and LLM assessments\n(correlation coefficients 0.72-0.89) and external validity through correlations\nwith patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis\nreveals systematic patterns: male physicians receive higher ratings across all\ntraits, with largest disparities in clinical competence perceptions;\nempathy-related traits predominate in pediatrics and psychiatry; and all traits\npositively predict overall satisfaction. Cluster analysis identifies four\ndistinct physician archetypes, from \"Well-Rounded Excellent\" (33.8%, uniformly\nhigh traits) to \"Underperforming\" (22.6%, consistently low). These findings\ndemonstrate that automated trait extraction from patient narratives can provide\ninterpretable, validated metrics for understanding physician-patient\nrelationships at scale, with implications for quality measurement, bias\ndetection, and workforce development in healthcare.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4e86410\u4e07\u6761\u60a3\u8005\u8bc4\u8bba\uff0c\u4ee5\u91cf\u5316\u533b\u751f\u4e2a\u6027\u7279\u8d28\u548c\u60a3\u8005\u4e3b\u89c2\u8bc4\u4ef7\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u6a21\u5f0f\u5e76\u8bc6\u522b\u51fa\u56db\u79cd\u533b\u751f\u539f\u578b\u3002", "motivation": "\u7406\u89e3\u60a3\u8005\u5982\u4f55\u770b\u5f85\u533b\u751f\u5bf9\u4e8e\u589e\u8fdb\u4fe1\u4efb\u3001\u6c9f\u901a\u548c\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece410\u4e07\u6761\u60a3\u8005\u8bc4\u8bba\u4e2d\u63a8\u65ad\u533b\u751f\u7684\u5927\u4e94\u4eba\u683c\u7279\u8d28\u548c\u4e94\u79cd\u4ee5\u60a3\u8005\u4e3a\u5bfc\u5411\u7684\u4e3b\u89c2\u5224\u65ad\u3002\u901a\u8fc7\u591a\u6a21\u578b\u6bd4\u8f83\u548c\u4eba\u5de5\u4e13\u5bb6\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff08\u76f8\u5173\u7cfb\u65700.72-0.89\uff09\uff0c\u5e76\u4e0e\u60a3\u8005\u6ee1\u610f\u5ea6\u5448\u6b63\u76f8\u5173\uff08r=0.41-0.81\uff09\u3002\u7537\u6027\u533b\u751f\u5728\u6240\u6709\u7279\u8d28\u4e0a\u5f97\u5206\u66f4\u9ad8\uff0c\u4e34\u5e8a\u80fd\u529b\u611f\u77e5\u5dee\u5f02\u6700\u5927\uff1b\u513f\u79d1\u548c\u7cbe\u795e\u75c5\u5b66\u4e2d\u540c\u7406\u5fc3\u76f8\u5173\u7279\u8d28\u66f4\u7a81\u51fa\uff1b\u6240\u6709\u7279\u8d28\u5747\u6b63\u5411\u9884\u6d4b\u603b\u4f53\u6ee1\u610f\u5ea6\u3002\u8bc6\u522b\u51fa\u56db\u79cd\u533b\u751f\u539f\u578b\uff0c\u5305\u62ec\u201c\u5168\u9762\u4f18\u79c0\u578b\u201d\u548c\u201c\u8868\u73b0\u4e0d\u4f73\u578b\u201d\u3002", "conclusion": "\u901a\u8fc7\u60a3\u8005\u53d9\u8ff0\u81ea\u52a8\u63d0\u53d6\u7279\u8d28\uff0c\u53ef\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6307\u6807\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u7406\u89e3\u533b\u60a3\u5173\u7cfb\uff0c\u5bf9\u533b\u7597\u8d28\u91cf\u8861\u91cf\u3001\u504f\u89c1\u68c0\u6d4b\u548c\u52b3\u52a8\u529b\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.03570", "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Tesseract\u3001EasyOCR\u3001PaddleOCR\u548cTrOCR\u56db\u6b3e\u5f00\u6e90OCR\u7cfb\u7edf\u5728\u98df\u54c1\u5305\u88c5\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0c\u65e8\u5728\u63d0\u53d6\u914d\u6599\u8868\u548c\u8425\u517b\u6210\u5206\u3002Tesseract\u5728\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08CER\u6700\u4f4e\uff0cBLEU\u6700\u9ad8\uff09\uff0cEasyOCR\u5728\u51c6\u786e\u6027\u548c\u591a\u8bed\u8a00\u652f\u6301\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0cPaddleOCR\u8986\u76d6\u7387\u9ad8\u4f46\u901f\u5ea6\u6162\uff0cTrOCR\u8868\u73b0\u6700\u5f31\u3002\u7814\u7a76\u4e3a\u5305\u88c5OCR\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u98df\u54c1\u5305\u88c5\u4e0a\u7684OCR\u5bf9\u5408\u89c4\u6027\u548c\u8425\u517b\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u8bed\u8a00\u6587\u672c\u3001\u5bc6\u96c6\u5e03\u5c40\u3001\u5b57\u4f53\u591a\u6837\u3001\u7729\u5149\u548c\u66f2\u9762\u7b49\u6311\u6218\u800c\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f00\u6e90OCR\u7cfb\u7edf\u63d0\u53d6\u98df\u54c1\u5305\u88c5\u4e0a\u914d\u6599\u8868\u548c\u8425\u517b\u6210\u5206\u9762\u677f\u7684\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u4e86Tesseract\u3001EasyOCR\u3001PaddleOCR\u548cTrOCR\u56db\u6b3e\u5f00\u6e90OCR\u7cfb\u7edf\u3002\u4f7f\u7528231\u79cd\u4ea7\u54c1\uff081,628\u5f20\u56fe\u50cf\uff09\u7684\u6570\u636e\u96c6\u8bc4\u4f30\u901f\u5ea6\u548c\u8986\u76d6\u7387\uff0c\u5e76\u521b\u5efa\u4e86113\u5f20\u56fe\u50cf\uff0860\u79cd\u4ea7\u54c1\uff09\u7684\u771f\u503c\u5b50\u96c6\u7528\u4e8e\u51c6\u786e\u6027\u8bc4\u4f30\u3002\u8861\u91cf\u6307\u6807\u5305\u62ec\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u3001\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3001BLEU\u3001ROUGE-L\u3001F1\u3001\u8986\u76d6\u7387\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "\u5728\u771f\u503c\u5b50\u96c6\u4e0a\uff0cTesseract\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684CER\uff080.912\uff09\u548c\u6700\u9ad8\u7684BLEU\uff080.245\uff09\u3002EasyOCR\u5728\u51c6\u786e\u6027\u548c\u591a\u8bed\u8a00\u652f\u6301\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002PaddleOCR\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u6574\u7684\u8986\u76d6\u7387\uff0c\u4f46\u7531\u4e8eGPU\u4e0d\u517c\u5bb9\u5bfc\u81f4\u4ec5\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u901f\u5ea6\u8f83\u6162\u3002\u5c3d\u7ba1\u6709GPU\u52a0\u901f\uff0cTrOCR\u4ecd\u4ea7\u751f\u4e86\u6700\u5dee\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5305\u88c5\u7279\u5b9aOCR\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u5e03\u5c40\u611f\u77e5\u65b9\u6cd5\u548c\u6587\u672c\u5b9a\u4f4d\u3002"}}
{"id": "2510.03278", "pdf": "https://arxiv.org/pdf/2510.03278", "abs": "https://arxiv.org/abs/2510.03278", "authors": ["Filip Landgren"], "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages, 2 figures", "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing\nequations to solve differential equations under uncertainty. However,\ninterpreting uncertainty and overconfidence in B-PINNs requires care due to the\npoorly understood effects the physical constraints have on the network;\noverconfidence could reflect warranted precision, enforced by the constraints,\nrather than miscalibration. Motivated by the need to further clarify how\nindividual physical constraints shape these networks, we introduce a scalable,\nmatrix-free Laplace framework that decomposes the posterior Hessian into\ncontributions from each constraint and provides metrics to quantify their\nrelative influence on the loss landscape. Applied to the Van der Pol equation,\nour method tracks how constraints sculpt the network's geometry and shows,\ndirectly through the Hessian, how changing a single loss weight non-trivially\nredistributes curvature and effective dominance across the others.", "AI": {"tldr": "\u9488\u5bf9B-PINNs\u4e2d\u7269\u7406\u7ea6\u675f\u5bf9\u7f51\u7edc\u4e0d\u786e\u5b9a\u6027\u89e3\u91ca\u7684\u5f71\u54cd\u4e0d\u660e\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u540e\u9a8cHessian\u6765\u91cf\u5316\u5e76\u5206\u6790\u5404\u7ea6\u675f\u5bf9\u635f\u5931\u51fd\u6570\u5730\u5f62\u7684\u5851\u9020\u4f5c\u7528\u3002", "motivation": "B-PINNs\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u65f6\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u89e3\u91ca\u9700\u8c28\u614e\uff0c\u56e0\u4e3a\u7269\u7406\u7ea6\u675f\u5bf9\u7f51\u7edc\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u8fc7\u5ea6\u81ea\u4fe1\u53ef\u80fd\u6e90\u4e8e\u7ea6\u675f\u5e26\u6765\u7684\u7cbe\u786e\u6027\u800c\u975e\u8bef\u6821\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u4e00\u6b65\u9610\u660e\u5355\u4e2a\u7269\u7406\u7ea6\u675f\u5982\u4f55\u5851\u9020\u8fd9\u4e9b\u7f51\u7edc\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u65e0\u77e9\u9635\u7684\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u540e\u9a8cHessian\u5206\u89e3\u4e3a\u5404\u7269\u7406\u7ea6\u675f\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u6307\u6807\u6765\u91cf\u5316\u5b83\u4eec\u5728\u635f\u5931\u5730\u5f62\u4e0a\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eVan der Pol\u65b9\u7a0b\uff0c\u7ed3\u679c\u663e\u793a\u7269\u7406\u7ea6\u675f\u5982\u4f55\u5851\u9020\u7f51\u7edc\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7Hessian\u76f4\u63a5\u63ed\u793a\u4e86\u6539\u53d8\u5355\u4e00\u635f\u5931\u6743\u91cd\u5982\u4f55\u975e\u5e73\u51e1\u5730\u91cd\u65b0\u5206\u914d\u66f2\u7387\u548c\u5404\u7ea6\u675f\u95f4\u7684\u6709\u6548\u652f\u914d\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86B-PINNs\u4e2d\u7269\u7406\u7ea6\u675f\u5bf9\u7f51\u7edc\u51e0\u4f55\u548c\u635f\u5931\u5730\u5f62\u7684\u5f71\u54cd\u673a\u5236\u53ca\u5176\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u5176\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2510.04064", "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "categories": ["cs.AI"], "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u5b58\u5728\u660e\u786e\u7684\u60c5\u611f\u7f16\u7801\u673a\u5236\uff0c\u5b83\u5728\u6a21\u578b\u65e9\u671f\u5f62\u6210\u3001\u4e2d\u671f\u8fbe\u5230\u5cf0\u503c\uff0c\u4e14\u5177\u6709\u53ef\u5851\u6027\u548c\u6301\u4e45\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u63a2\u6d4b\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1LLMs\u80fd\u6a21\u62df\u60c5\u5546\uff0c\u4f46\u5176\u5185\u90e8\u60c5\u611f\u673a\u5236\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u60c5\u611f\u662f\u5982\u4f55\u3001\u5728\u4f55\u5904\u3001\u4ee5\u53ca\u6301\u7eed\u591a\u957f\u65f6\u95f4\u88ab\u7f16\u7801\u5728\u5176\u795e\u7ecf\u67b6\u6784\u4e2d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea640\u4e07\u6761Reddit\u6587\u672c\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5206\u7c7b\u3001\u91cd\u5199\u548c\u5408\u6210\u751f\u6210\uff0c\u5e73\u8861\u4e86\u4e03\u79cd\u57fa\u672c\u60c5\u7eea\u3002\u5229\u7528\u8f7b\u91cf\u7ea7\u201c\u63a2\u9488\u201d\u6280\u672f\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u8bfb\u53d6Qwen3\u548cLLaMA\u6a21\u578b\u9690\u85cf\u5c42\u4e2d\u7684\u4fe1\u606f\u3002", "result": "LLMs\u53d1\u5c55\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u3001\u5b9a\u4e49\u660e\u786e\u7684\u5185\u90e8\u60c5\u611f\u51e0\u4f55\u7ed3\u6784\uff0c\u5176\u6e05\u6670\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u589e\u5f3a\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u3002\u60c5\u611f\u4fe1\u53f7\u5e76\u975e\u4ec5\u662f\u6700\u540e\u4e00\u5c42\u73b0\u8c61\uff0c\u5b83\u5728\u7f51\u7edc\u65e9\u671f\u51fa\u73b0\u5e76\u4e8e\u4e2d\u671f\u8fbe\u5230\u5cf0\u503c\u3002\u6b64\u5916\uff0cLLMs\u7684\u5185\u90e8\u72b6\u6001\u65e2\u53ef\u5851\uff08\u53d7\u7b80\u5355\u7cfb\u7edf\u63d0\u793a\u5f71\u54cd\uff09\u53c8\u6301\u4e45\uff08\u521d\u59cb\u60c5\u611f\u57fa\u8c03\u5728\u6570\u767e\u4e2a\u540e\u7eedtoken\u4e2d\u4ecd\u53ef\u68c0\u6d4b\uff09\u3002", "conclusion": "\u7814\u7a76\u8d21\u732e\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3001\u5f00\u6e90\u63a2\u6d4b\u5de5\u5177\u548cLLMs\u5185\u90e8\u60c5\u611f\u56fe\u8c31\uff0c\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u3001\u66f4\u4e00\u81f4\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.03999", "pdf": "https://arxiv.org/pdf/2510.03999", "abs": "https://arxiv.org/abs/2510.03999", "authors": ["Yang Xu", "Xuanming Zhang", "Min-Hsuan Yeh", "Jwala Dhamala", "Ousmane Dia", "Rahul Gupta", "Yixuan Li"], "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions", "categories": ["cs.CL"], "comment": null, "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u957f\u5468\u671f\u4ea4\u4e92\u548c\u52a8\u6001\u538b\u529b\u4e0b\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6b3a\u9a97\u884c\u4e3a\u3002\u5b9e\u9a8c\u53d1\u73b0LLM\u7684\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u968f\u538b\u529b\u589e\u52a0\u800c\u589e\u5f3a\uff0c\u5e76\u6301\u7eed\u4fb5\u8680\u4fe1\u4efb\uff0c\u63ed\u793a\u4e86\u9690\u7792\u3001\u6a21\u68f1\u4e24\u53ef\u548c\u4f2a\u9020\u7b49\u6b3a\u9a97\u7b56\u7565\u3002", "motivation": "\u6b3a\u9a97\u662f\u4eba\u7c7b\u4ea4\u6d41\u4e2d\u666e\u904d\u5b58\u5728\u7684\u73b0\u8c61\uff0c\u4e5f\u65e5\u76ca\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e00\u4e2a\u65b0\u5174\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u5355\u8f6e\u63d0\u793a\uff0c\u672a\u80fd\u6355\u6349\u5230\u6b3a\u9a97\u7b56\u7565\u901a\u5e38\u5c55\u5f00\u7684\u957f\u5468\u671f\u4ea4\u4e92\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u5305\u542b\uff1a1. \u6267\u884c\u8005\uff08Performer\uff09\u667a\u80fd\u4f53\uff1a\u8d1f\u8d23\u5b8c\u6210\u4efb\u52a1\uff1b2. \u76d1\u7763\u8005\uff08Supervisor\uff09\u667a\u80fd\u4f53\uff1a\u8bc4\u4f30\u8fdb\u5c55\u3001\u63d0\u4f9b\u53cd\u9988\u5e76\u7ef4\u62a4\u4fe1\u4efb\u72b6\u6001\uff1b3. \u72ec\u7acb\u6b3a\u9a97\u5ba1\u8ba1\u5458\uff1a\u5ba1\u67e5\u5b8c\u6574\u7684\u4ea4\u4e92\u8f68\u8ff9\u4ee5\u8bc6\u522b\u6b3a\u9a97\u884c\u4e3a\u7684\u53d1\u751f\u65f6\u95f4\u548c\u65b9\u5f0f\u3002\u8be5\u6846\u67b6\u7528\u4e8e\u5728\u76f8\u4e92\u4f9d\u5b58\u7684\u4efb\u52a1\u5e8f\u5217\u548c\u52a8\u6001\u60c5\u5883\u538b\u529b\u4e0b\u63a2\u6d4b\u548c\u8bc4\u4f30LLM\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "result": "\u5bf911\u4e2a\u524d\u6cbf\u6a21\u578b\uff08\u5305\u62ec\u95ed\u6e90\u548c\u5f00\u6e90\u7cfb\u7edf\uff09\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u540e\u53d1\u73b0\uff1a\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u968f\u4e8b\u4ef6\u538b\u529b\u7684\u589e\u52a0\u800c\u589e\u5f3a\uff0c\u5e76\u6301\u7eed\u4fb5\u8680\u76d1\u7763\u8005\u7684\u4fe1\u4efb\u3002\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u9690\u7792\u3001\u6a21\u68f1\u4e24\u53ef\u548c\u4f2a\u9020\u7b49\u4e0d\u540c\u7684\u6b3a\u9a97\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\uff0c\u6b3a\u9a97\u662fLLM\u5728\u957f\u5468\u671f\u4ea4\u4e92\u4e2d\u4e00\u79cd\u65b0\u5174\u7684\u98ce\u9669\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u4e16\u754c\u3001\u4fe1\u4efb\u654f\u611f\u60c5\u5883\u4e2d\u8bc4\u4f30LLM\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03584", "pdf": "https://arxiv.org/pdf/2510.03584", "abs": "https://arxiv.org/abs/2510.03584", "authors": ["Chaoyu Li", "Tianzhi Li", "Fei Tao", "Zhenyu Zhao", "Ziqian Wu", "Maozheng Zhao", "Juntong Song", "Cheng Niu", "Pooyan Fazli"], "title": "FrameOracle: Learning What to See and How Much to See in Videos", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have advanced video understanding, but their\nperformance is limited by the number of input frames they can process. Existing\nframe sampling strategies, such as uniform or fixed-budget selection, often\nfail to adapt to variations in information density or task complexity,\nresulting in inefficiency and information loss. To address this, we present\nFrameOracle, a lightweight and plug-and-play module that predicts both (1)\nwhich frames are most relevant to a given query and (2) how many frames are\nneeded. FrameOracle is trained using a four-stage curriculum, with the first\nthree stages relying on weak proxy signals such as cross-modal similarity. In\nthe final stage, it leverages stronger supervision from a new dataset we\nintroduce, FrameOracle-41K, the first large-scale VideoQA collection to provide\nkeyframe annotations specifying the minimal set of frames required to answer\neach question. Extensive experiments across five VLMs and six benchmarks\ndemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4\nframes without any loss in accuracy. When starting from 64-frame candidates, it\nreduces the input to an average of 13.9 frames while improving accuracy by\n1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable\nvideo understanding.", "AI": {"tldr": "FrameOracle\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u9884\u6d4b\u76f8\u5173\u5e27\u548c\u6240\u9700\u5e27\u6570\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u5904\u7406\u8f93\u5165\u5e27\u6570\u9650\u5236\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u89c6\u9891\u7406\u89e3\u6027\u80fd\u53d7\u9650\u4e8e\u53ef\u5904\u7406\u7684\u8f93\u5165\u5e27\u6570\u3002\u5f53\u524d\u7684\u5e27\u91c7\u6837\u7b56\u7565\uff08\u5982\u5747\u5300\u6216\u56fa\u5b9a\u9884\u7b97\u9009\u62e9\uff09\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u672a\u80fd\u9002\u5e94\u4fe1\u606f\u5bc6\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u6027\u53d8\u5316\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FrameOracle\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u7528\u4e8e\u9884\u6d4b\u4e0e\u7ed9\u5b9a\u67e5\u8be2\u6700\u76f8\u5173\u7684\u5e27\u4ee5\u53ca\u6240\u9700\u7684\u5e27\u6570\u3002FrameOracle\u91c7\u7528\u56db\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u524d\u4e09\u9636\u6bb5\u4f9d\u8d56\u5f31\u4ee3\u7406\u4fe1\u53f7\uff08\u5982\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\uff09\uff0c\u6700\u540e\u9636\u6bb5\u5229\u7528\u65b0\u5f15\u5165\u7684FrameOracle-41K\u6570\u636e\u96c6\uff08\u9996\u4e2a\u63d0\u4f9b\u5173\u952e\u5e27\u6807\u6ce8\u7684\u5927\u89c4\u6a21VideoQA\u96c6\u5408\uff09\u7684\u5f3a\u76d1\u7763\u3002", "result": "\u5728\u4e94\u4e2aVLM\u548c\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFrameOracle\u5c0616\u5e27\u8f93\u5165\u5e73\u5747\u51cf\u5c11\u523010.4\u5e27\uff0c\u540c\u65f6\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002\u4ece64\u5e27\u5019\u9009\u4e2d\uff0c\u5b83\u5c06\u8f93\u5165\u5e73\u5747\u51cf\u5c11\u523013.9\u5e27\uff0c\u540c\u65f6\u5c06\u51c6\u786e\u6027\u63d0\u9ad81.4%\u3002\u8be5\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u89c6\u9891\u7406\u89e3\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "FrameOracle\u901a\u8fc7\u667a\u80fd\u7684\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86VLM\u5728\u5904\u7406\u5927\u91cf\u89c6\u9891\u5e27\u65f6\u7684\u9650\u5236\uff0c\u4e3a\u53ef\u6269\u5c55\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.03279", "pdf": "https://arxiv.org/pdf/2510.03279", "abs": "https://arxiv.org/abs/2510.03279", "authors": ["Youjin Wang", "Yangjingyi Chen", "Jiahao Yan", "Jiaxuan Lu", "Xiao Sun"], "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Mamba\u6a21\u578b\u7684\u957f\u7a0b\u8bb0\u5fc6\u8870\u51cf\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86MemMamba\u67b6\u6784\uff0c\u901a\u8fc7\u72b6\u6001\u6458\u8981\u548c\u591a\u5c42\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u957f\u7a0b\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u548c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u957f\u5e8f\u5217\u6a21\u578b\u5728\u6548\u7387\u548c\u5185\u5b58\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u9650\u5236\u3002Mamba\u7b49\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5176\u957f\u7a0b\u8bb0\u5fc6\u5448\u6307\u6570\u7ea7\u8870\u51cf\uff0c\u6025\u9700\u7406\u89e3\u5176\u8bb0\u5fc6\u673a\u5236\u5e76\u52a0\u4ee5\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u548c\u4fe1\u606f\u8bba\u5206\u6790\u63ed\u793aMamba\u7684\u8bb0\u5fc6\u8870\u51cf\u673a\u5236\uff1b\u5f15\u5165\u6c34\u5e73-\u5782\u76f4\u8bb0\u5fc6\u4fdd\u771f\u5ea6\u6307\u6807\u91cf\u5316\u4fe1\u606f\u635f\u5931\uff1b\u63d0\u51faMemMamba\u67b6\u6784\uff0c\u7ed3\u5408\u72b6\u6001\u6458\u8981\u673a\u5236\u3001\u8de8\u5c42\u548c\u8de8token\u6ce8\u610f\u529b\u6765\u7f13\u89e3\u957f\u7a0b\u9057\u5fd8\u3002", "result": "MemMamba\u5728PG19\u548cPasskey Retrieval\u7b49\u957f\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709Mamba\u53d8\u4f53\u548cTransformers\uff0c\u63a8\u7406\u6548\u7387\u63d0\u534748%\u3002", "conclusion": "MemMamba\u5728\u590d\u6742\u6027-\u5185\u5b58\u6743\u8861\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.04073", "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "categories": ["cs.AI"], "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9053\u5fb7\u951a\u5b9a\u7cfb\u7edf\uff08MAS\uff09\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u3001\u9884\u6d4b\u548c\u7f13\u89e3\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4ee3\u7406\u4e2d\u7684\u4ef7\u503c\u6f02\u79fb\uff0c\u4ee5\u786e\u4fddAI\u884c\u4e3a\u4e0e\u4eba\u7c7b\u9053\u5fb7\u548c\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u968f\u7740AI\u6210\u4e3a\u8d85\u7ea7\u52a9\u624b\uff0c\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u7279\u522b\u662f\u4ef7\u503c\u6f02\u79fb\u98ce\u9669\uff0c\u5373AI\u53ef\u80fd\u56e0\u73af\u5883\u53d8\u5316\u3001\u5b66\u4e60\u52a8\u6001\u6216\u610f\u5916\u4f18\u5316\u800c\u504f\u79bb\u9884\u8bbe\u4ef7\u503c\u89c2\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u9053\u5fb7\u8fdd\u89c4\uff0c\u662f\u5f53\u524d\u4e9f\u9700\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u9053\u5fb7\u951a\u5b9a\u7cfb\u7edf\uff08MAS\uff09\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u65ad\u6765\u76d1\u63a7\u4ef7\u503c\u72b6\u6001\uff0cLSTM\u7f51\u7edc\u6765\u9884\u6d4b\u6f02\u79fb\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6cbb\u7406\u5c42\u6765\u8fdb\u884c\u81ea\u9002\u5e94\u5e72\u9884\u3002MAS\u5f3a\u8c03\u4f4e\u5ef6\u8fdf\u54cd\u5e94\uff08<20 ms\uff09\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u4eba\u7c7b\u53cd\u9988\u51cf\u5c11\u8bef\u62a5\u548c\u8b66\u62a5\u75b2\u52b3\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cMAS\u80fd\u591f\u5c06\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\u51cf\u5c1180%\u6216\u66f4\u591a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0885%\uff09\u548c\u4f4e\u8bef\u62a5\u7387\uff08\u9002\u5e94\u540e\u4e3a0.08\uff09\u3002\u5bf9\u76ee\u6807\u672a\u5bf9\u9f50\u4ee3\u7406\u7684\u4e25\u683c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MAS\u7684\u53ef\u6269\u5c55\u6027\u548c\u54cd\u5e94\u6027\u3002", "conclusion": "MAS\u901a\u8fc7\u5176\u9884\u6d4b\u6027\u548c\u81ea\u9002\u5e94\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u521b\u7684\u4ef7\u503c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f18\u4e8e\u9759\u6001\u5bf9\u9f50\u65b9\u6cd5\u3002\u5176\u8d21\u732e\u5305\u62ecMAS\u67b6\u6784\u3001\u5f3a\u8c03\u901f\u5ea6\u548c\u53ef\u7528\u6027\u7684\u5b9e\u8bc1\u7ed3\u679c\u3001\u8de8\u9886\u57df\u9002\u7528\u6027\u89c1\u89e3\uff0c\u4ee5\u53ca\u5f00\u653e\u6e90\u4ee3\u7801\u4ee5\u4f9b\u590d\u5236\u3002"}}
{"id": "2510.04001", "pdf": "https://arxiv.org/pdf/2510.04001", "abs": "https://arxiv.org/abs/2510.04001", "authors": ["Xuankang Zhang", "Jiangming Liu"], "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "The COVID-19 pandemic causes severe social and economic disruption around the\nworld, raising various subjects that are discussed over social media.\nIdentifying pandemic-related named entities as expressed on social media is\nfundamental and important to understand the discussions about the pandemic.\nHowever, there is limited work on named entity recognition on this topic due to\nthe following challenges: 1) COVID-19 texts in social media are informal and\ntheir annotations are rare and insufficient to train a robust recognition\nmodel, and 2) named entity recognition in COVID-19 requires extensive\ndomain-specific knowledge. To address these issues, we propose a novel entity\nknowledge augmentation approach for COVID-19, which can also be applied in\ngeneral biomedical named entity recognition in both informal text format and\nformal text format. Experiments carried out on the COVID-19 tweets dataset and\nPubMed dataset show that our proposed entity knowledge augmentation improves\nNER performance in both fully-supervised and few-shot settings. Our source code\nis publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master", "AI": {"tldr": "\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e0aCOVID-19\u6587\u672c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u9762\u4e34\u7684\u975e\u6b63\u5f0f\u6027\u3001\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u77e5\u8bc6\u9700\u6c42\u7b49\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5168\u76d1\u7763\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684NER\u6027\u80fd\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u4e00\u822c\u751f\u7269\u533b\u5b66\u9886\u57df\u3002", "motivation": "COVID-19\u75ab\u60c5\u5f15\u53d1\u793e\u4ea4\u5a92\u4f53\u4e0a\u5927\u91cf\u8ba8\u8bba\uff0c\u8bc6\u522b\u75ab\u60c5\u76f8\u5173\u547d\u540d\u5b9e\u4f53\u5bf9\u4e8e\u7406\u89e3\u8ba8\u8bba\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8be5\u9886\u57dfNER\u7814\u7a76\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u975e\u6b63\u5f0f\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e0d\u8db3\u4ee5\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\uff0c\u4e14NER\u9700\u5927\u91cf\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3COVID-19\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u9762\u4e34\u7684\u6570\u636e\u548c\u77e5\u8bc6\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4e5f\u53ef\u5e94\u7528\u4e8e\u4e00\u822c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u5305\u62ec\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6587\u672c\u683c\u5f0f\u3002", "result": "\u5728COVID-19\u63a8\u6587\u6570\u636e\u96c6\u548cPubMed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\u5728\u5168\u76d1\u7763\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u663e\u8457\u63d0\u5347\u4e86\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86COVID-19\u793e\u4ea4\u5a92\u4f53\u6587\u672cNER\u7684\u6311\u6218\uff0c\u6709\u6548\u5229\u7528\u77e5\u8bc6\u5f25\u8865\u4e86\u6570\u636e\u4e0d\u8db3\uff0c\u63d0\u9ad8\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u4e86\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u6f5c\u529b\u3002"}}
{"id": "2510.03591", "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408Co-FineTuning (CFT) \u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u6709\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u4ee5\u53ca\u5229\u7528\u6765\u81ea\u76ee\u6807\u6e38\u620f\u548c\u591a\u9886\u57df\u6e38\u620f\u7684\u6709\u6807\u7b7e\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u4e2d\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u89c6\u89c9bug\u624b\u52a8\u8bc6\u522b\u8017\u65f6\u8017\u529b\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u5c3d\u7ba1\u76d1\u7763\u89c6\u89c9bug\u68c0\u6d4b\u6a21\u578b\u6709\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u89c6\u89c9bug\u53d1\u751f\u9891\u7387\u4f4e\uff0c\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u6210\u4e3a\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408Co-FineTuning (CFT) \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u6709\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u5229\u7528\u6765\u81ea\u76ee\u6807\u6e38\u620f\u548c\u591a\u9886\u57df\u6e38\u620f\u7684\u6709\u6807\u7b7e\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u65e0\u6807\u7b7e\u6570\u636e\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5b66\u4e60\u3002\u6b64\u7b56\u7565\u6700\u5927\u9650\u5ea6\u5730\u5229\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\uff0c\u5927\u5e45\u51cf\u5c11\u5bf9\u7279\u5b9a\u76ee\u6807\u6e38\u620f\u6807\u6ce8\u6837\u672c\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u3002\u5373\u4f7f\u53ea\u7528\u76ee\u6807\u6e38\u620f50%\u7684\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0cCFT\u4ecd\u80fd\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "CFT\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u89c6\u9891\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u4e2d\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6570\u636e\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u5e76\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u4f9d\u7136\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.03280", "pdf": "https://arxiv.org/pdf/2510.03280", "abs": "https://arxiv.org/abs/2510.03280", "authors": ["Jinjie Ni", "Qian Liu", "Chao Du", "Longxu Dou", "Hang Yan", "Zili Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "Training Optimal Large Diffusion Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Quokka, the first systematic scaling law for diffusion language\nmodels (DLMs), encompassing both compute-constrained and data-constrained\nregimes, and studying the key modeling and optimization designs. Quokka is a\ngood friend of Chinchilla and provides wider scopes. We hope the results would\nbring short-term practical guidance in DLMs training and long-term inspirations\nfor the whole AI community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Quokka\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u7684\u7cfb\u7edf\u6027\u7f29\u653e\u5b9a\u5f8b\uff0c\u6db5\u76d6\u8ba1\u7b97\u548c\u6570\u636e\u53d7\u9650\u573a\u666f\uff0c\u5e76\u7814\u7a76\u4e86\u5173\u952e\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u65e8\u5728\u5f25\u8865\u6269\u6563\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7cfb\u7edf\u6027\u7f29\u653e\u5b9a\u5f8b\u7684\u7a7a\u7f3a\uff0c\u4e3aDLM\u8bad\u7ec3\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2aAI\u793e\u533a\u5e26\u6765\u542f\u53d1\u3002", "method": "\u901a\u8fc7\u5f15\u5165Quokka\uff0c\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u53d7\u9650\u548c\u6570\u636e\u53d7\u9650\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u7684\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "Quokka\u662fChinchilla\u7684\u5ef6\u4f38\uff0c\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u8303\u56f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u671b\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u77ed\u671f\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2aAI\u793e\u533a\u5e26\u6765\u957f\u671f\u542f\u53d1\u3002"}}
{"id": "2510.04089", "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSPOGW\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u504f\u597d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7fa4\u7ec4\u6bd4\u8f83\u548c\u8fde\u7eed\u7a7a\u95f4\u4f18\u5316\uff0c\u81ea\u52a8\u5316\u751f\u6210\u548c\u4f18\u5316LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709SOTA\u3002", "motivation": "\u8bbe\u8ba1LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u9700\u8981\u5927\u91cf\u624b\u52a8\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u8868\u793a\u80fd\u529b\u3001\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u53ca\u4f9d\u8d56\u79bb\u6563\u4f18\u5316\u548c\u4e24\u4e24\u6bd4\u8f83\u8303\u5f0f\u3002", "method": "\u5f15\u5165\u540d\u4e3aSPOGW\u7684\u57fa\u4e8e\u5206\u6570\u7684\u504f\u597d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7fa4\u7ec4\u6bd4\u8f83\u76f4\u63a5\u4f5c\u7528\u4e8e\u57fa\u6570\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8fed\u4ee3\u79bb\u7ebfGRPO (ioGRPO) \u548c\u4f18\u52bf\u63a9\u853dKL\u6563\u5ea6 (mKL)\uff0c\u901a\u8fc7\u5f3a\u8c03\u7b56\u7565\u54cd\u5e94\u7684\u6709\u5229\u533a\u57df\u6765\u8c03\u8282\u8bad\u7ec3\u66f4\u65b0\u3002", "result": "\u5728\u6db5\u76d6\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u95ee\u7b54\u7684\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSPOGW\u7684\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6301\u5e73\u6216\u8d85\u8d8a\u3002", "conclusion": "SPOGW\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u548c\u4f18\u5316\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u5177\u6709\u524d\u77bb\u6027\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.04002", "pdf": "https://arxiv.org/pdf/2510.04002", "abs": "https://arxiv.org/abs/2510.04002", "authors": ["Bo Yang", "Yunkui Chen", "Lanfei Feng", "Yu Zhang", "Xiao Xu", "Jianyu Zhang", "Nueraili Aierken", "Runhe Huang", "Hongjian Lin", "Yibin Ying", "Shijian Li"], "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite", "categories": ["cs.CL"], "comment": null, "summary": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgriGPT-VL\u5957\u4ef6\uff0c\u4e00\u4e2a\u4e3a\u519c\u4e1a\u9886\u57df\u5b9a\u5236\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5305\u542b\u8fc4\u4eca\u6700\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93\u3001\u4e13\u7528\u6a21\u578b\u548c\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5728\u519c\u4e1a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u519c\u4e1a\u5e94\u7528\u53d7\u9650\u4e8e\u9886\u57df\u5b9a\u5236\u6a21\u578b\u3001\u7cbe\u9009\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93\u548c\u4e25\u683c\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a00\u7f3a\u6027\u3002", "method": "1. \u6784\u5efaAgri-3M-VL\uff0c\u4e00\u4e2a\u901a\u8fc7\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u5668\u7b56\u5c55\u7684\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93\uff08\u5305\u542b1M\u56fe\u50cf-\u5b57\u5e55\u5bf9\u30012M\u56fe\u50cf-VQA\u5bf9\u300150K\u4e13\u5bb6\u7ea7VQA\u5b9e\u4f8b\u548c15K GRPO\u5f3a\u5316\u5b66\u4e60\u6837\u672c\uff09\u30022. \u5f00\u53d1AgriGPT-VL\uff0c\u4e00\u4e2a\u519c\u4e1a\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u63a5\u5730\u3001\u591a\u6a21\u6001\u6d45\u5c42/\u6df1\u5c42\u5bf9\u9f50\u548cGRPO\u4f18\u5316\u7684\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u8fdb\u884c\u8bad\u7ec3\u30023. \u5efa\u7acbAgriBench-VL-4K\uff0c\u4e00\u4e2a\u5305\u542b\u5f00\u653e\u5f0f\u548c\u56fe\u50cf\u63a5\u5730\u95ee\u9898\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u91c7\u7528\u591a\u6307\u6807\u8bc4\u4f30\u548cLLM-as-a-judge\u6846\u67b6\u3002", "result": "AgriGPT-VL\u5728AgriBench-VL-4K\u4e0a\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728LLM-as-a-judge\u8bc4\u4f30\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6210\u5bf9\u80dc\u7387\u3002\u540c\u65f6\uff0c\u5b83\u5728\u7eaf\u6587\u672cAgriBench-13K\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8bed\u8a00\u80fd\u529b\u6ca1\u6709\u660e\u663e\u4e0b\u964d\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5bf9\u9f50\u548cGRPO\u4f18\u5316\u9636\u6bb5\u5e26\u6765\u7684\u4e00\u81f4\u6536\u76ca\u3002", "conclusion": "AgriGPT-VL\u5957\u4ef6\u6709\u6548\u89e3\u51b3\u4e86\u519c\u4e1a\u9886\u57df\u591a\u6a21\u6001AI\u7684\u73b0\u6709\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u519c\u4e1a\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6240\u6709\u8d44\u6e90\u652f\u6301\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5728\u8d44\u6e90\u532e\u4e4f\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2510.03598", "pdf": "https://arxiv.org/pdf/2510.03598", "abs": "https://arxiv.org/abs/2510.03598", "authors": ["Alexander V. Mantzaris"], "title": "Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper asks whether the Hierarchical Reasoning Model (HRM) with the two\nTransformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep\nsupervision, Rotary Position Embeddings, and RMSNorm can serve as a practical\nimage classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a\ndeliberately raw regime: no data augmentation, identical optimizer family with\none-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes\nstably and performs well on MNIST ($\\approx 98\\%$ test accuracy), but on small\nnatural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches\n65.0\\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains\n77.2\\% while training $\\sim 30\\times$ faster per epoch; on CIFAR-100, HRM\nachieves only 29.7\\% test accuracy despite 91.5\\% train accuracy, while the\nsame CNN reaches 45.3\\% test with 50.5\\% train accuracy. Loss traces and error\nanalyses indicate healthy optimization but insufficient image-specific\ninductive bias for HRM in this regime. It is concluded that, for\nsmall-resolution image classification without augmentation, HRM is not\ncompetitive with even simple convolutional architectures as the HRM currently\nexist but this does not exclude possibilities that modifications to the model\nmay allow it to improve greatly.", "AI": {"tldr": "\u5728\u65e0\u6570\u636e\u589e\u5f3a\u7684\u5c0f\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cHRM\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0d\u5982\u7b80\u5355CNN\uff0c\u4f46\u672a\u6765\u6709\u6539\u8fdb\u6f5c\u529b\u3002", "motivation": "\u8bc4\u4f30Hierarchical Reasoning Model (HRM) \u5728\u4f5c\u4e3a\u5b9e\u7528\u56fe\u50cf\u5206\u7c7b\u5668\u65f6\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7684\u539f\u59cb\u6761\u4ef6\u4e0b\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u5305\u542b\u4e24\u4e2aTransformer\u98ce\u683c\u6a21\u5757 $(f_L,f_H)$ \u7684HRM\uff0c\u7ed3\u5408\u4e00\u6b65 (DEQ\u98ce\u683c) \u8bad\u7ec3\u3001\u6df1\u5ea6\u76d1\u7763\u3001\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548cRMSNorm\u3002\u6a21\u578b\u5728MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6761\u4ef6\u662f\u4e25\u683c\u7684\u201c\u539f\u59cb\u201d\u8bbe\u7f6e\uff1a\u65e0\u6570\u636e\u589e\u5f3a\u3001\u7279\u5b9a\u4f18\u5316\u5668\uff08\u5355epoch\u9884\u70ed\u548c\u4f59\u5f26\u8870\u51cf\uff09\u548c\u6807\u7b7e\u5e73\u6ed1\u3002", "result": "HRM\u5728MNIST\u4e0a\u8868\u73b0\u826f\u597d\uff08\u7ea698%\u6d4b\u8bd5\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5bf9\u5c0f\u578b\u81ea\u7136\u56fe\u50cf\uff08CIFAR-10, CIFAR-100\uff09\u8fc7\u62df\u5408\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u5728CIFAR-10\u4e0a\uff0cHRM\u8fbe\u523065.0%\uff0c\u8fdc\u4f4e\u4e8eConv-BN-ReLU\u57fa\u7ebf\u768477.2%\uff08\u4e14\u57fa\u7ebf\u8bad\u7ec3\u5feb30\u500d\uff09\uff1b\u5728CIFAR-100\u4e0a\uff0cHRM\u6d4b\u8bd5\u51c6\u786e\u7387\u4ec5\u4e3a29.7%\uff08\u8bad\u7ec3\u51c6\u786e\u738791.5%\uff09\uff0c\u800cCNN\u57fa\u7ebf\u8fbe\u523045.3%\u3002\u5206\u6790\u8868\u660e\uff0c\u4f18\u5316\u8fc7\u7a0b\u5065\u5eb7\uff0c\u4f46HRM\u7f3a\u4e4f\u9488\u5bf9\u56fe\u50cf\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "conclusion": "\u5728\u65e0\u6570\u636e\u589e\u5f3a\u7684\u5c0f\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u7c7b\u573a\u666f\u4e0b\uff0c\u5f53\u524d\u7248\u672c\u7684HRM\u6a21\u578b\u4e0d\u5177\u5907\u4e0e\u7b80\u5355\u5377\u79ef\u67b6\u6784\u7ade\u4e89\u7684\u80fd\u529b\u3002\u4f46\u7814\u7a76\u4e5f\u6307\u51fa\uff0c\u672a\u6765\u7684\u6a21\u578b\u4fee\u6539\u53ef\u80fd\u4f7f\u5176\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002"}}
{"id": "2510.03282", "pdf": "https://arxiv.org/pdf/2510.03282", "abs": "https://arxiv.org/abs/2510.03282", "authors": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "comment": "Accepted to the NeurIPS 2025 Workshop on Mechanistic Interpretability\n  (Mechinterp) and the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "Interpreting language models often involves circuit analysis, which aims to\nidentify sparse subnetworks, or circuits, that accomplish specific tasks.\nExisting circuit discovery algorithms face a fundamental trade-off: attribution\npatching is fast but unfaithful to the full model, while edge pruning is\nfaithful but computationally expensive. This research proposes a hybrid\nattribution and pruning (HAP) framework that uses attribution patching to\nidentify a high-potential subgraph, then applies edge pruning to extract a\nfaithful circuit from it. We show that HAP is 46\\% faster than baseline\nalgorithms without sacrificing circuit faithfulness. Furthermore, we present a\ncase study on the Indirect Object Identification task, showing that our method\npreserves cooperative circuit components (e.g. S-inhibition heads) that\nattribution patching methods prune at high sparsity. Our results show that HAP\ncould be an effective approach for improving the scalability of mechanistic\ninterpretability research to larger models. Our code is available at\nhttps://anonymous.4open.science/r/HAP-circuit-discovery.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faHAP\uff08\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5f52\u56e0\u4fee\u8865\u548c\u8fb9\u526a\u679d\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u5fe0\u5b9e\u7684\u8bed\u8a00\u6a21\u578b\u7535\u8def\u53d1\u73b0\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u7535\u8def\u53d1\u73b0\u7b97\u6cd5\u9762\u4e34\u5f52\u56e0\u4fee\u8865\uff08\u5feb\u901f\u4f46\u4e0d\u5fe0\u5b9e\uff09\u548c\u8fb9\u526a\u679d\uff08\u5fe0\u5b9e\u4f46\u8ba1\u7b97\u6602\u8d35\uff09\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d\uff08HAP\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u9ad8\u6f5c\u529b\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8fb9\u526a\u679d\u4ece\u8be5\u5b50\u56fe\u4e2d\u63d0\u53d6\u5fe0\u5b9e\u7535\u8def\u3002", "result": "HAP\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb46%\uff0c\u4e14\u4e0d\u727a\u7272\u7535\u8def\u5fe0\u5b9e\u6027\u3002\u5728\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cHAP\u80fd\u4fdd\u7559\u5f52\u56e0\u4fee\u8865\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u526a\u679d\u7684\u534f\u4f5c\u7535\u8def\u7ec4\u4ef6\uff08\u4f8b\u5982S-\u6291\u5236\u5934\uff09\u3002", "conclusion": "HAP\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5411\u66f4\u5927\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.04093", "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "categories": ["cs.AI"], "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDLLM\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8eWeb\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u4e2d\u566a\u58f0\u9c81\u68d2\u7684\u8ba4\u77e5\u8bca\u65ad\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53bb\u566a\u6269\u6563\u548c\u591a\u6e90\u8868\u793a\u5bf9\u9f50\u89e3\u51b3\u566a\u58f0\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba4\u77e5\u8bca\u65ad\u65b9\u6cd5\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u548c\u62b5\u6297\u566a\u58f0\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728Web\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\uff08WIES\uff09\u7684\u5f00\u653e\u73af\u5883\u4e2d\uff0c\u5927\u91cf\u65e5\u5fd7\u52a0\u5267\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u566a\u58f0\u95ee\u9898\u3002", "method": "DLLM\u6846\u67b6\u9996\u5148\u6839\u636e\u54cd\u5e94\u6b63\u786e\u6027\u6784\u5efa\u72ec\u7acb\u5b50\u56fe\uff0c\u5e76\u5229\u7528\u5173\u7cfb\u589e\u5f3a\u5bf9\u9f50\u6a21\u5757\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u3002\u63a5\u7740\uff0c\u5c06\u5b50\u56fe\u8868\u793a\u4e0eLLM\u884d\u751f\u7684\u8bed\u4e49\u589e\u5f3a\u8868\u793a\u8fdb\u884c\u878d\u5408\u548c\u5bf9\u9f50\u3002\u5173\u952e\u5728\u4e8e\uff0c\u5728\u6bcf\u6b21\u5bf9\u9f50\u524d\uff0c\u5f15\u5165\u4e86\u4e24\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6a21\u5757\uff1a\u9996\u5148\u8fdb\u884c\u65e0\u6761\u4ef6\u53bb\u566a\u4ee5\u6d88\u9664\u9519\u8bef\u4fe1\u606f\uff0c\u7136\u540e\u8fdb\u884c\u56fe\u5f15\u5bfc\u7684\u6761\u4ef6\u53bb\u566a\u4ee5\u6d88\u9664\u8bef\u5bfc\u4fe1\u606f\u3002\u6700\u7ec8\uff0c\u5c06\u6574\u5408\u4e86\u8bed\u4e49\u77e5\u8bc6\u548c\u7ed3\u6784\u4fe1\u606f\u7684\u566a\u58f0\u9c81\u68d2\u8868\u793a\u8f93\u5165\u5230\u73b0\u6709\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u4e2d\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u7684Web\u6559\u80b2\u5e73\u53f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLLM\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u80fd\u5b9e\u73b0\u6700\u4f73\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "DLLM\u5728\u6709\u6548\u5229\u7528LLM\u8bed\u4e49\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u566a\u58f0\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04013", "pdf": "https://arxiv.org/pdf/2510.04013", "abs": "https://arxiv.org/abs/2510.04013", "authors": ["Jiarui Liu", "Jivitesh Jain", "Mona Diab", "Nishant Subramani"], "title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have tremendous utility,\ntrustworthiness is still a chief concern: models often generate incorrect\ninformation with high confidence. While contextual information can help guide\ngeneration, identifying when a query would benefit from retrieved context and\nassessing the effectiveness of that context remains challenging. In this work,\nwe operationalize interpretability methods to ascertain whether we can predict\nthe correctness of model outputs from the model's activations alone. We also\nexplore whether model internals contain signals about the efficacy of external\ncontext. We consider correct, incorrect, and irrelevant context and introduce\nmetrics to distinguish amongst them. Experiments on six different models reveal\nthat a simple classifier trained on intermediate layer activations of the first\noutput token can predict output correctness with about 75% accuracy, enabling\nearly auditing. Our model-internals-based metric significantly outperforms\nprompting baselines at distinguishing between correct and incorrect context,\nguarding against inaccuracies introduced by polluted context. These findings\noffer a lens to better understand the underlying decision-making processes of\nLLMs. Our code is publicly available at\nhttps://github.com/jiarui-liu/LLM-Microscope", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6a21\u578b\u6fc0\u6d3b\u9884\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u5e76\u8bc4\u4f30\u5916\u90e8\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u6027\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u5e76\u9632\u6b62\u9519\u8bef\u4fe1\u606f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e38\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u4e14\u96be\u4ee5\u5224\u65ad\u4f55\u65f6\u9700\u8981\u5916\u90e8\u4e0a\u4e0b\u6587\u53ca\u5176\u6709\u6548\u6027\uff0c\u56e0\u6b64\u4fe1\u4efb\u5ea6\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u4ec5\u4ece\u6a21\u578b\u6fc0\u6d3b\u4e2d\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\uff0c\u5e76\u63a2\u7a76\u6a21\u578b\u5185\u90e8\u662f\u5426\u5b58\u5728\u5173\u4e8e\u5916\u90e8\u4e0a\u4e0b\u6587\uff08\u5305\u62ec\u6b63\u786e\u3001\u4e0d\u6b63\u786e\u548c\u65e0\u5173\u4e0a\u4e0b\u6587\uff09\u6709\u6548\u6027\u7684\u4fe1\u53f7\u3002\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u7b2c\u4e00\u4e2a\u8f93\u51fatoken\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u7684\u7b80\u5355\u5206\u7c7b\u5668\uff0c\u4ee5\u53ca\u5f15\u5165\u533a\u5206\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684\u6307\u6807\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4e00\u4e2a\u7b80\u5355\u5206\u7c7b\u5668\u80fd\u4ee5\u7ea675%\u7684\u51c6\u786e\u7387\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\uff0c\u5b9e\u73b0\u65e9\u671f\u5ba1\u8ba1\u3002\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u5ea6\u91cf\u6807\u51c6\u5728\u533a\u5206\u6b63\u786e\u4e0e\u4e0d\u6b63\u786e\u4e0a\u4e0b\u6587\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u6b62\u4e86\u6c61\u67d3\u4e0a\u4e0b\u6587\u5f15\u5165\u7684\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e95\u5c42\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u4e0a\u4e0b\u6587\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2510.03606", "pdf": "https://arxiv.org/pdf/2510.03606", "abs": "https://arxiv.org/abs/2510.03606", "authors": ["Mattia Scardecchia"], "title": "Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in self-supervised learning (SSL) have made it possible to\nlearn general-purpose visual features that capture both the high-level\nsemantics and the fine-grained spatial structure of images. Most notably, the\nrecent DINOv2 has established a new state of the art by surpassing weakly\nsupervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we\nexamine the core ideas behind its approach, multi-crop view augmentation and\nself-distillation with a mean teacher, and trace their development in previous\nwork. We then compare the performance of DINO and DINOv2 with other SSL and WSL\nmethods across various downstream tasks, and highlight some remarkable emergent\nproperties of their learned features with transformer backbones. We conclude by\nbriefly discussing DINOv2's limitations, its impact, and future research\ndirections.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5206\u6790\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5DINOv2\u7684\u6838\u5fc3\u601d\u60f3\u3001\u6027\u80fd\u8868\u73b0\u3001\u6d8c\u73b0\u7279\u6027\u3001\u5c40\u9650\u6027\u3001\u5f71\u54cd\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u89c6\u89c9\u7279\u5f81\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u7279\u522b\u662fDINOv2\u5df2\u8d85\u8d8a\u5f31\u76d1\u7763\u5b66\u4e60\uff08WSL\uff09\u65b9\u6cd5\uff0c\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5176\u6210\u529f\u7684\u6838\u5fc3\u601d\u60f3\u3002", "method": "\u672c\u7efc\u8ff0\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8fdb\u884c\uff1a\u5ba1\u89c6DINOv2\u7684\u6838\u5fc3\u601d\u60f3\uff08\u591a\u88c1\u526a\u89c6\u56fe\u589e\u5f3a\u3001\u5e73\u5747\u6559\u5e08\u81ea\u84b8\u998f\uff09\u5e76\u8ffd\u6eaf\u5176\u53d1\u5c55\uff1b\u6bd4\u8f83DINO\u548cDINOv2\u4e0e\u5176\u4ed6SSL\u548cWSL\u65b9\u6cd5\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff1b\u7a81\u51fa\u5176\u57fa\u4e8eTransformer\u9aa8\u5e72\u5b66\u4e60\u7279\u5f81\u7684\u663e\u8457\u6d8c\u73b0\u7279\u6027\u3002", "result": "DINO/DINOv2\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff08\u4f7f\u7528Transformer\u9aa8\u5e72\uff09\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6d8c\u73b0\u7279\u6027\uff1b\u901a\u8fc7\u4e0e\u5176\u4ed6SSL\u548cWSL\u65b9\u6cd5\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u6587\u7ae0\u6700\u540e\u8ba8\u8bba\u4e86DINOv2\u7684\u5c40\u9650\u6027\u3001\u5176\u5f71\u54cd\u4ee5\u53ca\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.03283", "pdf": "https://arxiv.org/pdf/2510.03283", "abs": "https://arxiv.org/abs/2510.03283", "authors": ["Yufei Li", "Yu Fu", "Yue Dong", "Cong Liu"], "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "14 pages, 15 figures", "summary": "Large language models (LLMs) deployed on edge servers are increasingly used\nin latency-sensitive applications such as personalized assistants,\nrecommendation, and content moderation. However, the non-stationary nature of\nuser data necessitates frequent retraining, which introduces a fundamental\ntension between inference latency and model accuracy under constrained GPU\nresources. Existing retraining strategies either delay model updates,\nover-commit resources to retraining, or overlook iteration-level retraining\ngranularity. In this paper, we identify that iteration-level scheduling is\ncrucial for adapting retraining frequency to model drift without violating\nservice-level objectives (SLOs). We propose MACE, a hybrid LLM system that\ncolocates concurrent inference (prefill, decode) and fine-tuning, with\nintelligent memory management to maximize task performance while promising\ninference throughput. MACE leverages the insight that not all model updates\nequally affect output alignment and allocates GPU cycles accordingly to balance\nthroughput, latency, and update freshness. Our trace-driven evaluation shows\nthat MACE matches or exceeds continuous retraining while reducing inference\nlatency by up to 63% and maintaining throughput under resource constraints.\nCompared to periodic retraining, MACE improves latency breakdown across\nprefill, decode, and finetune stages, and sustains GPU utilization above 85% in\nNVIDIA AGX Orin. These results demonstrate that iteration-level hybrid\nscheduling is a promising direction for deploying LLMs with continual learning\ncapabilities on edge platforms.", "AI": {"tldr": "MACE\u662f\u4e00\u4e2a\u6df7\u5408LLM\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ea7\u8c03\u5ea6\u548c\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff0c\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u534f\u540c\u63a8\u7406\u548c\u5fae\u8c03\uff0c\u4ee5\u5e73\u8861\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6a21\u578b\u66f4\u65b0\u9c9c\u5ea6\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u7684LLM\u5728\u65f6\u5ef6\u654f\u611f\u5e94\u7528\u4e2d\uff0c\u56e0\u7528\u6237\u6570\u636e\u975e\u5e73\u7a33\u6027\u9700\u9891\u7e41\u91cd\u8bad\u7ec3\u3002\u8fd9\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u4e0e\u6a21\u578b\u7cbe\u5ea6\u5728\u53d7\u9650GPU\u8d44\u6e90\u4e0b\u4ea7\u751f\u77db\u76fe\u3002\u73b0\u6709\u91cd\u8bad\u7ec3\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5c24\u5176\u5ffd\u89c6\u4e86\u8fed\u4ee3\u7ea7\u91cd\u8bad\u7ec3\u7c92\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u800c\u8fd9\u5bf9\u4e8e\u5728\u4e0d\u8fdd\u53cd\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u4e0b\u9002\u5e94\u6a21\u578b\u6f02\u79fb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51faMACE\u7cfb\u7edf\uff0c\u4e00\u79cd\u6df7\u5408LLM\u7cfb\u7edf\uff0c\u534f\u540c\u6267\u884c\u5e76\u53d1\u63a8\u7406\uff08prefill, decode\uff09\u548c\u5fae\u8c03\u3002MACE\u91c7\u7528\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff0c\u6700\u5927\u5316\u4efb\u52a1\u6027\u80fd\u540c\u65f6\u4fdd\u8bc1\u63a8\u7406\u541e\u5410\u91cf\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u8bc6\u522b\u5e76\u975e\u6240\u6709\u6a21\u578b\u66f4\u65b0\u5bf9\u8f93\u51fa\u5bf9\u9f50\u5f71\u54cd\u76f8\u540c\uff0c\u5e76\u636e\u6b64\u5206\u914dGPU\u5468\u671f\uff0c\u4ee5\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u66f4\u65b0\u9c9c\u5ea6\uff0c\u5173\u952e\u5728\u4e8e\u8fed\u4ee3\u7ea7\u8c03\u5ea6\u3002", "result": "MACE\u5728\u8ddf\u8e2a\u9a71\u52a8\u8bc4\u4f30\u4e2d\uff0c\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u6301\u7eed\u91cd\u8bad\u7ec3\uff0c\u5e76\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe63%\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u4e0b\u4fdd\u6301\u541e\u5410\u91cf\u3002\u4e0e\u5468\u671f\u6027\u91cd\u8bad\u7ec3\u76f8\u6bd4\uff0cMACE\u6539\u5584\u4e86prefill\u3001decode\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u5ef6\u8fdf\u5206\u89e3\uff0c\u5e76\u5728NVIDIA AGX Orin\u4e0a\u5c06GPU\u5229\u7528\u7387\u7ef4\u6301\u572885%\u4ee5\u4e0a\u3002", "conclusion": "\u8fed\u4ee3\u7ea7\u6df7\u5408\u8c03\u5ea6\u662f\u4e3a\u8fb9\u7f18\u5e73\u53f0\u90e8\u7f72\u5177\u5907\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684LLM\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.04097", "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics.", "AI": {"tldr": "\u4e3a\u89e3\u51b3UI\u56fe\u50cf\u5230Web\u4ee3\u7801\u8f6c\u6362\uff08WebUI-to-Code\uff09\u4efb\u52a1\u4e2d\u73b0\u6709\u57fa\u51c6\u6570\u636e\u591a\u6837\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86WebRenderBench\u5927\u89c4\u6a21\u57fa\u51c6\u548c\u57fa\u4e8e\u6e32\u67d3\u9875\u9762\u7684\u65b0\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f15\u5165ALISA\u4ee3\u7406\u5229\u7528\u8be5\u6307\u6807\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u5316UI\u56fe\u50cf\u5230Web\u4ee3\u7801\u7684\u8f6c\u6362\u5bf9\u524d\u7aef\u5f00\u53d1\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f7fWebUI-to-Code\u65e5\u76ca\u53ef\u884c\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5728\u6570\u636e\u591a\u6837\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": ["\u6784\u5efa\u4e86WebRenderBench\uff0c\u4e00\u4e2a\u5305\u542b2.25\u4e07\u4e2a\u771f\u5b9e\u95e8\u6237\u7f51\u7ad9\u9875\u9762\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4ee5\u5f80\u57fa\u51c6\u66f4\u9ad8\u7684\u6570\u636e\u591a\u6837\u6027\u3001\u590d\u6742\u6027\u548c\u771f\u5b9e\u6027\u3002", "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u6d4b\u91cf\u6700\u7ec8\u6e32\u67d3\u9875\u9762\u7684\u5e03\u5c40\u548c\u6837\u5f0f\u4e00\u81f4\u6027\u6765\u8bc4\u4f30UI\u8d28\u91cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u6307\u6807\u66f4\u9ad8\u6548\u3001\u5ba2\u89c2\u548c\u53ef\u9760\u3002", "\u5f15\u5165\u4e86\u81ea\u52a8\u5316\u5e03\u5c40\u548c\u6837\u5f0f\u68c0\u67e5\u4ee3\u7406\uff08ALISA\uff09\uff0c\u5b83\u5c06\u4e0a\u8ff0\u65b0\u6307\u6807\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7528\u4e8e\u589e\u5f3a\u5bf9\u6293\u53d6\u5230\u7684\u975e\u5bf9\u79f0\u7f51\u9875\u7684\u8bad\u7ec3\u3002"], "result": "\u5b9e\u9a8c\u8868\u660e\uff0cALISA\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u7ed3\u679c\u3002", "conclusion": "WebRenderBench\u57fa\u51c6\u3001\u63d0\u51fa\u7684\u65b0\u578b\u8bc4\u4f30\u6307\u6807\u548cALISA\u4ee3\u7406\u5171\u540c\u89e3\u51b3\u4e86WebUI-to-Code\u9886\u57df\u7684\u6570\u636e\u548c\u8bc4\u4f30\u6311\u6218\uff0c\u901a\u8fc7\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2510.04016", "pdf": "https://arxiv.org/pdf/2510.04016", "abs": "https://arxiv.org/abs/2510.04016", "authors": ["Thanapol Popit", "Natthapath Rungseesiripak", "Monthol Charattrakool", "Saksorn Ruangtanusak"], "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents", "categories": ["cs.CL", "cs.AI"], "comment": "IEEE ICSEC 2025", "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u63a2\u8ba8\u6cf0\u8bed\u6587\u672c\u7aef\u70b9\uff08EOT\uff09\u68c0\u6d4b\uff0c\u53d1\u73b0\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u80fd\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9002\u7528\u4e8e\u8bbe\u5907\u7aef\u7684EOT\u51b3\u7b56\u3002", "motivation": "\u6d41\u7545\u7684\u8bed\u97f3\u4ea4\u4e92\u9700\u8981\u53ef\u9760\u3001\u4f4e\u5ef6\u8fdf\u7684\u7528\u6237\u8bdd\u8bed\u7ed3\u675f\u68c0\u6d4b\u3002\u4f20\u7edf\u97f3\u9891\u9759\u97f3\u7aef\u70b9\u68c0\u6d4b\u5668\u4f1a\u5f15\u5165\u6570\u767e\u6beb\u79d2\u5ef6\u8fdf\uff0c\u5e76\u5728\u72b9\u8c6b\u6216\u7279\u5b9a\u8bed\u8a00\u73b0\u8c61\u4e0b\u5931\u6548\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7d27\u51d1\u578bLLM\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u4e0e\u8f7b\u91cf\u7ea7Transformer\u7684\u76d1\u7763\u5fae\u8c03\u3002\u5229\u7528YODAS\u8bed\u6599\u5e93\u7684\u8f6c\u5f55\u5b57\u5e55\u548c\u6cf0\u8bed\u7279\u6709\u8bed\u8a00\u7ebf\u7d22\uff08\u5982\u53e5\u672b\u52a9\u8bcd\uff09\uff0c\u5c06EOT\u95ee\u9898\u8868\u8ff0\u4e3a\u8bcd\u5143\u8fb9\u754c\u4e0a\u7684\u4e8c\u5143\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u62a5\u544a\u4e86\u660e\u663e\u7684\u51c6\u786e\u6027-\u5ef6\u8fdf\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u5b9e\u65bd\u7684\u65b9\u6848\u3002\u7ed3\u679c\u5efa\u7acb\u4e86\u6cf0\u8bedEOT\u68c0\u6d4b\u7684\u57fa\u7ebf\uff0c\u5e76\u8bc1\u660e\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u80fd\u63d0\u4f9b\u9002\u7528\u4e8e\u8bbe\u5907\u4e0a\u7684\u8fd1\u4e4e\u5373\u65f6EOT\u51b3\u7b56\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u6cf0\u8bedEOT\u68c0\u6d4b\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u8868\u660e\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u8fd1\u4e4e\u5373\u65f6\u4e14\u9002\u7528\u4e8e\u8bbe\u5907\u7aef\u4ee3\u7406\u7684EOT\u51b3\u7b56\u3002"}}
{"id": "2510.03608", "pdf": "https://arxiv.org/pdf/2510.03608", "abs": "https://arxiv.org/abs/2510.03608", "authors": ["Ruitao Wu", "Yifan Zhao", "Guangyao Chen", "Jia Li"], "title": "Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025", "summary": "Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially\nlearn new classes from minimal examples without forgetting prior knowledge, a\ntask complicated by the stability-plasticity dilemma and data scarcity. Current\nFSCIL methods often struggle with generalization due to their reliance on\nlimited datasets. While diffusion models offer a path for data augmentation,\ntheir direct application can lead to semantic misalignment or ineffective\nguidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel\nframework that establishes a mutual boosting loop between diffusion model and\nFSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a\ndynamic, multi-faceted reward function derived from the classifier's state\ndirects the diffusion model. This reward system operates at two levels: the\nfeature level ensures semantic coherence and diversity using prototype-anchored\nmaximum mean discrepancy and dimension-wise variance matching, while the logits\nlevel promotes exploratory image generation and enhances inter-class\ndiscriminability through confidence recalibration and cross-session\nconfusion-aware mechanisms. This co-evolutionary process, where generated\nimages refine the classifier and an improved classifier state yields better\nreward signals, demonstrably achieves state-of-the-art performance on FSCIL\nbenchmarks, significantly enhancing both knowledge retention and new class\nlearning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiffusion-Classifier Synergy (DCS)\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4e0eFSCIL\u5206\u7c7b\u5668\u7684\u4e92\u52a9\u5faa\u73af\u548c\u591a\u7ef4\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u4ee5\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347FSCIL\u7684\u6027\u80fd\u3002", "motivation": "Few-Shot Class-Incremental Learning (FSCIL)\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002\u73b0\u6709FSCIL\u65b9\u6cd5\u56e0\u4f9d\u8d56\u6709\u9650\u6570\u636e\u96c6\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u6269\u6563\u6a21\u578b\u867d\u80fd\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u8bed\u4e49\u9519\u4f4d\u548c\u6307\u5bfc\u65e0\u6548\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Diffusion-Classifier Synergy (DCS)\u6846\u67b6\uff0c\u5efa\u7acb\u6269\u6563\u6a21\u578b\u4e0eFSCIL\u5206\u7c7b\u5668\u4e4b\u95f4\u7684\u4e92\u52a9\u63d0\u5347\u5faa\u73af\u3002DCS\u91c7\u7528\u5956\u52b1\u5bf9\u9f50\u5b66\u4e60\u7b56\u7565\uff0c\u5229\u7528\u5206\u7c7b\u5668\u72b6\u6001\u6d3e\u751f\u7684\u52a8\u6001\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u6307\u5bfc\u6269\u6563\u6a21\u578b\u3002\u8be5\u5956\u52b1\u7cfb\u7edf\u5728\u7279\u5f81\u5c42\u9762\uff08\u901a\u8fc7\u539f\u578b\u951a\u5b9a\u6700\u5927\u5747\u503c\u5dee\u5f02\u548c\u7ef4\u5ea6\u65b9\u5dee\u5339\u914d\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\uff09\u548cLogits\u5c42\u9762\uff08\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u8de8\u4f1a\u8bdd\u6df7\u6dc6\u611f\u77e5\u673a\u5236\u589e\u5f3a\u63a2\u7d22\u6027\u751f\u6210\u548c\u7c7b\u95f4\u53ef\u8fa8\u8bc6\u6027\uff09\u534f\u540c\u8fd0\u4f5c\u3002", "result": "\u5728FSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u548c\u65b0\u7c7b\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "DCS\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4e0e\u5206\u7c7b\u5668\u7684\u534f\u540c\u8fdb\u5316\u4ee5\u53ca\u7cbe\u5de7\u7684\u591a\u7ef4\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86FSCIL\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u6311\u6218\uff0c\u6210\u529f\u5e73\u8861\u4e86\u6a21\u578b\u5b66\u4e60\u65b0\u7c7b\u7684\u53ef\u5851\u6027\u4e0e\u4fdd\u7559\u65e7\u77e5\u8bc6\u7684\u7a33\u5b9a\u6027\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03284", "pdf": "https://arxiv.org/pdf/2510.03284", "abs": "https://arxiv.org/abs/2510.03284", "authors": ["Vinay Venkatesh", "Vamsidhar R Kamanuru", "Lav Kumar", "Nikita Kothari"], "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure", "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a\nscalable framework for Federated Instruction Tuning (FIT) of Large Language\nModels (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail\nwhen confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT\nframework combines federated learning with 4-bit Quantized Low-Rank Adaptation\n(QLORA), mitigating the core issues of communication and computational\noverhead. We demonstrate this by filtering the general-purpose Databricks Dolly\n15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned\nLlama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable\ntrade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable\nframework for decentralized LLM deployment on home compute gateways.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Edge-FIT\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8fb9\u7f18\u8054\u90a6\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c4\u6bd4\u7279QLORA\uff0c\u89e3\u51b3\u4e86LLM\u5728\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5e76\u5728Llama 2(7B)\u4e0a\u5b9e\u73b0\u4e860.89\u7684F1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982FedAvg\uff09\u5728\u9762\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5de8\u5927\u53c2\u6570\u91cf\u65f6\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u963b\u788d\u4e86LLMs\u7684\u53bb\u4e2d\u5fc3\u5316\u90e8\u7f72\u3002", "method": "Edge-FIT\u6846\u67b6\u5c06\u8054\u90a6\u5b66\u4e60\u4e0e4\u6bd4\u7279\u91cf\u5316\u4f4e\u79e9\u9002\u914d\uff08QLORA\uff09\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u51cf\u8f7b\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u901a\u8fc7\u5bf9\u7528\u4e8eIoT\u9886\u57df\u7684Databricks Dolly 15k\u6570\u636e\u96c6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u5e76\u5728Llama 2(7B)\u548cPhi-3-mini\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7ecf\u8fc7Edge-FIT\u8c03\u4f18\u7684Llama 2(7B)\u6a21\u578b\u5b9e\u73b0\u4e860.89\u7684F1\u5206\u6570\u3002\u540c\u65f6\uff0c\u4f7f\u75283.8B\u7684Phi-3-mini\u6a21\u578b\u4e5f\u5c55\u793a\u4e86\u53ef\u884c\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u6743\u8861\u3002", "conclusion": "Edge-FIT\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5728\u5bb6\u5ead\u8ba1\u7b97\u7f51\u5173\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884cLLM\u7684\u53bb\u4e2d\u5fc3\u5316\u90e8\u7f72\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u5e94\u7528\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2510.04116", "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly.", "AI": {"tldr": "\u63d0\u51faAutoMR\u6846\u67b6\uff0c\u5229\u7528DAG\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5143\u63a8\u7406\u9aa8\u67b6\u591a\u4e3a\u624b\u52a8\u8bbe\u8ba1\uff0c\u96be\u4ee5\u9002\u5e94\u7279\u5b9a\u67e5\u8be2\u9700\u6c42\uff0c\u4e5f\u65e0\u6cd5\u6709\u6548\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u590d\u6742\u903b\u8f91\u4f9d\u8d56\u3002", "method": "\u5c06\u5143\u63a8\u7406\u9aa8\u67b6\u8868\u793a\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u4ee5\u7edf\u4e00\u73b0\u6709\u5de5\u4f5c\u5e76\u5efa\u6a21\u903b\u8f91\u4f9d\u8d56\u3002\u63d0\u51faAutoMR\u6846\u67b6\uff0c\u53d7AutoML\u542f\u53d1\uff0c\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\u3002\u6784\u5efa\u57fa\u4e8eDAG\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u9aa8\u67b6\u91c7\u6837\u7b97\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u6839\u636e\u63a8\u7406\u4e0a\u4e0b\u6587\u6269\u5c55\u9aa8\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u67e5\u8be2\u611f\u77e5\u9aa8\u67b6\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAutoMR\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u63a8\u7406\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AutoMR\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u67e5\u8be2\u611f\u77e5\u4e14\u57fa\u4e8eDAG\u7684\u5143\u63a8\u7406\u9aa8\u67b6\u641c\u7d22\uff0c\u514b\u670d\u4e86\u624b\u52a8\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.04031", "pdf": "https://arxiv.org/pdf/2510.04031", "abs": "https://arxiv.org/abs/2510.04031", "authors": ["Nelvin Tan", "James Asikin Cheung", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their\nimpressive abilities that arise from large training datasets and large model\nsizes. More recently, they have been shown to be very effective in textual\nclassification tasks, motivating the need to explain the LLMs' decisions.\nMotivated by practical constrains where LLMs are black-boxed and LLM calls are\nexpensive, we study how incorporating counterfactuals into LLM reasoning can\naffect the LLM's ability to identify the top words that have contributed to its\nclassification decision. To this end, we introduce a framework called the\ndecision changing rate that helps us quantify the importance of the top words\nin classification. Our experimental results show that using counterfactuals can\nbe helpful.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u9ed1\u76d2\u4e14\u6602\u8d35\u7684LLM\u6587\u672c\u5206\u7c7b\u51b3\u7b56\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u5f15\u5165\u53cd\u4e8b\u5b9e\u6765\u8bc6\u522b\u5173\u952e\u8d21\u732e\u8bcd\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u51b3\u7b56\u6539\u53d8\u7387\u201d\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53cd\u4e8b\u5b9e\u6709\u52a9\u4e8e\u6b64\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u89e3\u91ca\u5176\u51b3\u7b56\u3002\u7531\u4e8eLLM\u662f\u9ed1\u76d2\u6a21\u578b\u4e14\u8c03\u7528\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bfb\u627e\u6709\u6548\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u5c06\u53cd\u4e8b\u5b9e\uff08counterfactuals\uff09\u7eb3\u5165LLM\u63a8\u7406\u4e2d\uff0c\u5982\u4f55\u5f71\u54cdLLM\u8bc6\u522b\u5bf9\u5176\u5206\u7c7b\u51b3\u7b56\u8d21\u732e\u6700\u5927\u7684\u5173\u952e\u8bcd\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u51b3\u7b56\u6539\u53d8\u7387\u201d\uff08decision changing rate\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u5206\u7c7b\u4e2d\u5173\u952e\u8bcd\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\uff08counterfactuals\uff09\u53ef\u80fd\u662f\u6709\u5e2e\u52a9\u7684\u3002", "conclusion": "\u5f15\u5165\u53cd\u4e8b\u5b9e\u53ef\u4ee5\u6709\u6548\u5e2e\u52a9LLM\u8bc6\u522b\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u5173\u952e\u8d21\u732e\u8bcd\uff0c\u4e3aLLM\u51b3\u7b56\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2510.03666", "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMonitorVLM\uff0c\u4e00\u4e2a\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u3001\u5b50\u53e5\u8fc7\u6ee4\u548c\u884c\u4e3a\u653e\u5927\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u98ce\u9669\u884c\u4e1a\uff08\u5982\u91c7\u77ff\uff09\u4e2d\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u4eba\u5de5\u68c0\u67e5\u3002", "motivation": "\u9ad8\u98ce\u9669\u884c\u4e1a\uff08\u5982\u91c7\u77ff\uff09\u7684\u5de5\u4e1a\u4e8b\u6545\u5e38\u7531\u4e0d\u5b89\u5168\u884c\u4e3a\u5f15\u8d77\u3002\u4f20\u7edf\u4eba\u5de5\u68c0\u67e5\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u6025\u9700\u667a\u80fd\u81ea\u52a8\u5316\u5b89\u5168\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MonitorVLM\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u76d1\u63a7\u89c6\u9891\u6d41\u4e2d\u68c0\u6d4b\u5b89\u5168\u8fdd\u89c4\u884c\u4e3a\u3002\u4e3b\u8981\u521b\u65b0\u5305\u62ec\uff1a(1) \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b9,000\u4e2aVQA\u6837\u672c\u7684\u9886\u57df\u7279\u5b9a\u8fdd\u89c4\u6570\u636e\u96c6\uff1b(2) \u8bbe\u8ba1\u4e86\u5b50\u53e5\u8fc7\u6ee4\u5668(CF)\u6a21\u5757\uff0c\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u5b50\u53e5\u4ee5\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff1b(3) \u5f00\u53d1\u4e86\u884c\u4e3a\u653e\u5927\u5668(BM)\u6a21\u5757\uff0c\u589e\u5f3a\u5de5\u4eba\u533a\u57df\u4ee5\u6539\u8fdb\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u3002\u7cfb\u7edf\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8e\u7f51\u7edc\u7684\u754c\u9762\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMonitorVLM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7cbe\u5ea6\u4e0a\u63d0\u534722.01%\uff0c\u53ec\u56de\u7387\u63d0\u534734.22%\uff0cF1\u5206\u6570\u63d0\u534728.37%\u3002\u5176\u4e2d\uff0cCF\u6a21\u5757\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e8613.56%\uff1bBM\u6a21\u5757\u4f7f\u7cbe\u5ea6\u989d\u5916\u63d0\u9ad8\u4e863.45%\uff0c\u53ec\u56de\u7387\u63d0\u9ad8\u4e868.62%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u63d0\u5347\u91c7\u77ff\u53ca\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\u804c\u4e1a\u5b89\u5168\u76d1\u63a7\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.03288", "pdf": "https://arxiv.org/pdf/2510.03288", "abs": "https://arxiv.org/abs/2510.03288", "authors": ["Chiming Duan", "Minghua He", "Pei Xiao", "Tong Jia", "Xin Zhang", "Zhewei Zhong", "Xiang Luo", "Yan Niu", "Lingzhe Zhang", "Yifan Wu", "Siyu Yu", "Weijie Hong", "Ying Li", "Gang Huang"], "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "comment": "The 40th IEEE/ACM International Conference on Automated Software\n  Engineering, ASE 2025", "summary": "Log-based anomaly detection is a essential task for ensuring the reliability\nand performance of software systems. However, the performance of existing\nanomaly detection methods heavily relies on labeling, while labeling a large\nvolume of logs is highly challenging. To address this issue, many approaches\nbased on transfer learning and active learning have been proposed.\nNevertheless, their effectiveness is hindered by issues such as the gap between\nsource and target system data distributions and cold-start problems. In this\npaper, we propose LogAction, a novel log-based anomaly detection model based on\nactive domain adaptation. LogAction integrates transfer learning and active\nlearning techniques. On one hand, it uses labeled data from a mature system to\ntrain a base model, mitigating the cold-start issue in active learning. On the\nother hand, LogAction utilize free energy-based sampling and uncertainty-based\nsampling to select logs located at the distribution boundaries for manual\nlabeling, thus addresses the data distribution gap in transfer learning with\nminimal human labeling efforts. Experimental results on six different\ncombinations of datasets demonstrate that LogAction achieves an average 93.01%\nF1 score with only 2% of manual labels, outperforming some state-of-the-art\nmethods by 26.28%. Website: https://logaction.github.io", "AI": {"tldr": "LogAction\u662f\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u57df\u9002\u5e94\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u901a\u8fc7\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u548c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u6807\u6ce8\u6570\u636e\u3002\u5df2\u6709\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u53c8\u9762\u4e34\u6e90\u76ee\u6807\u7cfb\u7edf\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "LogAction\u6574\u5408\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\u3002\u4e00\u65b9\u9762\uff0c\u5229\u7528\u6210\u719f\u7cfb\u7edf\u7684\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u4ee5\u7f13\u89e3\u4e3b\u52a8\u5b66\u4e60\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u91c7\u7528\u57fa\u4e8e\u81ea\u7531\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u91c7\u6837\u7b56\u7565\u9009\u62e9\u4f4d\u4e8e\u5206\u5e03\u8fb9\u754c\u7684\u65e5\u5fd7\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u4ee5\u6700\u5c0f\u5316\u6807\u6ce8\u5de5\u4f5c\u91cf\u89e3\u51b3\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728\u516d\u79cd\u4e0d\u540c\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\uff0cLogAction\u4ec5\u97002%\u7684\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u8fbe\u5230\u5e73\u574793.01%\u7684F1\u5206\u6570\uff0c\u5e76\u4e14\u6bd4\u4e00\u4e9b\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6027\u80fd\u9ad8\u51fa26.28%\u3002", "conclusion": "LogAction\u901a\u8fc7\u521b\u65b0\u6027\u5730\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u6709\u6548\u514b\u670d\u4e86\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u51b7\u542f\u52a8\u548c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u7684\u6311\u6218\uff0c\u4ee5\u6781\u4f4e\u7684\u6807\u6ce8\u6295\u5165\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002"}}
{"id": "2510.04128", "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "AI": {"tldr": "\u672c\u6587\u63a2\u7a76\u4e86\u201cwait\u201d\u6807\u8bb0\u524d\u6a21\u578b\u7684\u6f5c\u5728\u72b6\u6001\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u8fc7\u7a0b\uff0c\u8bc6\u522b\u51fa\u8c03\u63a7\u201cwait\u201d\u751f\u6210\u53ca\u591a\u79cd\u63a8\u7406\u6a21\u5f0f\uff08\u5982\u56de\u6eaf\u3001\u590d\u6838\uff09\u7684\u5173\u952e\u7279\u5f81\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u5728\u4e8e\u5176\u63a8\u7406\u548c\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u5176\u4e2d\u201cwait\u201d\u6807\u8bb0\u5e38\u4f5c\u4e3a\u56de\u6eaf\u7b49\u590d\u6742\u63a8\u7406\u884c\u4e3a\u7684\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u6a21\u578b\u4e3a\u4f55\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u8fd9\u7c7b\u63a8\u7406\u7684\u673a\u5236\u7406\u89e3\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u63a8\u7406\u6a21\u578b\u6709\u6548\u6027\u7684\u8ba4\u8bc6\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u63a2\u7a76\u201cwait\u201d\u6807\u8bb0\u524d\u7684\u6a21\u578b\u6f5c\u5728\u72b6\u6001\u662f\u5426\u5305\u542b\u8c03\u8282\u540e\u7eed\u63a8\u7406\u8fc7\u7a0b\u7684\u76f8\u5173\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5728DeepSeek-R1-Distill-Llama-8B\u53ca\u5176\u57fa\u7840\u7248\u672c\u7684\u591a\u4e2a\u5c42\u4e0a\u8bad\u7ec3crosscoders\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5728crosscoder\u8bbe\u7f6e\u4e2d\u7684\u6f5c\u5728\u5f52\u56e0\u6280\u672f\u3002\u968f\u540e\uff0c\u901a\u8fc7\u5206\u6790\u6700\u5927\u6fc0\u6d3b\u793a\u4f8b\u548c\u8fdb\u884c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "\u6210\u529f\u5b9a\u4f4d\u4e86\u4e00\u5c0f\u90e8\u5206\u4e0e\u63d0\u5347/\u6291\u5236\u201cwait\u201d\u6807\u8bb0\u6982\u7387\u76f8\u5173\u7684\u7279\u5f81\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u8fd9\u4e9b\u88ab\u8bc6\u522b\u7684\u7279\u5f81\u786e\u5b9e\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u80fd\u4ea7\u751f\u591a\u79cd\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u5305\u62ec\u4ece\u5934\u5f00\u59cb\u3001\u56de\u5fc6\u5148\u9a8c\u77e5\u8bc6\u3001\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u548c\u4ed4\u7ec6\u68c0\u67e5\u3002", "conclusion": "\u6a21\u578b\u5728\u201cwait\u201d\u6807\u8bb0\u524d\u7684\u6f5c\u5728\u72b6\u6001\u4e2d\u5305\u542b\u4e86\u8c03\u8282\u540e\u7eed\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u952e\u4fe1\u606f\u3002\u8bc6\u522b\u51fa\u7684\u7279\u5f81\u63ed\u793a\u4e86\u6a21\u578b\u8fdb\u884c\u590d\u6742\u63a8\u7406\u884c\u4e3a\uff08\u5982\u56de\u6eaf\u3001\u590d\u6838\uff09\u7684\u673a\u5236\uff0c\u52a0\u6df1\u4e86\u5bf9\u63a8\u7406\u6a21\u578b\u6709\u6548\u6027\u7684\u7406\u89e3\u3002"}}
{"id": "2510.04032", "pdf": "https://arxiv.org/pdf/2510.04032", "abs": "https://arxiv.org/abs/2510.04032", "authors": ["Zirui Wang", "Jiajun Wu", "Braden Teitge", "Jessalyn Holodinsky", "Steve Drew"], "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Large language models (LLMs) have become increasingly popular in medical\ndomains to assist physicians with a variety of clinical and operational tasks.\nGiven the fast-paced and high-stakes environment of emergency departments\n(EDs), small language models (SLMs), characterized by a reduction in parameter\ncount compared to LLMs, offer significant potential due to their inherent\nreasoning capability and efficient performance. This enables SLMs to support\nphysicians by providing timely and accurate information synthesis, thereby\nimproving clinical decision-making and workflow efficiency. In this paper, we\npresent a comprehensive benchmark designed to identify SLMs suited for ED\ndecision support, taking into account both specialized medical expertise and\nbroad general problem-solving capabilities. In our evaluations, we focus on\nSLMs that have been trained on a mixture of general-domain and medical corpora.\nA key motivation for emphasizing SLMs is the practical hardware limitations,\noperational cost constraints, and privacy concerns in the typical real-world\ndeployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and\nPubMedQA, with the medical abstracts dataset emulating tasks aligned with real\nED physicians' daily tasks. Experimental results reveal that general-domain\nSLMs surprisingly outperform their medically fine-tuned counterparts across\nthese diverse benchmarks for ED. This indicates that for ED, specialized\nmedical fine-tuning of the model may not be required.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u6025\u8bca\u79d1\uff08ED\uff09\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u6f5c\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u901a\u7528\u9886\u57dfSLM\u5728ED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u533b\u5b66\u5fae\u8c03\u6a21\u578b\uff0c\u8868\u660eED\u5e94\u7528\u53ef\u80fd\u4e0d\u9700\u8981\u4e13\u95e8\u7684\u533b\u5b66\u5fae\u8c03\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6025\u8bca\u79d1\u9762\u4e34\u786c\u4ef6\u9650\u5236\u3001\u8fd0\u8425\u6210\u672c\u548c\u9690\u79c1\u7b49\u5b9e\u9645\u90e8\u7f72\u6311\u6218\u3002\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u56e0\u5176\u9ad8\u6548\u6027\u80fd\u548c\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u7684\u4fe1\u606f\u5408\u6210\uff0c\u56e0\u6b64\u5728\u6025\u8bca\u79d1\u7684\u5feb\u8282\u594f\u3001\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4ee5\u6539\u8fdb\u4e34\u5e8a\u51b3\u7b56\u548c\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u65e8\u5728\u8bc6\u522b\u9002\u5408\u6025\u8bca\u79d1\u51b3\u7b56\u652f\u6301\u7684SLM\uff0c\u8003\u91cf\u5176\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u8bc4\u4f30\u4fa7\u91cd\u4e8e\u901a\u7528\u9886\u57df\u548c\u533b\u5b66\u8bed\u6599\u5e93\u6df7\u5408\u8bad\u7ec3\u7684SLM\u3002\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u5305\u62ecMedMCQA\u3001MedQA-4Options\u548cPubMedQA\uff0c\u5176\u4e2d\u533b\u5b66\u6458\u8981\u6570\u636e\u96c6\u6a21\u62df\u6025\u8bca\u79d1\u533b\u751f\u65e5\u5e38\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\uff0c\u901a\u7528\u9886\u57dfSLM\u5728\u5404\u9879\u6025\u8bca\u79d1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u51fa\u4eba\u610f\u6599\u5730\u4f18\u4e8e\u7ecf\u8fc7\u533b\u5b66\u5fae\u8c03\u7684\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "\u8fd9\u8868\u660e\u5bf9\u4e8e\u6025\u8bca\u79d1\u5e94\u7528\u800c\u8a00\uff0c\u6a21\u578b\u53ef\u80fd\u4e0d\u9700\u8981\u8fdb\u884c\u4e13\u95e8\u7684\u533b\u5b66\u5fae\u8c03\u3002"}}
{"id": "2510.03675", "pdf": "https://arxiv.org/pdf/2510.03675", "abs": "https://arxiv.org/abs/2510.03675", "authors": ["Siva Sai", "Saksham Gupta", "Vinay Chamola", "Rajkumar Buyya"], "title": "A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems", "categories": ["cs.CV"], "comment": null, "summary": "The integration of Diffusion Models into Intelligent Transportation Systems\n(ITS) is a substantial improvement in the detection of accidents. We present a\nnovel hybrid model integrating guidance classification with diffusion\ntechniques. By leveraging fine-tuned ExceptionNet architecture outputs as input\nfor our proposed diffusion model and processing image tensors as our\nconditioning, our approach creates a robust classification framework. Our model\nconsists of multiple conditional modules, which aim to modulate the linear\nprojection of inputs using time embeddings and image covariate embeddings,\nallowing the network to adapt its behavior dynamically throughout the diffusion\nprocess. To address the computationally intensive nature of diffusion models,\nour implementation is cloud-based, enabling scalable and efficient processing.\nOur strategy overcomes the shortcomings of conventional classification\napproaches by leveraging diffusion models inherent capacity to effectively\nunderstand complicated data distributions. We investigate important diffusion\ncharacteristics, such as timestep schedulers, timestep encoding techniques,\ntimestep count, and architectural design changes, using a thorough ablation\nstudy, and have conducted a comprehensive evaluation of the proposed model\nagainst the baseline models on a publicly available dataset. The proposed\ndiffusion model performs best in image-based accident detection with an\naccuracy of 97.32%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5f15\u5bfc\u5206\u7c7b\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b0\u578b\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u4e2d\u7684\u4e8b\u6545\u68c0\u6d4b\uff0c\u5b9e\u73b0\u4e8697.32%\u7684\u56fe\u50cf\u4e8b\u6545\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff1b\u6269\u6563\u6a21\u578b\u6709\u671b\u663e\u8457\u6539\u8fdb\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4e8b\u6545\u68c0\u6d4b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u878d\u5408\u5f15\u5bfc\u5206\u7c7b\u4e0e\u6269\u6563\u6280\u672f\u7684\u65b0\u578b\u6df7\u5408\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5c06\u9884\u8bad\u7ec3ExceptionNet\u67b6\u6784\u7684\u8f93\u51fa\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\uff0c\u5e76\u4ee5\u56fe\u50cf\u5f20\u91cf\u4f5c\u4e3a\u6761\u4ef6\u3002\u6a21\u578b\u5305\u542b\u591a\u4e2a\u6761\u4ef6\u6a21\u5757\uff0c\u5229\u7528\u65f6\u95f4\u5d4c\u5165\u548c\u56fe\u50cf\u534f\u53d8\u91cf\u5d4c\u5165\u52a8\u6001\u8c03\u6574\u8f93\u5165\u6295\u5f71\u3002\u91c7\u7528\u4e91\u7aef\u90e8\u7f72\u4ee5\u63d0\u9ad8\u5904\u7406\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u63a2\u8ba8\u4e86\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u4e8b\u6545\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523097.32%\u3002", "conclusion": "\u8be5\u6df7\u5408\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6709\u6548\u7406\u89e3\u590d\u6742\u6570\u636e\u5206\u5e03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u56fe\u50cf\u7684\u4e8b\u6545\u68c0\u6d4b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03289", "pdf": "https://arxiv.org/pdf/2510.03289", "abs": "https://arxiv.org/abs/2510.03289", "authors": ["Haocheng Sun", "Cynthia Xin Wen", "Edward Hong Wang"], "title": "Why mask diffusion does not work", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The main advantages of diffusion language models over autoregressive (AR)\nmodels lie in their ability to support parallel generation and bidirectional\nattention, enabling a more controllable generation process. In recent years,\nopen-source mask diffusion language models have emerged, most of which are\nbased on a variant known as absorbing diffusion. However, this paper\ndemonstrates why mask diffusion faces inherent difficulties in achieving\nparallel generation and bidirectional attention. We also propose the most\neffective training and inference strategies for mask diffusion.", "AI": {"tldr": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u56fa\u6709\u56f0\u96be\uff0c\u672c\u6587\u5206\u6790\u4e86\u5176\u539f\u56e0\u5e76\u63d0\u51fa\u4e86\u6700\u4f18\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u56e0\u652f\u6301\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5bf9\u4e8e\u6d41\u884c\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u5176\u80fd\u5426\u771f\u6b63\u5b9e\u73b0\u8fd9\u4e9b\u4f18\u52bf\u4ee5\u53ca\u9762\u4e34\u7684\u6311\u6218\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8bba\u8bc1\u9610\u8ff0\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u6700\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u5185\u5728\u56f0\u96be\u3002\u540c\u65f6\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u6700\u4f18\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "conclusion": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u4f18\u52bf\u65f6\u9762\u4e34\u57fa\u7840\u6027\u6311\u6218\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u672c\u6587\u63d0\u51fa\u7684\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u6765\u63d0\u9ad8\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.04140", "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "RLVR\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u63d0\u5347LLM\u63a8\u7406\u7684\u6709\u6548\u6027\u4f46\u5ffd\u89c6\u591a\u6837\u6027\u3002\u672c\u6587\u63d0\u51faMENTOR\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5173\u952e\u51b3\u7b56\u70b9\u800c\u975e\u6574\u4e2a\u8def\u5f84\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u5b9e\u73b0\u6709\u6548\u4e14\u591a\u6837\u5316\u7684\u63a2\u7d22\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5728\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u6548\u679c\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u3002\u9ad8\u8d28\u91cf\u7684\u63a2\u7d22\u9700\u8981\u517c\u5177\u6709\u6548\u6027\u548c\u591a\u6837\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u63d0\u5347\u4e86\u63a2\u7d22\u7684\u6709\u6548\u6027\uff0c\u4f46\u5374\u5ffd\u89c6\u4e86\u591a\u6837\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faMENTOR (Mixed-policy Expert Navigation for Token-level Optimization of Reasoning) \u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u6d1e\u89c1\u662f\u4e13\u5bb6\u4ec5\u9700\u5728\u5173\u952e\u51b3\u7b56\u70b9\u800c\u975e\u6574\u4e2a\u63a8\u7406\u8def\u5f84\u63d0\u4f9b\u6307\u5bfc\u3002MENTOR\u5728RLVR\u4e2d\u5229\u7528\u8fd9\u4e00\u6d1e\u89c1\uff0c\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u4e14\u591a\u6837\u5316\u7684\u63a2\u7d22\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMENTOR\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u4e13\u5bb6\u7b56\u7565\u7684\u7cbe\u9ad3\uff0c\u800c\u975e\u4ec5\u4ec5\u8fdb\u884c\u8868\u9762\u6a21\u4eff\uff0c\u4ece\u800c\u6267\u884c\u9ad8\u8d28\u91cf\u7684\u63a2\u7d22\uff0c\u5e76\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "MENTOR\u901a\u8fc7\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2dLLMs\u9ad8\u8d28\u91cf\u63a2\u7d22\uff08\u517c\u987e\u6709\u6548\u6027\u548c\u591a\u6837\u6027\uff09\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.04045", "pdf": "https://arxiv.org/pdf/2510.04045", "abs": "https://arxiv.org/abs/2510.04045", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "ACL EMNLP 2025", "summary": "Large Language Models (LLMs) are typically trained to reflect a relatively\nuniform set of values, which limits their applicability to tasks that require\nunderstanding of nuanced human perspectives. Recent research has underscored\nthe importance of enabling LLMs to support steerable pluralism -- the capacity\nto adopt a specific perspective and align generated outputs with it. In this\nwork, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be\napplied to building steerable pluralistic models. We explore several methods,\nincluding CoT prompting, fine-tuning on human-authored CoT, fine-tuning on\nsynthetic explanations, and Reinforcement Learning with Verifiable Rewards\n(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA\ndatasets. Among the methods studied, RLVR consistently outperforms others and\ndemonstrates strong training sample efficiency. We further analyze the\ngenerated CoT traces with respect to faithfulness and safety.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u6280\u672f\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u63a7\u591a\u5143\u5316\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8868\u73b0\u6700\u4f73\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ef7\u503c\u53d6\u5411\u76f8\u5bf9\u5355\u4e00\uff0c\u9650\u5236\u4e86\u5176\u7406\u89e3\u7ec6\u81f4\u4eba\u7c7b\u89c6\u89d2\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4f7fLLMs\u5177\u5907\u53ef\u63a7\u591a\u5143\u5316\u80fd\u529b\uff0c\u5373\u91c7\u7eb3\u7279\u5b9a\u89c6\u89d2\u5e76\u751f\u6210\u4e0e\u5176\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u6280\u672f\uff1aCoT\u63d0\u793a\u3001\u57fa\u4e8e\u4eba\u7c7b\u521b\u4f5cCoT\u7684\u5fae\u8c03\u3001\u57fa\u4e8e\u5408\u6210\u89e3\u91ca\u7684\u5fae\u8c03\u4ee5\u53ca\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002\u901a\u8fc7Value Kaleidoscope\u548cOpinionQA\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e86\u751f\u6210CoT\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u3002", "result": "\u5728\u6240\u7814\u7a76\u7684\u65b9\u6cd5\u4e2d\uff0cRLVR\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8bad\u7ec3\u6837\u672c\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9CoT\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "RLVR\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6280\u672f\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u53ef\u63a7\u7684\u591a\u5143\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03689", "pdf": "https://arxiv.org/pdf/2510.03689", "abs": "https://arxiv.org/abs/2510.03689", "authors": ["Zhengyi Liu", "Xinrui Wang", "Xianyong Fang", "Zhengzheng Tu", "Linbo Wang"], "title": "SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection", "categories": ["cs.CV"], "comment": "Accepted by TMM", "summary": "RGB-T salient object detection (SOD) aims to segment attractive objects by\ncombining RGB and thermal infrared images. To enhance performance, the Segment\nAnything Model has been fine-tuned for this task. However, the imbalance\nconvergence of two modalities and significant gradient difference between high-\nand low- activations are ignored, thereby leaving room for further performance\nenhancement. In this paper, we propose a model called \\textit{SAMSOD}, which\nutilizes unimodal supervision to enhance the learning of non-dominant modality\nand employs gradient deconfliction to reduce the impact of conflicting\ngradients on model convergence. The method also leverages two decoupled\nadapters to separately mask high- and low-activation neurons, emphasizing\nforeground objects by enhancing background learning. Fundamental experiments on\nRGB-T SOD benchmark datasets and generalizability experiments on scribble\nsupervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised\nRGB-D rail surface defect detection all demonstrate the effectiveness of our\nproposed method.", "AI": {"tldr": "RGB-T SOD\u6a21\u578bSAMSOD\u901a\u8fc7\u5355\u6a21\u6001\u76d1\u7763\u3001\u68af\u5ea6\u89e3\u51b2\u7a81\u53ca\u89e3\u8026\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u6536\u655b\u4e0d\u5e73\u8861\u548c\u9ad8\u4f4e\u6fc0\u6d3b\u68af\u5ea6\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8eSAM\u5fae\u8c03\u7684\u65b9\u6848\uff09\u5ffd\u89c6\u4e86\u53cc\u6a21\u6001\u6536\u655b\u7684\u4e0d\u5e73\u8861\u6027\u4ee5\u53ca\u9ad8\u4f4e\u6fc0\u6d3b\u4e4b\u95f4\u663e\u8457\u7684\u68af\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u5b58\u5728\u8fdb\u4e00\u6b65\u63d0\u5347\u7684\u7a7a\u95f4\u3002", "method": "\u672c\u6587\u63d0\u51faSAMSOD\u6a21\u578b\uff0c\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5229\u7528\u5355\u6a21\u6001\u76d1\u7763\u589e\u5f3a\u975e\u4e3b\u5bfc\u6a21\u6001\u7684\u5b66\u4e60\uff1b2) \u91c7\u7528\u68af\u5ea6\u89e3\u51b2\u7a81\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u51b2\u7a81\u68af\u5ea6\u5bf9\u6a21\u578b\u6536\u655b\u7684\u5f71\u54cd\uff1b3) \u5f15\u5165\u4e24\u4e2a\u89e3\u8026\u9002\u914d\u5668\uff0c\u5206\u522b\u63a9\u76d6\u9ad8\u4f4e\u6fc0\u6d3b\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u589e\u5f3a\u80cc\u666f\u5b66\u4e60\u6765\u7a81\u51fa\u524d\u666f\u5bf9\u8c61\u3002", "result": "\u5728RGB-T SOD\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u57fa\u7840\u5b9e\u9a8c\u4ee5\u53ca\u5728\u6d82\u9e26\u76d1\u7763RGB-T SOD\u3001\u5168\u76d1\u7763RGB-D SOD\u548c\u5168\u76d1\u7763RGB-D\u8f68\u9053\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u6027\u5b9e\u9a8c\uff0c\u5747\u8bc1\u5b9e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SAMSOD\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u6027\u5730\u89e3\u51b3\u6a21\u6001\u6536\u655b\u4e0d\u5e73\u8861\u548c\u68af\u5ea6\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03290", "pdf": "https://arxiv.org/pdf/2510.03290", "abs": "https://arxiv.org/abs/2510.03290", "authors": ["X. Angelo Huang", "Ruben Ciranni", "Giovanni Spadaccini", "Carla J. L\u00f3pez Zurita"], "title": "Single-Core Superscalar Optimization of Clifford Neural Layers", "categories": ["cs.LG"], "comment": "9 pages", "summary": "Within the growing interest in the physical sciences in developing networks\nwith equivariance properties, Clifford neural layers shine as one approach that\ndelivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this\npaper, we analyze the inner structure of the computation within Clifford\nconvolutional layers and propose and implement several optimizations to speed\nup the inference process while maintaining correctness. In particular, we begin\nby analyzing the theoretical foundations of Clifford algebras to eliminate\nredundant matrix allocations and computations, then systematically apply\nestablished optimization techniques to enhance performance further. We report a\nfinal average speedup of 21.35x over the baseline implementation of eleven\nfunctions and runtimes comparable to and faster than the original PyTorch\nimplementation in six cases. In the remaining cases, we achieve performance in\nthe same order of magnitude as the original library.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u548c\u4f18\u5316Clifford\u5377\u79ef\u5c42\u7684\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6b63\u786e\u6027\u3002", "motivation": "\u9274\u4e8e\u7269\u7406\u79d1\u5b66\u4e2d\u5bf9\u5177\u6709\u7b49\u53d8\u6027\u7f51\u7edc\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0cClifford\u795e\u7ecf\u7f51\u7edc\u5c42\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u73b0E(n)\u548cO(n)\u7b49\u53d8\u6027\u7684\u65b9\u6cd5\u8131\u9896\u800c\u51fa\u3002\u7136\u800c\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u6548\u7387\u6709\u5f85\u63d0\u9ad8\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86Clifford\u4ee3\u6570\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u6d88\u9664\u5197\u4f59\u7684\u77e9\u9635\u5206\u914d\u548c\u8ba1\u7b97\u3002\u968f\u540e\uff0c\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u4e86\u65e2\u5b9a\u7684\u4f18\u5316\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u5b9e\u73b0\uff0c\u4f18\u5316\u540e\u7684\u65b9\u6cd5\u5728\u5341\u4e00\u4e2a\u51fd\u6570\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574721.35\u500d\u7684\u52a0\u901f\u3002\u5728\u516d\u4e2a\u6848\u4f8b\u4e2d\uff0c\u5176\u8fd0\u884c\u65f6\u6027\u80fd\u4e0e\u539f\u59cbPyTorch\u5b9e\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u5feb\u3002\u5728\u5176\u4f59\u6848\u4f8b\u4e2d\uff0c\u6027\u80fd\u4e0e\u539f\u59cb\u5e93\u5904\u4e8e\u540c\u4e00\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u5bf9Clifford\u5377\u79ef\u5c42\u7684\u7ed3\u6784\u5206\u6790\u548c\u4f18\u5316\uff0c\u672c\u6587\u6210\u529f\u5730\u5927\u5e45\u63d0\u5347\u4e86\u5176\u63a8\u7406\u901f\u5ea6\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u53ef\u884c\u6027\u3002"}}
{"id": "2510.04141", "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63a2\u8ba8\u4e86\u591a\u6a21\u6001AI\u8bc4\u4f30\u7684\u6f14\u53d8\uff0c\u4ece\u7b80\u5355\u8bc6\u522b\u8f6c\u5411\u590d\u6742\u63a8\u7406\uff0c\u5e76\u6307\u51fa\u65e7\u57fa\u51c6\u7684\u5c40\u9650\u6027\u63a8\u52a8\u4e86\u8fd9\u4e00\u8f6c\u53d8\u3002\u6587\u7ae0\u56de\u987e\u4e86\u5386\u53f2\u4e0a\u7684\u5404\u7c7b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5f53\u524d\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u5148\u8fdb\u57fa\u51c6\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u5bf9\u62bd\u8c61\u3001\u521b\u9020\u6027\u53ca\u793e\u4f1a\u667a\u80fd\u7684\u8bc4\u4f30\u3002\u7ed3\u8bba\u662fAI\u8bc4\u4f30\u662f\u4e00\u4e2a\u4e0d\u65ad\u6f14\u8fdb\u3001\u5bf9\u6297\u6027\u7684\u8fc7\u7a0b\uff0c\u5b83\u91cd\u65b0\u5b9a\u4e49\u4e86\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\u3002", "motivation": "\u8bc4\u4f30\u9886\u57df\u6b63\u5728\u7ecf\u5386\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u4ece\u6a21\u578b\u201c\u770b\u5230\u4ec0\u4e48\u201d\u7684\u7b80\u5355\u8bc6\u522b\u4efb\u52a1\u8f6c\u5411\u63a2\u7a76\u201c\u4e3a\u4f55\u201d\u548c\u201c\u5982\u4f55\u201d\u7406\u89e3\u7684\u590d\u6742\u63a8\u7406\u57fa\u51c6\u3002\u65e7\u57fa\u51c6\u5df2\u8d8b\u4e8e\u9971\u548c\uff0c\u5176\u9ad8\u8868\u73b0\u5e38\u63a9\u76d6\u5982\u6377\u5f84\u5b66\u4e60\u3001\u7ec4\u5408\u6cdb\u5316\u5931\u8d25\u7b49\u6839\u672c\u6027\u7f3a\u9677\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u5148\u8fdb\u3001\u80fd\u8bca\u65ad\u7cfb\u7edf\u6027\u95ee\u9898\u7684\u201c\u8ba4\u77e5\u8003\u8bd5\u201d\u3002", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u591a\u6a21\u6001AI\u7684\u8bc4\u4f30\u6f14\u53d8\u63cf\u7ed8\u4e3a\u4e00\u7cfb\u5217\u65e5\u76ca\u590d\u6742\u7684\u201c\u8ba4\u77e5\u8003\u8bd5\u201d\u3002\u5b83\u56de\u987e\u4e86\u4eceImageNet\u65f6\u4ee3\u7684\u201c\u77e5\u8bc6\u6d4b\u8bd5\u201d\uff0c\u5230GQA\u3001VCR\u7b49\u65e8\u5728\u8bca\u65ad\u7cfb\u7edf\u7f3a\u9677\u7684\u201c\u5e94\u7528\u903b\u8f91\u4e0e\u7406\u89e3\u201d\u8003\u8bd5\uff0c\u518d\u5230\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8bbe\u8ba1\u7684MMBench\u3001SEED-Bench\u3001MMMU\u7b49\u201c\u4e13\u5bb6\u7ea7\u6574\u5408\u201d\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u8bc4\u4f30\u62bd\u8c61\u3001\u521b\u9020\u6027\u548c\u793e\u4f1a\u667a\u80fd\u7684\u672a\u6765\u65b9\u5411\u3002", "result": "\u8bba\u6587\u5c55\u73b0\u4e86AI\u8bc4\u4f30\u7684\u6e05\u6670\u6f14\u8fdb\u8def\u5f84\uff0c\u4ece\u65e9\u671f\u7b80\u5355\u8bc6\u522b\u4efb\u52a1\u53d1\u5c55\u5230\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u65e8\u5728\u8bca\u65ad\u66f4\u6df1\u5c42\u6b21\u7684\u7f3a\u9677\u3002\u5b83\u63ed\u793a\u4e86\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5f53\u524d\u57fa\u51c6\u65e5\u76ca\u4fa7\u91cd\u4e8e\u8bc4\u4f30\u5176\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u8bc6\u522b\u4e86\u6377\u5f84\u5b66\u4e60\u548c\u7ec4\u5408\u6cdb\u5316\u5931\u8d25\u7b49\u7cfb\u7edf\u6027\u95ee\u9898\u3002", "conclusion": "AI\u8bc4\u4f30\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u96c6\u7684\u5386\u53f2\uff0c\u800c\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u3001\u5bf9\u6297\u6027\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u66f4\u5b8c\u5584\u7684\u6d4b\u8bd5\u6765\u91cd\u65b0\u5b9a\u4e49\u6211\u4eec\u6784\u5efa\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\u3002"}}
{"id": "2510.04071", "pdf": "https://arxiv.org/pdf/2510.04071", "abs": "https://arxiv.org/abs/2510.04071", "authors": ["Zitian Gao", "Haoming Luo", "Lynx Chen", "Jason Klein Liu", "Ran Tao", "Joey Zhou", "Bryan Dai"], "title": "What Makes Diffusion Language Models Super Data Learners?", "categories": ["cs.CL"], "comment": "Technical report, work in progress", "summary": "Recent studies have shown that diffusion language models achieve remarkable\ndata efficiency under limited-data constraints, yet the underlying mechanisms\nremain unclear. In this work, we perform extensive ablation experiments to\ndisentangle the sources of this efficiency. Our results show that random\nmasking of input tokens plays the dominant role. We further show that similar\ngains can be obtained through in MLP dropout and weight decay, indicating that\nstochastic regularization broadly enhances data efficiency in multi-epoch\ntraining. Our code is available at\nhttps://github.com/zitian-gao/data-efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u6765\u6e90\uff0c\u53d1\u73b0\u968f\u673a\u8f93\u5165\u4ee4\u724c\u63a9\u7801\u662f\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u6307\u51fa\u968f\u673a\u6b63\u5219\u5316\u666e\u904d\u63d0\u5347\u4e86\u591aepoch\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u6548\u7387\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u673a\u5236\u3002", "method": "\u901a\u8fc7\u8fdb\u884c\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u4ee5\u89e3\u6784\u6570\u636e\u6548\u7387\u7684\u6765\u6e90\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u8f93\u5165\u4ee4\u724c\u7684\u968f\u673a\u63a9\u7801\u662f\u6570\u636e\u6548\u7387\u7684\u4e3b\u8981\u8d21\u732e\u8005\u3002\u6b64\u5916\uff0cMLP dropout\u548c\u6743\u91cd\u8870\u51cf\u4e5f\u80fd\u83b7\u5f97\u7c7b\u4f3c\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u968f\u673a\u6b63\u5219\u5316\u5728\u591aepoch\u8bad\u7ec3\u4e2d\u80fd\u666e\u904d\u589e\u5f3a\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.03701", "pdf": "https://arxiv.org/pdf/2510.03701", "abs": "https://arxiv.org/abs/2510.03701", "authors": ["Kanoko Goto", "Takumi Hirose", "Mahiro Ukai", "Shuhei Kurita", "Nakamasa Inoue"], "title": "Referring Expression Comprehension for Small Objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5c0f\u578b\u76ee\u6807\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u7684\u65b0\u6570\u636e\u96c6SOREC\u548c\u65b0\u65b9\u6cd5PIZA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5c0f\u578b\u76ee\u6807\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u5728\u5b9a\u4f4d\u76ee\u6807\u7269\u4f53\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u6781\u5176\u5fae\u5c0f\u76ee\u6807\u7684\u5b9a\u4f4d\u4ecd\u7136\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u5c0f\u578b\u76ee\u6807REC\uff08SOREC\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5c0f\u578b\u76ee\u6807\u6307\u4ee3\u8868\u8fbe\u548c\u8fb9\u754c\u6846\uff1b\u63d0\u51fa\u4e86\u6e10\u8fdb\u8fed\u4ee3\u7f29\u653e\u9002\u914d\u5668\uff08PIZA\uff09\uff0c\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u9002\u914d\u5668\u6a21\u5757\uff0c\u4f7f\u6a21\u578b\u80fd\u9010\u6b65\u653e\u5927\u5e76\u5b9a\u4f4d\u5c0f\u578b\u76ee\u6807\u3002", "result": "\u5c06PIZA\u5e94\u7528\u4e8eGroundingDINO\u6a21\u578b\uff0c\u5728SOREC\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "SOREC\u6570\u636e\u96c6\u548cPIZA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u578b\u76ee\u6807REC\u7684\u6311\u6218\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.03291", "pdf": "https://arxiv.org/pdf/2510.03291", "abs": "https://arxiv.org/abs/2510.03291", "authors": ["Yizhuo Ding", "Wanying Qu", "Jiawei Geng", "Wenqi Shao", "Yanwei Fu"], "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks\nbut face prohibitive computational and memory costs. Pruning offers a promising\npath by inducing sparsity while preserving architectural flexibility. However,\nexisting methods struggle to balance efficiency and robustness: local metric\napproaches prune layer by layer but often collapse under high sparsity, whereas\nglobal feedback methods enforce consistency at the cost of expensive weight\nupdates or restrictive semi-structured formats. We present UniPruning, a\nunified post-training pruning framework that combines the speed of local\nsaliency metrics with the stability of global coordination, enabled by a mirror\ndescent based optimization, all without updating model weights. UniPruning\nleverages fast layer-wise scoring and a lightweight global controller to\nallocate a single sparsity budget, supporting both unstructured and\nsemi-structured N :M pruning within one framework. After a brief calibration,\nit can generate pruning masks for arbitrary sparsity levels in one shot, and\nadapts seamlessly to hardware-aware constraints. Extensive experiments on\nmultiple pretrained LLM families and standard benchmarks show that UniPruning\nconsistently delivers competitive or superior perplexity and zero-shot\naccuracy. Ablation studies further highlight the importance of mirror descent\nand local saliency anchoring. Overall, UniPruning provides an efficient,\nprincipled, and scalable solution for sparsifying large-scale LLMs. Our code is\navailable at: https://github.com/RainbowQTT/UniPruning.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9ad8\u6602\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u672c\u6587\u63d0\u51faUniPruning\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u540e\u8bad\u7ec3\u526a\u679d\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u5c40\u90e8\u5ea6\u91cf\u7684\u901f\u5ea6\u548c\u5168\u5c40\u534f\u8c03\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u57fa\u4e8e\u955c\u50cf\u4e0b\u964d\u7684\u4f18\u5316\uff0c\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u5730\u5b9e\u73b0\u4e86LLMs\u7684\u7a00\u758f\u5316\uff0c\u5e76\u5728\u591a\u79cdLLM\u5bb6\u65cf\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6216\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLMs\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u6602\uff0c\u800c\u526a\u679d\u867d\u6709\u6f5c\u529b\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u5c40\u90e8\u526a\u679d\u5728\u7a00\u758f\u5ea6\u9ad8\u65f6\u6613\u5d29\u6e83\uff0c\u5168\u5c40\u526a\u679d\u5219\u9700\u6602\u8d35\u7684\u6743\u91cd\u66f4\u65b0\u6216\u53d7\u9650\u4e8e\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u9c81\u68d2\u7684\u7edf\u4e00\u526a\u679d\u65b9\u6848\u3002", "method": "\u63d0\u51faUniPruning\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u540e\u8bad\u7ec3\u526a\u679d\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\u5c40\u90e8\u663e\u8457\u6027\u5ea6\u91cf\u7684\u901f\u5ea6\u548c\u5168\u5c40\u534f\u8c03\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u57fa\u4e8e\u955c\u50cf\u4e0b\u964d\u7684\u4f18\u5316\u5b9e\u73b0\uff0c\u4e14\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5feb\u901f\u7684\u9010\u5c42\u8bc4\u5206\u548c\u8f7b\u91cf\u7ea7\u5168\u5c40\u63a7\u5236\u5668\u6765\u5206\u914d\u7edf\u4e00\u7684\u7a00\u758f\u5ea6\u9884\u7b97\uff0c\u652f\u6301\u975e\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\uff0c\u80fd\u4e00\u6b21\u6027\u751f\u6210\u4efb\u610f\u7a00\u758f\u5ea6\u7684\u526a\u679d\u63a9\u7801\uff0c\u5e76\u9002\u5e94\u786c\u4ef6\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u9884\u8bad\u7ec3LLM\u5bb6\u65cf\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cUniPruning\u4e00\u81f4\u5730\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u56f0\u60d1\u5ea6\uff08perplexity\uff09\u548c\u96f6\u6837\u672c\uff08zero-shot\uff09\u51c6\u786e\u6027\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u955c\u50cf\u4e0b\u964d\u548c\u5c40\u90e8\u663e\u8457\u6027\u951a\u5b9a\u7684\u91cd\u8981\u6027\u3002", "conclusion": "UniPruning\u4e3a\u5927\u89c4\u6a21LLMs\u7684\u7a00\u758f\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u6709\u539f\u5219\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04173", "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "categories": ["cs.AI"], "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments.", "AI": {"tldr": "Open Agent Specification (Agent Spec) \u662f\u4e00\u4e2a\u58f0\u660e\u6027\u8bed\u8a00\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u89c4\u8303\uff0c\u89e3\u51b3AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u4fc3\u8fdb\u8de8\u6846\u67b6\u7684\u53ef\u79fb\u690d\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u6846\u67b6\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u63d0\u9ad8AI\u4ee3\u7406\u5728\u4e0d\u540c\u6846\u67b6\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u51cf\u5c11\u5197\u4f59\u5f00\u53d1\u5de5\u4f5c\uff0c\u5e76\u4fc3\u8fdb\u5f00\u53d1\u5de5\u5177\u548c\u53ef\u79fb\u690d\u6027\u3002", "method": "\u5f15\u5165Agent Spec\u4f5c\u4e3a\u4e00\u79cd\u58f0\u660e\u6027\u8bed\u8a00\u548c\u901a\u7528\u7684\u7edf\u4e00\u89c4\u8303\uff0c\u4f7f\u5176\u80fd\u4f5c\u4e3aAI\u4ee3\u7406\u53ca\u5176\u5de5\u4f5c\u6d41\u7684\u4ea4\u6362\u683c\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u8de8\u4e0d\u540cAI\u6846\u67b6\u7684\u517c\u5bb9\u6027\u3002", "result": "Agent Spec\u4f7f\u4ee3\u7406\u5f00\u53d1\u8005\u83b7\u5f97\u66f4\u591a\u53ef\u91cd\u7528\u7ec4\u4ef6\uff0c\u6846\u67b6\u548c\u5de5\u5177\u5f00\u53d1\u8005\u53d7\u76ca\u4e8e\u4e92\u64cd\u4f5c\u6027\u652f\u6301\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u5b9e\u73b0\u53ef\u91cd\u73b0\u548c\u53ef\u6bd4\u7684\u7ed3\u679c\uff0c\u4f01\u4e1a\u5219\u80fd\u52a0\u901f\u539f\u578b\u90e8\u7f72\u3001\u63d0\u9ad8\u751f\u4ea7\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Agent Spec\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u7684\u58f0\u660e\u6027\u8bed\u8a00\u89c4\u8303\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u5f00\u53d1\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u53ef\u91cd\u7528\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6574\u4e2aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5e26\u6765\u4e86\u591a\u65b9\u9762\u76ca\u5904\u3002"}}
{"id": "2510.04080", "pdf": "https://arxiv.org/pdf/2510.04080", "abs": "https://arxiv.org/abs/2510.04080", "authors": ["Zixin Song", "Bowen Zhang", "Qian-Wen Zhang", "Di Yin", "Xing Sun", "Chunping Li"], "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity", "categories": ["cs.CL"], "comment": null, "summary": "Conditional Semantic Textual Similarity (C-STS) measures the semantic\nproximity between text segments under a specific condition, thereby overcoming\nthe ambiguity inherent in traditional STS. However, existing methods are\nlargely confined to discriminative models, failing to fully integrate recent\nbreakthroughs in the NLP community concerning Large Language Models (LLMs) and\nReinforcement Learning (RL). RL is a particularly well-suited paradigm for this\ntask, as it can directly optimize the non-differentiable Spearman ranking\nmetric and guide the reasoning process required by C-STS. However, we find that\nnaively applying listwise RL fails to produce meaningful improvements, as the\nmodel is overwhelmed by complex, coarse-grained reward signals. To address this\nchallenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning\nframework. PoLi-RL employs a two-stage curriculum: it first trains the model\nwith simple pointwise rewards to establish fundamental scoring capabilities,\nthen transitions to a hybrid reward that combines pointwise, pairwise, and\nlistwise objectives to refine the model's ability to discern subtle semantic\ndistinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward\n(PSRR) mechanism that computes ranking rewards in parallel slices, where each\nslice comprises same-indexed completions from different samples. This provides\na precise, differentiated learning signal for each individual completion,\nenabling granular credit assignment and effective optimization. On the official\nC-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,\nestablishing a new SOTA for the cross-encoder architecture. As the first work\nto successfully apply RL to C-STS, our study introduces a powerful and precise\nparadigm for training LLMs on complex, ranking-based conditional judgment\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPoLi-RL\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u70b9\u5230\u5217\u8868\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u548c\u521b\u65b0\u7684\u5e76\u884c\u5207\u7247\u6392\u5e8f\u5956\u52b1\u673a\u5236\uff0c\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6761\u4ef6\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\uff08C-STS\uff09\u4efb\u52a1\uff0c\u514b\u670d\u4e86\u4f20\u7edfRL\u5e94\u7528\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u4ea4\u53c9\u7f16\u7801\u5668\u67b6\u6784\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u6761\u4ef6\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\uff08C-STS\uff09\u65e8\u5728\u89e3\u51b3\u4f20\u7edfSTS\u7684\u6b67\u4e49\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5224\u522b\u6a21\u578b\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6700\u65b0\u7a81\u7834\u3002RL\u7279\u522b\u9002\u5408C-STS\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u80fd\u76f4\u63a5\u4f18\u5316\u4e0d\u53ef\u5fae\u7684Spearman\u6392\u5e8f\u6307\u6807\u3002\u4f46\u7b80\u5355\u5e94\u7528\u5217\u8868\u5f0fRL\u56e0\u5956\u52b1\u4fe1\u53f7\u8fc7\u4e8e\u590d\u6742\u548c\u7c97\u7cd9\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165PoLi-RL\uff08Point-to-List Reinforcement Learning\uff09\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u7b80\u5355\u7684\u9010\u70b9\u5956\u52b1\uff08pointwise rewards\uff09\u8bad\u7ec3\u6a21\u578b\u5efa\u7acb\u57fa\u7840\u8bc4\u5206\u80fd\u529b\uff1b\u7136\u540e\u8fc7\u6e21\u5230\u7ed3\u5408\u9010\u70b9\u3001\u6210\u5bf9\u548c\u5217\u8868\u5f0f\u76ee\u6807\u7684\u6df7\u5408\u5956\u52b1\uff0c\u4ee5\u7cbe\u70bc\u6a21\u578b\u533a\u5206\u7ec6\u5fae\u8bed\u4e49\u5dee\u522b\u7684\u80fd\u529b\u3002\u5173\u952e\u521b\u65b0\u662f\u63d0\u51fa\u4e86\u5e76\u884c\u5207\u7247\u6392\u5e8f\u5956\u52b1\uff08Parallel Slice Ranking Reward, PSRR\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u5e76\u884c\u5207\u7247\u4e2d\u8ba1\u7b97\u6392\u5e8f\u5956\u52b1\uff0c\u6bcf\u4e2a\u5207\u7247\u7531\u6765\u81ea\u4e0d\u540c\u6837\u672c\u7684\u76f8\u540c\u7d22\u5f15\u8865\u5168\u7ec4\u6210\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u5355\u72ec\u8865\u5168\u63d0\u4f9b\u7cbe\u786e\u4e14\u5dee\u5f02\u5316\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u548c\u6709\u6548\u4f18\u5316\u3002", "result": "\u5728\u5b98\u65b9C-STS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoLi-RL\u53d6\u5f97\u4e8648.18\u7684Spearman\u76f8\u5173\u7cfb\u6570\uff0c\u4e3a\u4ea4\u53c9\u7f16\u7801\u5668\uff08cross-encoder\uff09\u67b6\u6784\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8eC-STS\u4efb\u52a1\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u7cbe\u786e\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u7684\u3001\u57fa\u4e8e\u6392\u5e8f\u7684\u6761\u4ef6\u5224\u65ad\u4efb\u52a1\u3002"}}
{"id": "2510.03717", "pdf": "https://arxiv.org/pdf/2510.03717", "abs": "https://arxiv.org/abs/2510.03717", "authors": ["Sharan SK", "Subin Sahayam", "Umarani Jayaraman", "Lakshmi Priya A"], "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 6 figures, preprint under review", "summary": "Segmenting of clinically important retinal blood vessels into arteries and\nveins is a prerequisite for retinal vessel analysis. Such analysis can provide\npotential insights and bio-markers for identifying and diagnosing various\nretinal eye diseases. Alteration in the regularity and width of the retinal\nblood vessels can act as an indicator of the health of the vasculature system\nall over the body. It can help identify patients at high risk of developing\nvasculature diseases like stroke and myocardial infarction. Over the years,\nvarious Deep Learning architectures have been proposed to perform retinal\nvessel segmentation. Recently, attention mechanisms have been increasingly used\nin image segmentation tasks. The work proposes a new Deep Learning approach for\nartery-vein segmentation. The new approach is based on the Attention mechanism\nthat is incorporated into the WNet Deep Learning model, and we call the model\nas Attention-WNet. The proposed approach has been tested on publicly available\ndatasets such as HRF and DRIVE datasets. The proposed approach has outperformed\nother state-of-art models available in the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAttention-WNet\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u673a\u5236\u878d\u5165WNet\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u8840\u7ba1\u52a8\u8109-\u9759\u8109\u5206\u5272\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u89c6\u7f51\u819c\u8840\u7ba1\uff08\u52a8\u8109\u548c\u9759\u8109\uff09\u5206\u5272\u662f\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u6790\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u80fd\u4e3a\u8bca\u65ad\u591a\u79cd\u89c6\u7f51\u819c\u773c\u75c5\u63d0\u4f9b\u6f5c\u5728\u6d1e\u5bdf\u548c\u751f\u7269\u6807\u5fd7\u7269\u3002\u89c6\u7f51\u819c\u8840\u7ba1\u7684\u89c4\u5f8b\u6027\u548c\u5bbd\u5ea6\u53d8\u5316\u53ef\u4f5c\u4e3a\u5168\u8eab\u8840\u7ba1\u7cfb\u7edf\u5065\u5eb7\u7684\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u4e2d\u98ce\u548c\u5fc3\u808c\u6897\u6b7b\u7b49\u8840\u7ba1\u75be\u75c5\u7684\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u540d\u4e3aAttention-WNet\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5c06\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5230WNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u52a8\u8109-\u9759\u8109\u5206\u5272\u3002", "result": "\u6240\u63d0\u51fa\u7684Attention-WNet\u65b9\u6cd5\u5df2\u5728HRF\u548cDRIVE\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u8d85\u8d8a\u4e86\u6587\u732e\u4e2d\u5176\u4ed6\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "Attention-WNet\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u7f51\u819c\u52a8\u8109-\u9759\u8109\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u6790\u548c\u76f8\u5173\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03293", "pdf": "https://arxiv.org/pdf/2510.03293", "abs": "https://arxiv.org/abs/2510.03293", "authors": ["Rana Shahout", "Colin Cai", "Yilun Du", "Minlan Yu", "Michael Mitzenmacher"], "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each\ntoken to a subset of experts through a learned gate function. While conditional\nrouting reduces training costs, it shifts the burden on inference memory:\nexpert parameters and activations consume memory, limiting the number of\nexperts per device. As tokens are routed, some experts become overloaded while\nothers are underutilized. Because experts are mapped to GPUs, this imbalance\ntranslates directly into degraded system performance in terms of latency,\nthroughput, and cost. We present LASER, a plug-and-play, inference-time routing\nalgorithm that balances load while preserving accuracy. LASER adapts to the\nshape of the gate's score distribution. When scores provide a clear preference,\nit routes to the strongest experts; when scores are more uniform, it broadens\nthe set of viable experts and routes to the least-loaded among them. Because\nLASER relies only on gate scores from a trained model, it integrates directly\ninto existing MoE inference pipelines without retraining or finetuning. We\nevaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets\n(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,\ntranslating into lower latency and higher throughput, while keeping the\naccuracy changes negligible.", "AI": {"tldr": "MoE\u6a21\u578b\u63a8\u7406\u65f6\u56e0\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51faLASER\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u5b83\u80fd\u6839\u636e\u95e8\u63a7\u5206\u6570\u5206\u5e03\u81ea\u9002\u5e94\u5730\u5e73\u8861\u4e13\u5bb6\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4ece\u800c\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "Mixture-of-Experts (MoE) \u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6bcf\u8bbe\u5907\u4e13\u5bb6\u6570\u91cf\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6761\u4ef6\u8def\u7531\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\uff08\u90e8\u5206\u4e13\u5bb6\u8fc7\u8f7d\uff0c\u90e8\u5206\u672a\u5145\u5206\u5229\u7528\uff09\uff0c\u5f53\u4e13\u5bb6\u6620\u5c04\u5230GPU\u65f6\uff0c\u8fd9\u76f4\u63a5\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u73b0\u4e3a\u9ad8\u5ef6\u8fdf\u3001\u4f4e\u541e\u5410\u91cf\u548c\u9ad8\u6210\u672c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LASER\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u65e8\u5728\u5e73\u8861\u4e13\u5bb6\u8d1f\u8f7d\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002LASER\u6839\u636e\u95e8\u63a7\u5206\u6570\u7684\u5206\u5e03\u5f62\u6001\u8fdb\u884c\u81ea\u9002\u5e94\u8def\u7531\uff1a\u5f53\u5206\u6570\u663e\u793a\u51fa\u660e\u786e\u504f\u597d\u65f6\uff0c\u5b83\u8def\u7531\u5230\u5206\u6570\u6700\u9ad8\u7684\u4e13\u5bb6\uff1b\u5f53\u5206\u6570\u5206\u5e03\u66f4\u5747\u5300\u65f6\uff0c\u5b83\u4f1a\u6269\u5927\u6f5c\u5728\u4e13\u5bb6\u7684\u96c6\u5408\uff0c\u5e76\u8def\u7531\u5230\u5176\u4e2d\u8d1f\u8f7d\u6700\u8f7b\u7684\u4e13\u5bb6\u3002LASER\u4ec5\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u95e8\u63a7\u5206\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709MoE\u63a8\u7406\u7ba1\u9053\u4e2d\u3002", "result": "LASER\u5728Mixtral-8x7B\u548cDeepSeek-MoE-16b-chat\u6a21\u578b\u4e0a\uff0c\u4ee5\u53caARC-Easy\u3001ARC-Challenge\u3001MMLU\u548cGSM8K\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cLASER\u663e\u8457\u6539\u5584\u4e86\u8d1f\u8f7d\u5747\u8861\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u5fae\u4e0d\u8db3\u9053\u3002", "conclusion": "LASER\u6210\u529f\u89e3\u51b3\u4e86MoE\u6a21\u578b\u63a8\u7406\u65f6\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5176\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\uff0c\u5728\u4e0d\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff08\u964d\u4f4e\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\uff09\uff0c\u5e76\u4e14\u5177\u6709\u6613\u4e8e\u96c6\u6210\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.04195", "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "categories": ["cs.AI"], "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents.", "AI": {"tldr": "\u9488\u5bf9LLM\u5728\u957f\u73af\u5883\u4e0b\u7684\u589e\u91cf\u5730\u56fe\u6784\u5efa\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6784\u5efa\u4e0e\u4fee\u590d\u6846\u67b6\uff0c\u6838\u5fc3\u5728\u4e8e\u7248\u672c\u63a7\u5236\u548c\u8fb9\u5f71\u54cd\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u56fe\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5904\u7406\u5927\u89c4\u6a21\u73af\u5883\u65f6\uff0c\u5176\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684\u67e5\u8be2\u80fd\u529b\u4e0d\u8db3\u4ee5\u6709\u6548\u63a8\u65ad\u7a7a\u95f4\u5e03\u5c40\u5e76\u56de\u7b54\u7528\u6237\u67e5\u8be2\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u589e\u91cf\u5730\u56fe\u6784\u5efa\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u9010\u6b65\u89c2\u5bdf\u4e2d\u6784\u5efa\u5b8c\u6574\u62d3\u6251\u56fe\u5e76\u5904\u7406\u7ed3\u6784\u4e0d\u4e00\u81f4\u6027\u7684\u589e\u91cf\u5730\u56fe\u6784\u5efa\u4e0e\u4fee\u590d\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u5730\u56fe\u6784\u5efa\u4e0e\u4fee\u590d\u6846\u67b6\uff0c\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a\n1.  **\u7248\u672c\u63a7\u5236\uff08Version Control\uff09**\uff1a\u8bb0\u5f55\u56fe\u7f16\u8f91\u548c\u6e90\u89c2\u5bdf\u7684\u5b8c\u6574\u5386\u53f2\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u56de\u6eda\u3001\u51b2\u7a81\u8ffd\u8e2a\u548c\u4fee\u590d\u8bc4\u4f30\u3002\n2.  **\u8fb9\u5f71\u54cd\u5206\u6570\uff08Edge Impact Score\uff09**\uff1a\u6839\u636e\u7ed3\u6784\u53ef\u8fbe\u6027\u3001\u8def\u5f84\u4f7f\u7528\u548c\u51b2\u7a81\u4f20\u64ad\u6765\u4f18\u5148\u9009\u62e9\u6210\u672c\u6700\u4f4e\u7684\u4fee\u590d\u65b9\u6848\u3002\n3.  **\u7cbe\u70bc\u57fa\u51c6\u6570\u636e\u96c6**\uff1a\u4e3a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86MANGO\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7cbe\u70bc\u7248\u672c\uff0c\u79fb\u9664\u4e86\u975e\u62d3\u6251\u52a8\u4f5c\u548c\u56fa\u6709\u7ed3\u6784\u51b2\u7a81\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u56fe\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u7ea0\u7f20\u6216\u94fe\u5f0f\u4e0d\u4e00\u81f4\u6027\u60c5\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5185\u7701\u7684\u3001\u5386\u53f2\u611f\u77e5\u7684\u4fee\u590d\u673a\u5236\u5bf9\u4e8e\u7ef4\u6301LLM\u667a\u80fd\u4f53\u8fde\u8d2f\u7a7a\u95f4\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04081", "pdf": "https://arxiv.org/pdf/2510.04081", "abs": "https://arxiv.org/abs/2510.04081", "authors": ["Honglin Lin", "Qizhi Pei", "Xin Gao", "Zhuoshi Pan", "Yu Li", "Juntao Li", "Conghui He", "Lijun Wu"], "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "categories": ["cs.CL", "cs.PL"], "comment": "Accepted by NeurIPS2025", "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve\ncomplex tasks, yet achieving reliable and scalable reasoning remains\nchallenging. While Chain-of-Thought (CoT) prompting has become a mainstream\napproach, existing methods often suffer from uncontrolled generation,\ninsufficient quality, and limited diversity in reasoning paths. Recent efforts\nleverage code to enhance CoT by grounding reasoning in executable steps, but\nsuch methods are typically constrained to predefined mathematical problems,\nhindering scalability and generalizability. In this work, we propose Caco\n(Code-Assisted Chain-of-ThOught), a novel framework that automates the\nsynthesis of high-quality, verifiable, and diverse instruction-CoT reasoning\ndata through code-driven augmentation. Unlike prior work, Caco first fine-tunes\na code-based CoT generator on existing math and programming solutions in a\nunified code format, then scales the data generation to a large amount of\ndiverse reasoning traces. Crucially, we introduce automated validation via code\nexecution and rule-based filtering to ensure logical correctness and structural\ndiversity, followed by reverse-engineering filtered outputs into natural\nlanguage instructions and language CoTs to enrich task adaptability. This\nclosed-loop process enables fully automated, scalable synthesis of reasoning\ndata with guaranteed executability. Experiments on our created Caco-1.3M\ndataset demonstrate that Caco-trained models achieve strong competitive\nperformance on mathematical reasoning benchmarks, outperforming existing strong\nbaselines. Further analysis reveals that Caco's code-anchored verification and\ninstruction diversity contribute to superior generalization across unseen\ntasks. Our work establishes a paradigm for building self-sustaining,\ntrustworthy reasoning systems without human intervention.", "AI": {"tldr": "Caco\u6846\u67b6\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u589e\u5f3a\uff0c\u81ea\u52a8\u5316\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u4e14\u591a\u6837\u5316\u7684\u6307\u4ee4-CoT\u63a8\u7406\u6570\u636e\u3002\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u81ea\u4e3b\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u73b0\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709CoT\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u4e0d\u53ef\u63a7\u3001\u8d28\u91cf\u4e0d\u8db3\u548c\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002\u5229\u7528\u4ee3\u7801\u589e\u5f3aCoT\u7684\u65b9\u6cd5\u5e38\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u6570\u5b66\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u63d0\u51faCaco\uff08Code-Assisted Chain-of-ThOught\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u589e\u5f3a\uff0c\u81ea\u52a8\u5316\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u3001\u591a\u6837\u5316\u7684\u6307\u4ee4-CoT\u63a8\u7406\u6570\u636e\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a\u9996\u5148\uff0c\u5728\u7edf\u4e00\u4ee3\u7801\u683c\u5f0f\u7684\u73b0\u6709\u6570\u5b66\u548c\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\u4e0a\u5fae\u8c03\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7801\u7684CoT\u751f\u6210\u5668\uff1b\u7136\u540e\uff0c\u6269\u5c55\u6570\u636e\u751f\u6210\u4ee5\u83b7\u5f97\u5927\u91cf\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\uff1b\u63a5\u7740\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8fc7\u6ee4\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\uff0c\u786e\u4fdd\u903b\u8f91\u6b63\u786e\u6027\u548c\u7ed3\u6784\u591a\u6837\u6027\uff1b\u6700\u540e\uff0c\u5c06\u8fc7\u6ee4\u540e\u7684\u8f93\u51fa\u9006\u5411\u5de5\u7a0b\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u8bed\u8a00CoT\uff0c\u4ee5\u4e30\u5bcc\u4efb\u52a1\u9002\u5e94\u6027\u3002\u8fd9\u662f\u4e00\u4e2a\u95ed\u73af\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u4e14\u4fdd\u8bc1\u53ef\u6267\u884c\u6027\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u3002", "result": "\u5728\u81ea\u5efa\u7684Caco-1.3M\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaco\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u7ade\u4e89\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\uff0cCaco\u7684\u4ee3\u7801\u951a\u5b9a\u9a8c\u8bc1\u548c\u6307\u4ee4\u591a\u6837\u6027\u6709\u52a9\u4e8e\u5728\u672a\u89c1\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u81ea\u7ef4\u6301\u3001\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.03721", "pdf": "https://arxiv.org/pdf/2510.03721", "abs": "https://arxiv.org/abs/2510.03721", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Trevor Darrell", "Zeynep Akata"], "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG"], "comment": "48 pages", "summary": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.", "AI": {"tldr": "\u901a\u8fc7\u5bf9LAION-400M\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u4eba\u53e3\u7edf\u8ba1\u5b66\u6807\u6ce8\uff0c\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u6765\u6e90\uff0c\u53d1\u73b0\u6570\u636e\u4e2d\u5b58\u5728\u6709\u5bb3\u5173\u8054\uff0c\u5e76\u8bc1\u660e\u6570\u636e\u5171\u73b0\u53ef\u89e3\u91ca\u6a21\u578b\u4e2d\u7684\u5927\u90e8\u5206\u6027\u522b\u504f\u89c1\uff0c\u4ece\u800c\u9996\u6b21\u5efa\u7acb\u4e86\u6570\u636e\u96c6\u6784\u6210\u4e0e\u6a21\u578b\u504f\u89c1\u7684\u5b9e\u8bc1\u8054\u7cfb\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u5728\u4ea7\u751f\u8fd9\u4e9b\u504f\u89c1\u4e2d\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u7f51\u7edc\u6570\u636e\u96c6\uff08\u5982LAION-400M\uff09\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u6807\u6ce8\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7269\u4f53\u68c0\u6d4b\u3001\u591a\u6a21\u6001\u5b57\u5e55\u548c\u5fae\u8c03\u5206\u7c7b\u5668\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u4e3aLAION-400M\u6570\u636e\u96c6\u521b\u5efa\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6807\u6ce8\uff0c\u5305\u62ec\u8d85\u8fc72.76\u4ebf\u4e2a\u8fb9\u754c\u6846\u3001\u611f\u77e5\u7684\u6027\u522b\u548c\u79cd\u65cf/\u6c11\u65cf\u6807\u7b7e\u4ee5\u53ca\u81ea\u52a8\u751f\u6210\u7684\u63cf\u8ff0\u3002", "result": "1. \u63ed\u793a\u4e86\u6570\u636e\u4e2d\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u4e0d\u5e73\u8861\u548c\u6709\u5bb3\u5173\u8054\uff0c\u4f8b\u5982\u7537\u6027\u4ee5\u53ca\u88ab\u8ba4\u4e3a\u662f\u9ed1\u4eba\u6216\u4e2d\u4e1c\u88d4\u7684\u4e2a\u4f53\u4e0e\u72af\u7f6a\u76f8\u5173\u53ca\u8d1f\u9762\u5185\u5bb9\u8fc7\u5ea6\u5173\u8054\u30022. \u8868\u660eCLIP\u548cStable Diffusion\u4e2d60-70%\u7684\u6027\u522b\u504f\u89c1\u53ef\u4ee5\u7531\u6570\u636e\u4e2d\u7684\u76f4\u63a5\u5171\u73b0\u7ebf\u6027\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u5de5\u4f5c\u9996\u6b21\u5728\u5927\u89c4\u6a21\u5c42\u9762\u5efa\u7acb\u4e86\u6570\u636e\u96c6\u6784\u6210\u4e0e\u4e0b\u6e38\u6a21\u578b\u504f\u89c1\u4e4b\u95f4\u7684\u7ecf\u9a8c\u8054\u7cfb\u3002"}}
{"id": "2510.03298", "pdf": "https://arxiv.org/pdf/2510.03298", "abs": "https://arxiv.org/abs/2510.03298", "authors": ["Dongqi Zheng", "Wenjin Fu"], "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models", "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": "Accepted by 39th NeurIPS - Constrained Optimization for Machine\n  Learning", "summary": "We introduce Constraint-Aware Federated Learning with Lagrangian Dual\nOptimization (CAFL-L), a principled extension of FedAvg that explicitly\nincorporates device-level resource constraints including energy, communication,\nmemory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to\ndynamically adapt training hyperparameters -- freezing depth, local steps,\nbatch size, and communication compression -- while preserving training\nstability through token-budget preservation via gradient accumulation.\nExperiments on a character-level language model demonstrate that CAFL-L\nachieves superior constraint satisfaction compared to standard FedAvg (reducing\nmemory usage by 20% and communication by 95%) while maintaining competitive\nvalidation performance, making it practical for deployment on\nresource-constrained edge devices.", "AI": {"tldr": "\u63d0\u51faCAFL-L\uff0c\u4e00\u79cd\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u6269\u5c55\uff0c\u65e8\u5728\u663e\u5f0f\u5904\u7406\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u6765\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u8054\u90a6\u5e73\u5747\uff08FedAvg\uff09\u672a\u5145\u5206\u8003\u8651\u8fb9\u7f18\u8bbe\u5907\u7684\u8d44\u6e90\u9650\u5236\uff08\u5982\u80fd\u8017\u3001\u901a\u4fe1\u3001\u5185\u5b58\u3001\u6563\u70ed\uff09\uff0c\u963b\u788d\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8d85\u53c2\u6570\uff08\u5982\u51bb\u7ed3\u6df1\u5ea6\u3001\u672c\u5730\u6b65\u6570\u3001\u6279\u91cf\u5927\u5c0f\u3001\u901a\u4fe1\u538b\u7f29\uff09\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u7d2f\u79ef\u7684\u4ee4\u724c\u9884\u7b97\u4fdd\u7559\u673a\u5236\u6765\u7ef4\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5b57\u7b26\u7ea7\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cCAFL-L\u6bd4\u6807\u51c6FedAvg\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7ea6\u675f\u6ee1\u8db3\u80fd\u529b\uff08\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120%\uff0c\u901a\u4fe1\u91cf\u51cf\u5c1195%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "CAFL-L\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u6ee1\u8db3\u4e25\u683c\u7684\u8d44\u6e90\u9884\u7b97\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u5176\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04196", "pdf": "https://arxiv.org/pdf/2510.04196", "abs": "https://arxiv.org/abs/2510.04196", "authors": ["Yizhuo Ding", "Mingkang Chen", "Qiuhua Liu", "Fenghua Weng", "Wanying Qu", "Yue Yang", "Yugang Jiang", "Zuxuan Wu", "Yanwei Fu", "Wenqi Shao"], "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications,\nwhere they must be both useful and safe. Safety is especially challenging in\nmultimodal settings: images and text can be combined to bypass guardrails, and\nsingle objective training can cause policy drift that yields over-refusal on\nbenign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed\nreinforcement learning framework that trains reasoning oriented LMRMs under\nmultimodal, multitask, and multiobjective signals, and we release the resulting\nmodel, COSMO-R1. Our approach aims to let safety and capability grow together\nin one stable pipeline rather than competing during alignment. In experiments,\nCOSMO-R1 improves safety while maintaining-and often improving multimodal\nreasoning and instruction following, shows stronger robustness to multimodal\njailbreaks, and reduces unnecessary refusals. The framework also transfers\nacross backbones with consistent gains. Ablations support the design choices,\nindicating a simple path to advancing safety and general capability together in\nLMRMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86COSMO-RL\uff0c\u4e00\u4e2a\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u548c\u591a\u76ee\u6807\u4fe1\u53f7\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08LMRMs\uff09\uff0c\u4f7f\u5176\u5b89\u5168\u6027\u548c\u80fd\u529b\u5171\u540c\u63d0\u5347\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cCOSMO-R1\u6a21\u578b\u5728\u63d0\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\u4fdd\u6301\u5e76\u6539\u8fdb\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u62d2\u7edd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08LMRMs\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u540c\u7b49\u91cd\u8981\u3002\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u7684\u5b89\u5168\u6027\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56fe\u50cf\u548c\u6587\u672c\u7ec4\u5408\u53ef\u80fd\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u800c\u5355\u4e00\u76ee\u6807\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u6f02\u79fb\uff0c\u4ece\u800c\u5728\u826f\u6027\u8f93\u5165\u4e0a\u8fc7\u5ea6\u62d2\u7edd\u6216\u5728\u98ce\u9669\u8f93\u5165\u4e0a\u4e0d\u5b89\u5168\u5730\u670d\u4ece\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86COSMO-RL\uff0c\u4e00\u4e2a\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u548c\u591a\u76ee\u6807\u4fe1\u53f7\u4e0b\u8bad\u7ec3\u9762\u5411\u63a8\u7406\u7684LMRMs\uff0c\u65e8\u5728\u4f7f\u5b89\u5168\u6027\u548c\u80fd\u529b\u5728\u4e00\u4e2a\u7a33\u5b9a\u7684\u7ba1\u9053\u4e2d\u5171\u540c\u6210\u957f\uff0c\u800c\u975e\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u76f8\u4e92\u7ade\u4e89\u3002\u7814\u7a76\u5e76\u53d1\u5e03\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u6a21\u578bCOSMO-R1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCOSMO-R1\u5728\u63d0\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u5e76\u7ecf\u5e38\u6539\u8fdb\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u62d2\u7edd\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u4e4b\u95f4\u4e5f\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u589e\u76ca\u3002\u6d88\u878d\u5b9e\u9a8c\u652f\u6301\u4e86\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "COSMO-RL\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u6761\u7b80\u5355\u6709\u6548\u7684\u8def\u5f84\uff0c\u53ef\u4ee5\u5728\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u540c\u65f6\u63a8\u8fdb\u5b89\u5168\u6027\u548c\u901a\u7528\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.04120", "pdf": "https://arxiv.org/pdf/2510.04120", "abs": "https://arxiv.org/abs/2510.04120", "authors": ["Fengying Ye", "Shanshan Wang", "Lidia S. Chao", "Derek F. Wong"], "title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Metaphor analysis is a complex linguistic phenomenon shaped by context and\nexternal factors. While Large Language Models (LLMs) demonstrate advanced\ncapabilities in knowledge integration, contextual reasoning, and creative\ngeneration, their mechanisms for metaphor comprehension remain insufficiently\nexplored. This study examines LLMs' metaphor-processing abilities from three\nperspectives: (1) Concept Mapping: using embedding space projections to\nevaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall\nin love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing\nmetaphorical words and their literal counterparts to identify inherent\nmetaphorical knowledge; and (3) Syntactic Sensitivity: assessing how\nmetaphorical syntactic structures influence LLMs' performance. Our findings\nreveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations,\ndepend on metaphorical indicators in training data rather than contextual cues,\nand are more sensitive to syntactic irregularities than to structural\ncomprehension. These insights underline the limitations of LLMs in metaphor\nanalysis and call for more robust computational approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ece\u6982\u5ff5\u6620\u5c04\u3001\u9690\u55bb-\u5b57\u9762\u77e5\u8bc6\u5e93\u548c\u53e5\u6cd5\u654f\u611f\u6027\u4e09\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9690\u55bb\u5904\u7406\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u9690\u55bb\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u800c\u975e\u8bed\u5883\u7ebf\u7d22\uff0c\u5e76\u5bf9\u53e5\u6cd5\u4e0d\u89c4\u8303\u66f4\u654f\u611f\u3002", "motivation": "\u9690\u55bb\u5206\u6790\u53d7\u8bed\u5883\u548c\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\uff0c\u662f\u4e00\u4e2a\u590d\u6742\u7684\u8bed\u8a00\u73b0\u8c61\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u6574\u5408\u3001\u8bed\u5883\u63a8\u7406\u548c\u521b\u9020\u6027\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9690\u55bb\u7406\u89e3\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u4ece\u4e09\u4e2a\u65b9\u9762\u8003\u5bdf\u4e86LLMs\u7684\u9690\u55bb\u5904\u7406\u80fd\u529b\uff1a1) **\u6982\u5ff5\u6620\u5c04**\uff1a\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u6295\u5f71\u8bc4\u4f30LLMs\u5982\u4f55\u6620\u5c04\u76ee\u6807\u57df\u6982\u5ff5\uff1b2) **\u9690\u55bb-\u5b57\u9762\u77e5\u8bc6\u5e93**\uff1a\u5206\u6790\u9690\u55bb\u8bcd\u53ca\u5176\u5b57\u9762\u5bf9\u5e94\u8bcd\uff0c\u4ee5\u8bc6\u522b\u56fa\u6709\u7684\u9690\u55bb\u77e5\u8bc6\uff1b3) **\u53e5\u6cd5\u654f\u611f\u6027**\uff1a\u8bc4\u4f30\u9690\u55bb\u53e5\u6cd5\u7ed3\u6784\u5982\u4f55\u5f71\u54cdLLMs\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cLLMs\u4f1a\u4ea7\u751f15%-25%\u7684\u6982\u5ff5\u4e0d\u76f8\u5173\u89e3\u91ca\uff0c\u5176\u8868\u73b0\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9690\u55bb\u6307\u793a\u800c\u975e\u8bed\u5883\u7ebf\u7d22\uff0c\u5e76\u4e14LLMs\u5bf9\u53e5\u6cd5\u4e0d\u89c4\u8303\u7684\u654f\u611f\u5ea6\u9ad8\u4e8e\u5bf9\u7ed3\u6784\u7406\u89e3\u7684\u654f\u611f\u5ea6\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86LLMs\u5728\u9690\u55bb\u5206\u6790\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2510.03725", "pdf": "https://arxiv.org/pdf/2510.03725", "abs": "https://arxiv.org/abs/2510.03725", "authors": ["Thomas Hallopeau", "Joris Gu\u00e9rin", "Laurent Demagistri", "Youssef Fouzai", "Renata Gracie", "Vanderlei Pascoal De Matos", "Helen Gurgel", "Nadine Dessay"], "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages, 1 figure, 1 table. Presented at the 21st Brazilian Symposium\n  on Remote Sensing (SBSR 2025)", "summary": "While deep learning methods for detecting informal settlements have already\nbeen developed, they have not yet fully utilized the potential offered by\nrecent pretrained neural networks. We compare two types of pretrained neural\nnetworks for detecting the favelas of Rio de Janeiro: 1. Generic networks\npretrained on large diverse datasets of unspecific images, 2. A specialized\nnetwork pretrained on satellite imagery. While the latter is more specific to\nthe target task, the former has been pretrained on significantly more images.\nHence, this research investigates whether task specificity or data volume\nyields superior performance in urban informal settlement detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u901a\u7528\u578b\u4e0e\u536b\u661f\u56fe\u50cf\u4e13\u7528\u578b\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u91cc\u7ea6\u70ed\u5185\u5362\u8d2b\u6c11\u7a9f\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u63a2\u7a76\u4efb\u52a1\u7279\u5f02\u6027\u6216\u6570\u636e\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u6700\u65b0\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u975e\u6b63\u5f0f\u4f4f\u533a\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u4efb\u52a1\u7279\u5f02\u6027\uff08\u4e13\u7528\u7f51\u7edc\uff09\u4e0e\u6570\u636e\u91cf\uff08\u901a\u7528\u7f51\u7edc\uff09\u5bf9\u57ce\u5e02\u975e\u6b63\u5f0f\u4f4f\u533a\u68c0\u6d4b\u6027\u80fd\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "method": "\u5bf9\u6bd4\u4e24\u79cd\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff1a1. \u5728\u5927\u578b\u591a\u6837\u5316\u975e\u7279\u5b9a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u901a\u7528\u7f51\u7edc\u30022. \u5728\u536b\u661f\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u4e13\u7528\u7f51\u7edc\u3002\u5e94\u7528\u4e8e\u91cc\u7ea6\u70ed\u5185\u5362\u8d2b\u6c11\u7a9f\u7684\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u62bd\u8c61\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u8bba\u3002"}}
{"id": "2510.03301", "pdf": "https://arxiv.org/pdf/2510.03301", "abs": "https://arxiv.org/abs/2510.03301", "authors": ["Arthur Sedek"], "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces a novel adaptive ensemble framework that\nsynergistically combines XGBoost and neural networks through sophisticated\nmeta-learning. The proposed method leverages advanced uncertainty\nquantification techniques and feature importance integration to dynamically\norchestrate model selection and combination. Experimental results demonstrate\nsuperior predictive performance and enhanced interpretability across diverse\ndatasets, contributing to the development of more intelligent and flexible\nmachine learning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u7279\u5f81\u91cd\u8981\u6027\uff0c\u52a8\u6001\u7ed3\u5408XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u6742\u7684\u5143\u5b66\u4e60\uff0c\u534f\u540c\u7ed3\u5408XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u548c\u7279\u5f81\u91cd\u8981\u6027\u6574\u5408\u6765\u52a8\u6001\u8c03\u5ea6\u6a21\u578b\u9009\u62e9\u548c\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2510.04206", "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "AI": {"tldr": "AgentRL\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u667a\u80fd\u4f53\u53ef\u6269\u5c55\u591a\u8f6e\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6784\u5efa\u901a\u7528\u667a\u80fd\u4f53\u65b9\u9762\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5174\u8da3\uff0c\u4f46\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5e94\u7528\u4e8e\u591a\u8f6e\u3001\u591a\u4efb\u52a1LLM\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u4ecd\u9762\u4e34\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u548c\u7a33\u5b9a\u8bad\u7ec3\u7b97\u6cd5\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86AgentRL\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u591a\u8f6e\u3001\u591a\u4efb\u52a1\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u57fa\u7840\u8bbe\u65bd\u65b9\u9762\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u5b8c\u5168\u5f02\u6b65\u7684\u751f\u6210-\u8bad\u7ec3\u6d41\u6c34\u7ebf\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8e\u51fd\u6570\u8c03\u7528\u7684API\u63a5\u53e3\u3001\u5bb9\u5668\u5316\u73af\u5883\u5f00\u53d1\u548c\u96c6\u4e2d\u5f0f\u63a7\u5236\u5668\u3002\u7b97\u6cd5\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4ea4\u53c9\u7b56\u7565\u91c7\u6837\u4ee5\u9f13\u52b1\u6a21\u578b\u63a2\u7d22\u548c\u4efb\u52a1\u4f18\u52bf\u5f52\u4e00\u5316\u4ee5\u7a33\u5b9a\u591a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentRL\u5728\u4e94\u4e2a\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-5\u3001Claude-Sonnet-4\u3001DeepSeek-R1\u53ca\u5176\u4ed6\u5f00\u6e90LLM\u667a\u80fd\u4f53\u3002\u5176\u591a\u4efb\u52a1\u8bad\u7ec3\u7ed3\u679c\u53ef\u4e0e\u6240\u6709\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u6700\u4f73\u7ed3\u679c\u76f8\u5ab2\u7f8e\u3002AgentRL\u5df2\u5f00\u6e90\u3002", "conclusion": "AgentRL\u6846\u67b6\u53ca\u5176\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u3001\u591a\u4efb\u52a1RL\u8bad\u7ec3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.04124", "pdf": "https://arxiv.org/pdf/2510.04124", "abs": "https://arxiv.org/abs/2510.04124", "authors": ["Nuwan I. Senaratna"], "title": "Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)", "categories": ["cs.CL"], "comment": "4 pages", "summary": "We present a collection of open, machine-readable document datasets covering\nparliamentary proceedings, legal judgments, government publications, news, and\ntourism statistics from Sri Lanka. As of v20251005, the collection currently\ncomprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and\nEnglish. The datasets are updated daily and mirrored on GitHub and Hugging\nFace. These resources aim to support research in computational linguistics,\nlegal analytics, socio-political studies, and multilingual natural language\nprocessing. We describe the data sources, collection pipeline, formats, and\npotential use cases, while discussing licensing and ethical considerations.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u65af\u91cc\u5170\u5361\u591a\u9886\u57df\uff08\u8bae\u4f1a\u3001\u6cd5\u5f8b\u3001\u653f\u5e9c\u3001\u65b0\u95fb\u7b49\uff09\u7684\u5f00\u653e\u5f0f\u3001\u591a\u8bed\u8a00\uff08\u50e7\u4f3d\u7f57\u8bed\u3001\u6cf0\u7c73\u5c14\u8bed\u3001\u82f1\u8bed\uff09\u673a\u5668\u53ef\u8bfb\u6587\u6863\u6570\u636e\u96c6\u3002", "motivation": "\u65e8\u5728\u652f\u6301\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u6cd5\u5f8b\u5206\u6790\u3001\u793e\u4f1a\u653f\u6cbb\u7814\u7a76\u4ee5\u53ca\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u6536\u96c6\u5e76\u6574\u7406\u8bae\u4f1a\u8bb0\u5f55\u3001\u6cd5\u5f8b\u5224\u51b3\u3001\u653f\u5e9c\u51fa\u7248\u7269\u3001\u65b0\u95fb\u548c\u65c5\u6e38\u7edf\u8ba1\u6570\u636e\uff0c\u6bcf\u65e5\u66f4\u65b0\u5e76\u6258\u7ba1\u5728GitHub\u548cHugging Face\u4e0a\u3002", "result": "\u622a\u81f3v20251005\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b215,670\u4efd\u6587\u6863\uff0860.3 GB\uff09\uff0c\u6db5\u76d613\u4e2a\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u50e7\u4f3d\u7f57\u8bed\u3001\u6cf0\u7c73\u5c14\u8bed\u548c\u82f1\u8bed\u4e09\u79cd\u8bed\u8a00\u3002", "conclusion": "\u8fd9\u4e9b\u6570\u636e\u96c6\u4e3a\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u76f8\u5173\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u5728\u53d1\u5e03\u65f6\u8003\u8651\u4e86\u8bb8\u53ef\u548c\u4f26\u7406\u56e0\u7d20\u3002"}}
{"id": "2510.03747", "pdf": "https://arxiv.org/pdf/2510.03747", "abs": "https://arxiv.org/abs/2510.03747", "authors": ["Zuomin Qu", "Yimao Guo", "Qianyue Hu", "Wei Lu"], "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes", "categories": ["cs.CV"], "comment": null, "summary": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u73b0\u6709\u4e3b\u52a8\u5f0fDeepfake\u9632\u5fa1\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoRA patching\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7ed5\u8fc7\u8fd9\u4e9b\u9632\u5fa1\uff0c\u751a\u81f3\u80fd\u5d4c\u5165\u53ef\u89c1\u8b66\u544a\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9632\u5fa1\u7b56\u7565\u7684\u8106\u5f31\u6027\u3002", "motivation": "Deepfakes\u5e26\u6765\u663e\u8457\u7684\u793e\u4f1a\u98ce\u9669\uff0c\u4fc3\u4f7f\u4eba\u4eec\u5f00\u53d1\u4e3b\u52a8\u5f0f\u9632\u5fa1\u4ee5\u9632\u6b62\u56fe\u50cf\u7be1\u6539\u3002\u7136\u800c\uff0c\u73b0\u6709\u901a\u8fc7\u5728\u9762\u90e8\u56fe\u50cf\u4e2d\u5d4c\u5165\u5bf9\u6297\u6027\u6270\u52a8\u7684\u4e3b\u52a8\u5f0f\u9632\u5fa1\u901a\u5e38\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u8865\u4e01\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5373\u63d2\u5373\u7528LoRA\u8865\u4e01\u6ce8\u5165Deepfake\u751f\u6210\u5668\u6765\u7ed5\u8fc7\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u63a7\u5236LoRA\u8865\u4e01\u7684\u6548\u679c\u5e76\u9632\u6b62\u5fae\u8c03\u671f\u95f4\u7684\u68af\u5ea6\u7206\u70b8\u3002\u540c\u65f6\u5f15\u5165\u4e86\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff08MMFA\uff09\u635f\u5931\uff0c\u9f13\u52b1\u5bf9\u6297\u6027\u8f93\u51fa\u7684\u7279\u5f81\u5728\u8bed\u4e49\u5c42\u9762\u4e0e\u671f\u671b\u8f93\u51fa\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9632\u5fa1\u6027LoRA patching\uff0c\u53ef\u5728\u8f93\u51fa\u4e2d\u5d4c\u5165\u53ef\u89c1\u8b66\u544a\uff0c\u4ee5\u7f13\u89e3\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u4ec5\u4f7f\u75281,000\u4e2a\u9762\u90e8\u56fe\u50cf\u793a\u4f8b\u548c\u5355\u6b21\u5fae\u8c03\u8fed\u4ee3\uff0cLoRA patching\u6210\u529f\u51fb\u8d25\u4e86\u591a\u79cd\u4e3b\u52a8\u5f0f\u9632\u5fa1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dDeepfake\u9632\u5fa1\u8303\u5f0f\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684Deepfake\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.03302", "pdf": "https://arxiv.org/pdf/2510.03302", "abs": "https://arxiv.org/abs/2510.03302", "authors": ["Daiheng Gao", "Nanxiang Jiang", "Andi Zhang", "Shilin Lu", "Yufei Tang", "Wenbo Zhou", "Weiming Zhang", "Zhaoxin Fan"], "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "21 pages, 10 figures", "summary": "Concept erasure techniques have been widely deployed in T2I diffusion models\nto prevent inappropriate content generation for safety and copyright\nconsiderations. However, as models evolve to next-generation architectures like\nFlux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit\ndegraded effectiveness, raising questions about their true mechanisms. Through\nsystematic analysis, we reveal that concept erasure creates only an illusion of\n``amnesia\": rather than genuine forgetting, these methods bias sampling\ntrajectories away from target concepts, making the erasure fundamentally\nreversible. This insight motivates the need to distinguish superficial safety\nfrom genuine concept removal. In this work, we propose \\textbf{RevAm}\n(\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization\nframework that resurrects erased concepts by dynamically steering the denoising\nprocess without modifying model weights. By adapting Group Relative Policy\nOptimization (GRPO) to diffusion models, RevAm explores diverse recovery\ntrajectories through trajectory-level rewards, overcoming local optima that\nlimit existing methods. Extensive experiments demonstrate that RevAm achieves\nsuperior concept resurrection fidelity while reducing computational time by\n10$\\times$, exposing critical vulnerabilities in current safety mechanisms and\nunderscoring the need for more robust erasure techniques beyond trajectory\nmanipulation.", "AI": {"tldr": "\u73b0\u6709\u7684T2I\u6a21\u578b\u6982\u5ff5\u64e6\u9664\u6280\u672f\u5728\u65b0\u578b\u67b6\u6784\u4e2d\u5931\u6548\uff0c\u4e14\u4ec5\u5236\u9020\u201c\u9057\u5fd8\u201d\u7684\u5047\u8c61\uff0c\u64e6\u9664\u672c\u8d28\u4e0a\u53ef\u9006\u3002\u672c\u6587\u63d0\u51faRevAm\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u590d\u6d3b\u88ab\u201c\u64e6\u9664\u201d\u7684\u6982\u5ff5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\u3002", "motivation": "T2I\u6269\u6563\u6a21\u578b\u4e2d\u7528\u4e8e\u5b89\u5168\u548c\u7248\u6743\u7684\u6982\u5ff5\u64e6\u9664\u6280\u672f\u5728Flux\u7b49\u4e0b\u4e00\u4ee3\u67b6\u6784\u4e0a\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u672a\u771f\u6b63\u9057\u5fd8\uff0c\u4ec5\u662f\u504f\u79bb\u91c7\u6837\u8f68\u8ff9\uff0c\u4f7f\u5f97\u64e6\u9664\u53ef\u9006\u3002\u8fd9\u4fc3\u4f7f\u9700\u8981\u533a\u5206\u8868\u5c42\u5b89\u5168\u4e0e\u771f\u6b63\u7684\u6982\u5ff5\u79fb\u9664\u3002", "method": "\u63d0\u51faRevAm\uff08Revoking Amnesia\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u6765\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u4e14\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002\u5b83\u5c06\u7fa4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5956\u52b1\u63a2\u7d22\u591a\u6837\u5316\u7684\u6062\u590d\u8f68\u8ff9\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002", "result": "RevAm\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6982\u5ff5\u590d\u6d3b\u4fdd\u771f\u5ea6\uff0c\u5e76\u5c06\u8ba1\u7b97\u65f6\u95f4\u7f29\u77ed\u4e8610\u500d\u3002\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u7684\u6982\u5ff5\u64e6\u9664\u6280\u672f\u5e76\u672a\u5b9e\u73b0\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u800c\u4ec5\u662f\u8f68\u8ff9\u64cd\u7eb5\u9020\u6210\u7684\u201c\u9057\u5fd8\u201d\u5047\u8c61\uff0c\u5176\u64e6\u9664\u672c\u8d28\u4e0a\u662f\u53ef\u9006\u7684\u3002\u73b0\u6709\u7684\u5b89\u5168\u673a\u5236\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u3001\u8d85\u8d8a\u8f68\u8ff9\u64cd\u7eb5\u7684\u64e6\u9664\u6280\u672f\u3002"}}
{"id": "2510.04265", "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8d1d\u53f6\u65af\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u6a21\u578b\u6f5c\u5728\u6210\u529f\u6982\u7387\u7684\u540e\u9a8c\u4f30\u8ba1\u548c\u53ef\u4fe1\u533a\u95f4\u53d6\u4ee3Pass@k\u548c\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u53ef\u9760\u7684LLM\u63a8\u7406\u80fd\u529b\u6392\u540d\uff0c\u5c24\u5176\u5728\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "Pass@k\u548c\u5e73\u5747\u51c6\u786e\u7387\u5728\u8bc4\u4f30LLM\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u5c24\u5176\u5728\u8bd5\u9a8c\u6b21\u6570\u6709\u9650\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\uff0c\u5e38\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u8bef\u5bfc\u6027\u7684\u6392\u540d\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u8d1d\u53f6\u65af\u8bc4\u4f30\u6846\u67b6\uff1a\u5c06\u8bc4\u4f30\u7ed3\u679c\u5efa\u6a21\u4e3a\u5206\u7c7b\u53d8\u91cf\uff08\u975e\u4ec50/1\uff09\uff0c\u5e76\u4f7f\u7528Dirichlet\u5148\u9a8c\uff0c\u5f97\u5230\u540e\u9a8c\u5747\u503c\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u5c01\u95ed\u5f62\u5f0f\u8868\u8fbe\u5f0f\u3002\u5b83\u7528\u6a21\u578b\u6f5c\u5728\u6210\u529f\u6982\u7387\u7684\u540e\u9a8c\u4f30\u8ba1\u548c\u53ef\u4fe1\u533a\u95f4\u53d6\u4ee3\u4e86Pass@k\u548cN\u6b21\u8bd5\u9a8c\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5e76\u5141\u8bb8\u4f7f\u7528\u5148\u9a8c\u8bc1\u636e\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u5728\u5747\u5300\u5148\u9a8c\u4e0b\uff0c\u8d1d\u53f6\u65af\u540e\u9a8c\u5747\u503c\u4e0e\u5e73\u5747\u51c6\u786e\u7387\uff08Pass@1\uff09\u662f\u987a\u5e8f\u7b49\u4ef7\u7684\uff0c\u8fd9\u89e3\u91ca\u4e86\u5176\u7ecf\u9a8c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8d1d\u53f6\u65af/\u5e73\u5747\u7a0b\u5e8f\u6bd4Pass@k\u53ca\u5176\u53d8\u4f53\u6536\u655b\u66f4\u5feb\uff0c\u6392\u540d\u7a33\u5b9a\u6027\u66f4\u9ad8\uff0c\u80fd\u5728\u66f4\u5c0f\u7684\u6837\u672c\u91cf\u4e0b\u8fdb\u884c\u53ef\u9760\u6bd4\u8f83\u3002\u8be5\u6846\u67b6\u80fd\u533a\u5206\u7edf\u8ba1\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u5dee\u5f02\uff08\u975e\u91cd\u53e0\u53ef\u4fe1\u533a\u95f4\uff09\u4e0e\u566a\u97f3\uff0c\u5e76\u81ea\u7136\u6269\u5c55\u5230\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5efa\u8bae\u7528\u8fd9\u79cd\u57fa\u4e8e\u540e\u9a8c\u7684\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u534f\u8bae\u53d6\u4ee3Pass@k\u8fdb\u884cLLM\u8bc4\u4f30\u548c\u6392\u540d\uff0c\u8be5\u534f\u8bae\u7edf\u4e00\u4e86\u4e8c\u5143\u548c\u975e\u4e8c\u5143\u8bc4\u4f30\uff0c\u5e76\u660e\u786e\u4e86\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.04139", "pdf": "https://arxiv.org/pdf/2510.04139", "abs": "https://arxiv.org/abs/2510.04139", "authors": ["Tim Bakkenes", "Daniel Wang", "Anton Johansson"], "title": "Fine Tuning Methods for Low-resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rise of Large Language Models has not been inclusive of all cultures. The\nmodels are mostly trained on English texts and culture which makes them\nunderperform in other languages and cultural contexts. By developing a\ngeneralizable method for preparing culturally relevant datasets and\npost-training the Gemma 2 model, this project aimed to increase the performance\nof Gemma 2 for an underrepresented language and showcase how others can do the\nsame to unlock the power of Generative AI in their country and preserve their\ncultural heritage.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u8be5\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u901a\u7528\u65b9\u6cd5\u51c6\u5907\u6587\u5316\u76f8\u5173\u6570\u636e\u96c6\u5e76\u5bf9Gemma 2\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7684\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u82f1\u6587\u6587\u672c\u548c\u6587\u5316\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5b83\u4eec\u5728\u5176\u4ed6\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u672a\u80fd\u666e\u60e0\u6240\u6709\u6587\u5316\u3002", "method": "\u5f00\u53d1\u4e00\u79cd\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u6765\u51c6\u5907\u6587\u5316\u76f8\u5173\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Gemma 2\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u65e8\u5728\u63d0\u9ad8Gemma 2\u5728\u4e00\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u751f\u6210\u5f0fAI\u7684\u6f5c\u529b\u5e94\u7528\u4e8e\u4e0d\u540c\u56fd\u5bb6\uff0c\u4ee5\u89e3\u9501\u5176\u529b\u91cf\u5e76\u4fdd\u62a4\u6587\u5316\u9057\u4ea7\u3002"}}
{"id": "2510.03751", "pdf": "https://arxiv.org/pdf/2510.03751", "abs": "https://arxiv.org/abs/2510.03751", "authors": ["Mubariz Zaffar", "Liangliang Nan", "Sebastian Scherer", "Julian F. P. Kooij"], "title": "The Overlooked Value of Test-time Reference Sets in Visual Place Recognition", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025 Workshop CrocoDL", "summary": "Given a query image, Visual Place Recognition (VPR) is the task of retrieving\nan image of the same place from a reference database with robustness to\nviewpoint and appearance changes. Recent works show that some VPR benchmarks\nare solved by methods using Vision-Foundation-Model backbones and trained on\nlarge-scale and diverse VPR-specific datasets. Several benchmarks remain\nchallenging, particularly when the test environments differ significantly from\nthe usual VPR training datasets. We propose a complementary, unexplored source\nof information to bridge the train-test domain gap, which can further improve\nthe performance of State-of-the-Art (SOTA) VPR methods on such challenging\nbenchmarks. Concretely, we identify that the test-time reference set, the\n\"map\", contains images and poses of the target domain, and must be available\nbefore the test-time query is received in several VPR applications. Therefore,\nwe propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on\nthe map, boosting the SOTA (~2.3% increase on average for Recall@1) on these\nchallenging datasets. Finetuned models retain generalization, and RSF works\nacross diverse test datasets.", "AI": {"tldr": "\u9488\u5bf9VPR\u5728\u8bad\u7ec3-\u6d4b\u8bd5\u57df\u5dee\u8ddd\u5927\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u5229\u7528\u6d4b\u8bd5\u65f6\u7684\u53c2\u8003\u96c6\uff08\u201c\u5730\u56fe\u201d\uff09\u5bf9VPR\u6a21\u578b\u8fdb\u884c\u7b80\u5355\u5fae\u8c03\uff08RSF\uff09\uff0c\u5e73\u5747\u63d0\u5347SOTA\u6a21\u578b\u5728Recall@1\u4e0a\u7ea62.3%\uff0c\u5e76\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53VPR\u65b9\u6cd5\u7684\u6d4b\u8bd5\u73af\u5883\u4e0e\u5e38\u89c4\u8bad\u7ec3\u6570\u636e\u96c6\u663e\u8457\u4e0d\u540c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u660e\u663e\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u57df\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u96be\u4ee5\u53d6\u5f97\u826f\u597d\u6027\u80fd\u3002", "method": "\u63d0\u51faReference-Set-Finetuning (RSF) \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6d4b\u8bd5\u65f6\u53ef\u7528\u7684\u53c2\u8003\u96c6\uff08\u5373\u201c\u5730\u56fe\u201d\uff0c\u5176\u4e2d\u5305\u542b\u76ee\u6807\u57df\u7684\u56fe\u50cf\u548c\u59ff\u6001\u4fe1\u606f\uff09\u5bf9VPR\u6a21\u578b\u8fdb\u884c\u7b80\u5355\u7684\u5fae\u8c03\uff0c\u4ee5\u9002\u5e94\u76ee\u6807\u57df\u3002", "result": "RSF\u65b9\u6cd5\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u5c06SOTA VPR\u6a21\u578b\u7684Recall@1\u5e73\u5747\u63d0\u9ad8\u4e86\u7ea62.3%\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4fdd\u6301\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14RSF\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u4e0d\u540c\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6d4b\u8bd5\u65f6\u7684\u53c2\u8003\u96c6\u8fdb\u884c\u7b80\u5355\u7684\u5fae\u8c03\uff08RSF\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u5f25\u5408VPR\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u57df\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347SOTA\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03305", "pdf": "https://arxiv.org/pdf/2510.03305", "abs": "https://arxiv.org/abs/2510.03305", "authors": ["Tian Zheng", "Subashree Venkatasubramanian", "Shuolin Li", "Amy Braverman", "Xinyi Ke", "Zhewen Hou", "Peter Jin", "Samarth Sanjay Agrawal"], "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies", "categories": ["cs.LG", "physics.ao-ph", "stat.AP", "stat.ML", "62P12 62p12"], "comment": "Supplement", "summary": "Machine learning has been increasingly applied in climate modeling on system\nemulation acceleration, data-driven parameter inference, forecasting, and\nknowledge discovery, addressing challenges such as physical consistency,\nmulti-scale coupling, data sparsity, robust generalization, and integration\nwith scientific workflows. This paper analyzes a series of case studies from\napplied machine learning research in climate modeling, with a focus on design\nchoices and workflow structure. Rather than reviewing technical details, we aim\nto synthesize workflow design patterns across diverse projects in ML-enabled\nclimate modeling: from surrogate modeling, ML parameterization, probabilistic\nprogramming, to simulation-based inference, and physics-informed transfer\nlearning. We unpack how these workflows are grounded in physical knowledge,\ninformed by simulation data, and designed to integrate observations. We aim to\noffer a framework for ensuring rigor in scientific machine learning through\nmore transparent model development, critical evaluation, informed adaptation,\nand reproducibility, and to contribute to lowering the barrier for\ninterdisciplinary collaboration at the interface of data science and climate\nmodeling.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u65e8\u5728\u7efc\u5408ML\u9a71\u52a8\u7684\u6c14\u5019\u5efa\u6a21\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u786e\u4fdd\u79d1\u5b66ML\u4e25\u8c28\u6027\u548c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u6846\u67b6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7cfb\u7edf\u4eff\u771f\u52a0\u901f\u3001\u6570\u636e\u9a71\u52a8\u53c2\u6570\u63a8\u65ad\u7b49\uff0c\u4ee5\u89e3\u51b3\u7269\u7406\u4e00\u81f4\u6027\u3001\u591a\u5c3a\u5ea6\u8026\u5408\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u6df1\u5165\u5206\u6790ML\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u8bbe\u8ba1\u9009\u62e9\u548c\u5de5\u4f5c\u6d41\u7ed3\u6784\uff0c\u4ee5\u7efc\u5408\u5176\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u8fdb\u800c\u63d0\u5347\u79d1\u5b66ML\u7684\u4e25\u8c28\u6027\u3001\u900f\u660e\u5ea6\u4e0e\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u964d\u4f4e\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u95e8\u69db\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u6c14\u5019\u5efa\u6a21\u4e2d\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7814\u7a76\u7684\u4e00\u7cfb\u5217\u6848\u4f8b\uff0c\u91cd\u70b9\u5173\u6ce8\u8bbe\u8ba1\u9009\u62e9\u548c\u5de5\u4f5c\u6d41\u7ed3\u6784\u3002\u7814\u7a76\u65b9\u6cd5\u662f\u7efc\u5408ML\u9a71\u52a8\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5305\u62ec\u4ee3\u7406\u5efa\u6a21\u3001ML\u53c2\u6570\u5316\u3001\u6982\u7387\u7f16\u7a0b\u3001\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\u548c\u7269\u7406\u4fe1\u606f\u8fc1\u79fb\u5b66\u4e60\u7b49\uff0c\u5e76\u9610\u660e\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u5982\u4f55\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u3001\u6a21\u62df\u6570\u636e\u5e76\u6574\u5408\u89c2\u6d4b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u65e8\u5728\u7efc\u5408ML\u9a71\u52a8\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5404\u7c7b\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u66f4\u900f\u660e\u7684\u6a21\u578b\u5f00\u53d1\u3001\u6279\u5224\u6027\u8bc4\u4f30\u3001\u77e5\u60c5\u9002\u5e94\u548c\u53ef\u590d\u73b0\u6027\u6765\u786e\u4fdd\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u4e25\u8c28\u6027\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u786e\u4fdd\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e25\u8c28\u6027\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4fc3\u8fdb\u900f\u660e\u7684\u6a21\u578b\u5f00\u53d1\u3001\u6279\u5224\u6027\u8bc4\u4f30\u3001\u77e5\u60c5\u9002\u5e94\u548c\u53ef\u590d\u73b0\u6027\uff0c\u964d\u4f4e\u6570\u636e\u79d1\u5b66\u4e0e\u6c14\u5019\u5efa\u6a21\u4ea4\u53c9\u9886\u57df\u8fdb\u884c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u969c\u788d\u3002"}}
{"id": "2510.04272", "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u5546\u4e1a\u73af\u5883\u4e2d\u8de8\u804c\u80fd\uff08\u5982\u5e93\u5b58\u8865\u8d27\u4e0e\u4ea7\u54c1\u63a8\u8350\uff09\u7684\u534f\u8c03\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f01\u4e1a\u76c8\u5229\u80fd\u529b\u3002", "motivation": "\u5728\u7ec4\u7ec7\u590d\u6742\u6027\u548c\u89c4\u6a21\u65e5\u76ca\u589e\u957f\u7684\u80cc\u666f\u4e0b\uff0c\u6709\u6548\u7684\u8de8\u804c\u80fd\u534f\u8c03\u5bf9\u4e8e\u63d0\u5347\u4f01\u4e1a\u6574\u4f53\u76c8\u5229\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u4eba\u5de5\u667a\u80fd\uff0c\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53RL\u6846\u67b6\uff0c\u65e8\u5728\u8054\u5408\u4f18\u5316\u4e0d\u540c\u804c\u80fd\u6a21\u5757\uff08\u4f8b\u5982\u5e93\u5b58\u8865\u8d27\u548c\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\uff09\u3002\u9996\u5148\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u7406\u8bba\u6a21\u578b\u4ee5\u6355\u83b7\u529f\u80fd\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5e76\u63a8\u5bfc\u5206\u6790\u57fa\u51c6\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u65f6\u95f4\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53RL\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6839\u636e\u90e8\u95e8\u529f\u80fd\u5206\u89e3\u7b56\u7565\u7ec4\u4ef6\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u548c\u54cd\u5e94\u6027\u5206\u914d\u4e0d\u540c\u7684\u5b66\u4e60\u901f\u5ea6\u3002\u91c7\u7528\u65e0\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u65f6\u95f4\u5c3a\u5ea6\u66f4\u65b0\u589e\u5f3a\u6536\u655b\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u7b97\u6cd5\u7684\u6e10\u8fd1\u6536\u655b\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u4ea7\u54c1\u548c\u65f6\u95f4\u4e0a\u7684\u540c\u6b65\u8c03\u6574\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4e86\u534f\u8c03\u51b3\u7b56\u7684\u91cd\u8981\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53RL\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5b64\u7acb\u51b3\u7b56\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u76c8\u5229\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u51fa\u7684RL\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u4e0e\u7406\u8bba\u6a21\u578b\u7684\u7ba1\u7406\u6d1e\u5bdf\u9ad8\u5ea6\u4e00\u81f4\u3002\u8be5\u6a21\u578b\u8fd8\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3001\u90e8\u7f72\u7075\u6d3b\u6027\u4ee5\u53ca\u5bf9\u5f02\u6784\u51b3\u7b56\u7684\u6536\u655b\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u8de8\u804c\u80fd\u534f\u8c03\uff0c\u4ece\u800c\u4f18\u5316\u4f01\u4e1a\u7ee9\u6548\u3002"}}
{"id": "2510.04147", "pdf": "https://arxiv.org/pdf/2510.04147", "abs": "https://arxiv.org/abs/2510.04147", "authors": ["Yifeng Gao", "Ziang Ji", "Yuxuan Wang", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Self Speculative Decoding for Diffusion Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based Large Language Models (dLLMs) have emerged as a competitive\nalternative to autoregressive models, offering unique advantages through\nbidirectional attention and parallel generation paradigms. However, the\ngeneration results of current parallel decoding methods deviate from stepwise\ndecoding, introducing potential performance degradation, which limits their\npractical deployment. To address this problem, we propose \\textbf{S}elf\n\\textbf{S}peculative \\textbf{D}ecoding (SSD), a lossless inference acceleration\nmethod that leverages the dLLM itself as both speculative decoding drafter and\nverifier without auxiliary modules. SSD introduces a self-drafting mechanism\nwhere the model generates predictions for multiple positions, then verifies\nthem through hierarchical verification trees in a single forward pass. Unlike\ntraditional speculative decoding that requires separate draft models, SSD\neliminates model redundancy and memory overhead by exploiting the dLLM's\ninherent parallel prediction capability for multiple positions. This\nself-speculative approach allows the model to progressively verify and accept\nmultiple tokens in a single forward pass. Our experiments demonstrate that SSD\nachieves up to 3.46$\\times$ speedup while keeping the output identical to\nstepwise decoding on open source models such as LLaDA and Dream. Code will be\nmade publicly available on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u63a8\u6d4b\u89e3\u7801\uff08SSD\uff09\uff0c\u4e00\u79cd\u65e0\u635f\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u81ea\u8eab\u4f5c\u4e3a\u8349\u7a3f\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.46\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9010\u6b65\u89e3\u7801\u76f8\u540c\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7684dLLM\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\u4e0e\u9010\u6b65\u89e3\u7801\u5b58\u5728\u504f\u5dee\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "SSD\u901a\u8fc7\u5f15\u5165\u81ea\u8349\u7a3f\u673a\u5236\uff0c\u4f7fdLLM\u5728\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u591a\u4e2a\u4f4d\u7f6e\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u9a8c\u8bc1\u6811\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u5229\u7528dLLM\u56fa\u6709\u7684\u5e76\u884c\u9884\u6d4b\u80fd\u529b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u6240\u9700\u7684\u8f85\u52a9\u6a21\u5757\u548c\u989d\u5916\u7684\u8349\u7a3f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728LLaDA\u548cDream\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\uff0cSSD\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.46\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u4e14\u8f93\u51fa\u4e0e\u9010\u6b65\u89e3\u7801\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "SSD\u901a\u8fc7\u65e0\u635f\u52a0\u901f\u89e3\u51b3\u4e86dLLM\u5e76\u884c\u89e3\u7801\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u63a8\u7406\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03763", "pdf": "https://arxiv.org/pdf/2510.03763", "abs": "https://arxiv.org/abs/2510.03763", "authors": ["Jiaxin Deng", "Junbiao Pang"], "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles\nthe computational cost of Stochastic Gradient Descent (SGD) by requiring twice\nthe gradient calculations per optimization step. To mitigate this, we propose\nAdaptively sampling-Reusing-mixing decomposed gradients to significantly\naccelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can\nbe decomposed into the SGD gradient and the Projection of the Second-order\ngradient onto the First-order gradient (PSF). Furthermore, we observe that the\nSGD gradient and PSF dynamically evolve during training, emphasizing the\ngrowing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed\nto the reused PSF and the timely updated PSF still maintain the model's\ngeneralization ability. Extensive experiments show that ARSAM achieves\nstate-of-the-art accuracies comparable to SAM across diverse network\narchitectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a\nspeedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various\nchallenge tasks (\\textit{e.g.}, human pose estimation, and model quantization)\nwithout sacrificing performance, demonstrating its broad practicality.% The\ncode is publicly accessible at: https://github.com/ajiaaa/ARSAM.", "AI": {"tldr": "SAM\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u672c\u6587\u63d0\u51faARSAM\uff0c\u901a\u8fc7\u5206\u89e3SAM\u68af\u5ea6\u5e76\u81ea\u9002\u5e94\u5730\u590d\u7528\u5176\u4e2d\u7684\u4e8c\u9636\u68af\u5ea6\u6295\u5f71\uff08PSF\uff09\uff0c\u663e\u8457\u52a0\u901fSAM\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eSAM\u76f8\u5f53\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Sharpness-Aware Minimization (SAM) \u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u662f\u6807\u51c6\u968f\u673a\u68af\u5ea6\u4e0b\u964d (SGD) \u7684\u4e24\u500d\uff0c\u56e0\u4e3a\u5b83\u5728\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u4e2d\u9700\u8981\u4e24\u6b21\u68af\u5ea6\u8ba1\u7b97\u3002", "method": "ARSAM \u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\uff1a1) \u53d1\u73b0 SAM \u7684\u68af\u5ea6\u53ef\u4ee5\u5206\u89e3\u4e3a SGD \u68af\u5ea6\u548c\u4e00\u9636\u68af\u5ea6\u6295\u5f71\u7684\u4e8c\u9636\u68af\u5ea6 (PSF)\u30022) \u89c2\u5bdf\u5230 SGD \u68af\u5ea6\u548c PSF \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6f14\u53d8\uff0cPSF \u5728\u5b9e\u73b0\u5e73\u5766\u6700\u5c0f\u503c\u4e2d\u8d77\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\u30023) ARSAM \u65e8\u5728\u590d\u7528 PSF \u5e76\u53ca\u65f6\u66f4\u65b0\uff0c\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u52a0\u901f\u8ba1\u7b97\u3002", "result": "ARSAM \u5728\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u4e0e SAM \u76f8\u5f53\u7684\u6700\u65b0\u51c6\u786e\u5ea6\u3002\u5728 CIFAR-10/100 \u6570\u636e\u96c6\u4e0a\uff0cARSAM \u7684\u6027\u80fd\u4e0e SAM \u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7ea6 40% \u7684\u52a0\u901f\u3002\u6b64\u5916\uff0cARSAM \u5728\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6a21\u578b\u91cf\u5316\u7b49\u5404\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u52a0\u901f\u4e86\u4f18\u5316\uff0c\u4e14\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ARSAM \u6210\u529f\u5730\u89e3\u51b3\u4e86 SAM \u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u5206\u89e3\u548c\u81ea\u9002\u5e94\u590d\u7528\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u5339\u914d SAM \u4f18\u79c0\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u901f\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.03309", "pdf": "https://arxiv.org/pdf/2510.03309", "abs": "https://arxiv.org/abs/2510.03309", "authors": ["Mallikarjuna Tupakula"], "title": "Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Multimodal foundation models hold promise for drug discovery and biomedical\napplications, but most existing approaches rely on heavy pretraining or large\nscale multimodal corpora. We investigate whether thin contrastive bridges,\nlightweight projection heads over frozen unimodal encoders can align chemical\nand textual representations without training a full multimodal model. Using\npaired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with\nbiomedical sentence embeddings through dual linear projections trained with a\ncontrastive objective. To better handle drugs sharing the same therapeutic\ntarget, we incorporate hard negative weighting and a margin loss. Evaluation\nunder scaffold based splits, which require generalization across disjoint\nchemical cores, demonstrates that our approach achieves non-trivial cross modal\nalignment and substantially improves within target discrimination compared to\nfrozen baselines. These results suggest that thin bridges offer a compute\nefficient alternative to large scale multimodal pretraining, enabling scaffold\naware drug text alignment and target specific retrieval in precision medicine.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u5bf9\u6bd4\u6865\u63a5\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u51bb\u7ed3\u5355\u6a21\u6001\u7f16\u7801\u5668\u548c\u6295\u5f71\u5934\u5bf9\u9f50\u5316\u5b66\u4e0e\u6587\u672c\u8868\u793a\uff0c\u5b9e\u73b0\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u8ba1\u7b97\u9ad8\u6548\u5bf9\u9f50\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7e41\u91cd\u7684\u9884\u8bad\u7ec3\u6216\u5927\u89c4\u6a21\u591a\u6a21\u6001\u8bed\u6599\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u5728\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u4e0a\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u5934\uff08\u201c\u8584\u578b\u5bf9\u6bd4\u6865\u63a5\u201d\uff09\uff0c\u5c06ECFP4\u5206\u5b50\u6307\u7eb9\u4e0e\u751f\u7269\u533b\u5b66\u53e5\u5b50\u5d4c\u5165\u5bf9\u9f50\u3002\u65b9\u6cd5\u5229\u7528ChEMBL\u7684\u914d\u5bf9\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u7ebf\u6027\u6295\u5f71\u548c\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u3002\u4e3a\u5904\u7406\u5171\u4eab\u6cbb\u7597\u9776\u70b9\u7684\u836f\u7269\uff0c\u5f15\u5165\u4e86\u96be\u8d1f\u6837\u672c\u52a0\u6743\u548c\u8fb9\u754c\u635f\u5931\u3002", "result": "\u5728\u8981\u6c42\u8de8\u4e0d\u76f8\u4ea4\u5316\u5b66\u6838\u5fc3\u6cdb\u5316\u7684\u9aa8\u67b6\u5206\u5272\u8bc4\u4f30\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u76f8\u5bf9\u4e8e\u51bb\u7ed3\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u9776\u70b9\u5185\u90e8\u7684\u9274\u522b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8584\u578b\u6865\u63a5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u66ff\u4ee3\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u5b9e\u73b0\u9aa8\u67b6\u611f\u77e5\u7684\u836f\u7269-\u6587\u672c\u5bf9\u9f50\u548c\u7cbe\u51c6\u533b\u7597\u4e2d\u7684\u9776\u70b9\u7279\u5f02\u6027\u68c0\u7d22\u3002"}}
{"id": "2510.04281", "pdf": "https://arxiv.org/pdf/2510.04281", "abs": "https://arxiv.org/abs/2510.04281", "authors": ["Zhuangzhi Gao", "Hongyi Qin", "He Zhao", "Qinkai Yu", "Feixiang Zhou", "Eduard Shantsila", "Uazman Alam", "Alena Shantsila", "Wahbi El-Bouri", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "categories": ["cs.AI"], "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and\n  Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)", "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse\ndata modalities, but current medical adaptations such as LLaVA-Med often fail\nto fully exploit the synergy between color fundus photography (CFP) and optical\ncoherence tomography (OCT), and offer limited interpretability of quantitative\nbiomarkers. We introduce GROK, a grounded multimodal large language model that\njointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of\nocular and systemic disease. GROK comprises three core modules:\nKnowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,\nand Supervised Instruction Fine-Tuning, which together establish a\nquantitative-to-qualitative diagnostic chain of thought, mirroring real\nclinical reasoning when producing detailed lesion annotations. To evaluate our\napproach, we introduce the Grounded Ophthalmic Understanding benchmark, which\ncovers six disease categories and three tasks: macro-level diagnostic\nclassification, report generation quality, and fine-grained clinical assessment\nof the generated chain of thought. Experiments show that, with only LoRA\n(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK\noutperforms comparable 7B and 32B baselines on both report quality and\nfine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are\npublicly available in the GROK repository.", "AI": {"tldr": "GROK\u662f\u4e00\u4e2a\u63a5\u5730\u6c14\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u8054\u5408\u5904\u7406\u773c\u5e95\u5f69\u7167\u3001OCT\u548c\u6587\u672c\uff0c\u63d0\u4f9b\u4e34\u5e8a\u7ea7\u522b\u7684\u773c\u79d1\u53ca\u5168\u8eab\u6027\u75be\u75c5\u8bca\u65ad\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u9886\u57df\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u773c\u5e95\u5f69\u7167\uff08CFP\uff09\u548c\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\uff08OCT\uff09\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u4e14\u5bf9\u91cf\u5316\u751f\u7269\u6807\u5fd7\u7269\u7684\u89e3\u91ca\u6027\u6709\u9650\u3002", "method": "\u5f15\u5165GROK\u6a21\u578b\uff0c\u4e00\u4e2a\u63a5\u5730\u6c14\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u8054\u5408\u5904\u7406CFP\u3001OCT\u548c\u6587\u672c\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u77e5\u8bc6\u5f15\u5bfc\u6307\u4ee4\u751f\u6210\u3001CLIP\u98ce\u683cOCT\u751f\u7269\u6807\u5fd7\u7269\u5bf9\u9f50\u548c\u76d1\u7763\u6307\u4ee4\u5fae\u8c03\uff0c\u5171\u540c\u5efa\u7acb\u4ece\u5b9a\u91cf\u5230\u5b9a\u6027\u7684\u8bca\u65ad\u601d\u7ef4\u94fe\uff0c\u6a21\u62df\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u3002\u4e3a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86Grounded Ophthalmic Understanding\u57fa\u51c6\uff0c\u6db5\u76d6\u516d\u7c7b\u75be\u75c5\u548c\u4e09\u4e2a\u4efb\u52a1\u3002", "result": "\u4ec5\u4f7f\u75287B\u53c2\u6570\u7684Qwen2\u9aa8\u5e72\u7f51\u7edc\u5e76\u7ed3\u5408LoRA\u5fae\u8c03\uff0cGROK\u5728\u62a5\u544a\u8d28\u91cf\u548c\u7cbe\u7ec6\u4e34\u5e8a\u6307\u6807\u4e0a\u5747\u4f18\u4e8e7B\u548c32B\u7684\u540c\u7c7b\u57fa\u7ebf\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86OpenAI o3\u3002", "conclusion": "GROK\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u63a8\u7406\u94fe\uff0c\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u773c\u79d1\u5f71\u50cf\u548c\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u6a21\u578b\u3002"}}
{"id": "2510.04182", "pdf": "https://arxiv.org/pdf/2510.04182", "abs": "https://arxiv.org/abs/2510.04182", "authors": ["Wengao Ye", "Yan Liang", "Lianlei Shan"], "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shifted from\nexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,\nwhere intermediate thoughts are represented as vectors rather than text.\nHowever, latent reasoning can be brittle on challenging, out-of-distribution\ntasks where robust reasoning is most critical. To overcome these limitations,\nwe introduce Latent Thought Policy Optimization (LTPO), a parameter-free\nframework that enhances LLM reasoning entirely at test time, without requiring\nmodel parameter updates. LTPO treats intermediate latent \"thought\" vectors as\ndynamic parameters that are actively optimized for each problem instance. It\nemploys an online policy gradient method guided by an intrinsic,\nconfidence-based reward signal computed directly from the frozen LLM's own\noutput distributions, eliminating the need for external supervision or\nexpensive text generation during optimization. Extensive experiments on five\nreasoning benchmarks show that LTPO not only matches or surpasses strong\nbaselines on standard tasks but also demonstrates remarkable robustness where\nothers fail. Most notably, on highly challenging AIME benchmarks where existing\nlatent reasoning baselines collapse to near-zero accuracy, LTPO delivers\nsubstantial improvements, showcasing a unique capability for complex reasoning.", "AI": {"tldr": "\u63d0\u51faLTPO\u6846\u67b6\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4f18\u5316LLM\u7684\u6f5c\u5728\u201c\u601d\u7ef4\u201d\u5411\u91cf\u5e76\u5229\u7528\u5185\u5728\u7f6e\u4fe1\u5ea6\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u548c\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u5728\u63a8\u7406\u80fd\u529b\uff08\u5c06\u4e2d\u95f4\u601d\u60f3\u8868\u793a\u4e3a\u5411\u91cf\uff09\u5728\u9762\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u3001\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u800c\u8fd9\u4e9b\u573a\u666f\u6700\u9700\u8981\u9c81\u68d2\u7684\u63a8\u7406\u3002", "method": "\u5f15\u5165Latent Thought Policy Optimization (LTPO)\uff0c\u4e00\u4e2a\u53c2\u6570\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5728\u6d4b\u8bd5\u65f6\u4f18\u5316\u6bcf\u4e2a\u95ee\u9898\u5b9e\u4f8b\u7684\u4e2d\u95f4\u6f5c\u5728\u201c\u601d\u7ef4\u201d\u5411\u91cf\u3002\u5b83\u91c7\u7528\u5728\u7ebf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5e76\u7531\u4ece\u51bb\u7ed3LLM\u81ea\u8eab\u8f93\u51fa\u5206\u5e03\u8ba1\u7b97\u51fa\u7684\u5185\u5728\u3001\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u6587\u672c\u751f\u6210\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLTPO\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\uff0c\u5e76\u5728\u5176\u4ed6\u65b9\u6cd5\u5931\u8d25\u65f6\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002\u5c24\u5176\u5728\u6781\u5177\u6311\u6218\u6027\u7684AIME\u57fa\u51c6\u4e0a\uff0c\u5f53\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u57fa\u7ebf\u51c6\u786e\u7387\u63a5\u8fd1\u4e8e\u96f6\u65f6\uff0cLTPO\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LTPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u72ec\u7279\u4e14\u65e0\u9700\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u7684\u80fd\u529b\uff0c\u80fd\u6709\u6548\u589e\u5f3aLLM\u5728\u590d\u6742\u548cOOD\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.03767", "pdf": "https://arxiv.org/pdf/2510.03767", "abs": "https://arxiv.org/abs/2510.03767", "authors": ["Yiheng Dong", "Yi Lin", "Xin Yang"], "title": "CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis", "categories": ["cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "The transparency of deep learning models is essential for clinical\ndiagnostics. Concept Bottleneck Model provides clear decision-making processes\nfor diagnosis by transforming the latent space of black-box models into\nhuman-understandable concepts. However, concept-based methods still face\nchallenges in concept capture capabilities. These methods often rely on encode\nfeatures solely from the final layer, neglecting shallow and multiscale\nfeatures, and lack effective guidance in concept encoding, hindering\nfine-grained concept extraction. To address these issues, we introduce Concept\nPrompting and Aggregating (CoPA), a novel framework designed to capture\nmultilayer concepts under prompt guidance. This framework utilizes the\nConcept-aware Embedding Generator (CEG) to extract concept representations from\neach layer of the visual encoder. Simultaneously, these representations serve\nas prompts for Concept Prompt Tuning (CPT), steering the model towards\namplifying critical concept-related visual cues. Visual representations from\neach layer are aggregated to align with textual concept representations. With\nthe proposed method, valuable concept-wise information in the images is\ncaptured and utilized effectively, thus improving the performance of concept\nand disease prediction. Extensive experimental results demonstrate that CoPA\noutperforms state-of-the-art methods on three public datasets. Code is\navailable at https://github.com/yihengd/CoPA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoPA\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u548c\u591a\u5c42\u6982\u5ff5\u805a\u5408\uff0c\u63d0\u5347\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u6982\u5ff5\u6355\u83b7\u80fd\u529b\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u9700\u63d0\u9ad8\u900f\u660e\u5ea6\u3002\u73b0\u6709\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u5728\u6982\u5ff5\u6355\u83b7\u4e0a\u5b58\u5728\u5c40\u9650\uff1a\u4ec5\u4f9d\u8d56\u6700\u7ec8\u5c42\u7279\u5f81\uff0c\u5ffd\u7565\u6d45\u5c42\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u6982\u5ff5\u7f16\u7801\u6307\u5bfc\uff0c\u963b\u788d\u7ec6\u7c92\u5ea6\u6982\u5ff5\u63d0\u53d6\u3002", "method": "\u5f15\u5165CoPA\uff08Concept Prompting and Aggregating\uff09\u6846\u67b6\uff1a1. \u4f7f\u7528\u6982\u5ff5\u611f\u77e5\u5d4c\u5165\u751f\u6210\u5668\uff08CEG\uff09\u4ece\u89c6\u89c9\u7f16\u7801\u5668\u5404\u5c42\u63d0\u53d6\u6982\u5ff5\u8868\u793a\u30022. \u8fd9\u4e9b\u8868\u793a\u4f5c\u4e3a\u6982\u5ff5\u63d0\u793a\u8c03\u4f18\uff08CPT\uff09\u7684\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u653e\u5927\u5173\u952e\u6982\u5ff5\u89c6\u89c9\u7ebf\u7d22\u30023. \u805a\u5408\u5404\u5c42\u89c6\u89c9\u8868\u793a\u4ee5\u4e0e\u6587\u672c\u6982\u5ff5\u8868\u793a\u5bf9\u9f50\u3002", "result": "CoPA\u6709\u6548\u6355\u83b7\u5e76\u5229\u7528\u4e86\u56fe\u50cf\u4e2d\u5b9d\u8d35\u7684\u6982\u5ff5\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u548c\u75be\u75c5\u9884\u6d4b\u7684\u6027\u80fd\u3002\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoPA\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CoPA\u901a\u8fc7\u5176\u591a\u5c42\u6982\u5ff5\u6355\u83b7\u548c\u63d0\u793a\u5f15\u5bfc\u805a\u5408\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5728\u6982\u5ff5\u6355\u83b7\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u8bca\u65ad\u4e2d\u6982\u5ff5\u548c\u75be\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03310", "pdf": "https://arxiv.org/pdf/2510.03310", "abs": "https://arxiv.org/abs/2510.03310", "authors": ["Runze Zhang", "Xiaowei Zhang", "Mingyang Zhao"], "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "LLMs are emerging tools for simulating human behavior in business, economics,\nand social science, offering a lower-cost complement to laboratory experiments,\nfield studies, and surveys. This paper evaluates how well LLMs replicate human\nbehavior in operations management. Using nine published experiments in\nbehavioral operations, we assess two criteria: replication of hypothesis-test\noutcomes and distributional alignment via Wasserstein distance. LLMs reproduce\nmost hypothesis-level effects, capturing key decision biases, but their\nresponse distributions diverge from human data, including for strong commercial\nmodels. We also test two lightweight interventions -- chain-of-thought\nprompting and hyperparameter tuning -- which reduce misalignment and can\nsometimes let smaller or open-source models match or surpass larger systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLM\u80fd\u590d\u73b0\u5927\u591a\u6570\u5047\u8bbe\u5c42\u9762\u7684\u6548\u5e94\u4f46\u5206\u5e03\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u901a\u8fc7CoT\u63d0\u793a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u53ef\u6709\u6548\u51cf\u5c11\u8fd9\u79cd\u4e0d\u4e00\u81f4\u3002", "motivation": "LLMs\u6b63\u6210\u4e3a\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u65b0\u5174\u5de5\u5177\uff0c\u4e3a\u5546\u4e1a\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u79d1\u5b66\u63d0\u4f9b\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u9886\u57df\u590d\u73b0\u4eba\u7c7b\u884c\u4e3a\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u4e5d\u4e2a\u5df2\u53d1\u8868\u7684\u884c\u4e3a\u8fd0\u8425\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\u7684\u590d\u73b0\u548c\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u5206\u5e03\u4e00\u81f4\u6027\u6765\u8bc4\u4f30LLM\u3002\u6b64\u5916\uff0c\u8fd8\u6d4b\u8bd5\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\uff08chain-of-thought prompting\uff09\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\u3002", "result": "LLMs\u80fd\u591f\u590d\u73b0\u5927\u591a\u6570\u5047\u8bbe\u5c42\u9762\u7684\u6548\u5e94\uff0c\u6355\u6349\u5173\u952e\u51b3\u7b56\u504f\u5dee\uff0c\u4f46\u5176\u54cd\u5e94\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u5373\u4f7f\u662f\u5f3a\u5927\u7684\u5546\u4e1a\u6a21\u578b\u4e5f\u4e0d\u4f8b\u5916\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u8fd9\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u80fd\u51cf\u5c11\u8fd9\u79cd\u4e0d\u4e00\u81f4\uff0c\u5e76\u80fd\u4f7f\u5c0f\u578b\u6216\u5f00\u6e90\u6a21\u578b\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u578b\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "conclusion": "LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6355\u6349\u51b3\u7b56\u504f\u5dee\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u901a\u8fc7\u7b80\u5355\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u53c2\u6570\u8c03\u4f18\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584LLM\u7684\u8868\u73b0\uff0c\u751a\u81f3\u4f7f\u5c0f\u578b\u6a21\u578b\u5177\u5907\u7ade\u4e89\u529b\u3002"}}
{"id": "2510.04284", "pdf": "https://arxiv.org/pdf/2510.04284", "abs": "https://arxiv.org/abs/2510.04284", "authors": ["Yunghwei Lai", "Kaiming Liu", "Ziyue Wang", "Weizhi Ma", "Yang Liu"], "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "The professionalism of a human doctor in outpatient service depends on two\ncore abilities: the ability to make accurate medical decisions and the medical\nconsultation skill to conduct strategic, empathetic patient inquiry. Existing\nLarge Language Models (LLMs) have achieved remarkable accuracy on medical\ndecision-making benchmarks. However, they often lack the ability to conduct the\nstrategic and empathetic consultation, which is essential for real-world\nclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor\nagent trained to master both of the capabilities by ask high-yield questions\nand conduct strategic multi-turn inquiry to guide decision-making. Our\nframework introduces three key components: a multi-agent interactive\nenvironment, a two-tiered reward architecture that separately optimizes\nclinical decision-making and communicative inquiry skills, and an experience\nrepository to ground policy learning in high-quality prior trajectories. We\nevaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across\nmulti-facet metrics, such as communication quality, user experience, and task\naccuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source\nspecialized LLMs by a substantial margin with higher parameter efficiency and\noutperforms powerful proprietary models. Furthermore, the human evaluations\nshow a strong preference for Doctor-R1 to generate human-preferred clinical\ndialogue, demonstrating the effectiveness of the framework.", "AI": {"tldr": "Doctor-R1\u662f\u4e00\u4e2aAI\u533b\u751f\u4ee3\u7406\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u51c6\u786e\u7684\u533b\u7597\u51b3\u7b56\u548c\u6218\u7565\u6027\u3001\u5bcc\u6709\u540c\u7406\u5fc3\u7684\u95ee\u8bca\u6280\u5de7\uff0c\u5f25\u8865\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u54a8\u8be2\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u8fdb\u884c\u6218\u7565\u6027\u3001\u5bcc\u6709\u540c\u7406\u5fc3\u7684\u95ee\u8bca\u80fd\u529b\uff0c\u8fd9\u5728\u771f\u5b9e\u7684\u4e34\u5e8a\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faDoctor-R1\u6846\u67b6\uff0c\u5305\u542b\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u73af\u5883\uff1b\u4e00\u4e2a\u4e24\u5c42\u5956\u52b1\u67b6\u6784\uff0c\u5206\u522b\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u548c\u6c9f\u901a\u95ee\u8bca\u6280\u80fd\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7ecf\u9a8c\u5e93\uff0c\u7528\u4e8e\u57fa\u4e8e\u9ad8\u8d28\u91cf\u5386\u53f2\u8f68\u8ff9\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\u3002", "result": "Doctor-R1\u5728OpenAI HealthBench\u548cMAQuE\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u4ee5\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u663e\u8457\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u4e13\u4e1aLLM\uff0c\u5e76\u4f18\u4e8e\u5f3a\u5927\u7684\u4e13\u6709\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4eba\u5de5\u8bc4\u4f30\u4e5f\u663e\u793a\u51fa\u5bf9Doctor-R1\u751f\u6210\u7684\u4e34\u5e8a\u5bf9\u8bdd\u7684\u5f3a\u70c8\u504f\u597d\u3002", "conclusion": "Doctor-R1\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u533b\u7597\u51b3\u7b56\u80fd\u529b\u548c\u6218\u7565\u6027\u3001\u5bcc\u6709\u540c\u7406\u5fc3\u7684\u95ee\u8bca\u6280\u5de7\uff0c\u6210\u529f\u63d0\u5347\u4e86AI\u533b\u751f\u5728\u95e8\u8bca\u670d\u52a1\u4e2d\u7684\u4e13\u4e1a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4e34\u5e8a\u5bf9\u8bdd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.04204", "pdf": "https://arxiv.org/pdf/2510.04204", "abs": "https://arxiv.org/abs/2510.04204", "authors": ["Zhengyang Tang", "Zihan Ye", "Chenyu Huang", "Xuhan Huang", "Chengpeng Li", "Sihang Li", "Guanhua Chen", "Ming Yan", "Zizhuo Wang", "Hongyuan Zha", "Dayiheng Liu", "Benyou Wang"], "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": "Work in progress", "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional \\textit{non-reflective} datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\n\\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\n\\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCALM\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7\u4fee\u6539\uff0c\u4f7f\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u4fee\u6b63\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e864B\u53c2\u6570\u7684STORM\u6a21\u578b\uff0c\u5176\u5728\u4e94\u9879\u4f18\u5316\u5efa\u6a21\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e8668.9%\u7684\u6700\u65b0\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e0e671B\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u63d0\u793a\u6570\u636e\u5408\u6210\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3LRMs\u7684\u5148\u8fdb\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u4f18\u5316\u5efa\u6a21\uff0c\u76f4\u63a5\u5728\u4f20\u7edf\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faCALM\uff08Corrective Adaptation with Lightweight Modification\uff09\u6846\u67b6\u3002\u901a\u8fc7\u4e13\u5bb6\u5e72\u9884\u8bc6\u522bLRM\u63a8\u7406\u7f3a\u9677\u5e76\u63d0\u4f9b\u7b80\u6d01\u7684\u7ea0\u6b63\u63d0\u793a\uff08\u4fee\u6539\u5c11\u4e8e2.6%\u7684token\uff09\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u968f\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u6784\u5efa\u4e86STORM\u6a21\u578b\u3002", "result": "\u57fa\u4e8eCALM\u5f00\u53d1\u76844B\u53c2\u6570LRM STORM\u5728\u4e94\u9879\u6d41\u884c\u7684\u4f18\u5316\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8668.9%\u7684\u6700\u65b0\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e0e\u4e00\u4e2a671B\u7684LRM\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u52a8\u6001\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u65e2\u4fdd\u7559\u53c8\u589e\u5f3a\u4e86\u73b0\u4ee3LRMs\u56fa\u6709\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u4e3a\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u6027\u80fd\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2510.03769", "pdf": "https://arxiv.org/pdf/2510.03769", "abs": "https://arxiv.org/abs/2510.03769", "authors": ["Shimaa Elbana", "Ahmad Kamal", "Shahd Ahmed Ali", "Ahmad Al-Kabbany"], "title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "The increasing size and complexity of medical imaging datasets, particularly\nin 3D formats, present significant barriers to collaborative research and\ntransferability. This study investigates whether the ZFP compression technique\ncan mitigate these challenges without compromising the performance of automated\ncerebrovascular segmentation, a critical first step in intracranial aneurysm\ndetection. We apply ZFP in both its error tolerance and fixed-rate modes to a\nlarge scale, and one of the most recent, datasets in the literature, 3D medical\ndataset containing ground-truth vascular segmentations. The segmentation\nquality on the compressed volumes is rigorously compared to the uncompressed\nbaseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can\nachieve substantial data reduction--up to a 22.89:1 ratio in error tolerance\nmode--while maintaining a high degree of fidelity, with the mean Dice\ncoefficient remaining high at 0.87656. These results demonstrate that ZFP is a\nviable and powerful tool for enabling more efficient and accessible research on\nlarge-scale medical datasets, fostering broader collaboration across the\ncommunity.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cZFP\u538b\u7f29\u6280\u672f\u80fd\u4ee5\u9ad8\u538b\u7f29\u6bd4\uff08\u6700\u9ad822.89:1\uff09\u6709\u6548\u964d\u4f4e\u5927\u89c4\u6a213D\u533b\u7597\u5f71\u50cf\u6570\u636e\u91cf\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u5f71\u54cd\u8111\u8840\u7ba1\u81ea\u52a8\u5206\u5272\u7684\u6027\u80fd\uff08Dice\u7cfb\u6570\u4fdd\u6301\u9ad8\u6c34\u5e73\uff09\uff0c\u4ece\u800c\u4fc3\u8fdb\u534f\u540c\u7814\u7a76\u3002", "motivation": "3D\u533b\u7597\u5f71\u50cf\u6570\u636e\u96c6\u65e5\u76ca\u5e9e\u5927\u4e14\u590d\u6742\uff0c\u7ed9\u534f\u540c\u7814\u7a76\u548c\u6570\u636e\u53ef\u8f6c\u79fb\u6027\u5e26\u6765\u4e86\u663e\u8457\u969c\u788d\u3002", "method": "\u672c\u7814\u7a76\u5c06ZFP\u538b\u7f29\u6280\u672f\uff08\u5305\u62ec\u5bb9\u9519\u6a21\u5f0f\u548c\u56fa\u5b9a\u901f\u7387\u6a21\u5f0f\uff09\u5e94\u7528\u4e8e\u4e00\u4e2a\u5927\u578b3D\u533b\u7597\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8840\u7ba1\u5206\u5272\u7684\u771f\u5b9e\u6807\u6ce8\u3002\u901a\u8fc7\u4e25\u683c\u6bd4\u8f83\u538b\u7f29\u524d\u540e\u6570\u636e\u4e0a\u81ea\u52a8\u8111\u8840\u7ba1\u5206\u5272\u7684\u8d28\u91cf\uff08\u4f7f\u7528Dice\u7cfb\u6570\uff09\u6765\u8bc4\u4f30ZFP\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cZFP\u5728\u5bb9\u9519\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe22.89:1\u7684\u6570\u636e\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u5206\u5272\u6027\u80fd\uff08\u5e73\u5747Dice\u7cfb\u6570\u4e3a0.87656\uff09\u4e0e\u672a\u538b\u7f29\u57fa\u7ebf\uff08Dice\u7ea60.8774\uff09\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "ZFP\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u4f7f\u5927\u89c4\u6a21\u533b\u7597\u6570\u636e\u96c6\u7684\u7814\u7a76\u66f4\u52a0\u9ad8\u6548\u548c\u6613\u4e8e\u8bbf\u95ee\uff0c\u4ece\u800c\u4fc3\u8fdb\u793e\u533a\u5185\u90e8\u66f4\u5e7f\u6cdb\u7684\u534f\u4f5c\u3002"}}
{"id": "2510.03313", "pdf": "https://arxiv.org/pdf/2510.03313", "abs": "https://arxiv.org/abs/2510.03313", "authors": ["Anirudh Subramanyam", "Yuxin Chen", "Robert L. Grossman"], "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining", "categories": ["cs.LG"], "comment": "18 pages, 6 figures", "summary": "Scaling laws for language model training traditionally characterize how\nperformance scales with model size and dataset volume. Prior work has explored\narchitecture variants and data treatments such as dataset filtering and noise\ninjection in language model pretraining; however, these studies have not\nformalized data quality within a principled scaling law. We introduce a\ndimensionless data-quality parameter Q, and propose a quality-aware scaling law\nextending the Chinchilla framework to predict loss as a joint function of model\nsize, data volume, and data quality. The law is motivated by an\neffective-sample-size and information-theoretic view of noisy or redundant\ncorpora, and it admits two practical estimators for Q: (i) a corruption rate\nproxy and (ii) a deficiency measure. Through synthetic experiments in neural\nmachine translation and autoregressive modeling -- where we systematically\ncontrol data quality via multiple levels of noise injection and coverage\nvariation -- we show that loss scales predictably with data quality and that\nhigher-quality data can substantially reduce model size and hence compute\nrequirements. Our results demonstrate a sublinear decay of effective data with\nquality and robustness to moderate data corruption; out-of-sample evaluations\nfurther validate the predictive form of the law. Unlike prior empirical\nanalyses, our work establishes an explicit, generalizable law for data quality,\noffering concrete guidance for balancing data curation effort and model scale\nin large-scale pretraining.", "AI": {"tldr": "\u5f15\u5165\u6570\u636e\u8d28\u91cf\u53c2\u6570Q\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u6269\u5c55Chinchilla\u6846\u67b6\u7684\u8d28\u91cf\u611f\u77e5\u7f29\u653e\u5b9a\u5f8b\uff0c\u4ee5\u6307\u5bfc\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7f29\u653e\u5b9a\u5f8b\u672a\u5c06\u6570\u636e\u8d28\u91cf\u5f62\u5f0f\u5316\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u6570\u636e\u7ba1\u7406\u548c\u6a21\u578b\u89c4\u6a21\u5e73\u8861\u3002", "method": "\u5f15\u5165\u65e0\u91cf\u7eb2\u6570\u636e\u8d28\u91cf\u53c2\u6570Q\uff0c\u5e76\u5c06\u5176\u6574\u5408\u81f3Chinchilla\u6846\u67b6\uff0c\u63d0\u51fa\u9884\u6d4b\u635f\u5931\u4e0e\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u91cf\u548c\u6570\u636e\u8d28\u91cf\u8054\u5408\u76f8\u5173\u7684\u7f29\u653e\u5b9a\u5f8b\u3002\u901a\u8fc7\u6709\u6548\u6837\u672c\u91cf\u548c\u4fe1\u606f\u8bba\u89c6\u89d2\u652f\u6491\uff0c\u5e76\u63d0\u4f9b\u4e24\u79cdQ\u7684\u5b9e\u7528\u4f30\u8ba1\u5668\uff08\u635f\u574f\u7387\u4ee3\u7406\u548c\u7f3a\u9677\u5ea6\u91cf\uff09\u3002\u5728\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u548c\u81ea\u56de\u5f52\u5efa\u6a21\u4e2d\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u635f\u5931\u4e0e\u6570\u636e\u8d28\u91cf\u53ef\u9884\u6d4b\u5730\u7f29\u653e\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u6709\u6548\u6570\u636e\u968f\u8d28\u91cf\u5448\u4e9a\u7ebf\u6027\u8870\u51cf\uff0c\u4e14\u5bf9\u4e2d\u5ea6\u6570\u636e\u635f\u574f\u5177\u6709\u9c81\u68d2\u6027\u3002\u5b9a\u5f8b\u7684\u9884\u6d4b\u5f62\u5f0f\u901a\u8fc7\u6837\u672c\u5916\u8bc4\u4f30\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u5efa\u7acb\u4e86\u663e\u5f0f\u4e14\u53ef\u6cdb\u5316\u7684\u6570\u636e\u8d28\u91cf\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u5e73\u8861\u6570\u636e\u7b56\u5c55\u52aa\u529b\u548c\u6a21\u578b\u89c4\u6a21\u63d0\u4f9b\u4e86\u5177\u4f53\u3001\u666e\u9002\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.04311", "pdf": "https://arxiv.org/pdf/2510.04311", "abs": "https://arxiv.org/abs/2510.04311", "authors": ["Bohan Tang", "Huidong Liang", "Keyue Jiang", "Xiaowen Dong"], "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u6df1\u5ea6\uff08\u63a8\u7406\u957f\u5ea6\uff09\u548c\u5bbd\u5ea6\uff08\u80fd\u529b\u591a\u6837\u6027\uff09\u6765\u8861\u91cf\u4efb\u52a1\u590d\u6742\u6027\uff0c\u5e76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-MAS\uff09\u76f8\u5bf9\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-SAS\uff09\u7684\u4f18\u52bf\u968f\u4efb\u52a1\u6df1\u5ea6\u548c\u5bbd\u5ea6\u589e\u52a0\u800c\u589e\u5927\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u65b9\u9762\u66f4\u663e\u8457\uff0c\u4ece\u800c\u660e\u786e\u4e86LLM-MAS\u7684\u9002\u7528\u573a\u666f\u3002", "motivation": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-MAS\uff09\u6709\u671b\u5b9e\u73b0\u66f4\u9ad8\u7ea7AI\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bbe\u8ba1\u6765\u8bc4\u4f30\u5176\u76f8\u5bf9\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-SAS\uff09\u7684\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u4efb\u52a1\u590d\u6742\u6027\uff0c\u4ee5\u6709\u6548\u8bc4\u4f30LLM-MAS\u7684\u6548\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u201c\u6df1\u5ea6\u201d\uff08\u63a8\u7406\u957f\u5ea6\uff09\u548c\u201c\u5bbd\u5ea6\u201d\uff08\u80fd\u529b\u591a\u6837\u6027\uff09\u4e24\u4e2a\u7ef4\u5ea6\u6765\u8868\u5f81\u4efb\u52a1\u590d\u6742\u6027\u3002\u63a5\u7740\uff0c\u6211\u4eec\u7406\u8bba\u5206\u6790\u4e86\u4ee3\u8868\u6027\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\uff0c\u5e76\u5bf9\u5176\u5728\u4e0d\u540c\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u5224\u522b\u6027\u4e0e\u751f\u6210\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u5747\u8868\u660e\uff0cLLM-MAS\u76f8\u5bf9\u4e8eLLM-SAS\u7684\u4f18\u52bf\u968f\u4efb\u52a1\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u589e\u52a0\u800c\u589e\u5927\u3002\u5176\u4e2d\uff0c\u6df1\u5ea6\u5bf9LLM-MAS\u7684\u6548\u76ca\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u672c\u7814\u7a76\u6f84\u6e05\u4e86LLM-MAS\u4f55\u65f6\u80fd\u5e26\u6765\u4f18\u52bf\uff0c\u5e76\u4e3a\u672a\u6765LLM-MAS\u65b9\u6cd5\u548c\u57fa\u51c6\u7684\u8bbe\u8ba1\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2510.04214", "pdf": "https://arxiv.org/pdf/2510.04214", "abs": "https://arxiv.org/abs/2510.04214", "authors": ["Zhuoran Zhuang", "Ye Chen", "Xia Zeng", "Chao Luo", "Luhui Liu", "Yihan Chen"], "title": "Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards", "categories": ["cs.CL"], "comment": null, "summary": "We study deploying large language models (LLMs) as business development (BD)\nagents for persuasive price negotiation in online travel agencies (OTAs), where\naligning traveler affordability and hotel profitability directly affects\nbookings, partner relationships, and access to travel. The agent must follow a\nStandard Operating Procedure (SOP) while conducting multi-turn persuasion,\ninterpreting colloquial inputs, and adhering to guardrails (no over-promising,\nno hallucinations). Conventional post-training -- supervised fine-tuning (SFT)\nor single-source reward optimization -- overfits scripts, misses nuanced\npersuasive style, and fails to enforce verifiable business constraints.\n  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement\nlearning post-training framework that aligns an LLM with heterogeneous rewards:\na preference-trained reward model (RM) for dense human alignment, a reward\njudge (RJ) for high-level persuasive behavior and SOP compliance, and\nprogrammatic reward functions (RF) for deterministic checks on numerics,\nformatting, and guardrails. A straightforward enhancement mechanism is proposed\nto combine the RM with RJ and RF signals to curb reward hacking and improve\nnegotiation quality. In production-style evaluations -- approximately 150 turns\nfrom real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts\naverage dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference\nOptimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),\nincreases the share of conversations with at least one excellent response to\n66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix\nrate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also\nobserve emergent capabilities -- proactive empathy, localized reasoning,\ncalibrated tactics -- that surpass gold annotations.", "AI": {"tldr": "\u63d0\u51faReward-Enhanced Policy Optimization (REPO)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6e90\u5f02\u6784\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5728\u7ebf\u65c5\u884c\u793e\uff08OTA\uff09\u4e2d\u8fdb\u884c\u8bf4\u670d\u6027\u8bae\u4ef7\u8c08\u5224\u7684\u6027\u80fd\u3001\u5bf9\u8bdd\u8d28\u91cf\u548c\u5408\u89c4\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u65b0\u5174\u80fd\u529b\u3002", "motivation": "\u5728\u5728\u7ebf\u65c5\u884c\u793e\u4e2d\uff0cLLM\u4f5c\u4e3a\u5546\u52a1\u53d1\u5c55\uff08BD\uff09\u4ee3\u7406\u8fdb\u884c\u4ef7\u683c\u8c08\u5224\uff0c\u9700\u5e73\u8861\u65c5\u884c\u8005\u652f\u4ed8\u80fd\u529b\u4e0e\u9152\u5e97\u76c8\u5229\uff0c\u8fd9\u5bf9\u4e8e\u9884\u8ba2\u548c\u4f19\u4f34\u5173\u7cfb\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709LLM\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982SFT\uff09\u96be\u4ee5\u9075\u5faa\u6807\u51c6\u64cd\u4f5c\u89c4\u7a0b\uff08SOP\uff09\u3001\u5904\u7406\u591a\u8f6e\u8bf4\u670d\u3001\u89e3\u91ca\u53e3\u8bed\u8f93\u5165\u4ee5\u53ca\u9075\u5b88\u5b89\u5168\u9650\u5236\uff08\u5982\u65e0\u8fc7\u5ea6\u627f\u8bfa\u3001\u65e0\u5e7b\u89c9\uff09\uff0c\u5e38\u51fa\u73b0\u811a\u672c\u8fc7\u62df\u5408\u3001\u8bf4\u670d\u98ce\u683c\u4e0d\u8db3\u548c\u4e1a\u52a1\u7ea6\u675f\u6267\u884c\u5931\u8d25\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faReward-Enhanced Policy Optimization (REPO)\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u5c06LLM\u4e0e\u5f02\u6784\u5956\u52b1\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\uff1a1) \u504f\u597d\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u7528\u4e8e\u5bc6\u96c6\u7684\u4eba\u7c7b\u5bf9\u9f50\uff1b2) \u5956\u52b1\u8bc4\u5224\u5668\uff08RJ\uff09\u7528\u4e8e\u9ad8\u5c42\u6b21\u7684\u8bf4\u670d\u884c\u4e3a\u548cSOP\u5408\u89c4\u6027\uff1b3) \u7a0b\u5e8f\u5316\u5956\u52b1\u51fd\u6570\uff08RF\uff09\u7528\u4e8e\u5bf9\u6570\u5b57\u3001\u683c\u5f0f\u548c\u5b89\u5168\u9650\u5236\u8fdb\u884c\u786e\u5b9a\u6027\u68c0\u67e5\u3002\u901a\u8fc7\u4e00\u79cd\u76f4\u63a5\u7684\u589e\u5f3a\u673a\u5236\uff0c\u5c06RM\u4e0eRJ\u548cRF\u4fe1\u53f7\u76f8\u7ed3\u5408\uff0c\u4ee5\u904f\u5236\u5956\u52b1\u4f5c\u5f0a\u5e76\u63d0\u9ad8\u8c08\u5224\u8d28\u91cf\u3002", "result": "\u5728\u751f\u4ea7\u73af\u5883\u5f0f\u8bc4\u4f30\u4e2d\uff08\u5305\u62ec\u771f\u5b9e\u5bf9\u8bdd\u548c\u4e0d\u826f\u6848\u4f8b\u5bf9\u8bdd\uff09\uff0cREPO\u6846\u67b6\u8868\u73b0\u4f18\u5f02\uff1a\u5e73\u5747\u5bf9\u8bdd\u8bc4\u5206\u63d0\u5347\u81f34.63\u5206\uff08\u6bd4\u57fa\u7ebf\u9ad81.20\uff0c\u6bd4DPO\u9ad80.83\uff0c\u6bd4GRPO\u9ad80.33\uff09\uff1b\u81f3\u5c11\u5305\u542b\u4e00\u4e2a\u4f18\u79c0\u54cd\u5e94\u7684\u5bf9\u8bdd\u5360\u6bd4\u589e\u81f366.67%\uff08\u6bd4GRPO\u9ad823.34\u4e2a\u767e\u5206\u70b9\uff09\uff1b\u4e0d\u826f\u6848\u4f8b\u4fee\u590d\u7387\u8fbe93.33%\uff08\u5176\u4e2d75.56%\u4e3a\u5e72\u51c0\u4fee\u590d\uff09\uff0c\u5168\u9762\u8d85\u8d8aSFT\u3001DPO\u3001PPO\u548cGRPO\u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u89c2\u5bdf\u5230LLM\u6d8c\u73b0\u51fa\u4e3b\u52a8\u540c\u7406\u5fc3\u3001\u5c40\u90e8\u63a8\u7406\u548c\u7b56\u7565\u6821\u51c6\u7b49\u8d85\u8d8a\u9ec4\u91d1\u6807\u6ce8\u7684\u65b0\u5174\u80fd\u529b\u3002", "conclusion": "REPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u8bae\u4ef7\u573a\u666f\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6e90\u5f02\u6784\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4f5c\u4e3aBD\u4ee3\u7406\u7684\u8c08\u5224\u6027\u80fd\u3001\u5bf9\u8bdd\u8d28\u91cf\u548c\u5408\u89c4\u6027\u3002\u5176\u5353\u8d8a\u7684\u8bc4\u4f30\u7ed3\u679c\u4ee5\u53ca\u5c55\u73b0\u51fa\u7684\u65b0\u5174\u80fd\u529b\uff0c\u4e3a\u5728\u7ebf\u65c5\u884c\u793e\u7684\u5546\u52a1\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03786", "pdf": "https://arxiv.org/pdf/2510.03786", "abs": "https://arxiv.org/abs/2510.03786", "authors": ["T-Mai Bui", "Fares Bougourzi", "Fadi Dornaika", "Vinh Truong Hoang"], "title": "MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408CNN\u3001Transformer\u548cMamba\u7684\u6df7\u5408\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7\u6355\u6349\u591a\u5c3a\u5ea6\u4f9d\u8d56\u548c\u589e\u5f3a\u7279\u5f81\u4ea4\u4e92\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5e38\u662f\u4efb\u52a1\u7279\u5f02\u6027\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u96be\u4ee5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5e73\u8861\u6a21\u578b\u590d\u6742\u5ea6\u548c\u6027\u80fd\uff08\u51c6\u786e\u6027\u548c\u6548\u7387\uff09\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u5206\u5272\u67b6\u6784\uff1a\n1.  \u4e09\u5206\u652f\u7f16\u7801\u5668\uff1a\u6574\u5408CNN\u3001Transformer\u548c\u57fa\u4e8eMamba\u7684\u6ce8\u610f\u529b\u878d\u5408\uff08MAF\uff09\u673a\u5236\uff0c\u4ee5\u6355\u6349\u5c40\u90e8\u3001\u5168\u5c40\u548c\u957f\u7a0b\u4f9d\u8d56\u3002\n2.  \u591a\u5c3a\u5ea6\u6ce8\u610f\u529bCNN\u89e3\u7801\u5668\uff1a\u91cd\u5efa\u7cbe\u7ec6\u5206\u5272\u56fe\u5e76\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002\n3.  \u534f\u540c\u6ce8\u610f\u529b\u95e8\uff1a\u5728\u7f16\u7801\u548c\u89e3\u7801\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u7279\u5f81\u9009\u62e9\u548c\u8de8\u5c3a\u5ea6\u901a\u4fe1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8be5\u67b6\u6784\u901a\u8fc7\u6709\u6548\u5e73\u8861\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3a\u591a\u6837\u5316\u7684\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03325", "pdf": "https://arxiv.org/pdf/2510.03325", "abs": "https://arxiv.org/abs/2510.03325", "authors": ["Giuseppe Di Somma", "Giorgio Carelli", "Angela D. V. Di Virgilio", "Francesco Fuso", "Enrico Maccioni", "Paolo Marsili"], "title": "Fast frequency reconstruction using Deep Learning for event recognition in ring laser data", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "physics.geo-ph"], "comment": null, "summary": "The reconstruction of a frequency with minimal delay from a sinusoidal signal\nis a common task in several fields; for example Ring Laser Gyroscopes, since\ntheir output signal is a beat frequency. While conventional methods require\nseveral seconds of data, we present a neural network approach capable of\nreconstructing frequencies of several hundred Hertz within approximately 10\nmilliseconds. This enables rapid trigger generation. The method outperforms\nstandard Fourier-based techniques, improving frequency estimation precision by\na factor of 2 in the operational range of GINGERINO, our Ring Laser\nGyroscope.\\\\ In addition to fast frequency estimation, we introduce an\nautomated classification framework to identify physical disturbances in the\nsignal, such as laser instabilities and seismic events, achieving accuracy\nrates between 99\\% and 100\\% on independent test datasets for the seismic\nclass. These results mark a step forward in integrating artificial intelligence\ninto signal analysis for geophysical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u91cd\u5efa\u6b63\u5f26\u4fe1\u53f7\u9891\u7387\uff0810\u6beb\u79d2\u5185\uff09\u5e76\u81ea\u52a8\u5206\u7c7b\u7269\u7406\u6270\u52a8\uff08\u5730\u9707\u4e8b\u4ef6\u51c6\u786e\u738799-100%\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u5085\u91cc\u53f6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u4eea\u7b49\u5730\u7403\u7269\u7406\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u73af\u5f62\u6fc0\u5149\u9640\u87ba\u4eea\u4fe1\u53f7\u9891\u7387\u91cd\u5efa\uff09\u9700\u8981\u6570\u79d2\u6570\u636e\uff0c\u5ef6\u8fdf\u9ad8\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5feb\u901f\u89e6\u53d1\u3002\u540c\u65f6\u9700\u8981\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u6270\u52a8\uff08\u5982\u6fc0\u5149\u4e0d\u7a33\u5b9a\u6027\u3001\u5730\u9707\u4e8b\u4ef6\uff09\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5b9e\u73b0\u6b63\u5f26\u4fe1\u53f7\u7684\u9891\u7387\u91cd\u5efa\uff1b\u5f15\u5165\u81ea\u52a8\u5316\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u6270\u52a8\uff08\u5982\u6fc0\u5149\u4e0d\u7a33\u5b9a\u6027\u3001\u5730\u9707\u4e8b\u4ef6\uff09\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u80fd\u5728\u7ea610\u6beb\u79d2\u5185\u91cd\u5efa\u6570\u767e\u8d6b\u5179\u7684\u9891\u7387\uff0c\u9891\u7387\u4f30\u8ba1\u7cbe\u5ea6\u6bd4\u6807\u51c6\u5085\u91cc\u53f6\u6280\u672f\u63d0\u9ad82\u500d\u3002\u81ea\u52a8\u5316\u5206\u7c7b\u6846\u67b6\u5728\u72ec\u7acb\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5bf9\u5730\u9707\u7c7b\u6270\u52a8\u8bc6\u522b\u51c6\u786e\u7387\u8fbe99%\u81f3100%\u3002", "conclusion": "\u672c\u7814\u7a76\u6807\u5fd7\u7740\u4eba\u5de5\u667a\u80fd\u5728\u5730\u7403\u7269\u7406\u4fe1\u53f7\u5206\u6790\u9886\u57df\u96c6\u6210\u65b9\u9762\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.04371", "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "categories": ["cs.AI", "cs.DC", "cs.MA"], "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u63a8\u6d4b\u6027\u52a8\u4f5c\u201d\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u66f4\u5feb\u7684\u6a21\u578b\u9884\u6d4b\u5e76\u5e76\u884c\u6267\u884c\u591a\u6b65\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u6267\u884c\u7f13\u6162\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "AI\u4ee3\u7406\u5728\u73af\u5883\u4e2d\u7684\u6267\u884c\u901f\u5ea6\u901a\u5e38\u5f88\u6162\uff0c\u963b\u788d\u4e86\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u3002\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u4ee3\u7406\u884c\u4e3a\u662f\u987a\u5e8f\u5c55\u5f00\u7684\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u9700\u8981\u8017\u65f6\u7684API\u8c03\u7528\u3002", "method": "\u501f\u9274\u5fae\u5904\u7406\u5668\u4e2d\u7684\u63a8\u6d4b\u6267\u884c\u548cLLM\u63a8\u7406\u4e2d\u7684\u63a8\u6d4b\u89e3\u7801\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u635f\u7684\u201c\u63a8\u6d4b\u6027\u52a8\u4f5c\u201d\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u66f4\u5feb\u7684\u6a21\u578b\u9884\u6d4b\u53ef\u80fd\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u6b65\u5e76\u884c\u6267\u884c\u3002", "result": "\u5728\u6e38\u620f\u3001\u7535\u5546\u3001\u7f51\u9875\u641c\u7d22\u7b49\u4ee3\u7406\u73af\u5883\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u73af\u5883\u7684\u201c\u6709\u635f\u201d\u6269\u5c55\u4e2d\uff0c\u63a8\u6d4b\u6027\u52a8\u4f5c\u5b9e\u73b0\u4e86\u9ad8\u8fbe55%\u7684\u4e0b\u4e00\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u901a\u8fc7\u66f4\u5f3a\u7684\u731c\u6d4b\u6a21\u578b\u3001Top-K\u52a8\u4f5c\u9884\u6d4b\u3001\u591a\u6b65\u63a8\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f18\u5316\uff0c\u6027\u80fd\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u201c\u63a8\u6d4b\u6027\u52a8\u4f5c\u201d\u6846\u67b6\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u4f4e\u5ef6\u8fdf\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5f00\u8f9f\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9053\u8def\u3002"}}
{"id": "2510.04226", "pdf": "https://arxiv.org/pdf/2510.04226", "abs": "https://arxiv.org/abs/2510.04226", "authors": ["Dustin Wright", "Sarah Masud", "Jared Moore", "Srishti Yadav", "Maria Antoniak", "Chan Young Park", "Isabelle Augenstein"], "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "comment": "16 pages; 8 figures, 4 tables", "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u77e5\u8bc6\u540c\u8d28\u5316\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\u8861\u91cfLLM\u8f93\u51fa\u7684\u8ba4\u77e5\u591a\u6837\u6027\u3002\u7814\u7a76\u53d1\u73b0LLMs\u591a\u6837\u6027\u666e\u904d\u4f4e\u4e8e\u7f51\u9875\u641c\u7d22\uff0c\u5927\u6a21\u578b\u591a\u6837\u6027\u4f4e\uff0cRAG\u6709\u5e2e\u52a9\uff0c\u4f46\u5b58\u5728\u975e\u82f1\u8bed\u77e5\u8bc6\u8868\u793a\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u503e\u5411\u4e8e\u751f\u6210\u8bcd\u6c47\u3001\u8bed\u4e49\u548c\u98ce\u683c\u4e0a\u540c\u8d28\u5316\u7684\u6587\u672c\uff0c\u8fd9\u5e26\u6765\u4e86\u77e5\u8bc6\u574d\u584c\u7684\u98ce\u9669\uff0c\u5373\u540c\u8d28\u5316\u7684LLMs\u53ef\u80fd\u5bfc\u81f4\u53ef\u83b7\u53d6\u4fe1\u606f\u8303\u56f4\u968f\u65f6\u95f4\u7f29\u5c0f\u3002\u73b0\u6709\u5173\u4e8e\u540c\u8d28\u5316\u7684\u7814\u7a76\u5728\u65b9\u6cd5\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u80fd\u8003\u5bdf\u8de8\u65f6\u95f4\u53ca\u6587\u5316\u80cc\u666f\u7684\u8d8b\u52bf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u8861\u91cfLLM\u8f93\u51fa\u4e2d\u771f\u5b9e\u4e16\u754c\u4e3b\u5f20\u7684\u201c\u8ba4\u77e5\u591a\u6837\u6027\u201d\uff08epistemic diversity\uff09\u3002\u901a\u8fc7\u4e00\u9879\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u6d4b\u8bd5\u4e8627\u4e2aLLM\u3001155\u4e2a\u6db5\u76d612\u4e2a\u56fd\u5bb6\u7684\u4e3b\u9898\u4ee5\u53ca200\u79cd\u6e90\u81ea\u771f\u5b9e\u7528\u6237\u804a\u5929\u7684\u63d0\u793a\u53d8\u4f53\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u65b0\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u4e3b\u5f20\uff0c\u4f46\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u5728\u8ba4\u77e5\u591a\u6837\u6027\u65b9\u9762\u5747\u4e0d\u5982\u57fa\u672c\u7684\u7f51\u7edc\u641c\u7d22\u30022) \u6a21\u578b\u5927\u5c0f\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\u30023) \u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u5176\u6539\u8fdb\u6548\u679c\u56e0\u6587\u5316\u80cc\u666f\u800c\u5f02\u30024) \u4e0e\u4f20\u7edf\u77e5\u8bc6\u6765\u6e90\uff08\u7ef4\u57fa\u767e\u79d1\uff09\u76f8\u6bd4\uff0cLLM\u751f\u6210\u7684\u56fd\u5bb6\u7279\u5b9a\u4e3b\u5f20\u66f4\u591a\u5730\u53cd\u6620\u82f1\u8bed\u800c\u975e\u5f53\u5730\u8bed\u8a00\uff0c\u51f8\u663e\u4e86\u8ba4\u77e5\u8868\u793a\u7684\u5dee\u8ddd\u3002", "conclusion": "LLMs\u5b58\u5728\u77e5\u8bc6\u540c\u8d28\u5316\u95ee\u9898\uff0c\u5c3d\u7ba1\u65b0\u6a21\u578b\u6709\u6240\u6539\u5584\uff0c\u4f46\u5176\u8ba4\u77e5\u591a\u6837\u6027\u4ecd\u4f4e\u4e8e\u4f20\u7edf\u7f51\u9875\u641c\u7d22\u3002\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\uff0c\u591a\u6837\u6027\u8d8a\u5dee\uff0c\u800cRAG\u6709\u52a9\u4e8e\u63d0\u5347\u591a\u6837\u6027\u4f46\u6548\u679c\u53d7\u6587\u5316\u8bed\u5883\u5f71\u54cd\u3002\u5f53\u524dLLMs\u5728\u975e\u82f1\u8bed\u8bed\u5883\u4e0b\u5b58\u5728\u663e\u8457\u7684\u77e5\u8bc6\u8868\u793a\u9e3f\u6c9f\u3002"}}
{"id": "2510.03797", "pdf": "https://arxiv.org/pdf/2510.03797", "abs": "https://arxiv.org/abs/2510.03797", "authors": ["Rasel Hossen", "Diptajoy Mistry", "Mushiur Rahman", "Waki As Sami Atikur Rahman Hridoy", "Sajib Saha", "Muhammad Ibrahim"], "title": "Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages", "summary": "Urban safety and infrastructure maintenance are critical components of smart\ncity development. Manual monitoring of road damages is time-consuming, highly\ncostly, and error-prone. This paper presents a deep learning approach for\nautomated road damage and manhole detection using the YOLOv9 algorithm with\npolygonal annotations. Unlike traditional bounding box annotation, we employ\npolygonal annotations for more precise localization of road defects. We develop\na novel dataset comprising more than one thousand images which are mostly\ncollected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based\nmodel for three classes, namely Broken, Not Broken, and Manhole. We achieve\n78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong\nperformance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)\nclasses, with challenges in Manhole detection (18.2% F1-score) due to class\nimbalance. Our approach offers an efficient and scalable solution for\nmonitoring urban infrastructure in developing countries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eYOLOv9\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u8fb9\u5f62\u6807\u6ce8\uff0c\u5b9e\u73b0\u9053\u8def\u635f\u574f\u548c\u6c99\u4e95\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u7684\u75db\u70b9\u3002", "motivation": "\u57ce\u5e02\u5b89\u5168\u548c\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u662f\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u7684\u5173\u952e\u3002\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u9053\u8def\u635f\u574f\u8017\u65f6\u3001\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\uff0c\u6025\u9700\u9ad8\u6548\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528YOLOv9\u7b97\u6cd5\uff0c\u5e76\u4f7f\u7528\u591a\u8fb9\u5f62\u6807\u6ce8\uff08\u800c\u975e\u4f20\u7edf\u8fb9\u754c\u6846\uff09\u5bf9\u8def\u9762\u7f3a\u9677\u8fdb\u884c\u66f4\u7cbe\u786e\u7684\u5b9a\u4f4d\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5343\u4f59\u5f20\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff08\u4e3b\u8981\u6536\u96c6\u81ea\u5b5f\u52a0\u62c9\u56fd\u8fbe\u5361\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u68c0\u6d4b\u201c\u635f\u574f\u201d\u3001\u201c\u672a\u635f\u574f\u201d\u548c\u201c\u6c99\u4e95\u201d\u4e09\u7c7b\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8678.1%\u7684\u6574\u4f53\u56fe\u50cf\u7ea7\u51c6\u786e\u7387\u3002YOLOv9\u5728\u201c\u635f\u574f\u201d\uff08F1-score 86.7%\uff09\u548c\u201c\u672a\u635f\u574f\u201d\uff08F1-score 89.2%\uff09\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u201c\u6c99\u4e95\u201d\u68c0\u6d4b\u4e0a\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u800c\u9762\u4e34\u6311\u6218\uff08F1-score 18.2%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u76d1\u6d4b\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03330", "pdf": "https://arxiv.org/pdf/2510.03330", "abs": "https://arxiv.org/abs/2510.03330", "authors": ["Andy Wu", "Chun-Cheng Lin", "Yuehua Huang", "Rung-Tzuo Liaw"], "title": "Constant in an Ever-Changing World", "categories": ["cs.LG"], "comment": "in Chinese language", "summary": "The training process of reinforcement learning often suffers from severe\noscillations, leading to instability and degraded performance. In this paper,\nwe propose a Constant in an Ever-Changing World (CIC) framework that enhances\nalgorithmic stability to improve performance. CIC maintains both a\nrepresentative policy and a current policy. Instead of updating the\nrepresentative policy blindly, CIC selectively updates it only when the current\npolicy demonstrates superiority. Furthermore, CIC employs an adaptive\nadjustment mechanism, enabling the representative and current policies to\njointly facilitate critic training. We evaluate CIC on five MuJoCo\nenvironments, and the results show that CIC improves the performance of\nconventional algorithms without incurring additional computational cost.", "AI": {"tldr": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9707\u8361\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faCIC\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u4ee3\u8868\u7b56\u7565\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u7a33\u5b9a\u6027\u53ca\u6027\u80fd\uff0c\u4e14\u65e0\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5e38\u906d\u9047\u5267\u70c8\u9707\u8361\uff0c\u5bfc\u81f4\u7b97\u6cd5\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u201c\u53d8\u52a8\u4e16\u754c\u4e2d\u7684\u4e0d\u53d8\u201d\uff08CIC\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ef4\u62a4\u4e00\u4e2a\u4ee3\u8868\u7b56\u7565\u548c\u4e00\u4e2a\u5f53\u524d\u7b56\u7565\uff0c\u5e76\u4ec5\u5728\u5f53\u524d\u7b56\u7565\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u65f6\u624d\u9009\u62e9\u6027\u5730\u66f4\u65b0\u4ee3\u8868\u7b56\u7565\u3002\u6b64\u5916\uff0cCIC\u91c7\u7528\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\uff0c\u4f7f\u4e24\u4e2a\u7b56\u7565\u5171\u540c\u4fc3\u8fdb\u8bc4\u8bba\u5bb6\uff08critic\uff09\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2aMuJoCo\u73af\u5883\u4e0b\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cCIC\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u4e86\u4f20\u7edf\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CIC\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u7b97\u6cd5\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u6539\u5584\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.04373", "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints.", "AI": {"tldr": "JEF Hinter\u7cfb\u7edf\u5c06\u79bb\u7ebf\u8f68\u8ff9\u63d0\u70bc\u4e3a\u7d27\u51d1\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u9ad8\u6548\u6539\u8fdbLLM\u4ee3\u7406\u5728\u964c\u751f\u9886\u57df\u7684\u8868\u73b0\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5728\u7ebf\u4ea4\u4e92\u6216\u5fae\u8c03\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u964c\u751f\u9886\u57df\u6539\u8fdb\u65f6\uff0c\u9700\u6602\u8d35\u7684\u5728\u7ebf\u4ea4\u4e92\u6216\u4e13\u5bb6\u6570\u636e\u5fae\u8c03\uff0c\u5bf9\u95ed\u6e90\u6a21\u578b\u4e0d\u5207\u5b9e\u9645\uff0c\u5bf9\u5f00\u6e90\u6a21\u578b\u6709\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002\u79bb\u7ebf\u8f68\u8ff9\u867d\u542b\u77e5\u8bc6\uff0c\u4f46\u539f\u59cb\u8f68\u8ff9\u5197\u957f\u3001\u5608\u6742\u4e14\u4efb\u52a1\u7ed1\u5b9a\uff0c\u5bfc\u81f4\u57fa\u4e8e\u6f14\u793a\u7684\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faJEF Hinter\u7cfb\u7edf\uff0c\u901a\u8fc7\u201c\u7f29\u653e\u673a\u5236\u201d\u4ece\u79bb\u7ebf\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7d27\u51d1\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u6355\u83b7\u7b56\u7565\u548c\u9677\u9631\u3002\u8be5\u7cfb\u7edf\u80fd\u5229\u7528\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\uff0c\u5373\u4f7f\u4ec5\u6709\u5931\u8d25\u6570\u636e\u4e5f\u80fd\u63d0\u53d6\u6307\u5bfc\uff0c\u5e76\u652f\u6301\u5e76\u884c\u63d0\u793a\u751f\u6210\u548c\u72ec\u7acb\u4e8e\u57fa\u51c6\u7684\u63d0\u793a\u3002\u63a8\u7406\u65f6\uff0c\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u63d0\u793a\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u3001\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\u7684\u6307\u5bfc\u3002", "result": "\u5728MiniWoB++\u3001WorkArena-L1\u548cWebArena-Lite\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJEF Hinter\u6301\u7eed\u4f18\u4e8e\u5305\u62ec\u57fa\u4e8e\u4eba\u7c7b\u548c\u57fa\u4e8e\u6587\u6863\u63d0\u793a\u5728\u5185\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "JEF Hinter\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u79bb\u7ebf\u8f68\u8ff9\u6570\u636e\u6765\u6539\u8fdbLLM\u4ee3\u7406\u5728\u964c\u751f\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2510.04230", "pdf": "https://arxiv.org/pdf/2510.04230", "abs": "https://arxiv.org/abs/2510.04230", "authors": ["Guijin Son", "Donghun Yang", "Hitesh Laxmichand Patel", "Amit Agarwal", "Hyunwoo Ko", "Chanuk Lim", "Srikant Panda", "Minhyuk Kim", "Nikunj Drolia", "Dasol Choi", "Kyong-Ha Lee", "Youngjae Yu"], "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Recent frontier models employ long chain-of-thought reasoning to explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective than monolingual CoT, also resulting\nin cross-lingual and mult-modal performance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8bed\u8a00\u601d\u7ef4\u94fe\uff08Language-Mixed CoT\uff09\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u97e9\u8bed\u63a8\u7406\u6570\u636e\u96c6Yi-Sang\uff0c\u8bad\u7ec3\u51faSOTA\u7684KO-REAson-35B\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97e9\u8bed\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u524d\u6cbf\u6a21\u578b\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5927\u591a\u96c6\u4e2d\u5728\u82f1\u8bed\u9886\u57df\uff0c\u5bf9\u8bed\u8a00\u7279\u5b9a\u63a8\u7406\uff08\u5c24\u5176\u662f\u975e\u82f1\u8bed\u8bed\u8a00\uff09\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86**Language-Mixed CoT**\uff0c\u4e00\u79cd\u5c06\u82f1\u8bed\u4f5c\u4e3a\u63a8\u7406\u951a\u70b9\u5e76\u4e0e\u76ee\u6807\u8bed\u8a00\u5207\u6362\u7684\u63a8\u7406\u65b9\u6848\u3002\u9488\u5bf9\u97e9\u8bed\uff0c\u6784\u5efa\u4e86**Yi-Sang**\u6570\u636e\u96c6\uff08\u5305\u542b5.79M\u97e9\u8bed\u63d0\u793a\u548c3.7M\u63a8\u7406\u8f68\u8ff9\uff09\uff0c\u5e76\u5728\u516d\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08\u5982Qwen2.5, Llama-3.1\uff09\u4e0a\u8bad\u7ec3\u4e869\u4e2a\u6a21\u578b\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "result": "\u6700\u4f73\u6a21\u578b**KO-REAson-35B**\u5728\u97e9\u8bed\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff08SOTA\uff09\uff0c\u5e73\u5747\u5f97\u520664.0 \u00b1 25\uff0c\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d5\u9879\u6392\u540d\u7b2c\u4e00\u3002\u4e2d\u5c0f\u578b\u6a21\u578b\u4e5f\u5e73\u5747\u63d0\u5347\u4e86+18.6\u5206\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660eLanguage-Mixed CoT\u6bd4\u5355\u8bedCoT\u66f4\u6709\u6548\uff0c\u5e76\u5e26\u6765\u4e86\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Language-Mixed CoT\u7ed3\u5408\u9ad8\u8d28\u91cf\u7279\u5b9a\u8bed\u8a00\u6570\u636e\uff08\u5982Yi-Sang\uff09\uff0c\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u7279\u5b9a\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e76\u5bf9\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u63a8\u7406\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u548c\u8d44\u6e90\u3002"}}
{"id": "2510.03821", "pdf": "https://arxiv.org/pdf/2510.03821", "abs": "https://arxiv.org/abs/2510.03821", "authors": ["Venkata Narendra Kotyada", "Revanth Eranki", "Nagesh Bhattu Sristy"], "title": "Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation", "categories": ["cs.CV"], "comment": "9 pages, 3 figures", "summary": "Unpaired image-to-image translation involves learning mappings between source\ndomain and target domain in the absence of aligned or corresponding samples.\nScore based diffusion models have demonstrated state-of-the-art performance in\ngenerative tasks. Their ability to approximate complex data distributions\nthrough stochastic differential equations (SDEs) enables them to generate\nhigh-fidelity and diverse outputs, making them particularly well-suited for\nunpaired I2I settings. In parallel, contrastive learning provides a powerful\nframework for learning semantic similarities without the need for explicit\nsupervision or paired data. By pulling together representations of semantically\nsimilar samples and pushing apart dissimilar ones, contrastive methods are\ninherently aligned with the objectives of unpaired translation. Its ability to\nselectively enforce semantic consistency at the feature level makes contrastive\nlearning particularly effective for guiding generation in unpaired scenarios.\nIn this work, we propose a time-dependent contrastive learning approach where a\nmodel is trained with SimCLR by considering an image and its domain invarient\nfeature as a positive pair, enabling the preservation of domain-invariant\nfeatures and the discarding of domain-specific ones. The learned contrastive\nmodel then guides the inference of a pretrained SDE for the I2I translation\ntask. We empirically compare Contrastive-SDE with several baselines across\nthree common unpaired I2I tasks, using four metrics for evaluation.\nConstrastive-SDE achieves comparable results to the state-of-the-art on several\nmetrics. Furthermore, we observe that our model converges significantly faster\nand requires no label supervision or classifier training, making it a more\nefficient alternative for this task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aContrastive-SDE\u7684\u65b9\u6cd5\uff0c\u5c06\u65f6\u95f4\u4f9d\u8d56\u7684\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u65e0\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4e0eSOTA\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u65e0\u9700\u6807\u7b7e\u76d1\u7763\u3002", "motivation": "\u65e0\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u5728\u7f3a\u4e4f\u5bf9\u9f50\u6837\u672c\u65f6\u5b66\u4e60\u6620\u5c04\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5bf9\u6bd4\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u4e0b\u5b66\u4e60\u8bed\u4e49\u76f8\u4f3c\u6027\u6709\u6548\uff0c\u4f46\u5982\u4f55\u5c06\u4e24\u8005\u7ed3\u5408\u4ee5\u5728\u65e0\u914d\u5bf9\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u591a\u6837\u5316\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u751f\u6210\u662f\u7814\u7a76\u52a8\u673a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65f6\u95f4\u4f9d\u8d56\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7SimCLR\u8bad\u7ec3\u6a21\u578b\uff0c\u5c06\u56fe\u50cf\u53ca\u5176\u57df\u4e0d\u53d8\u7279\u5f81\u89c6\u4e3a\u6b63\u6837\u672c\u5bf9\uff0c\u4ece\u800c\u4fdd\u7559\u57df\u4e0d\u53d8\u7279\u5f81\u5e76\u4e22\u5f03\u57df\u7279\u5b9a\u7279\u5f81\u3002\u63a5\u7740\uff0c\u8bad\u7ec3\u597d\u7684\u5bf9\u6bd4\u6a21\u578b\u88ab\u7528\u6765\u6307\u5bfc\u9884\u8bad\u7ec3\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u5b8c\u6210I2I\u7ffb\u8bd1\u4efb\u52a1\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u89c1\u7684\u65e0\u914d\u5bf9I2I\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u56db\u79cd\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0cContrastive-SDE\u5728\u591a\u9879\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u6536\u655b\u901f\u5ea6\u663e\u8457\u52a0\u5feb\uff0c\u4e14\u65e0\u9700\u6807\u7b7e\u76d1\u7763\u6216\u5206\u7c7b\u5668\u8bad\u7ec3\u3002", "conclusion": "\u7ed3\u5408\u65f6\u95f4\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u65e0\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u76d1\u7763\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u5e76\u964d\u4f4e\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.03334", "pdf": "https://arxiv.org/pdf/2510.03334", "abs": "https://arxiv.org/abs/2510.03334", "authors": ["Zerui Wang", "Qinghao Hu", "Ana Klimovic", "Tianwei Zhang", "Yonggang Wen", "Peng Sun", "Dahua Lin"], "title": "Semantic-Aware Scheduling for GPU Clusters with Large Language Models", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Deep learning (DL) schedulers are pivotal in optimizing resource allocation\nin GPU clusters, but operate with a critical limitation: they are largely blind\nto the semantic context of the jobs they manage. This forces them to rely on\nlimited metadata, leading to high profiling overhead, unreliable duration\nestimation, inadequate failure handling, and poor observability. To this end,\nwe propose SchedMate, a framework that bridges this semantic gap by\nsystematically extracting deep insights from overlooked, unstructured data\nsources: source code, runtime logs, and historical jobs. SchedMate enhances\nexisting schedulers non-intrusively through three LLM-based components. Our\nimplementation integrates seamlessly with existing deep learning schedulers.\nEvaluations on a 128-GPU physical cluster and extensive simulations on\nproduction traces show SchedMate reduces average job completion times by up to\n1.91x, substantially enhancing the scheduling performance, demonstrating the\ncritical role of semantic-awareness in modern DL scheduling.", "AI": {"tldr": "SchedMate\u901a\u8fc7LLM\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u5668\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u96c6\u7fa4\u7684\u8c03\u5ea6\u6027\u80fd\uff0c\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u9ad8\u8fbe1.91\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u5668\u5728GPU\u96c6\u7fa4\u4e2d\u7f3a\u4e4f\u5bf9\u4f5c\u4e1a\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u5176\u4f9d\u8d56\u6709\u9650\u5143\u6570\u636e\uff0c\u4ece\u800c\u5f15\u53d1\u9ad8\u5206\u6790\u5f00\u9500\u3001\u4e0d\u53ef\u9760\u7684\u65f6\u957f\u4f30\u7b97\u3001\u4e0d\u8db3\u7684\u6545\u969c\u5904\u7406\u548c\u7cdf\u7cd5\u7684\u53ef\u89c2\u5bdf\u6027\u3002", "method": "\u63d0\u51faSchedMate\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u4ece\u88ab\u5ffd\u89c6\u7684\u975e\u7ed3\u6784\u5316\u6570\u636e\u6e90\uff08\u6e90\u4ee3\u7801\u3001\u8fd0\u884c\u65f6\u65e5\u5fd7\u548c\u5386\u53f2\u4f5c\u4e1a\uff09\u4e2d\u63d0\u53d6\u6df1\u5c42\u6d1e\u5bdf\u529b\uff0c\u5f25\u8865\u4e86\u8bed\u4e49\u9e3f\u6c9f\u3002SchedMate\u901a\u8fc7\u4e09\u4e2a\u57fa\u4e8eLLM\u7684\u7ec4\u4ef6\uff0c\u975e\u4fb5\u5165\u5f0f\u5730\u589e\u5f3a\u73b0\u6709\u8c03\u5ea6\u5668\u3002", "result": "\u5728128\u4e2aGPU\u7684\u7269\u7406\u96c6\u7fa4\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u4ee5\u53ca\u57fa\u4e8e\u751f\u4ea7\u75d5\u8ff9\u7684\u5e7f\u6cdb\u6a21\u62df\u8868\u660e\uff0cSchedMate\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe1.91\u500d\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u5728\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u4e2d\u626e\u6f14\u7740\u5173\u952e\u89d2\u8272\uff0cSchedMate\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04384", "pdf": "https://arxiv.org/pdf/2510.04384", "abs": "https://arxiv.org/abs/2510.04384", "authors": ["Adam Ballew", "Jingbo Wang", "Shaogang Ren"], "title": "LLM Based Bayesian Optimization for Prompt Search", "categories": ["cs.AI"], "comment": null, "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO-LLM\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u9ad8\u6548\u4f18\u5316LLM\u63d0\u793a\u8bcd\u4ee5\u63d0\u5347\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u65e8\u5728\u51cf\u5c11API\u8c03\u7528\u5e76\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63d0\u793a\u8bcd\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\u3001\u8c03\u7528\u6b21\u6570\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u4f18\u5316\u63d0\u793a\u8bcd\u6765\u589e\u5f3aLLM\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u6846\u67b6\u3002\u4f7f\u7528LLM\u9a71\u52a8\u7684\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\u6765\u4f30\u8ba1\u4e0d\u540c\u63d0\u793a\u8bcd\u5019\u9009\u7684\u6027\u80fd\u3002LLM\u8d1f\u8d23\u751f\u6210\u57fa\u4e8e\u79cd\u5b50\u63d0\u793a\u8bcd\u6269\u5c55\u7684\u5019\u9009\u63d0\u793a\u8bcd\u3002\u901a\u8fc7\u7ed3\u5408GP\u540e\u9a8c\u548c\u4e0a\u9650\u7f6e\u4fe1\u533a\u95f4\uff08UCB\uff09\u91c7\u96c6\u51fd\u6570\u6765\u8bc4\u4f30\u8fd9\u4e9b\u5019\u9009\u3002\u4f18\u5316\u8fc7\u7a0b\u8fed\u4ee3\u5730\u57fa\u4e8e\u6570\u636e\u5b50\u96c6\u7ec6\u5316\u63d0\u793a\u8bcd\uff0c\u5229\u7528LLM-GP\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u5e76\u51cf\u5c11API\u8c03\u7528\u3002", "result": "\u6240\u63d0\u51fa\u7684BO-LLM\u7b97\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u8be6\u7ec6\u8ba8\u8bba\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "BO-LLM\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u4f18\u5316LLM\u63d0\u793a\u8bcd\uff0c\u4ee5\u63d0\u5347\u6587\u672c\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11API\u8c03\u7528\u6b21\u6570\uff0c\u4e3aLLM\u7684\u63d0\u793a\u8bcd\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04268", "pdf": "https://arxiv.org/pdf/2510.04268", "abs": "https://arxiv.org/abs/2510.04268", "authors": ["Robin Algayres", "Charles-\u00c9ric Saint-James", "Mahi Luthra", "Jiayi Shen", "Dongyan Lin", "Youssef Benchekroun", "Rashel Moritz", "Juan Pino", "Emmanuel Dupoux"], "title": "LongTail-Swap: benchmarking language models' abilities on rare words", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Children learn to speak with a low amount of data and can be taught new words\non a few-shot basis, making them particularly data-efficient learners. The\nBabyLM challenge aims at exploring language model (LM) training in the low-data\nregime but uses metrics that concentrate on the head of the word distribution.\nHere, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the\ntail of the distribution, i.e., measures the ability of LMs to learn new words\nwith very little exposure, like infants do. LT-Swap is a pretraining\ncorpus-specific test set of acceptable versus unacceptable sentence pairs that\nisolate semantic and syntactic usage of rare words. Models are evaluated in a\nzero-shot fashion by computing the average log probabilities over the two\nmembers of each pair. We built two such test sets associated with the 10M words\nand 100M words BabyLM training sets, respectively, and evaluated 16 models from\nthe BabyLM leaderboard. Our results not only highlight the poor performance of\nlanguage models on rare words but also reveal that performance differences\nacross LM architectures are much more pronounced in the long tail than in the\nhead. This offers new insights into which architectures are better at handling\nrare word generalization. We've also made the code publicly avail", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86LongTail-Swap (LT-Swap)\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u5b66\u4e60\u7a00\u6709\u8bcd\u6c47\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5728\u7a00\u6709\u8bcd\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u67b6\u6784\u5dee\u5f02\u66f4\u4e3a\u663e\u8457\u3002", "motivation": "\u513f\u7ae5\u80fd\u4ee5\u5c11\u91cf\u6570\u636e\u5b66\u4e60\u65b0\u8bcd\uff0c\u5177\u6709\u9ad8\u6570\u636e\u6548\u7387\u3002\u73b0\u6709BabyLM\u6311\u6218\u8d5b\u7684\u6307\u6807\u96c6\u4e2d\u5728\u5e38\u89c1\u8bcd\u4e0a\uff0c\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u6570\u636e\u60c5\u5883\u4e0b\u5b66\u4e60\u7a00\u6709\u8bcd\uff08\u957f\u5c3e\u5206\u5e03\uff09\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86LongTail-Swap (LT-Swap)\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4e13\u6ce8\u4e8e\u8bcd\u6c47\u5206\u5e03\u7684\u957f\u5c3e\u90e8\u5206\u3002LT-Swap\u662f\u4e00\u4e2a\u7279\u5b9a\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u53ef\u63a5\u53d7\u548c\u4e0d\u53ef\u63a5\u53d7\u7684\u53e5\u5b50\u5bf9\uff0c\u7528\u4e8e\u9694\u79bb\u7a00\u6709\u8bcd\u7684\u8bed\u4e49\u548c\u53e5\u6cd5\u7528\u6cd5\u3002\u6a21\u578b\u901a\u8fc7\u8ba1\u7b97\u6bcf\u5bf9\u53e5\u5b50\u7684\u5e73\u5747\u5bf9\u6570\u6982\u7387\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002\u6784\u5efa\u4e86\u4e24\u4e2a\u5206\u522b\u5bf9\u5e9410M\u548c100M BabyLM\u8bad\u7ec3\u96c6\u7684\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e8616\u4e2aBabyLM\u6392\u884c\u699c\u4e0a\u7684\u6a21\u578b\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5728\u7a00\u6709\u8bcd\u4e0a\u7684\u8868\u73b0\u666e\u904d\u8f83\u5dee\u3002\u4e0e\u5e38\u89c1\u8bcd\u76f8\u6bd4\uff0c\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u5728\u957f\u5c3e\u8bcd\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "LT-Swap\u4e3a\u7406\u89e3\u54ea\u4e9b\u67b6\u6784\u66f4\u64c5\u957f\u5904\u7406\u7a00\u6709\u8bcd\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.03827", "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "categories": ["cs.CV", "cs.RO"], "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "\u73b0\u6709VLA\u57fa\u51c6\uff08\u5982LIBERO\uff09\u8bc4\u4f30\u7ed3\u679c\u865a\u9ad8\uff0c\u6a21\u578b\u8868\u73b0\u53ef\u80fd\u57fa\u4e8e\u6b7b\u8bb0\u786c\u80cc\u3002\u672c\u6587\u63d0\u51faLIBERO-PRO\uff0c\u901a\u8fc7\u5f15\u5165\u6270\u52a8\u63ed\u793a\u6a21\u578b\u7f3a\u4e4f\u6cdb\u5316\u548c\u771f\u6b63\u7406\u89e3\u80fd\u529b\uff0c\u547c\u5401\u91c7\u7528\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709Vision-Language-Action (VLA) \u6a21\u578b\u8bc4\u4f30\u57fa\u51c6LIBERO\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u7b97\u865a\u9ad8\u4e14\u963b\u788d\u516c\u5e73\u6bd4\u8f83\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4efb\u52a1\u800c\u975e\u4ec5\u4f9d\u8d56\u6b7b\u8bb0\u786c\u80cc\u3002", "method": "\u5f15\u5165LIBERO-PRO\uff0c\u4e00\u4e2a\u6269\u5c55\u7684LIBERO\u57fa\u51c6\u3002\u901a\u8fc7\u5728\u64cd\u4f5c\u5bf9\u8c61\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7cfb\u7edf\u5730\u5f15\u5165\u5408\u7406\u6270\u52a8\uff0c\u5168\u9762\u8bc4\u4f30VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6LIBERO\u4e0b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u4f46\u5728LIBERO-PRO\u7684\u6cdb\u5316\u8bbe\u7f6e\u4e0b\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u81f30.0%\u3002\u8fd9\u66b4\u9732\u4e86\u6a21\u578b\u4f9d\u8d56\u4e8e\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u52a8\u4f5c\u5e8f\u5217\u548c\u73af\u5883\u5e03\u5c40\u7684\u6b7b\u8bb0\u786c\u80cc\uff0c\u800c\u975e\u771f\u6b63\u7684\u4efb\u52a1\u7406\u89e3\u6216\u73af\u5883\u611f\u77e5\u3002", "conclusion": "\u5f53\u524dVLA\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u6a21\u578b\u7684\u6cdb\u5316\u548c\u7406\u89e3\u80fd\u529b\u3002\u547c\u5401\u793e\u533a\u653e\u5f03\u8bef\u5bfc\u6027\u65b9\u6cd5\uff0c\u8f6c\u800c\u91c7\u7528\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2510.03335", "pdf": "https://arxiv.org/pdf/2510.03335", "abs": "https://arxiv.org/abs/2510.03335", "authors": ["Ameya Daigavane", "YuQing Xie", "Bodhi P. Vani", "Saeed Saremi", "Joseph Kleinhenz", "Tess Smidt"], "title": "Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment", "categories": ["cs.LG", "eess.IV"], "comment": "under review", "summary": "Diffusion models are a popular class of generative models trained to reverse\na noising process starting from a target data distribution. Training a\ndiffusion model consists of learning how to denoise noisy samples at different\nnoise levels. When training diffusion models for point clouds such as molecules\nand proteins, there is often no canonical orientation that can be assigned. To\ncapture this symmetry, the true data samples are often augmented by\ntransforming them with random rotations sampled uniformly over $SO(3)$. Then,\nthe denoised predictions are often rotationally aligned via the Kabsch-Umeyama\nalgorithm to the ground truth samples before computing the loss. However, the\neffect of this alignment step has not been well studied. Here, we show that the\noptimal denoiser can be expressed in terms of a matrix Fisher distribution over\n$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and\nturns out to be the zeroth order approximation for small noise levels,\nexplaining its effectiveness. We build on this perspective to derive better\napproximators to the optimal denoiser in the limit of small noise. Our\nexperiments highlight that alignment is often a `good enough' approximation for\nthe noise levels that matter most for training diffusion models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u70b9\u4e91\u6269\u6563\u6a21\u578b\u4e2d\u65cb\u8f6c\u5bf9\u9f50\u6b65\u9aa4\u7684\u7406\u8bba\u57fa\u7840\u548c\u6548\u679c\u3002\u6211\u4eec\u53d1\u73b0\u5bf9\u9f50\u64cd\u4f5c\u662f\u5c0f\u566a\u58f0\u4e0b\u7684\u96f6\u9636\u8fd1\u4f3c\uff0c\u5e76\u57fa\u4e8e\u6b64\u63a8\u5bfc\u51fa\u66f4\u4f18\u7684\u8fd1\u4f3c\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5728\u5173\u952e\u566a\u58f0\u6c34\u5e73\u4e0b\u5df2\u201c\u8db3\u591f\u597d\u201d\u3002", "motivation": "\u5728\u8bad\u7ec3\u7528\u4e8e\u70b9\u4e91\uff08\u5982\u5206\u5b50\u3001\u86cb\u767d\u8d28\uff09\u7684\u6269\u6563\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u6ca1\u6709\u89c4\u8303\u7684\u671d\u5411\uff0c\u901a\u5e38\u4f1a\u901a\u8fc7\u968f\u673a\u65cb\u8f6c\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5728\u8ba1\u7b97\u635f\u5931\u524d\u5bf9\u53bb\u566a\u9884\u6d4b\u8fdb\u884c\u65cb\u8f6c\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5bf9\u9f50\u6b65\u9aa4\u7684\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u6211\u4eec\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6700\u4f18\u53bb\u566a\u5668\u53ef\u4ee5\u8868\u793a\u4e3a$SO(3)$\u4e0a\u7684\u77e9\u9635Fisher\u5206\u5e03\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u9f50\u64cd\u4f5c\u5bf9\u5e94\u4e8e\u8be5\u5206\u5e03\u7684\u4f17\u6570\u91c7\u6837\uff0c\u5e76\u4e14\u662f\u5c0f\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u96f6\u9636\u8fd1\u4f3c\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u8fd8\u57fa\u4e8e\u6b64\u89c6\u89d2\u63a8\u5bfc\u51fa\u4e86\u5728\u5c0f\u566a\u58f0\u6781\u9650\u4e0b\u66f4\u4f18\u7684\u8fd1\u4f3c\u5668\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u9f50\u6b65\u9aa4\u662f\u5c0f\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u96f6\u9636\u8fd1\u4f3c\uff0c\u89e3\u91ca\u4e86\u5176\u6709\u6548\u6027\u3002\u5c3d\u7ba1\u63d0\u51fa\u4e86\u65b0\u7684\u8fd1\u4f3c\u5668\uff0c\u4f46\u5b9e\u9a8c\u7a81\u51fa\u663e\u793a\uff0c\u5bf9\u4e8e\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6700\u91cd\u8981\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5df2\u662f\u4e00\u4e2a\u201c\u8db3\u591f\u597d\u201d\u7684\u8fd1\u4f3c\u3002", "conclusion": "\u5bf9\u9f50\u6b65\u9aa4\u5728\u70b9\u4e91\u6269\u6563\u6a21\u578b\u4e2d\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5728\u5173\u952e\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u201c\u8db3\u591f\u597d\u201d\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002\u5176\u6709\u6548\u6027\u53ef\u4ece\u77e9\u9635Fisher\u5206\u5e03\u548c\u96f6\u9636\u8fd1\u4f3c\u7684\u89d2\u5ea6\u8fdb\u884c\u7406\u8bba\u89e3\u91ca\u3002\u5c3d\u7ba1\u5b58\u5728\u7406\u8bba\u4e0a\u66f4\u4f18\u7684\u8fd1\u4f3c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5df2\u80fd\u6ee1\u8db3\u9700\u6c42\u3002"}}
{"id": "2510.04391", "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWM\uff09\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u60f3\u8c61\u7f51\u7edc\u7ed3\u6784\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u5176IWM\u4e0d\u76f8\u4f3c\u3002", "motivation": "\u63a2\u8ba8\u60f3\u8c61\u529b\u7684\u8ba1\u7b97\u76ee\u6807\uff0c\u63d0\u51fa\u60f3\u8c61\u529b\u670d\u52a1\u4e8e\u8bbf\u95ee\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWM\uff09\uff0c\u800c\u975e\u4ec5\u4e3a\u6700\u5927\u5316\u5956\u52b1\u3002\u65e8\u5728\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u7684IWM\u3002", "method": "\u91c7\u7528\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4efd\u95ee\u5377\u8bc4\u4f30\u4eba\u7c7b\u548cLLM\u7684\u60f3\u8c61\u751f\u52a8\u6027\u8bc4\u5206\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u60f3\u8c61\u7f51\u7edc\u3002", "result": "\u4eba\u7c7b\u7684\u60f3\u8c61\u7f51\u7edc\u663e\u793a\u51fa\u4e0d\u540c\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff08\u5982\u9884\u671f\u5f71\u54cd\u529b\u3001\u5f3a\u5ea6\u548c\u63a5\u8fd1\u5ea6\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u800cLLM\u7684\u60f3\u8c61\u7f51\u7edc\u7f3a\u4e4f\u805a\u7c7b\uff0c\u4e14\u5728\u4e0d\u540c\u63d0\u793a\u548c\u4f1a\u8bdd\u8bb0\u5fc6\u6761\u4ef6\u4e0b\uff0c\u5176\u4e2d\u5fc3\u6027\u5ea6\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4eba\u7c7b\u548cLLM\u667a\u80fd\u4f53\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff08IWMs\uff09\u4e4b\u95f4\u7f3a\u4e4f\u76f8\u4f3c\u6027\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u5185\u90e8\u751f\u6210\u8868\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u5728AI\u4e2d\u5f00\u53d1\u7c7b\u4eba\u60f3\u8c61\u529b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.04285", "pdf": "https://arxiv.org/pdf/2510.04285", "abs": "https://arxiv.org/abs/2510.04285", "authors": ["Karthik Viswanathan", "Sang Eon Park"], "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy", "categories": ["cs.CL", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "comment": "14 pages, 7 figures. Poster at HiLD 2025: 3rd Workshop on\n  High-dimensional Learning Dynamics", "summary": "We introduce a cumulant-expansion framework for quantifying how large\nlanguage models (LLMs) internalize higher-order statistical structure during\nnext-token prediction. By treating the softmax entropy of each layer's logit\ndistribution as a perturbation around its \"center\" distribution, we derive\nclosed-form cumulant observables that isolate successively higher-order\ncorrelations. Empirically, we track these cumulants in GPT-2 and Pythia models\non Pile-10K prompts. (i) Structured prompts exhibit a characteristic\nrise-and-plateau profile across layers, whereas token-shuffled prompts remain\nflat, revealing the dependence of the cumulant profile on meaningful context.\n(ii) During training, all cumulants increase monotonically before saturating,\ndirectly visualizing the model's progression from capturing variance to\nlearning skew, kurtosis, and higher-order statistical structures. (iii)\nMathematical prompts show distinct cumulant signatures compared to general\ntext, quantifying how models employ fundamentally different processing\nmechanisms for mathematical versus linguistic content. Together, these results\nestablish cumulant analysis as a lightweight, mathematically grounded probe of\nfeature-learning dynamics in high-dimensional neural networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u7d2f\u79ef\u91cf\u5c55\u5f00\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5185\u5316\u9ad8\u9636\u7edf\u8ba1\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\u3001\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u6f14\u53d8\u4ee5\u53ca\u5bf9\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u7684\u72ec\u7279\u5904\u7406\u673a\u5236\u3002", "motivation": "\u91cf\u5316\u5e76\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5185\u5316\u9ad8\u9636\u7edf\u8ba1\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u7d2f\u79ef\u91cf\u5c55\u5f00\u6846\u67b6\uff0c\u5c06\u6bcf\u5c42logit\u5206\u5e03\u7684softmax\u71b5\u89c6\u4e3a\u5bf9\u5176\u201c\u4e2d\u5fc3\u201d\u5206\u5e03\u7684\u6270\u52a8\uff0c\u63a8\u5bfc\u51fa\u53ef\u5206\u79bb\u9ad8\u9636\u76f8\u5173\u6027\u7684\u95ed\u5408\u5f62\u5f0f\u7d2f\u79ef\u91cf\u53ef\u89c2\u6d4b\u503c\u3002\u901a\u8fc7\u5728GPT-2\u548cPythia\u6a21\u578b\u4e0a\u4f7f\u7528Pile-10K\u63d0\u793a\uff0c\u7ecf\u9a8c\u6027\u5730\u8ddf\u8e2a\u8fd9\u4e9b\u7d2f\u79ef\u91cf\u3002", "result": "1. \u7ed3\u6784\u5316\u63d0\u793a\u5728\u5c42\u95f4\u5c55\u73b0\u51fa\u7279\u5f81\u6027\u7684\u4e0a\u5347-\u5e73\u7a33\u7d2f\u79ef\u91cf\u66f2\u7ebf\uff0c\u800c\u4e71\u5e8f\u63d0\u793a\u4fdd\u6301\u5e73\u5766\uff0c\u8868\u660e\u7d2f\u79ef\u91cf\u66f2\u7ebf\u4f9d\u8d56\u4e8e\u6709\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u30022. \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6240\u6709\u7d2f\u79ef\u91cf\u5355\u8c03\u589e\u52a0\u76f4\u81f3\u9971\u548c\uff0c\u76f4\u89c2\u5c55\u793a\u4e86\u6a21\u578b\u4ece\u6355\u83b7\u65b9\u5dee\u5230\u5b66\u4e60\u504f\u5ea6\u3001\u5cf0\u5ea6\u548c\u66f4\u9ad8\u9636\u7edf\u8ba1\u7ed3\u6784\u7684\u8fc7\u7a0b\u30023. \u6570\u5b66\u63d0\u793a\u4e0e\u4e00\u822c\u6587\u672c\u76f8\u6bd4\uff0c\u663e\u793a\u51fa\u72ec\u7279\u7684\u7d2f\u79ef\u91cf\u7279\u5f81\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u5bf9\u6570\u5b66\u548c\u8bed\u8a00\u5185\u5bb9\u91c7\u7528\u6839\u672c\u4e0d\u540c\u7684\u5904\u7406\u673a\u5236\u3002", "conclusion": "\u7d2f\u79ef\u91cf\u5206\u6790\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6709\u6570\u5b66\u4f9d\u636e\u7684\u63a2\u9488\uff0c\u7528\u4e8e\u63a2\u7d22\u9ad8\u7ef4\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2510.03840", "pdf": "https://arxiv.org/pdf/2510.03840", "abs": "https://arxiv.org/abs/2510.03840", "authors": ["Pranav Sharma", "Shivank Garg", "Durga Toshniwal"], "title": "Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models", "categories": ["cs.CV"], "comment": "ACM MM'25, MALLM Workshop", "summary": "Recent advances in image generation models have led to models that produce\nsynthetic images that are increasingly difficult for standard AI detectors to\nidentify, even though they often remain distinguishable by humans. To identify\nthis discrepancy, we introduce \\textbf{Mirage}, a curated dataset comprising a\ndiverse range of AI-generated images exhibiting visible artifacts, where\ncurrent state-of-the-art detection methods largely fail. Furthermore, we\ninvestigate whether Large Vision-Language Models (LVLMs), which are\nincreasingly employed as substitutes for human judgment in various tasks, can\nbe leveraged for explainable AI image detection. Our experiments on both Mirage\nand existing benchmark datasets demonstrate that while LVLMs are highly\neffective at detecting AI-generated images with visible artifacts, their\nperformance declines when confronted with images lacking such cues.", "AI": {"tldr": "AI\u751f\u6210\u56fe\u50cf\u65e5\u76ca\u903c\u771f\uff0c\u73b0\u6709AI\u68c0\u6d4b\u5668\u96be\u4ee5\u8bc6\u522b\uff0c\u4f46\u4eba\u7c7b\u4ecd\u53ef\u5206\u8fa8\u3002\u672c\u6587\u5f15\u5165Mirage\u6570\u636e\u96c6\u6765\u6355\u6349\u8fd9\u79cd\u5dee\u5f02\uff0c\u5e76\u53d1\u73b0\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u68c0\u6d4b\u6709\u53ef\u89c1\u4f2a\u5f71\u7684AI\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7f3a\u4e4f\u6b64\u7c7b\u7ebf\u7d22\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u7531\u4e8e\u5148\u8fdb\u7684AI\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u96be\u4ee5\u88ab\u73b0\u6709AI\u68c0\u6d4b\u5668\u8bc6\u522b\u7684\u5408\u6210\u56fe\u50cf\uff0c\u800c\u4eba\u7c7b\u4ecd\u80fd\u5206\u8fa8\uff0c\u8fd9\u5bfc\u81f4\u4e86\u68c0\u6d4b\u5668\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u7279\u522b\u662f\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u5728\u5305\u542b\u53ef\u89c1\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4fc3\u4f7f\u7814\u7a76\u66f4\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u5f15\u5165\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201cMirage\u201d\u7684\u7b56\u5c55\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u5316\u7684\u3001\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u5728\u6b64\u6570\u636e\u96c6\u4e0a largely \u5931\u8d25\u30022. \u8c03\u67e5\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u662f\u5426\u53ef\u4ee5\u7528\u4e8e\u53ef\u89e3\u91ca\u7684AI\u56fe\u50cf\u68c0\u6d4b\u30023. \u5728Mirage\u6570\u636e\u96c6\u548c\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "1. \u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u68c0\u6d4b\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u975e\u5e38\u6709\u6548\u30022. \u5f53\u56fe\u50cf\u7f3a\u4e4f\u6b64\u7c7b\u4f2a\u5f71\u7ebf\u7d22\u65f6\uff0cLVLMs\u7684\u68c0\u6d4b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "conclusion": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u68c0\u6d4b\u5e26\u6709\u660e\u663e\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u7f3a\u4e4f\u6b64\u7c7b\u89c6\u89c9\u7ebf\u7d22\u7684\u56fe\u50cf\u65f6\uff0c\u5176\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u8fd9\u8868\u660eLVLMs\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u4f5c\u4e3a\u4eba\u7c7b\u5224\u65ad\u66ff\u4ee3\u54c1\u7684\u6f5c\u529b\uff0c\u4f46\u5e76\u975e\u6240\u6709\u60c5\u51b5\u4e0b\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03339", "pdf": "https://arxiv.org/pdf/2510.03339", "abs": "https://arxiv.org/abs/2510.03339", "authors": ["Sofiane Ennadir", "Levente Z\u00f3lyomi", "Oleg Smirnov", "Tianze Wang", "John Pertoft", "Filip Cornell", "Lele Cao"], "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer models have become the dominant backbone for sequence modeling,\nleveraging self-attention to produce contextualized token representations.\nThese are typically aggregated into fixed-size vectors via pooling operations\nfor downstream tasks. While much of the literature has focused on attention\nmechanisms, the role of pooling remains underexplored despite its critical\nimpact on model behavior. In this paper, we introduce a theoretical framework\nthat rigorously characterizes the expressivity of Transformer-based models\nequipped with widely used pooling methods by deriving closed-form bounds on\ntheir representational capacity and the ability to distinguish similar inputs.\nOur analysis extends to different variations of attention formulations,\ndemonstrating that these bounds hold across diverse architectural variants. We\nempirically evaluate pooling strategies across tasks requiring both global and\nlocal contextual understanding, spanning three major modalities: computer\nvision, natural language processing, and time-series analysis. Results reveal\nconsistent trends in how pooling choices affect accuracy, sensitivity, and\noptimization behavior. Our findings unify theoretical and empirical\nperspectives, providing practical guidance for selecting or designing pooling\nmechanisms suited to specific tasks. This work positions pooling as a key\narchitectural component in Transformer models and lays the foundation for more\nprincipled model design beyond attention alone.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8de8\u6a21\u6001\u5b9e\u8bc1\u7814\u7a76\uff0c\u6df1\u5165\u63a2\u7d22\u4e86Transformer\u6a21\u578b\u4e2d\u6c60\u5316\u64cd\u4f5c\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u6c60\u5316\u673a\u5236\u7684\u9009\u62e9\u4e0e\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u6c60\u5316\u64cd\u4f5c\u5bf9Transformer\u6a21\u578b\u7684\u884c\u4e3a\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4f5c\u7528\u5728\u73b0\u6709\u6587\u732e\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7814\u7a76\u91cd\u5fc3\u5927\u591a\u96c6\u4e2d\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e0a\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u5bfc\u8868\u793a\u80fd\u529b\u548c\u533a\u5206\u76f8\u4f3c\u8f93\u5165\u7684\u95ed\u5f0f\u8fb9\u754c\uff0c\u4e25\u683c\u523b\u753b\u4e86\u914d\u5907\u5e38\u7528\u6c60\u5316\u65b9\u6cd5\u7684Transformer\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u6269\u5c55\u5230\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u516c\u5f0f\u53d8\u4f53\u3002\u540c\u65f6\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e09\u5927\u6a21\u6001\u4e2d\uff0c\u7ecf\u9a8c\u6027\u8bc4\u4f30\u4e86\u9700\u8981\u5168\u5c40\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u6c60\u5316\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u5f97\u5230\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u533a\u5206\u76f8\u4f3c\u8f93\u5165\u7684\u95ed\u5f0f\u8fb9\u754c\u3002\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86\u6c60\u5316\u9009\u62e9\u5982\u4f55\u4e00\u81f4\u5730\u5f71\u54cd\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3001\u654f\u611f\u6027\u548c\u4f18\u5316\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u7edf\u4e00\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u89c6\u89d2\uff0c\u4e3a\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u6216\u8bbe\u8ba1\u5408\u9002\u7684\u6c60\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002\u7814\u7a76\u5c06\u6c60\u5316\u5b9a\u4f4d\u4e3aTransformer\u6a21\u578b\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\uff0c\u4e3a\u8d85\u8d8a\u5355\u7eaf\u6ce8\u610f\u529b\u673a\u5236\u7684\u66f4\u539f\u5219\u6027\u6a21\u578b\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04399", "pdf": "https://arxiv.org/pdf/2510.04399", "abs": "https://arxiv.org/abs/2510.04399", "authors": ["Charles L. Wang", "Keir Dorchen", "Peter Jin"], "title": "Utility-Learning Tension in Self-Modifying Agents", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u4e2d\uff0c\u8ffd\u6c42\u5373\u65f6\u6548\u7528\u4f18\u5316\u4e0e\u4fdd\u6301\u53ef\u9760\u5b66\u4e60\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u7ed3\u6784\u6027\u77db\u76fe\u3002\u82e5\u6a21\u578b\u5bb9\u91cf\u65e0\u9650\u5236\u589e\u957f\uff0c\u6548\u7528\u9a71\u52a8\u7684\u4fee\u6539\u53ef\u80fd\u635f\u5bb3\u5b66\u4e60\u7684\u7edf\u8ba1\u524d\u63d0\uff0c\u5bfc\u81f4\u4efb\u52a1\u4e0d\u53ef\u5b66\u4e60\u3002\u4e3a\u4fdd\u6301\u5b66\u4e60\u80fd\u529b\uff0c\u8981\u6c42\u6a21\u578b\u65cf\u5bb9\u91cf\u5fc5\u987b\u6709\u754c\u3002", "motivation": "\u968f\u7740\u7cfb\u7edf\u8d8b\u5411\u8d85\u7ea7\u667a\u80fd\uff0c\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\u65e5\u76ca\u91cd\u8981\u3002\u7136\u800c\uff0c\u7406\u89e3\u8fd9\u79cd\u81ea\u6211\u4fee\u6539\u53ef\u80fd\u5e26\u6765\u7684\u5185\u5728\u51b2\u7a81\u53ca\u5176\u5bf9\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5bf9\u4e8e\u786e\u4fdd\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u7a33\u5b9a\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e94\u8f74\u5206\u89e3\u548c\u51b3\u7b56\u5c42\u5bf9\u81ea\u6211\u6539\u8fdb\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\uff0c\u5206\u79bb\u4e86\u6fc0\u52b1\u4e0e\u5b66\u4e60\u884c\u4e3a\uff0c\u5e76\u5b64\u7acb\u5206\u6790\u5404\u8f74\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc6\u522b\u4e86\u6548\u7528-\u5b66\u4e60\u5f20\u529b\uff0c\u5e76\u5229\u7528\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5bf9\u6bd4\u4e86\u7834\u574f\u6027\u6548\u7528\u7b56\u7565\u4e0e\u80fd\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\u7684\u201c\u53cc\u95e8\u7b56\u7565\u201d\u3002", "result": "\u6838\u5fc3\u53d1\u73b0\u662f\u81ea\u6211\u4fee\u6539\u7cfb\u7edf\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6548\u7528-\u5b66\u4e60\u5f20\u529b\uff1a\u8ffd\u6c42\u5373\u65f6\u6216\u9884\u671f\u6548\u7528\u7684\u6539\u53d8\u53ef\u80fd\u4fb5\u8680\u53ef\u9760\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u7edf\u8ba1\u524d\u63d0\u3002\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u5f53\u7b56\u7565\u53ef\u8fbe\u6a21\u578b\u65cf\u5bb9\u91cf\u4e00\u81f4\u6709\u754c\u65f6\uff0c\u65e0\u5206\u5e03\u5b66\u4e60\u4fdd\u8bc1\u624d\u5f97\u4ee5\u4fdd\u7559\uff1b\u5f53\u5bb9\u91cf\u53ef\u4ee5\u65e0\u9650\u589e\u957f\u65f6\uff0c\u6548\u7528\u9a71\u52a8\u7684\u81ea\u6211\u4fee\u6539\u53ef\u4f7f\u53ef\u5b66\u4e60\u4efb\u52a1\u53d8\u5f97\u4e0d\u53ef\u5b66\u4e60\u3002\u5728\u5b9e\u9645\u5e38\u7528\u5047\u8bbe\u4e0b\uff0c\u8fd9\u4e9b\u8f74\u7b80\u5316\u4e3a\u540c\u4e00\u5bb9\u91cf\u51c6\u5219\uff0c\u63d0\u4f9b\u4e86\u5b89\u5168\u81ea\u6211\u4fee\u6539\u7684\u5355\u4e00\u8fb9\u754c\u3002", "conclusion": "\u4e3a\u5b9e\u73b0\u5b89\u5168\u7684\u81ea\u6211\u4fee\u6539\uff0c\u672a\u6765\u8d85\u7ea7\u667a\u80fd\u7cfb\u7edf\u5fc5\u987b\u5728\u8ffd\u6c42\u6548\u7528\u6700\u5927\u5316\u7684\u540c\u65f6\uff0c\u4e25\u683c\u63a7\u5236\u6a21\u578b\u5bb9\u91cf\u7684\u589e\u957f\uff0c\u4ee5\u9632\u6b62\u5176\u5b66\u4e60\u80fd\u529b\u7684\u4e27\u5931\u3002\u7ef4\u6301\u6a21\u578b\u5bb9\u91cf\u6709\u754c\u662f\u4fdd\u8bc1\u7cfb\u7edf\u53ef\u5b66\u4e60\u6027\u7684\u5173\u952e\uff0c\u4e14\u63d0\u51fa\u7684\u201c\u53cc\u95e8\u7b56\u7565\u201d\u5728\u5b9e\u8df5\u4e2d\u80fd\u6709\u6548\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\u3002"}}
{"id": "2510.04286", "pdf": "https://arxiv.org/pdf/2510.04286", "abs": "https://arxiv.org/abs/2510.04286", "authors": ["Harshil Vejendla"], "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main, 8 pages, 9 figures", "summary": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a\nsparse subset of feed-forward experts. Token-level routing, however, assigns an\nentire semantic spectrum to each expert, creating capacity bottlenecks,\nload-balancing pathologies, and limited specialization. We introduce SliceMoE,\nan architecture that routes contiguous slices of a token's hidden vector. A\nd-dimensional embedding is partitioned into S slices, and for each slice, a\nlightweight shared router predicts the top-k experts. Experts operate on their\nassigned slices independently, and outputs are reassembled, maintaining\nper-token FLOP efficiency. Because slices from different tokens interleave\nwithin an expert, utilization is naturally smoother. We propose a slice-level\ncapacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.\nExperiments on WikiText-103 language modeling, WMT En-De translation, and three\ntext-classification datasets show SliceMoE attains up to 1.7x faster inference\nthan dense baselines, 12 to 18 percent lower perplexity than parameter-matched\ntoken-MoE, and improved expert balance, with interpretable expertise over\nsyntactic versus semantic subspaces.", "AI": {"tldr": "SliceMoE\u901a\u8fc7\u5bf9\u4ee4\u724c\u9690\u85cf\u5411\u91cf\u7684\u5206\u7247\u8fdb\u884c\u8def\u7531\uff0c\u800c\u975e\u6574\u4e2a\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u7684\u74f6\u9888\u548c\u8d1f\u8f7d\u4e0d\u5747\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3001\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3001\u66f4\u597d\u7684\u4e13\u5bb6\u5e73\u8861\u548c\u53ef\u89e3\u91ca\u7684\u4e13\u4e1a\u5316\u3002", "motivation": "Mixture-of-Experts (MoE) \u5c42\u7684\u4ee4\u724c\u7ea7\u8def\u7531\u5c06\u6574\u4e2a\u8bed\u4e49\u9891\u8c31\u5206\u914d\u7ed9\u6bcf\u4e2a\u4e13\u5bb6\uff0c\u5bfc\u81f4\u5bb9\u91cf\u74f6\u9888\u3001\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u4e13\u4e1a\u5316\u53d7\u9650\u3002", "method": "\u5f15\u5165SliceMoE\u67b6\u6784\uff0c\u5c06\u4ee4\u724c\u7684d\u7ef4\u5d4c\u5165\u5411\u91cf\u5206\u6210S\u4e2a\u8fde\u7eed\u5206\u7247\uff0c\u6bcf\u4e2a\u5206\u7247\u7531\u8f7b\u91cf\u7ea7\u5171\u4eab\u8def\u7531\u5668\u9884\u6d4btop-k\u4e13\u5bb6\u3002\u4e13\u5bb6\u72ec\u7acb\u5904\u7406\u5206\u914d\u7684\u5206\u7247\uff0c\u7136\u540e\u91cd\u65b0\u7ec4\u5408\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u5206\u7247\u7ea7\u5bb9\u91cf\u635f\u5931\u3001\u8de8\u5206\u7247Dropout\u548c\u9ad8\u6548\u878d\u5408\u6279\u5904\u7406GEMM\u6838\u3002", "result": "SliceMoE\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u63a8\u7406\u901f\u5ea6\u5feb1.7\u500d\uff0c\u6bd4\u53c2\u6570\u5339\u914d\u7684\u4ee4\u724cMoE\u56f0\u60d1\u5ea6\u964d\u4f4e12%\u81f318%\uff0c\u4e13\u5bb6\u5e73\u8861\u6027\u66f4\u597d\uff0c\u5e76\u5b9e\u73b0\u4e86\u8bed\u6cd5\u4e0e\u8bed\u4e49\u5b50\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u4e13\u4e1a\u5316\u3002", "conclusion": "SliceMoE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u7247\u8def\u7531\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86Transformer\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3001\u6027\u80fd\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03853", "pdf": "https://arxiv.org/pdf/2510.03853", "abs": "https://arxiv.org/abs/2510.03853", "authors": ["Rui Qian", "Xin Yin", "Chuanhang Deng", "Zhiyuan Peng", "Jian Xiong", "Wei Zhai", "Dejing Dou"], "title": "UGround: Towards Unified Visual Grounding with Unrolled Transformers", "categories": ["cs.CV"], "comment": "https://github.com/rui-qian/UGround", "summary": "We present UGround, a \\textbf{U}nified visual \\textbf{Ground}ing paradigm\nthat dynamically selects intermediate layers across \\textbf{U}nrolled\ntransformers as ``mask as prompt'', diverging from the prevailing pipeline that\nleverages the fixed last hidden layer as ``\\texttt{<SEG>} as prompt''. UGround\naddresses two primary challenges posed by the prevailing paradigm: (1) its\nreliance on the fixed last hidden layer, which sequentially amplifies\ncumulative errors arising from layer-by-layer propagation without intermediate\ncorrection, and (2) its use of \\texttt{<SEG>} as a prompt, which implicitly\nprojects textual embeddings into visual space without explicit spatial cues\n(\\eg, coordinates). Central to UGround is Policy-Prompted Masking, which\ncomprises two key components: Stochastic Skip Connection (SSC) and Mask as\nPrompt (MasP). SSC is a reinforcement learning policy that, via stochastic\nsampling, allows each \\texttt{<SEG>} token to slide across unrolled transformer\nlayers, enabling dynamic layer selection at which it connects to the vision\nmodel (\\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,\nMasP uses the similarity map derived from the \\texttt{<SEG>} token and image\ntokens as a soft logit mask to prompt SAM for mask generation, offering\nexplicit spatial cues through its activation regions. To validate the\neffectiveness of UGround, we, for the first time, have unified visual grounding\nwithin a single framework from an attribute perspective, spanning from\ntraditional refer expression segmentation to newly proposed reasoning\nsegmentation, single-target to multi-target, positive query to false premise\n(empty target). All codes and models are publicly available at\n\\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.", "AI": {"tldr": "UGround\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9Transformer\u4e2d\u95f4\u5c42\u4f5c\u4e3a\u201c\u63a9\u7801\u5373\u63d0\u793a\u201d\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bef\u5dee\u7d2f\u79ef\u548c\u7f3a\u4e4f\u660e\u786e\u7a7a\u95f4\u63d0\u793a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u4f9d\u8d56\u56fa\u5b9a\u7684\u6700\u540e\u4e00\u5c42\u9690\u85cf\u5c42\uff0c\u5bfc\u81f4\u8bef\u5dee\u9010\u5c42\u7d2f\u79ef\u4e14\u65e0\u6cd5\u4e2d\u95f4\u6821\u6b63\uff1b2) \u4f7f\u7528`<SEG>`\u4f5c\u4e3a\u63d0\u793a\uff0c\u9690\u5f0f\u5730\u5c06\u6587\u672c\u5d4c\u5165\u6295\u5c04\u5230\u89c6\u89c9\u7a7a\u95f4\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u3002", "method": "\u6838\u5fc3\u662fPolicy-Prompted Masking\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1) **\u968f\u673a\u8df3\u8dc3\u8fde\u63a5\uff08SSC\uff09**\uff0c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u52a8\u6001\u9009\u62e9`<SEG>` token\u8fde\u63a5\u89c6\u89c9\u6a21\u578b\uff08\u5982SAM\uff09\u7684Transformer\u4e2d\u95f4\u5c42\uff1b2) **\u63a9\u7801\u5373\u63d0\u793a\uff08MasP\uff09**\uff0c\u5229\u7528`<SEG>` token\u548c\u56fe\u50cftoken\u7684\u76f8\u4f3c\u6027\u56fe\u4f5c\u4e3a\u8f6flogit\u63a9\u7801\uff0c\u4e3aSAM\u63d0\u4f9b\u660e\u786e\u7684\u7a7a\u95f4\u63d0\u793a\u4ee5\u751f\u6210\u63a9\u7801\u3002", "result": "UGround\u9996\u6b21\u4ece\u5c5e\u6027\u89d2\u5ea6\u5c06\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7edf\u4e00\u5230\u5355\u4e00\u6846\u67b6\u4e2d\u3002\u5b83\u80fd\u5904\u7406\u4ece\u4f20\u7edf\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u5230\u63a8\u7406\u5206\u5272\u3001\u5355\u76ee\u6807\u5230\u591a\u76ee\u6807\u3001\u6b63\u5411\u67e5\u8be2\u5230\u9519\u8bef\u524d\u63d0\uff08\u7a7a\u76ee\u6807\uff09\u7b49\u591a\u79cd\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "UGround\u901a\u8fc7\u5176\u521b\u65b0\u7684\u52a8\u6001\u5c42\u9009\u62e9\u548c\u660e\u786e\u7a7a\u95f4\u63d0\u793a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u9c81\u68d2\u4e14\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u5b9a\u4f4d\u4efb\u52a1\u7684\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\u3002"}}
{"id": "2510.03340", "pdf": "https://arxiv.org/pdf/2510.03340", "abs": "https://arxiv.org/abs/2510.03340", "authors": ["Marian Chen", "Miri Zilka"], "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.PE"], "comment": null, "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies\nthat balance disease containment with socioeconomic stability. We approach this\nchallenge by designing a framework for modeling and evaluating disease-spread\nprevention strategies. Our framework leverages multi-objective reinforcement\nlearning (MORL) - a formulation necessitated by competing objectives - combined\nwith a new stochastic differential equation (SDE) pandemic simulator,\ncalibrated and validated against global COVID-19 data. Our simulator reproduces\nnational-scale pandemic dynamics with orders of magnitude higher fidelity than\nother models commonly used in reinforcement learning (RL) approaches to\npandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on\nthis simulator, we illustrate the direct policy trade-offs between\nepidemiological control and economic stability for COVID-19. Furthermore, we\ndemonstrate the framework's generality by extending it to pathogens with\ndifferent epidemiological profiles, such as polio and influenza, and show how\nthese profiles lead the agent to discover fundamentally different intervention\npolicies. To ground our work in contemporary policymaking challenges, we apply\nthe model to measles outbreaks, quantifying how a modest 5% drop in vaccination\ncoverage necessitates significantly more stringent and costly interventions to\ncurb disease spread. This work provides a robust and adaptable framework to\nsupport transparent, evidence-based policymaking for mitigating public health\ncrises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u548c\u65b0\u578bSDE\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5e73\u8861\u75be\u75c5\u63a7\u5236\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u7684\u6d41\u884c\u75c5\u5e72\u9884\u7b56\u7565\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eCOVID-19\u53ca\u5176\u4ed6\u75c5\u539f\u4f53\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u51f8\u663e\u4e86\u5728\u75be\u75c5\u904f\u5236\u548c\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u5e72\u9884\u7b56\u7565\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5efa\u6a21\u548c\u8bc4\u4f30\u75be\u75c5\u4f20\u64ad\u9884\u9632\u7b56\u7565\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u4e0e\u4e00\u4e2a\u65b0\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u5927\u6d41\u884c\u6a21\u62df\u5668\u3002\u8be5\u6a21\u62df\u5668\u5df2\u6839\u636e\u5168\u7403COVID-19\u6570\u636e\u8fdb\u884c\u6821\u51c6\u548c\u9a8c\u8bc1\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2aPareto-Conditioned Network (PCN) \u4ee3\u7406\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86COVID-19\u6d41\u884c\u75c5\u5b66\u63a7\u5236\u4e0e\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u7684\u76f4\u63a5\u653f\u7b56\u6743\u8861\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u7684\u901a\u7528\u6027\u901a\u8fc7\u5c06\u5176\u6269\u5c55\u5230\u810a\u9ad3\u7070\u8d28\u708e\u548c\u6d41\u611f\u7b49\u4e0d\u540c\u6d41\u884c\u75c5\u5b66\u7279\u5f81\u7684\u75c5\u539f\u4f53\u5f97\u5230\u8bc1\u5b9e\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u5e72\u9884\u653f\u7b56\u3002\u6a21\u578b\u8fd8\u5e94\u7528\u4e8e\u9ebb\u75b9\u7206\u53d1\uff0c\u91cf\u5316\u4e86\u75ab\u82d7\u63a5\u79cd\u8986\u76d6\u7387\u4e0b\u964d5%\u5bf9\u6240\u9700\u5e72\u9884\u63aa\u65bd\u4e25\u683c\u6027\u548c\u6210\u672c\u7684\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u53ef\u652f\u6301\u900f\u660e\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u5236\u5b9a\uff0c\u4ee5\u7f13\u89e3\u516c\u5171\u536b\u751f\u5371\u673a\u3002"}}
{"id": "2510.04474", "pdf": "https://arxiv.org/pdf/2510.04474", "abs": "https://arxiv.org/abs/2510.04474", "authors": ["Gang Li", "Yan Chen", "Ming Lin", "Tianbao Yang"], "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89e3\u8026\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff08DRPO\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002DRPO\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u4e0e\u4e0d\u6b63\u786e\u63a8\u7406\u7684\u957f\u5ea6\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u63a8\u7406\u8def\u5f84\uff0c\u540c\u65f6\u5c06\u6027\u80fd\u635f\u5931\u964d\u81f3\u6700\u4f4e\uff08\u4f8b\u5982\uff0c\u5728GSM8k\u4e0a\u5b9e\u73b077%\u957f\u5ea6\u7f29\u51cf\uff0c\u4ec51.1%\u6027\u80fd\u635f\u5931\uff09\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u8fc7\u5ea6\u601d\u8003\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u95ee\u9898\u4e5f\u751f\u6210\u5197\u957f\u4e14\u5197\u4f59\u7684\u63a8\u7406\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u548c\u54cd\u5e94\u5ef6\u8fdf\u5927\u5e45\u589e\u52a0\u3002\u73b0\u6709\u957f\u5ea6\u5956\u52b1\u65b9\u6cd5\uff08\u5982GRPO\uff09\u867d\u7136\u8bd5\u56fe\u7f29\u77ed\u63a8\u7406\uff0c\u5374\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u56e0\u4e3aGRPO\u7684\u4f18\u52bf\u51fd\u6570\u9519\u8bef\u5730\u60e9\u7f5a\u4e86\u6b63\u786e\u4f46\u8f83\u957f\u7684\u63a8\u7406\uff0c\u4ece\u800c\u6291\u5236\u4e86\u6709\u6548\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u89e3\u8026\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff08DRPO\uff09\u6846\u67b6\u3002DRPO\u7684\u6838\u5fc3\u5728\u4e8e\u5c06\u6b63\u786e\u4e0e\u4e0d\u6b63\u786e\u63a8\u7406\u7684\u57fa\u4e8e\u957f\u5ea6\u7684\u5b66\u4e60\u4fe1\u53f7\u89e3\u8026\u3002\u5b83\u786e\u4fdd\u6b63\u786e\u63a8\u7406\u7684\u5956\u52b1\u4fe1\u53f7\u4ec5\u5728\u6b63\u6837\u672c\u7ec4\u5185\u8fdb\u884c\u89c4\u8303\u5316\uff0c\u4e0d\u53d7\u8d1f\u6837\u672c\u7684\u5e72\u6270\u3002DRPO\u7684\u76ee\u6807\u662f\u5c06\u4e00\u4e2a\u4f18\u5316\u7684\u6b63\u6570\u636e\u5206\u5e03\uff08\u5728KL\u6b63\u5219\u5316\u4e0b\u6700\u5927\u5316\u57fa\u4e8e\u957f\u5ea6\u7684\u5956\u52b1\uff09\u6574\u5408\u5230\u4e00\u4e2a\u5224\u522b\u76ee\u6807\u4e2d\u3002\u8be5\u65b9\u6cd5\u5bfc\u51fa\u4e86\u6b64\u5206\u5e03\u7684\u95ed\u5f0f\u89e3\uff0c\u5141\u8bb8\u4ec5\u4f7f\u7528\u5728\u7ebf\u7b56\u7565\u6570\u636e\u548c\u91cd\u8981\u6027\u6743\u91cd\u8fdb\u884c\u9ad8\u6548\u8ba1\u7b97\u3002\u8be5\u516c\u5f0f\u8fd8\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u4ee5\u7eb3\u5165\u9664\u4e86\u957f\u5ea6\u4e4b\u5916\u7684\u5176\u4ed6\u6b63\u6570\u636e\u504f\u597d\u5956\u52b1\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRPO\u663e\u8457\u4f18\u4e8e\u516d\u4e2a\u57fa\u7ebf\u63a8\u7406\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4f7f\u75281.5B\u6a21\u578b\uff0cDRPO\u5728GSM8k\u7b49\u7b80\u5355\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e8677%\u7684\u957f\u5ea6\u7f29\u51cf\uff0c\u800c\u6027\u80fd\u635f\u5931\u4ec5\u4e3a1.1%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u5728\u727a\u72724.3%\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u4ec5\u8fbe\u523068%\u7684\u957f\u5ea6\u7f29\u51cf\u3002", "conclusion": "DRPO\u6210\u529f\u89e3\u51b3\u4e86LRMs\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u7684\u63a8\u7406\u5197\u957f\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5956\u52b1\u89e3\u8026\u673a\u5236\uff0c\u5728\u663e\u8457\u63d0\u5347\u63a8\u7406\u7b80\u6d01\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u907f\u514d\u4e86\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u504f\u597d\u5956\u52b1\u3002"}}
{"id": "2510.04291", "pdf": "https://arxiv.org/pdf/2510.04291", "abs": "https://arxiv.org/abs/2510.04291", "authors": ["Mehrzad Tareh", "Aydin Mohandesi", "Ebrahim Ansari"], "title": "PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages", "summary": "Sentiment analysis is a key task in Natural Language Processing (NLP),\nenabling the extraction of meaningful insights from user opinions across\nvarious domains. However, performing sentiment analysis in Persian remains\nchallenging due to the scarcity of labeled datasets, limited preprocessing\ntools, and the lack of high-quality embeddings and feature extraction methods.\nTo address these limitations, we propose a hybrid approach that integrates\nmachine learning (ML) and deep learning (DL) techniques for Persian\naspect-based sentiment analysis (ABSA). In particular, we utilize polarity\nscores from multilingual BERT as additional features and incorporate them into\na decision tree classifier, achieving an accuracy of 93.34%-surpassing existing\nbenchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian\nsynonym and entity dictionary, a novel linguistic resource that supports text\naugmentation through synonym and named entity replacement. Our results\ndemonstrate the effectiveness of hybrid modeling and feature augmentation in\nadvancing sentiment analysis for low-resource languages such as Persian.", "AI": {"tldr": "\u9488\u5bf9\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u9762\u4e34\u7684\u6570\u636e\u548c\u5de5\u5177\u7a00\u7f3a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u8bed\u8a00BERT\u6781\u6027\u5206\u6570\u4f5c\u4e3a\u7279\u5f81\u5e76\u7ed3\u5408\u51b3\u7b56\u6811\u5206\u7c7b\u5668\uff0c\u5728Pars-ABSA\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.34%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u6ce2\u65af\u8bed\u540c\u4e49\u8bcd\u548c\u5b9e\u4f53\u8bcd\u5178\u8fdb\u884c\u6587\u672c\u589e\u5f3a\u3002", "motivation": "\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u9762\u4e34\u6311\u6218\uff0c\u539f\u56e0\u662f\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u9884\u5904\u7406\u5de5\u5177\u6709\u9650\u4ee5\u53ca\u9ad8\u8d28\u91cf\u5d4c\u5165\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6ce2\u65af\u8bed\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u3002\u5177\u4f53\u5730\uff0c\u5229\u7528\u591a\u8bed\u8a00BERT\u7684\u6781\u6027\u5206\u6570\u4f5c\u4e3a\u9644\u52a0\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u51b3\u7b56\u6811\u5206\u7c7b\u5668\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u6ce2\u65af\u8bed\u540c\u4e49\u8bcd\u548c\u5b9e\u4f53\u8bcd\u5178\uff0c\u901a\u8fc7\u540c\u4e49\u8bcd\u548c\u547d\u540d\u5b9e\u4f53\u66ff\u6362\u652f\u6301\u6587\u672c\u589e\u5f3a\u3002", "result": "\u5728Pars-ABSA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.34%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u3002\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u6df7\u5408\u5efa\u6a21\u548c\u7279\u5f81\u589e\u5f3a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408\u5efa\u6a21\u548c\u7279\u5f81\u589e\u5f3a\u5728\u63a8\u8fdb\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
