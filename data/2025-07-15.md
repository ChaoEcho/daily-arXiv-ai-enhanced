<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.NI](#cs.NI) [Total: 14]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 提出Spatial ModernBERT模型，结合空间嵌入和多头token分类，用于从金融文档中高精度提取表格和键值对。


<details>
  <summary>Details</summary>
Motivation: 从金融文档中提取表格和键值对对于审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 引入基于Transformer并集成空间嵌入的Spatial ModernBERT模型。将提取任务建模为三头（标签、列、行）的token分类。模型在PubTables-1M上预训练，并在金融文档数据集上微调。采用后处理方法，通过B-I-IB标记合并token，重建表格布局并提取键值对。

Result: 实验证明，Spatial ModernBERT能有效利用文本和空间信息，在真实金融文档中实现高精度的表格及键值对提取。

Conclusion: Spatial ModernBERT为金融文档中复杂表格和键值对的提取提供了高效且准确的解决方案，有望显著提升相关业务流程的自动化水平。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: 本文提出了SEALGuard，一种多语言安全护栏，旨在弥补现有大语言模型（LLM）护栏在处理多语言不安全输入方面的不足，确保LLM系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏（如LlamaGuard）在处理英语不安全输入方面表现良好，但在多语言（尤其是低资源语言）不安全输入上效果不佳，导致LLM系统易受攻击。

Method: 研究通过低秩适应（LoRA）技术，将通用多语言模型改编为多语言安全护栏。同时，构建了一个包含超过26万条提示（涵盖安全、不安全和越狱情景）的SEALSBench多语言安全对齐数据集，用于模型评估。

Result: 实验结果表明，LlamaGuard在多语言不安全和越狱提示上的防御成功率（DSR）分别下降9%和18%。相比之下，SEALGuard在检测多语言不安全和越狱提示方面显著优于现有护栏，将DSR提高了48%，并取得了最佳的DSR、精度和F1分数。

Conclusion: SEALGuard通过引入有效的多语言护栏，显著提升了LLM系统的安全对齐能力，解决了多语言环境下的安全漏洞问题。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 该研究评估了医学问答中大型语言模型（LLM）的现有数据集质量，发现其普遍缺乏临床真实性与透明度。结论强调需要建立标准化的、协作的评估框架和更严谨的数据集。


<details>
  <summary>Details</summary>
Motivation: 评估当前大型语言模型（LLM）在医学问答方面的局限性，特别是关注用于其评估的数据集质量。

Method: 审查了广泛使用的基准数据集（如MedQA、MedMCQA、PubMedQA、MMLU），以评估其严谨性、透明度和与临床场景的相关性；同时分析了医学期刊中的挑战问题作为潜在的无偏评估工具。

Result: 大多数现有数据集缺乏临床真实性、透明度和鲁棒的验证过程。公开的挑战问题虽有益处，但受限于规模小、范围窄，且可能已暴露于LLM训练中。这些不足凸显了对安全、全面和有代表性数据集的需求。

Conclusion: 建立一个标准化的医学LLM评估框架至关重要。机构和政策制定者之间需要协作努力，以确保数据集和方法是严谨、无偏见并能反映临床复杂性。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文介绍了两个针对韩国工业领域的专家级韩语LLM评估基准：KMMLU-Redux（基于技术资格考试）和KMMLU-Pro（基于专业许可考试），旨在更有效地评估LLM在实际场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）评估基准未能充分涵盖工业领域，导致难以有效评估LLM在实际应用场景中的表现。因此，需要开发更全面、包含工业知识的基准。

Method: 引入了两个韩国专家级评估基准：1. KMMLU-Redux：在现有KMMLU基础上重建，包含韩国国家技术资格考试题目，并修正了关键错误以提高可靠性。2. KMMLU-Pro：基于韩国国家专业许可考试，旨在反映韩国的专业知识。

Result: 实验结果表明，这些新基准能够全面代表韩国的工业知识。数据集已公开发布。

Conclusion: 论文成功构建并发布了两个可靠的韩国专家级评估基准，有效解决了当前LLM评估在工业领域覆盖不足的问题，为评估LLM在真实世界场景中的适用性提供了重要工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: 本文提出SIMS，首个无需外部监督的自改进模型引导框架，通过迭代自改进生成和优化对比样本，显著提升了LLM的引导效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有模型引导方法过度依赖外部标注数据，限制了其在不同情境下的适应性，且效果受标注质量影响。

Method: SIMS通过迭代自改进周期自主生成并优化对比样本，实现自适应、上下文特定的模型引导。此外，还采用提示词排序和对比采样等新策略。

Result: 在多种LLM和基准测试上的广泛评估表明，SIMS在引导效果和适应性方面显著优于现有方法。

Conclusion: 自改进模型引导为LLM推理时对齐的未来研究提供了一个有前景的方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究发现电子健康记录（EHR）中存在污名化语言，其在历史上受污名化患者群体中出现率更高，并由多种医护人员类型（如护士、社工）传播。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）是医护团队中患者污名化得以延续的关键媒介，本研究旨在探究其具体表现及影响因素。

Method: 研究通过扩展词典匹配和监督学习分类器，在MIMIC-III电子健康记录中识别了怀疑标记和污名化标签的语言特征。随后使用泊松回归模型评估了这些语言特征出现率的预测因子。

Result: 研究发现，在非裔美国患者（风险比RR: 1.16）、拥有医疗保险/医疗补助或政府运营保险的患者（RR: 2.46）、自费患者（RR: 2.12），以及患有各种污名化疾病和精神健康状况的患者中，每份病历中的污名化标签使用率更高。怀疑标记的模式类似，但男性患者的怀疑标记率更高（RR: 1.25）。此外，护士（RR: 1.40）和社会工作者（RR: 2.25）更多地使用了污名化标签，怀疑标记也呈现类似模式。

Conclusion: 污名化语言在历史上受污名化患者群体中出现率更高，且由多种医护人员类型（包括护士和社会工作者）持续传播。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 本研究发现，个体的心像能力（imagery spectrum）差异显著影响Ganzflicker诱导的视觉幻觉内容：心像能力强者描述复杂自然图像，弱者则报告简单几何图案。


<details>
  <summary>Details</summary>
Motivation: 探索个体心像能力（从无心像到生动心像）的差异是否会影响Ganzflicker诱导的视觉幻觉的复杂性和内容，从而理解内部生成视觉经验的个体差异。

Method: 收集了超过4000名参与者对Ganzflicker诱导幻觉的自由文本描述，并使用自然语言处理工具进行分析。研究对比了视觉语言模型嵌入与纯文本语言模型在捕捉差异方面的效果。

Result: 心像能力强的参与者描述了复杂、自然主义的幻觉内容，而心像能力弱的参与者报告了简单的几何图案。视觉语言模型嵌入比纯文本语言模型能更好地捕捉这些差异，且心像能力强的参与者使用的语言具有更丰富的感官运动关联。

Conclusion: 这些发现可能反映了早期视觉区域与与心像谱相关的高阶区域之间协调的个体差异。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一个将预训练Transformer大语言模型转化为灵活、亚二次复杂度架构的线性化框架，旨在实现无限上下文生成并保持高性能。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在处理长上下文时面临严重的内存和计算瓶颈，这源于softmax注意力的二次复杂度和不断增大的KV缓存。

Method: Lizard引入了一种亚二次注意力机制来近似softmax注意力，并结合了受SOTA线性模型启发而设计的门控模块，以实现自适应内存控制和恒定内存推理。它采用混合机制，融合了门控线性注意力（全局上下文压缩）和增强元记忆的滑动窗口注意力（局部交互）。此外，还引入了硬件感知算法以加速训练。

Result: Lizard在标准语言建模任务上实现了对教师模型性能的近乎无损恢复，并显著优于之前的线性化方法。在5-shot MMLU基准测试中，性能比现有模型提高了18分，并在联想召回任务上也有显著改进。

Conclusion: Lizard成功地通过创新的线性化和混合注意力机制，解决了Transformer LLM的长上下文瓶颈，实现了高效的无限上下文生成，并在性能上超越了现有线性化方法。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统通过提示词实现大语言模型（LLM）决策助手的动态个性化对齐，支持细粒度属性配置和多类型分析，并已开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为决策辅助工具时，需要适应用户多样化的价值观和偏好。现有LLM工具主要侧重于基准测试，缺乏对LLM决策者进行动态对齐和个性化的新方法。

Method: 本文提出ALIGN系统，通过基于提示词的方法将LLM决策者与细粒度属性对齐，实现动态个性化。系统具备鲁棒的配置管理、带推理的结构化输出生成、可切换LLM骨干的算法实现，以及一个支持LLM并排定性比较和模块化后端的用户界面。此外，还进行了定量分析，比较了人口统计学对齐（公共意见调查）和价值对齐（医疗分诊决策）两种不同领域的对齐方法。

Result: ALIGN系统成功实现了LLM决策者的动态个性化对齐，并支持定性与定量分析。它能够对不同领域（如人口统计学和价值观）的对齐方法进行比较研究。该框架开源，为相关研究提供了工具。

Conclusion: ALIGN开源框架将推动关于可靠、负责任和个性化LLM决策助手的进一步研究，解决用户多样化需求下的LLM对齐与个性化挑战。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本研究引入了大规模代码推理数据集OpenCodeReasoning-II，并采用两阶段微调策略，显著提升了LLM在代码生成和评论方面的性能，同时扩展了LiveCodeBench以支持C++评估。


<details>
  <summary>Details</summary>
Motivation: 推理型大型语言模型（LLMs）在代码生成和评论方面潜力巨大，但其进展受限于缺乏大规模、高质量的数据集。

Method: 1. 构建了OpenCodeReasoning-II数据集，包含2.5M问答评论三元组（约35K独立编程问题）。
2. 采用两阶段监督微调策略：第一阶段专注于代码生成微调；第二阶段联合训练代码生成和评论模型。
3. 基于Qwen2.5-Instruct模型进行微调。
4. 扩展LiveCodeBench基准测试以支持C++编程语言。

Result: 1. 微调后的Qwen2.5-Instruct模型在代码生成方面性能超越或持平现有最佳开源蒸馏模型。
2. 代码生成和评论模型的集成显著提升了竞技编程性能。
3. LiveCodeBench基准测试扩展支持C++，促进了更全面的LLM评估。

Conclusion: 通过提供大规模高质量数据集和有效的两阶段微调策略，本研究显著提升了LLMs在代码生成和评论任务上的表现，并为未来的LLM评估提供了更全面的工具。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 针对语音大语言模型（SLLM）在长音频情感识别中上下文窗口受限的问题，本文提出动态参数记忆（DPM）机制。DPM通过LoRA模块动态记忆句子级上下文语义和情感，使SLLM能处理无限长音频。在IEMOCAP数据集上，DPM显著提升了SLLM的情感识别能力，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型（SLLM）应用于语音情感识别（SER）时，因语音模态的高帧率，其有限的上下文窗口严重限制了对长音频的处理能力。此外，SLLM的输入token压缩方法忽略了情感在多轮对话中的连续性和惯性。

Method: 本文提出动态参数记忆（Dynamic Parameter Memory, DPM）机制。DPM在推理过程中，将句子级信息和情感渐进式编码到临时的LoRA模块中，从而有效“记忆”上下文信息。研究者训练了一个情感SLLM作为骨干模型，并在对话情感识别（ERC）的推理阶段整合了DPM。

Result: 在IEMOCAP数据集上的实验结果表明，DPM显著提升了SLLM在处理长音频序列时的情感识别能力。该方法取得了最先进（State-of-the-Art, SOTA）的性能。

Conclusion: DPM机制有效解决了SLLM在处理长音频情感识别时上下文窗口的限制，通过动态记忆上下文语义和情感，显著提升了SLLM的性能，并在对话情感识别任务中实现了SOTA表现。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出CompassJudger-2，一个通用的LLM判别模型，通过任务驱动、多领域数据策略和改进学习目标，克服了现有判别模型专业化狭窄和鲁棒性不足的局限。该模型在多个基准测试中表现出色，并提出JudgerBenchV2作为新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 当前作为判别的LLM模型存在专业化狭窄和鲁棒性不足的问题，限制了它们进行全面评估的能力。

Method: 提出了CompassJudger-2，一个通用的判别模型。方法包括：1. 采用任务驱动、多领域数据策展策略。2. 通过可验证奖励监督判别任务，并利用拒绝采样引导内在批判性推理。3. 引入了带有边际策略梯度损失的改进学习目标。此外，还提出了JudgerBenchV2，一个全面的跨领域判别准确性和排名一致性评估基准。

Result: CompassJudger-2在多个判别和奖励基准测试中取得了卓越的成果。其7B模型展现出与DeepSeek-V3和Qwen3-235B-A22B等大模型相当的判别准确性。

Conclusion: 这些贡献推动了LLM判别的鲁棒性和可扩展性，并建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD是一种针对晶体学问答的开卷系统，它利用GPT-4.5生成简洁、领域特定的辅助内容，以帮助小模型理解X射线衍射概念。该系统在专家级XRD问题上显著提高了模型的准确性，尤其对于预训练较少的模型。


<details>
  <summary>Details</summary>
Motivation: 传统使用扫描教科书可能存在版权问题。此外，为了帮助小型语言模型更好地理解和推理晶体学领域的专业概念，尤其是在X射线衍射（XRD）方面，需要一种有效的方法来弥补它们的知识空白。

Method: 该工作提出了OPENXRD，一个结合文本提示和GPT-4.5生成的简洁支持内容的开卷式管道。该系统不使用扫描教科书，而是生成紧凑、领域特定的参考资料。研究团队在217个专家级XRD问题集上评估了OPENXRD，比较了GPT-4和LLaVA系列模型（如Mistral, LLaMA, QWEN）在闭卷和开卷条件下的性能。

Result: 实验结果显示，使用GPT-4.5生成摘要的模型，特别是那些在晶体学方面预训练较少的模型，准确性显著提高。这表明AI生成的文本能够帮助小模型在科学任务中更有效地进行推理。

Conclusion: OPENXRD成功利用大型模型的知识填补了晶体学领域的知识空白。研究证明了专门的开卷系统在材料科学领域的实用性，并为关键科学领域中更广泛的自然语言处理（NLP）工具奠定了基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 针对战略对话中高度不平衡的欺骗检测问题，提出轻量级PU-Lie模型，结合冻结BERT嵌入、语言和游戏特征与正例-无标记（PU）学习，实现宏观F1新高并显著减少参数。


<details>
  <summary>Details</summary>
Motivation: 战略对话中的欺骗检测因语言微妙性及欺骗与真实消息之间极端类别不平衡（欺骗消息少于5%）而复杂且风险高。传统二元分类器不适用仅少数欺骗消息被标记的情况，而准确检测欺骗比识别真实消息更关键。

Method: 提出名为PU-Lie的轻量级模型，它结合了冻结的BERT嵌入、可解释的语言和游戏特定特征，以及一个正例-无标记（PU）学习目标。该模型专为处理只有少量欺骗消息被标记而大多数未被标记的情况而设计。

Result: 模型在Diplomacy数据集上实现了0.60的宏观F1新最佳成绩，同时将可训练参数减少了超过650倍。通过对七个模型的综合评估和消融研究，证明了PU学习、语言可解释性和说话者感知表示的价值。

Conclusion: 在此问题设置中，准确检测欺骗比识别真实消息更关键。PU学习能够显式地对稀有但至关重要的欺骗类别进行建模，这表明PU学习、语言可解释性和说话者感知表示在处理高度类别不平衡的欺骗检测任务中具有重要价值。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 本文介绍了一种名为RAMA的新型检索增强多智能体框架，用于验证多媒体错误信息。该框架通过战略性查询制定、交叉验证证据聚合和多智能体集成架构，在基准数据集上取得了卓越性能，尤其擅长处理模糊或难以置信的声明。


<details>
  <summary>Details</summary>
Motivation: 多模态错误信息的迅速扩散对自动化事实核查系统构成了重大挑战，尤其当声明模糊不清或缺乏足够上下文时。

Method: 本文提出了RAMA，一个新颖的检索增强多智能体框架，用于验证多媒体错误信息。其核心创新包括：1) 将多模态声明转化为精确网络搜索查询的战略性查询制定；2) 从多样化、权威来源聚合交叉验证证据；3) 利用多个多模态大型语言模型和提示变体互补优势的多智能体集成架构。

Result: 广泛的实验表明，RAMA在基准数据集上取得了卓越的性能，尤其擅长通过将验证基于检索到的事实证据来解决模糊或难以置信的声明。

Conclusion: 研究结果强调了整合基于网络的证据和多智能体推理对于可信多媒体验证的必要性，为更可靠和可扩展的事实核查解决方案铺平了道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 本研究提出一种基于剪枝的微调方法，通过识别并移除大型语言模型中与数据集特定机制相关的神经元，以增强其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常学习到特定数据集的机制，导致高置信度但缺乏泛化性的预测，并在面对新任务或分布时性能下降，因此需要一种方法来提升模型的泛化能力。

Method: 引入一种微调方法，旨在识别并剪枝Transformer模型中与数据集特定机制相关的神经元。该方法利用集成梯度（Integrated Gradients）量化每个神经元对高置信度预测的影响，以找出那些过度依赖数据集特定性能而非稳健、可迁移推理的神经元。选择性剪枝这些神经元迫使模型依赖可泛化的表征。

Result: 在多项选择基准测试中，所提出的基于剪枝的微调方法显著提升了模型性能，并超越了以往的（非剪枝）适应方法。

Conclusion: 通过剪枝大型语言模型中与数据集特定机制相关的神经元，可以有效提升模型的泛化能力，为模型适应和性能提升提供了一种优于传统方法的策略。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 该研究通过构建迄今最大的藏语预训练语料库，训练并推出了多语言大模型Banzhida，在各项藏语任务中表现显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 藏语作为一种低资源语言，因高质量训练语料稀缺而在现有大语言模型中代表性不足，存在性能鸿沟。

Method: 1. 聚合多源数据并应用专用清洗流程，构建了迄今最大的藏语预训练语料库。2. 利用该语料库，对一个多语言基础模型进行持续预训练/后训练，得到Banzhida。3. 创建新的高质量藏语基准测试集，并结合现有公共基准评估模型性能。

Result: Banzhida在广泛的藏语任务中，持续且显著优于同等规模的开源模型和现有藏语专用模型。

Conclusion: Banzhida成功提升了藏语生成式AI的能力，为低资源语言的大模型发展提供了有效解决方案和强劲表现。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本研究创建了一个气候变化视觉隐喻图像数据库（MetaClimage），并评估了其传播效果。结果显示视觉隐喻理解难度较高但更具美感，虽在效能和唤起性上与字面图像无显著差异，但能促进更深层次的认知加工和更积极的体验，为环境传播策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 气候变化视觉隐喻被认为是应对环境挑战的重要工具，但由于材料分散，少有研究系统地检验其对传播的影响。

Method: 本研究构建了包含气候变化视觉隐喻和字面图像的MetaClimage数据库，并收集了人类对其理解难度、传播效能、艺术质量、情绪唤起及总结标签的评分。通过自然语言处理技术，进一步从标签中提取了语义和情感变量。

Result: 视觉隐喻被评价为更难理解但更具美感；在效能和唤起性上与字面图像无显著差异，但对于认知需求高的参与者，视觉隐喻的唤起性更高。此外，视觉隐喻获得了更多标签，常提及图像中未直接描绘的实体，并引发了更积极、更具支配性的词语。这表明视觉隐喻的认知负荷更高，但可能诱导更深层次的认知加工和抽象思维。

Conclusion: 尽管视觉隐喻存在认知负荷，但它们能带来更佳的审美体验和更积极的感受，并可能促使更深层次的认知。本研究通过提供一个数据库和阐明利弊权衡，有助于理解气候变化视觉隐喻的影响，为环境传播策略的制定提供参考。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: 该论文介绍了Swa-bhasha资源中心，该中心汇集了罗马化僧伽罗语到僧伽罗语音译的数据和算法，旨在推动僧伽罗语NLP研究，并提供了现有音译应用的比较分析。


<details>
  <summary>Details</summary>
Motivation: 推动僧伽罗语自然语言处理（NLP）研究，特别是在训练音译模型和开发涉及罗马化僧伽罗语的应用程序方面，通过提供所需的资源解决此领域的需求。

Method: 建立了Swa-bhasha资源中心，公开发布了2020年至2025年间开发的罗马化僧伽罗语到僧伽罗语音译的综合数据资源和算法。本文详细概述了这些贡献资源，并对现有音译应用进行了比较分析。

Result: Swa-bhasha资源中心成功提供了大量公开可用的数据资源和算法，这些资源在推动僧伽罗语NLP研究，尤其是在音译模型训练和应用开发方面发挥了重要作用。

Conclusion: Swa-bhasha资源中心是僧伽罗语NLP领域的一项重要贡献，通过提供关键的音译数据和工具，有力支持了该领域的研究进展和应用开发。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文提出一种心理学启发的幽默分解机制（HDM），结合思维链（CoT）和幽默理论，显著提升了大型语言模型（LLMs）的幽默翻译质量，解决了现有LLMs在幽默翻译中遇到的语言干扰和幽默缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在处理幽默翻译时面临挑战，常导致语言干扰和译文缺乏幽默感，这影响了跨文化理解和交流。

Method: 提出一种受心理学启发的幽默分解机制（HDM），该机制利用思维链（CoT）模拟人类思维过程以优化译文可读性。此外，HDM还整合了幽默理论以增强译文的幽默元素。

Result: 在开源幽默数据集上的自动评估显示，该方法显著提升了幽默翻译质量，译文的幽默性平均提升7.75%，流畅性提升2.81%，连贯性提升6.13%。

Conclusion: 所提出的HDM方法通过模拟人类思维和整合幽默理论，有效改善了LLMs在幽默翻译方面的表现，显著提高了翻译文本的幽默感、流畅性和连贯性。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [21] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 本文提出ClaritySpeech框架，通过整合ASR、文本模糊和零样本TTS，纠正痴呆症患者的言语并模糊痴呆特征，同时在低数据环境下保留说话者身份，从而提升隐私性和可及性。


<details>
  <summary>Details</summary>
Motivation: 痴呆症改变言语模式，导致沟通障碍并引发隐私担忧。现有语音技术（如ASR）难以有效处理痴呆症患者的非典型言语，进一步影响其可及性。

Method: 开发了一种名为ClaritySpeech的创新型言语模糊框架，该框架整合了自动语音转录（ASR）、文本模糊处理以及零样本文本转语音（TTS）技术。它旨在纠正受痴呆症影响的言语，同时在低数据环境下无需微调即可保留说话者身份。

Result: 实验结果显示，在ADReSS和ADReSSo数据集上，该框架在不同对抗设置和模态下，平均F1分数分别下降了16%和10%，同时保持了50%的说话者相似度。此外，系统显著改善了词错误率（WER），ADReSS从0.73降至0.08，ADReSSo降至0.15；语音质量也从1.65提高到约2.15。

Conclusion: ClaritySpeech框架能够有效纠正痴呆症患者的言语，模糊疾病相关特征，同时保留说话者身份，显著提高了言语处理的隐私性和可及性，尤其适用于数据受限的环境。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [22] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: 本文提出了DATE-LM，一个统一的基准，用于系统评估大型语言模型（LLM）中的数据归因方法。研究发现，现有方法在不同任务上表现各异，且性能受评估设计影响。


<details>
  <summary>Details</summary>
Motivation: 数据归因方法对于LLM的多种研究和应用至关重要。然而，目前缺乏针对LLM数据归因方法的系统性、以LLM为中心的评估。

Method: 引入了DATE-LM（LLM数据归因评估）基准，通过训练数据选择、毒性/偏见过滤和事实归因这三个关键任务来衡量归因质量。该基准设计易于使用，支持在不同任务和LLM架构上进行大规模评估，并被用于对现有数据归因方法进行大规模评估。

Result: 评估结果表明，没有单一方法能在所有任务中表现最佳；数据归因方法与简单基线存在权衡；方法性能对特定任务的评估设计敏感。

Conclusion: 该研究发布了一个公共排行榜，以促进方法比较和社区参与。希望DATE-LM能为未来LLM数据归因研究奠定基础。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [23] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 该研究通过优化DRAGON Longformer模型，显著提升了其在医疗案例描述二分类任务上的性能，为临床自然语言处理提供了更有效的工具。


<details>
  <summary>Details</summary>
Motivation: 旨在优化DRAGON Longformer基础模型，以提高其在临床文本分类（特别是医疗案例描述的二分类）任务中的性能和准确性。

Method: 使用包含500份临床案例（400份训练，100份验证）的数据集。在预训练的DRAGON Longformer基础模型上进行超参数调优、领域特定预处理和架构调整。具体修改包括将序列长度从512增加到1024，学习率从1e-05调整到5e-06，训练周期从5扩展到8，并融入专业医学术语。

Result: 优化后的模型性能显著提升：准确率从72.0%提高到85.2%，精确率从68.0%提高到84.1%，召回率从75.0%提高到86.3%，F1分数从71.0%提高到85.2%（p < .001）。模型在解释医学术语、解剖测量和临床观察方面表现出更强的能力。

Conclusion: 这些发现对领域特定语言模型研究有所贡献，并为临床自然语言处理应用提供了实际意义。优化后的模型在多种医疗条件下的强大性能，预示着其在医疗领域具有广泛的应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [24] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文概述了CoNLL-2013语法纠错共享任务，包括任务定义、数据集、评估方法、参赛团队的方法和评估结果。


<details>
  <summary>Details</summary>
Motivation: 组织CoNLL-2013共享任务旨在推动语法纠错（GEC）领域的研究进展，而本文的动机是全面记录和总结该任务的各项细节和成果，为社区提供参考。

Method: 论文描述了任务定义、数据集、评估指标和评分工具；概述了各参赛团队采用的各种方法；并展示了最终的评估结果。

Result: 论文展示了CoNLL-2013语法纠错共享任务的最终评估结果。

Conclusion: 该论文全面总结了CoNLL-2013语法纠错共享任务，为该领域的研究和后续发展提供了重要的基础性资料和方法参考。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [25] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本综述从统一的推理-检索视角，系统分析了检索增强生成（RAG）与推理方法的结合，探讨了两者如何相互优化，并重点介绍了协同框架及其在知识密集型任务中的应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）能提升大型语言模型（LLMs）的事实性但缺乏多步推理能力；而纯粹的推理方法虽然能处理复杂推理但常出现幻觉或事实偏差。因此，需要弥合RAG的事实性与推理方法的复杂推理能力之间的差距。

Method: 本综述采用综合分析方法，首先映射了高级推理如何优化RAG的各个阶段（推理增强RAG）；其次，展示了不同类型的检索知识如何为复杂推理提供缺失前提和扩展上下文（RAG增强推理）；最后，重点介绍了新兴的协同RAG-推理框架（如代理LLMs迭代交织搜索与推理）。此外，还对相关方法、数据集和开放挑战进行了分类。

Result: 协同RAG-推理框架通过迭代地结合检索与推理，在知识密集型基准测试中实现了最先进的性能。本综述成功地综合并分类了当前的研究进展，揭示了不同融合策略的有效性。

Conclusion: 未来的研究应着力于开发更有效、多模态适应、可信赖和以人为中心的深度RAG-推理系统。本文为该领域进一步发展提供了明确的研究方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [26] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 本论文介绍了M2SaG多模态讽刺生成数据集和ViSP框架，该框架利用PPO和对比学习来生成高质量的讽刺文本，并在实验中超越了现有基线模型，包括大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管讽刺研究有所进展，但讽刺生成仍未得到充分探索，主要是因为现有研究过度依赖文本模态而忽视视觉线索，以及现有数据集中图像内容与讽刺意图不匹配。

Method: 引入了M2SaG，一个包含4,970个样本的多模态讽刺生成数据集，每个样本包含图像、讽刺文本和讽刺目标。提出了ViSP生成框架，该框架结合了近端策略优化（PPO）和对比学习，其中PPO利用DIP的奖励分数指导讽刺文本生成，对比学习促使模型偏好高奖励分数的输出。

Result: ViSP在五组指标上超越了所有基线模型，包括大型语言模型。生成文本的平均讽刺分数（0.898 vs. 0.770）和事实不一致性（0.768 vs. 0.739）均高于原始数据集，证明ViSP生成了更高质量的讽刺内容。

Conclusion: M2SaG和ViSP显著推进了多模态讽刺生成领域，强调了视觉信息的重要性，并证明了ViSP在生成高质量讽刺内容方面的优越性，弥补了现有大型语言模型在此领域的局限性。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 该研究提出一种基于LLM并结合强化学习优化的数据增强方法，以解决方面级情感分析（ABSA）中数据稀缺和不平衡问题，并取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在ABSA任务中面临短文本上下文理解困难，以及训练数据量小且标签分布不平衡（正向情感居多）的挑战。数据增强虽是可行策略，但难以确保增强数据的高质量。

Method: 提出一种基于LLM的ABSA方法，通过LLM根据原始数据生成扩增训练数据，以增加数据量并平衡标签分布。为提高扩增数据的质量，引入强化学习来优化数据增强过程。

Result: 在ABSA英语基准数据集上的实验结果和分析表明，该方法有效，性能优于强基线模型和大多数现有研究。

Conclusion: 结合LLM进行数据增强并利用强化学习优化增强过程，能有效解决ABSA任务中数据稀疏和不平衡问题，显著提升模型性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax是一个协议驱动的多智能体协作框架，通过标准化A2A通信和分层记忆系统，解决了传统AI系统在复杂企业环境中协调性、记忆复用和任务分解不足的问题，显著提升了多智能体系统的适应性、协作性和经验复用能力。


<details>
  <summary>Details</summary>
Motivation: 现代企业环境需要能处理复杂、动态、多方面任务的智能系统，但传统的单用途AI系统缺乏足够的协调、记忆复用和任务分解能力，限制了它们在实际场景中的可扩展性。

Method: 本文提出了GoalfyMax，一个协议驱动的端到端多智能体协作框架。它引入了基于模型上下文协议（MCP）的标准化Agent-to-Agent（A2A）通信层，通过异步、符合协议的交互实现智能体协调。此外，它整合了经验包（XP）架构，一个分层记忆系统，用于保留任务原理和执行轨迹，以实现结构化知识保留和持续学习。系统还包括多轮上下文对话、长短期记忆模块和动态安全验证等高级功能。

Result: 在复杂任务编排基准测试和案例研究中，GoalfyMax与基线框架相比，展现出卓越的适应性、协调性和经验复用能力。

Conclusion: 这些发现突出了GoalfyMax作为多智能体智能系统可扩展、面向未来的基础的潜力。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了Ref-Long，一个用于评估长上下文语言模型(LCLMs)上下文引用能力的新基准，并揭示了包括GPT-4o在内的先进模型在此任务上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文语言模型(LCLMs)在长上下文理解任务中表现出色，但长上下文引用——一项要求LCLMs将感兴趣项归因于长上下文特定部分的任务——仍未得到充分探索。

Method: 论文提出了Referencing Evaluation for Long-context Language Models (Ref-Long)基准，要求LCLMs识别引用特定关键词的文档索引，强调上下文关系。该基准包含合成到真实场景的三个子集。研究对13个LCLMs进行了实验，并进行了包括人工评估、任务格式调整、微调和错误分析在内的综合分析。

Result: 对13个LCLMs（包括GPT-4o）的实验结果表明，它们在长上下文引用方面存在显著不足。

Conclusion: 长上下文语言模型，即使是先进的模型，在长上下文引用任务中也表现出明显的局限性，这突出了该领域需要进一步深入研究和改进的挑战。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 本研究系统评估了提示错误（包括人类可信和合成错误）对大型语言模型（LLMs）在机器翻译及其评估任务中性能的影响。结果表明，提示质量对翻译性能影响显著，不同噪声类型（如字符级）影响不同，且质量下降主要影响指令遵循能力，但LLMs在极端噪声下仍能翻译。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译中表现优异，但其性能对提示中的错误和扰动敏感。研究旨在系统探究用户提示中的错误如何影响LLM在机器翻译和机器翻译评估任务上的表现。

Method: 通过在用户提示中引入人类可信和合成错误，系统评估LLM在机器翻译和机器翻译评估任务上的性能。采用定量分析和定性洞察相结合的方法，研究模型对日益增加的提示噪声的响应。

Result: 提示质量对翻译性能有强烈影响；在错误较多时，即使是好的提示也可能表现不如无错误的极简或较差提示。不同噪声类型对翻译质量的影响不同，其中字符级和组合噪声比短语扰动更严重地降低性能。定性分析显示，提示质量下降主要导致指令遵循能力变差，而非直接影响翻译质量本身。此外，LLM在人类难以辨认的压倒性随机噪声场景下仍能进行翻译。

Conclusion: LLM对提示错误的敏感性主要体现在指令遵循能力的下降，而非直接翻译质量受损。不同噪声类型对性能影响程度各异，且LLM在极端噪声下仍展现出超乎人类预期的翻译能力，凸显了提示质量在LLM翻译任务中的关键作用。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 本文探讨了如何将定义建模系统应用于尚不支持的语言，以白俄罗斯语为例，构建了新的数据集，并发现适应模型所需数据量小，但现有自动评估指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 定义建模有助于词典编纂者记录更多方言和语言，但如何利用现有模型处理尚未支持的语言仍有待探索。

Method: 将现有定义建模模型应用于白俄罗斯语。为此，构建了一个包含43,150条定义的新数据集。

Result: 实验表明，适应定义建模系统仅需少量数据。然而，目前的自动评估指标在捕捉模型性能方面存在不足。

Conclusion: 定义建模系统可以有效地适应新语言，且对数据量要求不高，但需要开发更完善的自动评估指标以全面衡量其性能。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出了NMIXX，一套用于金融领域的跨语言嵌入模型，以及KorFinSTS，一个韩语金融语义相似度基准测试数据集，旨在解决通用模型在低资源语言金融语义理解上的不足，并展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型难以捕捉专业的金融语义，特别是在韩语等低资源语言中，这归因于领域特定术语、时态意义变化和双语词汇不对齐的问题。现有工具在金融领域适应性多语言表征学习方面存在空白。

Method: 开发了NMIXX，通过18.8K高质量三元组（领域内释义、语义漂移类型硬负例、精确韩英翻译）进行微调的跨语言嵌入模型。发布了KorFinSTS，一个包含1,921对韩语金融STS数据的基准，涵盖新闻、披露、研究报告和法规。将NMIXX与七个开源基线模型进行评估，并分析了韩语词元覆盖率对模型适应性的影响。

Result: NMIXX的多语言bge-m3变体在英语FinSTS上Spearman's rho提升了+0.10，在KorFinSTS上提升了+0.22，显著优于其预适应检查点并超越了其他模型。分析显示，韩语词元覆盖率更丰富的模型能更有效地适应。存在通用STS性能的轻微权衡。

Conclusion: NMIXX模型和KorFinSTS基准为金融领域中领域适应的多语言表征学习提供了强大工具，尤其对低资源语言至关重要。强调了分词器设计在低资源、跨语言环境中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: 本文介绍了SpreadPy，一个用于模拟认知网络中扩散激活的Python库，通过三个案例展示了其在研究认知过程、个体差异和临床障碍方面的效用。


<details>
  <summary>Details</summary>
Motivation: 现有研究需要一个工具来系统地测试认知过程中结构与功能的关系，并探索激活动态如何反映认知、心理和临床现象，从而提供对知识建模中既有理论的验证和深入洞察。

Method: 引入并使用了SpreadPy，一个Python库，用于在认知单层和多层网络中模拟扩散激活。通过以下三个案例研究展示其功能：(1) 在关联知识网络上模拟激活以区分数学焦虑程度；(2) 模拟创造力任务中的激活轨迹以研究认知负荷对词汇获取的影响；(3) 在失语症患者的词汇网络上模拟激活模式并与实际错误类型关联。

Result: (1) SpreadPy成功区分了高、低数学焦虑学生，揭示了概念组织中与焦虑相关的结构差异；(2) 模拟显示激活轨迹随创造力任务难度变化，揭示了认知负荷对词汇获取的调节作用；(3) 模拟的激活模式与失语症患者在图片命名任务中的语义或语音错误类型相关联，将网络结构与临床损伤联系起来。

Conclusion: SpreadPy提供了一个灵活的框架，可用于利用经验或理论网络建模认知过程，从而为理解个体差异和认知障碍提供机制性见解。该库的开放性将促进心理学、神经科学和教育研究的可复现性。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 该研究首次探索了阿拉伯语知识编辑（KE），发现指令微调方法更稳健，且多语言（阿拉伯语-英语）联合训练能提高可编辑性和迁移性，并发布了相关基准数据。


<details>
  <summary>Details</summary>
Motivation: 知识编辑（KE）在英语中已被广泛研究，但在阿拉伯语等形态丰富的语言中，其行为仍未得到充分探索。

Method: 研究评估了四种KE方法（ROME、MEMIT、ICE和LTE），在Llama-2-7B-chat模型上，使用ZsRE和Counterfact基准的阿拉伯语翻译版本，分析了多语言和跨语言设置。同时，将Learning-To-Edit (LTE) 扩展到多语言环境，并进行了阿拉伯语-英语联合训练。

Result: 参数化方法在跨语言泛化方面表现不佳，而指令微调方法则更稳健。此外，阿拉伯语-英语联合训练提高了LTE的可编辑性和迁移性。

Conclusion: 对于阿拉伯语等形态丰富的语言，指令微调的知识编辑方法表现更优。通过多语言联合训练（如阿拉伯语-英语），可以显著提升知识编辑的性能和跨语言能力。研究为未来的阿拉伯语KE研究提供了基准和数据支持。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 本文提出一种利用群组相对策略优化（GRPO）结合BGE-M3嵌入，以提升泰语法律问答RAG系统在复杂推理任务上的引用准确性和响应质量，并在降低成本的同时取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 泰语法律问答领域的检索增强生成（RAG）系统表现有限，尤其在处理需要广泛、复杂法律推理的问题时面临挑战。

Method: 引入群组相对策略优化（GRPO）方法来校准大型语言模型（LLMs），以提高法律引用准确性和响应质量。该方法利用BGE-M3嵌入作为成本效益高的语义相似性奖励，与使用大型语言模型作为判别器相比，计算成本降低了2.5倍。

Result: 在NitiBench基准测试中，GRPO相对于基础模型在引用F1分数上提升高达90%，相对于指令微调在综合质量指标上提升31%。此外，该方法在复杂法律推理任务上显示出比指令微调更强的鲁棒性。

Conclusion: 所提出的方法为增强泰语法律大型语言模型提供了一个有效且资源高效的解决方案，尤其在复杂法律推理任务上表现出显著改进和更强的鲁棒性。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了MCEval，一个新颖的多语言文化评估框架，用于评估大型语言模型（LLMs）的文化理解能力和偏见。研究发现LLMs的文化表现不仅与训练数据相关，还与语言-文化对齐有关，并揭示了英语中心评估的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在服务全球用户时，存在文化偏见和有限的跨文化理解能力。

Method: 提出了MCEval，一个多语言评估框架。该框架采用动态文化问题构建，并通过反事实重述和混淆因子重述实现因果分析。评估涵盖13种文化和13种语言，生成了39,897个文化意识实例和17,940个文化偏见实例，系统性评估LLMs的文化意识和偏见。

Result: 实验结果显示，LLMs在不同语言场景下的性能存在差异，最佳文化表现与训练数据分布及语言-文化对齐相关。此外，在英语场景下看似成功的方法会造成显著的公平性劣势。

Conclusion: MCEval是首个全面的多语言文化评估框架，为深入理解LLMs的文化理解能力提供了深刻见解。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）的语义信息在潜在空间中呈线性可分，且在更深层和特定提示下更为显著，为开发基于潜在表示的安全工具提供了基础。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）的潜在空间几何对于解释其行为和提高对齐性至关重要，但目前尚不清楚LLM在内部如何组织与语义理解相关的表示。

Method: 对11个基于Transformer的仅解码器LLM进行大规模实证研究，分析了它们在6个科学主题和每个模型的12个层中的隐藏状态。

Result: 高级语义信息始终位于低维子空间中，并形成跨不同领域的线性可分离表示。这种可分离性在更深的层中以及在触发结构化推理或对齐行为的提示下（即使表面内容不变）变得更加明显。这种几何结构支持在隐藏空间中进行简单而有效的因果干预（例如，思维链可由单个向量方向捕获）。作为概念验证，通过训练一个简单的MLP分类器作为轻量级潜空间护栏，以高精度检测了对抗性和恶意提示。

Conclusion: 这些发现支持开发直接作用于潜在表示的几何感知工具，利用其线性可分离性来检测和减轻有害或对抗性内容（例如，基于传输的防御方法），从而提高LLM的安全性和鲁棒性。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本研究提出一种自适应课程学习范式，利用预训练语言模型（PLMs）自身预测的难度分数来排序微调样本，解决了传统方法依赖手动定义难度的局限性，并在自然语言理解任务上实现了更快的收敛和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖手动定义的难度指标（如文本长度），这些指标可能无法准确反映模型自身对样本难度的感知，从而限制了学习效率和性能。

Method: 提出一种自适应课程学习范式，通过让预训练语言模型（PLMs）自身预测样本的难度分数。基于这些分数，探索了多种微调训练策略，包括从易到难、从难到易以及混合采样。该方法在四个自然语言理解（NLU）数据集上进行了评估，涵盖二分类和多分类任务。

Result: 实验结果显示，与标准随机采样相比，本研究提出的自适应课程学习方法能显著加快模型收敛速度并提高最终性能。

Conclusion: 通过让PLMs自适应地评估样本难度并指导训练过程，所提出的自适应课程学习范式有效克服了传统方法的局限，证明了其在提升模型学习效率和性能方面的有效性。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 本研究修订了点击诱饵的定义，提出以“好奇心差距”为核心的新定义，并创建了首个西班牙语开放数据集TA1C，为点击诱饵检测提供了新的方法和资源。


<details>
  <summary>Details</summary>
Motivation: 当前点击诱饵的定义缺乏共识，难以与哗众取宠等现象区分，这阻碍了有效的检测研究。本研究旨在提供一个清晰、操作性强的点击诱饵定义，并为相关研究创建高质量的开放数据集。

Method: 研究首先修订了点击诱饵的定义，将其核心特征界定为“故意省略部分信息以制造好奇心差距”。基于此新定义，提出并实施了一套减少主观性的数据集创建和标注标准。通过手动标注来自18个媒体来源的3,500条西班牙语推文，创建并发布了TA1C数据集。最后，在该数据集上实现了强基线模型以验证其有效性。

Result: 本研究成功提出了基于“好奇心差距”的点击诱饵新定义。创建并发布了首个西班牙语点击诱饵检测开放数据集TA1C，包含3,500条推文，标注者间一致性达到0.825 Fleiss' K。在该数据集上实现的强基线模型获得了0.84的F1分数。

Conclusion: 本研究通过提供一个明确的点击诱饵定义和高质量的开放数据集，为未来点击诱饵检测领域的研究奠定了坚实的基础，并提供了宝贵的资源。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 本研究利用“偏一加法”作为案例，通过电路风格的可解释性技术，揭示了大型语言模型中驱动任务级泛化的“函数归纳机制”，并发现该机制具有可重用性和可组合性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够通过上下文学习执行未见任务，但其内部实现这种任务级泛化的机制尚不清楚。

Method: 本研究以“偏一加法”（如1+1=3）这一反事实任务为例，利用路径修补（path patching）等电路风格的可解释性技术，分析模型内部的计算过程。

Result: 1. 发现了一种“函数归纳机制”，解释了模型从标准加法泛化到偏一加法的能力，该机制类似于并提升了先前的归纳头机制。
2. 表明+1函数的归纳由多个并行注意力头共同控制，每个头贡献+1函数的一部分。
3. 发现该函数归纳机制在更广泛的任务中被重用，包括移位选择题问答和八进制加法等算法任务。

Conclusion: 研究结果深入揭示了大型语言模型中可重用和可组合的结构如何实现任务级泛化能力。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [41] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本文提出一种新的RAG框架，通过分层文本分割和聚类生成更具语义一致性的知识块，从而提高信息检索的精确度和上下文相关性，优于传统分块方法。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG分块策略未能有效捕获足够的语义信息，因为它忽视了文本的底层结构，导致检索到的信息不够精确和领域相关。

Method: 本文提出了一个新颖的框架，通过整合分层文本分割和聚类来生成更具意义和语义连贯性的知识块。在推理阶段，该框架利用段落级和聚类级的向量表示进行信息检索。

Result: 在NarrativeQA、QuALITY和QASPER数据集上的评估表明，与传统分块技术相比，所提出的方法取得了改进的结果。

Conclusion: 通过整合分层文本分割和聚类，该方法能有效增强RAG系统，生成更精确和上下文相关的知识块，从而提升检索性能。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [42] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: 该研究提出TinyRM，一种小参数（4亿）的双向掩码语言模型，其在推理和安全偏好建模任务上表现出与大175倍的模型相当的性能，并显著降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型解码器语言模型在RLHF奖励建模中占据主导地位，但其高昂的推理成本日益成为部署中的主要问题。

Method: 引入TinyRM，一个由小型（最小4亿参数）双向掩码语言模型（MLMs）组成的家族。通过结合FLAN风格提示、定向低秩适应（DoRA）和层冻结等技术，在RewardBench上进行训练以实现高性能。

Result: TinyRM在推理和安全偏好建模任务上与比其大175倍的模型表现出不相上下的能力。在RewardBench测试中，尽管资源消耗显著减少，TinyRM仍表现出强大的性能。实验表明，小型模型受益于领域特定调优策略，尤其在推理任务中轻量级微调方法效果显著。

Conclusion: 初步结果表明，轻量级双向架构有望成为偏好建模的高效、可扩展替代方案，尽管在通用模型和对话偏好建模方面仍面临挑战。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [43] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: 本文提出TextOmics基准数据集和ToDi生成框架，旨在解决靶点特异性药物发现中缺乏异构数据和统一框架的问题。ToDi通过整合组学表达和分子文本描述，高效、可控地生成具有治疗潜力的类命中分子，并表现出超越现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 靶点特异性药物发现需要生成具有治疗潜力的类命中分子，但目前该领域缺乏异构数据以及整合多种分子表示的统一框架，这阻碍了高效和相关的分子生成。

Method: 1. **TextOmics基准：** 构建了一个开创性的异构数据集，建立了组学表达与分子文本描述之间的一一对应关系，旨在通过表示对齐促进分子生成。
2. **ToDi生成框架：** 基于TextOmics，提出了ToDi框架。该框架联合以组学表达和分子文本描述为条件，生成生物学相关、化学有效且具有治疗潜力的类命中分子。ToDi利用两个编码器（OmicsEn和TextEn）捕获多层次的生物学和语义关联，并开发了条件扩散模型（DiffGen）以实现可控生成。

Result: 广泛的实验证实了TextOmics的有效性。ToDi框架在分子生成方面显著优于现有最先进的方法，并且在零样本治疗分子生成中展现出卓越的潜力。

Conclusion: ToDi框架通过引入TextOmics基准和创新的生成模型，成功解决了分子生成领域中数据异构和缺乏统一框架的挑战。它能够高效且可控地生成高质量的类命中分子，为靶点特异性药物发现提供了强大的新范式，特别是在零样本场景下具有显著的潜在应用价值。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [44] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 该研究提出一个新框架，用于预测后续自杀风险，通过联合学习风险因素和保护因素的动态影响，并提供了可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重于检测当前自杀风险，但未能预测随时间变化的后续风险，且忽视了在自杀风险预测中起关键作用的保护性因素。

Method: 本研究提出了一个新颖的框架，通过联合学习风险因素和保护因素对用户自杀风险转换的动态影响来预测后续自杀风险。具体方法包括构建了一个“保护因素感知数据集”（基于12年Reddit帖子）和引入了“动态因素影响学习”方法。

Result: 所提出的模型在三个数据集上显著优于现有最先进的模型和大型语言模型。此外，所提出的动态因素影响学习方法提供了可解释的权重。

Conclusion: 该研究通过考虑动态变化和保护性因素，显著提升了后续自杀风险预测的准确性，并提供了有益于临床干预的解释性洞察。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [45] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo是一种基于进化算法和层折叠的LLM压缩方法，通过高效探索压缩空间，优化模型相似度，并首次建立了压缩与质量的帕累托前沿，性能超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）因计算需求巨大而面临部署和使用障碍。现有模型压缩方法（如结构化剪枝）需要昂贵的经验搜索且可能错过更优解。

Method: 本文提出了GeLaCo，一种通过层折叠实现LLM压缩的进化方法。该方法利用基于种群的搜索和包含注意力、前馈、隐藏状态表示的模块级相似度适应度函数，高效探索压缩解空间。GeLaCo支持单目标和多目标进化压缩搜索，并首次构建了压缩与质量轴的帕累托前沿。

Result: 通过对基础模型和指令微调模型进行困惑度评估和生成性评估，GeLaCo的解决方案表现出色，性能超越了现有最先进的替代方法。

Conclusion: GeLaCo通过其独特的进化层折叠方法，有效解决了LLM的计算开销问题，克服了传统压缩方法的局限性，并在压缩与模型质量之间实现了更优的平衡，取得了超越现有最佳方法的性能。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [46] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）未能代表多样化的文化道德框架，反而将其同质化，这挑战了它们在社会科学中的应用并揭示了当前AI对齐方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统（特别是LLMs）是否真正代表人类价值观，或仅仅是平均化处理，以及它们在跨文化道德表示方面的能力。

Method: 通过在19个文化背景下应用“道德基础问卷”（Moral Foundations Questionnaire），比较多个最先进的LLMs的输出与人类基线数据，以暴露AI生成和人类道德直觉之间的差距。

Result: LLMs未能代表多样化的文化道德框架，AI生成与人类道德直觉之间存在显著差距。这些模型系统性地同质化了道德多样性，且模型规模的增加并未持续改善文化代表性。

Conclusion: 研究结果挑战了将LLMs用作社会科学研究中“合成人群”的日益增长的趋势，并强调了当前AI对齐方法的基本局限性。呼吁超越提示工程，采用更基于数据驱动的对齐目标和评估指标，以确保AI系统能够代表多样化的人类价值观，而非抹平道德差异。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [47] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 提出“关键表示微调”（CRFT）方法，通过信息流分析识别并优化模型中的关键表示，以提升参数高效微调（ReFT）在复杂推理任务上的性能，并在多项基准测试和少样本设置下验证其有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法ReFT在复杂推理任务上表现不佳，因其固定位置的表示修改对输出影响不确定，无法有效捕捉复杂推理所需的关键信息。

Method: 提出CRFT方法，通过信息流分析识别并动态优化模型中对最终输出有重大影响的“关键表示”。该方法在监督学习框架下，在低秩线性子空间中优化这些关键表示，同时冻结基础模型。

Result: 在LLaMA和Mistral模型家族上，对8个算术和常识推理基准测试进行验证，CRFT展现出有效性和高效性。特别是在少样本设置下，将单样本准确率提高了16.4%。

Conclusion: 该研究揭示了表示层面优化在链式思考（CoT）推理中的巨大潜力，为传统参数高效微调方法提供了一种轻量级而强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [48] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 本文提出一种新型的Transformer架构，通过融合大型语言模型（LLM）和传统Transformer的表示，结合高层语义和时间动态，以提高时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列预测方法因LLM不擅长处理连续数值数据，性能可能不如传统Transformer；而传统Transformer又难以学习高层语义模式。因此，研究动机在于设计一种能互补利用二者优势的模型。

Method: 设计了一种新型的Transformer架构，它融合了LLM学习到的高层语义表示和时间序列Transformer编码的时间信息，从而获得一种包含历史时间动态和语义变异模式的混合表示。

Result: 融合后的表示使得模型能够预测更准确的未来值。在基准数据集上的实验证明了所提出方法的有效性。

Conclusion: 通过整合LLM的高层语义理解能力和传统Transformer的时间序列建模能力，所提出的混合架构能够显著提升时间序列预测的准确性。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [49] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 针对大语言模型（LLM）知识蒸馏中师生模型隐藏层维度不匹配的问题，本文提出一种新型基于任务的特征蒸馏方法，无需额外参数即可实现不同维度模型间的知识迁移，通过识别并蒸馏教师模型中与任务最相关的隐藏单元，在多种任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法通常假设师生模型共享相同的隐藏层维度，这限制了学生模型的架构灵活性。现有解决方案（如训练线性投影器）会引入额外参数，且在下游任务（特别是生成任务）上常导致性能下降。

Method: 本文提出一种新型的、基于任务的特征蒸馏方法。该方法无需引入新参数，即可实现不同隐藏层维度的师生模型之间的知识迁移。其核心在于识别教师模型中对特定下游任务最相关的隐藏单元，并直接将其激活值蒸馏给学生模型。

Result: 在分类、指令遵循和摘要等多种任务上，相比现有方法（特别是线性投影基线）实现了持续改进，性能提升最高达3%。

Conclusion: 本文提出的基于任务的特征蒸馏方法，有效解决了知识蒸馏中师生模型隐藏层维度不匹配的问题，且无需引入额外参数，提升了学生模型的灵活性和性能，在多类任务上均取得显著效果。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [50] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 本研究评估了LLMs（Gemini, GPT-4o, DeepSeek, Groq）将冒犯性文本转换为非冒犯性文本的能力，旨在保留原意。结果显示Groq表现独特，GPT-4o与DeepSeek-V3相似。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理方面有显著进展，但它们在分类和转换冒犯性文本方面的有效性仍有待深入探索。

Method: 使用LLMs（Gemini, GPT-4o, DeepSeek, Groq）将包含仇恨言论和脏话的冒犯性文本（推文和评论）转换为非冒犯性文本，并确保保留文本原意。通过情感分析和语义分析评估原始和转换后数据集的性能。

Result: Groq与其他大型语言模型相比提供了截然不同的结果。GPT-4o和DeepSeek-V3之间存在相似性。

Conclusion: 在冒犯性文本转换任务中，不同大型语言模型的表现存在显著差异，Groq提供了独特的结果，而GPT-4o与DeepSeek-V3则表现出相似的处理倾向。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [51] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 本文介绍 Absher，一个用于评估大型语言模型对沙特阿拉伯方言和文化细微理解的综合基准测试，结果显示模型在此方面存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在阿拉伯语自然语言处理应用中日益核心，尤其是在沙特阿拉伯等语言多样性强的地区，评估它们对区域方言和文化细微差别的理解至关重要。

Method: 引入了名为 Absher 的综合基准测试，包含超过18,000个多项选择题，涵盖六个不同类别（含义、真/假、填空、上下文使用、文化解释和地点识别），问题来源于沙特阿拉伯不同地区的方言词汇、短语和谚语。研究评估了多种最先进的大型语言模型，包括多语言和阿拉伯语专用模型。

Result: 研究结果揭示了显著的性能差距，特别是在需要文化推断或上下文理解的任务中。

Conclusion: 研究结果强调了迫切需要进行方言感知训练和文化对齐的评估方法，以提高大型语言模型在实际阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [52] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 本文提出一种两阶段演化搜索方法，结合语法指导遗传编程和局部搜索，用于自动化离散提示优化。该方法旨在解决复杂任务和小型LLM的提示优化难题，并在多个任务和模型上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化提示工程方法主要适用于简单任务和大型预训练语言模型（LLM），但对于需要详细信息的复杂任务，优化文本量增加，且小型LLM对提示设计更敏感。因此，需要一种更有效的自动化离散提示优化方法来应对这些挑战。

Method: 提出一种两阶段演化搜索方法。第一阶段：利用语法指导遗传编程（grammar-guided genetic programming），通过搜索由句法、基于字典和基于LLM的提示编辑函数组合而成的程序空间，来合成提示创建程序。第二阶段：应用局部搜索（local search），在表现最佳程序的邻域内进一步微调性能。

Result: 该方法在四项领域特定挑战任务中，对三种相对较小的通用LLM进行测试，结果表明其性能优于PromptWizard、OPRO和RL-Prompt这三种现有最先进的提示优化方法。本方法在几乎所有任务-模型组合中均能提升性能，即使未能提升，性能下降也微乎其微。

Conclusion: 所提出的演化搜索方法成功地实现了自动化离散提示优化，尤其对于复杂任务和小型LLM具有显著优势，其性能表现优于当前主流的提示优化方法。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [53] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出一种基于增长界限矩阵（GBM）的新型正则化技术，以提高NLP模型（包括LSTM、S4和CNN）对抗同义词替换攻击的鲁棒性，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP模型有进步，但仍易受对抗攻击，如同义词替换。现有研究主要关注前馈和卷积网络，而循环网络（如LSTM）和现代状态空间模型（如S4）的鲁棒性研究不足，它们具有独特的处理挑战。

Method: 引入基于增长界限矩阵（GBM）的正则化技术，通过降低输入扰动对模型输出的影响来提升鲁棒性。该方法专注于计算LSTM、S4和CNN这三种架构的GBM。

Result: 在多架构和基准数据集上的实验表明，该方法将对抗鲁棒性比现有基线提高了高达8.8%，并超越了多种先进的对抗防御方法。

Conclusion: 所提出的GBM正则化方法能有效提高NLP模型的对抗鲁棒性，特别是针对LSTM、S4和CNN架构，并首次系统分析了S4模型的鲁棒性。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [54] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）在语言学研究中作为分析工具的潜力，通过四项心理语言学实验对比了LLM与人类在情感意义判断上的表现，发现两者结果高度一致，表明LLMs可作为语言学研究的可靠协作工具。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多项任务中展现出潜力，但其复制细致入微的人类判断的能力仍有待检验。本研究旨在探索LLMs在语言学研究中，特别是在涉及运动方式动词的时间表达中情感意义的产生方面，能否作为可靠的分析工具。

Method: 进行了四项心理语言学研究（关于涌现意义、效价转变、情感语境中的动词选择以及句子-表情符号关联），首先由人类参与者完成，然后使用LLM（如GPT-4）复制相同的任务。通过统计分析（例如，Spearman相关系数）比较人类和AI的响应。

Result: 所有研究结果显示，人类和AI的响应之间存在显著的一致性。统计分析（例如，Spearman相关系数 = 0.73-0.96）表明，在评分模式和类别选择上均存在强相关性。尽管在某些情况下观察到细微差异，但这些差异并未改变整体解释结果。

Conclusion: 研究结果提供了令人信服的证据，表明LLMs可以增强传统的人类实验，从而在不损害解释有效性的前提下实现更大规模的研究。这种一致性不仅加强了先前基于人类发现的实证基础，还为通过AI生成假设和扩展数据开辟了可能性。最终，本研究支持将LLMs作为语言学探究中可信和信息丰富的协作者。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [55] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 本文提出了一种分层“洋葱”模型，用于处理隐喻意义，该模型整合了内容分析、概念融合和语用意图三个层面，旨在实现更丰富、认知上更合理的计算系统隐喻解释。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义并非概念间的简单映射，而是一种整合多层解释的复杂认知现象。现有的理解方式未能充分捕捉其复杂性，需要一种更具认知基础的方法来处理隐喻解释。

Method: 研究提出了一个分层的隐喻处理模型，将意义视为“洋葱”状的多层结构，包括三个维度：1) 内容分析（通过基本概念元素标注隐喻），2) 概念融合（建模概念组合及其涌现意义），3) 语用意图（引入语用词汇捕捉说话者意图、交际功能和语境效应）。这些层次被统一在一个形式框架中。

Result: 该三维框架为计算系统中的隐喻解释提供了一种更丰富、认知上更合理的方法。通过统一这些层次，该模型为计算方法奠定了基础，使其能够超越表面联想，表示更深层、更具语境敏感性的隐喻意义推理。

Conclusion: 该统一的分层模型为计算系统理解隐喻意义提供了深入且具语境敏感性的方法，超越了简单的表面关联，为未来隐喻处理研究奠定了基础。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [56] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）能够处理图推理任务，即使图结构嵌入在文本中。本文研究了Transformer模型如何理解潜在图结构，提出“诱导子结构过滤（ISF）”机制，并证明其能从各种图类型（包括分子图）中成功提取子结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLMs能解决图推理任务，即使图结构以文本形式给出。这引出了一个根本性问题：仅解码器Transformer架构如何理解并从文本序列中提取其底层的图结构？

Method: 研究从子结构提取任务入手，通过解释Transformer的内部机制和分析输入查询的影响来解决问题。具体而言，结合经验结果和理论分析，提出了“诱导子结构过滤（ISF）”这一视角，用于捕捉多层Transformer中的子结构识别过程。研究进一步在LLMs中验证了ISF过程，并引入“子结构思维”概念以高效提取复杂复合模式，成功地从包括分子图在内的属性图中提取子结构。

Result: 本文提出了“诱导子结构过滤（ISF）”机制，能解释多层Transformer中子结构的识别过程。实证结果和理论分析表明，ISF在LLMs的各层中展现出一致的内部动态。研究还证明了仅解码器Transformer能够通过“子结构思维”成功地从各种属性图（如分子图）中提取复杂的子结构。

Conclusion: 本研究的发现为基于序列的Transformer模型如何执行图数据的子结构提取任务提供了新的见解。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [57] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在异步任务导向对话中提出澄清问题的能力。通过构建新语料库并对比LLM与人类的行为，发现两者在处理歧义时的模式不同，且LLM的推理能力能提升其提问的频率和相关性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在异步、指令导向型对话中提出澄清问题的能力，并探究其行为与人类在处理歧义时的异同。此外，探讨LLM的推理能力是否是其提出澄清问题能力的基础。

Method: 1. 构建一个新语料库，该语料库整合了Minecraft对话语料库中关于指称、指称歧义和SDRT（含澄清）的现有标注，以支持对澄清与歧义关系的研究。2. 利用该语料库，对比LLM生成的问题与人类提出的澄清问题，分析二者在歧义情境下的行为模式。3. 通过测试不同的推理方法，评估推理能力对LLM提问频率和相关性的影响。

Result: 1. 歧义与人类提出澄清问题之间仅存在微弱关联，且人类与LLM之间存在低相关性。2. 人类很少对指称歧义提出澄清，但经常对基于任务的不确定性提出澄清。3. LLM则相反，它们更常对指称歧义提出澄清，但较少对任务不确定性提出。4. 推理能力似乎能增加LLM提问的频率和相关性。

Conclusion: LLM在异步任务导向对话中提出澄清问题的能力与人类存在显著差异，尤其是在处理不同类型的歧义时。研究表明，LLM的推理能力对其提出澄清问题的频率和相关性有积极影响。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [58] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 本研究旨在通过基准测试，评估大型自回归语言模型（LLMs）相较于早期双向编码器，在实际在线文本中仇恨言论检测方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在线平台在遏制仇恨言论的同时避免过度审查面临挑战。尽管早期双向Transformer编码器取得了进展，但超大型自回归LLMs的出现有望带来更深层次的上下文感知能力。然而，这种规模的提升是否能实际改善真实文本中的仇恨言论检测，尚未得到验证。

Method: 本研究通过在精选的在线交互语料库上，对经典编码器模型和下一代LLMs两种模型家族进行基准测试，以评估它们在仇恨言论检测（仇恨或非仇恨）中的性能。

Result: 摘要中未提供研究结果。

Conclusion: 摘要中未提供研究结论。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [59] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文介绍了一种名为MLAR的创新型RPA框架增强的求职者追踪系统（ATS），利用大型语言模型（LLMs）自动化简历筛选和匹配，显著提高了招聘效率。


<details>
  <summary>Details</summary>
Motivation: 传统招聘流程在简历筛选和候选人初选环节常因时间和资源限制而面临瓶颈，效率低下。

Method: MLAR框架采用三层LLMs处理：第一层从职位描述中提取关键特征，第二层解析申请人简历以识别教育、经验和技能，第三层进行相似度匹配。通过先进的语义算法匹配特征，并无缝集成到现有RPA流程中，实现简历解析、职位匹配和候选人通知的自动化。

Result: MLAR在处理大量简历任务时优于主流RPA平台（如UiPath和Automation Anywhere）。处理2400份简历时，MLAR平均每份简历处理时间为5.4秒，与Automation Anywhere相比处理时间减少约16.9%，与UiPath相比减少约17.1%。

Conclusion: MLAR展现出革新招聘工作流程的巨大潜力，提供了一个高效、准确且可扩展的解决方案，以满足现代招聘需求。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 针对视觉语言导航（VLNCE）中策略对视角变化敏感的问题，本文提出了VIL（视角不变学习）后训练策略。VIL通过对比学习和教师-学生框架，显著提升了现有策略对不同视角的鲁棒性，并在V2-VLNCE和标准VLNCE设置下均取得了SOTA性能，证明其即插即用的有效性。


<details>
  <summary>Details</summary>
Motivation: 连续环境中的视觉语言导航（VLNCE）是具身AI的关键问题，然而大多数导航策略对视点变化（如相机高度和视角）非常敏感，导致智能体的观察结果改变，进而影响导航性能。

Method: 1. 引入了广义场景V2-VLNCE（带有不同视点的VLNCE）。2. 提出了VIL（视角不变学习）策略，作为一种视角不变的后训练方法，增强现有导航策略的鲁棒性。3. VIL采用对比学习框架，学习稀疏且视角不变的特征。4. 为路径点预测器模块引入了教师-学生框架，将依赖视点的教师模型知识蒸馏到视角不变的学生模型中。5. 采用端到端训练范式，共同优化所有组件。

Result: 1. 在V2-VLNCE场景下，本方法在R2R-CE和RxR-CE两个标准基准数据集上，成功率比现有最先进方法提高了8-15%。2. 在标准VLNCE设置下，即使是为不同视点训练，VIL仍能提升性能。3. 在更具挑战性的RxR-CE数据集上，本方法在所有指标上均达到了最先进的性能（相较于其他无地图方法）。

Conclusion: VIL策略成功解决了视觉语言导航中的视角敏感性问题，显著增强了策略在多变视角下的鲁棒性。同时，它在标准视角设置下也能提升性能，可作为即插即用的后训练方法，对现有导航策略进行有效改进。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [61] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 本文提出了一种利用面部生物识别异常模式的法医机器学习技术，用于检测深度伪造视频冒充，并在大型数据集上验证了其可靠性和泛化能力，以应对日益增长的欺诈和虚假信息威胁。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术结合声音克隆和视觉生成，使得创建任何人的虚假视频变得相对容易。目前，这种深度伪造冒充常被用于欺诈、诈骗和政治虚假信息，构成严重威胁。

Method: 提出了一种新颖的法医机器学习技术，通过利用面部生物识别中不自然的模式来检测深度伪造视频冒充。

Result: 该技术已在一个包含多种深度伪造技术和冒充的大型数据集上进行了评估，并测试了其对视频清洗的可靠性以及对先前未见的深度伪造生成器的泛化能力。

Conclusion: 该研究提供了一种通过分析面部生物识别异常模式来检测深度伪造视频的有效方法，并在应对现有及新型伪造技术方面展现出良好的可靠性和泛化能力。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [62] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据且与任务无关的新型方法，通过LLM生成虚假相关描述并利用对比式去偏损失学习投影，从而减轻视觉-语言模型（VLMs）中的隐式虚假偏见，同时保持图像与文本嵌入的对齐。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在训练数据中继承并放大了偏见，导致预测结果出现偏差。

Method: PRISM方法分两阶段：首先，使用大型语言模型（LLM）根据简单类别提示生成包含虚假相关性的场景描述；其次，PRISM运用一种新颖的对比式去偏损失，学习一个投影，将嵌入映射到潜在空间，以最小化虚假相关性，同时保持图像和文本嵌入之间的对齐。

Result: 广泛的实验表明，PRISM在常用的Waterbirds和CelebA数据集上均优于当前的去偏方法。

Conclusion: PRISM提供了一种无需数据且与任务无关的有效解决方案，用于缓解视觉-语言模型中的偏见，并在实验中表现出优于现有去偏方法的效果，提高了模型预测的公平性。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [63] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 提出HMR-ViT，一种融合时序和运动学信息的人体网格恢复方法。通过构建时序-运动学特征图并结合Vision Transformer进行处理，在主流数据集上实现有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体网格恢复（HMR）方法或利用时序信息，或利用运动学关系，但没有方法同时利用两者，导致任务存在固有的模糊性。

Method: 提出“Video Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)”方法。该方法首先使用图像编码器从视频帧中提取特征向量，并构建“时序-运动学特征图”，其中通过通道重排矩阵（CRM）使相似的运动学特征在空间上接近。随后，该特征图由Vision Transformer进一步编码，最后通过回归网络推断出SMPL姿态和形状参数。

Result: 在3DPW和Human3.6M数据集上的广泛评估表明，该方法在人体网格恢复方面取得了有竞争力的性能。

Conclusion: HMR-ViT方法成功地结合了时序和运动学信息，有效解决了现有方法的局限性，并在人体网格恢复任务上展示了良好的性能。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [64] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 提出一种结合NeRF与MPM的框架，通过视觉观测反演颗粒材料的摩擦角。


<details>
  <summary>Details</summary>
Motivation: 解决在直接测量困难或不可能的实际场景中，有效推断颗粒材料物理特性的挑战。

Method: 首先生成犁与沙子交互的合成实验数据并渲染成图像观测；接着使用NeRF从初始多视角图像重建3D几何；然后用重建几何初始化MPM模拟（摩擦角未知），并渲染模拟图像；最后通过贝叶斯优化，最小化模拟与观测图像之间的损失，以估计最佳摩擦角。

Result: 摩擦角的估计误差在2度以内，证明了仅凭视觉观测进行反演分析的有效性。

Conclusion: 该方法为在难以直接测量的实际场景中，表征颗粒材料提供了一种有前景的解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [65] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，通过结合多阶段数据验证策略和人类专业知识，解决多模态基础模型生成标签质量不足的问题，旨在提升模型在开放词汇图像分割等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型（如CLIP、LLaVA）虽促进了大规模数据集的自动标注并提升了下游任务性能，但其生成标签的质量研究不足，现有方法侧重数量而非质量。在无真值数据下验证海量数据具有挑战性，现有验证方法缺乏全面性或仅依赖小样本人工验证，无法解决所有潜在问题。

Method: 提出VISTA视觉分析框架，旨在提高数据质量以增强多模态模型性能，特别针对开放词汇图像分割。VISTA整合了多阶段数据验证策略与人类专业知识，使研究人员能够识别、理解并纠正基础模型生成标签中的隐藏问题。

Result: 通过在两个基准数据集上的详细用例和专家评审，从定量和定性两方面证明了VISTA的有效性。

Conclusion: VISTA成功解决了验证和改进基础模型生成标签质量的挑战，通过结合自动化策略与人类洞察力，有效提升了多模态模型在复杂任务（如开放词汇图像分割）中的性能。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [66] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个Python工具包，用于简化和自动化脑部病灶图像分析流程的构建，提供从预处理到分割和性能评估的全面功能，旨在提供“无脑”的开发体验。


<details>
  <summary>Details</summary>
Motivation: 旨在通过提供一个用户友好、模块化的Python工具包，最小化认知负担，并简化临床和科学实践中复杂脑部病灶图像分析工作流程的创建。

Method: 遵循Pythonic原则设计，包含一个可适应的预处理模块，可执行共配准、图谱配准、颅骨剥离和面部去除。该工具包利用BraTS挑战赛的算法合成缺失模态、修复病灶并生成肿瘤分割，并集成panoptica等工具来量化分割模型性能（如病灶级指标）。

Result: 成功构建了一个多功能工具包，能够实现复杂的脑部病灶图像分析流程，包括多模态图像的预处理、缺失模态合成、病灶修复和病理特异性肿瘤分割，以及分割模型性能量化。它简化了开发过程，并可适应其他生物医学图像分析应用。

Conclusion: BrainLesion Suite是一个高效、灵活的工具包，显著简化了脑部病灶图像分析管道的开发，通过其模块化和易用性，有望在更广泛的生物医学图像分析领域发挥作用。其代码和教程可在GitHub上获取。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [67] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 本文提出两种对比损失函数，用于解决长尾分布数据下类别条件扩散模型对尾部类别图像生成多样性不足的问题。通过增加图像不相似性及引入条件-无条件生成对齐，有效提升了尾部类别的多样性，并在多个长尾数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 类别条件图像合成训练数据常呈长尾分布，导致尾部类别图像数量有限，进而引发模式坍塌并降低合成图像多样性。研究旨在提高长尾数据训练的类别条件扩散模型对尾部类别图像的生成多样性，同时不损害头部类别的保真度和多样性。

Method: 引入两种对比损失函数：1. 无监督InfoNCE损失，利用负样本增加合成图像，特别是尾部类别图像间的距离/不相似性。2. MSE损失，在大时间步长下对比类别条件生成与无条件生成，使去噪过程在初始阶段对类别条件不敏感，从而通过头部类别知识共享来丰富尾部类别。这是首次将此类条件-无条件对齐应用于扩散模型。

Result: 所提出的对比学习框架易于实现，并在CIFAR10/100-LT、PlacesLT、TinyImageNetLT和ImageNetLT等多个长尾数据集上，性能优于标准DDPM以及其他针对类别不平衡扩散模型的替代方法。

Conclusion: 本研究成功将对比学习应用于类别不平衡扩散模型，提出的框架有效提升了尾部类别图像的多样性，并超越了现有基线方法。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [68] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出“无限视频理解”作为多媒体研究的新前沿，旨在解决大型模型在处理极长甚至无尽视频内容时面临的计算、内存及连贯性挑战，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在视频理解方面取得进展，但现有模型在处理分钟或小时以上长视频时，仍面临严重的计算和内存限制，难以维持时间连贯性、追踪复杂事件和保留细粒度细节。

Method: 本文作为一篇立场论文，通过分析当前大型语言模型在长视频理解上的局限性，提出了“无限视频理解”这一蓝天研究目标。它概述了实现此能力的核心挑战，并指出了关键的研究方向，如流式架构、持久记忆机制和分层自适应表示等。

Result: 本文的主要“结果”是明确提出了“无限视频理解”这一愿景和研究目标，并识别了实现此目标所需的关键技术领域和研究方向，为未来多媒体和AI研究提供了指引。

Conclusion: “无限视频理解”被确立为多媒体和更广泛AI研究的下一个重要前沿，它将驱动流式架构、持久记忆等领域的创新，使模型能够持续处理和理解任意长度的视频数据。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [69] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: 针对大型视觉语言模型（VLMs）因图像数据导致预填充时间过长的问题，本文分析了VLM中的注意力稀疏模式，并提出了一种名为BlindSight的免训练优化方法。该方法通过应用输入模板感知的注意力稀疏掩码，显著降低了计算量（FLOPs），同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）在处理视觉数据时，会显著增加提示（prompt）长度。由于注意力计算的二次复杂度，这导致了预填充（prefill）时间过长，成为VLM推理的一个性能瓶颈。

Method: 研究人员分析了VLM的注意力模式，发现大部分层都存在显著稀疏性，即除了通过每个图像的注意力汇聚（attention-sink）令牌外，跨图像注意力极少。基于此观察，他们识别出几种稀疏注意力模式（如仅汇聚、文档掩码和混合文档-汇聚掩码）。在此基础上，他们提出了BlindSight，一种免训练的方法，通过应用输入模板感知的注意力稀疏掩码来优化VLM推理。具体地，该方法利用数据集样本为每个注意力头导出一个与提示无关的稀疏性分类。

Result: 该技术在Qwen2-VL、Qwen2.5-VL和Gemma-3等VLM上进行了评估。结果显示，BlindSight平均可将FLOPs（浮点运算）减少32%-41%，同时在大多数多图像理解基准测试中，与原始模型相比，精度变化仅为-2%至+2%。

Conclusion: BlindSight通过有效利用VLM中固有的注意力稀疏性，为优化其推理效率提供了一种高效且无需额外训练的解决方案。该方法显著降低了计算成本，同时基本保持了原始模型的性能表现，有效缓解了VLM预填充时间过长的瓶颈。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [70] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 论文系统回顾了定量遥感反演方法从物理模型到机器学习再到基础模型的演变，比较了各类方法的特点、局限性及应用场景，并展望了未来发展趋势。


<details>
  <summary>Details</summary>
Motivation: 随着遥感系统和人工智能的发展，传统的物理模型正逐渐被数据驱动和基础模型方法取代。为应对这一方法论演变，论文旨在系统梳理并评估遥感反演技术的最新进展和未来方向。

Method: 论文采用系统综述方法，回顾了遥感反演技术从物理模型（如PROSPECT）到机器学习（如深度学习、多模态融合）再到基础模型（如SatMAE、GFM）的演变过程。文章比较了各范式的建模假设、应用场景及局限性，并重点关注了基础模型在自监督预训练、多模态集成和跨任务适应方面的最新进展。

Result: 论文分析了各类遥感反演范式（物理模型、机器学习、基础模型）的特点、适用性及局限性。研究表明，基础模型展现出显著潜力，尤其是在多模态融合和跨任务适应方面，但仍面临物理可解释性、域泛化能力、有限监督及不确定性量化等挑战。

Conclusion: 未来遥感反演的基础模型将向统一建模能力、跨域泛化能力和物理可解释性方向发展。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [71] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 一种无需微调即可从预训练视频模型中提取光流的新方法，通过反事实提示（KL-tracing）利用生成式视频模型的特性，在多个数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 光流提取是计算机视觉的核心问题，但标签稀缺且合成数据存在模拟与现实差距，使得微调方法不切实际。受大型通用模型成功的启发，研究者旨在探究是否能无需微调，仅通过提示冻结的自监督视频模型（仅用于未来帧预测）来输出光流。

Method: 该研究受到反事实世界模型（CWM）范式的启发，并将其思想扩展到生成式视频模型。研究发现成功的零样本光流提取依赖于模型的三大特性：分布式未来帧预测、因子化隐变量和随机访问解码。基于这些在LRAS架构中独特存在的特性，提出了KL-tracing方法：在视频第一帧中注入局部扰动，模型推演一步，然后计算扰动和未扰动预测分布之间的Kullback-Leibler散度。

Result: 在无需任何光流特有微调的情况下，该方法在真实世界的TAP-Vid DAVIS数据集上，终点误差相对改进了16.6%；在合成的TAP-Vid Kubric数据集上，相对改进了4.7%，均优于现有最佳模型。

Conclusion: 研究结果表明，对可控生成式视频模型进行反事实提示，是获得高质量光流的一种可扩展且有效的替代方法，可替代传统的监督或光度损失方法。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [72] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 提出一种新的基于互信息的CNN视觉解释方法MI CAM，能提供因果解释，并在性能上超越部分现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于机器视觉在医疗和自动化工厂等关键领域的广泛应用，理解卷积神经网络的内部机制及其提供特定推断的原因变得至关重要。

Method: 本文提出一种名为MI CAM的新型后验视觉解释方法，该方法基于激活映射。与以往方法不同，MI CAM通过计算每个特征图与输入图像的互信息进行加权，并通过权重与激活图的线性组合生成显著性可视化。它还通过反事实分析验证了其因果解释能力。

Result: MI CAM在定性和定量指标上与所有最先进方法表现相当，尤其在某些方面性能更优。

Conclusion: MI CAM提供了一种有效的、无偏的模型推理过程可视化解释，有助于揭示CNN的决策原因，并在解释性能上达到了甚至超越了现有最佳水平。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [73] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: 本文提出RadEyeVideo，一种将放射科医生眼动注视数据作为视频序列整合到大型视觉语言模型(LVLMs)中的新方法，显著提升了胸部X光(CXR)报告生成和疾病诊断的性能，并使通用模型超越了特定任务的医学LVLMs。


<details>
  <summary>Details</summary>
Motivation: LVLMs在CXR分析中表现出色，但现有结合放射科医生眼动数据的方法（如热图或文本提示）忽略了眼动顺序。这种顺序能提供关于兴趣区域和检查顺序的宝贵信息，因此需要一种能捕捉眼动时空动态的方法。

Method: 提出RadEyeVideo方法，将放射科医生的眼动注视数据作为视频序列整合。该方法在CXR报告生成和疾病诊断任务中进行评估，使用了三个支持视频输入的通用领域开源LVLMs。

Result: 当LVLMs接收眼动视频提示时，模型性能显著提高：报告生成任务提升高达24.6%，两项任务平均提升15.2%（按比例评估指标）。RadEyeVideo还使通用领域LVLM（LLaVA-OneVision）的表现超越了特定任务的医学LVLMs（如MAIRA-2和CheXagent）。

Conclusion: 研究表明，有效整合领域专家知识（本例中为眼动信息）可以显著增强通用模型在临床任务中的能力。RadEyeVideo是实现LVLMs在医学图像分析中可扩展、以人为中心方法的重要一步。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [74] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: 该论文提出PointSD框架，利用大规模预训练的Stable Diffusion（SD）模型提升3D点云的自监督学习能力，旨在克服3D数据集规模有限的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前的3D自监督学习中，扩散模型因训练于规模有限的3D数据集而性能受限。作者假设，利用在更大规模数据集上训练的文本到图像扩散模型（特别是Stable Diffusion）的强大能力，可以克服这些限制。

Method: 研究者提出了PointSD框架，该框架利用Stable Diffusion模型进行3D自监督学习。具体步骤包括：1) 将SD模型的文本编码器替换为3D编码器，训练一个点云到图像的扩散模型，使点云能够引导渲染噪声图像的去噪过程。2) 利用训练好的点云到图像扩散模型，以无噪声图像为输入、点云为条件，提取SD特征。3) 通过将3D骨干网络的特征与提取到的SD特征对齐，训练3D骨干网络以实现直接的语义学习。

Result: 在下游点云任务和消融研究上的综合实验证明，Stable Diffusion模型能够有效增强点云的自监督学习性能。

Conclusion: Stable Diffusion模型可以成功地应用于3D点云自监督学习，并显著提升其性能，从而弥补了3D数据集规模受限所带来的不足。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [75] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 针对现有手语生成（SLP）模型在误差累积和实时性方面的不足，本研究首次提出一种结合自回归和扩散模型的混合方法，并引入多尺度姿态表示和置信度感知注意力机制，实现了高质量且高效的实时手语生成。


<details>
  <summary>Details</summary>
Motivation: 现有的手语生成（SLP）模型存在局限性：自回归方法在推理时会累积误差，而扩散模型虽能生成高质量输出但因其迭代性质不适用于实时任务。因此，需要一种能兼顾生成质量和实时效率的新方法。

Method: 本研究首次将自回归模型与扩散模型结合应用于手语生成，旨在结合两者的优点。具体方法包括：1. 设计了一个多尺度姿态表示模块，用于从不同发音器中提取并融合精细的身体运动特征。2. 引入了一个置信度感知因果注意力机制，利用关节级置信度分数动态指导姿态生成，以提高准确性和鲁棒性。

Result: 在PHOENIX14T和How2Sign数据集上进行的广泛实验表明，所提出的方法在生成质量和实时流效率方面均表现出显著的有效性。

Conclusion: 本研究提出的混合自回归-扩散模型，结合其创新的多尺度姿态表示和置信度感知注意力机制，成功克服了现有手语生成模型在高质量和实时性之间的权衡，为实时手语生成提供了有效解决方案。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [76] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文针对人-物交互（HOI）检测在真实世界复杂环境下的鲁棒性问题，首次提出了一个专门的鲁棒性基准RoHOI，并引入了语义感知掩蔽渐进学习（SAMPL）策略，显著提升了HOI模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人-物交互（HOI）检测对机器人-人类协助至关重要，但现有模型在真实世界复杂且不可预测的腐蚀（如环境变化、遮挡、噪声）下性能急剧下降，导致预测不准确。目前缺乏专门的基准来评估HOI检测模型的鲁棒性。

Method: 1. 引入首个HOI检测鲁棒性基准RoHOI，该基准基于HICO-DET和V-COCO数据集，包含了20种腐蚀类型，并提出了一种新的鲁棒性评估指标。2. 提出了一种语义感知掩蔽渐进学习（SAMPL）策略，通过整体和局部线索引导模型优化，动态调整学习过程以增强鲁棒特征学习。

Result: 系统分析显示，现有模型在腐蚀条件下性能显著下降。本文提出的SAMPL方法在广泛实验中超越了现有最先进的方法，为鲁棒HOI检测设立了新标准。

Conclusion: 本研究成功构建了首个HOI检测鲁棒性基准，并提出了一种有效的学习策略，显著提升了模型在复杂真实世界环境中的鲁棒性，为未来HOI检测的研究奠定了基础。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [77] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: CLIP持续学习中常忽视模态间隙问题。本文分析模态间隙变化发现其反映预训练知识保留度，并提出MG-CLIP，通过模态间隙保持和补偿策略提升CLIP在类增量学习中的性能，无需回放数据。


<details>
  <summary>Details</summary>
Motivation: 现有将对比语言-图像预训练模型（CLIP）应用于持续学习的研究，大多忽视了其固有的模态间隙，而该间隙是CLIP泛化和适应性的关键因素。因此，研究模态间隙的动态变化并利用其来提升持续学习性能，是本研究的动机。

Method: 本文首先分析了视觉-语言预训练模型微调过程中模态间隙的变化，并观察到模态间隙能有效反映预训练知识的保留程度。基于此，提出了一种名为MG-CLIP的方法，该方法通过“模态间隙保持”策略来缓解遗忘，并通过“模态间隙补偿”策略来增强对新数据的学习能力。

Result: 在多个基准测试上的大量实验表明，所提出的MG-CLIP方法性能优于现有持续学习方法，且无需额外的回放数据。

Conclusion: 本文引入了一种新颖的基于模态间隙的持续学习视角，通过有效利用和管理CLIP的模态间隙，显著提升了其在类增量学习任务中的表现，并有效缓解了遗忘问题。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [78] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: 本文发布了一个高质量、详尽的文本-动作数据集SnapMoGen，并提出先进的生成式掩码模型MoMask++，显著提升了文本到动作生成的精细控制和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法受限于数据集，只能处理短或通用文本提示，导致精细控制和对未知提示的泛化能力不足。

Method: 1. 引入了SnapMoGen数据集，包含20K动作片段（总计44小时）和122K详细文本描述（平均48词），并保留原始时间连续性以支持长期动作生成。2. 提出了MoMask++模型，该模型将动作转换为多尺度token序列，并利用单个生成式掩码Transformer学习生成所有token。3. 利用大型语言模型（LLM）重新格式化非正式用户提示，使其与数据集的表达风格对齐。

Result: MoMask++在HumanML3D和SnapMoGen基准测试上均取得了最先进的性能。此外，研究还展示了通过LLM处理随意用户提示的能力。

Conclusion: 通过构建更丰富、更高质量的数据集和创新的生成模型，本文有效解决了文本到动作生成中精细控制和泛化性的挑战，并为长期动作生成提供了便利。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [79] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出一个基于LLM的姿态估计框架，通过引入非线性MLP连接器取代线性投影器，显著提升了语言引导姿态估计的精度和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的姿态估计方法泛化能力有限。现有语言引导方法（如LocLLM）虽能零样本泛化，但其线性投影器无法有效捕捉复杂的空间-文本交互，限制了高精度定位。

Method: 提出PoseLLM，它是一个基于大型语言模型（LLM）的姿态估计框架。该框架用一个轻量级的两层非线性MLP（带GELU激活）取代了传统的线性投影器，作为视觉-语言连接器，实现分层跨模态特征转换，增强视觉信息和文本关键点描述的融合。

Result: 在COCO验证集上，PoseLLM取得了77.8 AP，比LocLLM提高了0.4 AP。同时，在Human-Art和MPII数据集上保持了强大的零样本泛化能力。

Conclusion: 研究表明，一个简单但强大的非线性连接器可以显著提升语言引导姿态估计的定位精度，且不牺牲泛化能力，推动了该领域的最新进展。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [80] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 提出I²-World，一个高效的4D占用率预测框架。通过解耦场景tokenization和编码器-解码器架构，实现高精度和实时推理，解决3D世界模型tokenization难题。


<details>
  <summary>Details</summary>
Motivation: 3D场景演化预测和新场景生成对自动驾驶系统中的极端情况处理具有巨大潜力。然而，高效地对复杂3D场景进行tokenization仍是3D世界模型面临的关键挑战。

Method: 提出I²-World，一个用于4D占用率预测的高效框架。它将场景tokenization解耦为场景内（使用多尺度残差量化策略压缩3D场景）和场景间（残差聚合时间依赖性）tokenizer。该方法采用编码器-解码器架构：编码器聚合空间上下文并预测变换矩阵以实现高级控制，解码器则基于此矩阵和历史token确保生成过程的时间一致性。

Result: I²-World在4D占用率预测上达到最先进的性能，mIoU和IoU分别比现有方法高出25.1%和36.9%。同时，其计算效率极高，训练内存仅需2.9 GB，并能实现37.0 FPS的实时推理。

Conclusion: I²-World通过创新的解耦tokenization和高效的编码器-解码器架构，有效解决了4D场景预测中复杂的tokenization问题，在性能和计算效率方面均取得了显著突破，为自动驾驶等领域提供了强大的解决方案。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [81] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出Stable Score Distillation (SSD) 框架，旨在解决现有扩散模型在文本引导图像和3D编辑中稳定性、空间控制和编辑强度不足的问题。SSD通过简化结构和引入新机制，实现了更稳健、高效且高保真的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本引导图像和3D编辑方法（如Delta Denoising Score）在稳定性、空间控制和编辑强度方面表现不佳。这些局限性源于对复杂辅助结构的依赖，其引入了冲突的优化信号并限制了精确的局部编辑。

Method: SSD框架通过将单个分类器锚定到源提示词来增强编辑过程的稳定性和对齐性。具体而言，SSD利用无分类器引导（CFG）方程实现跨提示对齐，并引入一个常数项的空文本分支来稳定优化过程。此外，它还包含一个提示增强分支以提高编辑强度，尤其适用于风格转换。

Result: SSD方法在2D和3D编辑任务（包括NeRF和文本驱动的风格编辑）中取得了最先进的结果，且具有更快的收敛速度和更低的复杂性。

Conclusion: SSD提供了一个稳定、高效且高保真的文本引导编辑解决方案，有效克服了现有扩散模型方法的局限性，显著提升了文本引导图像和3D编辑的性能。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [82] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出一个基于Vision Transformer的视觉骨干网络，融合RGB和深度信息以增强泛化能力，并结合对比无监督学习加速强化学习样本效率，通过课程学习实现sim2real迁移。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景外观变化具有鲁棒性并包含3D空间细节。研究旨在利用深度信息与RGB融合，以增强模型的泛化能力。

Method: 核心方法是一个基于Vision Transformer的视觉骨干网络，用于融合RGB和深度模态。具体实现为：不同模态首先由独立的CNN分支处理，然后将结合的卷积特征送入可扩展的Vision Transformer以获得视觉表示。此外，设计了一种基于掩码和非掩码token的对比无监督学习方案，以加速强化学习过程中的样本效率。为实现sim2real迁移，开发了灵活的课程学习调度，在训练过程中部署域随机化。

Result: 摘要中未明确提及具体的实验结果或性能数据。

Conclusion: 摘要中未明确提及研究结论，但提出的方法旨在提高模型泛化性、强化学习的样本效率以及实现sim2real迁移。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [83] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文首次探讨了池化提示方法在FSCIL中的应用，揭示了其性能下降的原因（token维度饱和），并提出LGSP-Prompt，通过空间维度提示有效解决了该问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Few-Shot Class-Incremental Learning (FSCIL)面临数据稀缺和增量学习的双重挑战。传统的池化提示方法在FSCIL中的有效性尚未探究，且现有研究发现其在此场景下存在性能下降，原因在于有限数据下过多的提示导致模型过拟合（token维度饱和）。

Method: 提出LGSP-Prompt（Local-Global Spatial Prompting），将池化提示学习从token维度转向空间维度。该方法通过结合局部空间特征和全局频域表示生成空间提示，以突出输入图像中的关键模式。构建了两个空间提示池，实现动态提示选择，以同时保持现有知识并有效学习新会话。

Result: LGSP-Prompt在多个FSCIL基准测试中取得了当前最先进的性能，在基础知识保留和增量学习方面均表现出显著优势。

Conclusion: LGSP-Prompt通过创新地将池化提示从token维度转向空间维度，有效克服了FSCIL中提示方法面临的挑战，实现了知识的有效保留和增量学习，为FSCIL领域提供了新的SOTA解决方案。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [84] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文揭示大型视觉语言模型（LVLMs）中的幻觉问题源于RoPE编码的长期衰减导致图像对齐偏差。为此，提出MCA-LLaVA，一种基于曼哈顿距离的2D多方向空间衰减位置编码，以缓解此偏差，有效减少幻觉。


<details>
  <summary>Details</summary>
Motivation: LVLMs中的幻觉是一个重大挑战，其关键因素是多模态特征未对齐。本研究发现，RoPE（旋转位置编码）的长期衰减导致指令令牌对图像令牌的感知不均，偏向二维空间中右下区域的令牌（因一维序列距离更近），形成“图像对齐偏差”，从而导致图像-指令交互不足和次优的多模态对齐。

Method: 提出MCA-LLaVA模型，它基于曼哈顿距离，将RoPE的长期衰减扩展为二维、多方向的空间衰减。MCA-LLaVA结合图像令牌的一维序列顺序和二维空间位置进行位置建模，旨在增强指令对不同空间位置图像令牌的均匀感知。

Result: MCA-LLaVA在各种幻觉和通用基准测试中均展现出有效性和通用性。

Conclusion: 通过缓解图像对齐偏差，MCA-LLaVA成功改善了LVLMs中的多模态对齐，有效缓解了幻觉问题，验证了其新颖的2D位置建模方法的有效性。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [85] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出THYME方法，通过分层特征聚合和循环时间细化，解决了视频场景图生成中空间细节和时间依赖捕获不足的问题，并在新数据集AeroEye-v1.0和现有数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频场景图生成方法在捕捉细粒度空间细节和长程时间依赖方面存在不足，导致表示破碎，难以实现动态场景的全面理解。这限制了其在自动驾驶、监控等应用中的有效性。

Method: 提出了“时间分层循环场景图”(THYME)方法。该方法协同整合了分层特征聚合和循环时间细化，以有效建模多尺度空间上下文并强制帧间时间一致性。此外，还发布了一个包含五种交互类型的新型航空视频数据集AeroEye-v1.0。

Result: 在ASPIRe和AeroEye-v1.0数据集上进行的广泛实验表明，所提出的THYME方法优于现有的最先进方法，在地面视角和航空场景中均提供了改进的场景理解能力。

Conclusion: THYME方法通过有效整合分层特征聚合和循环时间细化，成功解决了视频场景图生成中空间和时间建模的局限性，实现了更准确和连贯的场景图，并被证明在多种场景下均优于现有技术。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [86] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 一种从表面波视频推断材料厚度和刚度的方法。


<details>
  <summary>Details</summary>
Motivation: 表面波传播蕴含材料内部物理性质信息，研究旨在仅通过表面波视频推断结构厚度和刚度。

Method: 通过从视频中提取色散关系，然后解决一个基于物理的优化问题，以找到最佳拟合的厚度和刚度参数。

Result: 在模拟和真实数据上均得到验证，结果与实际测量值高度一致。

Conclusion: 该技术为医疗组织属性的居家健康监测提供了概念验证，并可应用于人机交互等领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [87] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 针对医学视觉语言模型(MedVLM)的不可靠性，本研究提出Expert-CFG框架，无需额外训练，通过专家指导和无分类器引导，显著提升MedVLM的可靠性和与临床专业知识的对齐度，且在小参数量下超越了现有的大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型(MedVLM)存在固有的不确定性，经常产生错误或未经证实的回应，这在医疗应用中具有严重后果。现有增强MedVLM性能的方法（如调整模型结构、数据微调）成本高昂，且未能充分与临床专业知识对齐。

Method: 提出了一种名为Expert-Controlled Classifier-Free Guidance (Expert-CFG)的专家在环框架，旨在无需额外训练的情况下使MedVLM与临床专业知识对齐。该框架通过不确定性估计策略识别不可靠输出，然后检索相关参考文献以协助专家突出关键术语，最后应用无分类器引导来精炼MedVLM的token嵌入，确保调整后的输出正确并与专家突出部分对齐。

Result: 在三个医学视觉问答基准上的评估显示，所提出的Expert-CFG模型（4.2B参数，有限专家标注）的性能优于参数量为13B的现有最先进模型。

Conclusion: 研究结果证明了在资源有限的环境中，部署Expert-CFG系统以用于临床使用的可行性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [88] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出S3AD算法，通过解耦2D/3D训练和基于置信度的异常评分，提升自动驾驶中3D检测模型的泛化能力和异常目标过滤能力，并构建了包含多种新类别的KITTI-AR数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测模型在开放道路上面对稀有异常类别时，易发生误检或漏检，且其泛化能力受限于2D/3D耦合训练和训练样本尺度分布多样性不足，因此需要增强模型对任意形状目标的泛化能力和异常过滤能力。

Method: 本文提出S3AD（Stereo-based 3D Anomaly object Detection）算法，该算法解耦了3D和2D的训练策略以提升任意3D前景检测的泛化能力，并基于前景置信度预测提出了目标级异常评分算法。为进一步验证和增强异常检测的泛化性，利用3D渲染方法合成了两个增强现实双目3D检测数据集KITTI-AR，包含97个新类别（共6k对立体图像），其中KITTI-AR-ExD用于解决样本稀疏问题，KITTI-AR-OoD用于模拟零样本场景并评估3D异常检测。

Result: 提出了S3AD算法，通过解耦2D/3D训练和引入基于置信度的异常评分机制，增强了3D检测模型对任意形状目标的泛化能力和异常过滤能力。构建了KITTI-AR增强现实数据集，包含ExD和OoD子集，有效支持了算法的训练与零样本异常检测的评估。实验验证了算法和数据集的性能。

Conclusion: S3AD算法通过创新的训练策略和异常评分机制，结合新构建的KITTI-AR数据集，有效提升了3D检测模型在自动驾驶场景下对稀有异常目标的泛化检测和过滤能力，为未来更安全的自动驾驶提供了解决方案。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [89] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出一种新颖的球面采样方法，使二维预训练模型能有效处理全景图像，减轻畸变，并在分割任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前全景图像任务缺乏大规模数据集，且现有二维预训练模型无法有效处理全景图像固有的畸变和不连续性，导致性能受限。

Method: 引入一种基于预训练模型权重的球面离散采样方法，使得现有二维预训练模型能够直接应用于全景图像并减轻畸变。该方法进一步应用于全景图像分割，将球面模型获得的特征用作特定通道注意力的掩码。

Result: 在全景图像分割任务中，该方法在常用室内数据集Stanford2D3D上取得了值得称赞的结果。

Conclusion: 本文提出的球面采样方法成功使二维预训练模型适应全景图像处理，有效缓解了畸变问题，并在实际应用中展现出良好性能。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [90] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 本文提出了一种名为Track-On的Transformer模型，实现了在线环境下（无未来帧信息）的长期点跟踪，并在七个公开基准上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的长期点跟踪方法大多依赖离线处理和未来帧信息，不适用于实时流媒体、具身AI等需要即时决策的在线应用场景。因此，研究在无未来信息访问限制下的在线长期点跟踪至关重要，并需考虑视角不变性。

Method: 作者首先评估了视觉基础模型在在线点跟踪任务中的适用性，发现它们可作为有效初始化并集成到跟踪管道中。为解决在线长期跟踪中的时间连贯性问题，本文引入了Track-On模型。该模型是一个基于Transformer的架构，将每个跟踪点视为一个查询，并逐帧处理视频，通过内置记忆机制传播外观和上下文信息以维持跨帧的连贯性。

Result: Track-On模型在七个公共基准测试中取得了新的最先进（SOTA）性能。

Conclusion: 本研究证明了在没有未来帧访问的严格在线设置下，实现长期点跟踪的可行性，为实时应用提供了有效的解决方案。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [91] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: 本文提出StaRFM，一个统一的即插即用框架，旨在解决基础模型在视觉语言分类和医学分割任务中面临的分布漂移和置信度错位问题，显著提升了泛化和校准性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如CLIP、SAM）虽推动了计算机视觉和医学影像发展，但其部署受限于两大挑战：训练与测试数据间的“分布漂移”和导致错误预测的“置信度错位”。现有解决方案是领域特化的。

Method: StaRFM框架引入了Fisher信息惩罚（FIP）来减少CLIP和SAM嵌入的协变量偏移（通过补丁级正则化扩展到3D医学数据），并引入了置信度错位惩罚（CMP）来校准分割任务中的不确定性（针对体素级预测重新 формуulated）。理论上，FIP通过Fisher-Rao范数控制泛化，CMP通过Brier分数优化最小化校准误差。

Result: StaRFM在19个视觉数据集上实现+3.5%准确率和28%更低的ECE；在医学分割任务中（如BraTS、ATLAS）实现84.7% DSC和4.8mm HD95；跨域性能差距比现有方法降低40%。

Conclusion: StaRFM是一个即插即用的框架，只需极小的架构修改即可与基础模型无缝集成。它有效统一解决了基础模型在不同领域中的分布漂移和置信度错位问题，并显著提升了性能和可靠性。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [92] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 本研究利用基于Stable Diffusion的生成模型，从单张自我中心视角图像重建可动画人物替身，旨在克服遮挡和失真，实现更经济、普适的远程临场体验。


<details>
  <summary>Details</summary>
Motivation: 理想的数字远程临场需要精确复制人物。自我中心视角虽能使用便携、低成本设备，却面临遮挡和身体比例失真等挑战。现有方法鲜少从自我中心视角重建人物外观，且多依赖多视角训练数据，缺乏基于生成先验的方法，限制了便携性和泛化能力。

Method: 本研究提出一个基于Stable Diffusion的生成流水线，首次使用生成骨干从自我中心输入重建可动画人物替身。该方法受SiTH和MagicMan启发，利用ControlNet和Stable Diffusion骨干从被遮挡的自上而下自我中心图像生成逼真的正面视图，随后将此正面表示输入图像到动作模型。

Result: 成功地将单张自上而下自我中心图像转换为逼真的正面表示，并能进一步生成人物替身动作。该方法通过使用Stable Diffusion作为生成骨干，显著降低了训练负担并提升了泛化能力。

Conclusion: 本研究是首个利用生成骨干从自我中心输入重建可动画人物替身的尝试，实现了从极简输入生成替身动作。这为构建更易访问和更具泛化性的远程临场系统铺平了道路。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [93] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种评估绘画过程的新框架，包括首个绘画过程数据集PPAD和基于Transformer的模型PPJudge，其性能超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有艺术图像评估方法主要关注静态最终图像，忽略了绘画过程的动态性和多阶段特性。

Method: 1. 引入绘画过程评估数据集（PPAD），这是第一个包含真实和合成绘画过程图像，并由领域专家标注了八个详细属性的大型数据集。 2. 提出了PPJudge模型，这是一个基于Transformer的模型，通过时间感知位置编码和异构专家混合架构进行了增强。

Result: 实验结果表明，所提出的方法在准确性、鲁棒性以及与人类判断的一致性方面均优于现有基线。

Conclusion: 该研究为计算创意和艺术教育提供了新的见解。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [94] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: 本文提出AGCD-Net模型，通过引入结合新型卷积编码器（Hybrid ConvNeXt）和基于因果干预的注意力引导模块（AG-CIM），有效解决了上下文感知情感识别中的情境偏见问题，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统上下文感知情感识别（CAER）方法存在情境偏见问题，即背景情境与情感标签之间存在虚假关联，导致识别不准确。

Method: 本文提出AGCD-Net模型，包含：1) Hybrid ConvNeXt，一个新型卷积编码器，通过整合Spatial Transformer Network和Squeeze-and-Excitation层增强特征校准。2) Attention Guided - Causal Intervention Module (AG-CIM)，核心模块，运用因果理论扰动情境特征，隔离虚假关联，并通过面部特征进行注意力驱动校正以减轻情境偏见。

Result: 在CAER-S数据集上的实验结果表明，AGCD-Net实现了最先进的性能，证明了其有效性。

Conclusion: AGCD-Net有效解决了情境偏见问题，显著提升了复杂场景下情感识别的鲁棒性，并突出了因果去偏在情感计算中的重要性。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [95] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 针对图像-文本匹配中高阶关联和语义歧义问题，本文提出了AAHR框架，通过动态聚类对比学习、多粒度特征提取、邻里关系建模及动量对比学习，显著提升了匹配的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有图像-文本匹配方法难以处理相似实例间的高阶关联和语义歧义（如软正负样本），且未能充分利用训练批次内的邻里关系，限制了模型学习高阶共享知识的能力。

Method: 本文提出AAHR框架，其主要方法包括：1) 通过动态聚类原型对比学习构建统一表示空间，缓解软正样本问题；2) 引入全局和局部特征提取机制及自适应聚合网络，增强全粒度语义理解；3) 采用模态内和模态间关联矩阵结合GNN，深入探索并增强实例间的邻里关系和语义交互；4) 整合动量对比学习以扩展负样本集，提升特征辨别能力。

Result: 实验结果表明，AAHR在Flickr30K、MSCOCO和ECCV Caption数据集上均超越了现有最先进的方法，显著提高了图像-文本匹配的准确性和效率。

Conclusion: AAHR框架通过有效解决语义歧义和充分利用高阶关联，显著提高了图像-文本匹配的性能，为该领域提供了新的解决方案。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [96] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出了一种分段感知视觉分词框架及双级对齐策略，旨在解决无标注手语翻译（SLT）中模型复杂度高和计算需求大的问题，显著降低了输入序列长度，提升了模型可扩展性，并在PHOENIX14T数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前无标注手语翻译（SLT）方法虽然性能优异，但普遍伴随模型复杂度高和计算需求大，导致在大规模手语数据集上的可扩展性受限。

Method: 1. 提出分段感知视觉分词框架，利用手语分段将连续视频转换为离散的、手语信息化的视觉token。
2. 引入token-to-token对比对齐目标。
3. 采用双级监督机制，同时对齐语言嵌入和中间隐藏状态，以实现细粒度跨模态对齐。

Result: 1. 相较于先前方法，输入序列长度减少高达50%，内存使用量降低2.67倍，显著提升了可扩展性。
2. 在PHOENIX14T基准测试中，性能显著超越了现有最先进方法。
3. 在不依赖词汇标注的情况下，改进了细粒度跨模态对齐。
4. 在可比较序列长度下，性能仍优于先前工作，验证了所提出分词和对齐策略的有效性。

Conclusion: 所提出的分段感知视觉分词和双级对齐策略，有效解决了无标注手语翻译的计算效率和可扩展性问题，并在提升模型性能方面展现出巨大潜力，为大规模手语翻译提供了可行方案。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [97] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 本文提出一种跨模态知识蒸馏（CKD）方法，利用RGB数据和ANNs提升脉冲神经网络（SNNs）在事件数据上的性能。CKD通过语义相似性、滑动替换和间接分阶段蒸馏解决跨模态和跨架构挑战，并在神经形态数据集上实现了超越现有最佳方法的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在计算机视觉领域展现出潜力，但受限于标注事件基数据集稀缺和SNN架构不成熟，其性能仍逊于人工神经网络（ANNs）。因此，亟需一种方法来提升SNNs在DVS数据上的性能。

Method: 本文提出跨模态知识蒸馏（CKD）方法，旨在利用RGB数据和高性能ANNs对SNNs进行知识蒸馏。为解决跨模态和跨架构挑战，CKD方案结合了语义相似性和滑动替换以缓解跨模态问题，并采用间接分阶段知识蒸馏来应对跨架构问题。

Result: 所提出的方法在N-Caltech101和CEP-DVS等主流神经形态数据集上进行了验证。实验结果表明，该方法超越了当前最先进（State-of-the-Art）的方法。

Conclusion: 所提出的跨模态知识蒸馏（CKD）方法有效地提升了SNNs在DVS数据上的性能，成功克服了跨模态和跨架构的难题，并取得了领先的实验结果，为SNNs的发展提供了新的有效途径。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [98] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: 针对多模态大语言模型(MLLMs)在医疗领域中存在的提示敏感性和高置信度错误问题，本文提出Prompt4Trust，一个基于强化学习的提示增强框架，旨在提升MLLMs的置信度校准和任务准确性，并在医疗VQA任务上达到SOTA，同时展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在医疗保健应用中前景广阔，但其在安全关键环境中的部署受限于两个关键问题：1) 对提示设计的敏感性；2) 以高置信度生成错误响应的倾向。由于临床医生可能依赖模型所表达的置信度来评估其预测的可靠性，因此当模型表达高置信度时，其预测也必须高度准确，这正是当前MLLMs的痛点。

Method: 本文提出了Prompt4Trust，一个首个针对MLLM置信度校准的强化学习（RL）提示增强框架。该方法训练一个轻量级LLM，使其能够生成上下文感知的辅助提示，这些提示用于引导下游任务MLLM，使其生成的响应中表达的置信度能更准确地反映预测准确性。与传统校准技术不同，Prompt4Trust特别优先考虑对安全和可信临床决策最关键的校准方面。

Result: 1. 除了提升临床驱动的校准目标外，该方法还提高了任务准确性。
2. 在PMC-VQA基准测试（包含多种医学影像模态的多项选择题）上，实现了医疗视觉问答（VQA）的最先进（SOTA）性能。
3. 使用小型下游任务MLLM训练的框架，在实验中对大型MLLM展现出有前景的零样本泛化能力，表明了无需额外计算成本即可实现可扩展校准的潜力。

Conclusion: 该工作证明了自动化且与人类对齐的提示工程在提升多模态大语言模型（MLLMs）在安全关键设置中的可信度方面的巨大潜力。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [99] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 本文提出一种新的盲运动去模糊框架，通过预训练深度生成模型优化模糊核初始化，解决了现有方法对初始核敏感的问题，并在均匀和非均匀去模糊任务上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度先验的盲运动去模糊（BMD）方法受限于其优化过程的高度非凸性，导致对初始模糊核的敏感性过高。

Method: 提出一个新颖的BMD框架，利用深度生成模型来编码核先验并改善模糊核的初始化。具体而言，预训练一个基于生成对抗网络（GAN）的核生成器来表征核的先验分布，以及一个核初始化器来提供高质量的起始点。通过结合这两个组件，将BMD解约束在一个紧凑的潜在核流形中，从而缓解了初始化敏感性。该方法可即插即用地集成到现有BMD方法中，并能扩展到盲非均匀运动去模糊。

Result: 在挑战性基准数据集上，实现了最先进的性能，并且无需额外先验即可处理盲非均匀运动去模糊问题。

Conclusion: 本研究通过引入优化的核初始化策略，成功解决了深度先验盲运动去模糊方法对初始核敏感的难题，显著提升了算法的鲁棒性和性能，并有效扩展到更复杂的非均匀去模糊任务，为该领域提供了普适且高效的解决方案。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [100] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文提出一种语义感知的平面图定位框架，通过联合估计深度和语义射线并采用由粗到精的方法构建概率体积，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有平面图定位技术主要关注基于深度的结构线索，忽略了平面图中丰富的语义信息（如窗户和门的位置）。

Method: 引入一种语义感知的定位框架，联合估计深度和语义射线，并整合两者来预测结构-语义概率体积。该体积采用由粗到精的方式构建：先少量采样获得初始低分辨率体积，再在概率高区域进行密集采样以细化，最终预测2D位置和方向角。

Result: 在两个标准平面图定位基准上，该方法显著优于现有最先进方法，召回率指标有显著提升。此外，该框架可轻松整合房间标签等额外元数据，进一步提高精度和效率。

Conclusion: 通过整合语义信息，本文提出的框架在平面图定位方面取得了突破性进展，表现出卓越的性能和灵活性，为未来的研究提供了新的方向。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [101] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: 本文提出Geo-RepNet，一个融合RGB和深度信息的几何感知卷积框架，旨在解决内窥镜手术阶段识别中视觉相似度和结构线索缺乏的挑战，并在真实ESD数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 外科手术阶段识别对于开发智能辅助系统至关重要，尤其是在内窥镜黏膜下剥离术（ESD）等微创手术中。然而，不同阶段间高度的视觉相似性以及RGB图像中缺乏结构线索，对识别提出了显著挑战。深度信息能提供宝贵的几何线索，补充外观特征。

Method: 本文率先利用深度信息进行手术阶段识别，并提出Geo-RepNet。该框架是一个几何感知卷积网络，整合了RGB图像和深度信息。它以可重参数化的RepVGG为骨干，并包含两个核心模块：深度引导几何先验生成（DGPG）模块，用于从原始深度图中提取几何先验；几何增强多尺度注意力（GEMA）模块，通过几何感知的交叉注意力及高效的多尺度聚合注入空间指导。为评估方法有效性，作者构建了一个包含九个阶段、密集帧级标注的ESD数据集。

Result: 在所构建的ESD数据集上进行的大量实验表明，Geo-RepNet在复杂和低纹理的手术环境下，实现了最先进的性能，同时保持了鲁棒性和高计算效率。

Conclusion: Geo-RepNet通过有效整合深度信息，显著提升了手术阶段识别的准确性和鲁棒性，为智能手术辅助系统提供了一个高效且先进的解决方案。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [102] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT和原型网络，在少样本图像分类中表现出色，超越CNN和部分Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer (ViTs) 的强大表示能力在少样本图像分类中未被充分利用。

Method: 引入ViT-ProtoNet，将ViT-Small骨干网络集成到原型网络框架中。通过平均支持样本的类别条件token嵌入来构建鲁棒原型。

Result: 在四个标准基准测试中，ViT-ProtoNet始终优于基于CNN的原型网络，5样本准确率提高高达3.2%，并在潜在空间中表现出优越的特征可分离性。它也优于或与使用更轻量级骨干的Transformer基竞争对手相当。

Conclusion: ViT-ProtoNet是少样本分类的强大灵活方法，为基于Transformer的元学习器建立了新的基线。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [103] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为深度角度A* (DAA*) 的新路径模仿学习方法，通过将路径角度自由度（PAF）引入A*算法，自适应地提高路径平滑度和与参考路径的相似性，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在专家演示的路径模仿学习中，路径平滑度常常被忽视，导致生成的路径不够优化或自然。

Method: 引入了深度角度A* (DAA*) 方法，该方法将路径角度自由度 (PAF) 整合到A*算法中。PAF旨在通过平衡移动角度的最小和最大值来探索其对路径节点扩展的影响，从而实现高自适应性。DAA*通过联合优化路径缩短（对应启发式距离）和路径平滑（对应PAF），使预测路径与参考路径紧密对齐，从而提高路径最优性。

Result: DAA*在7个数据集（包括迷宫、视频游戏和真实无人机视角数据集）上表现出显著改进：与Neural A*相比，路径相似性（SPR提高9.0%，ASIM提高6.9%，PSIM提高3.9%）和路径长度更优。在联合学习路径损耗和路径概率图损耗时，DAA*也显著优于最先进的TransPath（SPR提高6.7%，PSIM提高6.5%，ASIM提高3.7%）。

Conclusion: DAA*有效解决了路径模仿学习中的平滑度问题，实现了更高的路径相似性和最优性。研究也指出了路径最优性与搜索效率之间存在细微的权衡。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [104] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 针对室外多光谱点云分类中稀疏标注、尺度变化和长尾分布问题，本文提出了一种基于自适应多尺度融合的增强分类方法，通过平衡采样、多尺度特征融合和自适应混合损失显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法在应用于室外多光谱点云数据时面临挑战，包括标注稀疏、地物尺度差异大和类别呈长尾分布，导致分类性能不佳。

Method: 该方法包含三个阶段：1. **训练集生成：** 设计网格平衡采样策略以处理稀疏标注。2. **特征学习：** 提出多尺度特征融合模块，融合不同尺度地物的浅层特征，避免细节丢失。3. **分类：** 设计自适应混合损失模块，利用带自适应权重的多分类头，平衡各类别的学习能力，尤其提升小类别的分类性能。

Result: 在三个多光谱点云数据集上的实验结果表明，所提出的方法相比现有先进方法具有更好的有效性。

Conclusion: 该方法通过有效应对室外多光谱点云分类中的稀疏标注、尺度变化和长尾分布问题，显著提升了分类性能，特别是对小类别地物的识别能力。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [105] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 为解决透明图像生成难题，本文提出首个RGBA基准ALPHA和高效的ALPHAVAE模型，显著提升透明图像重建与生成质量，且训练数据量远低于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有潜变量扩散模型在RGB图像合成方面表现出色，但透明或分层内容（RGBA图像）的生成研究甚少，主要原因在于缺乏大规模基准。

Method: 本文提出了ALPHA，这是首个全面的RGBA基准，通过在规范背景上进行alpha混合来适配标准RGB指标。此外，引入了ALPHAVAE，一个统一的端到端RGBA VAE，它通过整合专用alpha通道来扩展预训练的RGB VAE。该模型采用复合目标函数进行训练，结合了alpha混合像素重建、块级保真度、感知一致性和双KL散度约束，以确保RGB和alpha表示的潜在保真度。

Result: 我们的RGBA VAE模型仅使用8K图像进行训练（而之前方法使用1M图像），在重建方面相较LayerDiffuse，PSNR提高了4.9 dB，SSIM提高了3.2%。在潜在扩散框架中进行微调时，它还能实现更出色的透明图像生成。

Conclusion: 本研究通过构建首个RGBA基准和开发高效的ALPHAVAE模型，成功解决了透明图像生成领域存在的空白，显著提升了透明图像的重建和生成质量，为该领域未来的研究奠定了基础。代码、数据和模型已公开，以确保可复现性。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [106] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出ProactiveBench基准和PAUC评估指标，旨在促进和准确评估多模态对话系统中的主动交互能力，特别是考虑响应的时间动态性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的深入，用户期望系统能进行更主动的交互，例如在视频播放时自主决定多轮响应时机，而非传统的轮流对话。现有评估方法未能有效衡量这种主动交互能力。

Method: 研究引入了ProactiveBench，这是首个用于评估系统主动交互能力的综合基准。同时，提出了PAUC，这是首个考虑模型响应时间动态性的评估指标，以更准确地评估主动式系统。通过对ProactiveBench上多个基线系统进行广泛基准测试，并进行用户偏好研究。

Result: 研究结果表明，PAUC与人类偏好的一致性优于传统仅考虑文本内容的评估指标。这通过广泛的基准测试和用户研究得到了验证。

Conclusion: PAUC能更忠实地评估主动交互场景中的用户体验。ProactiveBench和PAUC的提出，有助于推动多模态对话系统在主动交互领域的发展。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [107] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 本研究评估了多种机器学习方法（包括梯度提升和CNN）在去除高光谱卫星影像云影方面的表现。结果表明，轻量级CNN模型在保持高精度的同时，具有低存储和快速推理的优势，非常适合实时处理和卫星板载AI系统。


<details>
  <summary>Details</summary>
Motivation: 云和云影遮蔽是高光谱卫星成像中一项关键的预处理步骤，它对于提取高质量、可供分析的数据至关重要。

Method: 本研究评估了多种机器学习方法，包括梯度提升方法（如XGBoost和LightGBM）以及卷积神经网络（CNNs）。重点研究了带有特征降维的CNN及其参数量极小的变体。

Result: 所有提升方法和CNN模型都达到了超过93%的准确率。其中，带有特征降维的CNN被证明是最有效的，在准确性、存储需求和CPU/GPU推理速度之间取得了最佳平衡。其轻量级版本（最高597个可训练参数）在部署可行性、准确性和计算效率方面表现出最佳的权衡。

Conclusion: 这些结果展示了轻量级人工智能模型在实时高光谱图像处理方面的潜力，支持开发用于空间应用的卫星板载AI系统。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [108] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 本文提出DICCAE模型，通过细粒度类别对齐和动态混淆损失，解决音视频预训练中易混淆类别的区分问题，并在VGGSound数据集上取得接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频预训练范式仅关注整体模态对齐，未能像人类认知那样通过认知归纳和对比强化对易混淆类别的区分，导致模型在处理相似活动时辨别能力不足。

Method: 提出动态类间混淆感知编码器（DICCAE），它通过根据类间混淆程度动态调整混淆损失，实现细粒度、类别级别的音视频表示对齐，以增强相似活动区分能力。此外，引入了一个结合音视频及其融合的新型训练框架，并提出簇引导的音视频自监督预训练策略以缓解数据稀缺。

Result: DICCAE在VGGSound数据集上Top-1准确率达到65.5%，接近当前最佳性能。通过广泛的消融研究，验证了其特征表示质量以及各个模块的必要性。

Conclusion: DICCAE通过引入细粒度类别对齐和动态混淆损失，有效提升了音视频模型对易混淆类别的区分能力，并在数据稀缺环境下表现优异，为音视频理解任务提供了有效的新方法。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [109] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: 本文提出Fast3D，一个即插即用的视觉令牌剪枝框架，旨在解决3D多模态大语言模型（MLLMs）中过多的视觉令牌导致的计算效率低下问题，通过全局注意力预测和样本自适应剪枝实现高效加速。


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs虽然场景理解能力强，但因处理过多的以对象为中心的视觉令牌而面临严重的计算效率挑战。尽管2D MLLMs中视觉令牌剪枝已显示出潜力，但由于令牌结构的基本差异，其在3D领域的适用性尚未得到充分探索。

Method: 基于3D令牌冗余和全局注意力模式可预测非关键令牌的洞察，本文提出了Fast3D框架，包含两项技术创新：1) 全局注意力预测（GAP），利用轻量级神经网络预测目标模型的全局注意力分布，以有效估计令牌重要性进行精确剪枝；2) 样本自适应视觉令牌剪枝（SAP），通过基于注意力的复杂度评估引入动态令牌预算，根据输入特性自动调整逐层剪枝比例。这两个技术均不修改目标模型的参数。

Result: Fast3D在五个基准测试中进行了广泛评估，结果验证了其有效性，特别是在高视觉令牌剪枝率下表现出色。

Conclusion: Fast3D成功地通过有效的视觉令牌剪枝，解决了3D MLLMs的计算效率瓶颈，提高了其实用部署性，且无需修改现有模型参数，展现了良好的通用性和加速效果。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [110] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 本研究发现，通过强大的预训练，简单的编码器-only视频Vision Transformers (Video ViTs)模型在交通异常检测（TAD）中表现出色，甚至超越复杂的现有方法，同时效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有交通异常检测（TAD）方法常依赖复杂的多阶段或多表示融合架构，但其必要性存疑。鉴于基础模型在高级预训练支持下能以简单架构超越专用设计，作者旨在探究简单编码器-only方法在TAD中的潜力。

Method: 研究采用架构简单的编码器-only方法，使用普通的视频Vision Transformers (Video ViTs)。通过探索不同预训练策略（包括弱监督、全监督、自监督的掩码视频建模MVM）及其对TAD性能的影响，并进一步在未标记的驾驶视频上进行领域自适应预训练（DAPT）。

Result: 研究发现：(i) 强大的预训练使简单的编码器-only模型能够匹配甚至超越专业化的最先进TAD方法的性能，同时效率显著更高；(ii) 弱监督和全监督预训练在标准基准上虽有优势，但对TAD效果不佳，自监督的掩码视频建模（MVM）提供了最强的信号；(iii) 在未标记驾驶视频上进行领域自适应预训练（DAPT）进一步提高了下游性能，且无需异常样本。

Conclusion: 研究结果强调了预训练的重要性，并表明可以以最小的架构复杂性构建有效、高效且可扩展的交通异常检测（TAD）模型。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [111] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究开发了一个基于CNN的图像分类系统，通过叶片图像自动检测和分类八种常见作物病害，并提供治疗建议，部署于移动平台，旨在提高农业生产力和粮食安全。


<details>
  <summary>Details</summary>
Motivation: 作物病害严重影响农业生产力和全球粮食安全，特别是在大规模农业中，早期识别常延迟或不准确。

Method: 采用深度学习流程，包括从大型标注数据集中获取图像、通过调整大小、归一化和增强进行预处理，并使用TensorFlow和Keras的Sequential API训练CNN模型。CNN架构包含三层卷积层、最大池化层、展平层和全连接层，最终通过Softmax输出进行多分类。

Result: 系统训练准确率高达约90%，在未见数据上表现可靠（验证准确率约60%，存在轻微过拟合）。模型集成了治疗推荐模块，为检测到的病害提供可行的干预建议。该解决方案已部署在开源、移动兼容平台上，支持农民进行实时图像诊断。

Conclusion: 该研究为精准农业贡献了一个可扩展、易于访问的工具，有助于减少对人工检查的依赖，促进可持续病害管理，并强调了CNN在作物健康监测和提升全球粮食生产弹性方面的巨大潜力。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [112] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出一个低资源、本地部署的ML/AI流程，旨在帮助资源和计算能力有限的小型研究团队有效处理和分析相机陷阱数据。


<details>
  <summary>Details</summary>
Motivation: 尽管相机陷阱数据收集能力不断增强，但数据的处理和管理（尤其是整合ML/AI工具）仍面临巨大挑战，包括数据量庞大、标注困难、环境对数据质量的影响以及ML/AI工具集成所需的计算资源和专业知识。小型研究团队尤为受限于资源和计算专长。

Method: 本文提供了一个低资源、本地部署的相机陷阱数据处理流程指南，该流程整合了ML/AI能力。它专注于为资源和计算专业知识有限的小型研究团队提供实用且易于访问的数据传输、推理和评估方法。

Result: 该流程使研究人员，特别是小型研究团队，能够有效地处理不断增长的相机陷阱数据集，并从中发现有意义的洞察。

Conclusion: 该论文提供了一个实用的低资源ML/AI解决方案，赋能资源受限的小型研究团队高效处理和分析相机陷阱数据，从而从海量数据中获取宝贵的科研发现。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [113] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 本文提出一种用于实时在轨（in-situ）天体地貌特征追踪的新系统。该系统利用轻量级神经网络，结合改进的域适应方法进行特征检测和新颖的注意力对齐公式进行特征描述，旨在克服传统方法和现有学习方法的局限性，并在性能上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 自动航天应用（如TRN、EDL、危险分析和科学数据收集）迫切需要天体地貌特征的检测与追踪。传统方法受限于计算能力、成本高、处理慢且泛化性差。新兴的基于学习的计算机视觉技术虽有潜力，但其计算需求超出现有航天器硬件的实时处理能力，且缺乏多样的地外环境标注训练数据。

Method: 本文提出通过检测和描述实现地标追踪的新颖方案。采用轻量、计算高效的神经网络架构，专为现有航天器飞行处理器上的实时执行设计。地标检测方面，提出改进的域适应方法，使其能利用独立且成本低廉的训练数据识别天体地形特征。地标描述方面，引入新颖的注意力对齐公式，学习鲁棒的特征表示，即使地标视角发生显著变化也能保持对应关系。这些贡献共同构成一个统一的地标追踪系统。

Result: 该统一的地标追踪系统与现有最先进技术相比，展示出卓越的性能。

Conclusion: 该研究成功构建了一个高性能的在轨地标追踪系统，通过结合轻量级神经网络、改进的域适应和注意力对齐技术，解决了天体特征识别的挑战，为自主航天应用提供了关键支持。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [114] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的三维多人运动预测模型，通过简化时空交互，在保持SOTA性能的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 三维多人运动预测任务复杂，原因在于个体历史运动和个体间交互的依赖性；有效建模这些交互通常会产生高昂的计算成本。

Method: 通过简化时空交互，提出了一种计算高效的模型。具体方法包括：设计轻量级双分支以分别学习个体和多人的局部与全局表示；引入新型跨级别交互模块来整合两个分支的时空表示；显式融入空间个体间距离嵌入以进一步增强交互建模。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW等标准数据集上，在多个指标上实现了最先进的性能，同时显著降低了计算成本。

Conclusion: 所提出的计算高效的时空设计，成功解决了三维多人运动预测中交互建模的计算成本问题，并在多个标准数据集上取得了领先的预测性能。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [115] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: 提出SegVec3D，一个新颖的3D点云实例分割框架，它结合注意力、嵌入学习和跨模态对齐，实现无监督分割及3D数据与自然语言的零样本对齐。


<details>
  <summary>Details</summary>
Motivation: 旨在以最少监督统一3D点云实例分割和多模态理解，并提高实际部署性，优于现有方法如Mask3D和ULIP。

Method: 构建分层特征提取器以增强几何结构建模；通过对比聚类实现无监督实例分割；在共享语义空间中对齐3D数据与自然语言查询，支持零样本检索；整合了注意力机制、嵌入学习和跨模态对齐。

Result: 成功地以最少监督统一了3D点云实例分割和多模态理解，并具有良好的实际部署性，展现出优于现有方法的独特能力。

Conclusion: SegVec3D提供了一个创新的统一框架，有效解决了3D点云实例分割和多模态理解的挑战，尤其是在低监督和实际应用方面具有显著优势。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [116] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出CKAA框架，通过双层知识对齐和任务置信度引导的适配器混合，解决了PEFT-based持续学习方法在误导性任务ID下因特征子空间不对齐导致的决策模糊问题，提升了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于PEFT的持续学习方法，在面临误导性任务ID时，由于独立训练的子模块导致特征子空间不对齐，容易产生模糊决策，缺乏鲁棒性。

Method: 提出Cross-subspace Knowledge Alignment and Aggregation (CKAA) 框架，包含两项创新：1) 双层知识对齐 (DKA)：通过对齐跨子空间的类内特征分布和特征模拟学习全局分类器，提高模型区分能力。2) 任务置信度引导的适配器混合 (TC-MoA)：一种鲁棒的推理方案，基于任务置信度自适应聚合任务知识，避免误导性预测的过度自信。

Result: 广泛的实验证明，CKAA性能优于现有基于PEFT的持续学习方法。

Conclusion: CKAA通过创新的知识对齐和推理机制，有效提升了PEFT-based持续学习模型在复杂和误导性任务环境下的决策鲁棒性和整体性能。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [117] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net首次在双曲空间中结合掩码图像建模和知识蒸馏，实现了视觉-语义层次结构模型的高效训练，并在下游任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉和语义概念常以层次结构呈现，MERU已成功在双曲空间中捕捉到这种视觉-语义层次。然而，如何更高效地训练模型以捕捉和利用这种层次结构仍是一个关键挑战。

Method: 本文提出Hyperbolic Masked Image and Distillation Network (HMID-Net)，这是一种新颖且高效的方法，它将掩码图像建模 (MIM) 和知识蒸馏技术整合到双曲空间中。同时，还引入了一个专门为双曲空间设计的蒸馏损失函数，以促进有效的知识迁移。

Result: 实验证明，在双曲空间中应用MIM和知识蒸馏技术可以取得与欧几里得空间同样显著的成功。广泛的评估显示，HMID-Net在多种下游任务中表现出色，在图像分类和检索方面显著优于现有模型如MERU和CLIP。

Conclusion: HMID-Net是一种高效且有效的方法，能够在双曲空间中训练模型以捕捉视觉-语义层次结构，并在各种下游任务中展现出卓越的性能。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [118] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 通过识别并移除大型语言模型推理路径中的冗余（利用注意力分数和结构感知剪枝），本方法在不经训练的情况下显著提高了模型在推理密集型基准测试（特别是数学竞赛）上的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长篇推理中存在大量冗余，注意力模式分散，尤其错误答案的注意力稀疏度更高，这阻碍了其性能。研究旨在通过消除这些“干扰”来提升“清晰思考”能力。

Method: 1. **冗余识别:** 通过衡量token级别注意力与一个附加在每个中间推理步骤后的“思考结束”特殊token的关联来识别冗余。2. **结构感知剪枝:** 优先移除对推理贡献低的推理块中的token。3. **推理恢复:** 移除冗余token和注入的指令后，继续生成推理过程。

Result: 1. 在推理密集型基准测试上显著提高了整体准确率，且无需任何训练。2. 在AIME和AMC等数学竞赛基准测试上表现出色，这些测试中推理冗余更为常见。

Conclusion: 通过系统性地移除大型语言模型推理过程中的冗余，可以有效提升模型的推理性能和准确性，尤其对于复杂推理任务，证明了清除干扰以促进“清晰思考”的有效性。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [119] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 针对多标准评估中现有方法的局限性（如主观性、同质性假设），本文提出一种结合两种虚拟差距分析（VGA）模型的新型MCA方法，该方法基于线性规划，能有效整合定量和定性数据，提高评估的效率、公平性与可靠性。通过数值案例验证了其准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有多标准评估方法（如DEA、SFA、MCDM）存在依赖假设和受主观判断影响的局限性；在实际应用中，需要有效整合定量（基数）和定性（序数）数据；同时，尽管不同方案的准则值固有变异，但常用同质性假设会显著影响评估结果。为解决这些挑战并选择最合适的方案。

Method: 提出一种结合两种虚拟差距分析（VGA）模型的创新性多标准评估（MCA）方法。该VGA框架以线性规划为基础。

Result: 所提出的方法提高了评估效率和公平性，确保了评估的全面性和可靠性。通过两个综合数值示例展示了该方法的准确性和透明度。

Conclusion: 本方法提供了一个强大且适应性强的解决方案，旨在促进自动决策系统和决策支持系统的持续发展。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [120] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 本论文提出了一种受桌面角色扮演游戏（TTRPG）启发的实体-组件架构模式，旨在为多主体生成式AI提供灵活的场景定义框架。该方法实现了职责分离，促进了快速迭代、模块化和可伸缩性，并以Concordia库为例进行了展示。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在多主体环境中具有广泛应用（如社会科学建模、交互式叙事、AI评估），但要支持这些多样化的用例，需要一个灵活的场景定义框架。

Method: 研究受TTRPG中“游戏主持人”（GM）概念的启发，GM负责环境和故事的非玩家驱动部分。采用实体-组件架构模式，将GM本身也视为一个由组件构成的可配置实体。这种设计将底层实现与组件创建和配置分离开来，工程师负责实现，设计师负责组合和配置实体。

Result: 所提出的方法通过职责分离，实现了快速迭代、保持模块化并最终确保了可伸缩性。通过Concordia库的持续演进，证明了该方法能够让用户有效地配置符合其特定目标的场景。

Conclusion: 受TTRPG启发的实体-组件架构模式为多主体生成式AI的场景定义提供了一个灵活、可扩展且模块化的解决方案，极大地提高了开发效率和配置能力，并已在Concordia库中得到实践和验证。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [121] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: 本文介绍了BioAnalyst，一个基于Transformer的AI基础模型，专为生物多样性分析和保护规划设计。该模型通过在多模态数据集上预训练，在数据稀缺场景下表现出优于现有方法的泛化能力，并为生态预测建立了新的准确性基线。


<details>
  <summary>Details</summary>
Motivation: 生物多样性加速丧失对生态研究和保护策略构成了严峻挑战，亟需具备全面的监测、预测和保护规划能力。人工智能基础模型在其他科学领域已显示出巨大潜力，有望解决生物多样性保护面临的威胁。

Method: 研究引入了BioAnalyst，这是首个专为生物多样性分析和保护规划定制的基础模型。它采用基于Transformer的架构，在包含物种出现记录、遥感指标、气候和环境变量等大规模多模态数据集上进行预训练。BioAnalyst设计灵活，可针对物种分布建模、栖息地适宜性评估、入侵物种检测和种群趋势预测等一系列下游任务进行微调。

Result: 研究在两个下游用例中评估了BioAnalyst的性能，结果表明与现有方法相比，该模型展现出更强的泛化能力，尤其是在数据稀缺场景下，并为生态预测建立了新的准确性基线。

Conclusion: 通过开源BioAnalyst及其微调工作流，旨在促进科学界在生物多样性建模方面的合作，并推动人工智能驱动的解决方案以应对紧迫的生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [122] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，AI工具意外地降低了经验丰富的开源开发者的生产力，导致任务完成时间增加，与开发者和专家预测相悖。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具被广泛采用，但其对实际软件开发影响的研究仍不足。本研究旨在通过随机对照试验，理解2025年前沿AI工具对经验丰富的开源开发者生产力的影响。

Method: 本研究采用随机对照试验（RCT），招募16名拥有中等AI经验的开发者，在他们平均有5年经验的成熟开源项目中完成246项任务。每项任务随机分配为允许或禁止使用2025年初的AI工具（主要为Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 开发者预期AI工具能将完成时间缩短24%（事后估计为20%），但实际结果显示，允许AI工具使任务完成时间增加了19%。这一结果与经济学和机器学习专家预测的显著缩短（38-39%）相悖。研究还评估了20个可能导致减速效应的属性，发现减速效应具有鲁棒性。

Conclusion: AI工具意外地降低了经验丰富的开源开发者的工作效率，导致任务完成时间增加。尽管不能完全排除实验伪影，但减速效应的鲁棒性表明其并非主要源于实验设计。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [123] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 针对DeFi市场中日益猖獗的操纵行为，本文提出一个基于多智能体强化学习（MARL）的去中心化检测框架，通过模拟对抗博弈并整合多源数据，实现了高效且鲁棒的市场操纵检测。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融（DeFi）虽然带来了金融创新，但由于缺乏中心化监管，导致市场操纵（如拉高出货）行为日益猖獗，急需有效的去中心化检测方案。

Method: 本文提出一个多智能体强化学习（MARL）框架用于去中心化操纵检测，将操纵者与检测者建模为动态对抗博弈。该方法包含三项创新：1) Group Relative Policy Optimization (GRPO) 提升学习稳定性；2) 基于理性预期和信息不对称的奖励函数，区分价格发现与操纵噪声；3) 融合LLM语义特征、社交图谱信号和链上市场数据的多模态智能体管道。该框架集成于Symphony去中心化系统中，支持链上可验证评估和去中心化监控。

Result: 通过在100,000个真实语料事件上训练并在对抗性仿真中验证，该框架（Hide-and-Shill）在检测准确性和因果归因方面均表现出色。

Conclusion: 该研究将多智能体系统与金融监管相结合，为去中心化市场智能提供了一种新范式，实现了对全球DeFi生态的实时监控，有助于推进开放研究和可复现性。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [124] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 首次系统性安全评估了基于LLM的编码代理，发现其存在显著安全隐患，并强调未来设计中需关注安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM编码代理在软件开发中被广泛部署，但其安全影响尚不明确，且可能无意中引入不安全实践。

Method: 对五个主流LLM模型（GPT-4o, GPT-4.1, Claude变体）在93个真实软件设置任务中的12,000多项操作进行了首次系统性安全评估；开发了高精度检测系统以识别四类主要漏洞；并评估了反馈机制和安全提醒等缓解策略。

Result: 研究发现21%的代理操作轨迹包含不安全行为，模型间差异显著；信息泄露（CWE-200）是最普遍的漏洞类型；GPT-4.1在缓解策略中表现突出，缓解成功率达96.8%。

Conclusion: 本研究提出了评估编码代理安全的第一个综合框架，强调了下一代基于LLM的编码代理在设计时必须融入安全意识。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [125] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 报告分类并列举了AI可能导致的全人类灭绝事件，旨在促成预防措施。


<details>
  <summary>Details</summary>
Motivation: 作者旨在公开展示AI可能导致的全人类灭绝情景，以争取公众支持，促使机构采取预防措施，规避AI带来的灾难性风险。

Method: 该报告采用的方法是提出AI可能导致的全人类灭绝事件的分类（分类法）并提供具体示例。

Result: 报告识别并分类了AI可能导致几乎所有人类灭绝的潜在情景，强调这些是可避免的可能性而非必然。

Conclusion: 通过公开这些可能性，报告希望支持采取预防措施以应对AI带来的灾难性风险，强调其可避免性。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [126] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: 为解决多模态大语言模型在科学推理（特别是多步、可解释推理）中的不足，本文提出了端到端框架EduFlow，包含过程感知奖励模型EduPRM和领域自适应搜索框架EduMCTS，并构建大规模数据集，显著提升了推理的一致性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在科学任务中表现不佳，尤其是在需要多步和可解释推理的场景。其主要限制包括：缺乏科学推理模式、多步推理中缺乏全局连贯性、以及没有反思性自我纠正能力，这使得它们在结构化科学语境下不可靠。

Method: 本文提出了EduFlow，一个覆盖教育科学推理全流程的端到端框架，包括数据选择、基于MCTS的轨迹构建、模型训练和输出优化。其核心是EduPRM，一个过程感知奖励模型，通过标签和理由批判推理步骤，并利用MCTS轨迹、错误注入批判和师生对话三种互补监督源进行课程学习训练。同时提出EduMCTS，一个领域自适应搜索框架，引入了自我反思等引导动作以促进错误纠正，并利用EduPRM的细粒度反馈指导搜索。通过自洽性和拒绝采样，构建了EduMCTS-160K大型教育推理轨迹数据集。

Result: 广泛的实验表明，EduFlow显著增强了推理的一致性和连贯性。

Conclusion: 该研究通过引入EduFlow框架及其创新组件EduPRM和EduMCTS，并构建大规模教育推理数据集，有效解决了多模态大语言模型在科学推理中多步和可解释性不足的问题，提升了推理的性能和可靠性。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [127] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文探讨了如何融合AI的可解释性和适应性，重点研究了在代理检索增强生成（RAG）系统中，知识的表示（结构与复杂性）如何影响大型语言模型（LLM）查询知识图谱的效率。


<details>
  <summary>Details</summary>
Motivation: 前沿AI系统（如LLMs和生成式AI）亟需可解释性和可解释性，同时对新领域、新场景的适应性也至关重要。研究动机在于融合这两种能力，设计可迁移且可解释的神经符号AI系统。

Method: 专注于“代理检索增强生成”系统。系统性评估了不同知识概念化和表示方法（特别是结构和复杂性）对AI代理（本例中为LLM）有效查询三元组存储库的影响。

Result: 研究结果表明，不同的知识概念化和表示方法对AI代理（LLM查询三元组存储库）的性能确实存在影响。

Conclusion: 论文讨论了这些发现的影响和启示，强调知识的结构和复杂性对AI代理查询能力的重要性，为未来可迁移、可解释的神经符号AI系统设计提供了方向。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [128] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出LLM-Stackelberg博弈框架，将大型语言模型（LLMs）整合到序贯策略互动中，定义了新的均衡概念，旨在模拟LLM介导的决策，并以网络钓鱼案例展示其在网络安全等领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg博弈假设完全信息和理性代理，无法有效模拟LLM作为决策者的战略互动。现有模型缺乏对LLM通过结构化提示词进行推理、生成概率行为以及适应策略能力的建模，需要一个新框架来捕获有限理性、非对称信息和元认知适应。

Method: 引入LLM-Stackelberg博弈框架，允许代理通过结构化提示词进行推理，利用LLM生成概率行为，并通过内部认知和信念更新调整策略。定义了“推理与行为均衡”和“推测性推理均衡”两种均衡概念。通过一个网络钓鱼案例研究（发送者与接收者使用结构化推理提示词进行欺骗博弈）来演示该框架。

Result: 该框架能捕捉有限理性、非对称信息和元认知适应。网络钓鱼案例研究突显了LLM介导交互的认知丰富性和对抗潜力。LLM-Stackelberg博弈为网络安全、虚假信息和推荐系统等领域的决策建模提供了一个强大范式。

Conclusion: LLM-Stackelberg博弈框架为包含LLM的序贯战略决策提供了一种强大且新颖的建模范式，有效解决了传统模型的局限性，尤其适用于需要考虑有限理性、非对称信息和自适应策略的复杂应用场景，如网络安全、虚假信息和推荐系统。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [129] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 多智能体强化学习面临固有挑战，本文提出从反应式智能转向基于生成式AI的主动式智能，通过预测和协调行为，解决现有框架难以应对的复杂协作问题。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习（MARL）面临联合行动空间指数增长、环境非平稳性及部分可观测性等根本挑战。现有方法仅限于刺激-反应机制，在面对新颖场景时表现不佳。

Method: 本文倡导范式转变，将智能体重新概念化为复杂的生成式模型，通过生成式AI强化学习（generative-RL），使其能够合成多智能体动态、基于预测理解进行前瞻性决策，并能够建模环境演化、预测他者行为、生成协调动作序列及进行长期战略推理。

Result: 该方法将实现主动决策、通过增强通信实现无缝协调，并能动态适应不断变化的场景。有望解锁分布式智能的无限可能，从个体优化迈向代表真正协作智能的涌现集体行为。

Conclusion: 这一范式转变对自主系统、机器人和人机协作领域具有深远影响，有望为传统反应式框架下难以解决的协调挑战提供解决方案。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [130] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: 本文提出一致性轨迹规划（CTP），一种基于CTM的新型离线模型强化学习方法，旨在解决现有扩散模型规划计算成本高的问题，实现了快速、单步的轨迹生成，并在D4RL基准测试中表现优异，推理速度提升120倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有将扩散模型应用于规划的方法虽然性能强大，但由于迭代采样过程导致计算成本高昂，限制了其实用性。

Method: 本文引入一致性轨迹规划（CTP），这是一种新型的离线模型强化学习方法。CTP利用最新提出的一致性轨迹模型（CTM）进行高效轨迹优化，支持快速、单步的轨迹生成。该方法在D4RL基准测试上进行了评估。

Result: CTP在长时程、目标条件任务中持续优于现有基于扩散的规划方法，取得了更高的标准化回报，同时使用显著更少的去噪步骤。具体而言，CTP在推理时间上实现了超过120倍的加速，且性能可比。

Conclusion: CTP是一种实用且有效的离线规划方法，能够实现高性能和低延迟，解决了现有扩散模型规划的效率瓶颈，具有重要的实际应用价值。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [131] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 本文提出首个基于Metropolis-Hastings (MH) 采样的非梯度SNN训练框架，用于强化学习任务，在控制基准测试中表现优于传统DQL和现有SNN-RL方法，同时节省资源和训练时间。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络 (SNNs) 在实时控制中能效高，但由于其通信机制不可微，在强化学习 (RL) 任务中的训练面临挑战。

Method: 引入了一个新颖的框架，利用Metropolis-Hastings (MH) 采样（一种贝叶斯推断技术）来训练SNNs，用于RL环境中的动态智能体控制，无需依赖梯度方法。该方法基于累积奖励信号迭代地提议并概率性地接受网络参数更新。

Result: 在AcroBot和CartPole两个标准控制基准上进行评估，结果表明所提出的基于MH的方法在最大化累积奖励、最小化网络资源和训练回合方面，均优于传统的深度Q学习 (DQL) 基线和先前的基于SNN的RL方法。

Conclusion: 基于Metropolis-Hastings (MH) 采样的非梯度训练方法是解决SNNs在强化学习中训练挑战的有效途径，它不仅能提高性能，还能优化资源利用，并为直接在神经形态平台上部署SNNs提供可能。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [132] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个AI即服务平台，通过整合专有数据、工作流和大型语言模型，赋能企业实现AI资产控制、知识内化和数据安全，并通过AI Agent提供洞察和自动化。其在检索精度和生成质量上表现出色，适用于高风险领域。


<details>
  <summary>Details</summary>
Motivation: 企业需要完全控制其AI资产、实现AI知识内部保留、确保数据安全，并利用AI提升团队效率、自动化重复任务以驱动更好的业务成果。

Method: eSapiens平台整合了结构化文档摄取、混合向量检索和LangChain无代码编排，支持多种主流LLM。关键组件THOR Agent负责处理SQL查询并生成企业数据库洞察。系统通过法律语料库上的检索基准测试（优化块大小）和TRACe指标的生成质量测试进行评估。

Result: 在检索实验中，512 tokens的块大小在法律语料库上实现了最高的检索精度（Top-3准确率：91.3%）。在生成质量测试中，eSapiens的输出上下文一致性更高，事实对齐度提升高达23%。

Conclusion: eSapiens平台能有效支持高风险领域（如法律和金融）中可信赖、可审计的AI工作流。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [133] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 本文综述了人工智能（AI）快速发展带来的被忽视的环境和伦理挑战，重点关注能源消耗、电子垃圾、计算资源不平等及网络安全能耗四个方面，并呼吁AI发展应与伦理责任和环境管理相协调，以实现可持续的未来。


<details>
  <summary>Details</summary>
Motivation: 尽管AI取得了显著进步，但其快速扩张带来的环境和伦理挑战（如高能耗、电子垃圾、资源不平等）常被忽视，有必要对其超越性能之外的影响进行深入探讨。

Method: 本文采用综述方法，基于近期研究和机构报告，分析了AI在能源消耗、电子垃圾、计算资源获取不平等以及网络安全系统隐含能耗四个关键领域的影响。

Result: 研究结果揭示了AI领域存在的系统性问题，包括模型训练导致的高排放、硬件更替加速、全球基础设施差距以及保障AI安全的巨大能源需求。此外，综述还识别了关键的研究空白。

Conclusion: AI的进步必须与伦理责任和环境管理相结合，以确保一个更具包容性和可持续性的技术未来。文章倡导推行可持续、透明和公平的AI开发实践。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [134] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 本文提出一个神经符号框架，结合多模态语言模型和知识图谱/本体，以实现个人服务机器人中可互操作、平台无关的行为。


<details>
  <summary>Details</summary>
Motivation: 现有服务机器人系统因硬编码和特定硬件限制而缺乏互操作性与可扩展性。虽然本体/知识图谱可提供结构化知识，但难以处理原始感官输入；多模态语言模型善于解释感官数据，但缺乏透明度和知识基础。因此，需要结合两者优势。

Method: 开发了一个神经符号框架，融合多模态语言模型的感知能力与知识图谱/本体的结构化表示，旨在支持机器人应用的互操作性。该方法生成符合本体论的知识图谱，以实现平台无关的机器人行为。通过整合机器人感知数据、本体和五种多模态模型（3个LLaMA，2个GPT）进行评估，并分析不同神经符号交互模式下生成知识图谱的一致性和有效性。

Result: 实验结果显示，GPT-o1和LLaMA 4 Maverick模型表现优于其他模型。同时，研究指出新模型不一定带来更优结果，集成策略在生成符合本体论的知识图谱中起关键作用。

Conclusion: 该神经符号框架成功结合了多模态模型与知识图谱/本体的优势，实现了机器人行为的平台无关性。研究强调了集成策略对于生成高质量知识图谱的重要性，而非仅仅依赖模型的新颖性。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [135] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 本文介绍了一个开源的PyTorch工具包，利用随机控制技术为多智能体AI系统提供预先的公平性和鲁棒性保证，显著简化了相关复杂性。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统的监管要求预先（a priori）满足公平性和鲁棒性保证。然而，由于智能体响应的随机性，实现这些预先保证需要对随机系统进行复杂的推理。

Method: 研究者开发了一个基于PyTorch的开源工具包，该工具包利用随机控制技术来建模AI系统之间的互连及其重复使用的特性。

Result: 该工具包以闭环方式建模鲁棒性和公平性需求，并为AI系统之间的互连提供了预先保证。它显著降低了为多智能体系统的闭环模型提供公平性保证的复杂性。

Conclusion: 该PyTorch工具包成功地将随机控制技术应用于AI系统，有效地简化了多智能体AI系统中预先公平性和鲁棒性保证的实现，解决了传统方法的复杂性挑战。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [136] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务中表现卓越，但存在推理冗长低效问题。本综述旨在探讨LRMs高效推理的精简与自适应思维方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务上表现突出，但对简单问题会生成不必要的冗长推理链，导致推理资源浪费、响应时间增加，并阻碍实际应用。因此，急需缩短推理链并实现基于输入难度的快慢思维自适应推理。

Method: 本研究采用综述形式，全面概述了LRMs高效推理中精简与自适应思维的最新进展，内容涵盖相关方法论、基准测试以及未来探索的挑战。

Result: 本综述提供了一个关于大型推理模型精简与自适应思维领域的综合性概览，明确了当前的方法、评估标准以及有待解决的关键挑战。

Conclusion: 本综述旨在帮助研究人员快速掌握该领域全貌，并启发新的自适应思维理念，以促进大型推理模型更高效地应用。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [137] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 本文提出了一种因果关系引导的深度Q网络（Causal DQ）方法，用于在资源有限的AI驱动制造环境中优化传感器放置以快速检测异常，解决了现有方法忽略因果性或依赖不切实际干预的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动制造的发展，实时数据流监测需求剧增，但资源有限导致无法全面部署传感器。现有异常检测中的传感器放置方法常忽略关键的因果关系，或依赖不切实际的人工干预来识别因果效应，这在实际中是不可行的。

Method: 引入了一种名为“因果关系引导的深度Q网络”（Causal DQ）的方法，用于异常检测中的部分可观测传感器放置。该方法在深度Q网络训练的每个阶段都整合了因果信息。

Result: 所提出的Causal DQ方法实现了更快的收敛速度和更紧密的理论误差界限。在多种设置下，训练后的因果引导Q网络显著缩短了异常检测时间。

Conclusion: Causal DQ方法在大型真实数据流的传感器放置中表现出卓越的有效性。其基本思想和洞察力可推广应用于各种强化学习问题，为工程应用中基于因果关系的机器学习方法开辟了新的可能性。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [138] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 本文提出将大型语言模型（LLM）直接整合到次协调逻辑的形式语义解释函数中，以解决LLM输出的逻辑不一致问题，并构建了一个保持逻辑可靠性和完备性的神经符号推理理论框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致问题。研究旨在探索如何在形式推理中利用LLM的广泛参数知识，同时克服其固有的不一致性。

Method: 提出了一种将LLM直接整合到次协调逻辑（paraconsistent logic）的形式语义解释函数中的方法。通过使用从多个简短事实基准数据集创建的数据集，对该函数进行了评估，提供了实验证据。

Result: 实验结果证明了该方法的可行性。该方法提供了一个神经符号推理的理论框架，该框架能够有效利用LLM的知识，同时保持底层逻辑的可靠性（soundness）和完备性（completeness）属性。

Conclusion: 本研究成功地将LLM的知识与次协调逻辑相结合，提出了一种创新的神经符号推理方法，解决了LLM的逻辑不一致问题，并为构建既能利用LLM能力又能保持逻辑严谨性的形式推理系统提供了坚实的理论基础。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [139] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 本文提出并概述了旨在协调中止危险AI开发与部署的关键技术干预措施，以应对AI系统带来的潜在风险。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来失控、滥用、地缘政治不稳定和权力集中等前所未有的风险。为应对这些风险并避免最坏结果，政府可能需要建立协调中止危险AI开发和部署的能力。

Method: 本文概述并讨论了能够实现协调中止危险AI活动的关键技术干预措施。

Result: 研究展示了这些技术干预措施如何有助于限制各种危险AI活动，并能够形成潜在AI治理计划的技术基础。

Conclusion: 论文提出的技术干预措施可以为协调中止危险AI活动提供技术基础，从而帮助应对AI风险并支持未来的AI治理计划。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [140] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 研究表明，通过对基础模型（Qwen2.5-32B）进行轻量微调，仅使用20个高质量的专家长链式思考（CoT）示例，即可显著提升其推理能力，甚至超越更大的模型（Qwen2.5-Math-72B-Instruct）。同时发现，非专家CoT数据难以复现专家CoT的性能，强调了高质量数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，大语言模型通过生成长CoT轨迹展现出强大的推理能力，且基础模型可通过强化学习或蒸馏获得CoT能力。本文旨在探究是否仅通过提示或极少量微调就能在基础模型中诱导生成长CoT。

Method: 1. 轻量微调：使用来自推理模型QwQ-32B-Preview的20个长CoT示例对基础模型Qwen2.5-32B进行微调。2. 对比实验：探索使用来自非推理模型和人工标注者的CoT数据，并结合提示工程、多轮编辑和结构化指导进行增强，以评估其效果。3. 特性分析：分析影响推理蒸馏的关键数据属性，如问题难度、多样性和答案长度。

Result: 1. 性能显著提升：经过少量专家CoT示例微调的模型表现优于更大的Qwen2.5-Math-72B-Instruct模型，证明少量高质量数据能解锁强大推理能力。2. 数据质量差异：来自非推理模型和人工标注者的CoT数据未能达到推理模型CoT的性能，表明专家CoT存在难以复制的潜在特质。3. 分析了推理数据特性对蒸馏效果的影响。

Conclusion: 1. 即使是小量精心策划的高质量人工编写的CoT数据，也有望激活基础模型的推理行为。2. 专家CoT的内在质量对推理能力蒸馏至关重要。3. 发布了人工编写的数据集，以鼓励未来对小规模推理监督有效性的深入研究。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [141] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 本文将指令微调的大型语言模型重新解释为神经符号AI系统，其中自然语言作为符号层，模型内部表示空间实现接地。初步评估显示，此方法能有效提升学习效率和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI系统结合了神经网络和经典符号AI的优势，以实现大规模学习和可靠推理。本研究旨在探索一种新的方法，将指令微调的大型语言模型（LLMs）重新解释为神经符号系统，从而利用其互补优势，以期提高AI的学习效率和推理可靠性。

Method: 本文提出一个框架，将指令微调的大型语言模型重新解释为模型接地的符号AI系统。在此框架中，自然语言被用作符号层，而模型的内部表示空间则用于实现符号接地。研究者在此基础上开发了新型的学习和推理方法，这些方法保留了与传统学习和推理范式在结构上的相似性。通过对不同复杂度的公理化演绎推理过程进行初步评估。

Result: 初步评估结果表明，所提出的方法在提高学习效率和推理可靠性方面是有效的。

Conclusion: 将指令微调的大型语言模型重新解释为模型接地的神经符号AI系统，并以自然语言作为符号层，提供了一个有前景的途径，以结合大型语言模型的通用学习能力与符号AI的精确推理能力，从而有效提升AI的学习效率和推理可靠性。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [142] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 该研究提出了VerifyBench，一个跨领域的综合基准，用于系统评估大语言模型强化学习中使用的验证器。评估发现，专用验证器准确但召回率低，通用模型包容性强但精度不稳定，且验证器对输入结构敏感，跨领域泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）越来越依赖强化学习（RL）提升推理能力，但验证模型生成响应与参考答案的一致性是一个关键挑战。现有验证器（规则基、模型基、通用LLM判断器）各有不足，且缺乏对不同类型验证器在多领域性能的系统评估，这严重阻碍了可验证奖励强化学习（RLVR）的可靠发展。

Method: 为解决此问题，我们提出了VerifyBench——一个跨领域的综合评估基准。构建了包含数学、物理、化学、生物学4000个专家级问题的集合，每个问题配有参考答案和多样化响应，并由多学科专家团队进行严格标注。设计了四维实验框架，在提取答案与完整响应、短输出与长输出的组合条件下，全面比较专用验证器和通用LLM的性能边界。

Result: 评估揭示了验证器的基本权衡：专用验证器在准确性上领先，但在召回率上存在不足；通用模型显示出更强的包容性，但精度不稳定。更重要的是，我们发现验证器对输入结构高度敏感，且在跨领域泛化方面存在固有限制。

Conclusion: 本研究揭示了当前验证器技术瓶颈，即准确性与召回率、包容性与精度之间的权衡，以及对输入结构的敏感性和跨领域泛化的局限性，为未来可验证奖励强化学习的可靠发展提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [143] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: 本文分析了DeepSeek V3/R1模型，因其低成本、高性能和开源特性受全球关注。重点探讨其创新算法、工程突破，评估其对AI竞争格局的影响，并展望大型AI模型的未来发展。


<details>
  <summary>Details</summary>
Motivation: DeepSeek发布的V3和R1系列模型以其低成本、高性能和开源优势在全球范围内引起广泛关注，促使本研究深入分析其技术创新和对AI领域的影响。

Method: 论文首先回顾大型AI模型演进，包括主流LLM范式和DeepSeek范式。接着，详细阐述DeepSeek引入的新算法（如MLA、MoE、MTP、GRPO）及其在LLM扩展、训练、推理和系统级优化方面的工程突破。此外，还通过与主流LLM的比较，分析DeepSeek模型对AI竞争格局的影响。

Result: 本文分析了DeepSeek模型因其低成本、高性能和开源特性而获得的全球关注，并揭示了其在多头潜在注意力、MoE、多令牌预测等创新算法以及LLM扩展、训练、推理和系统优化等工程方面的显著突破，阐明了这些创新如何使其在AI竞争格局中脱颖而出，并与主流LLM形成对比。

Conclusion: 论文从DeepSeek的创新中汲取了见解，并讨论了未来大型AI模型在数据、训练和推理等方面的技术与工程发展趋势。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [144] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [145] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本研究提出一种基于Promise Theory语义时空模型的低成本、与语言无关的方法，通过分析多尺度异常和过程连贯性，识别数据中的潜在意向性，并将意图内容与环境上下文区分开来，无需大量训练或推理。


<details>
  <summary>Details</summary>
Motivation: 自Searle对哲学领域中意图和意向性进行解构以来，科学技术领域对“意图”的实际意义关注甚少。

Method: 该方法基于Promise Theory的语义时空模型（Semantic Spacetime），将其作为有效的微型语言模型。通过过程连贯性识别文本主题和概念，无需特定语言知识。通过寻找异常的多尺度异常并评估其形成所需的工作量，来评估数据中潜在的“意向性”程度。利用尺度分离和时空连贯性作为度量，将内容分为“意图内容”和“环境上下文”。

Result: 该方法提供了一种对潜在意向性的基本但实用的解释，计算成本极低，且不需要大量的训练或推理能力。该过程无需大规模人工概率批处理，甚至适用于基本生物体。概念形成的水平取决于代理的记忆容量。

Conclusion: 本研究提出了一种创新且计算高效的方法来识别数据中的潜在意向性，有效地区分了意图与上下文，填补了科学技术领域对意图实际意义关注不足的空白。其低成本和普适性使其具有广泛的应用潜力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [146] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 链式思考（CoT）推理易受中间步骤错误影响，本文提出一种新方法，通过利用模型内在真实性编码（特定注意力头激活）训练置信度预测器，结合束搜索校准CoT推理准确性，显著提升其在多种任务和模型上的性能与可靠性。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）推理在大型语言模型（LLMs）和多模态大型语言模型（MLLMs）中展现出卓越的深度推理能力，但其中间步骤错误的累积常损害其可靠性。

Method: 本文引入一种新颖方法来校准CoT推理的准确性，其核心是利用模型内在的真实性编码。研究发现特定的注意力头激活能可靠地反映CoT推理步骤的真实性。基于此洞察，作者训练了一个置信度预测器，利用这些对真实性敏感的激活来评估每个推理步骤的正确性，并通过束搜索（beam search）动态选择最合理的推理路径。

Result: 实验结果表明，该方法在数学、符号和常识推理任务中，无论是在单模态还是多模态设置下，均显著优于Few-Shot CoT、Self-Consistency和Self-Evaluation Guided Beam Search等现有最先进的基线方法，展现出更高的准确性和可靠性。该方法还在大型推理模型上得到进一步验证，证实了其对专业推理模型的适用性。

Conclusion: 这项工作为CoT推理提供了一条新颖的可靠性改进途径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [147] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）在不同知识图谱（KG）模式（如DBpedia/Wikidata、DBLP/OpenAlex）间自动翻译SPARQL查询的能力。结果显示，性能因模型和提示策略而异，且Wikidata到DBpedia的翻译效果明显优于DBpedia到Wikidata。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在填补知识图谱互操作性领域的显著空白，通过严格评估LLM在SPARQL-to-SPARQL翻译中的性能来促进不同知识图谱之间的互联互通。

Method: 构建了两个基准测试集：一个包含来自QALD-9-Plus的100个DBpedia-Wikidata对齐查询；另一个包含100个DBLP-OpenAlex对齐查询，以测试通用性。选用Llama-3-8B、DeepSeek-R1-Distill-Llama-70B和Mistral-Large-Instruct-2407三种开放LLM，采用零样本、少样本和两种思维链提示策略进行测试。LLM的输出与标准答案进行比较，并对错误进行分类。

Result: 研究发现，LLM的性能在不同模型和提示策略之间存在显著差异。具体而言，从Wikidata到DBpedia的SPARQL翻译效果远优于从DBpedia到Wikidata的翻译。

Conclusion: LLM在知识图谱间的SPARQL翻译中表现出潜力，但其性能受模型选择和提示策略的显著影响，且存在特定的翻译方向性偏好。这提示了在实现全面知识图谱互操作性方面，LLM仍面临挑战和改进空间。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [148] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文为假设基论证（ABA）提出了新的渐进语义，填补了该领域的一个空白。


<details>
  <summary>Details</summary>
Motivation: 渐进语义在计算论证中被广泛研究，但尚未应用于假设基论证（ABA）框架，尽管ABA是一种流行的结构化论证形式，且渐进语义对其有潜在价值。因此，研究旨在填补这一空白。

Method: 1. 提出了一系列新颖的渐进语义，用于赋予ABA框架中的核心组件——假设——辩证强度。2. 将两极集合论证框架作为（潜在非平面）ABA框架的抽象。3. 泛化了QBAF最先进的模块化渐进语义。4. 探索了一种直接利用既有QBAF模块化语义的基于论证的方法作为基线。5. 使用合成ABA框架进行实验，比较所提出的语义与基于论证的对应方法，并评估收敛性。

Result: 1. 所提出的渐进ABA语义满足QBAF渐进语义的理想性质（如平衡性、单调性）的适当改编。2. 通过实验将所提出的渐进ABA语义与其基于论证的对应方法进行了比较，并评估了收敛性。

Conclusion: 本文成功地为假设基论证（ABA）引入了渐进语义，填补了计算论证领域的一个重要空白，并证明了其性质和有效性，从而扩展了渐进语义的应用范围。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [149] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass是一个统一的AI安全框架，旨在集成现有碎片化安全工具，通过对视觉语言模型的案例分析，提升AI系统的安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强和普及，确保其安全性至关重要。然而，现有安全工具功能单一，无法提供全面保障，因此需要集成和复合的方法。

Method: 引入BlueGlass框架，提供统一基础设施以整合和组合操作于模型内部和输出的多种安全工具。通过在视觉语言模型的目标检测任务上进行三项安全分析来验证其效用：1) 分布评估；2) 基于探针的层动态分析；3) 稀疏自编码器识别可解释概念。

Result: 1) 分布评估揭示了跨分布的性能权衡和潜在故障模式；2) 基于探针的分析突出了通过相变实现的共享分层学习；3) 稀疏自编码器识别出可解释的概念。

Conclusion: 本工作为构建更鲁棒和可靠的AI系统提供了基础性框架和重要发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [150] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 本研究将边缘云应用迁移问题建模为汉诺塔问题，分析和比较了AI规划和强化学习方法，以实现自动迁移编排，并引入了新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 边缘云应用迁移有助于提高服务质量和成本效益，但其自动编排常依赖启发式方法。亟需理解并利用先进的AI规划和强化学习技术来有效管理新兴计算连续体环境中的此类迁移。

Method: 从马尔可夫决策过程（MDP）出发，将边缘云应用迁移问题建模为汉诺塔问题。识别、分析并比较了选定的AI规划和强化学习方法，并引入了一种基于状态空间定义的新分类方法进行深入分析。

Result: 通过对建模为汉诺塔问题的边缘云应用迁移进行AI规划和强化学习方法的识别、分析和比较，并结合提出的新分类方法，本研究旨在提供对现有编排技术的深入理解。

Conclusion: 本研究旨在理解并掌握在新兴计算连续体环境中，能够有效自动编排边缘云应用迁移的可用AI和强化学习技术。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [151] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 本文提出将人类心理学中的元认知提示（如“你可能错了吗？”）应用于大型语言模型（LLMs），以促使其识别自身偏差、提供更全面信息，并改进提示工程。


<details>
  <summary>Details</summary>
Motivation: 识别LLMs偏差是一个持续的挑战，且模型不断演进，需要通用、持久的去偏策略。人类决策去偏策略中，通过提示干预唤醒潜在知识的方法，为LLMs去偏提供了有前景的途径，因为LLMs虽包含偏差信息，但需被提示才能显现。

Method: 将人类决策文献中发展的元认知提示（特别是“你可能错了吗？”）应用于LLMs。在LLM初始响应后使用此提示，并通过一系列来自LLM偏差相关文章的问题（包括隐性歧视偏差和元认知失败）以及一个关于不完整信息的例子来演示其效果。

Result: “你可能错了吗？”的提示促使LLMs产生额外信息，包括其初始答案的理由、错误、偏差、矛盾证据和替代方案，这些在初始响应中均未出现。这种元知识常揭示LLMs和用户对提示的理解不一致。该提示能帮助LLMs识别自身偏差并进行连贯的元认知反思，并能有效纠正看似令人信服但不完整的信息。

Conclusion: 人类心理学为提示工程开辟了新途径，通过借鉴人类决策中基于提示的有效改进经验，可以有效改善LLMs的去偏和响应质量。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [152] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出一种基于大模型上下文学习（FRSICL）的无人机飞行资源分配方案，实时联合优化无人机飞行控制和数据采集，以最小化野火监测中的信息时效性（AoI），克服了传统深度强化学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火监测中至关重要，早期预警依赖于及时的数据（低AoI）。现有深度强化学习（DRL）方法在优化无人机传感器传输调度和速度方面存在采样效率低、模拟与现实差距大、训练复杂等局限性，使其不适用于野火监测等时间敏感型应用。因此，需要一种更高效、实时的优化方案。

Method: 论文提出了一种基于大模型上下文学习（LLM-Enabled In-Context Learning, FRSICL）的在线飞行资源分配方案。该方案通过自然语言任务描述和环境反馈，实时联合优化无人机的飞行控制和数据采集调度，旨在渐进地最小化地面传感器的平均AoI，且无需大量的再训练。

Result: 仿真结果证实，与近端策略优化（PPO）和最近邻等基准方法相比，所提出的FRSICL方案是有效的，并表现出优越的性能。

Conclusion: FRSICL方案通过利用大模型上下文学习，为野火监测中的无人机飞行和数据采集联合优化提供了一个有效的实时解决方案，成功克服了传统DRL方法的局限性，显著提高了信息时效性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [153] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 针对MARL在真实世界动态环境部署的挑战，本文提出“适应性”概念及评估框架，以更系统地衡量其在变化条件下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在模拟和受限场景中表现有效，但其在真实世界多智能体系统（MAS）中的部署受限。主要原因是真实环境复杂多变，存在智能体数量波动、任务目标演变和执行条件不一致等变异源，要求MARL算法在持续变化中仍能保持有效。

Method: 引入“适应性”概念，作为评估MARL算法在变化条件（学习或执行期间环境动力学变化）下可靠性的统一视角。围绕适应性概念，提出了一个结构化框架，包含三个关键维度：学习适应性、策略适应性和场景驱动适应性。

Result: 本研究通过引入适应性视角和评估框架，旨在支持对MARL性能进行更具原则性的评估，超越狭义的基准测试。

Conclusion: 本调查研究致力于推动开发更适用于动态、真实世界多智能体系统部署的MARL算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [154] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 本文引入了SwissFKG（瑞士食品知识图谱），一个整合瑞士食谱、配料、营养数据和个体饮食需求的知识图谱，并利用大型语言模型（LLM）进行丰富和查询，旨在构建更全面的膳食评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的膳食评估系统忽视了非视觉因素（如配料替换）和个体饮食需求（如过敏、限制、文化习俗）。此外，瑞士的食品信息分散，缺乏集中的、整合所有相关营养信息的存储库。

Method: 研究团队构建了SwissFKG，整合了食谱、配料、替换物、营养数据、饮食限制、过敏信息和国家营养指南。他们建立了一个由LLM驱动的知识图谱丰富管道，并首次对四种LLM（参数小于70B）在食品知识增强方面的性能进行了基准测试。此外，还实现了一个Graph-RAG（图增强检索）应用，以展示SwissFKG如何帮助LLM回答用户特定的营养查询，并通过比较LLM-嵌入对的用户查询响应与预定义答案来评估其效果。

Result: 研究结果表明，LLM能够有效地丰富知识图谱中的相关营养信息。SwissFKG不仅提供食谱推荐，还能提供配料层面的过敏原和饮食限制信息，并提供符合营养指南的指导。Graph-RAG应用成功展示了SwissFKG丰富的自然语言数据结构如何帮助LLM回答用户特定的营养查询。

Conclusion: 该工作为下一代膳食评估工具奠定了基础，这些工具将融合饮食的视觉、语境和文化维度，提供更全面和个性化的营养指导。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [155] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验证明，在稀疏奖励环境下，简单直接的Filtered Behavior Cloning (FBC) 方法比Decision Transformer (DT) 表现更优或相当，且效率更高，从而质疑了DT在离线强化学习中的普适性。


<details>
  <summary>Details</summary>
Motivation: 之前的研究（Bhargava et al., 2024）声称Decision Transformer (DT) 在稀疏奖励和低质量数据设置中表现优于传统的MLP基线方法。本文旨在通过实验验证这一说法，并探索是否存在更简单、高效的替代方案。

Method: 研究在机器人操作任务 (Robomimic) 和运动基准 (D4RL) 上进行。核心方法是比较MLP-based Filtered Behavior Cloning (FBC) 与Decision Transformer (DT) 的性能。FBC的实现方式是简单地从数据集中过滤掉低表现轨迹，然后对剩余的过滤数据集进行行为克隆。

Result: 实验结果显示，MLP-based FBC在稀疏奖励环境中取得了与DT相当或更优的性能。此外，FBC方法更为直接、所需训练数据更少且计算效率更高。

Conclusion: 研究结果表明，Decision Transformer (DT) 不适用于稀疏奖励环境。结合先前的研究，DT也可能不适用于密集奖励环境。因此，论文提出了“DT是否在任何情况下都具有优势？”的疑问，暗示了其应用范围可能不如预期的广泛。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [156] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 本文针对数据分析任务中可解释AI（XAI）研究的矛盾和缺乏指导问题，提出了一种XAI研究分类方法和详细的研究指南，旨在提升领域内研究的清晰度和实用性。


<details>
  <summary>Details</summary>
Motivation: 可解释AI（XAI）在数据分析任务中的研究存在大量矛盾，且缺乏具体的XAI设计建议，根本原因是对需要AI协助的任务理解不足。

Method: 结合可视化分析、认知科学和仪表盘设计等领域知识，提出了一种基于“什么、为什么、谁”三个维度来分类和比较XAI研究的方法。

Result: 识别出当前研究的主要问题包括：任务描述不充分、研究缺乏上下文、以及目标用户测试不足。

Conclusion: 建议研究应详细报告用户的领域、AI和数据分析专业知识以说明发现的普适性，并提出了设计和报告XAI任务的研究指南，以帮助XAI社区更好地理解和发展这一快速增长的领域，解决矛盾结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [157] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文综述了基于LLM的表格智能体，探讨其在处理真实世界复杂表格中的能力，定义了五项核心竞争力，揭示了Text-to-SQL智能体在实际应用中的性能差距，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注干净的学术数据集，而真实世界表格任务涉及噪声、结构异构性和语义复杂性，这些问题在现有研究中尚未得到充分探索。

Method: 本文综述了LLM驱动的表格智能体，通过定义五项核心能力（表格结构理解、表格与查询语义理解、表格检索与压缩、可追溯执行推理、跨域泛化）来分析和比较现有方法，并详细检查了Text-to-SQL智能体。

Result: 研究发现，Text-to-SQL智能体在学术基准和真实世界场景之间存在显著的性能差距，尤其是对于开源模型。

Conclusion: 提供了可行的见解，以提高LLM驱动的表格智能体在实际应用中的鲁棒性、泛化性和效率。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [158] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA），探索了车辆路径问题（CVRP）实例特性与元启发式算法性能之间的关系，并提供了一种新的实例分析方法。


<details>
  <summary>Details</summary>
Motivation: 推进CVRP研究，解决理解实例特征与元启发式（MH）算法性能之间复杂关系这一挑战。

Method: 采用实例空间分析（ISA）方法，结合DIMACS第12届车辆路径挑战赛数据集，识别出23个相关实例特征。通过PRELIM、SIFTED和PILOT阶段，运用降维和机器学习方法，创建实例空间的二维投影。

Result: 识别了23个CVRP相关实例特征；成功构建了实例空间的二维投影，揭示了实例结构对元启发式算法行为的影响；提供了一个投影矩阵，简化了新实例的纳入分析过程，为CVRP实例分析提供了新工具。

Conclusion: 本研究为CVRP领域的实例分析提供了新视角和方法，通过实例空间分析揭示了实例特征与元启发式性能间的关系，特别是提供了一个可复用的投影矩阵，便于未来研究。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [159] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 提出一个结合BERT情感分析与XGBoost的混合模型，用于预测在线学习辍学，准确率达84%。


<details>
  <summary>Details</summary>
Motivation: 在线学习中的辍学问题严重，早期检测和干预至关重要。现有辍学预测研究通常未充分整合多样化的数据源，包括社会人口、行为数据和情感分析，以准确预测辍学风险。

Method: 引入一种新模型，结合BERT模型对学生评论进行情感分析，并与通过XGBoost分析的社会人口统计和行为数据融合。BERT在学生评论上进行微调，情感特征随后与通过XGBoost特征重要性技术选择的关键特征合并。

Result: 模型在下一学年的未见数据上测试，预测准确率达到84%，优于基线模型的82%。同时，在精确率和F1-score等其他指标上也表现出优越性能。

Conclusion: 所提出的方法可作为开发个性化策略、有效降低辍学率和鼓励学生坚持学习的重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [160] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 本研究旨在解决数据稀缺领域（如计算化学、免疫学、医学影像）中无法训练大型预训练模型的问题。论文提出了基于神经记忆和超网络（Hypernetwork）等新颖架构，以实现高效的先验知识获取和知识迁移，从而在有限数据下也能进行有效的任务适应和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习依赖于大量数据训练大型预训练模型，但在计算化学、计算免疫学和医学影像等数据稀缺领域，这种方式不可行。研究动机在于设计新的架构，以在数据量不足的情况下也能高效地获取先验知识，克服传统方法的局限。

Method: ['设计用于在缺乏大量数据时高效获取先验知识的架构。', '利用神经记忆实现少量样本下对非平稳分布的适应。', '设计超网络（一个生成另一个网络的网络），并结合模型不可知元学习（MAML）进行训练。', '将超网络应用于3D场景生成，并扩展至3D分割任务。', '重新利用现有的分子生成方法作为预训练框架。']

Result: ['神经记忆能够使模型在仅有少量样本的非平稳分布上进行适应。', '与标准网络相比，通过MAML训练的超网络设计可以获得更具泛化性的先验知识。', '超网络能在少数训练场景中高效获取先验知识，从而加速文本到3D场景的生成。', '超网络框架能通过高效迁移早期场景的先验知识，在有限数据的新场景上执行3D分割。', '重塑的分子生成方法作为预训练框架，促进了分子属性预测的改进，解决了计算免疫学中的关键挑战。']

Conclusion: 本研究通过提出神经记忆和超网络等创新架构，以及利用现有分子生成方法进行预训练，有效解决了在数据稀缺领域进行高效先验知识获取和知识迁移的难题。这些方法在多种应用（包括3D场景生成、3D分割和分子属性预测）中展示了在有限数据下学习和适应的能力，为相关领域提供了重要的解决方案。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [161] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch Eco是一个基于LLM的智能代理系统，用于自动化科学综合。它通过递归探索提升文献检索多样性，实现用户可控的证据整合，并在生态学研究中显著提高源整合量和分析深度。


<details>
  <summary>Details</summary>
Motivation: 为了增强科学文献检索的多样性和细微性，并实现用户可控、高通量的领域特定证据整合，以克服传统RAG（检索增强生成）管线的局限性。

Method: 开发了DeepResearch$^{\text{Eco}}$，一个新颖的基于LLM的代理系统，支持对原始研究问题的递归、深度和广度控制探索。该系统还具备用户可控的综合、透明的推理和参数驱动的可配置性。

Result: 将DeepResearch应用于49个生态研究问题，实现了高达21倍的来源整合增长，以及每1000字整合来源数量14.9倍的提升。高参数设置下，系统展现了专家级的分析深度和上下文多样性。

Conclusion: DeepResearch$^{\text{Eco}}$成功地提升了自动化科学综合能力，显著增强了文献检索的多样性、效率和分析深度，是科学研究的有效工具。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [162] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 本文提出了一种名为“循环扩展（RE）”的新学习范式，超越传统深度学习，通过分析模型自身演化行为实现自我改进，并进一步扩展为MVRE、HMVRE及Sc-HMVRE，旨在构建可自省和自适应的AI。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习和深度学习主要关注从静态数据表示中学习。本研究旨在引入一个新维度，即从模型自身的演化行为中学习，以实现模型迭代式自我提升，并为开发能够反思自身学习动态的、可伸缩的、自省的、自适应的人工智能奠定基础。

Method: 该研究引入了“循环扩展（RE）”范式，通过同一深度架构多次映射数据，并结合内部表示（特征图）与性能信号（如损失）进行分析，实现迭代式自我改进。在此基础上，提出了“多宇宙RE（MVRE）”以聚合并行模型实例的信号，以及“异构MVRE（HMVRE）”以整合不同架构模型的多元视角。最后，引入了“可伸缩自适应变体（Sc-HMVRE）”，通过选择机制和尺度多样性实现实际部署。

Result: RE范式使模型能够进行迭代式自我提升，每个模型版本都能从前代中获取洞察。这标志着深度学习从纯粹的表示学习转向了行为感知、自我演化的系统。该研究为开发能够推理自身学习动态的新型智能模型奠定了基础，并指明了实现可伸缩、自省、自适应人工智能的途径。

Conclusion: 循环扩展（RE）代表了深度学习领域的一次重要范式转变，推动其从单纯的表示学习走向行为感知和自我演化系统。它为开发能够反思自身学习动态的新型智能模型奠定了基础，并为构建可伸伸、自省、自适应的人工智能提供了新的方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [163] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 本文提出一种基于可解释人工智能（XAI）的分层相关性传播（LRP）方法，以高效地选择深度神经网络（DNN）中需要三模冗余（TMR）保护的关键参数，从而在保持相同开销的前提下显著提高DNN对位翻转错误的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）在安全关键领域广泛应用，其可靠性至关重要。三模冗余（TMR）是提高DNN可靠性的有效技术，但其开销巨大。为降低开销，TMR需选择性地应用于模型输出贡献最高的参数和组件，因此，选择标准的准确性对TMR的效率至关重要。

Method: 本文提出一种利用可解释人工智能（XAI）的分层相关性传播（LRP）技术来提升DNN可靠性的方法。该方法使用LRP计算DNN参数的重要性得分，然后根据这些得分选择最关键的权重，并对其应用三模冗余（TMR）保护。

Result: 该方法在AlexNet模型上以10^-4的位错误率进行评估，结果显示其实现了超过60%的可靠性提升，同时保持与现有最先进方法相同的开销。

Conclusion: 基于XAI（特别是LRP）的选择性TMR是一种有效且高效的方法，可以在不增加额外开销的情况下，显著提高深度神经网络对位翻转错误的可靠性。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [164] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 本文提出一个面向发展中国家低识字率农民的语音AI决策支持系统，通过结合随机森林和LSTM模型，提供作物种植和市场价格预测，旨在帮助农民规避风险、实现利润最大化。


<details>
  <summary>Details</summary>
Motivation: 发展中国家农民面临极端市场和气候波动双重挑战，且因文化程度限制被排除在数字革命之外。现有系统多关注“能种什么”，而非“种什么最有利可图”。

Method: 开发了一种混合推荐引擎：使用随机森林分类器根据土壤、气候和实时天气数据评估农艺适宜性，并利用LSTM网络预测农作物市场价格。系统通过本地卡纳达语的端到端语音界面交付，利用微调的语音识别和高保真语音合成模型确保低识字率用户可访问性。

Result: 随机森林模型在适宜性预测方面达到98.5%的准确率，而LSTM模型能以低误差范围预测收获时节价格。该系统通过包容性界面提供数据驱动、经济优化的建议。

Conclusion: 该工作提供了一个可扩展且有影响力的解决方案，通过提供数据驱动的经济优化建议，增强边缘化农业社区的财务韧性，将重点从“能种什么”转向“种什么最有利可图”。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [165] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA在LLM微调中并非总能提供稳定的速度提升。本研究分析了其性能限制因素，并提出多种更高效的微调方法，这些方法在性能上与LoRA相当或更优，且速度提升更稳定。


<details>
  <summary>Details</summary>
Motivation: 观察到LoRA在不同模型架构和训练设置中，其速度提升并不一致，因此旨在深入分析其性能瓶颈和限制因素。

Method: 对LoRA的性能进行了全面分析，并调查了限制其速度提升的潜在因素。基于这些发现，提出了几种新的LLM高效微调方法，并通过实证评估与LoRA进行比较。

Result: 提出的方法在性能上与LoRA相当或更优，同时提供了更一致的训练速度改进。

Conclusion: 本研究为在资源受限情况下优化LLM微调提供了宝贵的见解和实用指导。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [166] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该论文提出一个基于物理信息神经网络（PINN）的框架，用于模拟海洋污染物扩散，通过嵌入物理定律和拟合含噪声数据，克服了传统方法的挑战，实现了物理一致且高效的模拟。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模拟广阔动态海洋域中复杂大规模的污染物传输时面临困难。

Method: 引入物理信息神经网络（PINN）框架来模拟受2D平流扩散方程控制的污染物扩散。通过将物理定律和来自有限差分法（FDM）的含噪声合成数据直接嵌入到神经网络训练过程中，实现物理一致性预测。训练采用混合损失函数，包含偏微分方程（PDE）残差、边界/初始条件符合性以及加权数据拟合项。利用Julia语言的科学计算生态系统进行高性能模拟。

Result: 该模型能够实现物理一致的预测。成功解决了非线性动力学以及边界和初始条件强制执行等挑战。该方法提供了一种可扩展且灵活的传统求解器替代方案。

Conclusion: PINN框架为复杂的海洋污染物传输建模提供了一个强大、可扩展且灵活的替代方案，有效克服了传统方法的局限性，并能生成物理一致的预测。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [167] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 本文提出一种基于Transformer的两步法，利用结构化时间序列数据（定性和定量），通过对比学习和Benjamini-Hochberg (BH) 程序进行洗钱检测，有效控制误报率并优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题，特别是需要一种能够有效利用复杂数据、减少人工监督并能同时检测欺诈者和非欺诈者的方法，同时控制误报率。

Method: 引入一种新的两步法，基于Transformer神经网络处理结构化时间序列（包含定性和定量数据）。第一步通过对比学习（无标签）学习时间序列的表示。第二步利用这些表示生成洗钱评分。为控制误报率，采用双阈值方法并结合Benjamini-Hochberg (BH) 程序。

Result: 实验证明，Transformer能够生成通用的时间序列表示，有效利用洗钱模式，且对领域专家的监督需求极少。新方法在检测欺诈者和非欺诈者方面均表现出更高的能力，同时能有效控制误报率。这显著优于基于规则或LSTM架构的现有方法。

Conclusion: 所提出的基于Transformer的新方法在洗钱检测方面表现出显著优势，尤其是在利用无标签数据、减少人工监督、提高检测能力和有效控制误报率方面，显著超越了传统的规则或LSTM模型。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [168] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: 本研究评估了CompactifAI压缩方法应用于Llama 3.1 8B大型语言模型的效果，发现其在显著降低计算资源消耗的同时，保持了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过模型压缩技术（CompactifAI），提高大型语言模型（Llama 3.1 8B）的效率、可扩展性和成本效益。

Method: 将CompactifAI压缩方法应用于Llama 3.1 8B模型。使用Codecarbon框架评估模型能耗效率，使用Ragas框架评估模型准确性。将压缩后的模型与原始全尺寸模型进行了对比。

Result: 研究发现，使用CompactifAI压缩后的模型显著降低了计算资源消耗，同时保持了原始模型的准确性。

Conclusion: CompactifAI方法能有效提高Llama 3.1 8B模型的效率、可扩展性和成本效益，且不影响其准确性。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [169] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 本文提出了一种名为 `wd1` 的新颖策略优化方法，旨在通过简化似然函数近似，解决扩散型大语言模型（dLLMs）在强化学习（RL）中推理能力提升所面临的计算开销和偏差问题，从而显著提升dLLMs的推理性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 通过强化学习提升扩散型大语言模型（dLLMs）的推理能力是一个未解决的难题。现有RL方法需要多次近似dLLMs的似然函数，导致计算开销大且存在潜在偏差，特别是在重要性采样的策略比率分母中出现近似误差时。

Method: 引入了一种名为 `wd1` 的新型策略优化方法。该方法将目标函数重构为加权似然形式，仅需对当前参数化策略似然进行单次近似，从而避免了多次近似带来的问题。

Result: 实验表明，`wd1` 在常用推理基准测试中，无需监督微调（SFT）或任何监督数据，其准确率比现有RL方法高出16%。此外，`wd1` 还带来了计算效率的提升，包括更短的训练时间以及每个梯度步中更少的功能评估次数（NFEs）。

Conclusion: `wd1` 因其卓越的性能、计算效率、实现简易性以及无需监督微调的特性，被证明是一种更有效、更高效的将强化学习应用于dLLMs推理的方法。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [170] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 针对路易体病(LBD)诊断中数据稀缺和域偏移问题，本研究提出一种可迁移性感知Transformer (TAT)模型，通过注意力机制从阿尔茨海默病(AD)数据中自适应地迁移知识，有效提升了LBD诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 路易体病(LBD)诊断面临数据稀缺的挑战，而阿尔茨海默病(AD)数据相对丰富。然而，LBD和AD数据存在明显的域偏移。因此，需要一种方法在缓解域偏移的同时有效利用AD数据来增强LBD诊断。

Method: 本研究提出了可迁移性感知Transformer (TAT)模型。该模型基于注意力机制，从结构磁共振(sMRI)导出的结构连接(SC)数据中学习。TAT能自适应地为疾病可迁移特征分配更高权重，同时抑制特定领域特征，以减少域偏移并提高诊断准确性。

Result: 实验结果表明，所提出的TAT模型是有效的。它在有限的LBD数据下，通过从AD数据迁移知识，显著提升了诊断准确率。

Conclusion: 本研究首次探索了在数据稀缺和域偏移条件下从AD到LBD的域适应诊断，为罕见疾病的域适应诊断提供了一个有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [171] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出一种名为WRCor的新型零样本NAS评估代理，显著提高了架构搜索的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）因需从头训练多个架构而计算昂贵且耗时；现有零样本NAS方法在有效性、稳定性、通用性方面仍有不足。

Method: 提出加权响应相关性（WRCor）作为训练无关的估计代理。WRCor利用不同输入样本的响应相关系数矩阵计算代理分数，以衡量估计架构的表达能力和泛化能力。

Result: 代理评估结果显示WRCor及其投票代理比现有代理更高效。架构搜索结果表明，所提出的零样本NAS算法在不同搜索空间中优于大多数现有NAS算法，能在4个GPU小时内发现ImageNet-1k数据集上测试误差为22.1%的架构。

Conclusion: WRCor是一种有效且高效的训练无关估计代理，显著加速了神经网络架构搜索并提升了其性能。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [172] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: 本文提出FedRAS框架，通过行动共享策略将联邦推荐系统中的梯度聚类为模型更新行动，显著降低通信开销，同时保持推荐性能，并适应异构环境。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FedRecs）面临两大挑战：一是由于大量物品嵌入导致极高的通信开销；二是网络环境和客户端设备异构性导致训练效率低下。现有压缩方法虽能减少通信，但会引入参数误差并导致模型性能下降。

Method: 本文提出FedRAS框架，采用行动共享策略，将物品嵌入的梯度聚类为特定数量的模型更新行动进行通信，而非直接压缩物品嵌入。由于梯度值远小于物品嵌入，限制梯度方向（即行动空间）引入的误差小于直接压缩整个物品嵌入矩阵。为适应异构设备和网络环境，FedRAS还集成了自适应聚类机制，动态调整行动数量。

Result: 实验结果表明，FedRAS可将通信载荷大小减少高达96.88%，同时在各种异构场景下不牺牲推荐性能。

Conclusion: FedRAS通过创新的行动共享策略和自适应聚类机制，有效解决了联邦推荐系统中的通信开销和训练效率问题，在大幅减少通信量的同时保持了出色的推荐性能。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [173] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: 本文提出了FLLL3M，一个结合联邦学习和大型语言模型的隐私保护下一位置预测框架，该框架在保持高准确性的同时，显著降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 为下一位置预测（NxLP）开发一个既能保护用户数据隐私，又能实现高精度和低资源消耗的框架。

Method: FLLL3M框架通过联邦学习将用户数据保留在本地，并通过高效的外积机制利用大型语言模型（LLMs）进行建模。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare等数据集上均取得了最先进（SOT）的准确率（例如Gowalla Acc@1: 12.55, MRR: 0.1422），同时将参数量减少了高达45.6%，内存使用减少了52.7%。

Conclusion: FLLL3M提供了一个高效、准确且隐私保护的下一位置预测解决方案，它成功地结合了联邦学习和大型语言模型的优势，在性能和资源效率方面均表现出色。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [174] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 提出DAFOS，一种动态自适应扇出优化采样器，通过优先重要节点和动态调整扇出，显著提升了大规模图神经网络(GNNs)的训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）中统一邻居采样和静态扇出设置限制了其在大规模图数据上的可扩展性和效率。

Method: 本文提出动态自适应扇出优化采样器（DAFOS），该方法根据模型性能动态调整扇出，并在训练过程中优先选择重要节点。具体而言，它利用基于节点度的节点评分来集中计算资源于结构上重要的节点，并随着训练进展增加扇出。DAFOS还集成了早停机制，以在性能提升减小时停止训练。

Result: DAFOS在ogbn-arxiv、Reddit和ogbn-products三个基准数据集上进行了实验验证，结果表明其相较于现有最佳方法显著提升了训练速度和准确性。例如，在ogbn-arxiv数据集上实现了3.57倍加速，在Reddit数据集上实现了12.6倍加速；同时，在ogbn-arxiv数据集上F1分数从68.5%提升至71.21%，在ogbn-products数据集上从73.78%提升至76.88%。

Conclusion: DAFOS提供了一个高效且可扩展的解决方案，有望应用于大规模图神经网络的训练。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [175] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 提出AMLAS-RL框架，为强化学习在网络物理系统中的安全应用提供系统性保证论证。


<details>
  <summary>Details</summary>
Motivation: 强化学习在网络物理系统（CPS）中应用日益广泛，但在安全关键场景中，其安全性保障面临重大挑战。现有安全强化学习方法缺乏系统性保证，且针对监督学习的安全框架（如AMLAS）不适用于强化学习的独特挑战。

Method: 通过迭代过程，改编并扩展了AMLAS方法，提出AMLAS-RL框架，旨在为启用强化学习的系统生成安全保证论证。

Result: 成功开发并展示了AMLAS-RL框架，该框架能够为强化学习驱动的系统提供系统性的安全保证论证，并通过一个轮式车辆避撞示例进行了演示。

Conclusion: AMLAS-RL提供了一个结构化方法，用于解决强化学习在安全关键型网络物理系统应用中缺乏系统性安全保证的难题，填补了现有方法的空白。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [176] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 研究表明，时间序列基础模型（TSFMs）在一致性预测中，尤其是在数据量有限的情况下，相比传统方法能提供更可靠的预测区间和更稳定的校准过程。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型的零样本能力在时间序列预测中进行一致性预测具有巨大潜力，因为可以将大部分可用数据用于校准。本研究旨在比较TSFMs与传统方法在一致性预测设置下的性能。

Method: 将时间序列基础模型（TSFMs）与包括统计模型和梯度提升在内的传统方法，在一致性预测环境下进行性能比较。

Result: 1. 在数据量有限时，TSFMs由于其卓越的预测精度，能提供比经典模型更可靠的一致性预测区间。2. 由于更多数据可用于校准，TSFMs的校准过程更稳定。3. 数据量越少，这些优势越明显，因为经典模型需要大量数据进行有效训练。

Conclusion: 基础模型在时间序列应用中，特别是在数据受限的情况下，具有提高一致性预测可靠性的巨大潜力。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [177] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 本文提出e-Profits，一种新颖的、以业务为导向的评估指标，用于衡量客户流失预测模型的财务表现。它通过Kaplan-Meier生存分析进行个性化评估，并证明比传统指标更能识别出具有更高投资回报率的模型。


<details>
  <summary>Details</summary>
Motivation: 客户关系管理中的挽留活动常依赖于流失预测模型，但传统评估指标（如AUC和F1-score）未能反映财务结果，可能导致错误的战略决策。

Method: 引入e-Profits，一个基于客户特定价值、留存概率和干预成本量化模型性能的业务对齐评估指标。与现有利润指标不同，e-Profits利用Kaplan-Meier生存分析估计个性化留存率，支持细粒度的逐客户评估。

Result: 在两个电信数据集（IBM Telco和Maven Telecom）上对六种分类器进行基准测试，结果显示e-Profits重塑了模型排名，揭示了传统指标（AUC或F1-score）曾忽视的模型的财务优势。该指标还提供了分段洞察，帮助识别哪些模型能最大化高价值客户的投资回报。

Conclusion: e-Profits是一个可理解的、事后分析工具，旨在支持商业环境下的模型评估，特别适用于优先考虑利润驱动决策的营销和分析团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [178] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 该论文提出了图神经网络（GNNs）在求解偏微分方程（PDEs）时所需消息传递迭代次数的精确下限，旨在减少超参数调优的需求。


<details>
  <summary>Details</summary>
Motivation: 为了显著减少GNNs求解PDEs时对消息传递迭代次数进行穷举式超参数调优的需求，并确保信息在网络中高效传播以获得准确的解。

Method: 通过将PDEs（包括双曲、抛物和椭圆型）的物理特性与GNNs的消息传递需求相关联，推导出消息传递迭代次数的下限。具体地，研究了方程的物理常数、空间和时间离散化以及GNNs消息传递机制之间的关系。

Result: 提出了在GNNs求解PDEs时所需消息传递迭代次数的精确下限。研究发现，当迭代次数低于这些限制时，信息传播效率低下，即使是深层GNNs也会产生糟糕的解；而当满足这些下限时，GNN模型能准确捕捉潜在现象，达到足够的精度。通过四个不同方程的示例验证了所提出下限的精确性。

Conclusion: 通过提供GNNs求解PDEs时消息传递迭代次数的精确下限，本研究优化了模型性能，确保了信息有效传播和求解精度，并显著减少了超参数调优的工作量。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [179] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究了数据偏见对算法歧视的影响，发现代理变量和标签偏见的组合比弱势群体代表不足更具歧视性。为此，提出并初步构建了“数据偏见档案（DBP）”以系统检测和记录偏见信号，从而预测歧视风险并指导干预。


<details>
  <summary>Details</summary>
Motivation: 数据中编码的不良偏见是算法歧视的关键驱动因素，尽管其重要性在算法公平性文献、反歧视立法和标准中得到广泛认可，但数据偏见本身仍未得到充分研究，阻碍了其检测和缓解计算最佳实践的发展。

Method: 1. ศึกษา并分析了三种常见数据偏见及其对算法歧视的单独和联合影响，涵盖多种数据集、模型和公平性度量。2. 开发了专用机制来检测特定类型的偏见。3. 将这些机制整合为一个初步的“数据偏见档案（DBP）”结构。4. 通过流行公平性数据集的案例研究，验证了DBP在预测歧视结果风险和评估公平性增强干预措施效用方面的有效性。

Result: 1. 训练集中弱势群体的代表不足对歧视的影响低于传统认知。2. 代理变量（proxies）和标签偏见（label bias）的组合可能更为关键，更能导致歧视。3. 初步构建的DBP能有效预测歧视性结果的风险，并证明了公平性增强干预措施的实用性。

Conclusion: 本文通过数据中心视角，弥合了算法公平性研究与反歧视政策之间的鸿沟。数据偏见档案（DBP）的初步构建，作为一个概念验证，展示了如何系统地记录不同的偏见信号，从而有助于预测歧视风险和指导干预措施。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [180] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出一种AI研究建议系统，通过结合小型模型、压缩文献数据库和结构化推理，在评估研究想法方面超越大型通用语言模型，尤其在高置信度下能达到高接受率，有望显著提升假设生成与实验设计的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在自动化假设生成和实验设计方面取得了显著进展，但在提供高质量、有充分理由的反馈以完善这些假设和设计方面，仍然缺乏可扩展的建议系统。

Method: 研究团队探索了模型大小、上下文长度、置信度估计和结构化推理过程等关键因素，以开发健壮的建议系统。具体方法是构建了一个相对较小的模型，并为其配备了良好压缩的文献数据库和结构化推理框架。

Result: 研究发现，一个配备了良好压缩文献数据库和结构化推理框架的相对小型模型，在ICLR 2025自评前30%提交的接受率方面，优于Deepseek-R1等强大的通用语言模型。此外，当仅限于高置信度预测时，该系统在ICLR 2025测试集上达到了超过90%的接受率。

Conclusion: 该系统有望通过提供高质量的反馈，显著提高假设生成和实验设计的质量和效率。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [181] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文提出一种基于学习的交通需求建模框架，该框架数据驱动、可扩展且可转移，在洛杉矶的实施和验证表明其性能媲美传统模型，但成本显著降低，精度高。


<details>
  <summary>Details</summary>
Motivation: 传统的基于活动的交通需求模型（ABMs）虽然植根于行为理论，但常依赖简化规则和假设，开发成本高昂，且难以适应不同区域。

Method: 本文提出一个学习型交通需求建模框架，基于家庭的社会人口特征综合生成家庭协调的日常活动模式。该框架将人口合成、协调活动生成、地点分配和大规模微观交通仿真整合到一个统一系统中，具有完全生成性、数据驱动、可扩展和可转移的特点。

Result: 该框架在洛杉矶（1000万人口）进行了全面实施。综合验证表明，模型能准确复现真实出行模式，性能与传统ABMs相当，但建模成本显著降低，可扩展性更强。具体而言，与SCAG ABM基准相比，起点-终点（OD）矩阵余弦相似度达0.97，每日车辆里程（VMT）的Jensen-Shannon散度为0.006，平均绝对百分比误差（MAPE）为9.8%。与Caltrans PeMS的真实观测数据相比，走廊级交通速度和流量评估的JSD为0.001，MAPE为6.11%。

Conclusion: 所提出的学习型交通需求建模框架是一种高效、准确且可扩展的解决方案，能够克服传统ABMs的局限性，为交通规划和政策制定提供有力工具。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [182] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 本文提出一种基于CLIP模型的语义通信框架，其核心优势在于无需训练，并利用强化学习在噪声无线网络中优化其部署，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信框架需要编码器和解码器在公共数据集上联合训练，且其语义信息在噪声无线网络中易受干扰，同时频谱资源有限。研究旨在提出一种无需训练的语义通信方法，并解决在噪声、时延和能耗约束下，如何优化CLIP模型架构和频谱资源分配以最大化通信性能的问题。

Method: 设计了一种基于CLIP模型（对比语言-图像预训练模型）的语义通信框架，该框架的发送端无需神经网络模型训练即可提取数据语义。为应对噪声无线信道并优化性能，采用基于近端策略优化（PPO）的强化学习算法，学习无线噪声如何影响语义通信性能，从而为每个用户找到最优的CLIP模型和频谱资源块（RB）分配。

Result: 仿真结果表明，所提出的方法与Soft Actor-Critic算法相比，收敛速度提高了40%，累积奖励提高了4倍。

Conclusion: 本研究成功设计并优化了一种无需训练的CLIP模型语义通信框架，有效解决了传统方法中的联合训练需求和噪声环境下的部署挑战，并在性能上取得了显著提升。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [183] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: 本文开发了一种名为VIPEEGNet的卷积神经网络模型，用于及时识别脑电图（EEG）中的有害脑活动。该模型在大规模临床数据集上展现出高准确性，性能与人类专家相当，并且在外部验证中以极低的参数量获得了顶尖排名。


<details>
  <summary>Details</summary>
Motivation: 及时识别脑电图（EEG）中的有害脑活动对于脑疾病的诊断和治疗至关重要，但现有的人工智能（AI）模型受限于判读差异、资源限制和泛化能力差，应用受限。

Method: 本研究开发并验证了一个名为VIPEEGNet的卷积神经网络模型。该模型使用来自麻省总医院/哈佛医学院的两个独立EEG数据集进行开发和验证，其中包括1950名患者的开发队列（106,800个EEG片段）和1532名患者的在线测试队列。

Result: VIPEEGNet在开发队列中对六类有害脑活动的二分类AUROC范围在0.930至0.972之间。在多分类任务中，敏感性范围为36.8%至88.2%，精确度范围为55.6%至80.4%，性能与人类专家相似。外部验证结果显示，模型的Kullback-Leibler散度（KLD）分别为0.223和0.273，在2767个竞争算法中排名第二，且参数量仅为排名第一算法的2.8%。

Conclusion: VIPEEGNet模型在识别有害脑活动方面表现出卓越的准确性和效率，其性能与人类专家水平相当，并显著优于现有顶级算法的参数效率。这表明它有望克服现有AI模型的局限性，促进有害脑活动的及时识别和临床应用。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [184] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 该研究提出了一种名为ODIA的新方法，通过知识蒸馏加速大语言模型的功能调用，显著降低延迟并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的功能调用（Function Calling）存在高延迟问题，严重影响用户体验。

Method: 本文提出了一种名为“在线加速导向蒸馏”（Oriented Distillation for Inline Acceleration, ODIA）的新方法。该方法利用在线用户交互数据，自动识别生产流量中的“简单查询”，并将知识从大型模型蒸馏到小型模型，以实现加速。

Result: ODIA方法使响应延迟预期降低45%，中位数降低78%，同时保持准确性。在实际音乐应用的部署中，小型模型成功处理了60%的流量，且准确性损失可忽略不计。

Conclusion: ODIA是一种高效且实用的解决方案，能够加速LLM的功能调用，显著改善用户体验，且由于其自动化数据收集和模型更新机制，非常适合生产环境。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [185] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 本文提出并评估了一种名为LL-HMC的深度神经网络最后一层哈密顿蒙特卡洛采样方法，旨在降低计算成本，同时在分布内分类和分布外检测方面表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 哈密顿蒙特卡洛（HMC）被认为是估计不确定性的黄金标准，但其计算需求限制了在大规模数据集和大型深度神经网络上的应用。为了在计算资源有限的数据密集型场景中应用HMC，需要降低其计算复杂度。

Method: 研究方法是限制HMC采样仅在深度神经网络的最后一层进行（LL-HMC）。将LL-HMC与五种其他最后一层概率深度学习（LL-PDL）方法在三个真实的驾驶员行为和意图视频数据集上进行比较。评估指标包括分布内分类性能、校准和分布外（OOD）检测。为确保结果的鲁棒性，对不同随机种子进行了五次超参数网格搜索。

Result: LL-HMC在分布内分类和OOD检测方面均表现出竞争力。额外采样的最后一层参数未能提升分类性能，但能改善OOD检测。多条链或不同起始位置并未带来一致的性能提升。

Conclusion: LL-HMC是一种有效且具有竞争力的最后一层概率深度学习方法，尤其适用于分布外检测，解决了传统HMC的计算限制。然而，过多的采样对分类无益，且多链或多起点并未带来一致的改进。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [186] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 本文提出了一种名为Fair-FLIP的后处理方法，旨在减轻深度伪造检测模型中存在的偏见，同时保持检测性能。实验表明，该方法可将公平性指标提升高达30%，而准确率仅略微下降0.25%。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成内容（特别是深度伪造）的恶意使用对公众信任和舆论构成严重威胁。尽管深度伪造检测方法具有高预测性能，但它们往往在种族和性别等人口统计学属性上表现出偏见。因此，研究的动机是应对公平的深度伪造检测挑战，旨在缓解这些偏见，同时保持强大的检测能力。

Method: 本研究提出了一种新颖的后处理方法，称为“公平导向的最终层输入优先排序”（Fair-FLIP）。该方法通过重新加权已训练模型的最终层输入，以减少子群体差异。它优先处理变异性低的输入，同时降低变异性高的输入的重要性。

Result: 实验结果显示，与基线（无公平性偏见缓解）和最先进的方法相比，Fair-FLIP可以将公平性指标提高高达30%，同时保持基线准确性，仅产生0.25%的微不足道的下降。

Conclusion: Fair-FLIP是一种有效的后处理方法，可以显著增强深度伪造检测的公平性，同时最大限度地减少对检测准确性的影响，为解决AI模型中的人口统计学偏见提供了有前景的途径。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [187] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出一种新的步长策略，使洗牌式梯度方法在不假设Lipschitz平滑的条件下收敛，并达到当前最佳收敛速率，拓宽了其适用性。


<details>
  <summary>Details</summary>
Motivation: 洗牌式梯度方法因其简洁和快速性能在实践中广受欢迎，但其收敛性证明通常需要Lipschitz平滑条件，这在许多机器学习模型中难以满足，导致理论与实践脱节。

Method: 研究者重新审视了在不假设Lipschitz平滑条件下的洗牌式梯度方法的收敛性。通过引入一种新的步长策略，在非凸、强凸和非强凸场景下，以及随机洗牌和任意洗牌方案下，于一般有界方差条件下，证明了收敛速率。并辅以数值实验验证算法性能。

Result: 所提出的洗牌式梯度算法在更弱的假设下（无需Lipschitz平滑）实现了收敛，并能匹配当前已知的最佳收敛速率。理论上证明了其在不同凸性（非凸、强凸、非强凸）和洗牌方案下的收敛性。数值实验进一步证实了算法的有效性和实用性。

Conclusion: 该研究成功解决了洗牌式梯度方法对Lipschitz平滑条件的依赖问题，提出了一种在理论上严谨且实践中高效的算法，显著提升了洗牌式梯度方法的适用范围和可靠性。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [188] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [189] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 本文分析了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维度建模捕捉用户兴趣迁移，并在实验中取得了良好表现，尤其在一个平台达到0.937的AUC值。


<details>
  <summary>Details</summary>
Motivation: 旨在提高跨平台广告推荐的准确性。

Method: 分析了一种基于图神经网络（GNN）的广告推荐方法。该方法通过多维度建模，整合了用户行为数据（如点击频率、活跃时长以揭示兴趣演变的时间模式）、广告内容（如类型、标签、时长以影响语义偏好）和平台特征（如设备类型、使用上下文以塑造兴趣过渡环境）。这些因素共同使GNN能够捕捉用户跨平台的兴趣迁移潜在路径。

Result: 实验基于三个平台的数据集。平台B的AUC值达到0.937，表现最佳。平台A和平台C的精度和召回率略有下降，原因是广告标签分布不均匀。通过调整学习率、批大小和嵌入维度等超参数，模型在异构数据中的适应性和鲁棒性得到了进一步提升。

Conclusion: 该基于GNN的方法通过多维度数据建模和捕捉用户兴趣迁移，有效提升了跨平台广告推荐的准确性。模型表现良好，通过超参数调整进一步提高了在异构数据中的适应性和鲁棒性。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [190] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文理论分析了离散扩散模型中的无分类器引导(CFG)，揭示了早期高引导的危害和现有CFG的不平衡过渡问题，并提出了一种简单有效的改进机制。


<details>
  <summary>Details</summary>
Motivation: 尽管无分类器引导(CFG)广泛用于条件生成和提高连续扩散模型样本质量，并已扩展到离散扩散，但其在离散设置下的行为，尤其是引导调度的作用，缺乏理论分析。此外，现有CFG实现存在缺陷，可能导致样本质量下降。

Method: 通过理论分析掩蔽离散扩散中的CFG，重点关注引导调度的作用。基于分析结果，提出了一种新的无分类器引导机制，通过平滑数据分布与初始分布间的传输来改善样本质量，该方法仅需一行代码修改。

Result: 理论分析表明，采样早期（输入高度掩蔽时）高引导会损害生成质量，而后期引导影响更大。分析还揭示了现有CFG实现可能导致不平衡过渡（如早期过快去掩蔽），从而降低样本质量。提出的新机制在ImageNet和QM9上的实验证明能有效提升样本质量。

Conclusion: 本研究为离散扩散模型中CFG的引导调度提供了理论解释，指出了现有实现的问题，并提出了一种简单、有效且普遍适用的CFG改进机制，显著提升了离散扩散模型的样本生成质量。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [191] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: 研究引入了ToxBench数据集和DualBind模型，旨在利用高质量AB-FEP数据，提升机器学习在蛋白质-配体结合亲和力预测中的准确性和效率，以加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体结合亲和力预测对药物研发至关重要。当前机器学习方法受限于可靠数据，而高精度的物理方法（如AB-FEP）计算成本过高。研究旨在弥合这一鸿沟，通过提供大规模、高质量的AB-FEP数据集来推动机器学习发展。

Method: 研究构建了ToxBench，这是首个大规模AB-FEP数据集，包含8,770个ERα-配体复合物的结合自由能（部分经实验验证，RMSE为1.75 kcal/mol），并设计了非重叠的配体拆分以评估模型泛化能力。在此基础上，研究基准测试了现有机器学习方法，并提出了基于双损失框架的DualBind模型。

Result: ToxBench为机器学习开发提供了可靠的大规模AB-FEP数据。基准测试结果表明，DualBind模型表现优异。此外，机器学习方法展示了以远低于物理方法的计算成本，近似AB-FEP计算的潜力。

Conclusion: 高质量的AB-FEP数据集（如ToxBench）结合先进的机器学习模型（如DualBind），能有效实现高精度和高效率的蛋白质-配体结合亲和力预测，为药物发现和毒性评估提供了有力的工具。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [192] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: 研究表明，设计适当的物理信息神经网络（PINN）能成功模拟二维和三维完全湍流，无需传统网格或训练数据。


<details>
  <summary>Details</summary>
Motivation: 湍流模拟对计算资源需求巨大，在高流速下成本过高。传统的基于网格的计算方法存在局限性，促使研究人员探索新的无网格、连续的解决方案。

Method: 本研究开发并使用了经过适当设计的PINN，结合了多项算法创新，包括自适应网络架构、因果训练和高级优化方法，使其能够直接从物理方程中学习，以模拟2D和3D的完全湍流。

Result: PINN成功模拟了完全湍流，并在严格验证后，准确再现了包括能量谱、动能、涡量和雷诺应力在内的关键流体统计量。

Conclusion: 研究结果表明，神经方程求解器（PINN）能够处理复杂的混沌系统，为连续湍流建模提供了新的可能性，并有望超越传统计算方法的限制。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [193] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本文提出模拟基础神经网络（SGNNs），通过利用机械模拟数据训练神经网络，旨在结合机械模型的可解释性与机器学习模型的灵活性，在多种科学预测和推断任务中实现最先进的性能，并提供新的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前科学建模面临核心局限：机械模型虽然可解释但难以应对真实世界的复杂性；机器学习模型灵活但需要大量标注数据，无法推断不可观测变量，且缺乏透明度（黑箱问题）。

Method: 引入模拟基础神经网络（SGNNs）框架，该框架利用机械模拟作为神经网络的训练数据。SGNNs通过在包含多样模型结构、参数范围、随机性和观测误差的合成语料库上进行预训练。

Result: SGNNs在多个科学领域和建模任务中均实现了最先进的性能：在预测任务中，将COVID-19预测技能较CDC基线提高了近三倍，化学产率预测误差减少了三分之一，并在生态预测中保持了高准确性。在推断任务中，SGNNs能够准确分类模拟社交网络中的信息传播源，并能更准确地估计COVID-19传染性等不可观测目标。此外，SGNNs还实现了“回溯模拟归因”，提供了一种新的机械可解释性，揭示模型认为哪些潜在动力学是活跃的。

Conclusion: SGNNs将科学理论与深度学习的灵活性相结合，开启了一种新的建模范式。它将模拟从僵化工具转变为灵活的监督来源，即使在缺乏真实数据的情况下，也能实现鲁棒且可解释的推断。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [194] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 本文提出一个系统性框架，通过整合表示引导来改进扩散模型，具体通过两种新策略（多模态配对学习和优化训练课程），显著提升了图像、蛋白质序列和分子生成任务的性能和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，将扩散模型的内部表示与预训练模型对齐能提升生成质量，但缺乏系统性方法。本文旨在提供一个系统性框架，以更有效地利用输入表示来改进扩散模型。

Method: 1. 提出一个系统性框架，包含去噪模型的多种分解及其训练标准，以确定辅助表示的整合方式和时机。
2. 引入两种新策略：a) 将样本与目标表示（源于自身或不同合成模态）配对，并学习多模态对的联合模型；b) 设计一个平衡表示学习和数据生成的优化训练课程。

Result: 1. 在图像、蛋白质序列和分子生成任务中展示了卓越的性能和训练加速。
2. 在ImageNet $256\times 256$类别条件基准测试中，训练速度比原始SiT-XL快23.3倍，比最先进方法REPA快4倍。

Conclusion: 通过系统性的表示引导框架和创新的训练策略，本文显著提升了扩散模型的生成质量和训练效率，并在多模态生成任务中取得了优异成果。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [195] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 研究揭示模型排行榜可被恶意利用，大规模隐蔽分发中毒模型，并提出防御机制需求。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习中毒攻击已被广泛研究，但攻击者如何大规模分发中毒模型的机制，特别是通过模型排行榜这一渠道，仍未被充分探索。

Method: 提出了一个名为TrojanClimb的通用框架。该框架允许在模型维持竞争力排行榜表现的同时，注入恶意行为。

Result: 通过在文本嵌入、文本生成、文本转语音和文本转图像四种不同模态上的实验，证明了TrojanClimb的有效性。攻击者可以成功地在排行榜上获得高排名，同时嵌入任意有害功能，如后门或偏见注入。

Conclusion: 研究揭示了机器学习生态系统中的一个重大漏洞，强调了重新设计排行榜评估机制以检测和过滤恶意模型的紧迫性，并警示了从未经证实来源采用模型所带来的广泛安全风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [196] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 该研究开发了一个自监督深度学习模型，通过分析多模态生理信号（EEG、ECG、呼吸）生成个体化心血管疾病（CVD）风险评分，显著提高了CVD事件和死亡率的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 论文旨在开发一种新颖的框架，能够直接从多模态生理信号（如PSG数据）中提取有意义的模式，从而生成个体化的CVD风险评分，以增强临床风险评估和支持个性化护理。

Method: 研究开发了一个自监督深度学习模型，用于从EEG、ECG和呼吸信号中提取模式。该模型在4,398名参与者的数据上训练，通过对比有无CVD结果个体的嵌入来得出投影分数。在1,093名参与者的独立队列中进行了外部验证。相关源代码已公开。

Result: 投影分数揭示了跨模态的独特且具临床意义的模式。ECG特征能预测既往和新发心脏病，尤其是CVD死亡率；EEG特征能预测新发高血压和CVD死亡率；呼吸信号提供了补充预测价值。将这些投影分数与Framingham风险评分结合，预测性能显著提高（不同结果的AUC值在0.607至0.965之间）。研究结果在外部测试队列中得到了鲁棒性复制和验证。

Conclusion: 研究结果表明，所提出的框架可以直接从PSG数据生成个体化的CVD风险评分。这些投影分数有潜力被整合到临床实践中，以增强风险评估并支持个性化护理。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [197] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 本研究利用人类注视信息优化计算成本高昂的RLHF，通过注视感知奖励模型和基于注视的稀疏奖励分布，实现了更快的收敛速度并保持了性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习与人类反馈（RLHF）在对齐语言模型与人类偏好方面有效，但其计算成本高昂。

Method: 探索了两种利用人类注视建模增强RLHF的方法：1) 注视感知的奖励模型；2) 基于注视的稀疏奖励在token级别的分布。

Result: 实验证明，注视增强的RLHF在保持或略微提升性能的同时，显著加速了收敛，从而降低了策略优化过程中的计算成本。

Conclusion: 人类注视是策略优化中一个有价值且未被充分利用的信号，有望显著提高RLHF的效率。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [198] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 本文揭示了当前大语言模型（LLM）推理系统评估中存在的普遍缺陷和“反模式”，提出了一套综合性清单和框架，旨在纠正这些问题，实现更鲁棒、更符合实际需求的评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理系统效率显著提升，但现有评估方法存在根本性缺陷，导致评估结果不准确，阻碍了科学进步。特别是在LLM推理的双阶段、异构工作负载及严格时间要求下，这些问题尤为突出。

Method: 通过对近期系统的系统分析和全面检查，识别出基线公平性、评估设置和度量设计三个关键维度上的常见评估反模式。本文论证了这些反模式如何导致误导性结论，并基于分析提出了一个综合性清单和框架，以识别并避免这些问题。同时，通过推测解码的案例研究，展示了所提框架的实用性。

Result: 研究揭示了当前LLM推理评估中常见的“反模式”及其对结果的误导性影响。识别并详细阐述了三大反模式维度。提出了一套全面的评估框架和清单，旨在帮助研究人员避免这些缺陷，从而实现更准确、可复现的性能评估。案例研究证明了该框架在分析复杂推理技术（如推测解码）时的有效性。

Conclusion: 本文为LLM推理系统的评估方法奠定了严格的基础，促进了有意义的比较和结果复现性。通过纠正现有评估方法的缺陷并使其与真实世界需求对齐，有望加速LLM推理技术的真正进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [199] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 一种新的分布式训练方法，通过在不同节点上训练小型子网络，有效降低大规模模型预训练的内存和通信开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式大模型预训练对单个节点内存要求高，且节点内通信开销大。

Method: 提出一种新颖方法：在独立工作节点上训练模型的小型结构化子网络，以减少内存需求。该方法避免了节点间激活通信，带宽需求与all-reduce数据并行方案相当或更低。评估了两种子网络构建策略：随机块丢弃（stochastic block dropping）和宽度方向子网络构建（width-wise subnetwork construction）。

Result: 随机块丢弃技术持续优于宽度方向子网络构建。性能优势归因于保留了带有跳跃连接的块的子网络中更强的梯度对齐。初步实验显示，内存使用减少20-40%，且性能无损失。

Conclusion: 该方法在显著降低内存消耗的同时保持了模型性能，为大规模分布式预训练提供了一个有前景的解决方案。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [200] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 本文提出R-MDN层，通过递归最小二乘算法在持续学习中去除混杂变量影响，减少灾难性遗忘，促进公平预测。


<details>
  <summary>Details</summary>
Motivation: 在持续学习背景下，学习对混杂变量不变的特征表示仍然是巨大挑战，传统方法如元数据归一化（MDN）未能有效解决。混杂变量导致虚假关联和有偏预测。

Method: 引入递归元数据归一化（R-MDN）层，可集成到任何深度学习架构和模型阶段。R-MDN通过递归最小二乘算法执行统计回归，持续更新内部模型状态，以适应数据和混杂变量不断变化的分布。

Result: 实验证明，R-MDN通过减少由混杂变量效应随时间变化引起的灾难性遗忘，在静态学习和持续学习的不同阶段都促进了跨人群组的公平预测。

Conclusion: R-MDN层能有效处理持续学习中的混杂变量影响，从而减轻灾难性遗忘并提升预测公平性。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [201] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 针对自主智能体快速在线探索与适应的挑战，本文提出“行为探索”方法。通过训练长上下文生成模型学习专家演示，使其能在上下文中模仿并选择不同专家行为，实现快速适应和有针对性的探索。该方法在模拟和真实机器人任务中均表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的自主智能体在快速在线探索和行为适应方面不及人类，后者能通过少量交互迅速获取新信息和技能，而算法常依赖随机探索和缓慢的梯度更新。

Method: 本文受上下文学习和大规模行为克隆启发，提出“行为探索”。具体地，利用专家演示数据集，训练一个长上下文生成模型，使其基于过去的观测上下文和专家行为的“探索性”度量来预测专家动作。这使模型不仅能模仿专家行为，还能通过其交互历史在上下文中选择不同的专家行为，从而实现快速在线适应和有针对性的“专家式”探索。

Result: 该方法在模拟的运动和操作设置以及真实世界的机器人操作任务中均得到验证，展示了其学习适应性探索行为的能力。

Conclusion: “行为探索”方法成功使自主智能体学习到类似人类的快速适应和探索行为，有望缩小与人类智能在探索和适应能力上的差距。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [202] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 本文提出一种新框架，通过识别高斯概率生成模型(GPGMs)去噪过程中的“高斯性”特征步，并用闭合形式的高斯近似替换后续生成轨迹，显著提升了GPGMs的计算效率和样本质量。


<details>
  <summary>Details</summary>
Motivation: 高斯概率生成模型(GPGMs)虽性能卓越，但其漫长（数百至数千步）的生成轨迹导致计算成本高昂，严重限制了其实际部署。

Method: 引入一个理论基础和经验验证的框架。核心思想是，数据在去噪过程中会迅速趋近高斯分布。该方法解析识别出数据获得足够高斯性的特征步，然后用闭合形式的高斯近似替换剩余的生成轨迹。此举避免了冗余的随机扰动，同时保留了学习动态的完整分辨率。

Result: 在多种数据模态上的实证结果表明，本方法在样本质量和计算效率方面均取得了显著提升。

Conclusion: 该框架有效解决了GPGMs高计算成本的限制，在不牺牲训练粒度或推理保真度的情况下，显著提高了生成效率和样本质量，使其更具实用性。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [203] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 本文提出了“动作分块”和“噪声注入”两种最小干预措施，以可证明地缓解连续状态动作模仿学习中的累积误差问题。


<details>
  <summary>Details</summary>
Motivation: 连续物理系统（如机器人、自动驾驶）中的模仿学习受累积误差困扰，导致不稳定性，与离散设置的成功形成对比。现有方法复杂，且纯粹从专家轨迹学习会不可避免地产生指数级累积误差，急需新解决方案。

Method: 为缓解连续模仿学习中的累积误差，本文提出两种“最小干预”措施：1. 对于开环稳定系统，采用“动作分块”，即开环预测并执行一系列动作。2. 对于可能不稳定系统，采用“噪声注入”，即在专家演示中加入噪声。研究结合了控制理论和强化学习的工具与见解。

Result: 所提出的“动作分块”和“噪声注入”干预措施被证明能有效缓解连续状态动作模仿学习中的累积误差。研究发现，这些干预措施带来的益处与它们在现代机器人学习中原设计目标的效果不同。分析结合控制理论和强化学习，揭示了单独考虑任一领域时不会出现的新的考量。

Conclusion: 所提出的最小干预措施能有效解决连续模仿学习中的累积误差问题，为稳定和不稳定系统提供了鲁棒的解决方案。该工作通过整合控制理论和强化学习，提供了新的理论见解。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [204] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 本文提出结合排队论和注意力模型的QT-SimAM，用于高精度、可泛化的航班延误预测。


<details>
  <summary>Details</summary>
Motivation: 航班延误给航空业带来严重的财务和运营影响。为了提升乘客体验并减少损失，需要开发精确且能跨不同网络泛化的航班延误预测模型。

Method: 本文提出一种结合排队论（Queue-Theory）与简单注意力模型（SimAM）的新方法，命名为QT-SimAM。该方法采用双向模型（QT-SimAM Bidirectional）进行预测。

Result: 在US Bureau of Transportation Statistics数据集上，QT-SimAM（Bidirectional）模型的准确率为0.927，F1分数为0.932，优于现有方法。在EUROCONTROL数据集上测试可迁移性时，模型表现强劲，准确率达到0.826，F1分数为0.791。

Conclusion: 本文提出了一种有效、端到端的航班延误预测方法。该模型能以高精度预测不同网络的航班延误，有助于降低乘客焦虑并改进运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [205] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [206] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 本文提出一种模型无关的隐式神经信号表示框架，用于泛化连续时间向量值信号的低秩分解问题（如PCA和ICA），使其能应用于点云和不规则采样信号。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩分解方法（如PCA、ICA）难以直接应用于连续时间向量值信号，尤其是在处理点云和不规则采样信号时，标准技术往往失效。

Method: 将信号建模为连续时间随机过程，并开发了一个模型无关的隐式神经信号表示框架。该框架通过在网络损失中引入一个对比函数项，统一了连续设置下PCA和ICA的方法，从而强制分解学习到的源信号满足所需的统计特性（如去相关性、独立性）。

Result: 成功地将低秩分解问题推广到连续时间域，并提供了一种学习数值近似解的方法。这种方法能够有效地处理标准技术无法应对的点云和不规则采样信号。

Conclusion: 将低秩分解扩展到连续域的创新方法，极大地拓宽了其应用范围，使其能够处理传统方法难以分析的数据类型，如点云和不规则采样信号，为复杂信号分析提供了新的工具。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [207] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 论文揭示了直接偏好优化（DPO）是机器学习中Savage损失函数和随机选择理论之间的一种特定且通用的联系形式。


<details>
  <summary>Details</summary>
Motivation: 理解DPO的运作原理至关重要，因为其模型应用广泛，DPO当前热度高，且许多先进的DPO变体都可被纳入或解释其在此理论框架中的位置，有助于理解其优缺点和规避潜在陷阱。

Method: 通过在通用层面上建立Savage损失函数与随机选择理论之间的联系，将DPO展示为这种联系的一种具体形式。此方法涵盖了选择理论中的弃权、机器学习中的非凸目标，并能够自然地解释DPO的现有扩展（如边际和长度修正）。

Result: 研究表明，DPO是Savage损失函数与随机选择理论之间的一种具体连接。这种普遍的连接支持选择理论中的弃权、机器学习中的非凸目标，并能免费地构建DPO的显著扩展，包括边际和长度修正。

Conclusion: 通过将DPO置于更广阔的理论框架中，可以深入理解其原理，整合现有扩展，识别当前最先进DPO变体的位置，并有助于理解偏离此框架可能带来的问题及寻找解决方案。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [208] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 本文提出DejaVu攻击，利用网络延迟在自动驾驶多模态融合中制造传感器时间错位，导致感知任务（如目标检测和跟踪）性能严重下降。为应对此威胁，提出AION防御机制，通过跨模态时间一致性检测此类攻击，并表现出高检测率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多模态融合（MMF）严重依赖精确的时间同步，这暴露了一个新的安全漏洞。网络诱导的延迟可能导致细微的时间错位，从而严重降级下游的MMF感知任务。本研究旨在揭示这一漏洞并提出有效的防御方案。

Method: 攻击方法（DejaVu）：利用网络引入的延迟，在相机和激光雷达传感器流之间制造时间错位。防御方法（AION）：作为现有感知模型的补丁，通过跨模态时间一致性监控时间对齐。具体采用多模态共享表示学习和动态时间规整（DTW）来确定时间对齐路径并计算异常分数。

Result: DejaVu攻击效果显著：单个激光雷达帧延迟可使车辆检测mAP降低高达88.5%；三个相机帧延迟可使车辆多目标跟踪准确率（MOTA）下降73%。研究还发现，目标检测高度依赖激光雷达输入，而目标跟踪高度依赖相机输入。AION防御在不同数据集和模型架构上均达到0.92-0.98的AUROC分数，误报率低，证明其对时间错位攻击具有鲁棒性和泛化性。

Conclusion: 时间同步是自动驾驶多模态融合的关键弱点，DejaVu攻击能够有效利用此弱点造成严重性能退化。AION防御机制为检测此类时间错位攻击提供了一个鲁棒且通用的解决方案，增强了自动驾驶感知系统的安全性。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [209] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 为解决电商购物中用户难以凑齐完整餐食的问题，本文将购物篮补全重构为集合到集合推荐任务，提出了基于Set Transformer的多任务学习框架S2SRec2，旨在推荐多缺失食材并评估购物篮完整性，实验证明其显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在生鲜电商中，顾客通常按饮食偏好选择食材，但缺乏创建完整餐食的专业知识。传统食谱补全方法只预测单个缺失食材，不符合实际多食材需求，且忽略缺失食材间的关系。因此，需要一种能推荐多个互补食材并考虑其内部关系的方法。

Method: 将购物篮补全重构为集合到集合（S2S）推荐问题，即输入不完整购物篮，预测一组互补食材。提出了S2SRec2框架，该框架基于Set Transformer，并采用多任务学习范式进行训练。S2SRec2协同学习两个任务：从现有食材表示中检索缺失食材；以及在预测后评估购物篮的完整性。这两个任务被共同优化。

Result: 在大型食谱数据集上的实验和定性分析表明，S2SRec2显著优于单目标基线方法。

Conclusion: S2SRec2提供了一种有前景的方法，可以有效增强在线杂货购物体验并激发烹饪创意。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [210] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文研究了特征选项在强化学习中加速信用分配的作用，发现预设特征选项对探索和信用分配均有益，但在线学习需权衡。


<details>
  <summary>Details</summary>
Motivation: 选项虽为强化学习的强大框架，但常需手动设计而非学习。特征选项在探索中表现出色，但其在信用分配中的作用尚未充分探索。

Method: 作者在表格和像素化网格世界中评估了特征选项，比较了预设与在线发现的效果。在深度强化学习中，提出了一种非线性函数近似下学习选项值的方法，并强调了终止条件的影响。

Result: 发现预设特征选项不仅有助于探索，还能加速信用分配；而在线发现的特征选项可能因过度偏置智能体的经验而阻碍学习。

Conclusion: 研究揭示了使用特征选项（及更广泛的选项）在强化学习中同时支持信用分配和探索的潜力和复杂性。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [211] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 本文提出GPAWP框架，结合图提示与权重剪枝，通过评估和剪枝负面提示，显著提高图神经网络（GNNs）中图提示的性能和效率，减少参数量。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）表现优异，但仍面临训练推理耗时长、难以捕捉复杂关系及特征提取不足等挑战。图预训练和图提示方法虽有潜力，但现有研究忽略了图提示在模型优化中的潜力，以及正负图提示对模型稳定性与效率的影响。

Method: 提出GPAWP（Graph Prompts with Weight Pruning）框架，将图提示与权重剪枝相结合。通过重要性评估函数确定不同粒度的正负权重，并采用分层结构剪枝来消除负面提示标签，旨在用更少的提示提升性能和效率。

Result: 在三个基准数据集上进行的大量实验证明了GPAWP的优越性，并在节点分类任务中显著减少了参数。

Conclusion: GPAWP框架通过对图提示的有效剪枝，成功提高了GNN模型中图提示的参数效率和性能，为解决GNN面临的挑战提供了一条新途径。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [212] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: 本文提出了POIFormer，一个基于Transformer的框架，通过整合丰富的时空、上下文和行为特征，解决了GPS不准确和POI高密度环境下兴趣点（POI）归因的挑战，实现了准确高效的归因。


<details>
  <summary>Details</summary>
Motivation: POI归因是移动分析、个性化服务和城市规划的基础任务。然而，由于GPS精度问题（2-20米误差）和城市中POI的高度空间密集性（如100米内50多个POI），仅依赖距离不足以准确判断实际访问的POI，这使得该任务充满挑战。

Method: 本文引入了POIFormer，一个新型的基于Transformer的POI归因框架。它利用Transformer的自注意力机制，联合建模了丰富的信号，包括空间邻近度、访问时间与时长、POI语义（上下文特征）、用户移动行为以及聚合的群体行为模式（通过预计算的KDEs）。POIFormer通过掩码当前访问来利用用户的过去和未来访问，并整合群体层面的行为模式，以实现准确高效的归因。

Result: 在真实世界移动数据集上的广泛实验表明，POIFormer显著优于现有基线，特别是在空间噪声和POI密集聚类等挑战性实际场景中表现突出，证实了其在大规模、嘈杂移动数据集中的准确性和效率。

Conclusion: POIFormer提供了一个实用的POI归因解决方案，其架构支持跨不同数据源和地理环境的泛化，避免了对难以获取数据层的依赖，使其适用于实际部署。它能在大规模、嘈杂的移动数据集中实现准确、高效的归因。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [213] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: 本研究提出MolecBioNet，一个新颖的图基框架，通过整合分子和生物医学知识，对药物-药物相互作用（DDI）进行鲁棒且可解释的预测，解决了现有方法忽视上下文依赖性和机制洞察力的问题。


<details>
  <summary>Details</summary>
Motivation: 药物-药物相互作用（DDIs）是药理学中的关键挑战，常导致不良药物反应，严重影响患者安全。现有图基方法多独立处理药物对，忽略了复杂的上下文依赖性互动，且难以整合生物相互作用网络和分子结构以提供有意义的机制洞察。

Method: MolecBioNet框架将药物对建模为统一实体，整合宏观生物相互作用和微观分子影响。它从生物医学知识图中提取局部子图，并从分子表示中构建分层交互图，利用经典图神经网络学习多尺度表示。为提高准确性和可解释性，引入了两个领域特定池化策略：上下文感知子图池化（CASPool）和注意力引导影响池化（AGIPool），并采用互信息最小化正则化增强嵌入融合的信息多样性。

Result: 实验结果表明，MolecBioNet在DDI预测上优于现有最先进方法。消融研究和嵌入可视化进一步验证了统一药物对建模和多尺度知识整合的优势。

Conclusion: MolecBioNet通过统一药物对建模和多尺度知识整合，提供了一种鲁棒且可解释的DDI预测方法，为理解DDIs提供了全面的视角。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [214] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 本文提出一种基于“跟随领导者”(FTL)在线世界模型的规划方法，以解决持续强化学习(CRL)中的灾难性遗忘问题，并构建了一个名为FTL在线智能体(OA)的新型智能体，实验证明其能持续学习新任务而不遗忘旧技能。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习(CRL)中，智能体在顺序学习多个任务时，面临遗忘之前任务解决方案的挑战，即灾难性遗忘，这是CRL发展的主要障碍。

Method: 作者提出通过在线世界模型进行规划来解决问题。具体方法包括：1) 学习一个“跟随领导者”浅层在线模型以捕捉世界动态。2) 利用模型预测控制(MPC)结合该在线模型进行规划。3) 该在线世界模型通过构建方式对遗忘具有免疫性，并具有理论上的遗憾界限。4) 规划器仅基于最新的在线模型搜索动作，形成了FTL在线智能体(OA)。5) 为评估OA，作者还设计了一个专用的CRL环境——Continual Bench。

Result: 经验结果表明，所提出的OA能够持续学习新任务，同时不遗忘旧技能。它优于基于深度世界模型并结合各种持续学习技术的智能体。

Conclusion: 该研究成功地通过在线世界模型和规划策略解决了持续强化学习中的灾难性遗忘问题，并证明了所提出的FTL在线智能体(OA)在学习效率和遗忘抑制方面的优越性。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [215] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个首创的、可观测扩展的纯AI全球天气预报系统，摆脱了对传统数值天气预报（NWP）的依赖，能在17秒内完成从数据同化到中期预报的全过程，并达到与NWP系统相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的天气预报模型过度依赖计算耗时（数小时）的NWP系统来准备初始条件，限制了预报效率。因此，研究动机是开发一个快速、完全由AI驱动且不依赖NWP的天气预报系统。

Method: 引入了XiChen系统，该系统基于一个预训练的天气预报基础模型。该模型被进一步微调以同时充当观测算子和数据同化模型，从而可扩展地同化常规和原始卫星观测。此外，系统融合了四维变分知识，以确保数据同化和中期预报的准确性。

Result: XiChen的整个预报流程（从数据同化到中期预报）仅需17秒完成。其数据同化和中期预报精度可与现有业务化NWP系统媲美，并能实现超过8.25天的有效预报时效。

Conclusion: XiChen的成功表明了其在实现完全AI驱动、独立于NWP系统的天气预报方面具有巨大潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [216] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: 本研究开发了DeepX-GAN模型，一个知识引导的深度生成模型，用于模拟超出历史记录的“未见”极端气候事件（包括直接命中和险些错过的），并捕捉其空间依赖性。研究发现这些未见极端事件对脆弱地区影响更大，并预测未来气候变暖将扩大并重新分配风险热点，强调需制定空间适应性政策。


<details>
  <summary>Details</summary>
Motivation: 现有气候极端事件记录无法提供完整的风险图景，遗漏了超越历史范围的“未见”极端事件，且忽略空间依赖性低估了同步灾害的风险。因此，需要一个能更好地捕捉罕见极端事件空间结构、模拟统计上合理但历史未见的极端事件的模型，以揭示潜在风险并避免虚假的韧性感知。

Method: 开发了DeepX-GAN (Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial Network) 模型，这是一个知识引导的深度生成模型。该模型旨在更好地捕捉罕见极端事件的空间结构，并具备零样本泛化能力，能够模拟历史经验之外但统计上合理的“未见”极端事件（包括“将军”和“僵局”两种类型）。模型应用于中东和北非(MENA)地区进行分析。

Result: 研究发现，在中东和北非地区，这些“未见”极端事件不成比例地影响着高脆弱性和低社会经济准备度的地区，且不同类型的事件在紧迫性和解读上有所差异。未来变暖可能扩大并重新分配这些“未见”极端事件，在印度-巴基斯坦和中非地区出现新的暴露热点。这揭示了传统灾害规划中的关键盲点。

Conclusion: 传统的灾害规划存在关键盲点，未能充分考虑“未见”的极端事件及其空间依赖性。因此，迫切需要制定具有空间适应性的政策，以预测新兴的风险热点，而非仅仅根据历史模式进行推断。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [217] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出“热启动”模型，通过预测条件化的优化起始噪声分布，大幅加速了扩散模型等迭代生成模型的条件生成过程，仅用极少评估次数便达到传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 迭代生成模型（如扩散和流匹配）虽能生成高质量样本，但其生成过程通常需要数百次函数评估，速度非常缓慢。

Method: 引入“热启动模型”，通过提供一个基于输入上下文预测的有信息先验噪声分布N(mu, sigma)，而非传统的无信息N(0, I)分布，来作为生成过程的起始点。同时，通过一个简单的条件归一化技巧，确保其能兼容任何标准生成模型和采样器。

Result: 在图像修复等任务中，该方法仅用11次函数评估（1次热启动，10次生成）即可达到与1000步DDPM基线相当的竞争性结果，显著减少了生成过程所需遍历的距离。

Conclusion: “热启动模型”通过提供更优的初始噪声分布，有效解决了迭代生成模型速度慢的挑战，实现了高效、高质量的条件生成，并具有良好的兼容性和与其他高效采样技术的组合潜力。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [218] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 提出一个通过频率分析和优先级基选择来提高计算效率的构造型小波神经网络（CWNN）框架。


<details>
  <summary>Details</summary>
Motivation: 现有小波神经网络（WNN）面临构建精确小波基的挑战和高计算成本限制。

Method: 本研究引入构造型WNN，首次分析未知非线性函数的频率特性，并基于主要频率分量选择初始小波。框架包含频率估计器和小波基增加机制，优先处理高能量基，并定义了高维小波的必要时频范围。

Result: 该框架显著提高了计算效率。通过四个实际应用示例（包括静态映射估计、数据集结合、时变映射识别和实时数据非线性依赖捕获），验证了其广泛的适用性和实用性。

Conclusion: 该构造型WNN框架通过智能选择和增加小波基，有效克服了传统WNN的局限，在实现高精度的同时显著提升了计算效率，具有广泛应用前景。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [219] [Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning](https://arxiv.org/abs/2507.09094)
*Xiaoren Xu,Hao Xu,Dongyu Wei,Walid Saad,Mehdi Bennis,Mingzhe Chen*

Main category: cs.NI

TL;DR: 提出一种基于流体天线系统(FAS)的无人机3D定位框架。通过注意力循环多智能体强化学习(AR-MARL)优化协作无人机的轨迹和天线选择，显著降低了移动目标无人机的定位误差。


<details>
  <summary>Details</summary>
Motivation: 针对运动目标无人机的实时3D定位挑战，需要优化协作无人机（主动及被动）的轨迹和天线端口选择，以最小化定位误差。

Method: 开发了FAS赋能的无人机3D定位框架，其中主动无人机通过目标反射向被动无人机发送测量信号，被动无人机测距并将信息传至基站计算位置。将该问题建模为优化问题以最小化定位误差，并提出了一种基于注意力机制的循环多智能体强化学习(AR-MARL)方案。该方案利用RNN整合历史状态-动作对，并通过注意力机制评估其重要性，以提高全局Q函数近似精度及定位准确性。

Result: 仿真结果表明，与VD-MARL方案相比，所提AR-MARL方案平均定位误差降低高达17.5%；与不使用FAS的方法相比，降低高达58.5%。

Conclusion: 所提出的AR-MARL方案在FAS赋能的无人机3D定位中表现出色，能有效提高移动目标无人机的实时定位精度。

Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid
antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In
the proposed framework, a set of controlled UAVs cooperatively estimate the
real-time 3D position of a target UAV. Here, the active UAV transmits a
measurement signal to the passive UAVs via the reflection from the target UAV.
Each passive UAV estimates the distance of the active-target-passive UAV link
and selects an antenna port to share the distance information with the base
station (BS) that calculates the real-time position of the target UAV. As the
target UAV is moving due to its task operation, the controlled UAVs must
optimize their trajectories and select optimal antenna port, aiming to estimate
the real-time position of the target UAV. We formulate this problem as an
optimization problem to minimize the target UAV positioning error via
optimizing the trajectories of all controlled UAVs and antenna port selection
of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement
learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use
the local Q function to determine its trajectory and antenna port while
optimizing the target UAV positioning performance without knowing the
trajectories and antenna port selections of other controlled UAVs. Different
from current MARL methods, the proposed method uses a recurrent neural network
(RNN) that incorporates historical state-action pairs of each controlled UAV,
and an attention mechanism to analyze the importance of these historical
state-action pairs, thus improving the global Q function approximation accuracy
and the target UAV positioning accuracy. Simulation results show that the
proposed AR-MARL scheme can reduce the average positioning error by up to 17.5%
and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.

</details>


### [220] [Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks](https://arxiv.org/abs/2507.09124)
*Syed Danial Ali Shah,Maryam Hafeez,Abdelaziz Salama,Syed Ali Raza Zaidi*

Main category: cs.NI

TL;DR: 本文提出了一个基于O-RAN的CAORA框架，利用强化学习和预测性编排，在共享基础设施上高效融合管理AI和RAN工作负载，为6G系统提供可行方案。


<details>
  <summary>Details</summary>
Motivation: AI-RAN融合的6G愿景缺乏具体的架构框架和智能资源编排策略，以实现在共享基础设施上统一支持AI和RAN工作负载。

Method: 提出基于O-RAN规范的“汇聚式AI和O-RAN架构（CAORA）”框架。在近实时RAN智能控制器（NRT-RIC）中设计定制xApps，通过Y1接口监测RAN KPI并将无线电分析数据暴露给端到端（E2E）编排器。该编排器集成了工作负载预测和异常检测模块，并增强了一个Soft Actor-Critic（SAC）强化学习智能体，用于主动管理资源分配，包括多实例GPU（MIG）分区。

Result: 基于巴塞罗那真实5G流量轨迹的仿真结果表明，CAORA在高度动态条件下实现了近99%的RAN需求满足率，支持动态AI工作负载，并最大化了基础设施利用率。

Conclusion: 研究结果表明，预测性编排显著提高了系统适应性、资源效率和服务连续性，为未来AI与RAN融合的6G系统提供了可行的蓝图。

Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims
to unlock a unified 6G platform capable of seamlessly supporting AI and RAN
workloads over shared infrastructure. However, the architectural framework and
intelligent resource orchestration strategies necessary to realize this vision
remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN
Architectural (CAORA) framework based on O-RAN specifications, enabling the
dynamic coexistence of real-time RAN and computationally intensive AI
workloads. We design custom xApps within the Near-Real-Time RAN Intelligent
Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an
End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The
orchestrator incorporates workload forecasting and anomaly detection modules,
augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that
proactively manages resource allocation, including Multi-Instance GPU (MIG)
partitioning. Using real-world 5G traffic traces from Barcelona, our
trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment
of RAN demands, supports dynamic AI workloads, and maximizes infrastructure
utilization even under highly dynamic conditions. Our results reveal that
predictive orchestration significantly improves system adaptability, resource
efficiency, and service continuity, offering a viable blueprint for future
AI-and-RAN converged 6G systems.

</details>


### [221] [On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response](https://arxiv.org/abs/2507.09153)
*Bilal Karaman,Ilhan Baştürk,Ferdi Kara,Engin Zeydan,Esra Aycan Beyazıt,Sezai Taşkın,Emil Björnson,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 本文提出一种基于高空平台站（HAPS）的需求驱动型通信系统，旨在自然灾害导致地面网络失效时，快速恢复通信能力，提升应急响应和灾害管理效率。


<details>
  <summary>Details</summary>
Motivation: 自然灾害常导致通信网络中断，严重阻碍应急响应和灾害管理。现有解决方案（如便携式通信单元和云基网络架构）在无线接入网（RAN）和回传基础设施均失效时，无法有效恢复通信。

Method: 本文提出一种由HAPS支持的需求驱动型通信系统，以恢复受灾区域的通信。该系统提供快速部署、弹性的通信基础设施，不仅能确保移动用户连接，还能在地面网络故障时恢复回传链路，并作为灾害管理中心与受灾区域之间的桥梁，促进实时信息交换和数据收集。

Result: 仿真结果表明，结合混合光/太赫兹链路的HAPS即使在恶劣条件下也能显著提升回传容量和弹性。HAPS支持的S波段和Ka波段RAN能为第一响应者和受灾民众提供可靠通信。

Conclusion: HAPS在紧急通信框架和标准中具有整合潜力，能够提高网络弹性并有效支持灾害管理，在确保受灾区域连接和增强灾害响应能力方面发挥关键作用。

Abstract: Natural disasters often disrupt communication networks and severely hamper
emergency response and disaster management. Existing solutions, such as
portable communication units and cloud-based network architectures, have
improved disaster resilience but fall short if both the Radio Access Network
(RAN) and backhaul infrastructure become inoperable. To address these
challenges, we propose a demand-driven communication system supported by High
Altitude Platform Stations (HAPS) to restore communication in an affected area
and enable effective disaster relief. The proposed emergency response network
is a promising solution as it provides a rapidly deployable, resilient
communications infrastructure. The proposed HAPS-based communication can play a
crucial role not only in ensuring connectivity for mobile users but also in
restoring backhaul connections when terrestrial networks fail. As a bridge
between the disaster management center and the affected areas, it can
facilitate the exchange of information in real time, collect data from the
affected regions, and relay crucial updates to emergency responders. Enhancing
situational awareness, coordination between relief agencies, and ensuring
efficient resource allocation can significantly strengthen disaster response
capabilities. In this paper, simulations show that HAPS with hybrid optical/THz
links boosts backhaul capacity and resilience, even in harsh conditions.
HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first
responders and disaster-affected populations. This paper also explores the
integration of HAPS into emergency communication frameworks and standards, as
it has the potential to improve network resilience and support effective
disaster management.

</details>


### [222] [Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications](https://arxiv.org/abs/2507.09270)
*Songhan Zhao,Yusi Long,Lanhua Li,Bo Gu,Shimin Gong,Zehui Xiong*

Main category: cs.NI

TL;DR: 在语义感知RIS-NOMA网络中，通过联合优化用户语义控制、RIS无源波束赋形和解码顺序，实现能耗最小化并满足用户需求，实验证明该方案显著提升NOMA传输能效。


<details>
  <summary>Details</summary>
Motivation: 针对语义感知RIS辅助的NOMA网络，旨在通过用户流量需求重塑和RIS信道重构，在满足各用户流量需求的前提下，最小化系统总能耗，以提高接入点（AP）解码叠加信号的灵活性。

Method: 提出一个联合优化问题，涵盖语义用户（SU）的解码顺序、语义控制和RIS的无源波束赋形策略。由于该问题具有复杂的耦合性，将其分解为两个子问题，并采用一系列近似方法进行求解。

Result: 数值结果表明，所提出的联合流量整形和信道重构方案与基准方法相比，显著提高了NOMA传输的节能性能。

Conclusion: 本研究成功通过联合优化用户语义控制和RIS无源波束赋形，有效解决了语义感知RIS-NOMA网络中的能耗最小化问题，并显著提升了系统的能量效率。

Abstract: In this paper, we consider a semantic-aware reconfigurable intelligent
surface (RIS)-assisted wireless network, where multiple semantic users (SUs)
simultaneously transmit semantic information to an access point (AP) by using
the non-orthogonal multiple access (NOMA) method. The SUs can reshape their
traffic demands by modifying the semantic extraction factor, while the RIS can
reconfigure the channel conditions via the passive beamforming. This provides
the AP with greater flexibility to decode the superimposed signals from the
SUs. We aim to minimize the system's overall energy consumption, while ensuring
that each SU's traffic demand is satisfied. Hence, we formulate a joint
optimization problem of the SUs' decoding order and semantic control, as well
as the RIS's passive beamforming strategy. This problem is intractable due to
the complicated coupling in constraints. To solve this, we decompose the
original problem into two subproblems and solve them by using a series of
approximate methods. Numerical results show that the joint traffic reshaping
and channel reconfiguration scheme significantly improves the energy saving
performance of the NOMA transmissions compared to the benchmark methods.

</details>


### [223] [Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks](https://arxiv.org/abs/2507.09341)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 本文旨在解决车载移动边缘计算(VEC)中任务卸载截止期限问题，通过在线模式下应用深度强化学习(DQN、PPO)和PSO，以最小化任务丢弃和端到端延迟。结果显示DQN在执行时间、任务丢弃率和延迟方面显著优于动态PSO。


<details>
  <summary>Details</summary>
Motivation: 车载移动边缘计算(VEC)是未来低延迟、高效率数据处理的关键，推动了自动驾驶、智能交通等领域的发展。然而，由于车辆在路边单元(RSU)覆盖范围内的停留时间短暂，VEC面临着严格的任务卸载截止期限挑战。

Method: 研究首先利用粒子群优化(PSO)在静态环境下建立理论性能上限。为应对动态场景，进一步在线部署了PSO、深度Q网络(DQN)和近端策略优化(PPO)模型，旨在最小化任务丢弃和端到端(E2E)延迟（包括通信和计算延迟）。

Result: 实验结果表明，DQN模型显著优于动态PSO方法。具体表现为执行时间减少99.2%，任务丢弃率相对动态PSO降低2.5%，端到端延迟降低18.6%。

Conclusion: 研究结果强调了深度强化学习(DRL)在实现VEC系统可扩展且高效任务管理方面的有效性。

Abstract: Vehicular Mobile Edge Computing (VEC) drives the future by enabling
low-latency, high-efficiency data processing at the very edge of vehicular
networks. This drives innovation in key areas such as autonomous driving,
intelligent transportation systems, and real-time analytics. Despite its
potential, VEC faces significant challenges, particularly in adhering to strict
task offloading deadlines, as vehicles remain within the coverage area of
Roadside Units (RSUs) for only brief periods. To tackle this challenge, this
paper evaluates the performance boundaries of task processing by initially
establishing a theoretical limit using Particle Swarm Optimization (PSO) in a
static environment. To address more dynamic and practical scenarios, PSO, Deep
Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented
in an online setting. The objective is to minimize dropped tasks and reduce
end-to-end (E2E) latency, covering both communication and computation delays.
Experimental results demonstrate that the DQN model considerably surpasses the
dynamic PSO approach, achieving a 99.2% reduction in execution time.
Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to
dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the
effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and
efficient task management for VEC systems.

</details>


### [224] [Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks](https://arxiv.org/abs/2507.09346)
*Arild Yonkeu,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 本文提出一种基于Pointer Network的任务调度方案，解决了移动边缘计算（MEC）中动态、时间敏感环境下的高效任务调度难题，显著降低推理时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 在MEC中，高效的任务调度对于满足低延迟需求至关重要，但现有方法存在局限性。传统方法（如启发式算法、混合整数规划）计算开销大，不适用于实时场景。现有深度学习方法则缺乏可扩展性和对动态工作负载的适应性。

Method: 提出一种基于Pointer Network的架构，用于动态边缘计算场景下的任务调度。该模型通过遗传算法生成的合成数据集进行训练，以确定最优任务排序。

Result: 实验结果表明，该模型相较于基线方法，实现了更低的任务丢弃率和等待时间，软序列准确率高达89.2%。在所有评估的任务数量下，模型的推理时间均保持在2秒以内，远低于整数规划（约18秒）和二进制规划（约90秒）方法。

Conclusion: 该模型为边缘任务管理提供了一个可扩展、高效且适应性强的解决方案，展现出强大的泛化能力和对实时变化的适应性。

Abstract: Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for
meeting the low-latency demands of modern IoT and dynamic task scheduling
scenarios. MEC reduces the processing burden on resource-constrained devices by
enabling task execution at nearby edge servers. However, efficient task
scheduling remains a challenge in dynamic, time-sensitive environments.
Conventional methods -- such as heuristic algorithms and mixed-integer
programming -- suffer from high computational overhead, limiting their
real-time applicability. Existing deep learning (DL) approaches offer faster
inference but often lack scalability and adaptability to dynamic workloads. To
address these issues, we propose a Pointer Network-based architecture for task
scheduling in dynamic edge computing scenarios. Our model is trained on a
generated synthetic dataset using genetic algorithms to determine the optimal
task ordering. Experimental results show that our model achieves lower drop
ratios and waiting times than baseline methods, and a soft sequence accuracy of
up to 89.2%. Our model consistently achieves inference times under 2 seconds
across all evaluated task counts, whereas the integer and binary programming
approaches require approximately up to 18 seconds and 90 seconds, respectively.
It also shows strong generalization across varying scenarios, and adaptability
to real-time changes, offering a scalable and efficient solution for edge-based
task management.

</details>


### [225] [Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling](https://arxiv.org/abs/2507.09352)
*Ghazal Asemian,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 针对移动边缘计算（MEC）中动态任务和干扰（如阻塞）带来的挑战，本文提出了一个考虑传输分集的动态MEC框架，以联合优化任务调度和资源分配，显著降低了任务丢弃率并提升了资源利用率。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）虽能通过计算靠近用户来降低延迟，但动态任务到达和如干扰等通信威胁，使得可靠的任务卸载和资源分配变得复杂。

Method: 本文提出了一个动态MEC框架，考虑传输分集，在存在干扰的情况下联合处理任务调度和资源块（RB）分配。该框架定义并评估了关键网络指标（如丢弃任务率、带宽利用率），同时通过考虑边缘服务器对现有任务的承诺来保持服务连续性。该方案利用传输分集和分布式gNBs间的优化调度，并与无传输分集场景、先来先服务（FCFS）和最短任务优先（STF）等基线策略进行了比较。

Result: 提出的算法有效缓解了干扰影响，增强了资源利用率，并最大程度地降低了任务丢弃率。在信噪比（SJNR）为4 dB时，所提方法的任务丢弃率为0.26，优于无传输分集场景（0.50）、STF（0.52）和FCFS（0.63）。

Conclusion: 该提出的算法高度适用于任务关键型MEC应用，因为它能在存在干扰的情况下有效降低任务丢弃率并提高资源利用率。

Abstract: Mobile Edge Computing (MEC) enables low-latency applications by bringing
computation closer to the user, but dynamic task arrivals and communication
threats like jamming complicate reliable task offloading and resource
allocation. In this paper, we formulate a dynamic MEC framework considering the
transmission diversity that jointly addresses task scheduling and resource
block (RB) assignment in the presence of jamming. First, we define and evaluate
key network metrics-including dropped task ratio and bandwidth
utilization-while maintaining service continuity by accounting for the existing
commitments of the edge server to previously offloaded tasks. Then, we propose
a jamming-aware offloading and RB allocation framework that leverages
transmission diversity and optimal scheduling across distributed gNBs. The
proposed solution is compared to a similar scenario without transmission
diversity and two baseline strategies of first-come-first-served (FCFS) and
shortest task first (STF). The proposed algorithm effectively mitigates the
impact of jamming while enhancing resource utilization and minimizing task drop
rates, making it highly suitable for mission-critical MEC applications. At
signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves
a $0.26$ task drop rate, outperforming the scenario without transmission
diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52
and 0.63 task drop rates, respectively.

</details>


### [226] [MobiWorld: World Models for Mobile Wireless Network](https://arxiv.org/abs/2507.09462)
*Haoye Chai,Yuan Yuan,Yong Li*

Main category: cs.NI

TL;DR: 本文提出MobiWorld，一个基于扩散模型的生成式世界模型，旨在为移动网络规划和优化提供高保真、灵活的仿真环境。它能整合异构多模态数据，生成多层次网络指标，并支持可控仿真以优化网络策略，在节能场景中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 移动网络的精确建模和仿真对实现智能且经济高效的网络优化至关重要。传统预测模型泛化能力有限，难以满足复杂的网络优化需求。

Method: MobiWorld是一个基于先进扩散模型的生成式世界模型。它通过整合传感器、移动设备、基站等异构数据源以及序列和图像等多模态数据，实现强大的普适性。该模型能够生成网络元素级观测（如流量负载、用户分布）和系统级性能指标（如吞吐量、能耗）。其核心是通过建模移动网络数据与时空背景、用户行为、优化策略等条件因素的联合分布，实现强大的可控生成能力。

Result: 研究在协作式节能场景中验证了MobiWorld的有效性，优化代理利用MobiWorld生成的观测和奖励来优化基站休眠和用户分流策略。实验结果表明，MobiWorld展示了强大的可控生成性能，并在能量优化方面超越了传统方法。

Conclusion: MobiWorld提供了一种高保真、灵活的移动网络仿真方案，通过为优化代理提供精确的环境反馈，促进了有效的决策制定，避免了昂贵的真实网络交互，为智能网络优化提供了新的途径。

Abstract: Accurate modeling and simulation of mobile networks are essential for
enabling intelligent and cost-effective network optimization. In this paper, we
propose MobiWorld, a generative world model designed to support high-fidelity
and flexible environment simulation for mobile network planning and
optimization. Unlike traditional predictive models constrained by limited
generalization capabilities, MobiWorld exhibits strong universality by
integrating heterogeneous data sources, including sensors, mobile devices, and
base stations, as well as multimodal data types such as sequences and images.
It is capable of generating both network element-level observations (e.g.,
traffic load, user distribution) and system-level performance indicators (e.g.,
throughput, energy consumption) to support a wide range of planning and
optimization tasks. Built upon advanced diffusion models, MobiWorld offers
powerful controllable generation capabilities by modeling the joint
distribution between mobile network data and diverse conditional factors
including spatio temporal contexts, user behaviors, and optimization policies.
This enables accurate simulation of dynamic network states under varying policy
configurations, providing optimization agents with precise environmental
feedback and facilitating effective decision-making without relying on costly
real-network interactions. We demonstrate the effectiveness of MobiWorld in a
collaborative energy-saving scenario, where an agent uses observations and
rewards generated by MobiWorld to optimize base station sleep and user
offloading policies. Experimental results show that MobiWorld exhibits strong
controllable generation performance and outperforms traditional methods in
energy optimization.

</details>


### [227] [Wi-Fi: Twenty-Five Years and Counting](https://arxiv.org/abs/2507.09613)
*Giovanni Geraci,Francesca Meneghello,Francesc Wilhelmi,David Lopez-Perez,Iñaki Val,Lorenzo Galati Giordano,Carlos Cordeiro,Monisha Ghosh,Edward Knightly,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文对Wi-Fi从IEEE 802.11b (Wi-Fi 1)到802.11bn (Wi-Fi 8)的25年技术演进进行了全面而深入的教程式回顾，重点关注其关键机制而非按代划分。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi技术在25年间取得了巨大发展，新增了许多早期规划中未有的功能。目前缺乏一篇全面涵盖Wi-Fi八个世代技术和历史演进的教程，因此有必要提供一个整体而深入的分析。

Method: 文章通过描述推动Wi-Fi发展的七个关键机制来展开，而非逐代阐述。这些机制包括：频谱分配与标准化周期、物理层（PHY）增强、介质访问控制（MAC）改进、多用户访问的引入、节能机制、多频段（2.4/5/6 GHz）频谱聚合以及接入点协作。附录还讨论了802.11bn之外的未来发展。

Result: 作为一篇教程，本文的结果是提供了一个对Wi-Fi技术演进的全面、系统性的理解框架，展示了其在数据速率（提升超千倍）、多用户支持、能效、频谱利用及AP协作方面的显著进步，并描绘了未来发展方向。它揭示了Wi-Fi如何突破“一次一用户”的范式。

Conclusion: 本文成功提供了横跨Wi-Fi八个世代的全面技术与历史指南，清晰阐述了其核心演进机制，为理解Wi-Fi如何从最初版本发展成为具有多用户、多频段聚合和高级节能功能的成熟技术奠定了基础，并展望了包括AI/ML在内的未来发展趋势。

Abstract: Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding
name, today's Wi-Fi boasts entirely new capabilities that were not even on the
roadmap 25 years ago. This article aims to provide a holistic and comprehensive
technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi
1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial
article to span these eight generations. Rather than a generation-by-generation
exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin
by discussing spectrum allocation and coexistence, and detailing the IEEE
802.11 standardization cycle. Second, we provide an overview of the physical
layer and describe key elements that have enabled data rates to increase by
over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been
enhanced from the original Distributed Coordination Function to now include
capabilities spanning from frame aggregation to wideband spectrum access.
Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and
introduced multi-user access. Fifth, given the increasing use of mobile,
battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the
generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate
spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,
reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access
Points to coordinate in order to improve performance and efficiency. In the
Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including
integrated mmWave operations, sensing, security and privacy extensions, and the
adoption of AI/ML.

</details>


### [228] [Towards Robust RTC in Sparse LEO Constellations](https://arxiv.org/abs/2507.09798)
*Aashish Gottipati,Lili Qiu*

Main category: cs.NI

TL;DR: 本文提出一种数据驱动的队列管理机制，通过动态调整发送端队列容量以适应LEO网络的切换活动，显著提升了稀疏低地球轨道（LEO）星座中WebRTC实时视频通信的性能。


<details>
  <summary>Details</summary>
Motivation: Google拥塞控制（GCC）在LEO网络中表现不佳；稀疏LEO星座为陆地基础设施有限区域的实时通信（RTC）提供机遇；现有WebRTC“一刀切”的发送端队列管理加剧了LEO高延迟和网络不稳定性导致的视频质量下降。

Method: 引入一种数据驱动的队列管理机制，根据预测的切换活动调整最大速率队列容量。具体而言，在稳定无切换阶段采用较短队列限制以降低延迟，而在切换活动增加时预先增大队列容量以吸收中断。

Result: 与默认WebRTC相比，视频比特率提升高达3倍，冻结率降低62%。

Conclusion: 所提出的数据驱动队列管理机制有效解决了稀疏LEO网络中实时通信的性能问题，显著提升了视频通信质量。

Abstract: Google's congestion control (GCC) has become a cornerstone for real-time
video and audio communication, yet its performance remains fragile in emerging
Low Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer
longer duration links and reduced handover frequency compared to dense
deployments, presenting a unique opportunity for high-quality real-time
communication (RTC) in environments with limited terrestrial network
infrastructure. In this paper, we study the behavior of videoconferencing
systems in sparse LEO constellations. We observe that video quality degrades
due to inherent delays and network instability introduced by the high altitude
and rapid movement of LEO satellites, with these effects exacerbated by
WebRTC's conventional ``one-size-fits-all'' sender-side pacing queue
management. To boost RTC performance, we introduce a data-driven queue
management mechanism that adapts the maximum pacing queue capacity based on
predicted handover activity. Specifically, our approach employs shorter queue
limits during stable, no-handover phases to prioritize low latency
communication, and preemptively increases pacing queue capacity when entering
periods of increased handover activity to absorb disruptions. Our method yields
up to $3$x improvements in video bitrate and reduces freeze rate by $62\%$
compared to default WebRTC.

</details>


### [229] [UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks](https://arxiv.org/abs/2507.09852)
*Zihao Zhou,Zipeng Dai,Linyi Huang,Cui Yang,Youjun Xiang,Jie Tang,Kai-kit Wong*

Main category: cs.NI

TL;DR: UavNetSim-v1是一个开源Python模拟平台，旨在实现无人机网络协议和算法的快速开发与评估。


<details>
  <summary>Details</summary>
Motivation: 无人机网络中的通信协议和算法对UAV间的协作至关重要；模拟是经济高效的解决方案，能避免现场实验的高昂开销，用于协议和算法的原型设计、调试和分析。

Method: 提出了一个名为“UavNetSim-v1”的开源Python模拟平台，该平台提供了路由/MAC协议、拓扑控制算法、移动性/能量模型等多种功能，并支持全面的性能评估和交互式可视化界面。

Result: UavNetSim-v1提供了开发者所需的大部分功能，同时保持易用性，并支持全面的性能评估和深入算法分析的交互式可视化。它是一个轻量级但功能强大的替代方案。

Conclusion: UavNetSim-v1既适用于快速原型开发，也适用于教育目的，可作为无人机通信研究中成熟网络模拟器的一种轻量级且功能强大的替代品。

Abstract: In unmanned aerial vehicle (UAV) networks, communication protocols and
algorithms are essential for cooperation and collaboration between UAVs.
Simulation provides a cost-effective solution for prototyping, debugging, and
analyzing protocols and algorithms, avoiding the prohibitive expenses of field
experiments. In this paper, we present ``UavNetSim-v1'', an open-source
Python-based simulation platform designed for rapid development, testing, and
evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''
provides most of the functionalities developers may need, including
routing/medium access control (MAC) protocols, topology control algorithms and
mobility/energy models, while maintaining ease of use. Furthermore, the
platform supports comprehensive performance evaluation and features an
interactive visualization interface for in-depth algorithm analysis. In short,
``UavNetSim-v1'' lends itself to both rapid prototyping and educational
purposes, and can serve as a lightweight yet powerful alternative to mature
network simulators for UAV communication research.

</details>


### [230] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: 优化大型语言模型(LLM)推理工作负载在异构边缘数据中心的分配，旨在降低能耗、碳排放和用水量，同时提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 鉴于边缘数据中心拥有本地可再生能源，并面临动态电价及可再生能源的时空波动性，如何将LLM推理工作负载进行最优分配，以最小化能耗、碳排放和用水量，并提升用户体验，是亟待解决的问题。

Method: 提出了一种新颖的优化模型，旨在帮助LLM服务提供商降低运营成本和环境影响。

Result: 数值结果验证了所提出方法的有效性。

Conclusion: 所提出的优化模型能有效实现LLM推理工作负载的优化分配，从而显著降低运营成本和环境影响。

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [231] [Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit](https://arxiv.org/abs/2507.10210)
*Thijs Havinga,Xianjun Jiao,Wei Liu,Baiheng Chen,Robbe Gaeremynck,Ingrid Moerman*

Main category: cs.NI

TL;DR: 本文演示了使用光纤回传实现Wi-Fi 6兼容的细粒度Co-OFDMA，并展示了其在同步性能和接收信号质量方面的显著提升。


<details>
  <summary>Details</summary>
Motivation: 在密集部署的无线网络中，传统竞争机制导致高延迟。尽管Wi-Fi 8提出了Co-OFDMA以更高效地共享频谱，但其细粒度资源分配因空中调度开销和物理层复杂性被认为不切实际。本研究旨在通过有线回传解决这些问题，并实现先进的多AP协调。

Method: 利用光纤回传、开源平台openwifi和White Rabbit，实现了Wi-Fi 6兼容的细粒度Co-OFDMA，并进行了双AP间的性能测试。

Result: 在载波频率偏移预补偿和时间同步方面，双AP间的性能超越了相关无线标准要求。此外，Co-OFDMA帧的接收星座图质量优于单个AP发送的帧。

Conclusion: 通过光纤回传，可以成功实现Wi-Fi 6兼容的细粒度Co-OFDMA，有效提升了密集无线网络中的协调性能和频谱效率。

Abstract: Proper coordination is needed to guarantee the performance of wireless
networks in dense deployments. Contention-based systems suffer badly in terms
of latency when multiple devices compete for the same resources. Coordinated
Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi
8 to remedy this, as it enables multiple Access Points (APs) to share spectrum
more efficiently. However, fine-grained resource allocation, namely within
20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling
overhead and complexity in terms of physical layer signaling. A wired backhaul
mitigates the need for over-the-air scheduling and synchronization, and it
allows for coordination even if APs are not in each others' range. Furthermore,
it forms the basis for more advanced multi-AP coordination schemes like
coordinated beamforming and joint transmission. In this work we demonstrate the
realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,
enabled by the open-source platforms openwifi and White Rabbit. We show that
the performance in terms of carrier frequency offset pre-compensation and time
synchronization between two APs exceeds related wireless standard requirements.
Furthermore, the quality of the received constellation of the Co-OFDMA frame as
reported by a wireless connectivity tester is better than individual frames
sent by the APs.

</details>


### [232] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: 针对AI视频聊天中因MLLM推理和网络传输导致的延迟问题，本文提出了Artic实时通信框架，通过上下文感知视频流和抗丢包自适应帧率技术优化视频传输效率，并构建了首个用于评估MLLM视频理解性能的基准DeViBench。


<details>
  <summary>Details</summary>
Motivation: AI视频聊天旨在实现更直观、如真人般的AI-人交互，但面临两大挑战：一是多模态大语言模型（MLLM）推理耗时过长，二是网络不确定性导致的传输延迟，两者共同造成高延迟，严重阻碍了AI的实时、流畅互动体验。

Method: 1. 提出AI导向的Artic实时通信框架，将网络需求从“人观看视频”转变为“AI理解视频”。2. 引入上下文感知视频流技术，根据视频区域对聊天的重要性智能分配比特率，以降低带宽并保持MLLM准确性。3. 设计抗丢包自适应帧率机制，利用先前帧替代丢失/延迟帧，避免重传和比特率浪费。4. 构建首个退化视频理解基准（DeViBench），用于评估视频流质量对MLLM准确性的影响。

Result: 本文成功提出并实现了Artic框架及其优化技术，显著降低了AI视频聊天的比特率，同时维持了MLLM的准确性，有效避免了丢包重传和比特率浪费。此外，还成功构建了DeViBench，为评估AI视频聊天中视频流质量对MLLM性能的影响提供了标准工具。

Conclusion: Artic框架通过创新的视频流和帧率管理技术，有效解决了AI视频聊天中的高延迟和带宽效率问题，推动了AI与人类之间更自然、实时的交互。DeViBench的建立则为相关研究提供了重要的评估基准，为未来AI视频聊天技术的发展奠定了基础。

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [233] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 为解决XL-MIMO系统中信道估计的复杂性和可扩展性问题，本文提出一种轻量级深度学习框架，利用基于分块的训练机制，显著提高估计精度并降低计算复杂度，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 6G等下一代无线技术（如XL-MIMO和RIS）需要精确的信道状态信息（CSI）。尽管深度学习在信道估计方面取得进展，但现有模型在XL-MIMO系统中存在可扩展性和部署限制，随着天线和RIS单元数量的增加，导致数据量、计算复杂度和能耗急剧上升，难以实现实时高效的信道估计。

Method: 本文提出一个轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计。该框架旨在最小化计算复杂度，并适用于资源受限的边缘设备。通过利用信道的空间相关性，引入了一种基于分块（patch-based）的训练机制，将输入维度降低到分块级表示，同时保留关键信息，从而实现大规模系统的可伸缩训练。

Result: 在多种条件下进行的仿真结果表明，本文提出的框架显著提高了信道估计精度，并降低了计算复杂度，且其性能不受XL-MIMO系统中天线和RIS单元数量增加的影响。

Conclusion: 所提出的轻量级深度学习框架有效解决了XL-MIMO系统中大规模信道估计所面临的计算复杂度和可扩展性挑战，实现了高效、高精度的信道估计，使其成为下一代无线技术实际部署的可行方案。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [234] [Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations](https://arxiv.org/abs/2507.09719)
*Jiaheng Xiong,Qiaolun Zhang,Yoann Piétri,Raja Yehia,Raouf Boutaba,Francesco Musumeci,Massimo Tornatore*

Main category: quant-ph

TL;DR: 分析量子密钥分发（QKD）网络的功耗。


<details>
  <summary>Details</summary>
Motivation: 评估和优化不同协议、探测器配置及网络拓扑下QKD网络的能耗表现。

Method: 在真实的网络拓扑下，比较离散变量（DV）与连续变量（CV）QKD，优化设备放置，并量化SNSPD与APD探测器的功耗权衡以及光旁路的好处。

Result: 量化并评估了不同QKD协议、探测器类型和网络优化（如设备放置和光旁路）对QKD网络功耗的影响和权衡。

Conclusion: 该研究为理解和优化不同配置下QKD网络的能耗提供了深入见解。

Abstract: We analyze the power consumption of quantum key distribution (QKD) networks
under various protocol and detector configurations. Using realistic network
topologies, we evaluate discrete-variable vs continuous-variable QKD and
optimize device placement, quantifying power trade-offs of SNSPD vs APD
detectors and the benefits of optical bypass.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [235] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: 为解决LLM分布式GPU推理中现有调度系统缺乏时空感知的问题，本文提出TORTA框架，通过两层调度设计优化资源利用率和响应时间。


<details>
  <summary>Details</summary>
Motivation: LLM服务对分布式GPU推理基础设施需求日益增长。现有调度系统仅基于瞬时状态决策，忽视任务需求和资源随时间演变，导致GPU利用率低、任务迁移开销大、动态负载下响应性差。

Method: 提出TORTA（Temporal Optimal Resource scheduling via Two-layer Architecture）框架，引入时空调度，捕获长期工作负载模式和短期执行约束。采用两层设计：宏观调度器利用强化学习和最优传输协调跨区域任务分配；微观分配器在区域内优化任务到服务器的分配，以减少延迟和切换成本。

Result: 相比现有基线方法，TORTA在多网络拓扑下：平均推理响应时间减少高达15%；负载均衡提升约4-5%；总运营成本降低10-20%。

Conclusion: TORTA通过引入时空感知和分层调度架构，有效克服了LLM分布式GPU推理中现有调度系统的局限性，显著提升了系统性能、负载均衡和成本效率。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [236] [Advancing network resilience theories with symbolized reinforcement learning](https://arxiv.org/abs/2507.08827)
*Yu Zheng,Jingtao Ding,Depeng Jin,Jianxi Gao,Yong Li*

Main category: physics.soc-ph

TL;DR: 本文提出一种AI驱动的自归纳方法，用于发现复杂网络的韧性理论。该方法首次发现了一个同时考虑拓扑和动力学的韧性理论，并显著提升了现有理论的准确性，为理解和预测系统性崩溃提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 复杂网络在多数情况下表现出韧性，但移除少数关键节点即可导致系统迅速崩溃。现有韧性理论仅从拓扑角度研究问题，忽略了系统动力学的关键作用，而拓扑与动力学之间的复杂耦合超出了人类分析能力。因此，亟需开发新的理论来更全面地衡量网络韧性，以预防灾难性崩溃。

Method: 本研究提出了一种自动韧性理论发现方法。该方法通过学习人工智能如何解决复杂的网络拆解问题，并将其网络攻击策略符号化为理论公式。这是一种自归纳（self-inductive）的方法。

Result: 研究结果包括：1) 发现了第一个同时考虑网络拓扑结构和系统动力学的韧性理论；2) 揭示了节点度与节点状态之间的关联如何影响整体网络韧性；3) 为设计系统性崩溃的早期预警信号提供了见解；4) 发现的公式将现有成熟韧性理论的准确性提高了37.5%以上。

Conclusion: 该AI驱动的方法通过发现结合拓扑和动力学的新韧性理论，并大幅提高了现有韧性理论的准确性，显著提升了人类对复杂网络的理解，为复杂系统的稳定性和预测提供了重要进展。

Abstract: Many complex networks display remarkable resilience under external
perturbations, internal failures and environmental changes, yet they can
swiftly deteriorate into dysfunction upon the removal of a few keystone nodes.
Discovering theories that measure network resilience offers the potential to
prevent catastrophic collapses--from species extinctions to financial
crise--with profound implications for real-world systems. Current resilience
theories address the problem from a single perspective of topology, neglecting
the crucial role of system dynamics, due to the intrinsic complexity of the
coupling between topology and dynamics which exceeds the capabilities of human
analytical methods. Here, we report an automatic method for resilience theory
discovery, which learns from how AI solves a complicated network dismantling
problem and symbolizes its network attack strategies into theoretical formulas.
This proposed self-inductive approach discovers the first resilience theory
that accounts for both topology and dynamics, highlighting how the correlation
between node degree and state shapes overall network resilience, and offering
insights for designing early warning signals of systematic collapses.
Additionally, our approach discovers formulas that refine existing
well-established resilience theories with over 37.5% improvement in accuracy,
significantly advancing human understanding of complex networks with AI.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [237] [Hybrid Quantum Security for IPsec](https://arxiv.org/abs/2507.09288)
*Javier Blanco-Romero,Pedro Otero García,Daniel Sobral-Blanco,Florina Almenares Mendoza,Ana Fernández Vilas,Manuel Fernández-Veiga*

Main category: cs.CR

TL;DR: 本文针对量子密钥分发（QKD）与现有安全协议（如IPsec）的集成挑战，提出并评估了并行混合QKD-PQC和纯QKD两种密钥建立策略。研究表明，并行方法显著优于串行方法，纯QKD可实现最小带宽开销，为关键基础设施提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发（QKD）虽然提供了信息理论安全性以对抗量子计算威胁，但由于预分发量子密钥与计算密钥交换范式的根本不匹配，将其有效集成到IPsec等现有安全协议中仍是一个未解决的挑战。

Method: ['首次系统比较了IPsec中串行与并行混合QKD-PQC密钥建立策略，并揭示了普适的协议设计原则。', '提出了两种将QKD整合到IKEv2的新方法，支持ETSI GS QKD 004有状态和ETSI GS QKD 014无状态API规范：1) 纯QKD方法，通过基于标识符的量子密钥协调取代计算密钥派生；2) 统一QKD-KEM抽象，在现有协议框架内实现量子与后量子密码方法的并行组合。', '使用基于Docker的测试框架和IDQuantique QKD硬件进行性能评估。']

Result: ['并行混合方法消除了RFC 9370规定的串行方法固有的乘法延迟惩罚，在实际网络条件下实现了显著的性能提升。', '在网络延迟条件下，并行混合方法显著优于串行方法。', '纯QKD通过基于标识符的密钥协调实现了最小带宽开销。']

Conclusion: 本研究的实现提供了实用的量子增强IPsec解决方案，适用于需要深度防御安全的关键基础设施部署。

Abstract: Quantum Key Distribution (QKD) offers information-theoretic security against
quantum computing threats, but integrating QKD into existing security protocols
remains an unsolved challenge due to fundamental mismatches between
pre-distributed quantum keys and computational key exchange paradigms. This
paper presents the first systematic comparison of sequential versus parallel
hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental
protocol design principles that extend beyond specific implementations. We
introduce two novel approaches for incorporating QKD into Internet Key Exchange
version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS
QKD 014 stateless API specifications: (1) a pure QKD approach that replaces
computational key derivation with identifier-based quantum key coordination,
and (2) a unified QKD-KEM abstraction that enables parallel composition of
quantum and post-quantum cryptographic methods within existing protocol
frameworks. Our key insight is that parallel hybrid approaches eliminate the
multiplicative latency penalties inherent in sequential methods mandated by RFC
9370, achieving significant performance improvements under realistic network
conditions. Performance evaluation using a Docker-based testing framework with
IDQuantique QKD hardware demonstrates that the parallel hybrid approach
significantly outperforms sequential methods under network latency conditions,
while pure QKD achieves minimal bandwidth overhead through identifier-based key
coordination. Our implementations provide practical quantum-enhanced IPsec
solutions suitable for critical infrastructure deployments requiring
defense-in-depth security.

</details>


### [238] [Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS](https://arxiv.org/abs/2507.09301)
*Julio Gento Suela,Javier Blanco-Romero,Florina Almenares Mendoza,Daniel Díaz-Sánchez*

Main category: cs.CR

TL;DR: 为应对量子计算机对DNSSEC的威胁，本文提出并实现了将后量子密码算法集成到CoreDNS中，使DNSSEC具备量子抗性，并评估了其性能权衡，发现部分算法是可行的过渡方案。


<details>
  <summary>Details</summary>
Motivation: 当前依赖RSA和ECDSA算法的DNSSEC等安全服务易受量子计算机攻击，存在重大安全威胁。

Method: 开发了一个CoreDNS插件，支持ML-DSA、FALCON、SPHINCS+、MAYO和SNOVA五种后量子密码签名算法，以实现量子抗性DNSSEC功能，并确保与现有DNS解析流程兼容，提供即时签名。

Result: 性能评估显示，安全性与效率之间存在显著权衡。后量子密码算法会引入操作开销，但有多种候选算法为DNSSEC向量子抗性密码过渡提供了可行的折衷方案。

Conclusion: 尽管引入后量子密码算法会增加性能开销，但通过选择合适的算法，将DNSSEC过渡到量子抗性加密是可行的。

Abstract: The emergence of quantum computers poses a significant threat to current
secure service, application and/or protocol implementations that rely on RSA
and ECDSA algorithms, for instance DNSSEC, because public-key cryptography
based on number factorization or discrete logarithm is vulnerable to quantum
attacks. This paper presents the integration of post-quantum cryptographic
(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.
We have developed a plugin that extends CoreDNS with support for five PQC
signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our
implementation maintains compatibility with existing DNS resolution flows while
providing on-the-fly signing using quantum-resistant signatures. A benchmark
has been performed and performance evaluation results reveal significant
trade-offs between security and efficiency. The results indicate that while PQC
algorithms introduce operational overhead, several candidates offer viable
compromises for transitioning DNSSEC to quantum-resistant cryptography.

</details>


### [239] [Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems](https://arxiv.org/abs/2507.09859)
*Guntur Dharma Putra,Bagus Rakadyanto Oktavianto Putra*

Main category: cs.CR

TL;DR: 本文提出了一个基于区块链的自 S-S I 框架，旨在解决物联网中凭证颁发受限的问题，允许任何具有可验证信任连接的个体作为凭证颁发者，并通过概念验证证明了其可行性和低开销。


<details>
  <summary>Details</summary>
Motivation: 现有物联网中的自 S-S I 框架将凭证颁发和撤销限制在少数可信实体（如物联网制造商），这限制了在动态物联网生态系统中的灵活性。

Method: 提出了一种基于区块链的自 S-S I 框架，采用分层架构。该框架允许任何具有可验证信任连接的个体充当凭证颁发者。信任通过基于背书的计算动态建立，并通过分层信任链机制维护。区块链作为可验证数据注册表，智能合约自动化了凭证颁发、验证和撤销等关键流程。

Result: 概念验证实现表明，所提出的框架是可行的，并且与基线相比，开销极小。

Conclusion: 该框架非常适合动态且资源受限的物联网环境。

Abstract: Self-Sovereign Identity (SSI) offers significant potential for managing
identities in the Internet of Things (IoT), enabling decentralized
authentication and credential management without reliance on centralized
entities. However, existing SSI frameworks often limit credential issuance and
revocation to trusted entities, such as IoT manufacturers, which restricts
flexibility in dynamic IoT ecosystems. In this paper, we propose a
blockchain-based SSI framework that allows any individual with a verifiable
trust linkage to act as a credential issuer, ensuring decentralized and
scalable identity management. Our framework incorporates a layered
architecture, where trust is dynamically established through endorsement-based
calculations and maintained via a hierarchical chain-of-trust mechanism.
Blockchain serves as the Verifiable Data Registry, ensuring transparency and
immutability of identity operations, while smart contracts automate critical
processes such as credential issuance, verification, and revocation. A
proof-of-concept implementation demonstrates that the proposed framework is
feasible and incurs minimal overheads compared to the baseline, making it
well-suited for dynamic and resource-constrained IoT environments.

</details>


### [240] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 提出一种基于机器学习的DNS隧道检测新方法。


<details>
  <summary>Details</summary>
Motivation: DNS隧道利用正常流量隐藏恶意行为，对安全构成挑战。传统基于规则或签名的方法不足以准确识别此类隐蔽通信。

Method: 结合机器学习算法，通过分析从DNS流量中提取的特征来检测DNS隧道。

Result: 分析结果表明，所提出的方法能够准确检测DNS隧道。

Conclusion: 所提出的机器学习方法是准确检测DNS隧道攻击的良好候选方案。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>
