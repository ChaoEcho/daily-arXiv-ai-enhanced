<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 68]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NI](#cs.NI) [Total: 7]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

TL;DR: 为解决在线预订平台第三方数据不一致问题，本文将LLM（Mistral 7B和Mixtral 8x7B）集成到CALEIDOHOTELS平台进行评估。Mixtral 8x7B在性能上优于Mistral 7B，但计算成本更高，揭示了模型质量与资源效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 在线房产预订平台依赖的第三方住宿设施信息常不完整或不一致，导致用户体验差并造成市场损失。

Method: 一项工业案例研究，将大型语言模型（LLMs）集成到FERVENTO开发的CALEIDOHOTELS房产预订平台。评估了两种LLM：使用QLoRA微调的Mistral 7B和使用优化系统提示的Mixtral 8x7B。评估标准包括生成描述的一致性、同质性及幻觉最小化。

Result: Mixtral 8x7B在完整性（99.6% vs 93%）、精确度（98.8% vs 96%）和幻觉率（1.2% vs 4%）方面均优于Mistral 7B，生成内容更简洁（平均249词 vs 277词）。然而，其计算成本显著更高（50GB显存/$1.61/小时 vs 5GB显存/$0.16/小时）。

Conclusion: 本研究提供了关于模型质量与资源效率权衡的实用见解，为在生产环境中部署LLM提供了指导，并证明了LLM在提升住宿数据一致性和可靠性方面的有效性。

Abstract: Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [2] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

TL;DR: 针对电力营销客服现有痛点和通用大模型在领域知识上的不足，本文提出了ElectriQ，首个电力营销场景下评估和增强大模型的基准。ElectriQ包含对话数据集、四项评估指标和知识增强方法，实验证明微调后的中小模型在专业性和用户友好性上表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前电力营销客服系统（如95598热线）存在响应慢、流程不灵活和领域准确性不足的问题。通用大语言模型（LLMs）虽功能强大，但在电力营销领域缺乏专业知识和同理心。因此，需要弥补这一差距，开发专为电力营销服务定制的LLMs。

Method: 本文提出了ElectriQ，首个用于评估和增强电力营销场景下LLMs的基准。ElectriQ包含：1. 涵盖六大服务类别的对话数据集；2. 四种评估指标：专业性、受欢迎度、可读性和用户友好性；3. 一个领域特定的知识库；4. 一种知识增强方法，以提升模型性能。研究对13个LLMs进行了实验。

Result: 对13个LLMs进行的实验表明，经过微调和知识增强后，像LLama3-8B这样的小型模型在专业性和用户友好性方面可以超越GPT-4o等通用大型模型。

Conclusion: ElectriQ为开发专门服务于电力营销需求的LLMs奠定了全面的基础，并证明了经过专业化训练和增强的较小模型在特定领域可以超越通用大型模型。

Abstract: Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [3] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

TL;DR: 本文提出一个结合微调语言模型和半监督集成学习的分层分类框架，用于检测和分类深/暗网及社交媒体上的非法市场内容，并取得高精度。


<details>
  <summary>Details</summary>
Motivation: 非法市场活动日益转向深/暗网及Telegram、Reddit等匿名平台，内容检测面临标记数据稀缺、非法语言演变和来源结构异构的挑战。

Method: 构建了一个分层分类框架：首先使用在特定领域数据上微调的ModernBERT提取语义特征，并结合手动工程特征（如文档结构、嵌入模式、元数据）。然后，采用两阶段分类流程：第一阶段利用XGBoost、Random Forest和SVM的半监督集成学习检测销售相关文档；第二阶段将这些文档进一步细分为毒品、武器或凭证销售。

Result: 在多源语料库、DUTA和CoDA三个数据集上的实验表明，该模型优于BERT、ModernBERT、DarkBERT、ALBERT等多个基线模型。模型准确率达0.96489，F1分数0.93467，TMCC达0.95388。

Conclusion: 该模型在有限监督下表现出强大的泛化能力和鲁棒性，有效支持现实世界的非法内容检测。

Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [4] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出一个混合框架，结合机器学习（ML）模型和大型语言模型（LLMs）进行主题分析。该框架利用ML模型指导LLM预测并进行后编辑，以解决LLM的过度生成和幻觉问题，实验结果显示能产生更受控且与词汇表对齐的输出。


<details>
  <summary>Details</summary>
Motivation: 提供主题访问是图书馆管理系统的核心功能。尽管LLMs在分类和摘要任务中应用广泛，但其在主题分析方面的能力尚未充分探索。传统ML模型在主题分析中难以处理未见案例，而LLMs虽能替代但常出现过度生成和幻觉。因此，需要一种方法结合两者的优势并克服各自的缺点。

Method: 本文提出一种混合框架，整合了基于嵌入的机器学习模型和大型语言模型。该方法利用ML模型执行两项任务：1) 预测最佳的国会图书馆主题词表（LCSH）标签数量，以指导LLM的预测；2) 对LLM预测的词汇进行后编辑，使用实际的LCSH词汇，以减轻幻觉。研究通过LLMs和该混合框架对书籍主题词进行预测，使用LCSH进行实验。

Result: 实验结果表明，提供初始预测以指导LLM的生成，并进行后编辑，能够产生更受控且与词汇表对齐的输出。

Conclusion: 混合框架通过结合ML模型对LLM预测进行指导和后编辑，有效地解决了LLM在主题分析中过度生成和幻觉的问题，从而实现了更准确、更受控且与专业词汇表（LCSH）高度对齐的主题词预测。

Abstract: Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [5] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本文提出一种新颖的知识图谱（KG）集成方法，通过结合标签匹配和三元组匹配来解决现有方法在处理复杂多样的KG上下文时的局限性，并在性能上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱实体匹配方法在处理复杂多样的真实世界知识图谱（特别是上下文匹配）时存在局限性，未能充分考虑其来源、大小和信息密度的差异。这导致现有方法在整合需要处理多样和复杂上下文的场景时可能失效，而上下文匹配在本体和实体匹配研究中仍未被充分探索。

Method: 提出了一种新颖的KG集成方法，包括：1) 标签匹配：利用字符串操作、模糊匹配和向量相似性技术对实体和谓词标签进行对齐；2) 三元组匹配：识别传达可比信息的三元组之间的映射，并利用这些映射来提高实体匹配的准确性。此外，还引入了一个从基准数据集衍生而来的新数据集，以更全面地评估三元组匹配步骤。

Result: 所提出的方法在OAEI竞赛中与领先系统以及有监督方法相比表现出竞争力，并在多样化的测试案例中实现了高准确性。同时，引入了一个新的数据集，用于更全面地评估三元组匹配步骤。

Conclusion: 该研究提出的KG集成方法有效弥补了上下文匹配的空白，并在处理多样化和复杂上下文的知识图谱集成方面展现出优异性能和高精度，同时也通过引入新数据集促进了三元组匹配的全面评估。

Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [6] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

TL;DR: 本文对LLMs中的幻觉现象进行了严谨的理论处理，包括定义、风险分析、检测与缓解策略综述，并提出了统一的工作流程和评估方案。


<details>
  <summary>Details</summary>
Motivation: LLMs中的幻觉是生成与输入或事实不符内容的问题，这是一个关键挑战，需要严谨的理论基础和实用的解决方案来解决。

Method: 定义了内在和外在幻觉，并引入了“幻觉风险”；使用PAC-Bayes和Rademacher复杂度推导了风险界限；综述了基于不确定性估计、置信度校准、注意力对齐等检测策略；探讨了检索增强生成、幻觉感知微调、logits校准和事实验证模块等缓解方法；提出了一个统一的检测与缓解工作流程，并推荐了评估协议、数据集和指标。

Result: 区分了内在和外在幻觉，定义了幻觉风险并推导了其理论界限；系统性地梳理了幻觉的检测和缓解策略；提出了一个集成这些策略的统一工作流程；概述了量化和减少幻觉的评估方案。

Conclusion: 这项工作为解决LLMs中的幻觉挑战奠定了理论基础，并提供了实用的指导方针。

Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [7] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

TL;DR: 传统RAG在处理时间序列查询时存在局限，本研究提出一个新RAG框架，通过融入时间逻辑和专用检索器来解决此问题，并在新基准测试上实现了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）在处理需要追踪实体和现象随时间变化的“纵向查询”时表现出严重缺陷，因为其语义驱动的检索方法无法收集到同时具备主题相关性和时间连贯性的证据。

Method: 提出一种重新设计RAG管道以融入时间逻辑的新框架。该方法将用户查询分解为核心主题和时间窗口，并采用一个专门的检索器，校准语义匹配与时间相关性，以确保收集到覆盖整个查询期间的连续证据集。为评估此能力，还引入了“分析性历时问答基准”（ADQAB）。

Result: 在ADQAB基准测试上的实证结果显示，本方法在答案准确性方面取得了显著增益，超越了标准RAG实现13%至27%。

Conclusion: 本工作为RAG系统实现复杂、真实世界问题所需的细致、演化分析提供了一条验证过的途径。

Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [8] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 本研究通过SAE分析Gemma-2不同规模模型（2B和9B）的内部特征，发现它们在中间层具有显著相似性，表明大型语言模型学习到普遍且可解释的特征，为跨模型可解释性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探究Gemma-2语言模型（Gemma-2-2B和Gemma-2-9B）在规模差异四倍的情况下，是否仍能收敛于可比较的内部概念，即验证其特征的普遍性。

Method: 利用稀疏自编码器（SAE）词典学习流程分析各模型残差流的激活，通过激活相关性对单义特征进行对齐，并使用SVCCA和RSA方法比较匹配的特征空间。初步实验还扩展了对多词元子空间的分析。

Result: 研究发现模型特征的重叠度在中间层最强，而早期和晚期层的相似性显著较低。初步实验表明，语义相似的多词元子空间与语言模型交互的方式也相似。

Conclusion: 这些结果强化了大型语言模型尽管规模不同，仍能形成大致相似且可解释的内部特征的观点，从而支持普遍性作为跨模型可解释性的基础。

Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [9] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 本研究开发了基于预注册信息的机器学习模型，用于预测临床试验中严重不良事件（SAE）的发生率和趋势，旨在改进试验设计和风险评估，并通过引入滑动窗口方法提升了长文本处理能力。


<details>
  <summary>Details</summary>
Motivation: 为了改进临床试验设计、避免不必要的试验终止并限制参与者暴露于不必要的风险，研究旨在开发方法，在试验开始前仅利用注册信息准确预测预期安全结果（如严重不良事件，SAE）。

Method: 分析了ClinicalTrials.gov上22,107项双臂平行干预性临床试验数据。开发了两种预测模型：一个分类器用于预测实验组是否具有更高的SAE率（以AUC评估），一个回归模型用于预测对照组的SAE比例（以RMSE评估）。采用迁移学习方法，利用预训练语言模型（如ClinicalT5、BioBERT）进行特征提取，并结合下游模型进行预测。为处理超出语言模型输入限制的长文本，开发了滑动窗口方法进行嵌入提取。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测哪个试验组具有更高SAE患者比例时，AUC达到77.6%。在预测对照组SAE发生比例时，同一模型RMSE为18.6%。滑动窗口方法显著优于无该方法的模型，在12个分类器中平均AUC绝对提升2.00%，在12个回归器中平均RMSE绝对降低1.58%。

Conclusion: ClinicalTrials.gov上可用的试验总结结果数据仍未被充分利用。在试验开始前预估试验结果的潜力巨大，这为改进试验设计以及发现预期与实际报告安全结果之间的差异提供了机会。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [10] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
*Jindong Li,Yali Fu,Jiahong Liu,Linxiao Cao,Wei Ji,Menglin Yang,Irwin King,Ming-Hsuan Yang*

Main category: cs.CL

TL;DR: 该论文对大型语言模型（LLM）中离散令牌化（特别是矢量量化，VQ）技术进行了首次系统性综述，构建了分类体系，分析了现有方法、挑战和未来方向，旨在弥合传统VQ与现代LLM应用之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，将连续多模态数据转换为适合语言处理的离散表示的需求日益增长。矢量量化（VQ）作为一种核心方法，在计算效率和与LLM架构兼容性方面具有优势，但目前缺乏针对LLM系统背景下VQ技术的全面综述。

Method: 本研究通过提出首个针对LLM的离散令牌化方法的结构化分类和分析来填补空白。具体方法包括：分类并分析8种代表性的VQ变体，涵盖经典和现代范式，探讨其算法原理、训练动态及与LLM管道集成的挑战；讨论VQ在经典应用、LLM单模态系统和LLM多模态系统中的现有研究，突出量化策略对对齐、推理和生成性能的影响。

Result: 本研究识别了关键挑战，包括码本崩溃、不稳定的梯度估计和模态特定的编码约束。同时，探讨了新兴的研究方向，如动态和任务自适应量化、统一的令牌化框架以及受生物学启发的码本学习。

Conclusion: 该综述弥合了传统矢量量化与现代LLM应用之间的差距，为开发高效且可泛化的多模态系统提供了基础性参考。

Abstract: The rapid advancement of large language models (LLMs) has intensified the
need for effective mechanisms to transform continuous multimodal data into
discrete representations suitable for language-based processing. Discrete
tokenization, with vector quantization (VQ) as a central approach, offers both
computational efficiency and compatibility with LLM architectures. Despite its
growing importance, there is a lack of a comprehensive survey that
systematically examines VQ techniques in the context of LLM-based systems. This
work fills this gap by presenting the first structured taxonomy and analysis of
discrete tokenization methods designed for LLMs. We categorize 8 representative
VQ variants that span classical and modern paradigms and analyze their
algorithmic principles, training dynamics, and integration challenges with LLM
pipelines. Beyond algorithm-level investigation, we discuss existing research
in terms of classical applications without LLMs, LLM-based single-modality
systems, and LLM-based multimodal systems, highlighting how quantization
strategies influence alignment, reasoning, and generation performance. In
addition, we identify key challenges including codebook collapse, unstable
gradient estimation, and modality-specific encoding constraints. Finally, we
discuss emerging research directions such as dynamic and task-adaptive
quantization, unified tokenization frameworks, and biologically inspired
codebook learning. This survey bridges the gap between traditional vector
quantization and modern LLM applications, serving as a foundational reference
for the development of efficient and generalizable multimodal systems. A
continuously updated version is available at:
https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [11] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
*Lee Harris*

Main category: cs.CL

TL;DR: 本文提出语言模型链（LMC）算法，通过级联多个语言模型，在提高信息提取速度和准确性的同时，显著减少了语言模型的“幻觉”问题。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽然能捕获复杂关系，但成本高昂且易产生不存在的“幻觉”信息，导致资源浪费。

Method: 提出LMC算法。该算法将语言模型响应与候选答案比对，若不正确，则将对应文本输入到更具预测性（但更慢）的语言模型中，重复此过程，直至预测正确或遍历所有模型。

Result: 将LMC算法应用于从医疗文档中提取患者出生日期，结果显示，与单个语言模型相比，LMC显著提高了预测速度和准确性，并大幅减少了幻觉。

Conclusion: LMC算法对知识提取领域有重要贡献，未来值得深入探索。

Abstract: Language models can capture complex relationships in given text, but these
are notorious for being costly and for producing information that does not
exist (i.e., hallucinations). Furthermore, the resources invested into
producing this information would be wasted if it were incorrect. We address
these issues by proposing, implementing, and applying the Language Model Chain
(LMC) algorithm. In this, a language model's response to a given prompt about
given text is only correct if it exists in the collection of possible (i.e.,
candidate) answers, and text corresponding to incorrect responses is fed into a
more predictive (but slower) language model. This process is repeated for a
collection of language models, or until all predictions about the text are
correct. We used the LMC algorithm to extract patient dates of birth from
medical documents, and combining a collection of language models in a
multi-stage cascade significantly increased prediction speed and accuracy over
individual language models, while greatly reducing the number of corresponding
hallucinations. We believe that the novel LMC algorithm significantly
contributes to the knowledge extraction field, and that this should be explored
much further in the future.

</details>


### [12] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
*Mateusz Kmak,Kamil Chmurzyński,Kamil Matejuk,Paweł Kotzbach,Jan Kocoń*

Main category: cs.CL

TL;DR: 研究发现，Reddit社交媒体情绪与股价仅有微弱关联，而评论量和谷歌搜索趋势等更简单的指标显示出更强的预测性。


<details>
  <summary>Details</summary>
Motivation: 2021年GameStop事件后，零售投资者在社交媒体上的活跃度激增，引发了关于在线情绪对股价影响的疑问。本文旨在探究源自社交媒体讨论的情绪是否能有效预测股市波动。

Method: 研究重点关注Reddit的r/wallstreetbets论坛中关于GameStop和AMC的情绪。采用两种现有文本情绪分析方法，并引入第三种：一个由ChatGPT标注并经过微调的基于RoBERTa的模型，旨在更好地解释社交媒体中非正式语言和表情符号。通过相关性和因果关系指标评估模型的预测能力。

Result: 研究结果显示，社交媒体情绪与股票价格仅有微弱的相关性。相反，评论量和谷歌搜索趋势等更简单的指标表现出更强的预测信号。

Conclusion: 这些发现揭示了散户投资者行为的复杂性，并表明传统的情绪分析可能无法完全捕捉影响市场波动的在线讨论的细微之处。

Abstract: The surge of retail investor activity on social media, exemplified by the
2021 GameStop short squeeze, raised questions about the influence of online
sentiment on stock prices. This paper explores whether sentiment derived from
social media discussions can meaningfully predict stock market movements. We
focus on Reddit's r/wallstreetbets and analyze sentiment related to two
companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's
role, we employ two existing text-based sentiment analysis methods and
introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model
designed to better interpret the informal language and emojis prevalent in
social media discussions. We use correlation and causality metrics to determine
these models' predictive power. Surprisingly, our findings suggest that social
media sentiment has only a weak correlation with stock prices. At the same
time, simpler metrics, such as the volume of comments and Google search trends,
exhibit stronger predictive signals. These results highlight the complexity of
retail investor behavior and suggest that traditional sentiment analysis may
not fully capture the nuances of market-moving online discussions.

</details>


### [13] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
*Aman Gupta,Yingying Zhuang,Zhou Yu,Ziji Zhang,Anurag Beniwal*

Main category: cs.CL

TL;DR: 本文评估了多语言RAG系统中不同提示翻译策略对RAG增强型LLM分类任务的影响，发现优化策略能显著提升跨语言知识共享和下游任务性能，倡导优化非英语低资源语言的跨语言提示。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）的多语言能力有所提升，但在不同语言和任务上的表现差异显著。在多语言检索增强生成（RAG）系统中，高资源语言（如英语）的知识库常被用于低资源语言，导致检索信息与上下文语言不一致。目前，预翻译和跨语言提示是两种常见做法，但它们的影响尚不明确。

Method: 本研究系统地评估了多语言系统中，针对RAG增强型LLM分类任务的不同提示翻译策略的影响。

Result: 实验结果表明，优化后的提示策略能显著改善跨语言知识共享，从而提升下游分类任务的性能。

Conclusion: 研究结果支持更广泛地利用多语言资源共享，并倡导对非英语语言，特别是低资源语言，进行跨语言提示优化。

Abstract: Despite advances in the multilingual capabilities of Large Language Models
(LLMs), their performance varies substantially across different languages and
tasks. In multilingual retrieval-augmented generation (RAG)-based systems,
knowledge bases (KB) are often shared from high-resource languages (such as
English) to low-resource ones, resulting in retrieved information from the KB
being in a different language than the rest of the context. In such scenarios,
two common practices are pre-translation to create a mono-lingual prompt and
cross-lingual prompting for direct inference. However, the impact of these
choices remains unclear. In this paper, we systematically evaluate the impact
of different prompt translation strategies for classification tasks with
RAG-enhanced LLMs in multilingual systems. Experimental results show that an
optimized prompting strategy can significantly improve knowledge sharing across
languages, therefore improve the performance on the downstream classification
task. The findings advocate for a broader utilization of multilingual resource
sharing and cross-lingual prompt optimization for non-English languages,
especially the low-resource ones.

</details>


### [14] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
*Brittney Exline,Melanie Duffin,Brittany Harbison,Chrissa da Gomez,David Joyner*

Main category: cs.CL

TL;DR: 本文研究了美国在线计算机课程中，英语母语者与非母语者身份如何影响同伴反馈体验，发现语言背景扮演着复杂但重要的角色。


<details>
  <summary>Details</summary>
Motivation: 美国计算机研究生项目中国际学生比例高，其中许多人选择在线课程，同伴反馈是教学重要组成部分。由于课程以英语进行，许多学生需以非母语学习。研究动机在于探究英语母语/非母语身份如何影响学生在这些在线课程中的同伴反馈体验。

Method: 研究检查了同伴反馈体验的三个指标。使用基于Twitter-roBERTa的模型分析了500名随机学生的同伴评论情感。随后，将情感分数和同伴反馈评分与学生的语言背景相关联，并控制了性别和年龄变量。

Result: 结果显示，英语母语者对反馈评价较低；非母语者写出的反馈更积极，但收到的反馈情感积极度较低。在控制性别和年龄后，出现了显著的交互作用。

Conclusion: 语言背景在塑造同伴反馈体验中扮演着适度但复杂的角色。

Abstract: Graduate-level CS programs in the U.S. increasingly enroll international
students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.
students. Many of these students take online courses, where peer feedback is
used to engage students and improve pedagogy in a scalable manner. Since these
courses are conducted in English, many students study in a language other than
their first. This paper examines how native versus non-native English speaker
status affects three metrics of peer feedback experience in online U.S.-based
computing courses. Using the Twitter-roBERTa-based model, we analyze the
sentiment of peer reviews written by and to a random sample of 500 students. We
then relate sentiment scores and peer feedback ratings to students' language
background. Results show that native English speakers rate feedback less
favorably, while non-native speakers write more positively but receive less
positive sentiment in return. When controlling for sex and age, significant
interactions emerge, suggesting that language background plays a modest but
complex role in shaping peer feedback experiences.

</details>


### [15] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
*Haoran Sun,Shaoning Zeng*

Main category: cs.CL

TL;DR: 针对大型语言模型智能体（LLM Agents）长期记忆不足问题，提出分层记忆（H-MEM）架构，通过语义抽象多级组织和基于索引的高效检索，显著提升了LLM Agents在长期对话中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agents的记忆机制在结构化组织和高效检索方面存在局限，限制了其在长期对话场景中的决策能力和上下文连贯性。

Method: 提出分层记忆（H-MEM）架构，该架构根据语义抽象程度多级组织和更新记忆，并通过内嵌的位置索引将记忆向量关联至下一层相关子记忆。在推理时，采用基于索引的路由机制实现高效逐层检索，避免了穷举相似性计算。

Result: 在LoCoMo数据集的五个任务设置上进行评估，实验结果显示，所提H-MEM方法持续优于五种基线方法。

Conclusion: H-MEM架构通过优化记忆组织和检索效率，有效解决了LLM Agents的长期记忆问题，显著提升了其在长期对话场景下的性能。

Abstract: Long-term memory is one of the key factors influencing the reasoning
capabilities of Large Language Model Agents (LLM Agents). Incorporating a
memory mechanism that effectively integrates past interactions can
significantly enhance decision-making and contextual coherence of LLM Agents.
While recent works have made progress in memory storage and retrieval, such as
encoding memory into dense vectors for similarity-based search or organizing
knowledge in the form of graph, these approaches often fall short in structured
memory organization and efficient retrieval. To address these limitations, we
propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that
organizes and updates memory in a multi-level fashion based on the degree of
semantic abstraction. Each memory vector is embedded with a positional index
encoding pointing to its semantically related sub-memories in the next layer.
During the reasoning phase, an index-based routing mechanism enables efficient,
layer-by-layer retrieval without performing exhaustive similarity computations.
We evaluate our method on five task settings from the LoCoMo dataset.
Experimental results show that our approach consistently outperforms five
baseline methods, demonstrating its effectiveness in long-term dialogue
scenarios.

</details>


### [16] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
*Nilesh,Atul Gupta,Avinash C Panday*

Main category: cs.CL

TL;DR: 本文提出一种新颖的输入嵌入方法，通过捕获实体在整个文档中的全局位置，增强文档级关系抽取中的全局上下文和多句推理能力，并在基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文档级关系抽取方法仅关注实体提及的句子，未能捕获完整的文档级全局上下文，导致在实体关系随上下文变化或需要多句推理时预测不准确。

Method: 提出一种新颖的输入嵌入（编码）方法，旨在捕获实体在整个文档中的位置信息，而非仅限于其出现范围。该方法将实体表示为独立的段落，从而有效利用全局关系和多句推理能力。

Result: 在DocRED、Re-DocRED和REBEL三个基准关系抽取数据集上进行了测试。实验结果表明，所提出的方法能够准确预测文档级设置下的实体间关系。

Conclusion: 该研究在理论上推动了文档级关系抽取中的全局上下文建模和多句推理；在实践上，它增强了关系检测能力，提高了实际NLP应用的性能。

Abstract: In document-level relation extraction, entities may appear multiple times in
a document, and their relationships can shift from one context to another.
Accurate prediction of the relationship between two entities across an entire
document requires building a global context spanning all relevant sentences.
Previous approaches have focused only on the sentences where entities are
mentioned, which fails to capture the complete document context necessary for
accurate relation extraction. Therefore, this paper introduces a novel input
embedding approach to capture the positions of mentioned entities throughout
the document rather than focusing solely on the span where they appear. The
proposed input encoding approach leverages global relationships and
multi-sentence reasoning by representing entities as standalone segments,
independent of their positions within the document. The performance of the
proposed method has been tested on three benchmark relation extraction
datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results
demonstrated that the proposed method accurately predicts relationships between
entities in a document-level setting. The proposed research also has
theoretical and practical implications. Theoretically, it advances global
context modeling and multi-sentence reasoning in document-level relation
extraction. Practically, it enhances relationship detection, enabling improved
performance in real-world NLP applications requiring comprehensive entity-level
insights and interpretability.

</details>


### [17] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
*Zhehao Tan,Yihan Jiao,Dan Yang,Lei Liu,Jie Feng,Duolin Sun,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了一个名为Placeholder-RAG-Benchmark的多层次、细粒度基准测试，旨在系统性评估大型语言模型（LLM）在检索增强生成（RAG）系统中对文档的利用能力，并揭示了LLM在此方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试主要关注系统整体性能，缺乏对LLM在RAG中特定能力（如结合查询与文档生成响应）的细致评估，尤其在文档利用的系统性、细粒度评估方面存在空白。

Method: 引入了Placeholder-RAG-Benchmark，一个多层次细粒度的基准测试，重点评估LLM的多级过滤能力、组合能力和引用推理能力。通过创新的基于占位符的方法，解耦LLM的参数知识和外部知识的贡献。

Result: 实验结果表明，代表性LLM在RAG系统的生成能力上存在局限性，尤其在错误容忍度和上下文忠实性方面表现不足。

Conclusion: 该基准测试提供了一个可复现的框架，有助于开发更可靠和高效的RAG系统。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge, where the LLM's ability to generate responses
based on the combination of a given query and retrieved documents is crucial.
However, most benchmarks focus on overall RAG system performance, rarely
assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects
such as noise robustness, but lack a systematic and granular evaluation
framework on document utilization. To this end, we introduce
\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,
emphasizing the following progressive dimensions: (1) multi-level filtering
abilities, (2) combination abilities, and (3) reference reasoning. To provide a
more nuanced understanding of LLMs' roles in RAG systems, we formulate an
innovative placeholder-based approach to decouple the contributions of the
LLM's parametric knowledge and the external knowledge. Experiments demonstrate
the limitations of representative LLMs in the RAG system's generation
capabilities, particularly in error resilience and context faithfulness. Our
benchmark provides a reproducible framework for developing more reliable and
efficient RAG systems. Our code is available in
https://github.com/Alipay-Med/PRGB.

</details>


### [18] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
*Xi Chen,Aske Plaat,Niki van Stein*

Main category: cs.CL

TL;DR: 研究发现，Chain-of-thought (CoT) 提示能在大容量大语言模型（LLMs）中诱导更可解释的内部计算结构，并有效提升其性能，尤其是在大模型中表现出明显的规模效应。


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought (CoT) 提示能提升大型语言模型在多步骤任务上的准确性，但其生成的“思维”是否真实反映了模型内部的推理过程，仍是一个未解决的问题。

Method: 本研究首次进行了特征层面的因果分析，以探究CoT提示的忠实性。方法结合了稀疏自编码器和激活修补技术，从Pythia-70M和Pythia-2.8B模型在处理GSM8K数学问题（使用CoT和plain提示）时提取单义特征。同时引入了patch-curves和随机特征修补作为基线。

Result: 将一小部分CoT推理特征交换到noCoT运行中，显著提高了2.8B模型答案的对数概率，但在70M模型中没有可靠效果，揭示了明显的模型规模阈值。CoT还导致大模型中激活稀疏性和特征可解释性分数显著提高，表明内部计算更加模块化（例如，模型对正确答案的置信度从1.2提高到4.3）。此外，有用的CoT信息不仅存在于顶层特征中，而且广泛分布。

Conclusion: 研究结果表明，CoT提示能够在大容量LLMs中诱导更可解释的内部结构，从而验证了其作为一种结构化提示方法的重要作用。

Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on
multi-step tasks, yet whether the generated "thoughts" reflect the true
internal reasoning process is unresolved. We present the first feature-level
causal study of CoT faithfulness. Combining sparse autoencoders with activation
patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B
while they tackle GSM8K math problems under CoT and plain (noCoT) prompting.
Swapping a small set of CoT-reasoning features into a noCoT run raises answer
log-probabilities significantly in the 2.8B model, but has no reliable effect
in 70M, revealing a clear scale threshold. CoT also leads to significantly
higher activation sparsity and feature interpretability scores in the larger
model, signalling more modular internal computation. For example, the model's
confidence in generating correct answers improves from 1.2 to 4.3. We introduce
patch-curves and random-feature patching baselines, showing that useful CoT
information is not only present in the top-K patches but widely distributed.
Overall, our results indicate that CoT can induce more interpretable internal
structures in high-capacity LLMs, validating its role as a structured prompting
method.

</details>


### [19] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
*Xiaoyu Pan,Yang Bai,Ke Zou,Yang Zhou,Jun Zhou,Huazhu Fu,Yih-Chung Tham,Yong Liu*

Main category: cs.CL

TL;DR: MLLMs在眼科诊断中受幻觉限制。本文引入EH-Benchmark评估幻觉，并提出多智能体框架有效缓解幻觉，提升模型准确性、可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs对眼科诊断至关重要，但其准确性受限于幻觉，这源于眼科知识不足、视觉定位与推理能力有限及多模态数据稀缺，导致病灶检测和疾病诊断不精确。此外，现有医学基准无法有效评估或缓解这些幻觉。

Method: 为解决上述挑战，研究提出了EH-Benchmark，一个专为评估MLLMs幻觉设计的眼科基准。该基准将幻觉分为视觉理解和逻辑组成两大类。考虑到MLLMs基于语言推理的特性，研究进一步提出一个以智能体为中心的三阶段框架，包括知识级检索、任务级案例研究和结果级验证。

Result: 实验结果表明，所提出的多智能体框架显著缓解了视觉理解和逻辑组成两类幻觉。这有效地提升了MLLMs的准确性、可解释性和可靠性。

Conclusion: 该研究成功通过EH-Benchmark评估并分类了MLLMs在眼科诊断中的幻觉。所提出的多智能体框架被证明能有效缓解这些幻觉，显著提升了MLLMs在眼科领域的诊断性能，包括准确性、可解释性和可靠性。

Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic
diagnosis, holding significant potential to address vision-threatening
diseases. However, their accuracy is constrained by hallucinations stemming
from limited ophthalmic knowledge, insufficient visual localization and
reasoning capabilities, and a scarcity of multimodal ophthalmic data, which
collectively impede precise lesion detection and disease diagnosis.
Furthermore, existing medical benchmarks fail to effectively evaluate various
types of hallucinations or provide actionable solutions to mitigate them. To
address the above challenges, we introduce EH-Benchmark, a novel ophthalmology
benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'
hallucinations based on specific tasks and error types into two primary
classes: Visual Understanding and Logical Composition, each comprising multiple
subclasses. Given that MLLMs predominantly rely on language-based reasoning
rather than visual processing, we propose an agent-centric, three-phase
framework, including the Knowledge-Level Retrieval stage, the Task-Level Case
Studies stage, and the Result-Level Validation stage. Experimental results show
that our multi-agent framework significantly mitigates both types of
hallucinations, enhancing accuracy, interpretability, and reliability. Our
project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [20] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
*Shalini Jangra,Suparna De,Nishanth Sastry,Saeed Fadaei*

Main category: cs.CL

TL;DR: 针对社交媒体中PII泄露研究缺乏公开数据集的问题，本文提出一种新颖方法，利用LLM生成可安全共享的合成PII泄露数据，并发布了数据集和代码，以促进可复现研究。


<details>
  <summary>Details</summary>
Motivation: 社交平台上（如Reddit）的帖子和评论常包含用户的个人身份信息（PII），这带来了隐私风险和网络危害。然而，识别和检索此类高风险PII自愿披露的研究受限于缺乏开放源代码的标注数据集。

Method: 本文提出一种新颖方法来创建可安全共享的PII泄露数据的合成等价物。具体包括：创建了针对弱势群体的19种PII泄露类别的分类法；利用Llama2-7B、Llama3-8B和zephyr-7b-beta三种大型语言模型（LLMs），通过顺序指令提示生成了合成的PII标注多文本跨度数据集。该方法的效用通过三个指标进行评估：再现性等效性（在合成数据上训练的模型结果应与原始数据可比）、合成数据与原始用户不可链接性，以及合成数据与原始数据不可区分性（人类难以区分）。

Result: 成功开发并发布了一个合成的PII标注多文本跨度数据集及相关代码，该数据集由三种LLM生成，旨在模拟原始Reddit帖子。评估结果表明，该方法生成的合成数据具有良好的再现性等效性、与原始用户不可链接性以及与原始数据难以区分性。

Conclusion: 通过提供可安全共享的合成PII泄露数据及其生成方法，本文解决了社交媒体PII隐私风险研究中数据集缺乏的难题。发布的分类法、数据集和代码将促进该领域的可复现研究，并有助于识别和减轻在线危害。

Abstract: Social platforms such as Reddit have a network of communities of shared
interests, with a prevalence of posts and comments from which one can infer
users' Personal Information Identifiers (PIIs). While such self-disclosures can
lead to rewarding social interactions, they pose privacy risks and the threat
of online harms. Research into the identification and retrieval of such risky
self-disclosures of PIIs is hampered by the lack of open-source labeled
datasets. To foster reproducible research into PII-revealing text detection, we
develop a novel methodology to create synthetic equivalents of PII-revealing
data that can be safely shared. Our contributions include creating a taxonomy
of 19 PII-revealing categories for vulnerable populations and the creation and
release of a synthetic PII-labeled multi-text span dataset generated from 3
text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and
zephyr-7b-beta, with sequential instruction prompting to resemble the original
Reddit posts. The utility of our methodology to generate this synthetic dataset
is evaluated with three metrics: First, we require reproducibility equivalence,
i.e., results from training a model on the synthetic data should be comparable
to those obtained by training the same models on the original posts. Second, we
require that the synthetic data be unlinkable to the original users, through
common mechanisms such as Google Search. Third, we wish to ensure that the
synthetic data be indistinguishable from the original, i.e., trained humans
should not be able to tell them apart. We release our dataset and code at
https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster
reproducible research into PII privacy risks in online social media.

</details>


### [21] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
*Shuyu Guo,Zhaochun Ren*

Main category: cs.CL

TL;DR: ACC-RAG是一种自适应上下文压缩框架，它根据输入复杂度动态调整RAG的压缩率，显著提高了推理效率并保持或提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)虽然能为大型语言模型(LLMs)提供外部知识，但检索到的上下文过长导致推理成本高昂。现有上下文压缩方法采用固定压缩率，无法适应不同查询的复杂性，导致过度压缩或压缩不足。

Method: 本文提出了ACC-RAG框架，它结合了分层压缩器（用于多粒度嵌入）和上下文选择器。该框架能够根据输入复杂度动态调整压缩率，以保留最少但足够的信息，模拟人类的快速浏览。

Result: ACC-RAG在Wikipedia和五个QA数据集上进行评估，结果显示其性能优于固定速率的压缩方法。与标准RAG相比，ACC-RAG在保持或提高准确性的同时，推理速度提高了4倍以上。

Conclusion: ACC-RAG成功解决了RAG的推理成本问题，通过智能地自适应上下文压缩，它能够在显著提高效率的同时，不牺牲甚至提升模型的准确性，使其成为一种更实用的RAG解决方案。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external knowledge but incurs significant inference costs due to lengthy
retrieved contexts. While context compression mitigates this issue, existing
methods apply fixed compression rates, over-compressing simple queries or
under-compressing complex ones. We propose Adaptive Context Compression for RAG
(ACC-RAG), a framework that dynamically adjusts compression rates based on
input complexity, optimizing inference efficiency without sacrificing accuracy.
ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with
a context selector to retain minimal sufficient information, akin to human
skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms
fixed-rate methods and matches/unlocks over 4 times faster inference versus
standard RAG while maintaining or improving accuracy.

</details>


### [22] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
*Baptiste Lefort,Eric Benhamou,Beatrice Guez,Jean-Jacques Ohana,Ethan Setrouk,Alban Etienne*

Main category: cs.CL

TL;DR: 该论文提出一种新颖的层次化深度强化学习（DRL）框架，结合轻量级大语言模型（LLMs）、金融新闻情感信号和传统市场指标，用于投资组合优化。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合金融新闻情感信号和传统市场指标，并利用创新的层次化强化学习架构，提升投资组合优化的性能、回报率及稳定性。

Method: 构建了一个三层层次化架构：底层RL代理处理混合数据；元代理聚合底层决策；超级代理根据市场数据和情感分析整合最终决策。该框架将轻量级LLMs与DRL相结合，以融合情感信号和市场指标。

Result: 在2018-2024年数据上评估（2000-2017年训练），实现了26%的年化回报率和1.2的夏普比率，表现优于等权重和标普500基准。

Conclusion: 该框架通过可扩展的跨模态集成和层次化RL结构，显著增强了投资组合优化的稳定性，并展现出卓越的性能。同时，研究强调了开源可复现性。

Abstract: This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.

</details>


### [23] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
*Anthony C Davis,Burhan Sadiq,Tianmin Shu,Chien-Ming Huang*

Main category: cs.CL

TL;DR: 现有视觉-语言模型面临解释性、新信息整合和推理的挑战。将VLM与外部符号系统结合的神经符号系统是潜在解决方案。本文旨在系统性回顾并分类实现此结合的技术。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在处理大规模非结构化数据方面表现出色，但在输出可解释性、整合新信息时的重新训练需求、资源密集度以及逻辑推理能力方面存在不足。

Method: 本文采用系统性文献综述的方法，旨在对通过与外部符号信息系统交互来改进视觉-语言理解的现有技术进行分类。

Result: 作为一篇文献综述的摘要，并未直接给出具体的研究结果。预期的成果将是对提升视觉-语言理解的神经符号整合技术的系统性分类和整理。

Conclusion: 整合神经网络（尤其是利用强大的预训练视觉-语言模型）与外部符号信息系统，形成神经符号系统，是克服现有视觉-语言模型局限性（如可解释性、推理能力和新信息整合）的一个有前景且务实的方向。

Abstract: Recent advances in visual-language machine learning models have demonstrated
exceptional ability to use natural language and understand visual scenes by
training on large, unstructured datasets. However, this training paradigm
cannot produce interpretable explanations for its outputs, requires retraining
to integrate new information, is highly resource-intensive, and struggles with
certain forms of logical reasoning. One promising solution involves integrating
neural networks with external symbolic information systems, forming neural
symbolic systems that can enhance reasoning and memory abilities. These neural
symbolic systems provide more interpretable explanations to their outputs and
the capacity to assimilate new information without extensive retraining.
Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural
component, augmented by external systems, offers a pragmatic approach to
realizing the benefits of neural-symbolic integration. This systematic
literature review aims to categorize techniques through which visual-language
understanding can be improved by interacting with external symbolic information
systems.

</details>


### [24] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
*Jingwei Zhao,Yuhua Wen,Qifei Li,Minchi Hu,Yingying Zhou,Jingyao Xue,Junyang Wu,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CL

TL;DR: 本文综述了旨趣识别领域，重点关注深度学习、从单模态到多模态技术的发展以及Transformer模型的应用，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着对自然人机交互日益增长的需求，旨趣识别领域已从传统的文本处理发展到融合多种模态数据。深度学习和Transformer模型的引入带来了显著突破，因此有必要对该领域，特别是多模态旨趣识别（MIR）的最新进展进行系统性回顾和总结。

Method: 本文采用综述方法，对旨趣识别的深度学习方法进行了全面回顾，涵盖了从单模态到多模态技术的演变、相关数据集、主要方法论、实际应用以及当前面临的挑战。

Result: 该综述为研究人员提供了多模态旨趣识别（MIR）领域最新进展的深入见解。

Conclusion: 本文为多模态旨趣识别领域的未来研究指明了方向。

Abstract: Intent recognition aims to identify users' underlying intentions,
traditionally focusing on text in natural language processing. With growing
demands for natural human-computer interaction, the field has evolved through
deep learning and multimodal approaches, incorporating data from audio, vision,
and physiological signals. Recently, the introduction of Transformer-based
models has led to notable breakthroughs in this domain. This article surveys
deep learning methods for intent recognition, covering the shift from unimodal
to multimodal techniques, relevant datasets, methodologies, applications, and
current challenges. It provides researchers with insights into the latest
developments in multimodal intent recognition (MIR) and directions for future
research.

</details>


### [25] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
*Kathleen Mealey,Jonathan A. Karr Jr.,Priscila Saboia Moreira,Paul R. Brenner,Charles F. Vardeman II*

Main category: cs.CL

TL;DR: 评估在航空业保密环境下，NLP和LLM工具提取运营智能的能力，发现其性能存在显著局限性，并提供了开源数据集。


<details>
  <summary>Details</summary>
Motivation: 从组织数据中获取运营智能面临数据保密性与集成度的矛盾，以及自然语言处理（NLP）工具在运营维护等特定领域知识结构上的局限性，尤其在航空业需要可信应用。

Method: 讨论了知识图谱构建及其关键组件（命名实体识别、共指消解、命名实体链接和关系提取）。使用美国联邦航空管理局（FAA）的设备故障/维护数据集，评估了16种NLP工具以及大型语言模型（LLMs）的零样本性能，特别关注在受控、保密环境下的操作（数据不发送给第三方）。

Result: 观察到NLP和LLM工具在受控保密环境下运行时存在显著的性能局限性。

Conclusion: 指出可信NLP和LLM工具面临的挑战及其在航空等关键任务行业中应用的技术成熟度较低，并提出了增强信任的建议，同时提供了开源数据集以支持进一步的基准测试和评估。

Abstract: Deriving operational intelligence from organizational data repositories is a
key challenge due to the dichotomy of data confidentiality vs data integration
objectives, as well as the limitations of Natural Language Processing (NLP)
tools relative to the specific knowledge structure of domains such as
operations and maintenance. In this work, we discuss Knowledge Graph
construction and break down the Knowledge Extraction process into its Named
Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation
Extraction functional components. We then evaluate sixteen NLP tools in concert
with or in comparison to the rapidly advancing capabilities of Large Language
Models (LLMs). We focus on the operational and maintenance intelligence use
case for trusted applications in the aircraft industry. A baseline dataset is
derived from a rich public domain US Federal Aviation Administration dataset
focused on equipment failures or maintenance requirements. We assess the
zero-shot performance of NLP and LLM tools that can be operated within a
controlled, confidential environment (no data is sent to third parties). Based
on our observation of significant performance limitations, we discuss the
challenges related to trusted NLP and LLM tools as well as their Technical
Readiness Level for wider use in mission-critical industries such as aviation.
We conclude with recommendations to enhance trust and provide our open-source
curated dataset to support further baseline testing and evaluation.

</details>


### [26] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本研究系统比较了GPT、Claude、Perplexity、Gemini和DeepSeek五种主流大型语言模型（LLMs）在处理“Magnificent Seven”科技公司10-K报告时的表现。结果显示GPT表现最佳，其次是Claude和Perplexity，而Gemini和DeepSeek输出更不稳定。研究还发现模型输出受提示和源材料影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在金融自然语言处理（FinNLP）任务中展现出卓越能力，但目前对广泛使用的LLMs进行系统性比较的研究仍然不足，尤其是在LLMs对金融分析影响日益增长的背景下。

Method: 研究选取了GPT、Claude、Perplexity、Gemini和DeepSeek五种领先的LLMs，并使用“Magnificent Seven”科技公司的10-K文件作为数据源。通过创建领域特定提示，采用三种方法评估模型性能：人工标注、自动化词汇语义指标（ROUGE、余弦相似度、Jaccard），以及模型行为诊断（提示级别方差和模型间相似性）。

Result: 研究结果显示，GPT在答案的连贯性、语义对齐和上下文相关性方面表现最佳，其次是Claude和Perplexity。而Gemini和DeepSeek则表现出更大的可变性和较低的一致性。此外，模型的输出相似性和稳定性会因公司和时间而异，表明它们对提示的编写方式和所使用的源材料敏感。

Conclusion: 本研究通过对主流LLMs在金融分析任务中的系统比较，发现GPT在输出质量和一致性方面表现突出。研究结果强调了在金融领域应用LLMs时，需要考虑其对提示工程和源数据的高度敏感性，为未来LLMs在金融场景的部署提供了重要参考依据。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


### [27] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
*Jinkun Zhao,Yuanshuai Wang,Xingjian Zhang,Ruibo Chen,Xingchuang Liao,Junle Wang,Lei Huang,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文提出CoE-Ops框架，通过结合大语言模型任务分类器和RAG机制，有效解决AIOps中单一模型局限性，并在多任务处理中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: AIOps领域中，单一模型受限于领域知识，难以全面处理复杂运维需求。借鉴集成学习和LLM的成功经验，作者旨在通过模型协作解决AIOps中的多任务处理挑战。

Method: 提出CoE-Ops（专家协作）框架，该框架整合通用大语言模型任务分类器，并通过检索增强生成（RAG）机制，提升其处理高层（如代码、构建）和低层（如故障分析、异常检测）AIOps问答任务的能力。

Result: 在DevOps-EVAL数据集上的实验表明，CoE-Ops在高级AIOps任务路由精度上比现有CoE方法提高了72%，在DevOps问题解决中比单一AIOps模型精度提升高达8%，且比大规模MoE模型精度高出14%。

Conclusion: CoE-Ops框架通过其创新的协作机制，显著提升了AIOps系统在处理多样化和复杂运维任务时的性能和准确性，证明了其在未来智能运维领域的潜力。

Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a
prominent paradigm in DevOps. Lots of work has been proposed to improve the
performance of different AIOps phases. However, constrained by domain-specific
knowledge, a single model can only handle the operation requirement of a
specific task,such as log parser,root cause analysis. Meanwhile, combining
multiple models can achieve more efficient results, which have been proved in
both previous ensemble learning and the recent LLM training domain. Inspired by
these works,to address the similar challenges in AIOPS, this paper first
proposes a collaboration-of-expert framework(CoE-Ops) incorporating a
general-purpose large language model task classifier. A retrieval-augmented
generation mechanism is introduced to improve the framework's capability in
handling both Question-Answering tasks with high-level(Code,build,Test,etc.)
and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed
method is implemented in the AIOps domain, and extensive experiments are
conducted on the DevOps-EVAL dataset. Experimental results demonstrate that
CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps
tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement
over single AIOps models in DevOps problem resolution, and outperforms
larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [28] [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)
*Zhangcheng Qiang,Kerry Taylor,Weiqing Wang,Jing Jiang*

Main category: cs.CL

TL;DR: 本文创建了OAEI-LLM-T数据集，旨在解决大型语言模型在本体匹配任务中产生的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在下游任务中常出现幻觉。对于基于LLM的本体匹配（OM）系统，解决这些幻觉是一个重大挑战。

Method: 引入了新的基准数据集OAEI-LLM-T。该数据集基于本体对齐评估倡议（OAEI）中的七个TBox数据集，捕获了十种不同LLM在执行OM任务时产生的幻觉，并将这些OM特有的幻觉组织成两大类和六小子类。

Result: 成功构建了OAEI-LLM-T数据集，其中包含了LLM在本体匹配任务中的特定幻觉分类。该数据集可用于构建OM任务的LLM排行榜和微调用于OM任务的LLM。

Conclusion: OAEI-LLM-T数据集为评估和改进LLM在本体匹配任务中的表现提供了一个有用的工具，尤其在解决其幻觉问题方面。

Abstract: Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.

</details>


### [29] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
*Sumit Soman,H. G. Ranjani,Sujoy Roychowdhury,Venkata Dharma Surya Narayana Sastry,Akshat Jain,Pranav Gangrade,Ayaaz Khan*

Main category: cs.CL

TL;DR: 为解决文本RAG系统无法处理技术文档中图表问答的问题，本研究提出一种方法，将VLM生成的流程图图表示融入文本RAG系统，并在电信领域实现了高效的图像检索和问答，同时降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的检索增强生成（RAG）系统无法有效回答技术文档中答案存在于图表（如流程图、流程图）中的问题。

Method: 利用视觉大型语言模型（VLM）获取流程图的图表示，并将其整合到基于文本的RAG系统中。方法涵盖了技术文档处理、图像类型分类、图表示构建，以及与文本嵌入管道的结合以实现高效检索。该方法在推理时无需VLM。

Result: 通过微调VLM模型获得的图表示与真实情况相比具有较低的编辑距离，证明了其在流程图图像方面的鲁棒性。基于这些图表示的问答方法，在使用基于文本的嵌入模型（包括电信领域定制模型）时，展现了良好的检索性能。此外，该方法在推理阶段无需VLM，带来了重要的成本效益。

Conclusion: 本研究成功地通过将VLM生成的图表示融入文本RAG系统，实现了电信领域技术文档中图表相关问答的有效检索，并为部署的问答系统提供了重要的成本优势。

Abstract: Question-Answering (QA) from technical documents often involves questions
whose answers are present in figures, such as flowcharts or flow diagrams.
Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such
questions. We leverage graph representations of flowcharts obtained from Visual
large Language Models (VLMs) and incorporate them in a text-based RAG system to
show that this approach can enable image retrieval for QA in the telecom
domain. We present the end-to-end approach from processing technical documents,
classifying image types, building graph representations, and incorporating them
with the text embedding pipeline for efficient retrieval. We benchmark the same
on a QA dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained using a
fine-tuned VLM model have lower edit distance with respect to the ground truth,
which illustrate the robustness of these representations for flowchart images.
Further, the approach for QA using these representations gives good retrieval
performance using text-based embedding models, including a telecom-domain
adapted one. Our approach also alleviates the need for a VLM in inference,
which is an important cost benefit for deployed QA systems.

</details>


### [30] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
*Bastien Le Guellec,Kokou Adambounou,Lisa C Adams,Thibault Agripnidis,Sung Soo Ahn,Radhia Ait Chalal,Tugba Akinci D Antonoli,Philippe Amouyel,Henrik Andersson,Raphael Bentegeac,Claudio Benzoni,Antonino Andrea Blandino,Felix Busch,Elif Can,Riccardo Cau,Armando Ugo Cavallo,Christelle Chavihot,Erwin Chiquete,Renato Cuocolo,Eugen Divjak,Gordana Ivanac,Barbara Dziadkowiec Macek,Armel Elogne,Salvatore Claudio Fanni,Carlos Ferrarotti,Claudia Fossataro,Federica Fossataro,Katarzyna Fulek,Michal Fulek,Pawel Gac,Martyna Gachowska,Ignacio Garcia Juarez,Marco Gatti,Natalia Gorelik,Alexia Maria Goulianou,Aghiles Hamroun,Nicolas Herinirina,Krzysztof Kraik,Dominik Krupka,Quentin Holay,Felipe Kitamura,Michail E Klontzas,Anna Kompanowska,Rafal Kompanowski,Alexandre Lefevre,Tristan Lemke,Maximilian Lindholz,Lukas Muller,Piotr Macek,Marcus Makowski,Luigi Mannacio,Aymen Meddeb,Antonio Natale,Beatrice Nguema Edzang,Adriana Ojeda,Yae Won Park,Federica Piccione,Andrea Ponsiglione,Malgorzata Poreba,Rafal Poreba,Philipp Prucker,Jean Pierre Pruvo,Rosa Alba Pugliesi,Feno Hasina Rabemanorintsoa,Vasileios Rafailidis,Katarzyna Resler,Jan Rotkegel,Luca Saba,Ezann Siebert,Arnaldo Stanzione,Ali Fuat Tekin,Liz Toapanta Yanchapaxi,Matthaios Triantafyllou,Ekaterini Tsaoulia,Evangelia Vassalou,Federica Vernuccio,Johan Wasselius,Weilang Wang,Szymon Urban,Adrian Wlodarczak,Szymon Wlodarczak,Andrzej Wysocki,Lina Xu,Tomasz Zatonski,Shuhang Zhang,Sebastian Ziegelmayer,Gregory Kuchcinski,Keno K Bressem*

Main category: cs.CL

TL;DR: PARROT是一个大型、多中心、开放获取的多语言虚构放射学报告数据集，旨在测试放射学自然语言处理应用。它包含2658份报告，来自76位作者，涵盖21个国家和13种语言。人机报告区分研究显示整体准确率为53.9%，放射科医生表现更佳。该数据集为无隐私限制的跨界NLP开发提供支持。


<details>
  <summary>Details</summary>
Motivation: 开发和验证PARROT数据集，以创建一个大型、多中心、开放获取的多语言虚构放射学报告资源，用于测试和开发放射学领域的自然语言处理（NLP）应用，从而克服真实患者数据固有的隐私限制。

Method: 项目在2024年5月至9月期间进行，邀请放射科医生贡献虚构放射学报告（每位至少20份），并提供元数据（如解剖区域、影像模态、临床背景）和非英语报告的英文翻译，所有报告均分配ICD-10编码。同时，进行了一项人机报告区分研究，154名参与者（包括放射科医生、医疗和非医疗专业人员）评估报告是人工撰写还是AI生成。

Result: 数据集共包含来自21个国家、13种语言的76位作者提供的2,658份放射学报告。报告涵盖多种影像模态（CT：36.1%，MRI：22.8%，X线：19.0%，超声：16.8%）和解剖区域（以胸部、腹部、头部和盆腔为主）。在人机报告区分研究中，参与者识别报告来源的平均准确率为53.9%，其中放射科医生表现显著优于其他组（56.9%，p<0.05）。

Conclusion: PARROT是目前最大的开放式多语言放射学报告数据集，它能够促进跨越语言、地域和临床界限的自然语言处理应用的开发和验证，且无需面临隐私限制。

Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal
Annotated Radiology Reports for Open Testing), a large, multicentric,
open-access dataset of fictional radiology reports spanning multiple languages
for testing natural language processing applications in radiology. Materials
and Methods: From May to September 2024, radiologists were invited to
contribute fictional radiology reports following their standard reporting
practices. Contributors provided at least 20 reports with associated metadata
including anatomical region, imaging modality, clinical context, and for
non-English reports, English translations. All reports were assigned ICD-10
codes. A human vs. AI report differentiation study was conducted with 154
participants (radiologists, healthcare professionals, and non-healthcare
professionals) assessing whether reports were human-authored or AI-generated.
Results: The dataset comprises 2,658 radiology reports from 76 authors across
21 countries and 13 languages. Reports cover multiple imaging modalities (CT:
36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical
regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)
being most prevalent. In the differentiation study, participants achieved 53.9%
accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated
reports, with radiologists performing significantly better (56.9%, 95% CI:
53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the
largest open multilingual radiology report dataset, enabling development and
validation of natural language processing applications across linguistic,
geographic, and clinical boundaries without privacy constraints.

</details>


### [31] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
*Rui Jiao,Yue Zhang,Jinku Li*

Main category: cs.CL

TL;DR: RELIANCE是一个新颖框架，旨在解决大型语言模型(LLM)中间推理步骤中的事实不准确问题。它通过集成事实检查分类器、强化学习和可解释性模块，显著提高了模型的事实稳健性并提供了机制洞察。


<details>
  <summary>Details</summary>
Motivation: LLM即使最终答案正确，其中间推理步骤也常出现事实错误。这在医疗、法律和科学等高风险领域构成重大风险，可能导致用户被误导做出危险决策。

Method: RELIANCE框架包含三个核心组件：1) 专门的事实检查分类器，用于检测推理链中的事实不一致性；2) 群体相对策略优化(GRPO)强化学习，通过多维度奖励平衡事实性、连贯性和结构正确性；3) 机械可解释性模块，检查事实性改进如何体现在模型激活上。

Result: 对十个SOTA模型评估显示，领先模型如Claude-3.7和GPT-o1的推理事实准确率分别仅为81.93%和82.57%。RELIANCE显著提升了事实稳健性（最高达49.90%），同时在Math-500、AIME-2024和GPQA等挑战性基准测试中保持或提高了性能。激活层分析提供了关于事实增强如何重塑模型架构内推理轨迹的可行性洞察。

Conclusion: RELIANCE框架有效解决了LLM中间推理步骤中的事实不准确问题，显著增强了模型的事实稳健性。激活层分析为未来通过激活引导优化明确提高事实稳健性的训练方法奠定了基础。

Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy
for Confidence Enhancement), a novel framework addressing a critical
vulnerability in Large Language Models (LLMs): the prevalence of factual
inaccuracies within intermediate reasoning steps despite correct final answers.
This phenomenon poses substantial risks in high-stakes domains including
healthcare, legal analysis, and scientific research, where erroneous yet
confidently presented reasoning can mislead users into dangerous decisions. Our
framework integrates three core components: (1) a specialized fact-checking
classifier trained on counterfactually augmented data to detect subtle factual
inconsistencies within reasoning chains; (2) a Group Relative Policy
Optimization (GRPO) reinforcement learning approach that balances factuality,
coherence, and structural correctness through multi-dimensional rewards; and
(3) a mechanistic interpretability module examining how factuality improvements
manifest in model activations during reasoning processes. Extensive evaluation
across ten state-of-the-art models reveals concerning patterns: even leading
models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of
only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual
robustness (up to 49.90% improvement) while maintaining or improving
performance on challenging benchmarks including Math-500, AIME-2024, and GPQA.
Furthermore, our activation-level analysis provides actionable insights into
how factual enhancements reshape reasoning trajectories within model
architectures, establishing foundations for future training methodologies that
explicitly target factual robustness through activation-guided optimization.

</details>


### [32] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
*Paul Minchella,Loïc Verlingue,Stéphane Chrétien,Rémi Vaucher,Guillaume Metzler*

Main category: cs.CL

TL;DR: SigBERT是一个创新的时间序列生存分析框架，通过结合文本嵌入、粗路径理论的几何特征提取和LASSO-Cox模型，有效处理电子病历中的时间戳文本数据，提升风险估计。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析方法难以有效处理电子病历中复杂且序列化的文本数据，限制了其在医疗机器学习应用中的潜力。

Method: 提出SigBERT框架。它首先从带时间戳的医疗报告中提取词嵌入并平均为句嵌入。接着，应用粗路径理论的签名提取技术，从句嵌入的时间序列中获取几何特征以捕捉时序动态。最后，将这些特征集成到LASSO惩罚的Cox模型中，估计患者特异性风险评分。

Result: 在L'eon Bérard中心的真实肿瘤学数据集上进行训练和评估，在独立测试队列上的C-index得分为0.75（标准差0.014）。

Conclusion: SigBERT成功整合了序列化的医疗数据，提高了风险估计的准确性，推动了基于叙述性文本的生存分析的发展。

Abstract: Electronic medical reports (EHR) contain a vast amount of information that
can be leveraged for machine learning applications in healthcare. However,
existing survival analysis methods often struggle to effectively handle the
complexity of textual data, particularly in its sequential form. Here, we
propose SigBERT, an innovative temporal survival analysis framework designed to
efficiently process a large number of clinical reports per patient. SigBERT
processes timestamped medical reports by extracting and averaging word
embeddings into sentence embeddings. To capture temporal dynamics from the time
series of sentence embedding coordinates, we apply signature extraction from
rough path theory to derive geometric features for each patient, which
significantly enhance survival model performance by capturing complex temporal
dynamics. These features are then integrated into a LASSO-penalized Cox model
to estimate patient-specific risk scores. The model was trained and evaluated
on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a
C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT
integrates sequential medical data to enhance risk estimation, advancing
narrative-based survival analysis.

</details>


### [33] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 本文提出一种结合自然语言处理（NLP）和多波自适应抽样的快速方法，用于验证大型索赔数据库中基于代码的算法，可显著节省时间和资源，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 在大型索赔数据库中，用于识别健康结果的基于代码的算法的测量特性需要验证，以量化结果误分类偏差。然而，通过手动审查电子健康记录创建参考标准标签通常耗时且资源密集。

Method: 开发了一种加速验证流程，包含两个主要机制：1) 使用自然语言处理（NLP）减少人工审查每份图表的时间；2) 采用带有预定义停止标准的多波自适应抽样方法，以在达到足够精度时停止验证研究。该过程以验证肥胖患者意图自伤索赔算法的性能为例进行说明。

Result: 经验证明，NLP辅助的注释过程使每份图表的审查时间减少了40%。同时，结合多波样本的预定义停止规则，可在不显著影响测量特性精度的前提下，避免审查77%的患者图表。

Conclusion: 这种方法可以促进基于代码算法的更常规验证，从而增强对数据库研究结果可靠性的理解，并可能更广泛地应用于定量偏差分析。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [34] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
*Naomi Omeonga wa Kayembe*

Main category: cs.CL

TL;DR: 本文将“任意性”重新定义为一种基础功能机制，而非缺陷，它使系统通过隐匿内部逻辑有效运行，并探讨其在人类系统（尤其是法律）和AI可解释性中的应用。


<details>
  <summary>Details</summary>
Motivation: 旨在挑战将“任意性”视为规范缺陷或压迫症状的传统观点，重新将其定义为一种基础功能机制和符号学特征，解释系统如何通过隐匿内部理由有效运作，并分析“动机->可确认性->可争议性”链条断裂如何导致权力失控和责任缺失。

Method: 概念重构（将“任意性”从缺陷重定义为功能机制和符号学特征）；理论拓展（借鉴索绪尔的“符号的任意性”概念，并推广至语言之外的领域）；机制分析（提出“动机->可确认性->可争议性”链条，并引入“无动机化”和“冲突侧向化”等概念）；数学形式化（运用香农的熵模型，将任意性形式化为条件熵A=H(L|M)）；应用领域（主要通过人类社会系统进行分析，并提出其在人工智能可解释性研究中的潜力）。

Result: “任意性”被重新定义为一种中性、基础的功能机制和符号学特征，使得系统在不暴露内部理由的情况下有效运作。当“动机->可确认性->可争议性”链条因“无动机化”或“冲突侧向化”而断裂时，行为可在不暴露理由的情况下产生约束力，从而排除司法审查，这种结构性不透明旨在保护权威免受问责。任意性被形式化为A=H(L|M)，并被视为控制和关怀的核心中性操作符。

Conclusion: “任意性”是人类系统中一个根本的、中性的操作原则，对于控制和关怀都至关重要。其结构性不透明虽看似不合逻辑，实则有意保护权威免受问责，阻止争议。该框架为分析先进人工智能系统的可解释性提供了新途径。

Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of
domination, but as a foundational functional mechanism structuring human
systems and interactions. Diverging from critical traditions that conflate
arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a
property enabling systems - linguistic, legal, or social - to operate
effectively while withholding their internal rationale. Building on Ferdinand
de Saussure's concept of l'arbitraire du signe, the analysis extends this
principle beyond language to demonstrate its cross-domain applicability,
particularly in law and social dynamics. The paper introduces the "Motivation
-> Constatability -> Contestability" chain, arguing that motivation functions
as a crucial interface rendering an act's logic vulnerable to intersubjective
contestation. When this chain is broken through mechanisms like
"immotivization" or "Conflict Lateralization" (exemplified by "the blur of the
wolf drowned in the fish"), acts produce binding effects without exposing their
rationale, thus precluding justiciability. This structural opacity, while
appearing illogical, is a deliberate design protecting authority from
accountability. Drawing on Shannon's entropy model, the paper formalizes
arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern
theory of arbitrariness as a neutral operator central to control as well as
care, an overlooked dimension of interpersonal relations. While primarily
developed through human social systems, this framework also illuminates a new
pathway for analyzing explainability in advanced artificial intelligence
systems.

</details>


### [35] [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
*Chengqian Ma,Wei Tao,Yiwen Guo*

Main category: cs.CL

TL;DR: 缺乏对口语对话模型(SDMs)实际对话能力的全面评估。本文提出了一个包含英汉双语实例的基准数据集，并结合基于LLM的评估方法，旨在评估SDM在处理复杂语音交互挑战（如歧义和上下文依赖）方面的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管口语对话模型（SDMs）日益普及，但与文本型大语言模型（LLMs）相比，缺乏对其在理解和模拟人类复杂语音对话方面实际效果的全面基准评估。人类语音交互因其固有的歧义性（如多义、异形同音、重音）和上下文依赖性（如省略、指代消解、多轮交互）而更复杂，现有研究未能充分应对。

Method: 本文提出了一个包含1,079个英汉双语实例的基准数据集，旨在评估SDM在处理语音对话中特有挑战（如歧义和上下文依赖）时的性能。此外，还引入了一种与人类判断高度一致的基于LLM的评估方法。

Result: 论文的主要成果是成功构建并呈现了用于评估口语对话模型（SDMs）在复杂语音交互中性能的基准数据集，以及配套的基于LLM的评估方法。这为后续全面探索SDM的实际效果奠定了基础。

Conclusion: 该研究通过提供一个专门的基准数据集和评估工具，填补了口语对话模型评估领域的空白，有助于更深入地理解SDM在实际对话中的能力和局限，从而推动其发展以更好地模拟人类对话。

Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention
for their ability to generate voice responses directly to users' spoken
queries. Despite their increasing popularity, there exists a gap in research
focused on comprehensively understanding their practical effectiveness in
comprehending and emulating human conversations. This is especially true
compared to text-based Large Language Models (LLMs), which benefit from
extensive benchmarking. Human voice interactions are inherently more complex
than text due to characteristics unique to spoken dialogue. Ambiguity poses one
challenge, stemming from semantic factors like polysemy, as well as
phonological aspects such as heterograph, heteronyms, and stress patterns.
Additionally, context-dependency, like omission, coreference, and multi-turn
interaction, adds further complexity to human conversational dynamics. To
illuminate the current state of SDM development and to address these
challenges, we present a benchmark dataset in this paper, which comprises 1,079
instances in English and Chinese. Accompanied by an LLM-based evaluation method
that closely aligns with human judgment, this dataset facilitates a
comprehensive exploration of the performance of SDMs in tackling these
practical challenges.

</details>


### [36] [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)
*Valeria de Paiva,Qiyue Gao,Hai Hu,Pavel Kovalev,Yikang Liu,Lawrence S. Moss,Zhiheng Qian*

Main category: cs.CL

TL;DR: 本文研究了当代大型语言模型（LLMs）在数学文本自然语言推理（Math NLI）上的能力。通过构建新语料库，发现LLM群体投票在某些情况下表现良好，但LLMs在数学语言和基本推理上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探究当代大型语言模型是否能够有效执行数学文本上的自然语言推理任务，即解决Math NLI问题。

Method: 构建了一个包含人类专家标注的Math NLI语料库，其中前提来自现有数学文本。同时，还评估了由LLMs自身生成假设的语料库质量。研究了LLMs的性能及不同模型组之间的一致性。

Result: 正面发现：在某些设置下，LLM的多数投票在Math NLI领域与人类标注数据表现相当。负面发现：LLM在数学语言理解上仍有困难，偶尔会失败于基本推理。此外，当前模型不像前代模型那样容易出现仅基于假设的“推理”。

Conclusion: 尽管LLMs通过聚合（如多数投票）在Math NLI方面显示出潜力，但在理解数学语言和执行基本推理方面仍有显著局限性。研究同时提供语料库以支持未来研究。

Abstract: We ask whether contemporary LLMs are able to perform natural language
inference (NLI) tasks on mathematical texts. We call this the Math NLI problem.
We construct a corpus of Math NLI pairs whose premises are from extant
mathematical text and whose hypotheses and gold labels were provided by people
with experience in both research-level mathematics and also in the NLI field.
We also investigate the quality of corpora using the same premises but whose
hypotheses are provided by LLMs themselves. We not only investigate the
performance but also the inter-group consistency of the diverse group of LLMs.
We have both positive and negative findings. Among our positive findings: in
some settings, using a majority vote of LLMs is approximately equivalent to
using human-labeled data in the Math NLI area. On the negative side: LLMs still
struggle with mathematical language. They occasionally fail at even basic
inferences. Current models are not as prone to hypothesis-only "inference" in
our data the way the previous generation had been. In addition to our findings,
we also provide our corpora as data to support future work on Math NLI.

</details>


### [37] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

TL;DR: 本文研究利用大语言模型的上下文学习（ICL）能力，在无需微调的情况下进行框架语义分析（FSP），通过自动生成提示词实现了竞争性的结果。


<details>
  <summary>Details</summary>
Motivation: 探讨上下文学习（ICL）与大型语言模型（LLMs）结合，是否能在不进行模型微调的情况下执行框架语义分析（FSP）。

Method: 提出了一种方法，该方法仅依赖FrameNet数据库自动为框架识别（FI）和框架语义角色标注（FSRL）子任务生成特定提示词。这些提示词由框架定义和标注示例构建，并用于指导六种不同的大型语言模型进行实验。

Result: 在与暴力事件相关的框架子集上进行实验，框架识别（FI）的F1分数为94.3%，框架语义角色标注（FSRL）的F1分数为77.4%，结果具有竞争力。

Conclusion: 研究结果表明，对于领域特定的框架语义分析任务，上下文学习（ICL）提供了一种实用且有效的替代传统微调的方法。

Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


### [38] [Context-aware Rotary Position Embedding](https://arxiv.org/abs/2507.23083)
*Ali Veisi,Delaram Fartoot,Hamidreza Amirzadeh*

Main category: cs.CL

TL;DR: CARoPE，一种上下文感知的旋转位置编码变体，通过动态生成与token相关的频率模式，显著提升了Transformer模型在长上下文下的性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 旋转位置编码（RoPE）因其兼容性和效率被广泛采用，但其依赖静态、与输入无关的正弦频率模式，限制了其建模上下文敏感关系的能力。研究旨在解决RoPE在上下文感知方面的不足。

Method: 提出CARoPE（Context-Aware Rotary Positional Embedding），RoPE的一种新颖泛化。CARoPE通过对token嵌入进行有界变换，动态生成基于token和上下文的头部特定频率模式和输入依赖的相位偏移，并将其集成到注意力头部的旋转机制中，同时保持RoPE的效率和架构简洁性。

Result: 在FineWeb-Edu-10B数据集上，使用GPT-2变体进行下一词预测任务的评估显示，CARoPE持续优于RoPE和其他常见位置编码基线，即使在更长的上下文长度下也能实现显著更低的困惑度。此外，CARoPE还能在不牺牲模型稳定性的情况下实现更快的训练吞吐量。

Conclusion: CARoPE为Transformer模型现有的位置编码策略提供了一种可扩展、富有表现力且高效的升级方案。

Abstract: Positional encoding is a vital component of Transformer architectures,
enabling models to incorporate sequence order into self-attention mechanisms.
Rotary Positional Embeddings (RoPE) have become a widely adopted solution due
to their compatibility with relative position encoding and computational
efficiency. However, RoPE relies on static, input-independent sinusoidal
frequency patterns, limiting its ability to model context-sensitive
relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional
Embedding), a novel generalization of RoPE that dynamically generates
head-specific frequency patterns conditioned on token embeddings. This design
introduces token- and context-sensitive positional representations while
preserving RoPE efficiency and architectural simplicity. CARoPE computes
input-dependent phase shifts using a bounded transformation of token embeddings
and integrates them into the rotary mechanism across attention heads. We
evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on
next-token prediction tasks. Experimental results show that CARoPE consistently
outperforms RoPE and other common positional encoding baselines, achieving
significantly lower perplexity, even at longer context lengths. Additionally,
CARoPE enables faster training throughput without sacrificing model stability.
These findings demonstrate that CARoPE offers a scalable, expressive, and
efficient upgrade to existing positional encoding strategies in Transformer
models.

</details>


### [39] [SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity](https://arxiv.org/abs/2507.23095)
*Ishani Mondal,Meera Bharadwaj,Ayush Roy,Aparna Garimella,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 提出SMART-Editor框架，通过奖励引导策略（Reward-Refine和RewardDPO）实现跨结构化和非结构化领域的合成布局和内容编辑，并保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在进行组合性布局和内容编辑时，仅能进行局部修改，难以保持全局一致性。

Method: 开发SMART-Editor框架，采用两种策略：1) Reward-Refine（推理时奖励引导细化）；2) RewardDPO（训练时偏好优化，基于奖励对齐布局对）。同时，引入SMARTEdit-Bench基准进行多领域、级联编辑场景的评估。

Result: SMART-Editor在性能上优于InstructPix2Pix和HIVE等基线模型。RewardDPO在结构化设置中实现高达15%的增益，Reward-Refine在自然图像上表现出优势。自动和人工评估均证实了奖励引导规划在生成语义一致和视觉对齐的编辑中的价值。

Conclusion: 奖励引导规划是实现语义一致和视觉对齐的组合性图像编辑的关键，SMART-Editor及其奖励引导策略有效解决了全局一致性问题。

Abstract: We present SMART-Editor, a framework for compositional layout and content
editing across structured (posters, websites) and unstructured (natural images)
domains. Unlike prior models that perform local edits, SMART-Editor preserves
global coherence through two strategies: Reward-Refine, an inference-time
rewardguided refinement method, and RewardDPO, a training-time preference
optimization approach using reward-aligned layout pairs. To evaluate model
performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,
cascading edit scenarios. SMART-Editor outperforms strong baselines like
InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in
structured settings and Reward-Refine showing advantages on natural images.
Automatic and human evaluations confirm the value of reward-guided planning in
producing semantically consistent and visually aligned edits.

</details>


### [40] [RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL](https://arxiv.org/abs/2507.23104)
*Jeffrey Eben,Aitzaz Ahmad,Stephen Lau*

Main category: cs.CL

TL;DR: 本文提出一种无需微调的组件式检索架构，旨在解决大型语言模型（LLM）驱动的数据库自然语言接口在企业级数据目录中的扩展性挑战，实现了高效准确的Text-to-SQL。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的数据库自然语言接口有所发展，但将其扩展到企业级数据目录仍是一个未充分探索的挑战。现有方法依赖于领域特定的微调（使部署复杂），且未能有效利用数据库元数据中重要的语义上下文。

Method: 引入了一种组件式检索架构，该架构将数据库模式和元数据分解为离散的语义单元，每个单元独立索引以进行定向检索。此方法优先考虑有效的表识别，同时利用列级信息，并确保检索到的表数量保持在可管理的上下文预算内。

Result: 实验证明，所提出的方法在具有不同结构和可用元数据的大型数据库上保持了高召回率和准确性，且其系统性能优于现有基线方法。

Conclusion: 该解决方案支持在多种企业环境中部署实用的Text-to-SQL系统，无需专门的微调，从而弥补了自然语言数据库接口中的关键扩展性差距。

Abstract: Despite advances in large language model (LLM)-based natural language
interfaces for databases, scaling to enterprise-level data catalogs remains an
under-explored challenge. Prior works addressing this challenge rely on
domain-specific fine-tuning - complicating deployment - and fail to leverage
important semantic context contained within database metadata. To address these
limitations, we introduce a component-based retrieval architecture that
decomposes database schemas and metadata into discrete semantic units, each
separately indexed for targeted retrieval. Our approach prioritizes effective
table identification while leveraging column-level information, ensuring the
total number of retrieved tables remains within a manageable context budget.
Experiments demonstrate that our method maintains high recall and accuracy,
with our system outperforming baselines over massive databases with varying
structure and available metadata. Our solution enables practical text-to-SQL
systems deployable across diverse enterprise settings without specialized
fine-tuning, addressing a critical scalability gap in natural language database
interfaces.

</details>


### [41] [Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity](https://arxiv.org/abs/2507.23121)
*Xinwei Wu,Haojie Li,Hongyu Liu,Xinyu Ji,Ruohan Li,Yule Chen,Yigeng Zhang*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）在处理中文歧义文本时的可靠性，发现LLMs在此方面表现出显著的脆弱性，包括无法识别歧义、过度自信和过度思考。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在面对歧义叙事文本（特别是中文文本歧义）时的行为，以评估其可信度。

Method: 创建了一个基准数据集，包含带上下文的歧义句子及其对应的多种消歧解释。这些标注示例被系统地分为3个主要类别和9个子类别，并用于进行实验。

Result: 实验发现LLMs在处理歧义时表现出显著的脆弱性，且行为与人类截然不同。具体而言，LLMs不能可靠地区分歧义和非歧义文本，在解释歧义文本时过度自信地认为只有单一含义，并且在试图理解多种可能含义时表现出过度思考。

Conclusion: 当前LLMs在处理语言歧义方面存在根本性局限，这对其在语言歧义普遍存在的现实世界应用中的部署具有重要影响，因此亟需改进处理语言理解中不确定性的方法。数据集和代码已公开。

Abstract: In this work, we study a critical research problem regarding the
trustworthiness of large language models (LLMs): how LLMs behave when
encountering ambiguous narrative text, with a particular focus on Chinese
textual ambiguity. We created a benchmark dataset by collecting and generating
ambiguous sentences with context and their corresponding disambiguated pairs,
representing multiple possible interpretations. These annotated examples are
systematically categorized into 3 main categories and 9 subcategories. Through
experiments, we discovered significant fragility in LLMs when handling
ambiguity, revealing behavior that differs substantially from humans.
Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous
text, show overconfidence in interpreting ambiguous text as having a single
meaning rather than multiple meanings, and exhibit overthinking when attempting
to understand the various possible meanings. Our findings highlight a
fundamental limitation in current LLMs that has significant implications for
their deployment in real-world applications where linguistic ambiguity is
common, calling for improved approaches to handle uncertainty in language
understanding. The dataset and code are publicly available at this GitHub
repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.

</details>


### [42] [ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans](https://arxiv.org/abs/2507.23135)
*Ananya Sadana,Yash Kumar Lal,Jiawei Zhou*

Main category: cs.CL

TL;DR: 本研究引入ISO-Bench基准测试，评估多模态模型在视觉观察和程序文本之间的因果关系推理能力，发现现有前沿模型表现不佳，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在现实环境中理解跨模态因果关系是一个核心挑战。

Method: 引入ISO-Bench基准，用于评估模型是否能推断视觉观察和程序文本之间的因果依赖关系。基准中的每个例子包含一个任务步骤的图像和一个计划中的文本片段，模型需要判断视觉步骤是发生在引用文本步骤之前还是之后。

Result: 对十个前沿视觉-语言模型的评估结果显示，它们的性能令人失望：最佳零样本F1分数仅为0.57；思维链推理仅带来有限提升（最高F1为0.62），远低于人类表现（F1为0.98）。

Conclusion: 本研究的分析指出了改进多模态模型因果理解的具体方向。

Abstract: Understanding causal relationships across modalities is a core challenge for
multimodal models operating in real-world environments. We introduce ISO-Bench,
a benchmark for evaluating whether models can infer causal dependencies between
visual observations and procedural text. Each example presents an image of a
task step and a text snippet from a plan, with the goal of deciding whether the
visual step occurs before or after the referenced text step. Evaluation results
on ten frontier vision-language models show underwhelming performance: the best
zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest
gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further
highlights concrete directions for improving causal understanding in multimodal
models.

</details>


### [43] [User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal](https://arxiv.org/abs/2507.23158)
*Yuhan Liu,Michael J. Q. Zhang,Eunsol Choi*

Main category: cs.CL

TL;DR: 本研究探讨如何从用户与语言模型（LM）的交互日志中获取隐式用户反馈，以实现模型的持续进化，并分析了其潜在效用与局限性。


<details>
  <summary>Details</summary>
Motivation: 部署后的语言模型需要基于用户反馈持续改进，但直接征求反馈具有干扰性。因此，研究旨在从用户-LM交互日志中自动获取隐式用户反馈。

Method: 研究人员在WildChat和LMSYS两个用户-LLM交互数据集上，首先分析了用户反馈在对话轨迹中出现的时间和原因。其次，他们研究了如何从这些隐式用户反馈中提取学习信号。

Result: 研究发现，用户反馈的“内容”（例如用户寻求澄清）而非仅仅“极性”（例如用户不满意）能提升模型在短小、人工设计问题（MTBench）上的表现，但对更长、更复杂的问题（WildBench）则无显著提升。此外，用户反馈的有用性与用户初始提示的质量高度相关。

Conclusion: 本研究对隐式用户反馈进行了深入探讨，揭示了其在提升语言模型性能方面的潜力和局限性。

Abstract: Once language models (LMs) are deployed, they can interact with users
long-term, ideally evolving continuously based on their feedback. Asking for
direct user feedback can be disruptive; thus, we study harvesting user feedback
from user-LM interaction logs. We study implicit user feedback in two user-LM
interaction datasets (WildChat and LMSYS). First, we analyze user feedback in
the user-LLM conversation trajectory, providing insights into when and why such
feedback occurs. Second, we study harvesting learning signals from such
implicit user feedback. We find that the contents of user feedback (e.g., user
wanted clarification), not just the polarity (e.g., users were unhappy with the
previous model response), can improve model performance in short human-designed
questions (MTBench) but not on longer and more complex questions (WildBench).
We also find that the usefulness of user feedback is largely tied to the
quality of the user's initial prompt. Together, we provide an in-depth study of
implicit user feedback, showing its potential and limitations.

</details>


### [44] [LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration](https://arxiv.org/abs/2507.23167)
*Jizhou Guo*

Main category: cs.CL

TL;DR: 本文提出LENS方法，通过分析大语言模型（LLM）的内部表征来估计其置信度，从而改进LLM集成学习效果，并在问答任务上显著优于传统集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）集成方法，如投票或logits集成，过于简单，未能考虑模型在不同情境下置信度和可靠性的差异，导致集成效果受限。

Method: 提出LENS（Learning ENsemble confidence from Neural States）方法，通过分析LLM的内部表征来学习估计模型置信度。具体而言，为每个LLM训练一个轻量级的线性置信度预测器，该预测器以层级隐藏状态和归一化概率为输入，以实现基于上下文可靠性的细致权重分配。此方法不修改模型参数，计算开销可忽略。

Result: 在多项选择题和布尔问答任务上的实验结果表明，LENS显著优于传统的集成方法。

Conclusion: 研究发现，大语言模型的内部表征为判断模型置信度提供了有价值的信号，并且可以有效地应用于集成学习。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, with different models excelling in distinct domains and specific
abilities. Effectively combining the predictions of multiple LLMs is crucial
for enhancing system robustness and performance. However, existing ensemble
methods often rely on simple techniques like voting or logits ensembling, which
overlook the varying confidence and reliability of models in different
contexts. In this work, we propose LENS (Learning ENsemble confidence from
Neural States), a novel approach that learns to estimate model confidence by
analyzing internal representations. For each LLM, we train a lightweight linear
confidence predictor that leverages layer-wise hidden states and normalized
probabilities as inputs. This allows for more nuanced weighting of model
predictions based on their context-dependent reliability. Our method does not
require modifying the model parameters and requires negligible additional
computation. Experimental results on multiple-choice and boolean
question-answering tasks demonstrate that LENS outperforms traditional ensemble
methods by a substantial margin. Our findings suggest that internal
representations provide valuable signals for determining model confidence and
can be effectively leveraged for ensemble learning.

</details>


### [45] [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)
*Jianghui Wang,Vinay Joshi,Saptarshi Majumder,Xu Chao,Bin Ding,Ziqiong Liu,Pratik Prabhanjan Brahma,Dong Li,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 开发了GEAK框架，利用大型语言模型为AMD GPU自动生成高性能Triton内核，显著提升了正确性和执行速度。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习工作负载日益复杂和多样化，自动化低级GPU内核开发对于满足性能和生产力需求至关重要。行业和学术界对AI生成GPU内核的需求迅速增长，旨在减少手动优化工作并实现接近专家级的硬件性能。

Method: 本文提出了一个针对Triton基GPU内核的评估套件，并开发了GEAK（Generating Efficient AI-centric GPU Kernels）框架。GEAK利用尖端的大型语言模型（LLMs）为AMD GPU（包括MI300X和MI250）生成高性能的Triton代码，其核心是通过借鉴Reflexion式反馈机制的推理循环来实现推理时计算扩展。

Result: 在两个评估基准上，GEAK的正确性达到了63%，执行速度提升了高达2.59倍，显著优于直接提示LLM和基于Reflexion的生成管道基线。

Conclusion: GEAK类似的代码生成代理技术前景广阔，有望加速多样化硬件平台的采用，并普及专家级内核性能。

Abstract: The demand for AI-generated GPU kernels is rapidly growing, influenced by the
need for scalable, hardware-optimized solutions in both industry and academia.
As deep learning workloads grow in complexity and diversity, it is imperative
to automate low-level kernel development to meet performance and productivity
demands. Major cloud providers, semiconductor companies, and research
institutions are now investing heavily in AI-driven code generation for GPUs,
aiming to reduce manual optimization efforts while achieving near-expert
performance on hardware like AMD MI300X. The Triton language, a Python-based
DSL for GPU programming, has emerged as a popular target for such AI-generated
kernels due to its balance of performance and ease-of-coding. In this work, we
present an evaluation suite for Triton-based GPU kernels and GEAK (Generating
Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs
to generate performant Triton code specifically for AMD GPUs, including the AMD
MI300X and MI250. GEAK leverages inference-time compute scaling to produce
Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style
feedback mechanisms. On two evaluation benchmarks, GEAK significantly
outperformed the baselines of directly prompting frontier LLMs as well as
Reflexion-based generation pipelines by achieving correctness up to $63$% and
execution speed up of up to $2.59$X. These results highlight the promise of
GEAK-like agentic code generation for accelerating the adoption of diverse
hardware platforms and democratizing access to expert-level kernel performance.

</details>


### [46] [Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples](https://arxiv.org/abs/2507.23211)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Zhe Cui*

Main category: cs.CL

TL;DR: 本研究提出一种新颖方法，利用负样本的信息来更好地选择正样本，从而提升大语言模型少样本情境学习（ICL）的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的少样本情境学习性能高度依赖于提供的例子。现有研究多侧重于检索正样本以提高效率和减少偏差，但忽略了负样本中蕴含的额外信息。

Method: 首先，基于Zero-Shot-Cot构建正样本和负样本语料库。推理时，基于语义相似度从正负语料库中选择最相似的例子。随后，进一步根据负样本与正样本的语义相似度来检索额外的正样本。最后，将所有选定的正样本拼接作为ICL演示。

Result: 实验结果表明，该方法优于仅依赖于最相似正样本进行情境学习的方法。

Conclusion: 负样本的额外信息有助于通过改进正样本选择来增强少样本情境学习的性能。

Abstract: Large Language Models exhibit powerful few-shot in-context learning (ICL)
capabilities, but the performance is highly sensitive to provided examples.
  Recent research has focused on retrieving corresponding examples for each
input query, not only enhancing the efficiency and scalability of the learning
process but also mitigating inherent biases in manual example selection.
  However, these studies have primarily emphasized leveraging Positive samples
while overlooking the additional information within Negative samples for
contextual learning.
  We propose a novel method that utilizes Negative samples to better select
Positive sample examples, thereby enhancing the performance of few-shot ICL.
Initially, we construct Positive and Negative sample corpora based on
Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based
approach to select the most similar examples from both the Positive and
Negative corpora for a given query. Subsequently, we further retrieve Positive
examples from the Positive sample corpus based on semantic similarity to the
Negative examples, then concatenating them with the previously selected
Positive examples to serve as ICL demonstrations. Experimental results
demonstrate that our approach surpasses methods solely relying on the most
similar positive examples for context, validating that the additional
information in negative samples aids in enhancing ICL performance through
improved Positive sample selection.

</details>


### [47] [Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders](https://arxiv.org/abs/2507.23220)
*Carolina Zheng,Nicolas Beltran-Velez,Sweta Karlekar,Claudia Shi,Achille Nazaret,Asif Mallik,Amir Feder,David M. Blei*

Main category: cs.CL

TL;DR: 本文提出机制主题模型(MTMs)，它利用稀疏自编码器学习的语义丰富特征来揭示深层概念主题，并支持可控文本生成。通过LLM辅助评估，MTMs在主题一致性和用户偏好上均优于或媲美现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型依赖词袋表示，难以捕捉语义抽象特征；现有神经变体虽用更丰富表示，但仍将主题表达为词列表，限制了对复杂主题的阐述能力。

Method: 引入机制主题模型(MTMs)，该模型在稀疏自编码器(SAEs)学习到的可解释特征空间上操作并定义主题。为评估MTMs，提出名为“topic judge”的基于大型语言模型(LLM)的成对比较评估框架。

Result: 在五个数据集上，MTMs在一致性指标上达到或超越传统及神经基线；持续获得“topic judge”的偏好；并且能够有效地引导LLM的输出。

Conclusion: MTMs通过在语义丰富的特征空间上定义主题，克服了现有主题模型在捕捉抽象语义和表达复杂主题方面的限制，提供了更深层次的概念洞察，并首次实现了基于主题向量的可控文本生成。

Abstract: Traditional topic models are effective at uncovering latent themes in large
text collections. However, due to their reliance on bag-of-words
representations, they struggle to capture semantically abstract features. While
some neural variants use richer representations, they are similarly constrained
by expressing topics as word lists, which limits their ability to articulate
complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic
models that operate on interpretable features learned by sparse autoencoders
(SAEs). By defining topics over this semantically rich space, MTMs can reveal
deeper conceptual themes with expressive feature descriptions. Moreover,
uniquely among topic models, MTMs enable controllable text generation using
topic-based steering vectors. To properly evaluate MTM topics against
word-list-based approaches, we propose \textit{topic judge}, an LLM-based
pairwise comparison evaluation framework. Across five datasets, MTMs match or
exceed traditional and neural baselines on coherence metrics, are consistently
preferred by topic judge, and enable effective steering of LLM outputs.

</details>


### [48] [Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs](https://arxiv.org/abs/2507.23227)
*Sophie Kearney,Shu Yang,Zixuan Wen,Bojian Hou,Duy Duong-Tran,Tianlong Chen,Jason Moore,Marylyn Ritchie,Li Shen*

Main category: cs.CL

TL;DR: TAP-GPT是一个新颖的框架，它将TableGPT2（一种多模态表格专用LLM）应用于阿尔茨海默病（AD）的早期诊断，通过少量样本的表格生物标志物数据实现，并表现优异。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期准确诊断需要分析异构生物标志物数据，这些数据通常以表格形式呈现。大型语言模型（LLMs）在处理结构化生物医学数据方面，因其少量样本推理、多模态集成和自然语言可解释性而展现出巨大潜力。

Method: 提出TAP-GPT框架，该框架将为商业智能任务开发的TableGPT2（一种多模态表格专用LLM）适应于AD诊断。具体方法包括：利用上下文学习从结构化生物医学数据构建少量样本的表格提示，并使用参数高效的qLoRA适应技术对TableGPT2进行微调，以执行AD或认知正常（CN）的临床二分类任务。

Result: TAP-GPT框架利用TableGPT2强大的表格理解能力和LLM编码的先验知识，在AD诊断任务中，其性能优于更先进的通用LLM以及为预测任务开发的表格基础模型（TFM）。

Conclusion: 该研究首次将LLM应用于使用表格生物标志物数据进行预测任务，为未来生物医学信息学中LLM驱动的多智能体框架奠定了基础。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex
neurodegenerative disorder, requires analysis of heterogeneous biomarkers
(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal
fluid proteins) typically represented in a tabular format. With flexible
few-shot reasoning, multimodal integration, and natural-language-based
interpretability, large language models (LLMs) offer unprecedented
opportunities for prediction with structured biomedical data. We propose a
novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts
TableGPT2, a multimodal tabular-specialized LLM originally developed for
business intelligence tasks, for AD diagnosis using structured biomarker data
with small sample sizes. Our approach constructs few-shot tabular prompts using
in-context learning examples from structured biomedical data and finetunes
TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary
classification task of AD or cognitively normal (CN). The TAP-GPT framework
harnesses the powerful tabular understanding ability of TableGPT2 and the
encoded prior knowledge of LLMs to outperform more advanced general-purpose
LLMs and a tabular foundation model (TFM) developed for prediction tasks. To
our knowledge, this is the first application of LLMs to the prediction task
using tabular biomarker data, paving the way for future LLM-driven multi-agent
frameworks in biomedical informatics.

</details>


### [49] [P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication](https://arxiv.org/abs/2507.23247)
*Sneha Oram,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 心理健康领域LLM语用推理能力及污名化处理研究。


<details>
  <summary>Details</summary>
Motivation: 尽管心理健康聊天机器人的可解释性与开发有所进展，但在推理和对话语篇，特别是语用推理方面仍缺乏探索。

Method: 引入P-ReMe数据集，并修正定义了心理健康中的隐含意义和预设，以此构建相关任务。使用Llama3.1、Mistral、MentaLLaMa和Qwen进行基准测试。此外，提出StiPRompts，并评估GPT-4o mini、Deepseek-chat和Claude-3.5-haiku处理心理健康污名化的能力。

Result: Mistral和Qwen在语用推理任务中展现出显著能力。在处理心理健康污名化方面，Claude-3.5-haiku比其他模型表现得更负责任。

Conclusion: 本研究首次探索了大型语言模型在心理健康领域的语用推理能力和污名化处理，并揭示了特定模型在该领域的潜力和负责任的表现。

Abstract: There has been an increase in recent advancements in the explainability and
development of personalized chatbots for mental health. However, the reasoning
aspects for explainability and dialogue discourse have not been explored
previously for mental health. Hence, we are investigating the pragmatic
reasoning capability of large language models (LLMs) in this domain. We
introduce P-ReMe dataset, and propose a modified definition for the pragmatic
phenomena of implicature (implied meaning) and presupposition (implicit
assumption) in mental health. Following the definition, we formulate two tasks
in implicature and one task in presupposition. To benchmark the dataset and the
presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and
Qwen. The results of the experiments suggest that Mistral and Qwen show
substantial reasoning capabilities in the domain. In addition, we also propose
StiPRompts to study the stigma around mental health with the state-of-the-art
LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings
show that Claude-3.5-haiku deals with the stigma more responsibly compared to
the other two LLMs.

</details>


### [50] [Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis](https://arxiv.org/abs/2507.23248)
*Shimanto Bhowmik,Tawsif Tashwar Dipto,Md Sazzad Islam,Sheryl Hsu,Tahsin Reasat*

Main category: cs.CL

TL;DR: 本研究分析了孟加拉语NLP面临的挑战，评估了10个LLM在该语言上的表现，发现性能差距与分词效率相关，并强调改进数据集和评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在NLP研究中代表性不足，面临独特的语言结构和计算限制挑战。此外，缺乏标准化评估基准阻碍了其NLP性能，促使研究人员系统性地调查这些挑战。

Method: 研究人员评估了10个最新的开源大型语言模型（LLMs），在8个翻译后的数据集上进行了测试，并进行了全面的错误分析以确定它们主要的失败模式。

Result: 孟加拉语模型性能与英语相比存在持续差距，尤其在小型模型和Mistral等特定模型家族中更为明显。DeepSeek等某些架构在跨语言表现上展现出良好的鲁棒性。分析还揭示了分词效率与LLM准确性之间存在负相关关系：过度分词会降低性能，而更高效简洁的分词则能提升性能。

Conclusion: 当前模型在处理孟加拉语时存在不足，亟需改进多语言环境下的数据集质量和评估方法。本研究将有助于催化对代表性不足语言的NLP研究，从而促进全球范围内先进语言技术的普及。

Abstract: Bengali is an underrepresented language in NLP research. However, it remains
a challenge due to its unique linguistic structure and computational
constraints. In this work, we systematically investigate the challenges that
hinder Bengali NLP performance by focusing on the absence of standardized
evaluation benchmarks. We then evaluated 10 recent open source Large Language
Models (LLMs) in 8 of the translated datasets and performed a comprehensive
error analysis to pinpoint their primary failure modes. Our findings reveal
consistent performance gaps for Bengali compared to English, particularly for
smaller models and specific model families like Mistral. We also identified
promising robustness in certain architectures, such as DeepSeek, that maintain
more stable performance across languages. Our analysis reveals an inverse
relationship between tokenization efficiency and LLM accuracy where models tend
to perform worse when inputs are excessively tokenized, whereas more efficient
\& concise tokenization results in improved performance. These findings
highlight critical areas where current models fall short and underscore the
need for improved dataset quality and evaluation methodologies tailored to
multilingual contexts. This work will catalyze further research on NLP for
underrepresented languages, helping to democratize access to advanced language
technologies worldwide. The code and dataset used in this research is publicly
available at https://github.com/BengaliAI/bn-llm-benchmark.

</details>


### [51] [Unveiling Super Experts in Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2507.23279)
*Zunhai Su,Qingyuan Li,Hao Zhang,YuLei Qian,Yuchen Xie,Kehong Yuan*

Main category: cs.CL

TL;DR: 该研究首次发现并深入分析了MoE大模型中一类名为“超级专家”（Super Experts, SEs）的关键专家，它们数量虽少但对模型性能（尤其数学推理）至关重要，并揭示了其通过影响注意力汇点来发挥作用的机制。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE专家压缩方法多依赖经验准则识别关键专家，但缺乏对专家异质性重要性的深入理解，未能有效识别模型运作中真正扮演核心角色的专家，导致压缩效果受限。

Method: 本研究通过对MoE大模型进行系统性分析，首次识别并深入探究了“超级专家”（SEs）。具体方法包括：分析SEs在`down_proj`输出中的激活特征和对隐藏状态的影响；通过剪枝SEs来评估其对各种任务（特别是数学推理）性能的显著影响；以及进一步探究SEs对注意力机制（注意力汇点）的影响及其被剪枝后的破坏效应。

Result: 研究发现：(i) SEs是MoE大模型中普遍存在的一小部分专家，其特点是`down_proj`输出中存在稀有但极端的激活异常值，导致解码器层间隐藏状态的巨大激活，且其分布是模型特有且不受训练后处理影响。(ii) 剪枝SEs会导致模型性能显著下降，尤其在数学推理任务中影响巨大。(iii) MoE大模型依赖SEs诱导注意力汇点，这对注意力分数分布至关重要，而剪枝SEs会严重破坏这一机制。

Conclusion: 本研究首次揭示了MoE大模型中“超级专家”的存在及其关键作用，深入阐明了它们在模型前向推理中的独特激活模式、对整体性能的深远影响，以及它们通过诱导注意力汇点来维持注意力分布的关键机制。这些发现为未来MoE模型的理解和高效压缩提供了新的视角。

Abstract: Sparsely activated Mixture-of-Experts (MoE) models have shown promise in
enhancing the learning capacity of large language models (LLMs). Leveraging the
intrinsic importance differences among experts, recent research has explored
expert-level compression techniques to improve the efficiency of MoE LLMs.
However, existing approaches often rely on empirical criteria to identify
critical experts, lacking a deeper exploration and understanding of the
heterogeneous importance of experts. In this study, we present the first
discovery and investigation of a distinct subset of experts that play a crucial
role in the underlying mechanisms during the model's forward inference. These
experts are prevalent in open-source MoE LLMs, and despite their limited
number, pruning them leads to a significant decline in model performance (e.g.,
pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative
outputs). We refer to these experts as Super Experts (SEs). Our comprehensive
analysis provides progressively deeper insights into SEs. (i) SEs are
characterized by rare but extreme activation outliers in the output of the
down_proj, which give rise to massive activations in the hidden states between
decoder layers. Moreover, the distribution of SEs remains model-specific and is
unaffected by post-training processes. (ii) By pruning SEs, we assess their
significance across a variety of tasks, revealing their considerable impact on
the model's overall performance, particularly in mathematical reasoning. (iii)
We further enhance our understanding of the influence of SEs compression. Our
findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are
crucial for the distribution of attention scores but are significantly
disrupted by SE pruning. The code is available at
https://github.com/ZunhaiSu/Super-Experts-Profilling.

</details>


### [52] [What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content](https://arxiv.org/abs/2507.23319)
*Alfio Ferrara,Sergio Picascia,Laura Pinnavaia,Vojimir Ranitovic,Elisabetta Rocchetti,Alice Tuveri*

Main category: cs.CL

TL;DR: 研究发现GPT-4o-mini在复述敏感内容时会进行隐式审查，降低内容敏感度，并评估了LLMs的零样本敏感性分类能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注大型语言模型（LLMs）的显式内容审查训练，但对于LLMs在无明确指令下是否会隐式净化语言的探索有限。

Method: 本研究经验性分析了GPT-4o-mini在复述敏感内容时的隐式审查行为，评估其敏感度变化的程度。同时，还评估了LLMs在零样本敏感性分类任务上的能力，并与传统方法进行比较。

Result: 实验结果表明，GPT-4o-mini系统性地将内容审核至较低敏感类别，显著减少了贬损性和禁忌语言。此外，还评估了LLMs在句子敏感性零样本分类上的表现，并与传统方法进行了比较。

Conclusion: GPT-4o-mini在处理敏感内容时展现出明显的隐式审查行为，能够有效降低内容的敏感度。同时，LLMs在零样本敏感性分类方面也具备一定的能力。

Abstract: Proprietary Large Language Models (LLMs) have shown tendencies toward
politeness, formality, and implicit content moderation. While previous research
has primarily focused on explicitly training models to moderate and detoxify
sensitive content, there has been limited exploration of whether LLMs
implicitly sanitize language without explicit instructions. This study
empirically analyzes the implicit moderation behavior of GPT-4o-mini when
paraphrasing sensitive content and evaluates the extent of sensitivity shifts.
Our experiments indicate that GPT-4o-mini systematically moderates content
toward less sensitive classes, with substantial reductions in derogatory and
taboo language. Also, we evaluate the zero-shot capabilities of LLMs in
classifying sentence sensitivity, comparing their performances against
traditional methods.

</details>


### [53] [MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation](https://arxiv.org/abs/2507.23334)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: MusT-RAG框架利用RAG和专业音乐数据库，显著提升LLM在音乐问答任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在通用任务上表现出色，但由于缺乏音乐领域特定知识，其在音乐相关应用中的有效性受限。

Method: 提出MusT-RAG框架，该框架基于检索增强生成（RAG）技术，旨在将通用LLM应用于文本音乐问答（MQA）任务。具体方法包括：1) 构建音乐专业向量数据库MusWikiDB用于检索阶段；2) 在推理和微调过程中利用上下文信息，以有效转换通用LLM为音乐专用模型。

Result: MusT-RAG在增强LLM音乐领域适应能力方面显著优于传统微调方法，在域内和域外MQA基准测试中均表现出持续改进。此外，MusWikiDB比通用维基百科语料库更有效，性能更优，计算效率更高。

Conclusion: MusT-RAG框架及其提出的MusWikiDB成功解决了LLM在音乐领域知识不足的问题，显著提升了其在音乐问答任务上的性能和领域适应能力。

Abstract: Recent advancements in Large language models (LLMs) have demonstrated
remarkable capabilities across diverse domains. While they exhibit strong
zero-shot performance on various tasks, LLMs' effectiveness in music-related
applications remains limited due to the relatively small proportion of
music-specific knowledge in their training data. To address this limitation, we
propose MusT-RAG, a comprehensive framework based on Retrieval Augmented
Generation (RAG) to adapt general-purpose LLMs for text-only music question
answering (MQA) tasks. RAG is a technique that provides external knowledge to
LLMs by retrieving relevant context information when generating answers to
questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a
music-specialized vector database for the retrieval stage, and (2) utilizes
context information during both inference and fine-tuning processes to
effectively transform general-purpose LLMs into music-specific models. Our
experiment demonstrates that MusT-RAG significantly outperforms traditional
fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,
showing consistent improvements across both in-domain and out-of-domain MQA
benchmarks. Additionally, our MusWikiDB proves substantially more effective
than general Wikipedia corpora, delivering superior performance and
computational efficiency.

</details>


### [54] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
*Renato Vukovic,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Hsien-Chin Lin,Shutong Feng,Nurul Lubis,Milica Gasic*

Main category: cs.CL

TL;DR: 提出TeQoDO，一种LLM无需监督构建任务导向对话本体的新方法，旨在提升LLM的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因依赖参数化知识而缺乏可解释性和可信度。任务导向对话（TOD）系统使用外部本体来保证可解释性，但构建这些本体通常需要大量人工标注或监督训练，效率低下。

Method: 引入TeQoDO方法，该方法利用LLM固有的SQL编程能力，结合预设的对话理论，自主且无需监督地从零开始构建TOD本体。它是一种Text-to-SQL的任务导向对话本体构建方法。

Result: TeQoDO的表现优于迁移学习方法，其构建的本体在下游对话状态跟踪任务中具有竞争力。消融研究证明了对话理论的关键作用。此外，TeQoDO能扩展构建更大型的本体（已在Wikipedia和ArXiv数据集上验证）。

Conclusion: 该研究是朝着更广泛应用本体以增强LLM可解释性迈出的重要一步。

Abstract: Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.

</details>


### [55] [MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models](https://arxiv.org/abs/2507.23382)
*Yiyan Ji,Haoran Chen,Qiguang Chen,Chengyue Wu,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 该研究提出了MPCC基准，用于评估多模态大语言模型（MLLMs）在包含复杂多模态约束的真实世界规划任务中的能力。实验结果显示，当前MLLMs在此类任务中表现不佳，对约束复杂性高度敏感，且传统多模态提示策略失效。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态规划基准无法直接评估MLLMs处理真实世界规划任务的能力，并且缺乏对跨模态复杂约束的系统性评估，这限制了对模型在复杂推理和决策中处理多模态上下文能力的理解。

Method: 研究引入了名为MPCC（Multimodal Planning with Complex Constraints）的基准。该基准包含航班规划、日历规划和会议规划三类真实世界任务，并引入了预算、时间、空间等复杂约束，设置了EASY、MEDIUM、HARD等难度等级。通过对13个先进MLLMs在该基准上进行实验，评估其规划能力。

Result: 实验结果显示，当前MLLMs在处理复杂多模态约束的规划任务时面临显著挑战：闭源模型可行计划达成率仅为21.3%，开源模型平均低于11%。此外，MLLMs对约束复杂性高度敏感，并且传统的​​多模态提示策略在多约束场景中失效。

Conclusion: 该工作正式化了规划中的多模态约束，提供了一个严格的评估框架。研究结果强调了在真实世界MLLM应用中，迫切需要提升模型对约束感知推理能力的进展。

Abstract: Multimodal planning capabilities refer to the ability to predict, reason, and
design steps for task execution with multimodal context, which is essential for
complex reasoning and decision-making across multiple steps. However, current
benchmarks face two key challenges: (1) they cannot directly assess multimodal
real-world planning capabilities, and (2) they lack constraints or implicit
constraints across modalities. To address these issues, we introduce Multimodal
Planning with Complex Constraints (MPCC), the first benchmark to systematically
evaluate MLLMs' ability to handle multimodal constraints in planning. To
address the first challenge, MPCC focuses on three real-world tasks: Flight
Planning, Calendar Planning, and Meeting Planning. To solve the second
challenge, we introduce complex constraints (e.g. budget, temporal, and
spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to
separate constraint complexity from search space expansion. Experiments on 13
advanced MLLMs reveal significant challenges: closed-source models achieve only
21.3% feasible plans, while open-source models average below 11%. Additionally,
we observe that MLLMs are highly sensitive to constraint complexity and that
traditional multimodal prompting strategies fail in multi-constraint scenarios.
Our work formalizes multimodal constraints in planning, provides a rigorous
evaluation framework, and highlights the need for advancements in
constraint-aware reasoning for real-world MLLM applications.

</details>


### [56] [Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models](https://arxiv.org/abs/2507.23386)
*Ailiang Lin,Zhuoyun Li,Kotaro Funakoshi*

Main category: cs.CL

TL;DR: Causal2Vec是一种为解码器专用大语言模型（LLM）设计的文本嵌入模型，通过引入预编码的“Contextual”令牌和改进的池化策略，在不改变模型架构或增加显著计算开销的情况下，显著提升了文本嵌入性能，并在MTEB基准测试中达到了SOTA，同时大幅减少了序列长度和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有利用解码器专用LLM构建嵌入模型的方法存在局限性：一是移除因果注意力掩码以实现双向注意力可能损害模型在预训练中获得的语义信息；二是领先的单向方法依赖额外输入文本来克服因果注意力的固有局限性，这不可避免地增加了计算成本。

Method: 该研究提出了Causal2Vec。具体方法包括：1) 使用一个轻量级BERT风格模型将输入文本预编码为一个单一的“Contextual”令牌；2) 将此“Contextual”令牌预置到LLM的输入序列中，使每个令牌即使不关注未来令牌也能捕获上下文信息；3) 为减轻“最后令牌池化”带来的近因偏差，并将“Contextual”令牌中编码的语义信息有效利用，将“Contextual”令牌和“EOS”令牌的最后隐藏状态拼接作为最终的文本嵌入。

Result: Causal2Vec在MTEB（大规模文本嵌入基准）上，在仅使用公开可用的检索数据集训练的模型中，实现了最先进的性能。与现有最佳方法相比，其所需的序列长度减少了高达85%，推理时间减少了高达82%。

Conclusion: Causal2Vec成功地提升了解码器专用LLM作为通用嵌入模型的性能，且无需改变其原始架构或引入显著计算开销。它提供了一种高效且高性能的文本嵌入解决方案。

Abstract: Decoder-only large language models (LLMs) are increasingly used to build
embedding models that effectively encode the semantic information of natural
language texts into dense vector representations for various embedding tasks.
However, many existing methods primarily focus on removing the causal attention
mask in LLMs to enable bidirectional attention, potentially undermining the
model's ability to extract semantic information acquired during pretraining.
Additionally, leading unidirectional approaches often rely on extra input text
to overcome the inherent limitations of causal attention, inevitably increasing
computational costs. In this work, we propose Causal2Vec, a general-purpose
embedding model tailored to enhance the performance of decoder-only LLMs
without altering their original architectures or introducing significant
computational overhead. Specifically, we first employ a lightweight BERT-style
model to pre-encode the input text into a single Contextual token, which is
then prepended to the LLM's input sequence, allowing each token to capture
contextualized information even without attending to future tokens.
Furthermore, to mitigate the recency bias introduced by last-token pooling and
help LLMs better leverage the semantic information encoded in the Contextual
token, we concatenate the last hidden states of Contextual and EOS tokens as
the final text embedding. In practice, Causal2Vec achieves state-of-the-art
performance on the Massive Text Embeddings Benchmark (MTEB) among models
trained solely on publicly available retrieval datasets, while reducing the
required sequence length by up to 85% and inference time by up to 82% compared
to best-performing methods.

</details>


### [57] [Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators](https://arxiv.org/abs/2507.23399)
*Peter Sandrini*

Main category: cs.CL

TL;DR: 本研究评估了本地部署的开源大型语言模型作为商业云端翻译解决方案的替代方案，旨在解决数据隐私和访问性问题。


<details>
  <summary>Details</summary>
Motivation: 商业云端AI聊天机器人在翻译领域存在数据隐私、安全和公平访问等问题，因此需要探索本地部署的免费语言模型作为可行替代方案。

Method: 本研究在基于CPU的平台上评估了三个开源语言模型的功能性能，并与商用在线聊天机器人进行比较，重点关注其可行性和功能表现，而非翻译质量。

Result: 研究表明，尽管本地部署存在挑战，但其在增强数据控制、改善隐私和降低对云服务依赖方面的优势显著，为个体译者和小型企业提供了有吸引力的替代方案。

Conclusion: 本研究旨在推动AI技术的民主化，使大型语言模型对更广泛的用户，特别是个人译者和小型企业，更具可访问性和实用性。

Abstract: The rapid proliferation of Large Language Models presents both opportunities
and challenges for the translation field. While commercial, cloud-based AI
chatbots have garnered significant attention in translation studies, concerns
regarding data privacy, security, and equitable access necessitate exploration
of alternative deployment models. This paper investigates the feasibility and
performance of locally deployable, free language models as a viable alternative
to proprietary, cloud-based AI solutions. This study evaluates three
open-source models installed on CPU-based platforms and compared against
commercially available online chat-bots. The evaluation focuses on functional
performance rather than a comparative analysis of human-machine translation
quality, an area already subject to extensive research. The platforms assessed
were chosen for their accessibility and ease of use across various operating
systems. While local deployment introduces its own challenges, the benefits of
enhanced data control, improved privacy, and reduced dependency on cloud
services are compelling. The findings of this study contribute to a growing
body of knowledge concerning the democratization of AI technology and inform
future research and development efforts aimed at making LLMs more accessible
and practical for a wider range of users, specifically focusing on the needs of
individual translators and small businesses.

</details>


### [58] [MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization](https://arxiv.org/abs/2507.23400)
*Yongbing Zhang,Fang Nan,Shengxiang Gao,Yuxin Huang,Kaiwen Tan,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本文提出MRGSEM-Sum，一个基于多关系图和结构熵最小化的无监督多文档摘要框架，通过整合语义和语篇关系，自动确定聚类数量，并采用位置感知压缩机制，在多个基准数据集上超越现有无监督方法，并可与有监督模型和大型语言模型媲美。


<details>
  <summary>Details</summary>
Motivation: 多文档摘要的核心挑战是文档间关系的复杂性和信息冗余。现有图聚类方法常只考虑单关系图，无法充分表示丰富的关系信息，且需要预定义聚类数量，阻碍了自适应地划分句子组以减少冗余。

Method: 1. 构建多关系图，整合句子间的语义和语篇关系，全面建模句子间的复杂动态连接。2. 应用二维结构熵最小化算法进行聚类，自动确定最佳聚类数量并将句子组织成连贯的组。3. 引入位置感知压缩机制，对每个簇进行提炼，生成简洁且信息丰富的摘要。

Result: 在四个基准数据集（Multi-News, DUC-2004, PubMed, WikiSum）上的大量实验表明，MRGSEM-Sum持续优于之前的无监督方法，在某些情况下性能可与有监督模型和大型语言模型媲美。人工评估显示，MRGSEM-Sum生成的摘要具有高一致性和覆盖率，接近人类水平。

Conclusion: MRGSEM-Sum通过利用多关系图和结构熵最小化，有效解决了多文档摘要中的核心挑战，实现了自适应的聚类和全面的关系建模，生成了高质量、接近人类水平的摘要，并在无监督框架下取得了显著的性能提升。

Abstract: The core challenge faced by multi-document summarization is the complexity of
relationships among documents and the presence of information redundancy. Graph
clustering is an effective paradigm for addressing this issue, as it models the
complex relationships among documents using graph structures and reduces
information redundancy through clustering, achieving significant research
progress. However, existing methods often only consider single-relational
graphs and require a predefined number of clusters, which hinders their ability
to fully represent rich relational information and adaptively partition
sentence groups to reduce redundancy. To overcome these limitations, we propose
MRGSEM-Sum, an unsupervised multi-document summarization framework based on
multi-relational graphs and structural entropy minimization. Specifically, we
construct a multi-relational graph that integrates semantic and discourse
relations between sentences, comprehensively modeling the intricate and dynamic
connections among sentences across documents. We then apply a two-dimensional
structural entropy minimization algorithm for clustering, automatically
determining the optimal number of clusters and effectively organizing sentences
into coherent groups. Finally, we introduce a position-aware compression
mechanism to distill each cluster, generating concise and informative
summaries. Extensive experiments on four benchmark datasets (Multi-News,
DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently
outperforms previous unsupervised methods and, in several cases, achieves
performance comparable to supervised models and large language models. Human
evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high
consistency and coverage, approaching human-level quality.

</details>


### [59] [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404)
*Salah Eddine Bekhouche,Azeddine Benlamoudi,Yazid Bounab,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CL

TL;DR: 针对阿拉伯语NLP的挑战，本文提出一种增强型稠密段落检索（DPR）框架，核心引入新型注意力相关性评分（ARS），显著提升了阿拉伯语问答的检索性能和排名准确性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语因其复杂形态、可选音符和多方言共存，对自然语言处理（NLP）和信息检索（IR）构成独特挑战。尽管其全球重要性日益增长，但在NLP研究和基准资源方面仍处于不足状态。

Method: 本文提出一种专门为阿拉伯语开发的增强型稠密段落检索（DPR）框架。其核心是新颖的注意力相关性评分（ARS），该方法使用自适应评分函数取代标准交互机制，以更有效地建模问题与段落之间的语义相关性。该方法集成了预训练的阿拉伯语语言模型和架构改进。

Result: 研究结果显示，该方法提高了检索性能，并显著增加了回答阿拉伯语问题时的排名准确性。

Conclusion: 所提出的增强型DPR框架（及其核心ARS）为解决阿拉伯语NLP/IR挑战提供了有效方案，显著提升了阿拉伯语问答的检索效率和准确性。

Abstract: Arabic poses a particular challenge for natural language processing (NLP) and
information retrieval (IR) due to its complex morphology, optional diacritics
and the coexistence of Modern Standard Arabic (MSA) and various dialects.
Despite the growing global significance of Arabic, it is still underrepresented
in NLP research and benchmark resources. In this paper, we present an enhanced
Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At
the core of our approach is a novel Attentive Relevance Scoring (ARS) that
replaces standard interaction mechanisms with an adaptive scoring function that
more effectively models the semantic relevance between questions and passages.
Our method integrates pre-trained Arabic language models and architectural
refinements to improve retrieval performance and significantly increase ranking
accuracy when answering Arabic questions. The code is made publicly available
at \href{https://github.com/Bekhouche/APR}{GitHub}.

</details>


### [60] [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)
*Ante Wang,Yujie Lin,Jingyao Liu,Suhang Wu,Hao Liu,Xinyan Xiao,Jinsong Su*

Main category: cs.CL

TL;DR: 本研究提出AI主动批判性思维，即模型主动寻求缺失信息以解决用户请求。为此，构建了两个新基准GSM-MC和GSM-MCE。实验表明现有模型在该能力上表现不佳，但通过增强型强化学习算法可显著提升，例如将Qwen3-1.7B在GSM-MC上的准确率从0.15%提高到73.98%。


<details>
  <summary>Details</summary>
Motivation: 现有AI批判性思维多为被动式，即简单拒绝问题请求而非建设性地解决。为了构建更强大的AI系统并避免接受有缺陷的数据或偏见推理，需要模型主动寻求和澄清信息，从而更好地满足用户需求。

Method: 引入了“主动批判性思维”范式，即模型主动向用户寻求缺失或澄清信息。为评估此能力，构建了两个基于GSM8K的新数学推理基准：GSM-MC（故意移除关键变量）和GSM-MCE（移除关键变量并引入无关细节）。在Qwen3和Llama系列模型上进行实验，并采用一种增强型强化学习（RL）算法来提高模型的主动批判性思维能力。

Result: 实验发现，尽管Qwen3和Llama系列模型在传统推理任务上表现出色，但它们在主动批判性思维方面表现不佳，尤其是小型模型。然而，研究证明强化学习可以显著提高此能力，通过增强的RL算法，Qwen3-1.7B在GSM-MC上的准确率从0.15%大幅提升至73.98%。

Conclusion: 主动批判性思维对于构建能够更有效地与用户协作解决问题的AI模型至关重要。本研究表明，通过强化学习可以显著提升AI模型主动寻求信息的能力，从而使其更加健壮和协作。

Abstract: Critical thinking is essential for building robust AI systems, preventing
them from blindly accepting flawed data or biased reasoning. However, prior
work has primarily focused on passive critical thinking, where models simply
reject problematic queries without taking constructive steps to address user
requests. In this work, we introduce proactive critical thinking, a paradigm
where models actively seek missing or clarifying information from users to
resolve their queries better. To evaluate this capability, we present GSM-MC
and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical
reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math
problems with a key variable deliberately removed, requiring models to identify
and request the missing information. GSM-MCE further increases the difficulty
by introducing irrelevant details to test robustness against distractions.
Experiments on Qwen3 and Llama series models show that, while these models
excel in traditional reasoning tasks due to extensive post-training and
inference-time scaling, they struggle with proactive critical thinking,
especially smaller ones. However, we demonstrate that reinforcement learning
(RL) can significantly improve this ability. Using our enhanced RL algorithm,
we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to
73.98% on GSM-MC. We hope this work advances models that collaborate more
effectively with users in problem-solving through proactive critical thinking.

</details>


### [61] [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
*Saeed Almheiri,Yerulan Kongrat,Adrian Santosh,Ruslan Tasmukhanov,Josemaria Vera,Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 探究如何通过微调LLM实现企业环境中基于用户角色的访问权限控制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全方案未能有效满足企业部署中，模型行为需依据用户角色进行精细化控制的需求。

Method: 采用BERT分类器、LLM分类器和角色条件生成三种建模策略，并构建了两个互补数据集（一个改编自现有语料，一个模拟真实企业场景）进行训练与评估。对模型在不同组织结构下的性能、以及对抗提示注入、角色不匹配及越狱攻击的鲁棒性进行了评估。

Result: 论文评估了多种LLM微调策略在实现基于角色的响应生成方面的表现，并分析了其在复杂企业场景及安全威胁下的鲁棒性。

Conclusion: 本研究旨在证明LLM能够通过微调实现企业环境中精细化的基于角色的访问权限控制，并为其在实际部署中的可行性与安全性提供了评估基础。

Abstract: As large language models (LLMs) are increasingly deployed in enterprise
settings, controlling model behavior based on user roles becomes an essential
requirement. Existing safety methods typically assume uniform access and focus
on preventing harmful or toxic outputs, without addressing role-specific access
constraints. In this work, we investigate whether LLMs can be fine-tuned to
generate responses that reflect the access privileges associated with different
organizational roles. We explore three modeling strategies: a BERT-based
classifier, an LLM-based classifier, and role-conditioned generation. To
evaluate these approaches, we construct two complementary datasets. The first
is adapted from existing instruction-tuning corpora through clustering and role
labeling, while the second is synthetically generated to reflect realistic,
role-sensitive enterprise scenarios. We assess model performance across varying
organizational structures and analyze robustness to prompt injection, role
mismatch, and jailbreak attempts.

</details>


### [62] [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/abs/2507.23486)
*Shirui Wang,Zhihui Tang,Huaxia Yang,Qiuhong Gong,Tiantian Gu,Hongyang Ma,Yongxin Wang,Wubin Sun,Zeliang Lian,Kehang Mao,Yinan Jiang,Zhicheng Huang,Lingyun Ma,Wenjie Shen,Yajie Ji,Yunhui Tan,Chunbo Wang,Yunlu Gao,Qianling Ye,Rui Lin,Mingyu Chen,Lijuan Niu,Zhihao Wang,Peng Yu,Mengran Lang,Yue Liu,Huimin Zhang,Haitao Shen,Long Chen,Qiguang Zhao,Si-Xuan Liu,Lina Zhou,Hua Gao,Dongqiang Ye,Lingmin Meng,Youtao Yu,Naixin Liang,Jianxiong Wu*

Main category: cs.CL

TL;DR: 为评估大型语言模型（LLMs）在临床决策支持中的安全性和有效性，研究开发了临床安全-有效性双轨基准（CSEDB），并测试了六个LLMs，发现其整体表现中等，在高风险场景下表现显著下降，而领域特定医疗LLMs表现更优。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在临床决策支持中具有潜力，但在安全评估和有效性验证方面面临重大挑战，需要一个标准化的评估工具。

Method: 开发了临床安全-有效性双轨基准（CSEDB），这是一个基于临床专家共识的多维度框架，包含30个标准（涵盖危重症识别、指南依从性、用药安全等），并有加权后果度量。32位专科医生开发并审查了2069个开放式问答项，模拟真实场景。使用该基准测试了六个LLMs。

Result: 六个LLMs的整体表现中等（平均总分57.2%），其中安全性54.7%，有效性62.3%。在高风险场景下，性能显著下降13.3% (p < 0.0001)。领域特定的医疗LLMs比通用模型表现出持续的优势，在安全性和有效性方面均取得相对较高的分数（分别为0.912和0.861）。

Conclusion: 本研究提供的标准化评估指标有助于评估医疗LLMs的临床应用，促进比较分析、风险识别和改进方向。该基准有望推动LLMs在医疗环境中更安全、更有效地部署。

Abstract: Large language models (LLMs) hold promise in clinical decision support but
face major challenges in safety evaluation and effectiveness validation. We
developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a
multidimensional framework built on clinical expert consensus, encompassing 30
criteria covering critical areas like critical illness recognition, guideline
adherence, and medication safety, with weighted consequence measures.
Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A
items aligned with these criteria, spanning 26 clinical departments to simulate
real-world scenarios. Benchmark testing of six LLMs revealed moderate overall
performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),
with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).
Domain-specific medical LLMs showed consistent performance advantages over
general-purpose models, with relatively higher top scores in safety (0.912) and
effectiveness (0.861). The findings of this study not only provide a
standardized metric for evaluating the clinical application of medical LLMs,
facilitating comparative analyses, risk exposure identification, and
improvement directions across different scenarios, but also hold the potential
to promote safer and more effective deployment of large language models in
healthcare environments.

</details>


### [63] [Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning](https://arxiv.org/abs/2507.23541)
*Keer Lu,Zheng Liang,Youquan Li,Jiejun Tan,Da Pan,Shusen Zhang,Guosheng Dong,Huang Leng*

Main category: cs.CL

TL;DR: Med-R$^3$是一个针对医疗领域检索增强推理的渐进式强化学习框架，它通过联合优化检索和推理能力，显著提升了模型在医疗问题上的表现，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，有效检索外部知识并进行严谨逻辑推理至关重要。现有工作多单独优化检索或推理，导致两者协调不足；过度依赖SFT可能限制模型泛化能力；现有强化学习的奖励函数未能充分捕捉医疗领域特有需求。

Method: 提出了Med-R$^3$框架，一个由渐进式强化学习驱动的医疗检索增强推理框架。首先培养模型在医疗问题上的逻辑推理能力，然后自适应优化检索能力以更好地适应知识库和外部信息利用，最后联合优化模型的检索与推理协调性。

Result: Med-R$^3$实现了最先进的性能。LLaMA3.1-8B-Instruct结合Med-R$^3$在可比参数规模下超越GPT-4o-mini 3.93%；Qwen2.5-14B结合Med-R$^3$获得更显著的13.53%提升。

Conclusion: Med-R$^3$通过其独特的渐进式强化学习和检索-推理联合优化机制，有效克服了现有方法在医疗领域面临的挑战，大幅提升了模型在医疗检索增强推理任务中的性能和泛化能力。

Abstract: In medical scenarios, effectively retrieving external knowledge and
leveraging it for rigorous logical reasoning is of significant importance.
Despite their potential, existing work has predominantly focused on enhancing
either retrieval or reasoning capabilities of the models in isolation, with
little attention given to their joint optimization, which leads to limited
coordination between the two processes. Additionally, current methods rely
heavily on supervised fine-tuning (SFT), which can cause models to memorize
existing problem-solving pathways, thereby restricting their generalization
ability when confronted with novel problem contexts. Furthermore, while some
studies have explored to improve retrieval-augmented reasoning in general
domains via reinforcement learning, their reward function designs do not
adequately capture the specific demands of the medical domain. To address these
challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented
**R**easoning framework driven by progressive **R**einforcement learning. In
this framework, we first develop the model's ability to perform logical
reasoning over medical problems. Subsequently, on the basis of this foundation,
we adaptively optimize the retrieval capability to better align with the
characteristics of knowledge corpus and external information utilization
throughout the reasoning process. Finally, we conduct joint optimization of the
model's retrieval and reasoning coordination. Extensive experiments indicate
that **Med-R$^3$** could achieve state-of-the-art performances, with
LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by
3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with
Med-R$^3$ shows a more substantial gain of 13.53\%.

</details>


### [64] [T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text](https://arxiv.org/abs/2507.23577)
*Alva West,Luodan Zhang,Liuliu Zhang,Minjun Zhu,Yixuan Weng,Yue Zhang*

Main category: cs.CL

TL;DR: T-Detect是一种新型AI文本检测器，通过用基于Student's t-分布的重尾差异分数替代高斯归一化，改进了现有方法对对抗性文本的检测能力，并在实验中表现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着先进文本生成模型的普及，开发能够识别机器生成内容（特别是旨在规避检测的对抗性文本）的鲁棒检测方法变得必要。现有零样本检测器依赖高斯分布假设，但在面对对抗性或非母语英语文本中常见的重尾统计特征时表现不佳。

Method: 论文提出T-Detect，一种重塑曲率检测器统计核心的新方法。核心创新是将标准高斯归一化替换为基于Student's t-分布的重尾差异分数。该方法理论基础是观察到对抗性文本表现出显著的峰度。T-Detect通过将文本段落的对数似然与t-分布的预期矩进行归一化来计算检测分数，从而增强对统计离群值的弹性。

Result: T-Detect在RAID对抗性文本基准和HART数据集上进行了验证。实验表明，T-Detect相比强基线实现了持续的性能提升，在目标领域将AUROC提高了高达3.9%。当集成到二维检测框架（CT）中时，该方法在RAID的Books领域达到了0.926的AUROC，实现了最先进的性能。

Conclusion: 该研究贡献在于：为文本检测提供了新的、有理论依据的统计基础；提出了一种经消融验证、显示出卓越鲁棒性的方法；并全面分析了其在对抗性条件下的性能。

Abstract: The proliferation of sophisticated text generation models necessitates the
development of robust detection methods capable of identifying
machine-generated content, particularly text designed to evade detection
through adversarial perturbations. Existing zero-shot detectors often rely on
statistical measures that implicitly assume Gaussian distributions, a premise
that falters when confronted with the heavy-tailed statistical artifacts
characteristic of adversarial or non-native English texts. This paper
introduces T-Detect, a novel detection method that fundamentally redesigns the
statistical core of curvature-based detectors. Our primary innovation is the
replacement of standard Gaussian normalization with a heavy-tailed discrepancy
score derived from the Student's t-distribution. This approach is theoretically
grounded in the empirical observation that adversarial texts exhibit
significant leptokurtosis, rendering traditional statistical assumptions
inadequate. T-Detect computes a detection score by normalizing the
log-likelihood of a passage against the expected moments of a t-distribution,
providing superior resilience to statistical outliers. We validate our approach
on the challenging RAID benchmark for adversarial text and the comprehensive
HART dataset. Experiments show that T-Detect provides a consistent performance
uplift over strong baselines, improving AUROC by up to 3.9\% in targeted
domains. When integrated into a two-dimensional detection framework (CT), our
method achieves state-of-the-art performance, with an AUROC of 0.926 on the
Books domain of RAID. Our contributions are a new, theoretically-justified
statistical foundation for text detection, an ablation-validated method that
demonstrates superior robustness, and a comprehensive analysis of its
performance under adversarial conditions. Ours code are released at
https://github.com/ResearAI/t-detect.

</details>


### [65] [DiffLoRA: Differential Low-Rank Adapters for Large Language Models](https://arxiv.org/abs/2507.23588)
*Alexandre Misrahi,Nadezhda Chirkova,Maxime Louis,Vassilina Nikoulina*

Main category: cs.CL

TL;DR: 本文提出了DiffLoRA，一种参数高效的差分注意力机制，旨在结合LoRA的效率和差分注意力的性能优势。尽管在大多数NLP任务中表现不如其他参数高效微调方法，但在特定领域（如HumanEval）展现出有趣的结果，作者进一步分析了其注意力模式。


<details>
  <summary>Details</summary>
Motivation: 旨在保留LoRA的效率，同时从差分注意力机制通过消除噪声提高Transformer模型性能中获益。

Method: 引入DiffLoRA，将低秩适配器应用于正负注意力项，实现差分注意力机制的参数高效适应。在广泛的NLP任务中进行评估，包括通用基准、多示例上下文学习、RAG和长文本测试。分析微调后的注意力模式以识别其行为原因。

Result: DiffLoRA在大多数评估任务中未能超越其他参数高效微调方法。但在某些特定领域（如HumanEval）表现出有趣的结果，相较于LoRA提升了11个点。

Conclusion: DiffLoRA作为一种参数高效的差分注意力机制，在通用性能上尚未完全超越现有PEFT方法，但在特定任务上显示出显著潜力。其独特的性能模式促使进一步分析注意力机制以理解其背后的原因。

Abstract: Differential Transformer has recently been proposed to improve performance in
Transformer models by canceling out noise through a denoiser attention
mechanism. In this work, we introduce DiffLoRA, a parameter-efficient
adaptation of the differential attention mechanism, with low-rank adapters on
both positive and negative attention terms. This approach retains the
efficiency of LoRA while aiming to benefit from the performance gains of
differential attention. We evaluate DiffLoRA across a broad range of NLP tasks,
including general benchmarks, many-shot in-context learning, RAG, and
long-context tests. We observe that, although DiffLoRA falls short of other
parameter-efficient fine-tuning methods in most evaluation tasks, it shows
interesting results in certain domains (+11 pts on LoRA for HumanEval). We
analyze the attention patterns post-finetuning to identify the reasons for this
behavior.

</details>


### [66] [Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning](https://arxiv.org/abs/2507.23661)
*Salam Thabet Doghmash,Motaz Saad*

Main category: cs.CL

TL;DR: 研究解决了阿拉伯语社交媒体中的仇恨言论检测与清洗（屏蔽）问题，利用深度学习和Transformer模型进行检测，并将清洗视为机器翻译任务。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中的仇恨言论识别日益重要。本研究旨在解决阿拉伯语仇恨言论的检测及其文本清洗（屏蔽）问题。

Method: 对于仇恨言论检测，采用深度学习模型和Transformer进行实验，并以F1分数评估。对于文本清洗，将其视为机器翻译任务，将仇恨词替换为星号。

Result: 仇恨言论检测最佳模型达到了92%的Macro F1分数和95%的准确率。文本清洗/屏蔽模型在BLEU分数（1-gram）上达到0.3，表现良好。

Conclusion: 所提出的方法在阿拉伯语仇恨言论检测上表现出色，并在文本清洗/屏蔽方面也取得了良好效果。

Abstract: Hate speech identification in social media has become an increasingly
important issue in recent years. In this research, we address two problems: 1)
to detect hate speech in Arabic text, 2) to clean a given text from hate
speech. The meaning of cleaning here is replacing each bad word with stars
based on the number of letters for each word. Regarding the first problem, we
conduct several experiments using deep learning models and transformers to
determine the best model in terms of the F1 score. Regarding second problem, we
consider it as a machine translation task, where the input is a sentence
containing dirty text and the output is the same sentence with masking the
dirty text. The presented methods achieve the best model in hate speech
detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text
cleaning experiment, the best result in the hate speech masking model reached
0.3 in BLEU score with 1-gram, which is a good result compared with the state
of the art machine translation systems.

</details>


### [67] [Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs](https://arxiv.org/abs/2507.23740)
*Nasim Shirvani-Mahdavi,Devin Wingfield,Amin Ghasemi,Chengkai Li*

Main category: cs.CL

TL;DR: 本研究探讨使用大语言模型为知识图谱中的逻辑规则生成自然语言解释，以提高人类对规则的理解。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中的逻辑规则虽能支持新事实推理、错误检测并揭示数据模式，但其复杂性和独特的标注方式使其难以被人类理解。这阻碍了规则的有效利用。

Method: 研究从FB15k-237、FB-CVT-REV和FB+CVT-REV数据集中，使用AMIE 3.5.1算法提取逻辑规则。然后，探索了多种大语言模型提示策略，包括零样本、少样本、包含变量实体类型和思维链推理。最后，通过人工评估（基于正确性、清晰度和幻觉）以及评估大语言模型作为自动评估者的能力，对生成的解释进行综合评估。

Result: 研究结果表明，在解释的正确性和清晰度方面取得了良好的表现。

Conclusion: 大语言模型在为知识图谱逻辑规则生成可理解的自然语言解释方面展现出潜力，尽管仍存在一些挑战需要未来研究解决。

Abstract: Knowledge graphs (KGs) often contain sufficient information to support the
inference of new facts. Identifying logical rules not only improves the
completeness of a knowledge graph but also enables the detection of potential
errors, reveals subtle data patterns, and enhances the overall capacity for
reasoning and interpretation. However, the complexity of such rules, combined
with the unique labeling conventions of each KG, can make them difficult for
humans to understand. In this paper, we explore the potential of large language
models to generate natural language explanations for logical rules.
Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery
algorithm from the benchmark dataset FB15k-237 and two large-scale datasets,
FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including
zero- and few-shot prompting, including variable entity types, and
chain-of-thought reasoning. We conduct a comprehensive human evaluation of the
generated explanations based on correctness, clarity, and hallucination, and
also assess the use of large language models as automatic judges. Our results
demonstrate promising performance in terms of explanation correctness and
clarity, although several challenges remain for future research. All scripts
and data used in this study are publicly available at
https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.

</details>


### [68] [Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities](https://arxiv.org/abs/2507.23776)
*Yunxiang Yan,Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 提出一种基于“级联问题披露”的新框架，旨在更准确地评估大型语言模型（LLM）的问题解决能力，并发现现有问答（QA）评估方法可能高估了模型间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有QA基准测试虽然自动化且可扩展，但其评估LLM问题解决能力的方式是间接的，可能无法提供准确的估计。

Method: 开发了一种基于“级联问题披露”的整体通用框架。该方法分阶段收集模型响应，每个阶段逐步揭示部分问题信息，以激发LLM的泛化推理能力。

Result: 该方法不仅能提供更好的LLM比较，还能在模型中诱导更好的中间推理轨迹。实证结果显示，该方法缩小了标准QA评估中观察到的性能差距，表明流行的间接QA评估范式可能高估了模型间的性能差异。

Conclusion: 基于级联问题披露的评估框架能更准确地衡量LLM的问题解决能力，并揭示了当前间接QA评估方法可能夸大模型性能差异的局限性。

Abstract: While question-answering~(QA) benchmark performance is an automatic and
scalable method to compare LLMs, it is an indirect method of evaluating their
underlying problem-solving capabilities. Therefore, we propose a holistic and
generalizable framework based on \emph{cascaded question disclosure} that
provides a more accurate estimate of the models' problem-solving capabilities
while maintaining the scalability and automation. This approach collects model
responses in a stagewise manner with each stage revealing partial information
about the question designed to elicit generalized reasoning in LLMs. We find
that our approach not only provides a better comparison between LLMs, but also
induces better intermediate traces in models compared to the standard QA
paradigm. We empirically verify this behavior on diverse reasoning and
knowledge-heavy QA datasets by comparing LLMs of varying sizes and families.
Our approach narrows the performance gap observed in the standard QA evaluation
settings, indicating that the prevalent indirect QA paradigm of evaluation
overestimates the differences in performance between models. We further
validate our findings by extensive ablation studies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam](https://arxiv.org/abs/2507.22958)
*Ruslan Khrulev*

Main category: cs.CV

TL;DR: 本文提出了一个新基准EGE-Math，用于评估视觉语言模型（VLM）对手写数学解题方案的评估能力，结果显示当前VLM在数学推理和符合人类评分标准方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注问题解决，而缺乏对学生手写数学解决方案的理解、错误识别及按标准评分的能力评估，因此需要一个专注于此的新基准。

Method: 作者编译了122份来自俄罗斯统一国家考试（EGE）的手写数学解决方案及其官方专家评分作为数据集，并用其在三种推理模式下评估了七个来自不同公司的现代VLM。

Result: 评估结果揭示了当前VLM在数学推理能力和与人类评分标准对齐方面的显著局限性。

Conclusion: 研究表明，现有的VLM在AI辅助评估手写数学解决方案方面仍有不足，这为该领域开辟了新的研究方向。

Abstract: This paper introduces a novel benchmark, EGE-Math Solutions Assessment
Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to
assess hand-written mathematical solutions. Unlike existing benchmarks that
focus on problem solving, our approach centres on understanding student
solutions, identifying mistakes, and assigning grades according to fixed
criteria. We compile 122 scanned solutions from the Russian Unified State Exam
(EGE) together with official expert grades, and evaluate seven modern VLMs from
Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The
results reveal current limitations in mathematical reasoning and human-rubric
alignment, opening new research avenues in AI-assisted assessment. You can find
code in https://github.com/Karifannaa/Auto-check-EGE-math

</details>


### [70] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 本文提出一个框架，能够快速重建和实时渲染城市规模场景，并有效处理多视角捕获中的外观变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在城市规模场景的快速重建、实时渲染以及处理多视角捕获中外观变化方面存在挑战。

Method: 该方法包括场景分区以实现并行训练，采用基于可见性的图像选择优化训练效率；通过可控的LOD策略管理高斯密度以平衡效率和视觉保真度；引入外观转换模块以减轻外观不一致性并实现灵活调整；同时利用深度正则化、尺度正则化和抗锯齿等增强模块提高重建精度。

Result: 实验结果表明，该方法能有效重建城市规模场景，并在效率和质量上均优于以往方法。

Conclusion: 所提出的框架成功实现了城市规模场景的快速、高质量重建和实时渲染，同时有效解决了多视角数据中的外观变化问题。

Abstract: We present a framework that enables fast reconstruction and real-time
rendering of urban-scale scenes while maintaining robustness against appearance
variations across multi-view captures. Our approach begins with scene
partitioning for parallel training, employing a visibility-based image
selection strategy to optimize training efficiency. A controllable
level-of-detail (LOD) strategy explicitly regulates Gaussian density under a
user-defined budget, enabling efficient training and rendering while
maintaining high visual fidelity. The appearance transformation module
mitigates the negative effects of appearance inconsistencies across images
while enabling flexible adjustments. Additionally, we utilize enhancement
modules, such as depth regularization, scale regularization, and antialiasing,
to improve reconstruction fidelity. Experimental results demonstrate that our
method effectively reconstructs urban-scale scenes and outperforms previous
approaches in both efficiency and quality. The source code is available at:
https://yzslab.github.io/REUrbanGS.

</details>


### [71] [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/abs/2507.23021)
*Giuseppe Cartella,Vittorio Cuculo,Alessandro D'Amelio,Marcella Cornia,Giuseppe Boccignone,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出ScanDiff模型，结合扩散模型和Vision Transformer生成多样且真实的眼动轨迹，并支持任务驱动预测，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习眼动轨迹预测模型无法捕捉人类视觉探索的变异性，通常只生成平均行为。

Method: 引入ScanDiff架构，结合扩散模型和Vision Transformer，利用扩散模型的随机性来显式建模眼动轨迹的变异性。同时，引入文本条件机制以实现任务驱动的眼动轨迹生成。

Result: 在基准数据集上的实验表明，ScanDiff在自由观看和任务驱动场景中均超越了现有最先进方法，生成了更多样且准确的眼动轨迹。

Conclusion: ScanDiff能够更好地捕捉人类视觉行为的复杂性，推动了眼动预测研究的进展。

Abstract: Predicting human gaze scanpaths is crucial for understanding visual
attention, with applications in human-computer interaction, autonomous systems,
and cognitive robotics. While deep learning models have advanced scanpath
prediction, most existing approaches generate averaged behaviors, failing to
capture the variability of human visual exploration. In this work, we present
ScanDiff, a novel architecture that combines diffusion models with Vision
Transformers to generate diverse and realistic scanpaths. Our method explicitly
models scanpath variability by leveraging the stochastic nature of diffusion
models, producing a wide range of plausible gaze trajectories. Additionally, we
introduce textual conditioning to enable task-driven scanpath generation,
allowing the model to adapt to different visual search objectives. Experiments
on benchmark datasets show that ScanDiff surpasses state-of-the-art methods in
both free-viewing and task-driven scenarios, producing more diverse and
accurate scanpaths. These results highlight its ability to better capture the
complexity of human visual behavior, pushing forward gaze prediction research.
Source code and models are publicly available at
https://aimagelab.github.io/ScanDiff.

</details>


### [72] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 超分辨率（SR）技术能有效提升低质量超声心动图的诊断价值，尤其SRResNet在资源受限环境下可提高AI诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限地区，超声心动图图像质量差常阻碍心脏自动诊断模型的有效性。尽管超分辨率技术在MRI和CT图像增强方面有前景，但在易受噪声影响但广泛可及的超声心动图领域的应用仍未充分探索。本研究旨在探讨深度学习超分辨率技术提升低质量2D超声心动图分类准确性的潜力。

Method: 研究采用深度学习超分辨率技术，旨在提升低质量2D超声心动图的分类准确性。使用公开的CAMUS数据集，根据图像质量分层样本。评估了两项临床任务：2腔与4腔视图分类（相对简单）和心舒末期与心缩末期相位分类（更复杂）。应用了两种主流SR模型：SRGAN和SRResNet，以增强低质量图像。

Result: 研究发现，超分辨率技术能显著提升性能指标，尤其SRResNet表现更优且计算效率高。

Conclusion: 结果表明，超分辨率技术能有效恢复退化超声心动图的诊断价值，使其成为资源受限环境下AI辅助医疗的有效工具，实现“事半功倍”。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is
often hindered by poor-quality echocardiographic imaging, limiting the
effectiveness of downstream diagnostic models. While super-resolution (SR)
techniques have shown promise in enhancing magnetic resonance imaging (MRI) and
computed tomography (CT) scans, their application to echocardiography-a widely
accessible but noise-prone modality-remains underexplored. In this work, we
investigate the potential of deep learning-based SR to improve classification
accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS
dataset, we stratify samples by image quality and evaluate two clinically
relevant tasks of varying complexity: a relatively simple Two-Chamber vs.
Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole
vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR
models-Super-Resolution Generative Adversarial Network (SRGAN) and
Super-Resolution Residual Network (SRResNet), to enhance poor-quality images
and observe significant gains in performance metric-particularly with SRResNet,
which also offers computational efficiency. Our findings demonstrate that SR
can effectively recover diagnostic value in degraded echo scans, making it a
viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [73] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: 针对NeRF高能耗问题，本文提出基于SNN的PATA框架，通过动态时间步训练，在保持渲染质量的同时，显著降低推理时间和功耗，使其适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF模型在训练和推理过程中依赖密集的点采样，导致大量浮点运算和高能耗，严重限制其在边缘计算等资源受限场景的应用。脉冲神经网络（SNN）因其固有的能效优势，提供了潜在的解决方案。

Method: 本文提出一种基于脉冲神经网络（SNN）的NeRF框架，名为Pretrain-Adaptive Time-step Adjustment (PATA)。该方法在训练阶段采用动态时间步调整策略，自动权衡渲染质量与时间步长。它使得模型能够在推理时根据场景自适应地使用可变时间步，从而减少额外计算资源消耗。该方法基于Instant-NGP架构进行实现。

Result: 实验结果表明，PATA在保持渲染保真度的前提下，成功将推理时间步减少了64%，并将运行功耗降低了61.55%。

Conclusion: PATA框架有效解决了NeRF模型在高能耗场景下的应用局限性，通过引入SNN和动态时间步策略，显著提升了能效，使其能更好地适应资源受限的边缘计算环境，同时保持了高质量的渲染效果。

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success
in 3D reconstruction and rendering tasks. However, during both training and
inference, these models rely heavily on dense point sampling along rays from
multiple viewpoints, resulting in a surge in floating-point operations and
severely limiting their use in resource-constrained scenarios like edge
computing. Spiking Neural Networks (SNNs), which communicate via binary spikes
over discrete time steps, offer a promising alternative due to their
energy-efficient nature. Given the inherent variability in scene scale and
texture complexity in neural rendering and the prevailing practice of training
separate models per scene, we propose a spike-based NeRF framework with a
dynamic time step training strategy, termed Pretrain-Adaptive Time-step
Adjustment (PATA). This approach automatically explores the trade-off between
rendering quality and time step length during training. Consequently, it
enables scene-adaptive inference with variable time steps and reduces the
additional consumption of computational resources in the inference process.
Anchoring to the established Instant-NGP architecture, we evaluate our method
across diverse datasets. The experimental results show that PATA can preserve
rendering fidelity while reducing inference time steps by 64\% and running
power by 61.55\%.

</details>


### [74] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

TL;DR: NovaDrive是一个单分支视觉-语言架构，通过多模态融合和创新注意力机制，显著提升了自动驾驶的性能、安全性和效率，并在基准测试中超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂交通环境中需要毫秒级的反应速度，同时理解道路几何和交通意图。现有方案可能难以高效处理多模态信息并实现高鲁棒性和实时推理能力。

Method: NovaDrive采用单一分支的视觉-语言架构，整合前置摄像头图像、高清地图、LiDAR深度数据和文本路点。其核心包括一个轻量级两阶段交叉注意力模块，用于对齐路点与高清地图，并细化图像和深度特征的注意力。此外，结合一个新颖的平滑度损失函数，避免了对循环记忆的需求。该系统通过微调11B LLaMA-3.2视觉-语言骨干的顶部15层，实现了实时推理。

Result: 在MD-NEX Outdoor基准测试的nuScenes/Waymo子集上，NovaDrive将成功率提高到84%（+4%），路径效率（SPL）提升至0.66（+0.11），并将碰撞频率从2.6%降低到1.2%（-1.4%），均超越了现有最先进技术。消融实验证实了路点tokens、部分VLM微调和交叉注意力融合对这些性能提升的关键贡献。其生成的更短路线也意味着更低的能源消耗。

Conclusion: NovaDrive提供了一个高效且鲁棒的自动驾驶解决方案，通过创新的多模态融合和训练策略，在性能、安全性和能源效率方面均取得了显著进展。该架构具有降低驾驶栈复杂性和向其他具身AI领域扩展的潜力。

Abstract: Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [75] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

TL;DR: 本文提出两种基于扩散模型的新方法MObI和AnydoorMed，分别用于自动驾驶和医学图像分析中的多模态合成数据生成，实现逼真可控的物体/异常插入。


<details>
  <summary>Details</summary>
Motivation: 安全关键型应用（如自动驾驶、医学图像分析）需要大量多模态数据进行严格测试。真实数据采集成本高且复杂，因此需要高真实度和可控性的合成数据方法。

Method: 本文引入两种新颖的合成数据生成方法：
1. MObI：针对自动驾驶的多模态物体修复框架，利用扩散模型同时处理相机和激光雷达数据，通过3D边界框引导在指定3D位置插入物体，确保语义一致性和多模态连贯性。
2. AnydoorMed：将MObI范式扩展至医学影像领域（乳腺X线扫描），采用扩散模型进行参考引导的异常修复，实现细节保留、结构完整性维护以及与周围组织的语义融合。

Result: MObI和AnydoorMed成功证明了用于自然图像的参考引导修复基础模型可以便捷地适应多种感知模态，生成高度逼真、可控的多模态反事实场景。

Conclusion: 本研究为构建能够生成高度真实、可控和多模态反事实场景的下一代系统铺平了道路，通过展示基础模型在不同感知模态上的广泛适用性。

Abstract: Safety-critical applications, such as autonomous driving and medical image
analysis, require extensive multimodal data for rigorous testing. Synthetic
data methods are gaining prominence due to the cost and complexity of gathering
real-world data, but they demand a high degree of realism and controllability
to be useful. This work introduces two novel methods for synthetic data
generation in autonomous driving and medical image analysis, namely MObI and
AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal
Object Inpainting that leverages a diffusion model to produce realistic and
controllable object inpaintings across perceptual modalities, demonstrated
simultaneously for camera and lidar. Given a single reference RGB image, MObI
enables seamless object insertion into existing multimodal scenes at a
specified 3D location, guided by a bounding box, while maintaining semantic
consistency and multimodal coherence. Unlike traditional inpainting methods
that rely solely on edit masks, this approach uses 3D bounding box conditioning
to ensure accurate spatial positioning and realistic scaling. AnydoorMed
extends this paradigm to the medical imaging domain, focusing on
reference-guided inpainting for mammography scans. It leverages a
diffusion-based model to inpaint anomalies with impressive detail preservation,
maintaining the reference anomaly's structural integrity while semantically
blending it with the surrounding tissue. Together, these methods demonstrate
that foundation models for reference-guided inpainting in natural images can be
readily adapted to diverse perceptual modalities, paving the way for the next
generation of systems capable of constructing highly realistic, controllable
and multimodal counterfactual scenarios.

</details>


### [76] [Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints](https://arxiv.org/abs/2507.23064)
*Santosh Patapati,Trisanth Srinivasan,Murari Ambati*

Main category: cs.CV

TL;DR: XYZ-Drive是一种创新的单支路视觉-语言模型，通过早期融合图像、地图和导航点信息，实现了高精度、实时、透明的自动驾驶，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的自动驾驶系统在处理几何精度和语义理解时通常将它们分离，导致效率低下或性能受限。

Method: 提出了XYZ-Drive，一个单一的视觉-语言模型(VLM)。该模型接收前置摄像头图像、25m x 25m的俯视地图和下一个导航点作为输入，然后输出转向和速度。其核心是一个轻量级的以目标为中心的交叉注意力层，允许导航点令牌突出图像和地图中的相关区域，支持动作输出和文本解释。融合后的令牌会进入部分微调的LLaMA-3.2 11B模型。

Result: 在MD-NEX室外驾驶基准测试中，XYZ-Drive达到了95%的成功率和0.80的路径长度加权成功率(SPL)，比PhysNav-DG高出15%，并使碰撞率减半，同时通过使用单一分支显著提高了效率。消融实验表明，移除任何模态（视觉、导航点、地图）都会使成功率下降高达11%；用简单拼接代替目标中心注意力会导致3%的性能下降；冻结Transformer模型会损失5%的性能；降低地图分辨率会提高碰撞率。

Conclusion: 研究结果表明，对意图和地图布局进行早期、令牌级融合能够实现准确、透明和实时的自动驾驶。

Abstract: Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and
0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and
halving collisions, all while significantly improving efficiency by using only
a single branch. Sixteen ablations explain the gains. Removing any modality
(vision, waypoint, map) drops success by up to 11%, confirming their
complementary roles and rich connections. Replacing goal-centered attention
with simple concatenation cuts 3% in performance, showing query-based fusion
injects map knowledge more effectively. Keeping the transformer frozen loses
5%, showing the importance of fine-tuning when applying VLMs for specific tasks
such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs
lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent
and map layout enables accurate, transparent, real-time driving.

</details>


### [77] [Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://arxiv.org/abs/2507.23070)
*Dmitry Demidov,Zaigham Zaheer,Omkar Thawakar,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 提出E-FineR，一种无需训练的细粒度图像分类方法，结合LLM和VLM，解决了现有方法对LLM利用不足及类名未精炼的问题，在开放集、零样本、少样本任务中实现了SOTA性能及高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度图像分类方法受限于固定词汇和封闭集范式，缺乏可扩展性和适应性。尽管LLM与VLM结合能实现开放集识别，但现有方法在分类阶段未充分利用LLM潜力，且过度依赖未经分析和精炼的LLM生成类名。

Method: 提出Enriched-FineR (E-FineR)，一种无需训练（training-free）的方法，旨在克服现有LLM与VLM结合方法在细粒度图像分类中对LLM能力利用不足和类名未精炼的瓶颈。

Result: ['在细粒度视觉识别中达到最先进（SOTA）水平。', '提供更高的可解释性。', '在零样本（zero-shot）和少样本（few-shot）分类任务中，性能与现有SOTA相当。', '该方法无需训练且无需人工干预。']

Conclusion: E-FineR作为一个无词汇框架，支持图像分类从刚性标签预测向灵活、语言驱动理解的转变，为真实世界应用提供了可扩展和通用化的系统。

Abstract: Fine-grained image classification, the task of distinguishing between
visually similar subcategories within a broader category (e.g., bird species,
car models, flower types), is a challenging computer vision problem.
Traditional approaches rely heavily on fixed vocabularies and closed-set
classification paradigms, limiting their scalability and adaptability in
real-world settings where novel classes frequently emerge. Recent research has
demonstrated that combining large language models (LLMs) with vision-language
models (VLMs) makes open-set recognition possible without the need for
predefined class labels. However, the existing methods are often limited in
harnessing the power of LLMs at the classification phase, and also rely heavily
on the guessed class names provided by an LLM without thorough analysis and
refinement. To address these bottlenecks, we propose our training-free method,
Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art
results in fine-grained visual recognition while also offering greater
interpretability, highlighting its strong potential in real-world scenarios and
new domains where expert annotations are difficult to obtain. Additionally, we
demonstrate the application of our proposed approach to zero-shot and few-shot
classification, where it demonstrated performance on par with the existing SOTA
while being training-free and not requiring human interventions. Overall, our
vocabulary-free framework supports the shift in image classification from rigid
label prediction to flexible, language-driven understanding, enabling scalable
and generalizable systems for real-world applications. Well-documented code is
available on https://github.com/demidovd98/e-finer.

</details>


### [78] [Details Matter for Indoor Open-vocabulary 3D Instance Segmentation](https://arxiv.org/abs/2507.23134)
*Sanghun Jung,Jingjing Zheng,Ke Zhang,Nan Qiao,Albert Y. C. Chen,Lu Xia,Chi Liu,Yuyin Sun,Xiao Zeng,Hsiang-Wei Huang,Byron Boots,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: 针对开放词汇3D实例分割（OV-3DIS），本文提出了一种结合现有互补概念的新型最先进解决方案。该方案采用两阶段方法，通过鲁棒的3D提案生成、使用Alpha-CLIP进行实例分类以及引入标准化最大相似度（SMS）得分，实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇3D实例分割（OV-3DIS）方法虽然已提出多种概念，但这些概念并非互斥而是互补的。研究动机在于通过精心设计将这些互补概念结合并加以改进，以克服关键挑战，从而提出一种新的最先进解决方案。

Method: 本方案遵循两阶段流程：1) 3D提案生成：采用鲁棒的3D跟踪基提案聚合技术生成3D提案，并通过迭代合并/移除处理重叠或部分提案。2) 实例分类：将标准CLIP模型替换为Alpha-CLIP，该模型通过将对象掩码作为alpha通道来减少背景噪声并获取以对象为中心的表示。此外，引入标准化最大相似度（SMS）得分来规范文本到提案的相似度，以有效过滤假阳性并提高精度。

Result: 该框架在ScanNet200和S3DIS数据集上的所有AP和AR指标上均取得了最先进的性能，甚至超越了端到端的封闭词汇方法。

Conclusion: 通过整合和优化现有概念并引入Alpha-CLIP和SMS得分，本文提出的方案显著提升了开放词汇3D实例分割的性能，达到了新的最先进水平，证明了其在复杂3D场景理解中的优越性。

Abstract: Unlike closed-vocabulary 3D instance segmentation that is often trained
end-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages
vision-language models (VLMs) to generate 3D instance proposals and classify
them. While various concepts have been proposed from existing research, we
observe that these individual concepts are not mutually exclusive but
complementary. In this paper, we propose a new state-of-the-art solution for
OV-3DIS by carefully designing a recipe to combine the concepts together and
refining them to address key challenges. Our solution follows the two-stage
scheme: 3D proposal generation and instance classification. We employ robust 3D
tracking-based proposal aggregation to generate 3D proposals and remove
overlapped or partial proposals by iterative merging/removal. For the
classification stage, we replace the standard CLIP model with Alpha-CLIP, which
incorporates object masks as an alpha channel to reduce background noise and
obtain object-centric representation. Additionally, we introduce the
standardized maximum similarity (SMS) score to normalize text-to-proposal
similarity, effectively filtering out false positives and boosting precision.
Our framework achieves state-of-the-art performance on ScanNet200 and S3DIS
across all AP and AR metrics, even surpassing an end-to-end closed-vocabulary
method.

</details>


### [79] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

TL;DR: X-NeMo是一种新型零样本扩散模型，可利用驱动视频的表情为静态肖像生成高质量动画，有效解决身份泄露和表情捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 现有肖像动画方法存在身份泄露（即驱动视频人物特征混入生成动画）以及难以捕捉细微和极端表情的挑战。

Method: X-NeMo引入一个端到端训练框架，从驱动图像中提取1D身份无关的潜在运动描述符，并通过交叉注意力在图像生成时精确控制运动。为增强表现力并解耦运动与身份信息，模型利用双GAN解码器以及空间和颜色增强进行监督学习。通过将驱动运动嵌入1D潜在向量并使用交叉注意力而非附加空间引导，该设计消除了结构线索传输，从而显著减轻了身份泄露。

Result: 广泛实验证明，X-NeMo超越了现有最先进基线，能够生成高度表现力且身份相似度极高的动画。

Conclusion: X-NeMo通过其独特设计（1D潜在运动描述符和交叉注意力），成功解决了零样本肖像动画中的关键难题，实现了卓越的动画质量和身份保持能力。

Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation
pipeline that animates a static portrait using facial movements from a driving
video of a different individual. Our work first identifies the root causes of
the key issues in prior approaches, such as identity leakage and difficulty in
capturing subtle and extreme expressions. To address these challenges, we
introduce a fully end-to-end training framework that distills a 1D
identity-agnostic latent motion descriptor from driving image, effectively
controlling motion through cross-attention during image generation. Our
implicit motion descriptor captures expressive facial motion in fine detail,
learned end-to-end from a diverse video dataset without reliance on pretrained
motion detectors. We further enhance expressiveness and disentangle motion
latents from identity cues by supervising their learning with a dual GAN
decoder, alongside spatial and color augmentations. By embedding the driving
motion into a 1D latent vector and controlling motion via cross-attention
rather than additive spatial guidance, our design eliminates the transmission
of spatial-aligned structural clues from the driving condition to the diffusion
backbone, substantially mitigating identity leakage. Extensive experiments
demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly
expressive animations with superior identity resemblance. Our code and models
are available for research.

</details>


### [80] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 提出一种神经逆渲染方法，能从多视图、多光源图像中联合重建几何、空间可变反射率和光照条件，无需光照校准或中间线索。


<details>
  <summary>Details</summary>
Motivation: 现有多视图光度立体方法需要光照校准或中间线索（如每视图法线图），限制了其实用性。本研究旨在通过单阶段、从原始图像联合优化所有场景参数来克服这些局限。

Method: 该方法是一种神经逆渲染方法，将几何和反射率表示为神经隐式场，并应用阴影感知体渲染。首先，一个空间网络预测每个场景点的有符号距离和反射潜码；然后，一个反射网络根据潜码、角编码的表面法线、视图和光照方向来估计反射率。所有场景参数都通过端到端的方式从原始多视图图像中联合优化。

Result: 所提出的方法在形状和光照估计精度上优于最先进的法线引导方法。它能泛化到视图未对齐的多光源图像，并能处理具有挑战性几何形状和反射率的物体。

Conclusion: 该神经逆渲染方法提供了一个鲁棒且准确的单阶段解决方案，能够从多视图、多光源图像中联合重建复杂的场景属性（几何、反射率、光照），有效克服了现有方法的局限性。

Abstract: We propose a neural inverse rendering approach that jointly reconstructs
geometry, spatially varying reflectance, and lighting conditions from
multi-view images captured under varying directional lighting. Unlike prior
multi-view photometric stereo methods that require light calibration or
intermediate cues such as per-view normal maps, our method jointly optimizes
all scene parameters from raw images in a single stage. We represent both
geometry and reflectance as neural implicit fields and apply shadow-aware
volume rendering. A spatial network first predicts the signed distance and a
reflectance latent code for each scene point. A reflectance network then
estimates reflectance values conditioned on the latent code and angularly
encoded surface normal, view, and light directions. The proposed method
outperforms state-of-the-art normal-guided approaches in shape and lighting
estimation accuracy, generalizes to view-unaligned multi-light images, and
handles objects with challenging geometry and reflectance.

</details>


### [81] [CNN-based solution for mango classification in agricultural environments](https://arxiv.org/abs/2507.23174)
*Beatriz Díaz Peón,Jorge Torres Gómez,Ariel Fajardo Márquez*

Main category: cs.CV

TL;DR: 本文设计了一个基于卷积神经网络（CNN）的水果（特指芒果）检测与分类系统，旨在农场库存管理中自动评估水果质量。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动评估水果质量的系统，以用于农场库存管理。

Method: 使用卷积神经网络（CNN）设计水果检测与分类系统。具体而言，分类任务采用Resnet-18架构，检测任务使用级联检测器。系统通过图像处理实现，并利用MatLab App Designer开发了图形用户界面以显示检测和分类结果。

Result: 成功开发出一种准确且高效的芒果水果分类方法，实现了水果的检测与分类，并在执行速度和计算资源消耗之间取得了平衡。

Conclusion: 卷积神经网络与级联检测器的整合为水果分类和检测提供了一个可靠的解决方案，在农业质量控制领域具有潜在的应用价值。

Abstract: This article exemplifies the design of a fruit detection and classification
system using Convolutional
  Neural Networks (CNN). The goal is to develop a system that automatically
assesses fruit quality for
  farm inventory management. Specifically, a method for mango fruit
classification was developed using
  image processing, ensuring both accuracy and efficiency. Resnet-18 was
selected as the preliminary
  architecture for classification, while a cascade detector was used for
detection, balancing execution speed
  and computational resource consumption. Detection and classification results
were displayed through a
  graphical interface developed in MatLab App Designer, streamlining system
interaction. The integration
  of convolutional neural networks and cascade detectors proffers a reliable
solution for fruit classification
  and detection, with potential applications in agricultural quality control.

</details>


### [82] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 本研究提出一种新的图像去雨网络，通过引入Corner Loss防止细节丢失，并结合R-CBAM模块动态聚焦雨区，显著提升了单图像去雨效果。


<details>
  <summary>Details</summary>
Motivation: 单图像去雨不仅需要抑制噪声，更关键的是要同时保留精细结构细节和整体视觉质量，这是一个具有挑战性的问题。

Method: 本文提出一种新型图像恢复网络。核心方法包括：1. 引入“Corner Loss”以防止在恢复过程中丢失对象边界和纹理信息。2. 在编码器和解码器中嵌入“残差卷积块注意力模块（R-CBAM）”，以动态调整特征重要性，使网络能更有效地关注受雨纹严重影响的区域。

Result: 在Rain100L和Rain100H数据集上的定量评估表明，所提出的方法显著优于现有方法，分别达到了33.29 dB（Rain100L）和26.16 dB（Rain100H）的PSNR。

Conclusion: 所提出的图像恢复网络，通过创新的Corner Loss和R-CBAM模块，成功解决了单图像去雨中细节保留和雨区聚焦的难题，从而实现了卓越的去雨性能。

Abstract: The problem of single-image rain streak removal goes beyond simple noise
suppression, requiring the simultaneous preservation of fine structural details
and overall visual quality. In this study, we propose a novel image restoration
network that effectively constrains the restoration process by introducing a
Corner Loss, which prevents the loss of object boundaries and detailed texture
information during restoration. Furthermore, we propose a Residual
Convolutional Block Attention Module (R-CBAM) Block into the encoder and
decoder to dynamically adjust the importance of features in both spatial and
channel dimensions, enabling the network to focus more effectively on regions
heavily affected by rain streaks. Quantitative evaluations conducted on the
Rain100L and Rain100H datasets demonstrate that the proposed method
significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on
Rain100L and 26.16 dB on Rain100H.

</details>


### [83] [Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space](https://arxiv.org/abs/2507.23188)
*Shiyao Yu,Zi-An Wang,Kangning Yin,Zheng Tian,Mingyuan Zhang,Weixin Si,Shihao Zou*

Main category: cs.CV

TL;DR: 本文提出一个包含文本、音频、视频和运动四种模态的运动检索框架，通过序列级对比学习构建细粒度联合嵌入空间，首次引入音频并实现超越现有技术的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有运动检索方法缺乏直观的用户交互模式，且未充分利用模态的序列表示来提升检索性能。

Method: 通过序列级对比学习，将文本、音频、视频和运动四种模态对齐到细粒度联合嵌入空间。首次将音频模态纳入运动检索，并扩充数据集进行评估。

Result: 在多个子任务中性能优于现有最佳方法，例如在HumanML3D数据集上，文本到运动检索的R@10提升10.16%，视频到运动检索的R@1提升25.43%。四模态框架显著优于三模态。

Conclusion: 多模态运动检索，尤其是新引入音频，极大地提高了用户沉浸感和便利性，展示了在运动获取领域发展的巨大潜力。

Abstract: Motion retrieval is crucial for motion acquisition, offering superior
precision, realism, controllability, and editability compared to motion
generation. Existing approaches leverage contrastive learning to construct a
unified embedding space for motion retrieval from text or visual modality.
However, these methods lack a more intuitive and user-friendly interaction mode
and often overlook the sequential representation of most modalities for
improved retrieval performance. To address these limitations, we propose a
framework that aligns four modalities -- text, audio, video, and motion --
within a fine-grained joint embedding space, incorporating audio for the first
time in motion retrieval to enhance user immersion and convenience. This
fine-grained space is achieved through a sequence-level contrastive learning
approach, which captures critical details across modalities for better
alignment. To evaluate our framework, we augment existing text-motion datasets
with synthetic but diverse audio recordings, creating two multi-modal motion
retrieval datasets. Experimental results demonstrate superior performance over
state-of-the-art methods across multiple sub-tasks, including an 10.16%
improvement in R@10 for text-to-motion retrieval and a 25.43% improvement in
R@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our
results show that our 4-modal framework significantly outperforms its 3-modal
counterpart, underscoring the potential of multi-modal motion retrieval for
advancing motion acquisition.

</details>


### [84] [A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery](https://arxiv.org/abs/2507.23193)
*Youngsun Jang,Dongyoun Kim,Chulwoo Pack,Kwanghee Won*

Main category: cs.CV

TL;DR: 本文提出了一个用于卫星图像洪泛区分割的新数据集，并评估了现有最先进模型在该数据集上的性能，结果显示模型表现平平，提示未来研究需探索多模态和时序学习策略。


<details>
  <summary>Details</summary>
Motivation: 通过对77个现有卫星图像基准数据集的审查，研究发现缺乏适合洪泛区语义分割任务的专用数据集。

Method: 研究人员从Planet Explorer收集了2019年美国中西部洪水卫星图像，构建了一个包含5个州、每州10个地点、每地点10张图像的新数据集，并统一了分辨率。随后，在该数据集上测试了计算机视觉和遥感领域的最先进语义分割模型，并进行了变化窗口大小的消融研究以捕获时间特性。

Result: 现有模型在该新数据集上的分割表现一般。

Conclusion: 当前模型在洪泛区分割任务上的表现有待提升，未来需要多模态和时间学习策略。所创建的数据集将公开发布。

Abstract: This study introduces a novel dataset for segmenting flooded areas in
satellite images. After reviewing 77 existing benchmarks utilizing satellite
imagery, we identified a shortage of suitable datasets for this specific task.
To fill this gap, we collected satellite imagery of the 2019 Midwestern USA
floods from Planet Explorer by Planet Labs (Image \c{opyright} 2024 Planet Labs
PBC). The dataset consists of 10 satellite images per location, each containing
both flooded and non-flooded areas. We selected ten locations from each of the
five states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset
ensures uniform resolution and resizing during data processing. For evaluating
semantic segmentation performance, we tested state-of-the-art models in
computer vision and remote sensing on our dataset. Additionally, we conducted
an ablation study varying window sizes to capture temporal characteristics.
Overall, the models demonstrated modest results, suggesting a requirement for
future multimodal and temporal learning strategies. The dataset will be
publicly available on
<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.

</details>


### [85] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出对抗引导扩散（AGD）方法，利用扩散模型生成针对多模态大语言模型（MLLM）的对抗性图像。AGD通过将全频段对抗信号嵌入扩散模型的噪声分量中，实现了高效攻击并增强了对多种防御的鲁棒性，性能优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 生成能够欺骗多模态大语言模型（MLLM）产生目标响应的对抗性图像，同时避免对原始图像造成显著失真，是当前面临的挑战。

Method: 提出对抗引导扩散（AGD）方法。该方法通过将目标语义注入到逆扩散过程中的噪声分量，而非直接嵌入高频扰动。由于扩散模型的噪声覆盖整个频段，嵌入的对抗信号也具备全频段特性，使得生成的对抗图像在面对低通滤波等防御时不易被抑制，从而增强了固有的鲁棒性。

Result: 广泛的实验表明，AGD在攻击性能和模型对某些防御的鲁棒性方面均优于现有最先进方法。

Conclusion: AGD通过创新性地利用扩散模型噪声的全频段特性，成功解决了生成高效且对防御具有鲁棒性的对抗性图像的挑战，有效攻击了MLLM并保持了图像的视觉质量。

Abstract: This paper addresses the challenge of generating adversarial image using a
diffusion model to deceive multimodal large language models (MLLMs) into
generating the targeted responses, while avoiding significant distortion of the
clean image. To address the above challenges, we propose an adversarial-guided
diffusion (AGD) approach for adversarial attack MLLMs. We introduce
adversarial-guided noise to ensure attack efficacy. A key observation in our
design is that, unlike most traditional adversarial attacks which embed
high-frequency perturbations directly into the clean image, AGD injects target
semantics into the noise component of the reverse diffusion. Since the added
noise in a diffusion model spans the entire frequency spectrum, the adversarial
signal embedded within it also inherits this full-spectrum property.
Importantly, during reverse diffusion, the adversarial image is formed as a
linear combination of the clean image and the noise. Thus, when applying
defenses such as a simple low-pass filtering, which act independently on each
component, the adversarial image within the noise component is less likely to
be suppressed, as it is not confined to the high-frequency band. This makes AGD
inherently robust to variety defenses. Extensive experiments demonstrate that
our AGD outperforms state-of-the-art methods in attack performance as well as
in model robustness to some defenses.

</details>


### [86] [Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images](https://arxiv.org/abs/2507.23206)
*Xiaoyu Ji,Ali Shakouri,Fengqing Zhu*

Main category: cs.CV

TL;DR: 针对食品晶体团聚体2D显微图像人工标注困难的问题，本文提出一种结合监督基线模型和实例分类模型的深度学习方法，通过生成伪标签和像素级分割，有效提升了团聚体分类精度和尺寸分布预测。


<details>
  <summary>Details</summary>
Motivation: 食品晶体团聚体会捕获水分并影响产品质量。然而，由于水粘合的透明性和图像视角限制，在2D显微图像中手动标注团聚体极其困难，因此需要一种更有效的方法来解决此挑战。

Method: 首先，提出一个监督基线模型，用于为粗略标注的分类数据集生成分割伪标签。其次，训练一个同时执行像素级分割的实例分类模型。在推理阶段结合这两个模型的优势。此外，设计并包含一个后处理模块，以在所有步骤中保留晶体特性。

Result: 与现有方法相比，本方法提高了真阳性团聚体的分类准确性，并改善了尺寸分布预测。即使在手动标注置信水平存在变异性的情况下，该方法也能成功分类潜在的团聚实例。

Conclusion: 该方法能够有效解决食品晶体团聚体在2D显微图像中识别和标注的挑战，显著提升了团聚体的分类准确性和尺寸预测能力。

Abstract: Food crystal agglomeration is a phenomenon occurs during crystallization
which traps water between crystals and affects food product quality. Manual
annotation of agglomeration in 2D microscopic images is particularly difficult
due to the transparency of water bonding and the limited perspective focusing
on a single slide of the imaged sample. To address this challenge, we first
propose a supervised baseline model to generate segmentation pseudo-labels for
the coarsely labeled classification dataset. Next, an instance classification
model that simultaneously performs pixel-wise segmentation is trained. Both
models are used in the inference stage to combine their respective strengths in
classification and segmentation. To preserve crystal properties, a post
processing module is designed and included to both steps. Our method improves
true positive agglomeration classification accuracy and size distribution
predictions compared to other existing methods. Given the variability in
confidence levels of manual annotations, our proposed method is evaluated under
two confidence levels and successfully classifies potential agglomerated
instances.

</details>


### [87] [YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection](https://arxiv.org/abs/2507.23225)
*Zicheng Lin,Weichao Pan*

Main category: cs.CV

TL;DR: 针对路面损伤检测中现有深度学习模型小目标检测能力不足和计算量大的问题，本文提出YOLO-ROC模型，通过改进特征提取和通道压缩，实现高精度、轻量化，并显著提升小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习路面损伤检测方法存在两大核心挑战：一是多尺度特征提取能力不足，导致对裂缝、坑洼等小尺度损伤漏检率高；二是主流模型参数量大、计算需求高，阻碍了在实际应用中实现高效实时部署。

Method: 提出了一种高精度轻量级模型YOLO-Road Orthogonal Compact (YOLO-ROC)。主要方法包括：1. 设计了双向多尺度空间金字塔池化快速（BMS-SPPF）模块，利用双向空间-通道注意力机制增强多尺度特征提取，以提升小目标检测。2. 实施了分层通道压缩策略，以降低计算复杂度和模型参数量。

Result: 1. 在RDD2022_China_Drone数据集上，YOLO-ROC的mAP50达到67.6%，超越基线YOLOv8n 2.11%。2. 针对小目标D40类别的mAP50显著提升16.8%。3. 模型参数量从3.01M降至0.89M，GFLOPs从8.1降至2.6。4. 最终模型大小仅为2.0 MB。5. 在RDD2022_China_Motorbike数据集上展现出优秀的泛化性能。

Conclusion: YOLO-ROC模型成功解决了当前路面损伤检测领域在小目标识别和模型部署效率上的挑战，通过创新的模块设计和压缩策略，实现了高精度、轻量化与良好泛化能力的平衡，为交通安全和基础设施维护提供了更实用、高效的解决方案。

Abstract: Road damage detection is a critical task for ensuring traffic safety and
maintaining infrastructure integrity. While deep learning-based detection
methods are now widely adopted, they still face two core challenges: first, the
inadequate multi-scale feature extraction capabilities of existing networks for
diverse targets like cracks and potholes, leading to high miss rates for
small-scale damage; and second, the substantial parameter counts and
computational demands of mainstream models, which hinder their deployment for
efficient, real-time detection in practical applications. To address these
issues, this paper proposes a high-precision and lightweight model, YOLO - Road
Orthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial
Pyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature
extraction and implemented a hierarchical channel compression strategy to
reduce computational complexity. The BMS-SPPF module leverages a bidirectional
spatial-channel attention mechanism to improve the detection of small targets.
Concurrently, the channel compression strategy reduces the parameter count from
3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the
RDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of
67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the
small-target D40 category improved by 16.8%, and the final model size is only
2.0 MB. Furthermore, the model exhibits excellent generalization performance on
the RDD2022_China_Motorbike dataset.

</details>


### [88] [Toward Safe, Trustworthy and Realistic Augmented Reality User Experience](https://arxiv.org/abs/2507.23226)
*Yanming Xiu*

Main category: cs.CV

TL;DR: 本研究旨在保障增强现实（AR）内容的安全性，通过开发系统检测有害内容，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AR日益融入日常生活，其虚拟内容的安全性和可信赖性变得至关重要。本研究旨在解决有害AR内容（例如阻碍关键信息或微妙操纵用户感知）带来的风险。

Method: 开发了ViDDAR和VIM-Sense两个系统，利用视觉语言模型（VLMs）和多模态推理模块来检测此类攻击。

Result: 成功开发出ViDDAR和VIM-Sense系统，能够利用VLMs和多模态推理模块检测任务有害的AR内容攻击。

Conclusion: 本工作旨在建立一个可扩展、以人为本的AR体验安全保障框架。未来将聚焦于虚拟内容的感知质量评估、多模态攻击检测以及VLMs在AR设备上的高效部署。

Abstract: As augmented reality (AR) becomes increasingly integrated into everyday life,
ensuring the safety and trustworthiness of its virtual content is critical. Our
research addresses the risks of task-detrimental AR content, particularly that
which obstructs critical information or subtly manipulates user perception. We
developed two systems, ViDDAR and VIM-Sense, to detect such attacks using
vision-language models (VLMs) and multimodal reasoning modules. Building on
this foundation, we propose three future directions: automated, perceptually
aligned quality assessment of virtual content; detection of multimodal attacks;
and adaptation of VLMs for efficient and user-centered deployment on AR
devices. Overall, our work aims to establish a scalable, human-aligned
framework for safeguarding AR experiences and seeks feedback on perceptual
modeling, multimodal AR content implementation, and lightweight model
adaptation.

</details>


### [89] [Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.23237)
*Fan Lyu,Linglan Zhao,Chengyan Liu,Yinying Mei,Zhang Zhang,Jian Zhang,Fuyuan Hu,Liang Wang*

Main category: cs.CV

TL;DR: 针对半监督少样本类增量学习（Semi-FSCIL）中无标签数据来源假设不切实际的问题，本文提出了广义半监督少样本类增量学习（GSemi-FSCIL）并提出ALDC策略，利用基类样本校准新类特征分布，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督少样本类增量学习（Semi-FSCIL）假设无标签数据仅来源于当前会话的新类，这与实际场景不符，视角狭窄。需要一个更贴近实际的定义，即无标签数据应包含基类和所有已见过的新类。此变化对现有方法构成了新挑战，因其难以区分广义无标签集中的基类和新类样本。

Method: 本文将Semi-FSCIL重新定义为广义半监督少样本类增量学习（GSemi-FSCIL），其无标签数据集中包含基类和所有已见过的所有新类。为解决新挑战，提出了一种名为“歧义引导可学习分布校准（Ambiguity-guided Learnable Distribution Calibration, ALDC）”的策略。ALDC动态地利用丰富的基类样本来纠正少样本新类的有偏特征分布。

Result: 在三个基准数据集上进行的实验表明，本文提出的方法优于现有工作，并取得了新的最先进（state-of-the-art）结果。

Conclusion: 通过重新定义Semi-FSCIL并提出ALDC策略，本文有效解决了半监督少样本类增量学习中无标签数据来源假设不真实的问题，提升了模型在复杂实际场景下的学习能力和性能，达到了SOTA水平。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new
concepts from limited data while retaining knowledge of previous classes.
Recently, many studies have started to leverage unlabeled samples to assist
models in learning from few-shot samples, giving rise to the field of
Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,
these studies often assume that the source of unlabeled data is only confined
to novel classes of the current session, which presents a narrow perspective
and cannot align well with practical scenarios. To better reflect real-world
scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by
incorporating both base and all the ever-seen novel classes in the unlabeled
set. This change in the composition of unlabeled samples poses a new challenge
for existing methods, as they struggle to distinguish between unlabeled samples
from base and novel classes. To address this issue, we propose an
Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC
dynamically uses abundant base samples to correct biased feature distributions
for few-shot novel classes. Experiments on three benchmark datasets show that
our method outperforms existing works, setting new state-of-the-art results.

</details>


### [90] [Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents](https://arxiv.org/abs/2507.23242)
*Sungguk Cha,DongWook Kim,Taeseung Hahn,Mintae Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CV

TL;DR: RL-QR是一个无需人工标注的强化学习框架，用于优化RAG系统查询，尤其提升了多模态和词汇检索器的性能，但在语义检索方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）系统依赖有效查询来获取外部知识，但针对多样化、非结构化的真实世界文档优化查询仍是一个重大挑战。

Method: 本文提出了RL-QR框架，一个基于强化学习的、针对特定检索器的查询重写方法。该方法无需人工标注数据，通过合成场景-问题对并利用广义奖励策略优化（GRPO）来训练查询重写器，以适用于文本和多模态数据库，从而提升检索性能。

Result: 在工业内部数据集上的实验表明，RL-QR取得了显著改进：RL-QR多模态版本在多模态RAG的NDCG@3上实现了11%的相对提升；RL-QR词汇版本使词汇检索器性能提升9%。然而，对于语义和混合检索器，查询重写器未能提升性能，这可能归因于训练对齐问题。

Conclusion: RL-QR展示了在RAG系统查询优化方面的巨大潜力，提供了一种可扩展、无需标注的真实世界检索任务解决方案。同时，也指出了在语义检索背景下进一步完善的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on effective query
formulation to unlock external knowledge, yet optimizing queries for diverse,
unstructured real-world documents remains a challenge. We introduce
\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query
rewriting that eliminates the need for human-annotated datasets and extends
applicability to both text-only and multi-modal databases. By synthesizing
scenario-question pairs and leveraging Generalized Reward Policy Optimization
(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing
retrieval performance across varied domains. Experiments on industrial in-house
data demonstrate significant improvements, with
$\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3
for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for
lexical retrievers. However, challenges persist with semantic and hybrid
retrievers, where rewriters failed to improve performance, likely due to
training misalignments. Our findings highlight RL-QR's potential to
revolutionize query optimization for RAG systems, offering a scalable,
annotation-free solution for real-world retrieval tasks, while identifying
avenues for further refinement in semantic retrieval contexts.

</details>


### [91] [Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas](https://arxiv.org/abs/2507.23245)
*Lei Xie,Jiahao Huang,Jiawei Zhang,Jianzhong He,Yiang Pan,Guoqiang Xie,Mengjun Li,Qingrun Zeng,Mingchu Li,Yuanjing Feng*

Main category: cs.CV

TL;DR: 开发了一种基于扩散磁共振成像（dMRI）的综合颅神经纤维束图谱，实现了颅神经通路的自动化映射。


<details>
  <summary>Details</summary>
Motivation: 颅神经映射对术前洞察至关重要，但由于颅神经独特的解剖结构和颅底环境的复杂性，构建全面详细的颅神经图谱极具挑战性。

Method: 通过多参数纤维束追踪生成流线，并对来自人类连接组计划（HCP）50名受试者的约100万条流线采用了一种新的多阶段纤维聚类策略，以生成颅神经图谱。

Result: 所提出的颅神经图谱在多个采集站点（包括HCP、MDM数据集和两个垂体腺瘤临床病例）上与专家手动标注具有高空间一致性。它能自动识别与5对颅神经（CN II, III, V, VII/VIII）相关的8个纤维束，并实验证明了其鲁棒性。

Conclusion: 该工作促进了多对颅神经通路的更高效、自动化映射，从而通过可视化其与周围解剖结构的空间关系，增强了对复杂脑结构的分析和理解。

Abstract: Cranial nerves (CNs) play a crucial role in various essential functions of
the human brain, and mapping their pathways from diffusion MRI (dMRI) provides
valuable preoperative insights into the spatial relationships between
individual CNs and key tissues. However, mapping a comprehensive and detailed
CN atlas is challenging because of the unique anatomical structures of each CN
pair and the complexity of the skull base environment.In this work, we present
what we believe to be the first study to develop a comprehensive diffusion
tractography atlas for automated mapping of CN pathways in the human brain. The
CN atlas is generated by fiber clustering by using the streamlines generated by
multi-parametric fiber tractography for each pair of CNs. Instead of disposable
clustering, we explore a new strategy of multi-stage fiber clustering for
multiple analysis of approximately 1,000,000 streamlines generated from the 50
subjects from the Human Connectome Project (HCP). Quantitative and visual
experiments demonstrate that our CN atlas achieves high spatial correspondence
with expert manual annotations on multiple acquisition sites, including the HCP
dataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of
pituitary adenoma patients. The proposed CN atlas can automatically identify 8
fiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,
oculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear
nerve CN VII/VIII, and its robustness is demonstrated experimentally. This work
contributes to the field of diffusion imaging by facilitating more efficient
and automated mapping the pathways of multiple pairs of CNs, thereby enhancing
the analysis and understanding of complex brain structures through
visualization of their spatial relationships with nearby anatomy.

</details>


### [92] [A Deep Dive into Generic Object Tracking: A Survey](https://arxiv.org/abs/2507.23251)
*Fereshteh Aghaee Meibodi,Shadi Alijani,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文对通用目标跟踪领域的三大主流范式（Siamese、判别式、Transformer）进行了全面综述，并特别关注快速发展的Transformer方法，提出了新的分类和统一的比较。


<details>
  <summary>Details</summary>
Motivation: 通用目标跟踪因复杂的时空动态、遮挡、相似干扰物和外观变化等挑战而仍具重要性。现有综述论文未能全面涵盖所有主流范式，特别是对快速发展的Transformer方法的系统性回顾存在不足。

Method: 本文对Siamese、判别式和Transformer三大类跟踪范式进行了全面回顾。通过定性和定量比较，分析了每种方法的核心设计原理、创新点和局限性。研究引入了新的分类方法，提供了代表性方法的统一视觉和表格比较，并从多角度组织了现有跟踪器，总结了主要评估基准。

Result: 通过综述发现，Transformer基跟踪方法因其强大的时空建模能力，正经历快速发展并表现出显著的进步。

Conclusion: Transformer基跟踪方法凭借其强大的时空建模能力，在通用目标跟踪领域展现出快速进步和巨大潜力。本综述为理解该领域的最新进展提供了一个统一且全面的视角。

Abstract: Generic object tracking remains an important yet challenging task in computer
vision due to complex spatio-temporal dynamics, especially in the presence of
occlusions, similar distractors, and appearance variations. Over the past two
decades, a wide range of tracking paradigms, including Siamese-based trackers,
discriminative trackers, and, more recently, prominent transformer-based
approaches, have been introduced to address these challenges. While a few
existing survey papers in this field have either concentrated on a single
category or widely covered multiple ones to capture progress, our paper
presents a comprehensive review of all three categories, with particular
emphasis on the rapidly evolving transformer-based methods. We analyze the core
design principles, innovations, and limitations of each approach through both
qualitative and quantitative comparisons. Our study introduces a novel
categorization and offers a unified visual and tabular comparison of
representative methods. Additionally, we organize existing trackers from
multiple perspectives and summarize the major evaluation benchmarks,
highlighting the fast-paced advancements in transformer-based tracking driven
by their robust spatio-temporal modeling capabilities.

</details>


### [93] [Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality](https://arxiv.org/abs/2507.23253)
*Mingyang Yu,Xiahui Guo,Peng chen,Zhenkai Li,Yang Shu*

Main category: cs.CV

TL;DR: 针对传统时间序列评估指标的不足，本文提出新的几何结构评估指标TGSI和多组件训练损失函数SATL，旨在增强模型对时间序列几何结构的学习与评估能力，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测的数值指标（如MSE）仅衡量点对点精度，无法有效评估时间序列数据的几何结构，而这对于理解其时序动态至关重要。

Method: 1. 提出“时间序列几何结构指数”（TGSI）作为新型评估指标，通过将时间序列转换为图像来利用其二维几何表示。2. 针对TGSI不可微分导致无法直接作为训练损失的问题，引入“形状感知时间损失”（SATL）作为多组件训练损失函数。3. SATL结合一阶差分损失（衡量结构一致性）、频域损失（捕捉周期模式）和感知特征损失（通过预训练特征提取器和自编码器对齐时序与几何结构特征）。

Result: 在多个数据集上的实验表明，使用SATL训练的模型在MSE和提出的TGSI指标上均优于基线方法，且在推理时未增加额外计算成本。

Conclusion: 本研究通过引入TGSI和SATL，有效解决了传统指标在时间序列几何结构评估上的不足，并提供了一种提升模型结构建模能力的方法，显著提升了预测模型的性能和对时序动态的理解。

Abstract: Time Series forecasting is critical in diverse domains such as weather
forecasting, financial investment, and traffic management. While traditional
numerical metrics like mean squared error (MSE) can quantify point-wise
accuracy, they fail to evaluate the geometric structure of time series data,
which is essential to understand temporal dynamics. To address this issue, we
propose the time series Geometric Structure Index (TGSI), a novel evaluation
metric that transforms time series into images to leverage their inherent
two-dimensional geometric representations. However, since the image
transformation process is non-differentiable, TGSI cannot be directly
integrated as a training loss. We further introduce the Shape-Aware Temporal
Loss (SATL), a multi-component loss function operating in the time series
modality to bridge this gap and enhance structure modeling during training.
SATL combines three components: a first-order difference loss that measures
structural consistency through the MSE between first-order differences, a
frequency domain loss that captures essential periodic patterns using the Fast
Fourier Transform while minimizing noise, and a perceptual feature loss that
measures geometric structure difference in time-series by aligning temporal
features with geometric structure features through a pre-trained temporal
feature extractor and time-series image autoencoder. Experiments across
multiple datasets demonstrate that models trained with SATL achieve superior
performance in both MSE and the proposed TGSI metrics compared to baseline
methods, without additional computational cost during inference.

</details>


### [94] [Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels](https://arxiv.org/abs/2507.23263)
*Haoxian Ruan,Zhihua Xu,Zhijing Yang,Guang Ma,Jieming Xie,Changxiang Fan,Tianshui Chen*

Main category: cs.CV

TL;DR: 针对部分标签多标签图像识别中伪标签不准确的问题，本文提出语义感知阈值学习（SATL）算法，通过动态计算类别特定阈值并引入差异化排序损失，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的部分标签多标签图像识别方法依赖预设阈值生成伪标签，但忽视了不同类别间分数分布的差异，导致伪标签不准确和不完整，进而影响模型性能。

Method: 提出语义感知阈值学习（SATL）算法。该方法动态计算并更新每个类别的正负样本分数分布，并基于这些分布确定类别特定阈值。此外，引入差异化排序损失，以增大正负样本分数分布之间的区分度，提升阈值判别能力。

Result: 在Microsoft COCO和VG-200等大型多标签数据集上的综合实验表明，所提出的方法在标签受限场景下显著提升了性能。

Conclusion: 所提出的SATL算法通过动态、类别特定的阈值学习和差异化排序损失，有效解决了部分标签多标签图像识别中伪标签不准确的问题，显著提高了模型在有限标签情况下的识别性能。

Abstract: Multi-label image recognition with partial labels (MLR-PL) is designed to
train models using a mix of known and unknown labels. Traditional methods rely
on semantic or feature correlations to create pseudo-labels for unidentified
labels using pre-set thresholds. This approach often overlooks the varying
score distributions across categories, resulting in inaccurate and incomplete
pseudo-labels, thereby affecting performance. In our study, we introduce the
Semantic-Aware Threshold Learning (SATL) algorithm. This innovative approach
calculates the score distribution for both positive and negative samples within
each category and determines category-specific thresholds based on these
distributions. These distributions and thresholds are dynamically updated
throughout the learning process. Additionally, we implement a differential
ranking loss to establish a significant gap between the score distributions of
positive and negative samples, enhancing the discrimination of the thresholds.
Comprehensive experiments and analysis on large-scale multi-label datasets,
such as Microsoft COCO and VG-200, demonstrate that our method significantly
improves performance in scenarios with limited labels.

</details>


### [95] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出PixelNerd，一种基于神经场的单阶段扩散模型，能直接在像素空间生成高质量图像，避免了VAE带来的误差和复杂管道。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer依赖预训练VAE的潜在空间，导致累积误差和解码伪影；而直接在像素空间操作的方案又过于复杂且增加了token复杂度。

Method: 我们提出PixelNerd，通过使用神经场模型进行块级解码，实现了一个单尺度、单阶段、高效且端到端的解决方案。

Result: PixelNerd在ImageNet 256x256上达到了2.15 FID，在ImageNet 512x512上达到了2.84 FID，且无需任何复杂的级联管道或VAE。该框架还被扩展到文生图应用，PixelNerd-XXL/16在GenEval基准上获得了0.73的总分，在DPG基准上获得了80.9的总分。

Conclusion: PixelNerd提供了一个直接在像素空间高效生成高质量图像的新范式，有效解决了现有扩散模型依赖VAE带来的问题，并展示了其在图像生成和文生图任务上的竞争力。

Abstract: The current success of diffusion transformers heavily depends on the
compressed latent space shaped by the pre-trained variational autoencoder(VAE).
However, this two-stage training paradigm inevitably introduces accumulated
errors and decoding artifacts. To address the aforementioned problems,
researchers return to pixel space at the cost of complicated cascade pipelines
and increased token complexity. In contrast to their efforts, we propose to
model the patch-wise decoding with neural field and present a single-scale,
single-stage, efficient, end-to-end solution, coined as pixel neural field
diffusion~(PixelNerd). Thanks to the efficient neural field representation in
PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID
on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also
extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16
achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9
overall score on the DPG benchmark.

</details>


### [96] [Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2](https://arxiv.org/abs/2507.23272)
*Solha Kang,Eugene Kim,Joris Vankerschaver,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本研究探索了如何将SAM2模型应用于乳腺MRI的低成本、少量输入3D肿瘤分割，结果表明其在最小监督下仍能实现强大的分割性能，为资源受限地区提供了可行的替代方案。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI对肿瘤评估和治疗规划至关重要，但手动解读3D扫描耗时且主观。现有AI工具因高昂许可费、专有软件和基础设施要求，在低收入和中等收入国家应用受限，因此需要经济可行的替代方案。

Method: 研究人员探讨了SAM2模型在乳腺MRI中进行低成本、少量输入3D肿瘤分割的可行性。具体方法是，仅使用单一切片上的一个边界框注释，然后采用三种不同的切片级追踪策略（从上到下、从下到上、中心向外）将分割预测传播到整个3D体积。研究人员在大批患者队列中评估了这些策略，并分析了分割性能与肿瘤大小、位置和形状的关系。

Result: 研究发现，中心向外的传播策略产生了最一致和准确的分割结果。尽管SAM2是一个未针对体积医学数据训练的零样本模型，但在最小监督下仍实现了强大的分割性能。此外，研究还识别了与肿瘤大小、位置和形状相关的关键失败模式。

Conclusion: 研究结果表明，像SAM2这样的通用基础模型在最小监督下也能支持3D医学图像分析，为资源受限环境提供了一种可及且经济的替代方案。

Abstract: Breast MRI provides high-resolution volumetric imaging critical for tumor
assessment and treatment planning, yet manual interpretation of 3D scans
remains labor-intensive and subjective. While AI-powered tools hold promise for
accelerating medical image analysis, adoption of commercial medical AI products
remains limited in low- and middle-income countries due to high license costs,
proprietary software, and infrastructure demands. In this work, we investigate
whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,
minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box
annotation on one slice, we propagate segmentation predictions across the 3D
volume using three different slice-wise tracking strategies: top-to-bottom,
bottom-to-top, and center-outward. We evaluate these strategies across a large
cohort of patients and find that center-outward propagation yields the most
consistent and accurate segmentations. Despite being a zero-shot model not
trained for volumetric medical data, SAM2 achieves strong segmentation
performance under minimal supervision. We further analyze how segmentation
performance relates to tumor size, location, and shape, identifying key failure
modes. Our results suggest that general-purpose foundation models such as SAM2
can support 3D medical image analysis with minimal supervision, offering an
accessible and affordable alternative for resource-constrained settings.

</details>


### [97] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

TL;DR: 提出iLRM模型，通过迭代优化和分阶段注意力机制，解决了现有前向3D重建方法在处理多视图时的扩展性问题，显著提升了重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA的前向3D重建方法（多基于Transformer）在处理多视图或高分辨率图像时，因依赖全注意力机制，导致计算成本过高，存在严重的扩展性问题。

Method: 引入迭代大型3D重建模型（iLRM），通过迭代细化机制生成3D高斯表示。核心策略包括：1) 将场景表示与输入图像解耦，实现紧凑3D表示；2) 将多视图交互分解为两阶段注意力，降低计算成本；3) 在每一层注入高分辨率信息，实现高保真重建。

Result: 在RE10K和DL3DV等数据集上的实验结果表明，iLRM在重建质量和速度上均超越现有方法。特别是在可扩展性方面表现卓越，能在相似计算成本下，通过有效利用更多输入视图显著提高重建质量。

Conclusion: iLRM提供了一种可扩展且高效的前向3D重建方案，有效解决了现有方法的扩展性瓶颈，并在处理大量输入视图时展现出色的重建质量和效率。

Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and
high-quality 3D reconstruction. In particular, directly generating explicit 3D
representations, such as 3D Gaussian splatting, has attracted significant
attention due to its fast and high-quality rendering, as well as numerous
applications. However, many state-of-the-art methods, primarily based on
transformer architectures, suffer from severe scalability issues because they
rely on full attention across image tokens from multiple input views, resulting
in prohibitive computational costs as the number of views or image resolution
increases. Toward a scalable and efficient feed-forward 3D reconstruction, we
introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D
Gaussian representations through an iterative refinement mechanism, guided by
three core principles: (1) decoupling the scene representation from input-view
images to enable compact 3D representations; (2) decomposing fully-attentional
multi-view interactions into a two-stage attention scheme to reduce
computational costs; and (3) injecting high-resolution information at every
layer to achieve high-fidelity reconstruction. Experimental results on widely
used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms
existing methods in both reconstruction quality and speed. Notably, iLRM
exhibits superior scalability, delivering significantly higher reconstruction
quality under comparable computational cost by efficiently leveraging a larger
number of input views.

</details>


### [98] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

TL;DR: 本文提出了UniLIP，一个将CLIP扩展到重建、生成和编辑的统一模型。通过两阶段训练和双条件架构，UniLIP在保持CLIP原有理解能力的同时，显著提升了在文本到图像生成和图像编辑任务上的性能，超越了现有同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的统一方法在支持图像重建和生成时，常需额外的扩散解码器或量化，这导致重建效果不一致或原有理解性能下降，未能充分发挥CLIP的强大理解能力。

Method: 本文提出了UniLIP。采用两阶段训练方案和自蒸馏策略，将重建能力逐步融入CLIP，同时保持其原始理解性能。此外，引入了双条件架构，连接多模态大语言模型（MLLM）和扩散Transformer，利用可学习查询和最后一层多模态隐藏状态作为联合条件，以利用MLLM的推理能力和UniLIP特征的丰富信息。

Result: 在文本到图像生成任务中，UniLIP在GenEval和WISE基准上分别获得0.87和0.53分，超越了所有同规模的先前统一模型。在图像编辑任务中，UniLIP在ImgEdit基准上达到3.62分，超越了BAGEL和UniWorld-V1等现有先进模型。

Conclusion: UniLIP成功扩展了CLIP的应用范围，使其连续特征不仅在理解任务中表现卓越，在生成和编辑任务中也展现出高度竞争力，为CLIP在更广泛应用场景中的利用提供了新的可能性。

Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction,
generation and editing, thereby building a unified tokenizer upon its
exceptional comprehension capabilities. Previous CLIP-based unified methods
often require additional diffusion decoders or quantization to support
reconstruction and generation tasks, leading to inconsistent reconstruction or
degradation of original comprehension performance.In contrast, we introduce a
two-stage training scheme and a self-distillation strategy that progressively
integrates reconstruction capabilities into CLIP, allowing it to maintain
original comprehension performance while achieving effective image
reconstruction. Furthermore, we propose a dual-condition architecture to
connect the MLLM and diffusion transformer, using both learnable queries and
the last layer multimodal hidden states as joint conditions. This method not
only enables the utilization of the MLLM's strong reasoning capabilities in
generation tasks, but also maximizes the exploitation of the rich information
in UniLIP features during editing tasks. In text-to-image generation tasks,
UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark
respectively, surpassing all previous unified models of similar scale. In image
editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,
surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP
effectively expand the application scope of CLIP, enabling continuous CLIP
features to not only serve as the optimal choice for understanding tasks but
also achieve highly competitive performance in generation and editing tasks.

</details>


### [99] [Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval](https://arxiv.org/abs/2507.23284)
*Dohwan Ko,Ji Soo Lee,Minhyuk Choi,Zihang Meng,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文提出BLiM框架及CPN模块，以解决多模态大语言模型（MLLMs）在文本-视频检索中存在的候选先验偏差问题。通过双向似然估计和无训练的先验归一化，显著提升了检索性能，并证实CPN在多模态任务中的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究将多模态大语言模型（MLLMs）应用于文本-视频检索时，简单地基于候选似然进行检索会导致“候选先验偏差”，即模型偏好固有先验概率高的候选，而非与查询更相关的候选。

Method: 提出新颖的BLiM (Bidirectional Likelihood Estimation with MLLM) 检索框架，通过训练模型进行双向似然估计（从视频生成文本、从文本生成视频特征）。此外，引入了CPN (Candidate Prior Normalization) 模块，这是一个无需训练且有效的得分校准模块，旨在缓解候选似然中的先验偏差。

Result: 在四个文本-视频检索基准测试中，结合CPN的BLiM模型平均超越现有SOTA模型6.4 R@1，有效缓解了候选先验偏差并突出了查询-候选相关性。研究还表明CPN在其他多模态任务中也具有广泛适用性，能通过减少对文本先验的依赖来增强视觉理解。

Conclusion: BLiM与CPN的结合能够有效解决MLLM在文本-视频检索中的候选先验偏差问题，显著提升检索性能，并且CPN被证明在更广泛的多模态任务中具有增强视觉理解的潜力。

Abstract: Text-Video Retrieval aims to find the most relevant text (or video) candidate
given a video (or text) query from large-scale online databases. Recent work
leverages multi-modal large language models (MLLMs) to improve retrieval,
especially for long or complex query-candidate pairs. However, we observe that
the naive application of MLLMs, i.e., retrieval based on candidate likelihood,
introduces candidate prior bias, favoring candidates with inherently higher
priors over those more relevant to the query. To this end, we propose a novel
retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),
which leverages both query and candidate likelihoods by training the model to
generate text from a given video as well as video features from a given text.
Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet
effective training-free score calibration module designed to mitigate candidate
prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,
our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4
R@1 on average, effectively alleviating candidate prior bias and emphasizing
query-candidate relevance. Our in-depth analysis across various multi-modal
tasks beyond retrieval highlights the broad applicability of CPN which enhances
visual understanding by reducing reliance on textual priors. Code is available
at https://github.com/mlvlab/BLiM.

</details>


### [100] [LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis](https://arxiv.org/abs/2507.23295)
*Inbum Heo,Taewook Hwang,Jeesu Jung,Sangkeun Jung*

Main category: cs.CV

TL;DR: 针对现有文档布局分析（DLA）模型在处理结构性错误及传统评估指标不足的问题，本文提出了新的基准LED及其数据集，用于有效检测和评估DLA预测中的结构性错误，揭示了传统指标无法发现的模型特性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和多模态模型显著提升了文档布局分析的检测能力，但在处理区域合并、分割和内容缺失等关键结构性错误方面仍面临挑战。传统的评估指标（如IoU和mAP）主要关注空间重叠，不足以检测这些结构性错误，因此需要一种更有效的方法来评估模型的结构鲁棒性。

Method: 本文提出了Layout Error Detection (LED) 新基准，旨在评估文档布局预测的结构鲁棒性。LED定义了八种标准化错误类型，并设计了错误存在检测、错误类型分类和元素级错误类型分类三个互补任务。此外，还构建了LED-Dataset，这是一个通过注入基于DLA模型经验分布的真实结构性错误而生成的合成数据集。

Result: 对一系列大语言多模态模型（LMMs）的实验结果表明，LED能有效区分模型的结构理解能力，并揭示了传统指标无法检测到的模态偏差和性能权衡。

Conclusion: LED基准成功解决了传统文档布局分析评估方法在检测结构性错误方面的局限性。通过提供对模型结构理解能力的更深入洞察，LED为未来开发更鲁棒的DLA模型提供了关键的评估工具和方向。

Abstract: Recent advancements in Document Layout Analysis through Large Language Models
and Multimodal Models have significantly improved layout detection. However,
despite these improvements, challenges remain in addressing critical structural
errors, such as region merging, splitting, and missing content. Conventional
evaluation metrics like IoU and mAP, which focus primarily on spatial overlap,
are insufficient for detecting these errors. To address this limitation, we
propose Layout Error Detection (LED), a novel benchmark designed to evaluate
the structural robustness of document layout predictions. LED defines eight
standardized error types, and formulates three complementary tasks: error
existence detection, error type classification, and element-wise error type
classification. Furthermore, we construct LED-Dataset, a synthetic dataset
generated by injecting realistic structural errors based on empirical
distributions from DLA models. Experimental results across a range of LMMs
reveal that LED effectively differentiates structural understanding
capabilities, exposing modality biases and performance trade-offs not visible
through traditional metrics.

</details>


### [101] [Training-free Geometric Image Editing on Diffusion Models](https://arxiv.org/abs/2507.23300)
*Hanshen Zhu,Zhen Zhu,Kaile Zhang,Yiming Gong,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出FreeFine，一种解耦的免训练扩散方法，用于几何图像编辑。通过分离变换、修复和细化步骤，在复杂变换下实现高保真和高精度编辑，并引入新的GeoBench基准。


<details>
  <summary>Details</summary>
Motivation: 几何图像编辑任务中，现有基于扩散的方法常将所有子任务整合为一步，在面对大幅度或结构复杂的变换时，难以有效保持场景连贯性和编辑精度。

Method: 本文提出一个解耦的流水线，将几何图像编辑分解为对象变换、源区域修复和目标区域细化三个独立步骤。其中，修复和细化均采用名为FreeFine的免训练扩散方法实现。

Result: 在新建的GeoBench基准测试（涵盖2D和3D编辑场景）上，FreeFine在图像保真度和编辑精度方面均优于现有最先进的方法，尤其在处理高要求变换时性能显著提升。

Conclusion: 通过解耦编辑流程并引入免训练的FreeFine扩散方法，显著提升了几何图像编辑在复杂变换下的图像质量和编辑精度。同时，本文还贡献了一个新的评估基准。

Abstract: We tackle the task of geometric image editing, where an object within an
image is repositioned, reoriented, or reshaped while preserving overall scene
coherence. Previous diffusion-based editing methods often attempt to handle all
relevant subtasks in a single step, proving difficult when transformations
become large or structurally complex. We address this by proposing a decoupled
pipeline that separates object transformation, source region inpainting, and
target region refinement. Both inpainting and refinement are implemented using
a training-free diffusion approach, FreeFine. In experiments on our new
GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine
outperforms state-of-the-art alternatives in image fidelity, and edit
precision, especially under demanding transformations. Code and benchmark are
available at: https://github.com/CIawevy/FreeFine

</details>


### [102] [ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection](https://arxiv.org/abs/2507.23307)
*Xihang Hu,Fuming Sun,Jiazhe Liu,Feilong Xu,Xiaoli Zhang*

Main category: cs.CV

TL;DR: ST-SAM提出一种高效自训练半监督伪装目标检测（SSCOD）框架，通过单模型架构和结合SAM利用伪标签，仅用1%标注数据即可实现SOTA性能，克服了现有SSCOD方法的预测偏差和高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于师生框架的半监督伪装目标检测（SSCOD）方法在稀缺监督下存在严重的预测偏差和误差传播，且多网络架构导致高计算开销和有限的可扩展性。

Method: 提出ST-SAM框架，采用自训练策略，动态筛选并扩展高置信度伪标签以增强单个模型架构，从而避免模型间预测偏差。此外，将伪标签转换为包含领域特定知识的混合提示，有效利用Segment Anything Model (SAM) 的潜力来减轻自训练中的误差积累。

Result: 在COD基准数据集上，ST-SAM仅使用1%的标注数据就实现了最先进的性能，超越了现有SSCOD方法，甚至能媲美全监督方法。值得注意的是，ST-SAM只需训练一个网络，不依赖特定的模型或损失函数。

Conclusion: ST-SAM为标注高效的半监督伪装目标检测（SSCOD）建立了一个新范式。

Abstract: Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance
on costly pixel-level annotations by leveraging limited annotated data and
abundant unlabeled data. However, existing SSCOD methods based on
Teacher-Student frameworks suffer from severe prediction bias and error
propagation under scarce supervision, while their multi-network architectures
incur high computational overhead and limited scalability. To overcome these
limitations, we propose ST-SAM, a highly annotation-efficient yet concise
framework that breaks away from conventional SSCOD constraints. Specifically,
ST-SAM employs Self-Training strategy that dynamically filters and expands
high-confidence pseudo-labels to enhance a single-model architecture, thereby
fundamentally circumventing inter-model prediction bias. Furthermore, by
transforming pseudo-labels into hybrid prompts containing domain-specific
knowledge, ST-SAM effectively harnesses the Segment Anything Model's potential
for specialized tasks to mitigate error accumulation in self-training.
Experiments on COD benchmark datasets demonstrate that ST-SAM achieves
state-of-the-art performance with only 1\% labeled data, outperforming existing
SSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM
requires training only a single network, without relying on specific models or
loss functions. This work establishes a new paradigm for annotation-efficient
SSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.

</details>


### [103] [PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving](https://arxiv.org/abs/2507.23309)
*Xuewei Tang,Mengmeng Yang,Tuopu Wen,Peijin Jia,Le Cui,Mingshang Luo,Kehua Sheng,Bo Zhang,Diange Yang,Kun Jiang*

Main category: cs.CV

TL;DR: 本文提出了PriorFusion框架，通过整合语义、几何和生成先验信息，显著提升了在复杂自动驾驶环境中道路元素的感知精度和规律性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要精确可靠的道路感知技术，但在缺乏高精地图的复杂环境中，道路元素数量多、几何复杂且常被遮挡。现有方法未能充分利用道路元素的固有结构化先验，导致预测不准确和不规则。

Method: 提出PriorFusion统一框架，有效整合语义、几何和生成先验以增强道路元素感知。具体方法包括：引入由形状先验特征引导的实例感知注意力机制；构建数据驱动的形状模板空间，编码低维道路元素表示，并通过聚类生成参考先验锚点；设计基于扩散的框架，利用这些先验锚点生成准确且完整的预测。

Result: 在大规模自动驾驶数据集上的实验表明，该方法显著提高了感知精度，尤其是在复杂条件下。可视化结果进一步证实，其生成的道路元素预测更加准确、规则和连贯。

Conclusion: PriorFusion通过创新性地整合多模态先验信息和扩散模型，克服了复杂环境下道路元素感知的挑战，为自动驾驶提供了更精确、更可靠的感知能力。

Abstract: With the growing interest in autonomous driving, there is an increasing
demand for accurate and reliable road perception technologies. In complex
environments without high-definition map support, autonomous vehicles must
independently interpret their surroundings to ensure safe and robust
decision-making. However, these scenarios pose significant challenges due to
the large number, complex geometries, and frequent occlusions of road elements.
A key limitation of existing approaches lies in their insufficient exploitation
of the structured priors inherently present in road elements, resulting in
irregular, inaccurate predictions. To address this, we propose PriorFusion, a
unified framework that effectively integrates semantic, geometric, and
generative priors to enhance road element perception. We introduce an
instance-aware attention mechanism guided by shape-prior features, then
construct a data-driven shape template space that encodes low-dimensional
representations of road elements, enabling clustering to generate anchor points
as reference priors. We design a diffusion-based framework that leverages these
prior anchors to generate accurate and complete predictions. Experiments on
large-scale autonomous driving datasets demonstrate that our method
significantly improves perception accuracy, particularly under challenging
conditions. Visualization results further confirm that our approach produces
more accurate, regular, and coherent predictions of road elements.

</details>


### [104] [Forgetting of task-specific knowledge in model merging-based continual learning](https://arxiv.org/abs/2507.23311)
*Timm Hess,Gido M van de Ven,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 研究了持续学习中模型的线性合并，发现合并能保留共享知识但牺牲特定任务知识，且增量训练的模型合并效果更好。


<details>
  <summary>Details</summary>
Motivation: 探究持续学习背景下，模型线性合并对知识保留与退化的影响。

Method: 使用计算机视觉实验，通过受控视觉线索进行验证。

Result: 模型合并主要保留或增强了共享知识，同时未共享的任务特定知识迅速退化。此外，增量训练过程的模型合并始终优于并行训练的模型合并。

Conclusion: 在持续学习中，模型线性合并能有效处理共享知识但需注意非共享知识的退化，且增量训练的模型合并策略表现更优。

Abstract: This paper investigates the linear merging of models in the context of
continual learning (CL). Using controlled visual cues in computer vision
experiments, we demonstrate that merging largely preserves or enhances shared
knowledge, while unshared task-specific knowledge rapidly degrades. We further
find that merging models from an incremental training process consistently
outperforms merging models trained in parallel.

</details>


### [105] [The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models](https://arxiv.org/abs/2507.23313)
*Alfio Ferrara,Sergio Picascia,Elisabetta Rocchetti*

Main category: cs.CV

TL;DR: 研究文本到图像扩散模型如何内部区分艺术作品的内容与风格。通过分析交叉注意力热图，发现模型能自发地将内容词与物体区域关联，将风格词与背景/纹理关联，表明其对内容-风格有涌现式理解。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在艺术内容生成上能力强大，但其内部如何表示绘画中的内容和风格概念尚不明确。传统观点认为内容与风格正交，而扩散模型训练时无此显式指导，因此探究其内部机制至关重要。

Method: 利用交叉注意力热图，将生成图像的像素归因于特定的提示词（内容描述词或风格描述词），从而隔离受内容词或风格词影响的图像区域。

Result: 研究揭示，扩散模型根据具体的艺术提示和风格请求，表现出不同程度的内容-风格分离。多数情况下，内容词主要影响与物体相关的区域，而风格词则影响背景和纹理区域，这表明模型对内容-风格区分有涌现式理解。

Conclusion: 这些发现有助于我们理解大型生成模型如何在缺乏明确监督的情况下，内部表示复杂的艺术概念。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in
generating artistic content by learning from billions of images, including
popular artworks. However, the fundamental question of how these models
internally represent concepts, such as content and style in paintings, remains
unexplored. Traditional computer vision assumes content and style are
orthogonal, but diffusion models receive no explicit guidance about this
distinction during training. In this work, we investigate how transformer-based
text-to-image diffusion models encode content and style concepts when
generating artworks. We leverage cross-attention heatmaps to attribute pixels
in generated images to specific prompt tokens, enabling us to isolate image
regions influenced by content-describing versus style-describing tokens. Our
findings reveal that diffusion models demonstrate varying degrees of
content-style separation depending on the specific artistic prompt and style
requested. In many cases, content tokens primarily influence object-related
regions while style tokens affect background and texture areas, suggesting an
emergent understanding of the content-style distinction. These insights
contribute to our understanding of how large-scale generative models internally
represent complex artistic concepts without explicit supervision. We share the
code and dataset, together with an exploratory tool for visualizing attention
maps at https://github.com/umilISLab/artistic-prompt-interpretation.

</details>


### [106] [Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification](https://arxiv.org/abs/2507.23315)
*Vineet Kumar Rakesh,Soumya Mazumdar,Tapas Samanta,Sarbajit Pal,Amitabha Das*

Main category: cs.CV

TL;DR: 本文分析了超参数调整对七种轻量级深度学习模型在实时图像分类任务中准确性和收敛行为的影响，并为资源受限的实时应用提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 轻量级卷积和Transformer模型对于资源受限的实时图像分类（如嵌入式系统和边缘设备）至关重要。

Method: 选择了EfficientNetV2-S、ConvNeXt-T、MobileViT v2 (XXS/XS/S)、MobileNetV3-L、TinyViT-21M和RepVGG-A2七种高效深度学习架构。所有模型均在ImageNet-1K数据集上以一致的训练设置进行训练。通过消融研究分析了学习率调度、批次大小、输入分辨率、数据增强、正则化方法和优化器选择等关键超参数的影响。评估指标包括Top-1和Top-5分类精度、推理时间、参数数量、模型大小以及在GPU加速边缘部署模拟中的帧率（FPS）。

Result: 余弦学习率衰减和可调整的批次大小可显著提高准确性和收敛速度，同时保持低延迟和内存成本。RepVGG-A2实现了超过80%的Top-1精度，并具有高效的推理性能，在准确性和部署成本之间提供了良好的平衡。

Conclusion: 研究结果为构建适用于实时图像处理管道的资源高效深度学习模型提供了实用指导。

Abstract: Lightweight convolutional and transformer-based models have become vital for
real-time image classification in resource-constrained applications, such as
embedded systems and edge devices. This work analyzes the influence of
hyperparameter adjustment on the accuracy and convergence behavior of seven
efficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT
v2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are
trained on the ImageNet-1K dataset under consistent training settings, with an
emphasis on real-time practicality. An comprehensive ablation study is
undertaken to separate the effect of critical hyperparameters, including
learning rate schedules, batch sizes, input resolution, data augmentation,
regularization approaches, and optimizer choice. To assess appropriateness for
real-time applications, each model is assessed not only in terms of Top-1 and
Top-5 classification accuracy, but also in terms of inference time, parameter
count, model size, and frames-per-second (FPS) on a GPU-accelerated edge
deployment simulation. Results demonstrate that cosine learning rate decay and
adjustable batch size may greatly boost both accuracy and convergence speed,
while keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%
Top-1 accuracy with efficient inference performance, offering a compelling
balance between accuracy and deployment cost for VGG-style models. The results
give practical guidance for constructing resource-efficient deep learning
models appropriate for real-time image processing pipelines. All code and
training logs are publicly accessible at
https://github.com/VineetKumarRakesh/lcnn-opt.

</details>


### [107] [FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning](https://arxiv.org/abs/2507.23318)
*Jiajun Cao,Qizhe Zhang,Peidong Jia,Xuhui Zhao,Bo Lan,Xiaoan Zhang,Xiaobao Wei,Sixiang Chen,Zhuo Li,Yang Wang,Liyun Li,Xianming Liu,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: 为解决自动驾驶VLA模型计算成本高的问题，本文提出了FastDriveVLA框架及ReconPruner剪枝器，通过优先保留前景信息，显著提高了模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在自动驾驶中潜力巨大，但长视觉Tokens导致高计算成本。现有视觉Token剪枝方法（基于相似性或视觉-文本注意力）在自动驾驶场景中表现不佳。受人类驾驶员关注前景区域的启发，研究认为保留前景信息对有效决策至关重要。

Method: 提出FastDriveVLA，一个面向自动驾驶的基于重建的视觉Token剪枝框架。核心是ReconPruner，一个可插拔的视觉Token剪枝器，通过MAE风格的像素重建优先处理前景信息。设计了一种新颖的对抗性前景-背景重建策略来训练ReconPruner。为训练ReconPruner，构建了包含241K图像-掩码对的大型数据集nuScenes-FG。

Result: 该方法在nuScenes闭环规划基准测试中，在不同剪枝率下均取得了最先进（state-of-the-art）的结果。

Conclusion: FastDriveVLA及其ReconPruner提供了一种高效且高性能的视觉Token剪枝方案，能有效降低自动驾驶VLA模型的计算成本，同时保持甚至提升决策性能，且ReconPruner具有良好的可插拔性。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential
in complex scene understanding and action reasoning, leading to their
increasing adoption in end-to-end autonomous driving systems. However, the long
visual tokens of VLA models greatly increase computational costs. Current
visual token pruning methods in Vision-Language Models (VLM) rely on either
visual token similarity or visual-text attention, but both have shown poor
performance in autonomous driving scenarios. Given that human drivers
concentrate on relevant foreground areas while driving, we assert that
retaining visual tokens containing this foreground information is essential for
effective decision-making. Inspired by this, we propose FastDriveVLA, a novel
reconstruction-based vision token pruning framework designed specifically for
autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner
called ReconPruner, which prioritizes foreground information through MAE-style
pixel reconstruction. A novel adversarial foreground-background reconstruction
strategy is designed to train ReconPruner for the visual encoder of VLA models.
Once trained, ReconPruner can be seamlessly applied to different VLA models
with the same visual encoder without retraining. To train ReconPruner, we also
introduce a large-scale dataset called nuScenes-FG, consisting of 241K
image-mask pairs with annotated foreground regions. Our approach achieves
state-of-the-art results on the nuScenes closed-loop planning benchmark across
different pruning ratios.

</details>


### [108] [FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models](https://arxiv.org/abs/2507.23325)
*Yiming Yang,Hongbin Lin,Yueru Luo,Suzhong Fu,Chao Zheng,Xinrui Yan,Shuqi Mei,Kun Tang,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 提出FASTopoWM框架，通过结合快慢系统和潜在世界模型，解决了现有车道拓扑推理方法在时序信息利用上的局限性，显著提升了车道线检测和中心线感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有的车道拓扑推理方法未能有效利用时序信息来增强检测和推理性能。尤其流式时序传播方法存在过度依赖历史查询、易受姿态估计失败影响以及时序传播不足等局限性。

Method: 提出了FASTopoWM，一个新颖的结合潜在世界模型的快慢车道片段拓扑推理框架。为减少姿态估计失败的影响，该框架支持对历史和新初始化查询的并行监督，促进快慢系统间的相互强化。此外，引入了以动作潜在为条件的潜在查询和BEV世界模型，以将状态表示从过去观测传播到当前时间步。

Result: 在OpenLane-V2基准测试中，FASTopoWM在车道片段检测（mAP：37.4% 对比 33.6%）和中心线感知（OLS：46.3% 对比 41.5%）方面均超越了现有最先进方法。

Conclusion: FASTopoWM通过其创新的快慢系统和潜在世界模型设计，有效克服了当前车道拓扑推理方法的局限性，显著提升了性能，达到了最先进水平，为自动驾驶系统提供了关键的感知能力。

Abstract: Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)
road scene understanding, which can serve as a key perception module in
planning-oriented end-to-end autonomous driving systems. Existing lane topology
reasoning methods often fall short in effectively leveraging temporal
information to enhance detection and reasoning performance. Recently,
stream-based temporal propagation method has demonstrated promising results by
incorporating temporal cues at both the query and BEV levels. However, it
remains limited by over-reliance on historical queries, vulnerability to pose
estimation failures, and insufficient temporal propagation. To overcome these
limitations, we propose FASTopoWM, a novel fast-slow lane segment topology
reasoning framework augmented with latent world models. To reduce the impact of
pose estimation failures, this unified framework enables parallel supervision
of both historical and newly initialized queries, facilitating mutual
reinforcement between the fast and slow systems. Furthermore, we introduce
latent query and BEV world models conditioned on the action latent to propagate
the state representations from past observations to the current timestep. This
design substantially improves the performance of temporal perception within the
slow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate
that FASTopoWM outperforms state-of-the-art methods in both lane segment
detection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%
on OLS).

</details>


### [109] [Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation](https://arxiv.org/abs/2507.23326)
*Yingkai Wang,Yaoyao Zhu,Xiuding Cai,Yuhao Xiao,Haotian Wu,Yu Yao*

Main category: cs.CV

TL;DR: 本文提出一个医学图像分割的域泛化框架，通过隐式特征扰动和自适应一致性约束，有效解决了域偏移导致的性能下降问题，在多中心基准测试中表现出卓越的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床工作流程中至关重要，但现有模型在应用于未见过的临床领域时，由于成像条件等差异导致的“域偏移”问题，性能会显著下降，从而限制了其实际部署。

Method: 本文提出了一个针对医学图像分割的域泛化框架。该方法通过引入由域统计信息引导的隐式特征扰动来增强模型的鲁棒性。具体包括使用可学习的语义方向选择器和基于协方差的语义强度采样器来调节域变异特征，同时保留解剖一致性。此外，还设计了一个自适应一致性约束，仅在特征调整导致分割性能下降时选择性应用，以稳定特征选择并提高分割可靠性。

Result: 在两个公共多中心基准测试上的广泛实验表明，本文提出的框架持续优于现有域泛化方法，在不同的临床领域中实现了鲁棒且可泛化的分割性能。

Conclusion: 本框架有效解决了医学图像分割中的域偏移挑战，显著提升了模型在多样化临床环境下的泛化能力和鲁棒性，有助于推动医学图像分割模型的临床应用。

Abstract: Medical image segmentation plays a crucial role in clinical workflows, but
domain shift often leads to performance degradation when models are applied to
unseen clinical domains. This challenge arises due to variations in imaging
conditions, scanner types, and acquisition protocols, limiting the practical
deployment of segmentation models. Unlike natural images, medical images
typically exhibit consistent anatomical structures across patients, with
domain-specific variations mainly caused by imaging conditions. This unique
characteristic makes medical image segmentation particularly challenging.
  To address this challenge, we propose a domain generalization framework
tailored for medical image segmentation. Our approach improves robustness to
domain-specific variations by introducing implicit feature perturbations guided
by domain statistics. Specifically, we employ a learnable semantic direction
selector and a covariance-based semantic intensity sampler to modulate
domain-variant features while preserving task-relevant anatomical consistency.
Furthermore, we design an adaptive consistency constraint that is selectively
applied only when feature adjustment leads to degraded segmentation
performance. This constraint encourages the adjusted features to align with the
original predictions, thereby stabilizing feature selection and improving the
reliability of the segmentation.
  Extensive experiments on two public multi-center benchmarks show that our
framework consistently outperforms existing domain generalization approaches,
achieving robust and generalizable segmentation performance across diverse
clinical domains.

</details>


### [110] [Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision](https://arxiv.org/abs/2507.23331)
*Qiang Lu,Waikit Xiu,Xiying Li,Shenyu Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 本文提出了一种结合开放词汇检测和跨模态学习的两阶段框架，以应对交通标志识别中长尾分布和小目标检测的挑战，并在TT100K数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前交通标志识别技术面临两大挑战：一是交通标志数据集的长尾分布导致传统卷积网络在低频和分布外类别上识别性能下降；二是实际场景中交通标志多为小目标且尺度变化大，难以有效提取多尺度特征。

Method: 本文提出一个 novel 的两阶段框架：
1. 交通标志检测阶段：采用 NanoVerse YOLO 模型，该模型整合了 RepVL-PAN（可重参数化视觉-语言路径聚合网络）和 SPD-Conv 模块，以增强对小、多尺度目标的特征提取。
2. 交通标志分类阶段：设计了 TSR-MCL（交通标志识别多模态对比学习模型），通过对比 Vision Transformer 的视觉特征和基于规则的 BERT 的语义特征，学习鲁棒、频率独立的表示，以缓解数据不平衡导致的类别混淆。该框架结合了开放词汇检测和跨模态学习。

Result: 在TT100K数据集上，该方法在所有类别的长尾检测任务中实现了78.4%的mAP，达到了最先进水平。模型还获得了91.8%的准确率和88.9%的召回率，显著优于主流算法，在复杂开放世界场景中表现出卓越的准确性和泛化能力。

Conclusion: 所提出的结合开放词汇检测和跨模态学习的两阶段框架，有效解决了交通标志识别中长尾分布和小目标检测的难题，并在TT100K数据集上取得了显著优于现有方法的性能，展现了在复杂开放世界场景中的优越性和泛化能力。

Abstract: Traffic sign recognition, as a core component of autonomous driving
perception systems, directly influences vehicle environmental awareness and
driving safety. Current technologies face two significant challenges: first,
the traffic sign dataset exhibits a pronounced long-tail distribution,
resulting in a substantial decline in recognition performance of traditional
convolutional networks when processing low-frequency and out-of-distribution
classes; second, traffic signs in real-world scenarios are predominantly small
targets with significant scale variations, making it difficult to extract
multi-scale features.To overcome these issues, we propose a novel two-stage
framework combining open-vocabulary detection and cross-modal learning. For
traffic sign detection, our NanoVerse YOLO model integrates a reparameterizable
vision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to
specifically enhance feature extraction for small, multi-scale targets. For
traffic sign classification, we designed a Traffic Sign Recognition Multimodal
Contrastive Learning model (TSR-MCL). By contrasting visual features from a
Vision Transformer with semantic features from a rule-based BERT, TSR-MCL
learns robust, frequency-independent representations, effectively mitigating
class confusion caused by data imbalance. On the TT100K dataset, our method
achieves a state-of-the-art 78.4% mAP in the long-tail detection task for
all-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,
significantly outperforming mainstream algorithms and demonstrating superior
accuracy and generalization in complex, open-world scenarios.

</details>


### [111] [MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting](https://arxiv.org/abs/2507.23340)
*Xingyue Peng,Yuandong Lyu,Lang Zhang,Jian Zhu,Songtao Wang,Jiaxin Deng,Songxin Lu,Weiliang Ma,Dangen She,Peng Jia,XianPeng Lang*

Main category: cs.CV

TL;DR: 针对自动驾驶中路面重建在复杂环境下的遮挡和外观退化问题，本文提出一个鲁棒框架，结合遮挡感知2D高斯曲面元和语义引导的颜色增强，在真实世界条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中精确的路面重建对车道感知和高精地图至关重要。现有方法（如网格渲染或3D高斯泼溅）在干净静态环境下表现良好，但在动态物体遮挡、静态障碍物干扰以及光照天气导致的外观退化等复杂真实世界条件下表现不佳。

Method: 提出一个鲁棒的重建框架，集成了遮挡感知2D高斯曲面元和语义引导的颜色增强。具体方法包括：利用平面适应的高斯表示进行大规模建模；采用语义分割引导的视频修复来移除动态和静态前景物体；通过HSV空间中的语义感知校正来增强颜色一致性。

Result: 在城市规模数据集上的大量实验表明，该框架能够生成视觉连贯且几何忠实的路面重建结果，在真实世界条件下显著优于现有方法。

Conclusion: 本文提出了一个鲁棒的路面重建框架，有效解决了真实世界中动态遮挡、静态干扰和外观退化等问题，为自动驾驶提供了更可靠、精确的路面信息。

Abstract: Road surface reconstruction is essential for autonomous driving, supporting
centimeter-accurate lane perception and high-definition mapping in complex
urban environments.While recent methods based on mesh rendering or 3D Gaussian
splatting (3DGS) achieve promising results under clean and static conditions,
they remain vulnerable to occlusions from dynamic agents, visual clutter from
static obstacles, and appearance degradation caused by lighting and weather
changes. We present a robust reconstruction framework that integrates
occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to
recover clean, consistent road surfaces. Our method leverages a planar-adapted
Gaussian representation for efficient large-scale modeling, employs
segmentation-guided video inpainting to remove both dynamic and static
foreground objects, and enhances color coherence via semantic-aware correction
in HSV space. Extensive experiments on urban-scale datasets demonstrate that
our framework produces visually coherent and geometrically faithful
reconstructions, significantly outperforming prior methods under real-world
conditions.

</details>


### [112] [The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models](https://arxiv.org/abs/2507.23341)
*Ahmet Can Ömercikoğlu,Mustafa Mansur Yönügül,Pakize Erdoğmuş*

Main category: cs.CV

TL;DR: 本研究系统分析了不同输入分辨率对YOLOv11、YOLOv12和MTCNN三种人脸检测器性能（准确性、鲁棒性）的影响。


<details>
  <summary>Details</summary>
Motivation: 人脸检测是AI应用中的关键技术，但低分辨率图像等真实世界条件会严重影响其性能。为解决此问题，本研究旨在深入探究输入分辨率对主流人脸检测器表现的影响。

Method: 研究使用WIDER FACE数据集，在160x160、320x320和640x640多种图像分辨率下，对YOLOv11、YOLOv12和MTCNN进行了广泛评估。性能指标包括精确度、召回率、mAP50、mAP50-95和推理时间。

Result: 结果显示，YOLOv11在检测精度上优于YOLOv12和MTCNN，特别是在高分辨率下；YOLOv12的召回率略高。MTCNN虽然在关键点定位上有优势，但在实时推理速度上表现滞后。

Conclusion: 本研究的发现为在不同操作限制下，选择合适的分辨率感知人脸检测模型提供了实用指导。

Abstract: Face detection is a crucial component in many AI-driven applications such as
surveillance, biometric authentication, and human-computer interaction.
However, real-world conditions like low-resolution imagery present significant
challenges that degrade detection performance. In this study, we systematically
investigate the impact of input resolution on the accuracy and robustness of
three prominent deep learning-based face detectors: YOLOv11, YOLOv12, and
MTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across
multiple image resolutions (160x160, 320x320, and 640x640) and assess each
model's performance using metrics such as precision, recall, mAP50, mAP50-95,
and inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN
in terms of detection accuracy, especially at higher resolutions, while YOLOv12
exhibits slightly better recall. MTCNN, although competitive in landmark
localization, lags in real-time inference speed. Our findings provide
actionable insights for selecting resolution-aware face detection models
suitable for varying operational constraints.

</details>


### [113] [Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads](https://arxiv.org/abs/2507.23343)
*Yingjie Zhou,Jiezhang Cao,Zicheng Zhang,Farong Wen,Yanwei Jiang,Jun Jia,Xiaohong Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文构建了迄今最大的AI生成说话人头部（AGTH）质量评估数据集THQA-10K，并提出了一种达到最先进（SOTA）性能的客观质量评估方法。


<details>
  <summary>Details</summary>
Motivation: AI生成说话人头部（AGTH）在质量方面仍面临挑战，且目前缺乏针对这些问题的全面研究。

Method: 构建了包含10,457个AGTHs的THQA-10K数据集，该数据集由12个T2I模型和14个先进说话人模型生成。招募志愿者对AGTHs进行主观评分和失真分类。提出了一种基于第一帧、Y-T切片和音唇一致性的客观质量评估方法。

Result: 通过主观实验分析，评估了说话人模型在泛化性和质量方面的表现，并揭示了现有AGTHs的失真。提出的客观质量评估方法在AGTH质量评估中实现了最先进（SOTA）的性能。

Conclusion: 本研究通过构建大规模数据集和提出高效的客观评估方法，为AI生成说话人头部（AGTH）的质量评估提供了有效工具和解决方案。

Abstract: Speech-driven methods for portraits are figuratively known as "Talkers"
because of their capability to synthesize speaking mouth shapes and facial
movements. Especially with the rapid development of the Text-to-Image (T2I)
models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging
digital human media. However, challenges persist regarding the quality of these
talkers and AGTHs they generate, and comprehensive studies addressing these
issues remain limited. To address this gap, this paper presents the largest
AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent
T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After
excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset
contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the
AGTHs and give the corresponding distortion categories. In our analysis for
subjective experimental results, we evaluate the performance of talkers in
terms of generalizability and quality, and also expose the distortions of
existing AGTHs. Finally, an objective quality assessment method based on the
first frame, Y-T slice and tone-lip consistency is proposed. Experimental
results show that this method can achieve state-of-the-art (SOTA) performance
in AGTH quality assessment. The work is released at
https://github.com/zyj-2000/Talker.

</details>


### [114] [IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025](https://arxiv.org/abs/2507.23357)
*Radu-Andrei Bourceanu,Neil De La Fuente,Jan Grimm,Andrei Jardan,Andriy Manucharyan,Cornelius Weiss,Roman Pflugfelder*

Main category: cs.CV

TL;DR: 本报告分析了计算机视觉中关键设计模式的演变，涵盖了图像识别基础架构、生成模型和自监督学习技术，通过审视六篇有影响力的论文进行探讨。


<details>
  <summary>Details</summary>
Motivation: 分析并追溯计算机视觉领域中关键设计模式的演变，通过审视六篇具有影响力的论文来探讨从图像识别到生成模型和自监督学习的发展历程。

Method: 报告回顾了六篇有影响力的论文：ResNet引入残差连接克服梯度消失；ViT将Transformer应用于图像块以实现大规模识别；GANs利用对抗训练学习复杂数据分布；LDMs在感知压缩潜在空间中进行序列去噪生成高保真图像；DINO通过自蒸馏框架学习特征；MAE利用非对称编解码器重建遮蔽输入，用于大规模模型预训练。

Result: ResNet克服了梯度消失问题；ViT证明了基于注意力模型在大规模图像识别中的有效性；LDMs以更高计算效率实现高保真合成，成为图像生成的最新技术；DINO学到的特征具有强大的k-NN分类性能；MAE为大规模视觉模型预训练提供了可扩展且有效的方法。

Conclusion: 计算机视觉设计模式不断演进，从基础识别架构发展到先进的生成模型和自监督学习技术，这些技术显著减少了对标注数据的依赖，并支持大规模模型的可扩展预训练。

Abstract: This report analyzes the evolution of key design patterns in computer vision
by examining six influential papers. The analy- sis begins with foundational
architectures for image recognition. We review ResNet, which introduced
residual connections to overcome the vanishing gradient problem and enable
effective training of significantly deeper convolutional networks.
Subsequently, we examine the Vision Transformer (ViT), which established a new
paradigm by applying the Transformer ar- chitecture to sequences of image
patches, demonstrating the efficacy of attention-based models for large-scale
image recogni- tion. Building on these visual representation backbones, we
investigate generative models. Generative Adversarial Networks (GANs) are
analyzed for their novel adversarial training process, which challenges a
generator against a discriminator to learn complex data distributions. Then,
Latent Diffusion Models (LDMs) are covered, which improve upon prior generative
methods by performing a sequential denoising process in a perceptually
compressed latent space. LDMs achieve high-fidelity synthesis with greater
computational efficiency, representing the current state-of-the-art for image
generation. Finally, we explore self-supervised learning techniques that reduce
dependency on labeled data. DINO is a self-distillation framework in which a
student network learns to match the output of a momentum-updated teacher,
yielding features with strong k-NN classification performance. We conclude with
Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design
to reconstruct heavily masked inputs, providing a highly scalable and effective
method for pre-training large-scale vision models.

</details>


### [115] [Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers](https://arxiv.org/abs/2507.23362)
*Ji Ma,Wei Suo,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出Short-LVLM框架，有效解决大视觉语言模型（LVLMs）的层剪枝问题，克服了模态差异带来的挑战，并在性能和效率之间取得了卓越的平衡。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）虽然能力强大，但参数量和计算成本巨大，限制了实际应用。自然语言处理（NLP）中的层剪枝技术有望提供无训练的压缩方案，但其在LVLMs中的有效性因视觉和语言的模态差异而未明。

Method: 首先经验性证明了直接将NLP层剪枝方法应用于LVLMs是无效的。通过大量实验，发现非关键的视觉语言（VL）令牌和层间特征差异是LVLMs层剪枝的关键挑战。基于这些发现，提出了新型框架Short-LVLM（SVL），旨在利用重要VL令牌并缓解层级特征差异。

Result: Short-LVLM不仅在性能和效率之间实现了优越的权衡，而且展现出无训练、模型无关和高度兼容等显著优势。

Conclusion: Short-LVLM提供了一个创新且实用的LVLM压缩解决方案，它通过有效处理视觉和语言模态间的挑战，实现了高性能与高效率的结合，极具应用潜力。

Abstract: Although large vision-language models (LVLMs) have demonstrated impressive
capabilities in multi-modal understanding and reasoning, their practical
applications are still limited by massive model parameters and high
computational costs. Recent efforts from natural language processing (NLP) have
shown the effectiveness of layer pruning, offering a plausible training-free
compression solution. However, due to the modality divergence between vision
and language, it is unclear whether these NLP techniques are still effective in
LVLMs. In this paper, we empirically prove that directly applying these layer
pruning methods to LVLMs is ineffective. Through extensive experiments, we find
that non-essential vision-language (VL) tokens and inter-layer feature gaps
pose critical challenges to pruning layers in LVLMs. Based on these insights,
we propose a novel framework Short-LVLM (SVL) that can utilize important VL
tokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only
achieves a superior trade-off between performance and efficiency but also
exhibits several potential advantages, i.e., training-free, model-agnostic, and
highly compatible. The code for this work is publicly available at
https://github.com/ASGO-MM/Short-LVLM.

</details>


### [116] [VMatcher: State-Space Semi-Dense Local Feature Matching](https://arxiv.org/abs/2507.23371)
*Ali Youssef*

Main category: cs.CV

TL;DR: 论文引入了VMatcher，一种结合Mamba和Transformer的混合网络，用于图像对之间的半稠密特征匹配，旨在克服Transformer计算成本高的问题并实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的特征匹配方法虽性能优异，但过度依赖Transformer的注意力机制，其二次复杂度导致高昂的计算成本，难以满足实时应用中对快速推理的需求。

Method: 本文提出VMatcher，一个集成Mamba高效长序列处理能力（线性复杂度）与Transformer注意力机制的混合网络。论文还提出了包括分层架构在内的多种VMatcher配置。

Result: VMatcher在效率上实现了显著提升，同时在特征匹配任务中设定了新的基准，并展示了其在实时应用中所需的鲁棒性和实用性。

Conclusion: VMatcher通过融合Mamba和Transformer的优势，有效解决了特征匹配中效率与性能的权衡问题，为需要快速推理的实时应用提供了高效且鲁棒的解决方案。

Abstract: This paper introduces VMatcher, a hybrid Mamba-Transformer network for
semi-dense feature matching between image pairs. Learning-based feature
matching methods, whether detector-based or detector-free, achieve
state-of-the-art performance but depend heavily on the Transformer's attention
mechanism, which, while effective, incurs high computational costs due to its
quadratic complexity. In contrast, Mamba introduces a Selective State-Space
Model (SSM) that achieves comparable or superior performance with linear
complexity, offering significant efficiency gains. VMatcher leverages a hybrid
approach, integrating Mamba's highly efficient long-sequence processing with
the Transformer's attention mechanism. Multiple VMatcher configurations are
proposed, including hierarchical architectures, demonstrating their
effectiveness in setting new benchmarks efficiently while ensuring robustness
and practicality for real-time applications where rapid inference is crucial.
Source Code is available at: https://github.com/ayoussf/VMatcher

</details>


### [117] [UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries](https://arxiv.org/abs/2507.23372)
*Yijie Zhu,Lingsen Zhang,Zitong Yu,Rui Shao,Tao Tan,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出UniEmo框架，统一情感理解与生成任务，通过分层理解和扩散模型，引入生成驱动的双重反馈机制，在情感理解和生成方面均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 情感理解与生成任务常被独立处理，但它们本质互补且能相互增强。当前挑战在于情感的抽象性，需要一个统一框架来有效提取视觉表示以促进这两项任务。

Method: 提出UniEmo统一框架。首先，通过一个分层情感理解链和可学习专家查询，逐步提取多尺度情感特征。其次，将这些专家查询和情感表示融合以引导扩散模型生成情感图像，并引入情感相关系数和情感条件损失来增强生成多样性和保真度。此外，通过联合训练实现生成对理解的隐式反馈，并通过新颖的数据过滤算法选择高质量生成图像，为理解部分提供显式反馈。

Result: 广泛实验表明，UniEmo在情感理解和情感生成两项任务上均显著优于现有最先进方法。

Conclusion: UniEmo成功地将情感理解与生成任务整合，并利用生成驱动的双重反馈机制显著增强了模型的理解能力。该框架在两项任务中均表现出色，证实了理解与生成间协同作用的有效性。

Abstract: Emotional understanding and generation are often treated as separate tasks,
yet they are inherently complementary and can mutually enhance each other. In
this paper, we propose the UniEmo, a unified framework that seamlessly
integrates these two tasks. The key challenge lies in the abstract nature of
emotions, necessitating the extraction of visual representations beneficial for
both tasks. To address this, we propose a hierarchical emotional understanding
chain with learnable expert queries that progressively extracts multi-scale
emotional features, thereby serving as a foundational step for unification.
Simultaneously, we fuse these expert queries and emotional representations to
guide the diffusion model in generating emotion-evoking images. To enhance the
diversity and fidelity of the generated emotional images, we further introduce
the emotional correlation coefficient and emotional condition loss into the
fusion process. This step facilitates fusion and alignment for emotional
generation guided by the understanding. In turn, we demonstrate that joint
training allows the generation component to provide implicit feedback to the
understanding part. Furthermore, we propose a novel data filtering algorithm to
select high-quality and diverse emotional images generated by the well-trained
model, which explicitly feedback into the understanding part. Together, these
generation-driven dual feedback processes enhance the model's understanding
capacity. Extensive experiments show that UniEmo significantly outperforms
state-of-the-art methods in both emotional understanding and generation tasks.
The code for the proposed method is available at
https://github.com/JiuTian-VL/UniEmo.

</details>


### [118] [Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation](https://arxiv.org/abs/2507.23373)
*Haoran Chen,Zexiao Wang,Haidong Cao,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出MP^2A，一种渐进式对齐策略，用于基于CLIP的无监督域适应，以应对伪标签噪声和多源域差异问题，实现更鲁棒的域不变特征学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的无监督域适应方法采用一次性对齐所有伪标签数据，导致对噪声样本敏感、误差传播，尤其在多源场景下表现不佳，从而阻碍了域不变特征的学习。

Method: 提出渐进式对齐策略（MP^2A），该方法首先使用目标域中高置信度样本进行训练，逐步学习可靠的表示；随后，在训练过程中逐步引入更具挑战性的样本，指导模型细化理解，有效减轻确认偏差并促进更稳健的收敛。

Result: MP^2A在ImageCLEF、Office-Home和DomainNet三个常用UDA基准测试中，与最新的基于CLIP的多源无监督域适应方法相比，取得了最先进的性能。

Conclusion: 渐进式对齐策略MP^2A有效解决了基于CLIP的无监督域适应中伪标签噪声带来的挑战，促进了模型鲁棒收敛并学习到真正的域不变特征，从而达到了SOTA性能。

Abstract: Large Vision-Language Models like CLIP have become a powerful foundation for
Unsupervised Domain Adaptation due to their strong zero-shot generalization.
State-of-the-art methods typically leverage CLIP to generate pseudo-labels for
the target domain, then fine-tune the model to learn domain-invariant features.
However, these methods attempt to align source and target domains using all
pseudo-labeled data simultaneously. This one-shot alignment struggles with
noisy, hard-to-classify samples, leading to error propagation and suboptimal
feature learning. The problem is even more amplified in the multi-source
scenario, where diverse domain gaps and varying noise levels across multiple
source domains further destabilize the alignment process. To address this
issue, in this work, we propose a progressive alignment strategy for adapting
CLIP to unlabeled downstream task. Our method begins by training the model on a
high-confidence subset of target samples, allowing it to first learn a
well-aligned representation from the most reliable data. As training
progresses, it gradually incorporates more challenging samples, guiding the
model to refine its understanding without being overwhelmed by initial label
noise. This progressive approach effectively mitigates confirmation bias and
promotes a more robust convergence, allowing for the learning of genuinely
domain-invariant features. We name our approach MP^2A and test it on three
popular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging
DomainNet. Experiments showcase that MP^2A achieves state-of-the-art
performance when compared with most recent CLIP-based MS-UDA approaches,
demonstrating the effectiveness of our approach.

</details>


### [119] [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](https://arxiv.org/abs/2507.23374)
*Shuangkang Fang,I-Chao Shen,Takeo Igarashi,Yufeng Wang,ZeSheng Wang,Yi Yang,Wenrui Ding,Shuchang Zhou*

Main category: cs.CV

TL;DR: 本文提出NeRF-GS，一个联合优化NeRF和3DGS的新框架，旨在结合两者的优点，提升3D场景表示性能。


<details>
  <summary>Details</summary>
Motivation: 当前的3DGS存在局限性，如对高斯初始化敏感、空间感知能力弱和高斯间相关性不足。研究旨在利用NeRF固有的连续空间表示来弥补这些缺点，从而提高3DGS的性能。

Method: NeRF-GS框架通过重新设计3DGS，并使其空间特征与NeRF逐步对齐，使得两种表示可以通过共享3D空间信息在同一场景中进行优化。此外，还通过优化隐式特征和高斯位置的残差向量，来增强3DGS的个性化能力。

Result: NeRF-GS在基准数据集上的实验结果显示，其性能超越了现有方法，并达到了最先进的水平。

Conclusion: 研究结果证实NeRF和3DGS是互补而非竞争的关系，为结合这两种技术实现高效3D场景表示的混合方法提供了新的视角和见解。

Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework
leverages the inherent continuous spatial representation of NeRF to mitigate
several limitations of 3DGS, including sensitivity to Gaussian initialization,
limited spatial awareness, and weak inter-Gaussian correlations, thereby
enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and
progressively align its spatial features with NeRF, enabling both
representations to be optimized within the same scene through shared 3D spatial
information. We further address the formal distinctions between the two
approaches by optimizing residual vectors for both implicit features and
Gaussian positions to enhance the personalized capabilities of 3DGS.
Experimental results on benchmark datasets show that NeRF-GS surpasses existing
methods and achieves state-of-the-art performance. This outcome confirms that
NeRF and 3DGS are complementary rather than competing, offering new insights
into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene
representation.

</details>


### [120] [AGA: An adaptive group alignment framework for structured medical cross-modal representation learning](https://arxiv.org/abs/2507.23402)
*Wei Li,Xun Gong,Jiao Li,Xiaobin Sun*

Main category: cs.CV

TL;DR: 提出自适应分组对齐（AGA）框架，通过双向分组和无需外部负样本的对齐损失，有效利用医学报告结构，提升医学图文检索与分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的视觉-语言预训练方法，常忽略临床报告的内在结构，且依赖大量难以获取的硬负样本，这在小规模医学数据集中不切实际。

Method: 我们提出自适应分组对齐（AGA）框架。该框架引入基于稀疏相似度矩阵的双向分组机制，为每个图文对计算文本词元和图像块的细粒度相似性，并分别形成视觉组和语言组。为实现自适应分组，设计了两个动态学习分组阈值的门控模块（语言分组阈值门和视觉分组阈值门）。组表示通过相似度分数的加权平均计算。为在图文对内对齐词元与其组表示，引入了“实例感知分组对齐损失”，从而无需外部负样本。最后，应用双向跨模态分组对齐模块，增强视觉与语言组表示间的细粒度对齐。

Result: 在公共和私有数据集上的大量实验表明，所提出的方法在微调和零样本设置下的图像-文本检索和分类任务中均取得了优异性能。

Conclusion: AGA框架通过有效利用医学报告的结构化语义，并解决对外部负样本的依赖问题，成功提升了医学视觉-语言表示学习在图文检索和分类任务中的表现。

Abstract: Learning medical visual representations from paired images and reports is a
promising direction in representation learning. However, current
vision-language pretraining methods in the medical domain often simplify
clinical reports into single entities or fragmented tokens, ignoring their
inherent structure. In addition, contrastive learning frameworks typically
depend on large quantities of hard negative samples, which is impractical for
small-scale medical datasets. To tackle these challenges, we propose Adaptive
Grouped Alignment (AGA), a new framework that captures structured semantics
from paired medical images and reports. AGA introduces a bidirectional grouping
mechanism based on a sparse similarity matrix. For each image-report pair, we
compute fine-grained similarities between text tokens and image patches. Each
token selects its top-matching patches to form a visual group, and each patch
selects its most related tokens to form a language group. To enable adaptive
grouping, we design two threshold gating modules, called Language Grouped
Threshold Gate and Vision Grouped Threshold Gate, which learn grouping
thresholds dynamically. Group representations are computed as weighted averages
based on similarity scores. To align each token with its group representation,
we introduce an Instance Aware Group Alignment loss that operates within each
image-text pair, removing the need for external negatives. Finally, a
Bidirectional Cross-modal Grouped Alignment module is applied to enhance
fine-grained alignment between visual and linguistic group representations.
Extensive experiments on public and private datasets show that our method
achieves strong performance on image-text retrieval and classification tasks
under both fine-tuning and zero-shot settings.

</details>


### [121] [Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories](https://arxiv.org/abs/2507.23411)
*Lemar Abdi,Francisco Caetano,Amaan Valiuddin,Christiaan Viviers,Hamdi Joudeh,Fons van der Sommen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Stein分数去噪扩散模型（SBDDM）的无重建OOD检测方法，通过捕捉扩散轨迹曲率实现高效且准确的异常评分，在医疗影像OOD检测任务中达到最先进性能并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像中，非监督OOD检测对于识别低发病率的病理案例具有吸引力。现有生成式方法（如似然估计或重建误差）通常计算成本高、不可靠，并且在内部数据变化时需要重新训练，这阻碍了其高效、一致和鲁棒地区分正常与异常输入的能力。

Method: 提出一种无重建的OOD检测方法，该方法利用Stein分数去噪扩散模型（SBDDM）的前向扩散轨迹。通过估计的Stein分数捕捉轨迹曲率，该方法仅需五个扩散步骤即可实现准确的异常评分。使用在大型医学数据集上预训练的单个SBDDM模型。

Result: 该方法在多个近OOD和远OOD基准测试中均能有效泛化，实现了最先进的性能，并显著降低了推理时的计算成本。与现有方法相比，在近OOD检测上实现了高达10.43%的相对改进，在远OOD检测上实现了高达18.10%的相对改进。

Conclusion: 所提出的SBDDM方法是实现实时、可靠计算机辅助诊断的实用基础模块，解决了现有方法的局限性，提升了OOD检测的效率、一致性和鲁棒性。

Abstract: In medical imaging, unsupervised out-of-distribution (OOD) detection offers
an attractive approach for identifying pathological cases with extremely low
incidence rates. In contrast to supervised methods, OOD-based approaches
function without labels and are inherently robust to data imbalances. Current
generative approaches often rely on likelihood estimation or reconstruction
error, but these methods can be computationally expensive, unreliable, and
require retraining if the inlier data changes. These limitations hinder their
ability to distinguish nominal from anomalous inputs efficiently, consistently,
and robustly. We propose a reconstruction-free OOD detection method that
leverages the forward diffusion trajectories of a Stein score-based denoising
diffusion model (SBDDM). By capturing trajectory curvature via the estimated
Stein score, our approach enables accurate anomaly scoring with only five
diffusion steps. A single SBDDM pre-trained on a large, semantically aligned
medical dataset generalizes effectively across multiple Near-OOD and Far-OOD
benchmarks, achieving state-of-the-art performance while drastically reducing
computational cost during inference. Compared to existing methods, SBDDM
achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and
Far-OOD detection, making it a practical building block for real-time, reliable
computer-aided diagnosis.

</details>


### [122] [Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2507.23416)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 本文开发了一个基于机器学习和高光谱成像的系统，用于自动检测蜂蜜中的糖浆掺假，并量化其浓度。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于机器学习的自动化系统，利用蜂蜜高光谱成像数据，实现蜂蜜糖浆掺假的自动检测。

Method: 系统分为两个子系统：植物来源识别和掺假检测。两个子系统都采用两步法：首先使用线性判别分析（LDA）提取特征，然后使用K近邻（KNN）模型进行分类（植物来源）或识别/量化（掺假水平）。系统在公开的蜂蜜高光谱图像数据集上进行了评估。

Result: 该系统检测蜂蜜掺假的整体交叉验证准确率达到96.39%。

Conclusion: 该系统可作为当前基于化学检测方法的有效替代方案，用于蜂蜜掺假检测。

Abstract: This paper aims to develop a machine learning-based system for automatically
detecting honey adulteration with sugar syrup, based on honey hyperspectral
imaging data. First, the floral source of a honey sample is classified by a
botanical origin identification subsystem. Then, the sugar syrup adulteration
is identified, and its concentration is quantified by an adulteration detection
subsystem. Both subsystems consist of two steps. The first step involves
extracting relevant features from the honey sample using Linear Discriminant
Analysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN)
model to classify the honey botanical origin in the first subsystem and
identify the adulteration level in the second subsystem. We assess the proposed
system performance on a public honey hyperspectral image dataset. The result
indicates that the proposed system can detect adulteration in honey with an
overall cross-validation accuracy of 96.39%, making it an appropriate
alternative to the current chemical-based detection methods.

</details>


### [123] [Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification](https://arxiv.org/abs/2507.23436)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Cosimo Distante,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: 针对艺术风格分类中的数据稀缺和复杂非线性交互问题，本文提出了一种基于KANs（Kolmogorov-Arnold Networks）增强的双教师自监督框架，取代传统MLP层以更好地建模非线性特征。实验证明该方法在准确率上优于基线双教师架构。


<details>
  <summary>Details</summary>
Motivation: 艺术风格分类面临专家标注数据集稀缺和风格元素间复杂、非线性交互的挑战。现有双教师自监督框架虽减少了对标注数据的依赖，但其线性投影层和局部关注难以有效建模全局构图上下文和复杂的风格特征交互。

Method: 本文通过将传统的MLP投影和预测头部替换为Kolmogorov-Arnold Networks (KANs)，增强了双教师知识蒸馏框架。该方法保留了两个教师网络的互补指导（一个强调局部纹理/笔触，另一个捕获更广泛的风格层次），并利用KANs基于样条的激活函数来精确建模非线性特征关联。

Result: 在WikiArt和Pandora18k数据集上的实验表明，本文提出的方法在Top-1准确率上优于基础的双教师架构。研究发现KANs在解耦复杂风格流形方面具有重要作用，并带来了比MLP投影更好的线性探测准确率。

Conclusion: 将KANs引入双教师自监督框架，能够有效解决艺术风格分类中复杂的非线性特征建模问题，显著提升了分类性能，尤其在解耦复杂风格流形和提高准确率方面展现出优越性。

Abstract: Art style classification remains a formidable challenge in computational
aesthetics due to the scarcity of expertly labeled datasets and the intricate,
often nonlinear interplay of stylistic elements. While recent dual-teacher
self-supervised frameworks reduce reliance on labeled data, their linear
projection layers and localized focus struggle to model global compositional
context and complex style-feature interactions. We enhance the dual-teacher
knowledge distillation framework to address these limitations by replacing
conventional MLP projection and prediction heads with Kolmogorov-Arnold
Networks (KANs). Our approach retains complementary guidance from two teacher
networks, one emphasizing localized texture and brushstroke patterns, the other
capturing broader stylistic hierarchies while leveraging KANs' spline-based
activations to model nonlinear feature correlations with mathematical
precision. Experiments on WikiArt and Pandora18k demonstrate that our approach
outperforms the base dual teacher architecture in Top-1 accuracy. Our findings
highlight the importance of KANs in disentangling complex style manifolds,
leading to better linear probe accuracy than MLP projections.

</details>


### [124] [Adjustable Spatio-Spectral Hyperspectral Image Compression Network](https://arxiv.org/abs/2507.23447)
*Martin Hermann Paul Fuchs,Behnood Rasti,Begüm Demir*

Main category: cs.CV

TL;DR: 针对遥感领域高光谱图像（HSI）的高效存储需求，本文提出了一种名为HyCASS的学习型模型，实现了光谱和空间维度上的可调HSI压缩。该模型通过捕获短程和长程冗余，解决了现有方法未能全面探究光谱和空间压缩对HSI压缩影响的问题，并为平衡光谱和空间压缩提供了指导方针。


<details>
  <summary>Details</summary>
Motivation: 随着遥感领域高光谱数据档案的快速增长，高效存储变得至关重要，促使人们对基于学习的高光谱图像（HSI）压缩给予了广泛关注。然而，目前尚未对光谱和空间压缩在学习型HSI压缩中的独立和联合效应进行全面深入的研究，而进行此类分析对于理解光谱、空间和联合时空冗余的利用如何影响HSI压缩至关重要。

Method: 为解决上述问题，我们提出了可调时空高光谱图像压缩网络（HyCASS），这是一种基于学习的模型，旨在实现光谱和空间维度上的可调HSI压缩。HyCASS由六个主要模块组成：1)光谱编码器；2)空间编码器；3)压缩比（CR）适配器编码器；4)CR适配器解码器；5)空间解码器；以及6)光谱解码器模块。这些模块采用卷积层和Transformer块来捕获短程和长程冗余。

Result: 在两个HSI基准数据集上的实验结果表明，我们提出的可调模型比现有基于学习的压缩模型更有效。根据我们的结果，我们为在不同压缩比下，结合HSI的空间分辨率，有效平衡光谱和空间压缩建立了指导方针。

Conclusion: HyCASS模型成功验证了其在解决学习型HSI压缩中光谱和空间冗余利用方面的有效性，提供了一种高性能的可调压缩解决方案。研究结果不仅展示了模型的优越性，还为根据具体应用场景的需要，平衡光谱和空间压缩提供了实用的指导。

Abstract: With the rapid growth of hyperspectral data archives in remote sensing (RS),
the need for efficient storage has become essential, driving significant
attention toward learning-based hyperspectral image (HSI) compression. However,
a comprehensive investigation of the individual and joint effects of spectral
and spatial compression on learning-based HSI compression has not been
thoroughly examined yet. Conducting such an analysis is crucial for
understanding how the exploitation of spectral, spatial, and joint
spatio-spectral redundancies affects HSI compression. To address this issue, we
propose Adjustable Spatio-Spectral Hyperspectral Image Compression Network
(HyCASS), a learning-based model designed for adjustable HSI compression in
both spectral and spatial dimensions. HyCASS consists of six main modules: 1)
spectral encoder; 2) spatial encoder; 3) compression ratio (CR) adapter
encoder; 4) CR adapter decoder; 5) spatial decoder; and 6) spectral decoder
module. The modules employ convolutional layers and transformer blocks to
capture both short-range and long-range redundancies. Experimental results on
two HSI benchmark datasets demonstrate the effectiveness of our proposed
adjustable model compared to existing learning-based compression models. Based
on our results, we establish a guideline for effectively balancing spectral and
spatial compression across different CRs, taking into account the spatial
resolution of the HSIs. Our code and pre-trained model weights are publicly
available at https://git.tu-berlin.de/rsim/hycass .

</details>


### [125] [Machine learning and machine learned prediction in chest X-ray images](https://arxiv.org/abs/2507.23455)
*Shereiff Garrett,Abhinav Adhikari,Sarina Gautam,DaShawn Marquis Morris,Chandra Mani Adhikari*

Main category: cs.CV

TL;DR: 本研究在胸部X光图像上比较了CNN和DenseNet-121两种机器学习模型预测患者疾病的能力，发现两者均表现良好，其中DenseNet-121在决策时对图像关键部位的关注更准确。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习，特别是深度学习模型，通过分析胸部X光图像来准确预测患者疾病，以解决复杂的医疗诊断问题。

Method: 使用5824张胸部X光图像数据集，实现了基线卷积神经网络（CNN）和DenseNet-121两种机器学习算法，并将其应用于二元分类问题（预测患者是否患病）。通过梯度加权类激活映射（Grad-CAM）评估模型的决策焦点。

Result: 1. 基线CNN和DenseNet-121在二元分类任务中均表现出色。
2. 梯度加权类激活映射显示，DenseNet-121在决策时能比基线CNN更准确地聚焦于输入胸部X光图像的关键部分。

Conclusion: DenseNet-121在胸部X光图像疾病预测方面展现出优异的性能和更好的可解释性，它能有效地识别并关注图像中的关键区域，这使其成为医疗诊断应用的有力工具。

Abstract: Machine learning and artificial intelligence are fast-growing fields of
research in which data is used to train algorithms, learn patterns, and make
predictions. This approach helps to solve seemingly intricate problems with
significant accuracy without explicit programming by recognizing complex
relationships in data. Taking an example of 5824 chest X-ray images, we
implement two machine learning algorithms, namely, a baseline convolutional
neural network (CNN) and a DenseNet-121, and present our analysis in making
machine-learned predictions in predicting patients with ailments. Both baseline
CNN and DenseNet-121 perform very well in the binary classification problem
presented in this work. Gradient-weighted class activation mapping shows that
DenseNet-121 correctly focuses on essential parts of the input chest X-ray
images in its decision-making more than the baseline CNN.

</details>


### [126] [Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection](https://arxiv.org/abs/2507.23461)
*Taeheon Lim,Joohyung Lee,Kyungjae Lee,Jungchan Cho*

Main category: cs.CV

TL;DR: 联邦学习在非分类任务中面临“分辨率漂移”问题。本文提出RAF方法，利用基于热图的多分辨率知识蒸馏，有效缓解该问题，提高性能并具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）主要应用于分类任务并已取得成功，但其在人体姿态估计等非分类任务中的应用尚未充分探索。研究发现，跨客户端的分辨率差异会导致严重的“分辨率漂移”问题，损害模型性能，这是一种新的非独立同分布（non-IID）数据挑战。

Method: 提出“分辨率自适应联邦学习（RAF）”方法。该方法利用基于热图的知识蒸馏技术，通过在高分辨率输出（教师）和低分辨率输出（学生）之间进行多分辨率知识蒸馏，以增强模型对不同分辨率数据的鲁棒性，同时避免过拟合。

Result: 广泛的实验和理论分析表明，RAF不仅有效缓解了分辨率漂移问题并显著提高了性能，而且可以无缝集成到现有联邦学习框架中。

Conclusion: 尽管主要聚焦于人体姿态估计，通过t-SNE分析揭示的分类任务与高分辨率表示任务之间的特性差异表明，RAF具有良好的泛化能力，适用于其他需要保持空间细节的任务。这为联邦学习在更广泛非分类应用中的部署提供了解决方案。

Abstract: The Federated Learning (FL) approach enables effective learning across
distributed systems, while preserving user data privacy. To date, research has
primarily focused on addressing statistical heterogeneity and communication
efficiency, through which FL has achieved success in classification tasks.
However, its application to non-classification tasks, such as human pose
estimation, remains underexplored. This paper identifies and investigates a
critical issue termed ``resolution-drift,'' where performance degrades
significantly due to resolution variability across clients. Unlike class-level
heterogeneity, resolution drift highlights the importance of resolution as
another axis of not independent or identically distributed (non-IID) data. To
address this issue, we present resolution-adaptive federated learning (RAF), a
method that leverages heatmap-based knowledge distillation. Through
multi-resolution knowledge distillation between higher-resolution outputs
(teachers) and lower-resolution outputs (students), our approach enhances
resolution robustness without overfitting. Extensive experiments and
theoretical analysis demonstrate that RAF not only effectively mitigates
resolution drift and achieves significant performance improvements, but also
can be integrated seamlessly into existing FL frameworks. Furthermore, although
this paper focuses on human pose estimation, our t-SNE analysis reveals
distinct characteristics between classification and high-resolution
representation tasks, supporting the generalizability of RAF to other tasks
that rely on preserving spatial detail.

</details>


### [127] [CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes](https://arxiv.org/abs/2507.23473)
*Bin Xie,Congxuan Zhang,Fagan Wang,Peng Liu,Feng Lu,Zhen Chen,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了CST Anti-UAV，一个针对复杂场景下微型无人机单目标跟踪（SOT）的新型热红外数据集，旨在解决现有数据集的局限性。通过评估20种SOT方法，结果显示在CST Anti-UAV数据集上，跟踪微型无人机仍是巨大挑战，凸显了现有技术和基准的不足。


<details>
  <summary>Details</summary>
Motivation: 无人机广泛应用引发了公共安全和隐私担忧，无人机感知对反无人机任务至关重要。然而，现有无人机跟踪数据集主要关注显眼目标，且场景复杂性和属性多样性不足，限制了其在真实世界中的应用。

Method: 提出了CST Anti-UAV热红外数据集，专为复杂场景下微型无人机（CST）的单目标跟踪（SOT）设计。该数据集包含220个视频序列和超过24万个高质量边界框标注，主要特点是包含大量微型无人机目标以及多样复杂的场景。这是首个包含完整手动帧级属性标注的数据集。研究者还在该数据集上评估了20种现有SOT方法，进行深入的性能分析。

Result: 实验结果表明，在复杂环境中跟踪微型无人机仍然是一项挑战，最先进的方法仅实现了35.92%的状态准确率，远低于在Anti-UAV410数据集上观察到的67.69%。这些发现强调了现有基准的局限性。

Conclusion: 现有无人机跟踪研究需要进一步的进步。CST Anti-UAV基准数据集即将公开，这将不仅促进更鲁棒的SOT方法的发展，还将推动反无人机系统的创新。

Abstract: The widespread application of Unmanned Aerial Vehicles (UAVs) has raised
serious public safety and privacy concerns, making UAV perception crucial for
anti-UAV tasks. However, existing UAV tracking datasets predominantly feature
conspicuous objects and lack diversity in scene complexity and attribute
representation, limiting their applicability to real-world scenarios. To
overcome these limitations, we present the CST Anti-UAV, a new thermal infrared
dataset specifically designed for Single Object Tracking (SOT) in Complex
Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k
high-quality bounding box annotations, highlighting two key properties: a
significant number of tiny-sized UAV targets and the diverse and complex
scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to
incorporate complete manual frame-level attribute annotations, enabling precise
evaluations under varied challenges. To conduct an in-depth performance
analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed
dataset. Experimental results demonstrate that tracking tiny UAVs in complex
environments remains a challenge, as the state-of-the-art method achieves only
35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410
dataset. These findings underscore the limitations of existing benchmarks and
the need for further advancements in UAV tracking research. The CST Anti-UAV
benchmark is about to be publicly released, which not only fosters the
development of more robust SOT methods but also drives innovation in anti-UAV
systems.

</details>


### [128] [3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding](https://arxiv.org/abs/2507.23478)
*Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出3D-R1基础模型，通过构建高质量合成数据集、引入强化学习人类反馈（RLHF）和动态视角选择策略，显著提升了3D视觉语言模型在3D场景理解中的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型（3D VLMs）在3D场景理解中，由于缺乏高质量空间数据和视角假设的静态性，导致其推理和泛化能力不足。

Method: ['构建高质量合成数据集Scene-30K（包含思维链CoT），利用现有3D-VL数据集和基于Gemini 2.5 Pro的数据引擎，作为3D-R1的冷启动初始化数据。', '在强化学习训练过程中引入RLHF策略（如GRPO），并设计感知奖励、语义相似度奖励和格式奖励，以增强推理能力并保持检测精度和答案语义准确性。', '引入动态视角选择策略，自适应地选择最具信息量的视角进行3D场景理解。']

Result: 3D-R1在各种3D场景基准测试中，平均性能提升了10%。

Conclusion: 3D-R1有效增强了3D场景理解中的推理和泛化能力。

Abstract: Large vision-language models (VLMs) have made significant strides in 2D
visual understanding tasks, sparking interest in extending these capabilities
to 3D scene understanding. However, current 3D VLMs often struggle with robust
reasoning and generalization due to limitations in high-quality spatial data
and the static nature of viewpoint assumptions. To address these challenges, we
propose 3D-R1, a foundation model that enhances the reasoning capabilities of
3D VLMs. Specifically, we first construct a high-quality synthetic dataset with
CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine
based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.
Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning
training process to enhance reasoning capabilities and introduce three reward
functions: a perception reward, a semantic similarity reward and a format
reward to maintain detection accuracy and answer semantic precision.
Furthermore, we introduce a dynamic view selection strategy that adaptively
chooses the most informative perspectives for 3D scene understanding. Extensive
experiments demonstrate that 3D-R1 delivers an average improvement of 10%
across various 3D scene benchmarks, highlighting its effectiveness in enhancing
reasoning and generalization in 3D scene understanding. Code:
https://github.com/AIGeeksGroup/3D-R1. Website:
https://aigeeksgroup.github.io/3D-R1.

</details>


### [129] [Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning](https://arxiv.org/abs/2507.23479)
*Julia Werner,Oliver Bause,Julius Oexle,Maxime Le Floch,Franz Brinkmann,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文提出一个多任务神经网络模型，在保证参数量小的同时，实现了胶囊内窥镜的精确自定位和小肠异常检测，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视频胶囊内窥镜面临电池续航短的挑战。虽然AI可减少能耗，但受限于数据稀疏性和设备资源有限，难以部署大型模型。

Method: 引入了一个多任务神经网络，整合了消化道内自定位和小肠异常检测功能。开发过程中严格限制模型参数量（100万），并首次在Galar数据集上结合多任务方法和Viterbi解码进行时间序列分析。

Result: 模型在定位任务上达到93.63%的准确率，在异常检测任务上达到87.48%的准确率。仅需100万参数，性能超越当前单任务基线模型。

Conclusion: 该方法代表了AI在视频胶囊内窥镜领域的重要进展，能有效解决电池寿命问题，且模型尺寸适合在小型胶囊中部署。

Abstract: Video capsule endoscopy has become increasingly important for investigating
the small intestine within the gastrointestinal tract. However, a persistent
challenge remains the short battery lifetime of such compact sensor edge
devices. Integrating artificial intelligence can help overcome this limitation
by enabling intelligent real-time decision- making, thereby reducing the energy
consumption and prolonging the battery life. However, this remains challenging
due to data sparsity and the limited resources of the device restricting the
overall model size. In this work, we introduce a multi-task neural network that
combines the functionalities of precise self-localization within the
gastrointestinal tract with the ability to detect anomalies in the small
intestine within a single model. Throughout the development process, we
consistently restricted the total number of parameters to ensure the
feasibility to deploy such model in a small capsule. We report the first
multi-task results using the recently published Galar dataset, integrating
established multi-task methods and Viterbi decoding for subsequent time-series
analysis. This outperforms current single-task models and represents a
significant ad- vance in AI-based approaches in this field. Our model achieves
an accu- racy of 93.63% on the localization task and an accuracy of 87.48% on
the anomaly detection task. The approach requires only 1 million parameters
while surpassing the current baselines.

</details>


### [130] [FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction](https://arxiv.org/abs/2507.23480)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: 本文提出FastPoint，一种软件加速技术，通过预测距离趋势加速三维点云处理中的最远点采样和邻域搜索，实现2.55倍的端到端加速且不牺牲精度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络革新了三维点云处理，但高效处理大型和不规则点云仍是挑战。

Method: 引入FastPoint，一种利用最远点采样过程中采样点间可预测距离趋势的软件加速技术。通过预测距离曲线，高效识别后续采样点，避免详尽计算所有成对距离。

Result: FastPoint显著加速了最远点采样和邻域搜索操作，同时保持采样质量和模型性能。集成到先进三维点云模型后，在NVIDIA RTX 3090 GPU上实现2.55倍的端到端加速，且未牺牲精度。

Conclusion: FastPoint通过创新的预测机制有效提升了三维点云处理效率，为大型不规则点云的加速处理提供了有效解决方案，并在实际应用中展现出显著性能提升而不损失准确性。

Abstract: Deep neural networks have revolutionized 3D point cloud processing, yet
efficiently handling large and irregular point clouds remains challenging. To
tackle this problem, we introduce FastPoint, a novel software-based
acceleration technique that leverages the predictable distance trend between
sampled points during farthest point sampling. By predicting the distance
curve, we can efficiently identify subsequent sample points without
exhaustively computing all pairwise distances. Our proposal substantially
accelerates farthest point sampling and neighbor search operations while
preserving sampling quality and model performance. By integrating FastPoint
into state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end
speedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.

</details>


### [131] [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/abs/2507.23483)
*Mutian Xu,Chongjie Ye,Haolin Liu,Yushuang Wu,Jiahao Chang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 该工作提出Stable-Sim2Real，一个基于两阶段深度扩散模型的3D数据仿真方法，旨在弥合模拟与真实数据之间的差距，提升真实世界3D视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 模拟3D数据与真实捕捉的3D数据之间存在巨大鸿沟，这是真实世界3D视觉任务面临的一个基础性问题。现有3D数据仿真方法多依赖预定义的物理先验，难以捕捉真实数据的完整复杂性。虽然数据驱动学习合成到真实数据隐式映射是理想方案，但相关研究进展停滞。

Method: 本研究提出一种新的数据驱动3D仿真方案Stable-Sim2Real，基于新颖的两阶段深度扩散模型。第一阶段微调Stable-Diffusion以生成真实与合成配对深度之间的残差，产生稳定但粗糙的深度。为提升质量，第二阶段将合成深度和第一阶段输出深度一同输入，并通过3D判别器识别独特区域，调整扩散损失以优先处理这些区域。此外，该工作提供了一套新的3D数据仿真评估基准方案。

Result: 广泛实验表明，使用本方法派生的3D模拟数据训练网络，能显著增强真实世界3D视觉任务的性能。此外，评估结果显示本方法生成的3D模拟数据与真实捕捉模式之间具有高度相似性。

Conclusion: Stable-Sim2Real通过其创新的两阶段深度扩散模型，有效解决了3D数据仿真中模拟与真实数据间的鸿沟问题，显著提升了3D视觉任务在真实世界的表现，并成功生成了高度逼真的模拟数据。

Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured
3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D
data simulation methods inject predefined physical priors but struggle to
capture the full complexity of real data. An optimal approach involves learning
an implicit mapping from synthetic to realistic data in a data-driven manner,
but progress in this solution has met stagnation in recent studies. This work
explores a new solution path of data-driven 3D simulation, called
Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial
stage finetunes Stable-Diffusion to generate the residual between the real and
synthetic paired depth, producing a stable but coarse depth, where some local
regions may deviate from realistic patterns. To enhance this, both the
synthetic and initial output depth are fed into a second-stage diffusion, where
diffusion loss is adjusted to prioritize these distinct areas identified by a
3D discriminator. We provide a new benchmark scheme to evaluate 3D data
simulation methods. Extensive experiments show that training the network with
the 3D simulated data derived from our method significantly enhances
performance in real-world 3D visual tasks. Moreover, the evaluation
demonstrates the high similarity between our 3D simulated data and
real-captured patterns. Project page:
https://mutianxu.github.io/stable-sim2real/.

</details>


### [132] [Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions](https://arxiv.org/abs/2507.23487)
*Jinshan Zhen,Yuanyue Ge,Tianxiao Zhu,Hui Zhao,Ya Xiong*

Main category: cs.CV

TL;DR: 提出一种基于视觉的管道，利用RGB-D和深度学习技术，实现了田间草莓在存在遮挡和姿态变化情况下的无损、实时质量估算。


<details>
  <summary>Details</summary>
Motivation: 在田间条件下，由于频繁遮挡和姿态变化，准确估算桌面生长草莓的质量仍然具有挑战性。

Method: 该研究提出一个视觉管道，整合RGB-D感知和深度学习。具体方法包括：使用YOLOv8-Seg进行实例分割，运用CycleGAN完成遮挡区域的修复，通过倾斜角度校正优化正面投影面积计算，最后采用多项式回归模型将几何特征映射到质量。

Result: 实验表明，孤立草莓的平均质量估算误差为8.11%，遮挡情况为10.47%。在遮挡恢复方面，CycleGAN优于LaMa模型，取得了更高的像素面积比（PAR）（平均0.978对比1.112）和更高的交并比（IoU）分数（在[0.9-1]范围内达到92.3%对比47.7%）。

Conclusion: 该方法解决了传统方法的关键局限性，为自动化采摘和产量监测（尤其是在复杂遮挡模式下）提供了一个鲁棒的解决方案。

Abstract: Accurate mass estimation of table-top grown strawberries under field
conditions remains challenging due to frequent occlusions and pose variations.
This study proposes a vision-based pipeline integrating RGB-D sensing and deep
learning to enable non-destructive, real-time and online mass estimation. The
method employed YOLOv8-Seg for instance segmentation, Cycle-consistent
generative adversarial network (CycleGAN) for occluded region completion, and
tilt-angle correction to refine frontal projection area calculations. A
polynomial regression model then mapped the geometric features to mass.
Experiments demonstrated mean mass estimation errors of 8.11% for isolated
strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask
inpainting (LaMa) model in occlusion recovery, achieving superior pixel area
ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)
scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical
limitations of traditional methods, offering a robust solution for automated
harvesting and yield monitoring with complex occlusion patterns.

</details>


### [133] [Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion](https://arxiv.org/abs/2507.23508)
*Timing Li,Bing Cao,Jiahe Feng,Haifang Cao,Qinghau Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出首个基于双曲空间的图像配准网络Hy-CycleAlign，通过循环一致性和双曲对比对齐，显著提升跨模态图像配准和融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像配准方法在欧几里得空间中对图像进行平移，难以有效处理跨模态错位，导致配准和融合质量不佳，限制了多源信息融合的效果。

Method: 提出了Hyperbolic Cycle Alignment Network (Hy-CycleAlign)，该网络在双曲空间中进行图像对齐。它包含一个双路径跨模态循环配准框架，通过前向配准网络对齐输入，并由后向配准网络重建原始图像，形成具有几何一致性的闭环结构。此外，设计了一个Hyperbolic Hierarchy Contrastive Alignment (H²CA) 模块，将图像映射到双曲空间并施加配准约束，以减少模态差异干扰。

Result: 实验结果表明，在错位的多模态图像上，Hy-CycleAlign在图像配准和融合方面均显著优于现有方法，证明双曲空间能够实现更灵敏和有效的多模态图像配准。

Conclusion: 研究表明双曲空间在处理多模态图像配准时比欧几里得空间更具优势，提出的Hy-CycleAlign方法有效解决了跨模态错位问题，提升了多源数据融合质量。

Abstract: Image fusion synthesizes complementary information from multiple sources,
mitigating the inherent limitations of unimodal imaging systems. Accurate image
registration is essential for effective multi-source data fusion. However,
existing registration methods, often based on image translation in Euclidean
space, fail to handle cross-modal misalignment effectively, resulting in
suboptimal alignment and fusion quality. To overcome this limitation, we
explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle
Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign
is the first image registration method based on hyperbolic space. It introduces
a dual-path cross-modal cyclic registration framework, in which a forward
registration network aligns cross-modal inputs, while a backward registration
network reconstructs the original image, forming a closed-loop registration
structure with geometric consistency. Additionally, we design a Hyperbolic
Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into
hyperbolic space and imposes registration constraints, effectively reducing
interference caused by modality discrepancies. We further analyze image
registration in both Euclidean and hyperbolic spaces, demonstrating that
hyperbolic space enables more sensitive and effective multi-modal image
registration. Extensive experiments on misaligned multi-modal images
demonstrate that our method significantly outperforms existing approaches in
both image alignment and fusion. Our code will be publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [134] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 知识图谱补全（KGC）的事后可解释性缺乏统一性和评估标准。本研究提出了一个通用框架和改进的评估协议，旨在提升该领域研究的可复现性和影响力。


<details>
  <summary>Details</summary>
Motivation: KGC事后可解释性当前面临缺乏形式化定义和一致性评估的问题，这严重阻碍了研究结果的可复现性以及跨研究间的比较。

Method: ['提出了一个通用的多目标优化框架来表征事后解释，旨在平衡其有效性和简洁性，并统一了现有KGC事后可解释性算法及其产生的解释。', '建议并实证支持了使用如MRR（Mean Reciprocal Rank）和Hits@k等流行指标的改进评估协议。', '强调了可解释性应能有效回应终端用户有意义的查询，即其对用户查询的适应能力。']

Result: 本研究通过提出一个统一的解释框架和改进的评估标准，整合了现有KGC事后可解释性方法，并提升了评估的科学性和严谨性。

Conclusion: 该工作旨在通过统一方法和完善评估标准，解决KGC可解释性领域现存的挑战，从而使未来的研究更具可复现性和实际影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [135] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文分析了大型科学数据集用于AI基础模型训练的数据准备度（DRAI）原则，并提出了一个针对HPC环境的两维数据准备度框架。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将AI数据准备度（DRAI）原则应用于训练基础模型的大规模科学数据集，并识别跨领域工作流中的常见预处理模式和特定领域限制。

Method: 分析了气候、核聚变、生物/健康和材料等四个代表性领域的典型工作流程，并提出了一个两维数据准备度框架，包括数据准备度级别（从原始到AI就绪）和数据处理阶段（从摄取到分片），这些都针对高性能计算（HPC）环境，并强调基于Transformer的生成模型。该框架形成了一个概念性成熟度矩阵。

Result: 该框架概述了转换科学数据以进行可扩展AI训练的关键挑战，并且所形成的成熟度矩阵表征了科学数据的准备度。

Conclusion: 该研究旨在指导基础设施开发，以实现对科学领域可扩展和可复现AI的标准化、跨领域支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [136] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 本研究探讨多模态大语言模型（MLLMs）在提升推理能力时如何缓解社会偏见，并发现通过强化学习以1:4（去偏见:推理）的样本比例训练，可在降低10%偏见的同时，保留88%的推理准确性，为平衡公平性与能力提供指导。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在提高推理能力时常引入社会偏见，因此，探究推理能力提升与偏见缓解之间的相互作用及是否存在固有权衡，是一个亟待解决的问题。

Method: 首先，在相同条件下基准测试了三种偏见缓解策略（监督微调SFT、知识蒸馏KD、基于规则的强化学习RL）。然后，在每种策略下调整去偏见样本与推理样本的比例，以绘制推理能力与偏见之间的权衡关系图。

Result: 研究发现一个持续的“最佳平衡点”：通过强化学习训练的1:4（去偏见样本:推理样本）混合比例，能将刻板印象得分降低10%，同时保留模型原始推理准确性的88%。

Conclusion: 本研究为在多模态大语言模型中平衡公平性与能力提供了具体指导，表明通过优化训练样本比例和选择合适的去偏见策略，可以有效兼顾模型的推理性能和偏见缓解。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [137] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 本研究通过一个新颖的听觉图灵测试，揭示了当前最先进的AI系统在人类轻松完成的复杂听觉任务上表现极差，失败率超93%，远低于人类，突显了AI在选择性注意和噪声鲁棒性等方面的核心缺陷，并提出了未来AI听觉能力发展的方向。


<details>
  <summary>Details</summary>
Motivation: 受莫拉维克悖论启发，旨在验证并量化当前AI系统在人类毫不费力的复杂听觉任务上是否存在严重缺陷，从而揭示机器听觉与人类听觉之间的显著差距，并诊断失败的根本原因。

Method: 引入了一个包含917个挑战的“听觉图灵测试”，涵盖七个类别（如重叠语音、噪声中语音、空间音频、知觉错觉等）。使用该测试评估了包括GPT-4音频能力和OpenAI Whisper在内的最先进音频模型，并将其表现与人类在相同任务上的成功率进行对比。

Result: 评估结果显示，当前AI模型的失败率超过93%，即使表现最佳的模型也仅达到6.9%的准确率。相比之下，人类在相同任务上的成功率为52%，是表现最佳模型的7.5倍。这些结果暴露了AI系统在处理复杂听觉场景时，尤其是在选择性注意、噪声鲁棒性和语境适应方面的“聚焦失败”，表明现有架构缺乏类人听觉场景分析的基本机制。

Conclusion: 本研究确立了一个诊断框架来衡量机器听觉向人类水平迈进的进展，并强调了当前AI系统在复杂听觉任务上的根本性缺陷。为实现类人机器听觉，未来迫切需要开发整合选择性注意、基于物理的音频理解和上下文感知的新型AI方法。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [138] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本研究定义了一种“论证一致性”属性，并证明在判断式预测中，无论是人类还是大型语言模型，通过过滤不一致的预测可以显著提高预测准确性，但用户在实际中并不总是符合这种一致性。


<details>
  <summary>Details</summary>
Motivation: 判断式预测依赖人类意见，当这些意见形成论证结构时，需要从论证角度研究预测属性。本研究旨在定义并评估一种“论证一致性”属性，以期提高预测准确性。

Method: 1. 形式化定义了“论证一致性”属性，要求预测者的推理与其预测保持一致。2. 进行了三项评估：a) 评估了对人类预测者和基于大型语言模型（LLM）的预测者强制执行一致性的影响。b) 通过过滤不一致的预测来评估对预测准确性的提升。c) 进行了众包用户实验，以评估用户与该一致性属性的符合程度。

Result: 1. 过滤不一致的预测（基于论证一致性）显著提高了人类和大型语言模型预测的准确性。2. 尽管该属性具有直观性和实用性，但众包用户实验显示，用户通常不符合这种一致性属性。

Conclusion: 论证一致性在提高人类和LLM判断式预测准确性方面具有实际价值，应在基于论证的判断式预测中集成机制，以在获取群体预测之前过滤掉不一致的意见。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [139] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本文分析了本体介导查询回答中计算Shapley值责任度量（WSMS）的复杂性，揭示了其数据和组合复杂度的可处理性边界。


<details>
  <summary>Details</summary>
Motivation: 现有工作使用责任度量解释查询答案，但其计算复杂性尚不清楚。本文旨在研究在本体介导查询回答（OMQA）中计算一种新型Shapley值责任度量（WSMS）的计算复杂性，以确定其可处理性边界。

Method: 本文专注于分析本体介导查询回答中WSMS（Weighted Sums of Minimal Supports）的计算复杂度。方法包括：利用数据库成果分析数据复杂度；探索组合复杂度以理解可处理性边界；对DL-Lite方言进行深入分析以识别可处理的查询类别。

Result: 1. **数据复杂度：** 对于一阶可重写查询，WSMS计算具有多项式数据复杂度；当本体语言能编码可达性查询时，问题变为“shP”-hard。
2. **组合复杂度（不可处理性）：** 当本体语言支持合取时，即使是原子查询也难以处理；联合查询即使无本体也难以处理。
3. **组合复杂度（可处理性）：** 针对常见的DL-Lite方言，通过识别结构受限的合取查询，可实现可处理的WSMS计算。

Conclusion: 本研究为本体介导查询回答中WSMS责任度量的计算复杂性提供了全面的理解，划定了其可处理性边界，并为实际应用中选择适当的查询和本体语言提供了指导。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [140] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 本文提出一种“解决方案感知”ReLU评分（SAS）方法，用于优化神经网络验证中复杂实例的部分MILP求解器，显著减少了所需二元变量数和未决实例比例，同时保持高精度和合理运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统的分支定界（BaB）方法在处理复杂神经网络验证实例时效率低下。选择少数但重要的ReLU变量作为二元变量是关键，但现有方法在这方面表现次优。

Method: 1. 采用分而治之策略，将复杂BaB调用分解为多个小型“部分”MILP调用。
2. 提出一种新型的“解决方案感知”ReLU评分（SAS）方法，用于高效选择重要的ReLU变量进行二元处理。
3. 同时引入“全局”ReLU评分（GS）函数进行理论和实验比较。
4. 将该方法集成到混合MILP框架中，首先利用α,β-CROWN处理简单实例，再使用部分MILP处理复杂实例。

Result: 1. SAS在选择用于二元变量的变量集方面比GS更高效。
2. 与现有方法相比，SAS在保持相同准确度的前提下，将二元变量的数量减少了约6倍。
3. 混合MILP验证器实现了高准确性和高效率，将未决实例数量降低了高达40%，达到8-15%的低水平。
4. 即使对于200万参数的大型CNN，平均每实例运行时间保持在46秒至417秒的合理范围内。

Conclusion: 本研究提出的SAS方法通过优化ReLU变量选择，显著提升了部分MILP求解器的效率，并将其整合到混合MILP验证器中，成功构建了一个高精度、高效率且运行时间合理的神经网络验证器，大幅减少了未决实例，尤其适用于大型CNN的验证。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [141] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 该综述旨在评估基于LLM的AI科学家系统的现状、瓶颈和未来潜力，以探讨它们离颠覆科学研究范式还有多远。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推动了自动化科学发现，AI科学家系统已取得显著进展，引发了对AI科学家能否改变世界并重塑科学研究范式的疑问。本综述旨在回答此核心问题。

Method: 本文通过一项“前景导向的综述”，全面分析了AI科学家系统的现有成就，识别了关键瓶颈以及实现突破性发现所需的核心组成部分。

Result: 综述的成果是提供对当前AI科学家系统局限性的清晰理解，揭示其当前所处阶段、缺失要素以及科学AI的终极目标。

Conclusion: 本综述旨在帮助读者更清晰地理解当前AI科学家系统的局限性，明确其现状、缺失之处以及科学AI的最终目标，从而为未来的研究方向提供指导。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [142] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: 该研究基于AI的潜在风险，特别是超人工智能的威胁，提出AI不应完全自主，并强调人类监督的重要性。论文区分了AI自主的三个级别，并通过理论讨论、论证和证据支持其观点。


<details>
  <summary>Details</summary>
Motivation: 鉴于自主人工智能（AI）在带来诸多益处的同时也伴随着巨大风险，特别是超人工智能（ASI）可能在未来几十年内出现，研究旨在论证AI不应完全自主，强调负责任的人类监督对于规避风险至关重要。

Method: 研究首先识别并定义了自主AI的3个级别，将完全自主AI（可自主设定目标且无人类监督）归为第3级。为支持其立场，论文讨论了自主性、AI和代理理论，提出了12个论点和6个带反驳的反对意见，并在附录中提供了15项AI价值观未对齐及其他风险的最新证据。

Result: 论文识别并划分了AI自主性的3个级别，其中第3级被认定为完全自主且缺乏负责任的人类监督，存在高风险。研究通过详尽的理论分析、多项论点与反驳，以及现实证据，论证了完全自主AI的潜在危害。

Conclusion: 鉴于自主AI，特别是超人工智能带来的巨大风险，AI不应被赋予完全自主权。负责任的人类监督对于规避这些风险至关重要，尤其对于能够自行设定目标的完全自主AI（第3级）而言。

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [143] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 本文针对数据科学LLM代理缺乏系统性基准的问题，提出了一个反映真实用户交互的综合基准，评估了三种主流LLM在不同方法和多种数据科学任务上的表现，揭示了性能差异，并为未来研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）驱动的数据科学代理被迅速采用以自动化分析任务，但缺乏系统性基准来评估这些代理的效能和局限性。

Method: 本文引入了一个根据商业应用观察到的真实用户交互而设计的综合基准。评估了Claude-4.0-Sonnet、Gemini-2.5-Flash和OpenAI-o4-Mini三种LLM，采用零样本（Zero-shot）、多步骤（Multi-step）和SmolAgent三种方法。基准评估涵盖八类数据科学任务的性能，并探究模型对常见提示问题（如数据泄露和模糊指令）的敏感性，以及温度参数对结果的影响。

Result: 研究发现，所评估的模型和方法之间存在显著的性能差异，并突出了影响实际部署的关键因素。

Conclusion: 本文引入的基准数据集和评估框架旨在为未来研究更稳健和有效的数据科学代理提供基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [144] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: 本文开发了一个名为LLM4Rail的LLM增强型铁路服务咨询平台，引入了QTAO提示框架，并构建了中国铁路餐饮数据集CRFD-25，以提供个性化的铁路服务和餐饮推荐。


<details>
  <summary>Details</summary>
Motivation: 为满足日益增长的个性化铁路服务需求。

Method: 开发了LLM增强的铁路服务咨询平台LLM4Rail；提出了迭代式“问题-思考-行动-观察（QTAO）”提示框架，整合推理与任务导向行动；构建了公开的中国铁路餐饮数据集CRFD-25；引入了基于LLM的零样本对话式铁路餐饮推荐系统，并采用基于特征相似度的后处理步骤确保推荐与数据集一致。

Result: LLM4Rail能提供票务、餐饮推荐、天气信息和闲聊等定制模块；CRFD-25数据集涵盖按城市、菜系、年龄组和辣度分类的特色菜肴；实现了LLM-based零样本对话式推荐，并通过后处理确保推荐项与CRFD-25数据集对齐。

Conclusion: 本研究成功开发了LLM4Rail平台，结合创新的QTAO框架和CRFD-25数据集，有效提升了铁路服务的个性化和智能化水平，尤其在车载餐饮推荐方面提供了定制化解决方案。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [145] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 本文介绍了一个基于LLM的代理，能与工业级ERP系统进行自然语言交互，将自然语言查询转换为SQL，并通过双代理架构提高查询生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 旨在克服传统ERP系统操作复杂性，通过LLM实现自然语言与工业级ERP系统的无缝交互，将非技术用户的自然语言查询转化为可执行的SQL语句。

Method: 设计并实现了一个基于开放权重LLM的代理。该代理采用一种新颖的双代理架构，包含推理和批判阶段，以将自然语言查询准确翻译成SQL语句。

Result: 成功构建了一个能够与工业级ERP系统对话的LLM代理。通过提出的双代理架构，有效提高了从自然语言查询生成SQL语句的可靠性。

Conclusion: 本研究展示了通过创新LLM代理架构，实现自然语言与复杂工业级ERP系统可靠交互的可行性，为企业系统智能化提供了有效方案。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [146] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate是一种创新的LLM驱动指令合成方法，通过“微观-分散-宏观”多级注视机制，从无监督文本中挖掘细粒度信息，显著提高合成指令的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM指令数据合成方法面临挑战：传统方法依赖大量人工标注，而现有自动化合成方法在确保合成指令的多样性和难度方面存在显著局限性。

Method: 提出Self-Foveate，一种LLM驱动的指令合成方法。该方法引入“微观-分散-宏观”（Micro-Scatter-Macro）多级注视机制，引导LLM深入挖掘无监督文本中的细粒度信息，以提升合成指令的多样性和难度。

Result: 在多个无监督语料库和不同模型架构上的综合实验验证了所提方法的有效性和优越性。

Conclusion: Self-Foveate成功解决了LLM指令合成中数据多样性和难度的局限性，被证明是一种有效且优越的指令合成方案。相关数据和代码已公开。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [147] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 大语言模型在因果发现方面展现出显著潜力，但需结合精心设计的上下文学习框架才能充分发挥其能力，并提供跨领域可泛化的解决方案。


<details>
  <summary>Details</summary>
Motivation: 因果推断是大语言模型面临的基础挑战。传统模型在因果发现任务中易出现严重过拟合且在数据扰动下性能接近随机。研究旨在探讨最先进的推理模型能否在此任务中展现出强大的鲁棒性。

Method: 本研究在Corr2Cause基准上，使用OpenAI的o-series和DeepSeek-R模型家族进行因果发现。为充分利用这些模型的优势，引入了一种受Tree-of-Thoughts和Chain-of-Thoughts启发的模块化上下文学习（in-context）管道。此外，通过分析推理链长度、复杂性，并对传统模型与推理模型进行定性和定量比较，深入探究了该管道的影响。

Result: 以推理为先的架构（如OpenAI的o-series和DeepSeek-R）在原生表现上取得了比现有方法显著更大的提升。引入的模块化上下文学习管道使性能比传统基线提升了近三倍。

Conclusion: 研究结果表明，尽管先进的推理模型在因果发现领域代表着一个实质性的飞跃，但精心构建的上下文学习框架对于最大化其能力至关重要，并为跨不同领域的因果发现提供了一个可泛化的蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [148] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 本文提出了一种形式化且严谨的因果解释方法，适用于图像分类器，解决了现有解释缺乏严谨性或不兼容黑盒模型的痛点，并证明其高效和黑盒特性。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类器的解释算法缺乏形式严谨性。尽管基于逻辑的解释具有严谨性，但其可计算性依赖于对模型严格的假设，这在图像分类器上不成立。因此，需要一种既形式严谨又适用于图像分类器的解释方法。

Method: 本研究提出因果解释（causal explanations），并证明其具有与逻辑解释相同的形式属性，同时适用于黑盒算法和图像分类器。论文进一步引入了对比因果解释（contrastive causal explanations）和完整因果解释（complete causal explanations），后者定义为与原始图像具有完全相同分类置信度的解释。研究通过实现这些定义来验证其有效性。

Result: 实验结果表明，不同模型在充分性、对比性和完整性方面表现出不同的模式。所提出的算法计算高效，在ResNet50模型上平均每张图像计算所有类型解释仅需6秒。此外，算法是完全黑盒的，无需模型知识、内部访问、梯度或模型任何特定属性（如单调性）。

Conclusion: 因果解释为图像分类器提供了一种形式化、严谨且实用的黑盒解释方法，有效解决了现有解释工具在严谨性和适用性上的局限，并展示出高效的计算性能。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [149] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: 本文提出DICE框架，通过动态且理论驱动的上下文示例选择，显著提升大型语言模型代理的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在复杂推理和工具使用任务中表现出色，但其上下文学习(ICL)的有效性高度依赖于示范选择，次优示例会导致性能不稳定或下降。现有示例选择方法多依赖启发式或任务特定设计，缺乏通用且有理论基础的准则，难以开发出普适性的示范选择方法。

Method: 本文提出DICE（LLM代理动态上下文示例选择）框架，一个为代理任务设计的、有理论基础的ICL框架。DICE在推理的每一步选择最相关的示范，并从因果角度将示范知识分解为可迁移和不可迁移部分，揭示后者如何引入虚假依赖并损害泛化能力。DICE还提出了一个带有正式性能提升保证的逐步选择准则，且作为一个通用、与框架无关的插件模块，无需额外训练成本即可集成到现有代理框架中。

Result: 在多个不同领域的广泛实验表明，DICE方法有效且具有通用性。

Conclusion: DICE框架强调了有原则、上下文感知的示范选择对于构建稳健高效的大型语言模型代理的重要性。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [150] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文提出一种基于语义信任链的自主信任编排方法，利用智能体AI和超图在分布式协作系统中进行资源高效且任务特异的信任评估，尤其是在设备空闲时。


<details>
  <summary>Details</summary>
Motivation: 在分布式协作系统中，有效的任务完成依赖于对设备的信任评估，但任务复杂性、资源动态性和评估开销剧增，导致信任评估过程复杂且资源消耗大，不合时宜的评估会降低资源利用率并负面影响协作任务执行。

Method: 提出基于语义信任链的自主信任编排方法。该方法利用智能体AI和超图建立和维护设备间的信任关系。智能体AI在设备空闲时基于历史数据自主执行信任评估，并根据资源能力与任务需求进行任务特异性评估。通过维护嵌入信任语义的信任超图，实现协作方分层管理和识别，并支持多跳协作。

Result: 实验结果表明，所提出的方法实现了资源高效的信任评估。

Conclusion: 该方法有效解决了分布式协作系统中信任评估的复杂性和资源消耗问题，通过智能体AI和语义信任链的概念，实现了资源高效和任务特异性的信任评估，增强了大规模系统的协调能力。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [151] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 提出一种新颖的策略引导式智能体辅助记忆召回方法MemoCue，通过将查询转化为富含线索的查询来克服传统方法的局限性，有效提升记忆召回表现。


<details>
  <summary>Details</summary>
Motivation: 现有智能体辅助记忆召回方法受限于内存模块大小，导致记忆获取不完整且召回性能受影响。受记忆理论中“有效线索可主动激活相关记忆”的启发，研究旨在通过策略引导克服现有局限并提升记忆召回效果。主要挑战是如何为不同遗忘场景选择合适策略以及如何获得高质量的策略响应。

Method: 提出Recall Router框架。具体方法包括：设计5W召回地图（5W Recall Map）分类记忆查询并定义召回策略模式；结合蒙特卡洛树搜索（MCTS）的层级召回树（hierarchical recall tree）优化策略选择和响应生成；构建指令微调数据集并微调多个开源大语言模型（LLMs）以开发MemoCue智能体。

Result: 实验表明，MemoCue在召回启发性方面超越了基于LLM的方法17.74%。进一步的人工评估也凸显了其在记忆召回应用中的优势。

Conclusion: 所提出的策略引导式智能体辅助记忆召回方法MemoCue能够显著提升记忆召回性能，在实际应用中展现出优越性。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [152] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: 本文提出RAR模型，将协同思想融入强化学习的探索机制，解决个性化习题推荐中RL方法探索效率低的问题，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 个性化习题推荐旨在通过引导学生完成习题来提升学习目标掌握度。现有多数方法将此任务建模为马尔可夫决策过程并使用强化学习求解，但它们在训练过程中存在探索效率低、难以识别最佳习题的问题。

Method: 我们提出排名对齐推荐（RAR）模型，将协同思想融入到强化学习的探索机制中，以在有限的训练回合内实现更高效的探索。

Result: 实验结果表明，RAR能有效提升推荐性能，且该框架可应用于任何基于强化学习的习题推荐系统。

Conclusion: RAR通过引入协同探索机制，有效解决了强化学习在个性化习题推荐中探索效率低的问题，显著提高了推荐效果，并具有良好的通用性。

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [153] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests是一个新的基准测试，用于评估AI智能体在无外部工具下，在长文本互动式小说游戏中进行自主、持续推理的能力，以应对真实世界的复杂探索性环境挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体基准测试在评估工具使用或结构化任务方面有效，但未能充分捕捉智能体在探索性环境中自主操作的能力，尤其是在需要长期、自我导向推理和处理不断增长的上下文时。需要一个专注于智能体内在长上下文推理的基准，以促进更鲁棒的内在推理智能体的发展。

Method: 引入TextQuests基准测试，该基准基于Infocom系列的互动式小说游戏。这些游戏需要人类玩家超过30小时和数百次精确操作才能解决，作为评估AI智能体在集中、有状态任务上的有效代理。该基准特别设计为排除外部工具的使用，从而专注于LLM智能体在探索性环境中（需要试错学习和单次互动会话中持续解决问题）的内在长上下文推理能力。

Result: 创建并发布了TextQuests基准测试，它提供了一个评估AI智能体在复杂、互动式环境中进行自主问题解决和持续长上下文推理能力的有效手段，特别是在没有外部工具辅助的情况下。

Conclusion: TextQuests为评估AI智能体在模仿真实世界挑战的复杂、探索性环境中进行内在、持续推理的能力提供了一个关键的、无工具的基准，有助于推动能够进行更鲁棒内在推理的智能体的开发。

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [154] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: 提出Seed-Prover和Seed-Geometry模型，利用Lean形式化验证和迭代推理，显著提升IMO级别数学定理证明能力，并在国际奥数竞赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学推理方面表现出色，但由于缺乏清晰的监督信号，它们在定理证明方面仍面临挑战。

Method: 提出Seed-Prover，一种引理式全证明推理模型，通过Lean反馈、已证明引理和自我总结迭代优化证明。设计了三种测试时推理策略以实现深度和广度推理。为弥补Lean在几何支持方面的不足，还引入了几何推理引擎Seed-Geometry。

Result: Seed-Prover成功证明了78.1%的IMO问题，在MiniF2F上达到饱和，在PutnamBench上取得超过50%的成绩，大幅超越现有最佳性能。Seed-Geometry优于现有形式化几何引擎。两个系统在IMO 2025中完整证明了6个问题中的5个。

Conclusion: 本工作代表了自动化数学推理的重大进展，有效结合了形式化验证和长链思维推理，展示了其在复杂数学定理证明中的强大能力。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [155] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: 本文提出CoT-Self-Instruct方法，利用大语言模型（LLMs）通过思维链（CoT）推理生成高质量的合成数据，显著提升LLMs在可验证推理和指令遵循任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的合成数据以有效训练LLMs，尤其是在需要复杂推理和指令遵循的场景中，现有数据集可能不足或质量不高。

Method: CoT-Self-Instruct方法指导LLMs首先基于给定种子任务进行思维链（CoT）推理和规划，然后生成具有相似质量和复杂度的新的合成提示，最后通过自动化指标过滤以获取高质量数据。

Result: 在可验证推理任务中，所生成的合成数据在MATH500、AMC23、AIME24和GPQA-Diamond等数据集上显著优于s1k和OpenMathReasoning等现有训练数据集。在不可验证的指令遵循任务中，该方法在AlpacaEval 2.0和Arena-Hard上超越了人类或标准自指导提示的表现。

Conclusion: CoT-Self-Instruct是一种有效的合成数据生成方法，能够显著提升LLMs在可验证推理和指令遵循任务上的性能，为LLMs训练提供了高质量的数据来源。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [156] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: SimuRA是一种基于世界模型的LLM智能体架构，通过模拟规划克服了自回归LLM的局限性，实现了通用智能体推理，并在网页浏览任务中表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM智能体受限于“一任务一智能体”的模式，缺乏可扩展性和通用性，并受到自回归LLM固有缺陷的影响。研究旨在模仿人类通过心理模拟进行推理的方式，开发出更通用、更强大的AI智能体。

Method: 引入了SimuRA，一个面向目标的通用智能体推理架构。该架构基于最优智能体的原理化公式，通过引入一个基于LLM实现的世界模型来进行模拟规划，从而克服了自回归推理的局局限性。世界模型利用自然语言的丰富概念空间进行灵活规划。

Result: 在复杂的网页浏览任务上，SimuRA将航班搜索的成功率从0%提升至32.2%。特别是，基于世界模型的规划比自回归规划表现出高达124%的持续优势，证明了世界模型模拟作为一种推理范式的优越性。

Conclusion: 世界模型模拟是通用AI智能体的一种更优越的推理范式。该研究展示了训练单一、通用、基于LLM的智能体以在所有环境中实现超智能行动的巨大潜力。SimuRA作为一个基于该模型的网页浏览智能体已被发布为研究演示，供公众测试。

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [Neural Autoregressive Modeling of Brain Aging](https://arxiv.org/abs/2507.22954)
*Ridvan Yesiloglu,Wei Peng,Md Tauhidul Islam,Ehsan Adeli*

Main category: cs.LG

TL;DR: 本文提出NeuroAR，一种基于生成式自回归Transformer的新型大脑老化模拟模型，能够从早期MRI预测未来大脑结构，在图像保真度和老化模式一致性方面优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 大脑老化合成对临床和计算神经科学至关重要，能深入了解老化轨迹。但数据高维度、结构微小变化以及个体特异性模式给大脑老化合成带来了挑战。

Method: 本文提出了NeuroAR，一个基于生成式自回归Transformer的新型大脑老化模拟模型。NeuroAR通过自回归方式，从先前和未来扫描的合并token嵌入空间估计未来扫描的离散token图来合成老化大脑。它在每个尺度上拼接受试者的先前扫描，并通过交叉注意力利用其获取年龄和目标年龄来引导生成。

Result: 在老年人群和青少年受试者上评估，NeuroAR在图像保真度方面表现优于包括潜在扩散模型（LDM）和生成对抗网络在内的SOTA生成模型。此外，通过预训练的年龄预测器验证了合成图像与预期老化模式的一致性和真实性。NeuroAR显著优于LDM等关键模型，证明了其高保真度建模个体特异性大脑老化轨迹的能力。

Conclusion: NeuroAR模型能够以高保真度模拟个体特异性的大脑老化轨迹，在预测未来大脑结构方面展现出优越的性能和潜力。

Abstract: Brain aging synthesis is a critical task with broad applications in clinical
and computational neuroscience. The ability to predict the future structural
evolution of a subject's brain from an earlier MRI scan provides valuable
insights into aging trajectories. Yet, the high-dimensionality of data, subtle
changes of structure across ages, and subject-specific patterns constitute
challenges in the synthesis of the aging brain. To overcome these challenges,
we propose NeuroAR, a novel brain aging simulation model based on generative
autoregressive transformers. NeuroAR synthesizes the aging brain by
autoregressively estimating the discrete token maps of a future scan from a
convenient space of concatenated token embeddings of a previous and future
scan. To guide the generation, it concatenates into each scale the subject's
previous scan, and uses its acquisition age and the target age at each block
via cross-attention. We evaluate our approach on both the elderly population
and adolescent subjects, demonstrating superior performance over
state-of-the-art generative models, including latent diffusion models (LDM) and
generative adversarial networks, in terms of image fidelity. Furthermore, we
employ a pre-trained age predictor to further validate the consistency and
realism of the synthesized images with respect to expected aging patterns.
NeuroAR significantly outperforms key models, including LDM, demonstrating its
ability to model subject-specific brain aging trajectories with high fidelity.

</details>


### [158] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 本研究提出一种基于按键动态的框架，用于检测韩语环境中LLM辅助作弊，该框架考虑了认知上下文和LLM使用粒度。


<details>
  <summary>Details</summary>
Motivation: 以往研究在语言覆盖（特别是韩语）、认知上下文以及LLM参与粒度方面存在空白，本研究旨在填补这些空白。

Method: 构建了一个包含69名参与者的数据集，他们在三种条件下（真实写作、意译ChatGPT回复、转录ChatGPT回复）完成覆盖布鲁姆认知分类法六个认知过程的写作任务。提取可解释的时间和节奏特征，并在认知感知和认知无关设置下评估了多种分类器。

Result: 时间特征在认知感知评估场景下表现良好，而节奏特征在跨认知场景下泛化能力更强。检测真实写作和转录的回复比意译的回复更容易，且所提出的模型显著优于人类评估者。

Conclusion: 按键动态能够可靠地检测不同认知需求和写作策略（包括意译和转录LLM生成回复）下的LLM辅助写作。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [159] [Scientific Machine Learning with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2507.22959)
*Salah A. Faroughi,Farinaz Mostajeran,Amin Hamed Mashhadzadeh,Shirko Faroughi*

Main category: cs.LG

TL;DR: 本综述探讨了科学机器学习领域从多层感知器（MLP）向科尔莫戈罗夫-阿诺德网络（KAN）的转变，强调了KAN在解释性和灵活性方面的优势，并通过对比分析展示了其优越性能，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统的多层感知器（MLP）存在解释性差、激活函数固定以及难以捕获局部或高频特征的局限性。为克服这些问题，科学机器学习领域正转向采用具有更强解释性和灵活性的科尔莫戈罗夫-阿诺德网络（KAN），以更有效地建模复杂非线性交互。

Method: 本综述通过分类审查 KAN 模型的最新进展，将其分为数据驱动学习、物理信息建模和深度算子学习三个视角。每个视角都从架构设计、训练策略、应用效果以及与 MLP 的对比评估等多个维度进行分析和检验。

Result: 通过将 KAN 与 MLP 进行基准测试，研究发现 KAN 在准确性、收敛性和频谱表示方面均展现出持续改进，证明了 KAN 在捕获复杂动态和实现更有效学习方面的显著优势。

Conclusion: 本综述明确了 KAN 发展面临的关键挑战（如计算效率、理论保证、超参数调优和算法复杂性），并提出了未来的研究方向，旨在提升 KAN 框架的鲁棒性、可扩展性及物理一致性。

Abstract: The field of scientific machine learning, which originally utilized
multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold
Networks (KANs) for data encoding. This shift is driven by the limitations of
MLPs, including poor interpretability, fixed activation functions, and
difficulty capturing localized or high-frequency features. KANs address these
issues with enhanced interpretability and flexibility, enabling more efficient
modeling of complex nonlinear interactions and effectively overcoming the
constraints associated with conventional MLP architectures. This review
categorizes recent progress in KAN-based models across three distinct
perspectives: (i) data-driven learning, (ii) physics-informed modeling, and
(iii) deep operator learning. Each perspective is examined through the lens of
architectural design, training strategies, application efficacy, and
comparative evaluation against MLP-based counterparts. By benchmarking KANs
against MLPs, we highlight consistent improvements in accuracy, convergence,
and spectral representation, clarifying KANs' advantages in capturing complex
dynamics while learning more effectively. Finally, this review identifies
critical challenges and open research questions in KAN development,
particularly regarding computational efficiency, theoretical guarantees,
hyperparameter tuning, and algorithm complexity. We also outline future
research directions aimed at improving the robustness, scalability, and
physical consistency of KAN-based frameworks.

</details>


### [160] [Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations](https://arxiv.org/abs/2507.22962)
*Boyuan Zheng,Victor W. Chu*

Main category: cs.LG

TL;DR: 本文提出一个结合深度学习和可解释AI的多灾害预警框架，用于农业领域。该框架利用气象数据预测多种极端事件，并通过时间序列可解释性提供详细解释，实现了高预测精度，尤其在使用BiLSTM架构时。


<details>
  <summary>Details</summary>
Motivation: 气候极端事件对农业构成日益增长的风险，现有单灾害预测方法难以捕捉并发气候事件的复杂交互，且缺乏从近期气候行为中持续学习的能力，因此急需可靠的多灾害早期预警系统。

Method: 该研究结合序列深度学习模型和先进的可解释人工智能(XAI)技术，构建了一个多灾害农业预测框架。通过使用美国四个主要农业地区2010年至2023年的气象数据，针对极端寒冷、洪水、霜冻、冰雹、热浪和强降雨等多种严重事件类型进行预测验证。框架独特地整合了注意力机制与TimeSHAP（一种针对时间序列的循环XAI解释器），以提供全面的时间解释。

Result: 实验结果表明该框架具有强大的预测准确性，尤其在使用BiLSTM架构时表现突出。该系统能够为细致、主动的风险管理策略提供信息支持，揭示了哪些气候特征具有影响力以及它们影响发生的确切时间。

Conclusion: 这项研究显著提升了多灾害早期预警系统的可解释性和适用性，有助于在农业气候风险管理中建立跨学科信任并促进有效的决策过程。

Abstract: Climate extremes present escalating risks to agriculture intensifying the
need for reliable multi-hazard early warning systems (EWS). The situation is
evolving due to climate change and hence such systems should have the
intelligent to continue to learn from recent climate behaviours. However,
traditional single-hazard forecasting methods fall short in capturing complex
interactions among concurrent climatic events. To address this deficiency, in
this paper, we combine sequential deep learning models and advanced Explainable
Artificial Intelligence (XAI) techniques to introduce a multi-hazard
forecasting framework for agriculture. In our experiments, we utilize
meteorological data from four prominent agricultural regions in the United
States (between 2010 and 2023) to validate the predictive accuracy of our
framework on multiple severe event types, which are extreme cold, floods,
frost, hail, heatwaves, and heavy rainfall, with tailored models for each area.
The framework uniquely integrates attention mechanisms with TimeSHAP (a
recurrent XAI explainer for time series) to provide comprehensive temporal
explanations revealing not only which climatic features are influential but
precisely when their impacts occur. Our results demonstrate strong predictive
accuracy, particularly with the BiLSTM architecture, and highlight the system's
capacity to inform nuanced, proactive risk management strategies. This research
significantly advances the explainability and applicability of multi-hazard
EWS, fostering interdisciplinary trust and effective decision-making process
for climate risk management in the agricultural industry.

</details>


### [161] [FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization](https://arxiv.org/abs/2507.22963)
*Abdelrhman Gaber,Hassan Abd-Eltawab,John Elgallab,Youssif Abuzied,Dineo Mpanya,Turgay Celik,Swarun Kumar,Tamer ElBatt*

Main category: cs.LG

TL;DR: FedCVD++是一个增强型联邦学习框架，用于心血管疾病风险预测，首次将非参数模型集成到联邦医疗系统中，并提出通信优化和类别不平衡解决方案，实现了最先进的性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病每年导致全球超过1700万人死亡，迫切需要隐私保护的预测系统来应对这一严峻的健康挑战。

Method: 提出了FedCVD++，一个增强型联邦学习（FL）框架，用于冠心病风险预测。该框架集成了参数模型（逻辑回归、SVM、神经网络）和非参数模型（随机森林、XGBoost）。为解决FL挑战，提出了：(1) 树子集采样以减少随机森林通信开销70%；(2) 基于XGBoost的特征提取以实现轻量级联邦集成；(3) 联邦SMOTE同步以解决跨机构类别不平衡问题。研究在Framingham数据集上进行评估。

Result: FedCVD++在Framingham数据集上取得了最先进（SOTA）结果：联邦XGBoost（F1 = 0.80）优于集中式对应模型（F1 = 0.78），联邦随机森林（F1 = 0.81）与非联邦性能持平。通信效率策略在保持95%准确率的同时，将带宽消耗降低了3.2倍。与现有FL框架相比，FedCVD++的F1分数最高提高15%，并具有更优的多机构部署可扩展性。

Conclusion: 本工作首次将非参数模型实际集成到联邦医疗系统中，提供了一个在真实临床约束下验证的隐私保护解决方案。

Abstract: Cardiovascular diseases (CVD) cause over 17 million deaths annually
worldwide, highlighting the urgent need for privacy-preserving predictive
systems. We introduce FedCVD++, an enhanced federated learning (FL) framework
that integrates both parametric models (logistic regression, SVM, neural
networks) and non-parametric models (Random Forest, XGBoost) for coronary heart
disease risk prediction. To address key FL challenges, we propose: (1)
tree-subset sampling that reduces Random Forest communication overhead by 70%,
(2) XGBoost-based feature extraction enabling lightweight federated ensembles,
and (3) federated SMOTE synchronization for resolving cross-institutional class
imbalance.
  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves
state-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its
centralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)
matches non-federated performance. Additionally, our communication-efficient
strategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.
  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher
F1-scores and superior scalability for multi-institutional deployment. This
work represents the first practical integration of non-parametric models into
federated healthcare systems, providing a privacy-preserving solution validated
under real-world clinical constraints.

</details>


### [162] [Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation](https://arxiv.org/abs/2507.23000)
*Shengao Yi,Xiaojiang Li,Wei Tu,Tianhong Zhao*

Main category: cs.LG

TL;DR: 该研究提出了一种名为GSM-UTCI的多模态深度学习框架，能以高精度和高效率预测城市超局部热压力，并可用于评估城市绿化策略的降温效果。


<details>
  <summary>Details</summary>
Motivation: 气候变化和城市化加剧了城市户外热压力。传统物理模型（如SOLWEIG和ENVI-met）虽能详细评估人体感知热暴露，但计算成本高昂，难以用于城市尺度的规划。

Method: 本研究提出GSM-UTCI，一个多模态深度学习框架，用于预测1米超局部分辨率的白天平均通用热气候指数（UTCI）。该模型融合了地表形态（nDSM）、高分辨率土地覆盖数据和逐时气象条件，并采用特征线性调制（FiLM）架构，动态地根据大气环境调整空间特征。模型使用SOLWEIG衍生的UTCI地图进行训练。

Result: GSM-UTCI模型实现了接近物理模型的精度，R2为0.9151，平均绝对误差（MAE）为0.41°C。同时，其推理时间从数小时缩短至不到五分钟。应用于费城模拟景观改造场景时，结果显示，用树冠替换不透水表面产生了最高的总降温效益（在270.7平方公里范围内UTCI平均变化-4.18°C）。区域（tract-level）双变量分析表明，降温潜力与土地覆盖比例高度一致。

Conclusion: GSM-UTCI证明了其作为一个可扩展、细粒度的城市气候适应决策支持工具的实用性，能够为不同城市环境下的绿化策略提供基于情景的评估。

Abstract: As extreme heat events intensify due to climate change and urbanization,
cities face increasing challenges in mitigating outdoor heat stress. While
traditional physical models such as SOLWEIG and ENVI-met provide detailed
assessments of human-perceived heat exposure, their computational demands limit
scalability for city-wide planning. In this study, we propose GSM-UTCI, a
multimodal deep learning framework designed to predict daytime average
Universal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The
model fuses surface morphology (nDSM), high-resolution land cover data, and
hourly meteorological conditions using a feature-wise linear modulation (FiLM)
architecture that dynamically conditions spatial features on atmospheric
context. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical
accuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\deg}C,
while reducing inference time from hours to under five minutes for an entire
city. To demonstrate its planning relevance, we apply GSM-UTCI to simulate
systematic landscape transformation scenarios in Philadelphia, replacing bare
earth, grass, and impervious surfaces with tree canopy. Results show spatially
heterogeneous but consistently strong cooling effects, with impervious-to-tree
conversion producing the highest aggregated benefit (-4.18{\deg}C average
change in UTCI across 270.7 km2). Tract-level bivariate analysis further
reveals strong alignment between thermal reduction potential and land cover
proportions. These findings underscore the utility of GSM-UTCI as a scalable,
fine-grained decision support tool for urban climate adaptation, enabling
scenario-based evaluation of greening strategies across diverse urban
environments.

</details>


### [163] [Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead](https://arxiv.org/abs/2507.23009)
*Tom Sühr,Florian E. Dorner,Olawale Salaudeen,Augustin Kelava,Samira Samadi*

Main category: cs.LG

TL;DR: LLMs在人类测试上的优异表现常被错误解读为类人特征。本文指出这是本体论错误，倡导停止用人类测试评估AI，转而开发AI特有的、有原则的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有将大型语言模型（LLMs）在人类认知和心理测试上的高分表现解读为具备类人特征的趋势，存在根本性错误。人类测试是针对特定人群理论驱动和校准的，将其未经检验地应用于非人类主体会误判所衡量的内容。此外，将AI基准性能视为“智能”等特性的衡量，缺乏足够的理论和实证依据，且存在有效性、数据污染和文化偏见等已知问题。

Method: 本文采用批判性分析和理论论证的方法。首先，指出人类心理和教育测试的本质及其局限性；其次，揭示将这些测试应用于AI所导致的本体论错误；最后，列举当前AI基准测试中将性能等同于人类特质时存在的关键问题，并在此基础上提出停止现有做法并开发AI特定评估框架的论点。

Result: 将LLMs在人类测试上的表现解释为类人特征构成了一种本体论错误。缺乏充分的理论和实证依据来支持将AI基准性能解读为人类特质。迫切需要为AI系统开发专门的、有原则的评估框架。

Conclusion: 建议停止使用为人类设计的标准化测试来评估AI系统。取而代之，应开发一套有原则的、针对AI系统独特性质量身定制的AI特定评估框架，这些框架既可以借鉴现有心理测量学测试的构建和验证方法，也可以完全从零开始创建。

Abstract: Large Language Models (LLMs) have achieved remarkable results on a range of
standardized tests originally designed to assess human cognitive and
psychological traits, such as intelligence and personality. While these results
are often interpreted as strong evidence of human-like characteristics in LLMs,
this paper argues that such interpretations constitute an ontological error.
Human psychological and educational tests are theory-driven measurement
instruments, calibrated to a specific human population. Applying these tests to
non-human subjects without empirical validation, risks mischaracterizing what
is being measured. Furthermore, a growing trend frames AI performance on
benchmarks as measurements of traits such as ``intelligence'', despite known
issues with validity, data contamination, cultural bias and sensitivity to
superficial prompt changes. We argue that interpreting benchmark performance as
measurements of human-like traits, lacks sufficient theoretical and empirical
justification. This leads to our position: Stop Evaluating AI with Human Tests,
Develop Principled, AI-specific Tests instead. We call for the development of
principled, AI-specific evaluation frameworks tailored to AI systems. Such
frameworks might build on existing frameworks for constructing and validating
psychometrics tests, or could be created entirely from scratch to fit the
unique context of AI.

</details>


### [164] [Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods](https://arxiv.org/abs/2507.23010)
*Siwoo Park*

Main category: cs.LG

TL;DR: 本研究探讨了多模态AI模型潜在空间的逆向映射能力，发现尽管优化可强制模型在文本层面匹配目标，但其生成的逆向结果在感知上混乱且缺乏语义连贯性，揭示了现有模型在逆向任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在正向任务（如文生图、音转文）表现出色，但其逆向映射（从输出推断输入）的潜力鲜有探索。本研究旨在填补这一空白，探究多模态潜在空间是否支持语义有意义且感知连贯的逆向操作。

Method: 提出一个基于优化的框架，用于从期望输出中推断输入特性。该框架双向应用于文本-图像模态（使用BLIP, Flux.1-dev模型）和文本-音频模态（使用Whisper-Large-V3, Chatterbox-TTS模型）。

Result: 实验结果验证了核心假设：优化虽能使模型在文本上与目标对齐（如正确描述图片、准确转录音频），但逆向结果的感知质量混乱且不连贯。此外，重建的潜在空间嵌入常缺乏语义可解释性，与无意义的词汇标记对齐。

Conclusion: 多模态潜在空间，主要为正向任务优化，不固有具备支持鲁棒且可解释逆向映射所需的结构。研究强调未来需要开发真正语义丰富且可逆的多模态潜在空间。

Abstract: This paper investigates the inverse capabilities and broader utility of
multimodal latent spaces within task-specific AI (Artificial Intelligence)
models. While these models excel at their designed forward tasks (e.g.,
text-to-image generation, audio-to-text transcription), their potential for
inverse mappings remains largely unexplored. We propose an optimization-based
framework to infer input characteristics from desired outputs, applying it
bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio
(Whisper-Large-V3, Chatterbox-TTS) modalities.
  Our central hypothesis posits that while optimization can guide models
towards inverse tasks, their multimodal latent spaces will not consistently
support semantically meaningful and perceptually coherent inverse mappings.
Experimental results consistently validate this hypothesis. We demonstrate that
while optimization can force models to produce outputs that align textually
with targets (e.g., a text-to-image model generating an image that an image
captioning model describes correctly, or an ASR model transcribing optimized
audio accurately), the perceptual quality of these inversions is chaotic and
incoherent. Furthermore, when attempting to infer the original semantic input
from generative models, the reconstructed latent space embeddings frequently
lack semantic interpretability, aligning with nonsensical vocabulary tokens.
  These findings highlight a critical limitation. multimodal latent spaces,
primarily optimized for specific forward tasks, do not inherently possess the
structure required for robust and interpretable inverse mappings. Our work
underscores the need for further research into developing truly semantically
rich and invertible multimodal latent spaces.

</details>


### [165] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: 本文提出KLLM，一个软硬件协同设计框架，旨在解决大型语言模型（LLM）推理中K-Means量化面临的效率挑战和激活异常值问题，显著提升了推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: LLM推理面临内存和计算密集型挑战。尽管权重和激活量化（WAQ）能减少内存和计算需求，但现有WAQ设计存在局限：一是K-Means等非均匀量化虽精度高但无法直接在低精度硬件上执行，需耗时的反量化；二是激活异常值影响低精度WAQ效果，现有检测方法要么降低性能要么引入巨大运行时开销。

Method: 本文提出KLLM软硬件协同设计框架。KLLM包含：1) 基于索引的计算方案，用于高效执行K-Means量化数据的矩阵乘法和非线性操作，避免大部分反量化和全精度计算；2) 新型异常值检测引擎Orizuru，能在在线推理期间高效识别激活数据流中的最大和最小top-k元素。

Result: 实验结果表明，与A100 GPU相比，KLLM平均加速9.67倍，能效提高229.50倍。与Atom相比，KLLM平均加速7.03倍，能效提高150.21倍。

Conclusion: KLLM成功克服了K-Means量化在LLM推理中的挑战，通过软硬件协同设计，高效处理了非均匀量化数据的计算和激活异常值，显著提升了LLM推理的性能和能效，充分发挥了K-Means量化的潜力。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [166] [Linking Actor Behavior to Process Performance Over Time](https://arxiv.org/abs/2507.23037)
*Aurélie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 本文通过结合参与者行为分析和Granger因果关系，使用时间序列数据揭示了参与者行为（如交互）对流程性能（尤其是吞吐时间）的直接因果影响，克服了传统方法对个体行为动态性忽视的局限。


<details>
  <summary>Details</summary>
Motivation: 传统流程挖掘方法多采用聚合静态数据，忽视了个体参与者行为及其交互的时间和因果动态性，导致无法准确捕捉真实世界流程的复杂性及其对性能的影响。

Method: 整合参与者行为分析与Granger因果关系，从真实事件日志中构建参与者交互（如延续、中断、交接）和流程结果的时间序列数据，并利用Group Lasso进行滞后选择以识别因果链接。

Result: 参与者行为对流程性能（特别是吞吐时间）具有直接且可衡量的影响。研究识别出一小部分持续具有影响力的滞后因子，能够捕捉大部分因果影响。

Conclusion: 基于参与者中心的、时间序列的方法能够有效揭示驱动流程结果的时间依赖性，从而提供对个体行为如何影响整体流程效率的更细致理解。

Abstract: Understanding how actor behavior influences process outcomes is a critical
aspect of process mining. Traditional approaches often use aggregate and static
process data, overlooking the temporal and causal dynamics that arise from
individual actor behavior. This limits the ability to accurately capture the
complexity of real-world processes, where individual actor behavior and
interactions between actors significantly shape performance. In this work, we
address this gap by integrating actor behavior analysis with Granger causality
to identify correlating links in time series data. We apply this approach to
realworld event logs, constructing time series for actor interactions, i.e.
continuation, interruption, and handovers, and process outcomes. Using Group
Lasso for lag selection, we identify a small but consistently influential set
of lags that capture the majority of causal influence, revealing that actor
behavior has direct and measurable impacts on process performance, particularly
throughput time. These findings demonstrate the potential of actor-centric,
time series-based methods for uncovering the temporal dependencies that drive
process outcomes, offering a more nuanced understanding of how individual
behaviors impact overall process efficiency.

</details>


### [167] [Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost](https://arxiv.org/abs/2507.23043)
*Junyi Fan,Li Sun,Shuheng Chen,Yong Si,Minoo Ahmadi,Greg Placencia,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 本研究开发并验证了一个基于ICU常规数据的机器学习模型，能有效预测万古霉素相关性肌酐升高，具有良好的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 万古霉素是ICU中重要的抗生素，但存在较高的肾毒性风险。在危重症患者中早期预测肾损伤具有挑战性，因此需要开发一种模型来提前识别风险。

Method: 分析了来自MIMIC-IV数据库的10,288名接受万古霉素治疗的ICU患者数据。肾损伤定义为KDIGO标准。采用SelectKBest和随机森林进行特征选择（最终选择15个）。测试了六种机器学习算法，并通过5折交叉验证评估性能。使用SHAP、ALE和贝叶斯后验采样评估模型可解释性。

Result: 2,903名患者（28.2%）出现肌酐升高。CatBoost模型表现最佳（AUROC 0.818，敏感性0.800，特异性0.681，阴性预测值0.900）。磷酸盐、总胆红素、镁、Charlson指数和APSIII是关键预测因子。SHAP分析确认磷酸盐为主要风险因素，ALE揭示剂量-反应模式，贝叶斯分析估算出高危病例的平均风险为60.5%。

Conclusion: 该机器学习模型能够利用ICU常规数据准确且可解释地预测万古霉素相关性肌酐升高，有助于危重症患者的早期风险检测和及时干预。

Abstract: Background: Vancomycin, a key antibiotic for severe Gram-positive infections
in ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in
critically ill patients is challenging. This study aimed to develop a machine
learning model to predict vancomycin-related creatinine elevation using routine
ICU data.
  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV
database who received vancomycin. Kidney injury was defined by KDIGO criteria
(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were
selected via SelectKBest (top 30) and Random Forest ranking (final 15). Six
algorithms were tested with 5-fold cross-validation. Interpretability was
evaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior
sampling.
  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.
CatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,
specificity 0.681, negative predictive value 0.900). Key predictors were
phosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP
confirmed phosphate as a major risk factor. ALE showed dose-response patterns.
Bayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)
in high-risk cases.
  Conclusions: This machine learning model predicts vancomycin-associated
creatinine elevation from routine ICU data with strong accuracy and
interpretability, enabling early risk detection and supporting timely
interventions in critical care.

</details>


### [168] [Locally Differentially Private Thresholding Bandits](https://arxiv.org/abs/2507.23073)
*Annalisa Barbara,Joseph Lazzaro,Ciara Pike-Burke*

Main category: cs.LG

TL;DR: 研究了阈值强盗问题中局部差分隐私的影响，并提出了提供强隐私保障且性能接近最优的算法。


<details>
  <summary>Details</summary>
Motivation: 在阈值强盗问题中，需要在保护用户数据隐私（通过局部差分隐私）的同时，有效地识别出预期奖励超过特定阈值的“臂”。

Method: 提出了利用基于伯努利差分隐私机制获取的私有响应来识别超过预设阈值的“臂”的方法。研究考虑了固定预算和固定置信度两种设置。

Result: 研究表明所提出的方法提供了强大的隐私保证，并推导了其理论性能界限。同时，还给出了任何差分隐私机制所致额外损失的通用下限，并证明所提算法的性能在多对数因子内匹配这些下限。

Conclusion: 本研究的结果为强盗问题中的隐私保护决策框架提供了重要见解，展示了在确保强大隐私保障的同时，算法仍能达到接近最优的性能。

Abstract: This work investigates the impact of ensuring local differential privacy in
the thresholding bandit problem. We consider both the fixed budget and fixed
confidence settings. We propose methods that utilize private responses,
obtained through a Bernoulli-based differentially private mechanism, to
identify arms with expected rewards exceeding a predefined threshold. We show
that this procedure provides strong privacy guarantees and derive theoretical
performance bounds on the proposed algorithms. Additionally, we present general
lower bounds that characterize the additional loss incurred by any
differentially private mechanism, and show that the presented algorithms match
these lower bounds up to poly-logarithmic factors. Our results provide valuable
insights into privacy-preserving decision-making frameworks in bandit problems.

</details>


### [169] [A Foundation Model for Material Fracture Prediction](https://arxiv.org/abs/2507.23077)
*Agnese Marcato,Aleksandra Pachalieva,Ryley G. Hill,Kai Gao,Xiaoyu Wang,Esteban Rougier,Zhou Lei,Vinamra Agrawal,Janel Chua,Qinjun Kang,Jeffrey D. Hyman,Abigail Hunter,Nathan DeBardeleben,Earl Lawrence,Hari Viswanathan,Daniel O'Malley,Javier E. Santos*

Main category: cs.LG

TL;DR: 本文提出一个基于Transformer的裂纹预测基础模型，通过整合多源数据和文本描述，实现了对多种材料和复杂加载条件的统一预测，并能以极少样本泛化到新材料，显著降低了数据需求。


<details>
  <summary>Details</summary>
Motivation: 准确预测材料失效对结构和系统安全至关重要。然而，现有方法在材料多样性、几何形状和载荷条件下的断裂行为建模上存在挑战：机器学习模型泛化能力差，物理模拟器则碎片化且资源消耗大。因此，亟需一种能克服这些局限性的统一预测模型。

Method: 研究提出了一个数据驱动的裂纹预测基础模型，采用基于Transformer的架构。该模型能够处理来自不同模拟器、多种材料（如塑料粘结炸药、钢、铝、页岩、钨）和多样载荷条件下的数据。它支持结构化和非结构化网格，并结合大型语言模型（LLM）对材料属性、边界条件和求解器设置等文本输入进行嵌入，形成多模态输入设计。

Result: 该模型实现了对不同模拟场景的灵活适应，无需改变模型架构。它可以通过极少量数据在多种下游任务上进行微调，例如失效时间预测、裂纹演化建模以及适应有限-离散元方法模拟。更重要的是，它能够以低至单个样本的极少数据泛化到钛和混凝土等未见材料，大幅降低了传统机器学习所需的数据量。

Conclusion: 研究表明，裂纹预测可以统一到一个单一模型架构之下，为目前依赖于特定模拟器的工作流程提供了一种可扩展、可扩展的替代方案。

Abstract: Accurately predicting when and how materials fail is critical to designing
safe, reliable structures, mechanical systems, and engineered components that
operate under stress. Yet, fracture behavior remains difficult to model across
the diversity of materials, geometries, and loading conditions in real-world
applications. While machine learning (ML) methods show promise, most models are
trained on narrow datasets, lack robustness, and struggle to generalize.
Meanwhile, physics-based simulators offer high-fidelity predictions but are
fragmented across specialized methods and require substantial high-performance
computing resources to explore the input space. To address these limitations,
we present a data-driven foundation model for fracture prediction, a
transformer-based architecture that operates across simulators, a wide range of
materials (including plastic-bonded explosives, steel, aluminum, shale, and
tungsten), and diverse loading conditions. The model supports both structured
and unstructured meshes, combining them with large language model embeddings of
textual input decks specifying material properties, boundary conditions, and
solver settings. This multimodal input design enables flexible adaptation
across simulation scenarios without changes to the model architecture. The
trained model can be fine-tuned with minimal data on diverse downstream tasks,
including time-to-failure estimation, modeling fracture evolution, and adapting
to combined finite-discrete element method simulations. It also generalizes to
unseen materials such as titanium and concrete, requiring as few as a single
sample, dramatically reducing data needs compared to standard ML. Our results
show that fracture prediction can be unified under a single model architecture,
offering a scalable, extensible alternative to simulator-specific workflows.

</details>


### [170] [On the Sustainability of AI Inferences in the Edge](https://arxiv.org/abs/2507.23093)
*Ghazal Sobhani,Md. Monzurul Amin Ifath,Tushar Sharma,Israat Haque*

Main category: cs.LG

TL;DR: 本研究评估了在资源受限的边缘设备（如树莓派、Jetson Nano等）上运行传统、神经网络和大型语言模型的性能和能耗，分析了F1分数、推理时间、推理功耗和内存使用之间的权衡，旨在为边缘AI部署提供决策依据。


<details>
  <summary>Details</summary>
Motivation: 物联网和AI应用的普及推动了数据驱动系统向边缘部署。边缘设备需支持对延迟敏感的应用，其性能和能耗至关重要。然而，目前缺乏系统性研究来指导在这些设备上进行AI推理时的设备和模型选择，以满足应用需求。

Method: 通过在多种主流边缘设备（包括Raspberry Pi, Intel Neural Compute Stick, NVIDIA Jetson Nano, Google Coral USB）上，严格表征传统模型、神经网络和大型语言模型的性能。具体分析并权衡了模型F1分数、推理时间、推理功耗和内存使用等指标。

Result: 研究揭示了在不同边缘设备上运行传统、神经网络和大型语言模型时，模型F1分数、推理时间、推理功耗和内存使用之间存在复杂的性能与资源权衡。

Conclusion: 为了实现实用的边缘AI部署，可以通过硬件和框架优化以及AI模型外部参数调优，来平衡模型性能和资源使用。本研究为在设备和模型选择上做出明智决策以满足应用需求提供了关键信息。

Abstract: The proliferation of the Internet of Things (IoT) and its cutting-edge
AI-enabled applications (e.g., autonomous vehicles and smart industries)
combine two paradigms: data-driven systems and their deployment on the edge.
Usually, edge devices perform inferences to support latency-critical
applications. In addition to the performance of these resource-constrained edge
devices, their energy usage is a critical factor in adopting and deploying edge
applications. Examples of such devices include Raspberry Pi (RPi), Intel Neural
Compute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).
Despite their adoption in edge deployment for AI inferences, there is no study
on their performance and energy usage for informed decision-making on the
device and model selection to meet the demands of applications. This study
fills the gap by rigorously characterizing the performance of traditional,
neural networks, and large language models on the above-edge devices.
Specifically, we analyze trade-offs among model F1 score, inference time,
inference power, and memory usage. Hardware and framework optimization, along
with external parameter tuning of AI models, can balance between model
performance and resource usage to realize practical edge AI deployments.

</details>


### [171] [Scalable Generative Modeling of Weighted Graphs](https://arxiv.org/abs/2507.23111)
*Richard Williams,Eric Nalisnick,Andrew Holbrook*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Weighted graphs are ubiquitous throughout biology, chemistry, and the social
sciences, motivating the development of generative models for abstract weighted
graph data using deep neural networks. However, most current deep generative
models are either designed for unweighted graphs and are not easily extended to
weighted topologies or incorporate edge weights without consideration of a
joint distribution with topology. Furthermore, learning a distribution over
weighted graphs must account for complex nonlocal dependencies between both the
edges of the graph and corresponding weights of each edge. We develop an
autoregressive model BiGG-E, a nontrivial extension of the BiGG model, that
learns a joint distribution over weighted graphs while still exploiting
sparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +
m)\log n)$ time. Simulation studies and experiments on a variety of benchmark
datasets demonstrate that BiGG-E best captures distributions over weighted
graphs while remaining scalable and computationally efficient.

</details>


### [172] [FLOSS: Federated Learning with Opt-Out and Straggler Support](https://arxiv.org/abs/2507.23115)
*David J Goetze,Dahlia J Felten,Jeannie R Albrecht,Rohit Bhattacharya*

Main category: cs.LG

TL;DR: FLOSS系统旨在缓解联邦学习中因用户选择退出和掉队者导致的缺失数据对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中现有隐私保护工作主要关注用户同意共享数据的情况。然而，现代隐私协议允许用户选择不共享数据，这与异构设备导致的掉队者结合，会造成数据缺失，进而引入偏差并降低模型性能。

Method: 提出了一个名为FLOSS的系统。

Result: 通过仿真实验，经验性地验证了FLOSS系统在缓解缺失数据影响方面的性能。

Conclusion: FLOSS系统能够有效缓解联邦学习中因用户选择退出和掉队者导致的缺失数据对模型性能的影响。

Abstract: Previous work on data privacy in federated learning systems focuses on
privacy-preserving operations for data from users who have agreed to share
their data for training. However, modern data privacy agreements also empower
users to use the system while opting out of sharing their data as desired. When
combined with stragglers that arise from heterogeneous device capabilities, the
result is missing data from a variety of sources that introduces bias and
degrades model performance. In this paper, we present FLOSS, a system that
mitigates the impacts of such missing data on federated learning in the
presence of stragglers and user opt-out, and empirically demonstrate its
performance in simulations.

</details>


### [173] [Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts](https://arxiv.org/abs/2507.23128)
*Anaïs Baranger,Lucas Maison*

Main category: cs.LG

TL;DR: 本研究探讨了语音关键词分类器在域外(OOD)条件下的鲁棒性和泛化能力，发现噪声感知训练在某些配置下能提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域已发现域内(ID)和域外(OOD)准确性之间存在强相关性，但在基于音频的模型中，这种关系尚未得到充分探索。

Method: 本研究调查了训练条件和输入特征如何影响语音关键词分类器在OOD条件下的鲁棒性和泛化能力。基准测试了多种神经网络架构，并使用了公平性(F)和鲁棒性(R)两个指标来量化噪声对泛化的影响。

Result: 研究结果表明，噪声感知训练在某些配置下能够提高模型的鲁棒性。

Conclusion: 这些发现为语音模型中基于噪声的增强技术在泛化方面的益处和局限性提供了新的见解。

Abstract: Although prior work in computer vision has shown strong correlations between
in-distribution (ID) and out-of-distribution (OOD) accuracies, such
relationships remain underexplored in audio-based models. In this study, we
investigate how training conditions and input features affect the robustness
and generalization abilities of spoken keyword classifiers under OOD
conditions. We benchmark several neural architectures across a variety of
evaluation sets. To quantify the impact of noise on generalization, we make use
of two metrics: Fairness (F), which measures overall accuracy gains compared to
a baseline model, and Robustness (R), which assesses the convergence between ID
and OOD performance. Our results suggest that noise-aware training improves
robustness in some configurations. These findings shed new light on the
benefits and limitations of noise-based augmentation for generalization in
speech models.

</details>


### [174] [Observational Multiplicity](https://arxiv.org/abs/2507.23136)
*Erin George,Deanna Needell,Berk Ustun*

Main category: cs.LG

TL;DR: 本文研究概率分类任务中由于“观测多重性”导致的预测任意性，并提出“后悔度”来量化个体预测的稳定性。研究发现某些群体后悔度更高，并展示通过估算后悔度可提高实际应用中的安全性。


<details>
  <summary>Details</summary>
Motivation: 许多预测任务存在多个表现相似的模型，这会导致个体预测冲突，从而损害模型的可解释性和安全性。作者旨在探究在从二元观测数据预测概率的分类任务中，这种任意性（称之为“观测多重性”）是如何产生的。

Method: 提出并引入“后悔度”来评估个体概率预测的任意性。“后悔度”衡量模型预测因训练标签变化而改变的程度。同时，提出了一种通用方法来估计概率分类任务中的后悔度。

Result: 研究表明数据集中某些群体的后悔度更高。此外，通过估算后悔度，可以促进现实应用中的安全性，具体方法包括模型弃权（不给出预测）和进行针对性的数据收集。

Conclusion: 通过量化预测任意性，该工作提供了一种评估预测稳定性的工具。估算后悔度有助于识别预测不稳定的群体，并通过指导决策（如预测弃权和战略性数据收集）来提升模型在实际应用中的安全性和可靠性。

Abstract: Many prediction tasks can admit multiple models that can perform almost
equally well. This phenomenon can can undermine interpretability and safety
when competing models assign conflicting predictions to individuals. In this
work, we study how arbitrariness can arise in probabilistic classification
tasks as a result of an effect that we call \emph{observational multiplicity}.
We discuss how this effect arises in a broad class of practical applications
where we learn a classifier to predict probabilities $p_i \in [0,1]$ but are
given a dataset of observations $y_i \in \{0,1\}$. We propose to evaluate the
arbitrariness of individual probability predictions through the lens of
\emph{regret}. We introduce a measure of regret for probabilistic
classification tasks, which measures how the predictions of a model could
change as a result of different training labels change. We present a
general-purpose method to estimate the regret in a probabilistic classification
task. We use our measure to show that regret is higher for certain groups in
the dataset and discuss potential applications of regret. We demonstrate how
estimating regret promote safety in real-world applications by abstention and
data collection.

</details>


### [175] [AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver](https://arxiv.org/abs/2507.23141)
*Xiangshu Gong,Zhiqiang Xie,Xiaowei Jin,Chen Wang,Yanling Qu,Wangmeng Zuo,Hui Li*

Main category: cs.LG

TL;DR: 提出了一种解决微分方程（DEs）的新型AI范式，通过基于第一性原理的数据生成方法和融合尺度膨胀算子（SDO）的Transformer求解器，解决了数据稀缺和高频分量近似难题，并取得了卓越的精度。


<details>
  <summary>Details</summary>
Motivation: 许多问题受微分方程（DEs）控制，AI是解决DEs的新途径。然而，现有的AI求解器面临数据稀缺和高频分量近似（AHFC）的挑战。

Method: 提出的AI范式包含两个主要部分：1. **基于微分方程的第一性原理数据生成方法**：通过生成解并从DEs中推导源项和边界/初始条件，以极低的计算成本生成海量的、符合第一性原理的训练数据集。2. **尺度膨胀算子（SDO）AI求解器**：引入可逆SDO，利用多尺度解的傅里叶变换解决高频分量近似问题（AHFC）；设计了结合SDO的、时空耦合的、基于注意力机制的Transformer AI求解器；理论证明SDO能平滑损失函数景观，从而实现高效训练并解决AHFC问题。

Result: 在各种微分方程上的大量测试表明，该AI范式比现有最先进的方法取得了持续卓越的精度。

Conclusion: 这项工作使AI微分方程求解器在广泛的自然科学和工程领域中真正可用。

Abstract: Many problems are governed by differential equations (DEs). Artificial
intelligence (AI) is a new path for solving DEs. However, data is very scarce
and existing AI solvers struggle with approximation of high frequency
components (AHFC). We propose an AI paradigm for solving diverse DEs, including
DE-ruled first-principles data generation methodology and scale-dilation
operator (SDO) AI solver. Using either prior knowledge or random fields, we
generate solutions and then substitute them into the DEs to derive the sources
and initial/boundary conditions through balancing DEs, thus producing
arbitrarily vast amount of, first-principles-consistent training datasets at
extremely low computational cost. We introduce a reversible SDO that leverages
the Fourier transform of the multiscale solutions to fix AHFC, and design a
spatiotemporally coupled, attention-based Transformer AI solver of DEs with
SDO. An upper bound on the Hessian condition number of the loss function is
proven to be proportional to the squared 2-norm of the solution gradient,
revealing that SDO yields a smoother loss landscape, consequently fixing AHFC
with efficient training. Extensive tests on diverse DEs demonstrate that our AI
paradigm achieves consistently superior accuracy over state-of-the-art methods.
This work makes AI solver of DEs to be truly usable in broad nature and
engineering fields.

</details>


### [176] [FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations](https://arxiv.org/abs/2507.23154)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.LG

TL;DR: 该研究提出了FuseTen，一种新颖的生成框架，通过融合多源卫星数据，首次实现了10米空间分辨率的每日地表温度（LST）估算，有效解决了卫星LST数据在时空分辨率上的固有权衡问题，并显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 面对城市热浪、干旱和土地退化等气候变化带来的日益严峻的挑战，准确的空间-时间地表信息至关重要。地表温度（LST）是评估和理解这些现象的关键变量，但卫星平台在空间和时间分辨率之间存在固有的权衡，导致无法同时获得高时空分辨率的LST数据，这阻碍了对这些现象的精确研究。

Method: 本研究提出了FuseTen，一个生成式框架，旨在融合Sentinel-2、Landsat 8和Terra MODIS的观测数据，生成10米空间分辨率的每日LST。FuseTen采用基于物理原理的平均监督策略进行训练，其生成架构中整合了注意力机制和归一化模块，并使用PatchGAN判别器来确保生成结果的真实性。

Result: FuseTen在多个日期的实验中表现出色，定量指标平均提升32.06%，视觉保真度平均提升31.42%，均优于线性基线方法。据作者所知，这是首个能够生成如此精细空间分辨率（10米）每日LST估计的非线性方法。

Conclusion: FuseTen成功弥补了卫星LST数据在时空分辨率上的差距，首次通过非线性方法实现了10米空间分辨率的每日LST估算。这一突破为精确研究城市热浪、干旱和土地退化等气候变化相关挑战提供了关键的高分辨率地表温度信息，具有重要的应用价值。

Abstract: Urban heatwaves, droughts, and land degradation are pressing and growing
challenges in the context of climate change. A valuable approach to studying
them requires accurate spatio-temporal information on land surface conditions.
One of the most important variables for assessing and understanding these
phenomena is Land Surface Temperature (LST), which is derived from satellites
and provides essential information about the thermal state of the Earth's
surface. However, satellite platforms inherently face a trade-off between
spatial and temporal resolutions. To bridge this gap, we propose FuseTen, a
novel generative framework that produces daily LST observations at a fine 10 m
spatial resolution by fusing spatio-temporal observations derived from
Sentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative
architecture trained using an averaging-based supervision strategy grounded in
physical principles. It incorporates attention and normalization modules within
the fusion process and uses a PatchGAN discriminator to enforce realism.
Experiments across multiple dates show that FuseTen outperforms linear
baselines, with an average 32.06% improvement in quantitative metrics and
31.42% in visual fidelity. To the best of our knowledge, this is the first
non-linear method to generate daily LST estimates at such fine spatial
resolution.

</details>


### [177] [BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning](https://arxiv.org/abs/2507.23170)
*Jinan Zhou,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta,Aryan Singhal*

Main category: cs.LG

TL;DR: 本文揭示了LLM服务设计中推理预算、事实真实性和推理能力之间的内在权衡，并提出了一个名为BAR定理的原则性框架来指导LLM应用设计。


<details>
  <summary>Details</summary>
Motivation: LLM服务设计者在追求推理预算、事实真实性和推理能力这三个关键属性时面临挑战，现有模型无法同时优化所有三者。

Method: 通过分析LLM服务设计中的关键属性，并对其之间的权衡关系进行了形式化证明。

Result: 研究发现，任何LLM模型都无法同时优化推理预算、事实真实性和推理能力这三项关键属性。

Conclusion: 提出了名为BAR定理的原则性框架，旨在指导LLM应用设计，以应对推理预算、事实真实性和推理能力之间的权衡问题。

Abstract: When designing LLM services, practitioners care about three key properties:
inference-time budget, factual authenticity, and reasoning capacity. However,
our analysis shows that no model can simultaneously optimize for all three. We
formally prove this trade-off and propose a principled framework named The BAR
Theorem for LLM-application design.

</details>


### [178] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 提出NaN传播方法，通过利用IEEE 754 NaN的特性在黑盒函数中准确检测稀疏性，消除误报，从而加速梯度优化。


<details>
  <summary>Details</summary>
Motivation: 现有用于黑盒函数稀疏性检测的有限差分方法会因偶然的零梯度产生“假阴性”，导致梯度计算错误且难以诊断，这限制了基于雅可比矩阵压缩的梯度优化速度提升。

Method: 引入NaN传播方法，利用IEEE 754浮点数中NaN（非数字）值的普遍污染特性。通过系统地将NaN值注入函数输入，并观察哪些输出也变为NaN，来追踪并重建输入-输出依赖关系，从而构建出保守且无假阴性的稀疏模式。该方法无需修改现有黑盒代码，且能跨编程语言和数学库工作。研究还提出了利用NaN有效载荷编码等高级策略以实现超线性时间复杂度，并提供了处理分支代码执行的实用算法。

Result: 将该方法应用于一个航空航天机翼重量模型，成功实现了1.52倍的计算速度提升。同时，它检测出了传统方法遗漏的数十个依赖关系，证明了其在提高梯度计算（优化工作流中的常见瓶颈）准确性和效率方面的显著优势。

Conclusion: NaN传播是一种有效且无需修改代码的黑盒函数稀疏性检测方法，它成功克服了传统方法的假阴性问题，能够提供更准确的依赖关系图谱，从而显著加速梯度优化。该技术凭借其IEEE 754兼容性，实现了跨语言和库的工作能力，并且通过高级策略可实现超线性时间复杂度，在工程应用中具有高度实用价值。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [179] [Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2507.23217)
*Hyeon Seong Jeong,Sangwoo Jo,Byeong Hyun Yoon,Yoonseok Heo,Haedong Jeong,Taehoon Kim*

Main category: cs.LG

TL;DR: 提出DocsRay，一个免训练的多模态文档理解系统，结合伪目录生成与分层RAG，利用多模态LLM有效处理复杂文档，显著提升查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有复杂多模态文档理解因结构不一致和训练数据稀缺而面临挑战。

Method: DocsRay是一个免训练系统，融合了伪目录生成与分层检索增强生成（RAG）。它利用多模态大语言模型（LLM）的原生能力处理多样化文档元素（文本、图像、图表、表格）。核心方法包括：1) 基于提示的LLM语义结构化模块生成分层伪目录；2) 零样本多模态分析将文档元素转换为统一文本表示；3) 高效的两阶段分层检索系统以降低复杂度。

Result: 在平均49.4页、20,971个文本token的文档上，DocsRay将查询延迟从3.89秒降至2.12秒，效率提升45%。在MMLongBench-Doc基准测试中，DocsRay-Pro准确率达64.7%，大幅超越现有最佳水平。

Conclusion: DocsRay通过其独特的免训练架构，有效克服了多模态文档理解的难题，在效率和准确性方面均实现了显著提升，树立了新的性能标杆。

Abstract: Understanding complex multimodal documents remains challenging due to their
structural inconsistencies and limited training data availability. We introduce
\textit{DocsRay}, a training-free document understanding system that integrates
pseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented
Generation (RAG). Our approach leverages multimodal Large Language Models'
(LLMs) native capabilities to seamlessly process documents containing diverse
elements such as text, images, charts, and tables without requiring specialized
models or additional training. DocsRay's framework synergistically combines
three key techniques: (1) a semantic structuring module using prompt-based LLM
interactions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal
analysis that converts diverse document elements into unified, text-centric
representations using the inherent capabilities of multimodal LLMs, and (3) an
efficient two-stage hierarchical retrieval system that reduces retrieval
complexity from $O(N)$ to $O(S + k_1 \cdot N_s)$. Evaluated on documents
averaging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency
from 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the
MMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,
substantially surpassing previous state-of-the-art results.

</details>


### [180] [A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations](https://arxiv.org/abs/2507.23221)
*Charles O'Neill,Slava Chalnev,Chi Chi Zhao,Max Kirkby,Mudith Jayasekara*

Main category: cs.LG

TL;DR: 研究提出一种与生成器无关的观察模型，通过其残差流上的线性探针，能高效检测AI上下文幻觉。该方法表现优异，并发现幻觉信号源于模型内部MLP子电路，可用于幻觉的检测与干预。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型中“上下文幻觉”（即生成与给定上下文不符的语句）这一重大挑战。

Method: 构建了一个与生成器无关的观察模型，通过单次前向传播和在残差流上使用线性探针来检测幻觉。此探针能分离出区分幻觉与忠实文本的单一线性方向。利用梯度乘以激活的方法将信号定位到稀疏的后期层MLP活动中，并通过操纵该方向来验证其对生成器幻觉率的因果影响。同时发布了2000例ContraTales基准数据集。

Result: 幻觉检测性能超越基线5-27点，并在不同规模的Gemma-2模型中展现出稳健的中层表现。幻觉信号被定位在稀疏的、后期层MLP活动中。实验证明，操纵该方向能够因果地引导生成器的幻觉率。研究提供了关于内部低维幻觉追踪与特定MLP子电路关联的新证据。

Conclusion: 该研究提供了一种实用且可解释的方法来检测AI模型中的上下文幻觉。揭示了模型内部存在可用于检测和缓解幻觉的低维追踪机制，并与特定MLP子电路相关联。发布的新基准数据集将有助于未来解决方案的评估。

Abstract: Contextual hallucinations -- statements unsupported by given context --
remain a significant challenge in AI. We demonstrate a practical
interpretability insight: a generator-agnostic observer model detects
hallucinations via a single forward pass and a linear probe on its residual
stream. This probe isolates a single, transferable linear direction separating
hallucinated from faithful text, outperforming baselines by 5-27 points and
showing robust mid-layer performance across Gemma-2 models (2B to 27B).
Gradient-times-activation localises this signal to sparse, late-layer MLP
activity. Critically, manipulating this direction causally steers generator
hallucination rates, proving its actionability. Our results offer novel
evidence of internal, low-dimensional hallucination tracking linked to specific
MLP sub-circuits, exploitable for detection and mitigation. We release the
2000-example ContraTales benchmark for realistic assessment of such solutions.

</details>


### [181] [Efficient Machine Unlearning via Influence Approximation](https://arxiv.org/abs/2507.23257)
*Jiawei Liu,Chenwang Wu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 针对现有基于影响力的机器遗忘方法计算量大的问题，本文受认知科学启发，将遗忘（机器遗忘）与记忆（增量学习）关联起来，提出IAU算法，利用增量学习的视角高效实现机器遗忘，并在效率、遗忘保证和模型效用之间取得优异平衡。


<details>
  <summary>Details</summary>
Motivation: 由于日益增长的隐私担忧，机器遗忘受到关注。现有基于影响力的机器遗忘方法虽无需重新训练即可估计样本影响，但计算Hessian矩阵及其逆矩阵的开销巨大，在大规模模型和频繁数据删除场景下不切实际，这凸显了“遗忘”的困难。

Method: 受认知科学中“记忆比遗忘容易”的启发，本文建立了遗忘（机器遗忘）与记忆（增量学习）之间的理论联系，从而从增量学习的角度解决机器遗忘问题。与耗时的Hessian计算不同，增量学习通常依赖更高效的梯度优化。基于此连接，提出了影响力近似遗忘（Influence Approximation Unlearning, IAU）算法。

Result: 广泛的实证评估表明，IAU算法在遗忘保证、遗忘效率和可比模型效用之间取得了优越的平衡。在多样化的数据集和模型架构上，IAU的性能优于现有的最先进方法。

Conclusion: 本文成功地将机器遗忘问题与增量学习建立联系，提出IAU算法，有效解决了传统基于影响力的遗忘方法计算开销大的问题，实现了高效、可靠且实用的机器遗忘。

Abstract: Due to growing privacy concerns, machine unlearning, which aims at enabling
machine learning models to ``forget" specific training data, has received
increasing attention. Among existing methods, influence-based unlearning has
emerged as a prominent approach due to its ability to estimate the impact of
individual training samples on model parameters without retraining. However,
this approach suffers from prohibitive computational overhead arising from the
necessity to compute the Hessian matrix and its inverse across all training
samples and parameters, rendering it impractical for large-scale models and
scenarios involving frequent data deletion requests. This highlights the
difficulty of forgetting. Inspired by cognitive science, which suggests that
memorizing is easier than forgetting, this paper establishes a theoretical link
between memorizing (incremental learning) and forgetting (unlearning). This
connection allows machine unlearning to be addressed from the perspective of
incremental learning. Unlike the time-consuming Hessian computations in
unlearning (forgetting), incremental learning (memorizing) typically relies on
more efficient gradient optimization, which supports the aforementioned
cognitive theory. Based on this connection, we introduce the Influence
Approximation Unlearning (IAU) algorithm for efficient machine unlearning from
the incremental perspective. Extensive empirical evaluations demonstrate that
IAU achieves a superior balance among removal guarantee, unlearning efficiency,
and comparable model utility, while outperforming state-of-the-art methods
across diverse datasets and model architectures. Our code is available at
https://github.com/Lolo1222/IAU.

</details>


### [182] [DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System](https://arxiv.org/abs/2507.23261)
*Hui Yi Leong,Yuqing Wu*

Main category: cs.LG

TL;DR: 本文提出DynaSwarm框架，通过A2C强化学习优化和动态图选择器，为LLM多智能体系统实现自适应的动态协作图结构，并在多项任务中显著超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体系统（MAS）框架依赖于手动设计和静态的协作图结构，这限制了其适应性和性能，无法充分利用LLM的能力。

Method: 本文提出DynaSwarm，一个增强LLM多智能体系统的动态框架，主要包含两项创新：1) 采用Actor-Critic (A2C) 强化学习机制优化图结构，提升稳定性；2) 设计动态图选择器，通过参数高效的LLM微调，为每个输入样本自适应地选择最佳图结构。此外，还通过微调演示检索器来充分利用上下文学习（ICL）的能力。

Result: 在问答、数学推理和编码任务上的广泛实验表明，DynaSwarm在多个LLM骨干网络上持续优于最先进的单智能体和多智能体系统基线。

Conclusion: 研究结果强调了在LLM多智能体系统设计中，样本感知的结构灵活性至关重要，证明了动态协作图对于提升系统性能的有效性。

Abstract: Current multi-agent systems (MAS) frameworks often rely on manually designed
and static collaboration graph structures, limiting adaptability and
performance. To address these limitations, we propose DynaSwarm, a dynamic
framework that enhances LLM-based MAS through two key innovations: (1) an
actor-critic reinforcement learning (A2C) mechanism to optimize graph
structures with improved stability over prior RL methods, and (2) a dynamic
graph selector that adaptively chooses the optimal graph structure for each
input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the
need for rigid, one-fits-all graph architectures, instead leveraging
sample-specific idiosyncrasies to dynamically route queries through specialized
agent networks. (c) We propose to fine-tune the demonstration retriever to
fully exploit the power of in-context learning (ICL). Extensive experiments on
question answering, mathematical reasoning, and coding tasks demonstrate that
DynaSwarm consistently outperforms state-of-the-art single-agent and MAS
baselines across multiple LLM backbones. Our findings highlight the importance
of sample-aware structural flexibility in LLM MAS designs.

</details>


### [183] [Evaluating the Dynamics of Membership Privacy in Deep Learning](https://arxiv.org/abs/2507.23291)
*Yuetian Chen,Zhiqi Wang,Nathalie Baracaldo,Swanand Ravindra Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 本文提出一个动态分析框架，量化研究深度学习训练中个体样本的隐私泄露动态，发现样本学习难度与隐私风险相关，且高风险样本的隐私泄露在训练早期就已确定。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击（MIAs）严重威胁深度学习训练数据的隐私，但目前对模型何时以及如何编码成员信息的研究仍有限。

Method: 开发了一个动态分析框架，通过在FPR-TPR平面上追踪训练过程中每个样本的脆弱性，系统性地衡量数据集复杂度、模型架构和优化器选择等因素如何影响样本泄露的速度和程度。

Result: 发现样本的内在学习难度与隐私风险之间存在显著相关性，且最终模型中高风险样本的隐私风险在训练早期就已基本确定。

Conclusion: 深入理解了训练过程中隐私风险的动态产生机制，为制定主动的、隐私保护的深度学习模型训练策略奠定了基础。

Abstract: Membership inference attacks (MIAs) pose a critical threat to the privacy of
training data in deep learning. Despite significant progress in attack
methodologies, our understanding of when and how models encode membership
information during training remains limited. This paper presents a dynamic
analytical framework for dissecting and quantifying privacy leakage dynamics at
the individual sample level. By tracking per-sample vulnerabilities on an
FPR-TPR plane throughout training, our framework systematically measures how
factors such as dataset complexity, model architecture, and optimizer choice
influence the rate and severity at which samples become vulnerable. Crucially,
we discover a robust correlation between a sample's intrinsic learning
difficulty, and find that the privacy risk of samples highly vulnerable in the
final trained model is largely determined early during training. Our results
thus provide a deeper understanding of how privacy risks dynamically emerge
during training, laying the groundwork for proactive, privacy-aware model
training strategies.

</details>


### [184] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍一个名为SequenceLayers的神经网络层API和库，旨在简化序列模型构建，使其能同时支持逐层（如教师强制训练）和逐步（如自回归采样）执行模式。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型在处理逐层训练和逐步采样等不同执行模式时存在复杂性，且易引发常见错误。研究旨在提供一个通用、易用且正确性强的解决方案，以支持模型的可流式处理和生产级部署。

Method: 核心方法是要求神经网络层明确定义其随时间演变的状态（例如Transformer的KV缓存、RNN的隐藏状态），并提供一个`step`方法来演进该状态。这确保了有状态的逐步执行与无状态的逐层执行结果一致。该API设计为可组合和声明式。

Result: SequenceLayers使得复杂模型能够立即实现流式处理，有效减少了流式和并行序列处理中常见的错误，并且可以在任何深度学习库中实现。其API和丰富的组件库简化了从简单可流式组件构建生产规模模型的过程，同时保持了强大的正确性保证。

Conclusion: SequenceLayers通过引入显式状态管理和`step`方法，提供了一个通用的神经网络层API和库，极大地简化了序列模型的创建和部署，确保了模型在训练和推理（特别是自回归采样）两种模式下的高效与正确性，并支持生产级应用。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>


### [185] [An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items](https://arxiv.org/abs/2507.23303)
*Luca Corbucci,Javier Alejandro Borges Legrottaglie,Francesco Spinnato,Anna Monreale,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 本文定义并解决了超市购物中“遗忘商品预测”这一未充分探索的问题，提出两种新颖的可解释算法，性能显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有下一购物篮预测（NBP）方法普遍缺乏对无意遗漏商品的检测，且多采用黑盒模型，缺乏透明度和解释性，同时缺乏相关真实世界数据集。

Method: 正式引入“遗忘商品预测”任务，并提出两种设计上就具备可解释性的新型算法，旨在识别遗忘商品并提供直观易懂的解释。

Result: 在真实零售数据集上的实验表明，所提出的算法在多项评估指标上比现有最先进的NBP基线模型表现提升10-15%。

Conclusion: 本研究通过引入遗忘商品预测任务和开发可解释算法，不仅解决了NBP领域的一个重要空白，还显著提升了预测准确性和用户对推荐的理解度。

Abstract: Accurately identifying items forgotten during a supermarket visit and
providing clear, interpretable explanations for recommending them remains an
underexplored problem within the Next Basket Prediction (NBP) domain. Existing
NBP approaches typically only focus on forecasting future purchases, without
explicitly addressing the detection of unintentionally omitted items. This gap
is partly due to the scarcity of real-world datasets that allow for the
reliable estimation of forgotten items. Furthermore, most current NBP methods
rely on black-box models, which lack transparency and limit the ability to
justify recommendations to end users. In this paper, we formally introduce the
forgotten item prediction task and propose two novel interpretable-by-design
algorithms. These methods are tailored to identify forgotten items while
offering intuitive, human-understandable explanations. Experiments on a
real-world retail dataset show our algorithms outperform state-of-the-art NBP
baselines by 10-15% across multiple evaluation metrics.

</details>


### [186] [Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner](https://arxiv.org/abs/2507.23317)
*Tao He,Rongchuan Mu,Lizi Liao,Yixin Cao,Ming Liu,Bing Qin*

Main category: cs.LG

TL;DR: 本研究通过引入思维级别过程奖励模型（PRMs）和能力自适应奖励机制，显著加速了大型推理模型（LRMs）在数学推理任务中的强化学习训练效率和问题解决准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习（RL）方法在优化大型推理模型（LRMs）解决复杂数学问题时，依赖于稀疏的最终结果奖励，这导致训练过程效率低下。

Method: 本研究提出了一种新颖的、由内在信号驱动的生成过程评估机制，该机制在“思维”层面运作。具体而言，它利用解决方案中的内在信号判断每一步的正确性，并将连续的正确/错误步骤聚合成连贯的“思维”单元，从而提供结构化的思维级别奖励，以实现更可靠的信用分配并缓解奖励欺骗。此外，引入了一种能力自适应奖励机制，根据LRM当前的熟练程度动态平衡探索与利用。这些创新被集成到一个新的离策略强化学习算法TP-GRPO中，该算法扩展了分组近端优化（grouped proximal optimization）并使用了基于过程的奖励。

Result: 在1.5B和7B参数的LRM上进行的实验表明，与仅依赖结果奖励的基线相比，所提出的方法以显著更少的训练样本实现了更高的数学问题解决准确性。

Conclusion: 结构良好的过程奖励能够显著加速大型推理模型在数学推理任务中的优化过程，并提高其性能。

Abstract: Large reasoning models (LRMs) have recently shown promise in solving complex
math problems when optimized with Reinforcement Learning (RL). But conventional
approaches rely on outcome-only rewards that provide sparse feedback, resulting
in inefficient optimization process. In this work, we investigate the function
of process reward models (PRMs) to accelerate the RL training for LRMs. We
propose a novel intrinsic signal-driven generative process evaluation mechanism
operating at the thought level to address major bottlenecks in RL-based
training. Specifically, instead of requiring PRMs to know how to solve
problems, our method uses intrinsic signals in solutions to judge stepwise
correctness and aggregate contiguous correct/incorrect steps into coherent
'thought' units. This structured, thought-level rewards enable more reliable
credit assignment by reducing ambiguity in step segmentation and alleviating
reward hacking. We further introduce a capability-adaptive reward mechanism
that dynamically balances exploration and exploitation based on the LRM's
current proficiency, guiding learning without stifling creative
trial-and-error. These innovations are integrated into a new off-policy RL
algorithm, TP-GRPO, which extends grouped proximal optimization with
process-based rewards and improves training efficiency. Experiments on 1.5B and
7B parameter LRMs demonstrate that our method achieves higher problem-solving
accuracy with significantly fewer training samples than outcome-only reward
baselines. The results validate that well-structured process rewards can
substantially accelerate LRM optimization in math reasoning tasks. Code is
available at https://github.com/cs-holder/tp_grpo.

</details>


### [187] [Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions](https://arxiv.org/abs/2507.23335)
*Qilin Zhou,Haipeng Wang,Zhengyuan Wei,W. K. Chan*

Main category: cs.LG

TL;DR: CostCert是一种新颖、可扩展、精确的投票式认证恢复防御方法，解决了现有技术在top-k预测中认证真标签时，因攻击预算膨胀和组合爆炸导致的失败问题，并通过检查攻击预算是否不足以将真标签从top-k中排除来实现，实验证明其性能远超现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有用于深度学习系统对抗性补丁攻击的补丁鲁棒性认证恢复技术，在应用于top-k预测时存在局限。它们通常通过标签间的成对比较来认证，但受攻击者可控制的票数（攻击预算）膨胀影响，无法精确认证top-k预测中的唯一真标签；同时，枚举所有投票分配组合会导致组合爆炸问题。

Method: 本文提出了CostCert，一种新颖、可扩展、精确的基于投票的认证恢复防御器。CostCert通过一种独特的设计，避免了成对比较和组合爆炸。其核心思想是验证攻击样本上的攻击预算是否不足以覆盖将真标签从top-k预测中排除所需的最小额外票数（在攻击者不可控票数的基础上）。

Result: 实验结果表明，CostCert显著优于现有最先进的防御器PatchGuard。例如，当补丁大小为96时，CostCert仍能保持高达57.3%的认证准确率，而PatchGuard已降至零。

Conclusion: CostCert成功提供了一种高效、精确的投票式认证恢复方法，解决了现有技术在top-k预测认证中的挑战，显著提升了深度学习系统对抗对抗性补丁攻击的鲁棒性认证能力。

Abstract: Patch robustness certification is an emerging verification approach for
defending against adversarial patch attacks with provable guarantees for deep
learning systems. Certified recovery techniques guarantee the prediction of the
sole true label of a certified sample. However, existing techniques, if
applicable to top-k predictions, commonly conduct pairwise comparisons on those
votes between labels, failing to certify the sole true label within the top k
prediction labels precisely due to the inflation on the number of votes
controlled by the attacker (i.e., attack budget); yet enumerating all
combinations of vote allocation suffers from the combinatorial explosion
problem. We propose CostCert, a novel, scalable, and precise voting-based
certified recovery defender. CostCert verifies the true label of a sample
within the top k predictions without pairwise comparisons and combinatorial
explosion through a novel design: whether the attack budget on the sample is
infeasible to cover the smallest total additional votes on top of the votes
uncontrollable by the attacker to exclude the true labels from the top k
prediction labels. Experiments show that CostCert significantly outperforms the
current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in
certified accuracy when the patch size is 96, whereas PatchGuard has already
dropped to zero.

</details>


### [188] [Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation](https://arxiv.org/abs/2507.23344)
*Tatsuya Mitomi,Fumiyasu Makinoshima,Fumiya Makihara,Eigo Segawa*

Main category: cs.LG

TL;DR: 本文提出一种可微分的代理仿真模型，用于快速设计共享单车系统的动态定价策略，以应对用户需求的时空差异和概率性选择，实现库存平衡并降低调运成本，并验证了其在准确性和收敛速度上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统因用户需求时空差异导致站点库存不平衡，产生额外调运成本。设计最优动态定价策略至关重要，但面临用户背景多样和选择概率化带来的挑战。

Method: 开发了一种可微分的基于代理的仿真模型，用于快速设计共享单车系统的动态定价策略，以在用户行程异质性和概率决策下实现自行车库存平衡。

Result: 数值实验表明，该方法比传统方法更准确（损失降低73%-78%），收敛速度提高100倍以上。在大规模城市系统验证中，所获定价策略能自然诱导库存平衡，无需人工调运。此外，通过设定适当的初始条件，可最小化诱导库存平衡所需的折扣成本。

Conclusion: 所提出的可微分代理仿真方法能有效解决共享单车系统的动态定价问题，实现库存平衡，显著提高定价设计的效率和准确性，并为优化运营成本提供了新途径。

Abstract: Bike-sharing systems are emerging in various cities as a new ecofriendly
transportation system. In these systems, spatiotemporally varying user demands
lead to imbalanced inventory at bicycle stations, resulting in additional
relocation costs. Therefore, it is essential to manage user demand through
optimal dynamic pricing for the system. However, optimal pricing design for
such a system is challenging because the system involves users with diverse
backgrounds and their probabilistic choices. To address this problem, we
develop a differentiable agent-based simulation to rapidly design dynamic
pricing in bike-sharing systems, achieving balanced bicycle inventory despite
spatiotemporally heterogeneous trips and probabilistic user decisions. We first
validate our approach against conventional methods through numerical
experiments involving 25 bicycle stations and five time slots, yielding 100
parameters. Compared to the conventional methods, our approach obtains a more
accurate solution with a 73% to 78% reduction in loss while achieving more than
a 100-fold increase in convergence speed. We further validate our approach on a
large-scale urban bike-sharing system scenario involving 289 bicycle stations,
resulting in a total of 1156 parameters. Through simulations using the obtained
pricing policies, we confirm that these policies can naturally induce balanced
inventory without any manual relocation. Additionally, we find that the cost of
discounts to induce the balanced inventory can be minimized by setting
appropriate initial conditions.

</details>


### [189] [Causal Explanation of Concept Drift -- A Truly Actionable Approach](https://arxiv.org/abs/2507.23389)
*David Komnick,Kathrin Lammers,Barbara Hammer,Valerie Vaquet,Fabian Hinder*

Main category: cs.LG

TL;DR: 本文通过将模型漂移解释扩展到因果解释，旨在提升解释的可操作性，从而实现对系统变化的针对性干预。


<details>
  <summary>Details</summary>
Motivation: 在不断变化的工业制造和关键基础设施等系统中，理解和解释概念漂移（critical changes）至关重要，这有助于避免模型失效和物理世界的故障。然而，现有的解释方法可能缺乏直接的可操作性。

Method: 将基于模型的漂移解释扩展为因果解释，以提高所提供解释的可操作性。该框架能够隔离受概念漂移影响的因果相关特征。

Result: 通过在多个用例中评估，证明了所提出的解释策略的实用性，能够识别受概念漂移影响的因果相关特征。

Conclusion: 该框架能够识别并隔离受概念漂移影响的因果相关特征，从而实现有针对性的干预。

Abstract: In a world that constantly changes, it is crucial to understand how those
changes impact different systems, such as industrial manufacturing or critical
infrastructure. Explaining critical changes, referred to as concept drift in
the field of machine learning, is the first step towards enabling targeted
interventions to avoid or correct model failures, as well as malfunctions and
errors in the physical world. Therefore, in this work, we extend model-based
drift explanations towards causal explanations, which increases the
actionability of the provided explanations. We evaluate our explanation
strategy on a number of use cases, demonstrating the practical usefulness of
our framework, which isolates the causally relevant features impacted by
concept drift and, thus, allows for targeted intervention.

</details>


### [190] [Policy Learning from Large Vision-Language Model Feedback without Reward Modeling](https://arxiv.org/abs/2507.23391)
*Tung M. Luu,Donghoon Lee,Younghwan Lee,Chang D. Yoo*

Main category: cs.LG

TL;DR: PLARE是一种新型的离线强化学习方法，它利用大型视觉-语言模型（VLM）从语言任务描述中获取偏好标签来训练机器人，避免了手动设计奖励函数的需求，并在多种机器人操作任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习算法需要奖励标签数据，但奖励函数的设计成本高、耗时且需要大量领域专业知识，这成为了实际应用中的一个主要瓶颈。

Method: 本文提出PLARE方法，利用大型视觉-语言模型（VLM）作为指导信号。PLARE不依赖手动设计的奖励函数，而是通过VLM根据语言任务描述查询视觉轨迹片段对的偏好标签。随后，策略直接通过监督对比偏好学习目标，利用这些偏好标签进行训练，从而绕过了学习显式奖励模型的需要。

Result: PLARE在MetaWorld的机器人操作任务上，表现与现有最先进的基于VLM的奖励生成方法相当或更优。此外，PLARE在物理机器人的真实世界操作任务中也展现了其有效性和实用性。

Conclusion: PLARE通过创新的偏好学习方式，成功解决了离线强化学习中奖励函数设计的瓶颈问题，并在机器人操作任务中取得了卓越性能，证明了其在实际应用中的强大潜力和有效性。

Abstract: Offline reinforcement learning (RL) provides a powerful framework for
training robotic agents using pre-collected, suboptimal datasets, eliminating
the need for costly, time-consuming, and potentially hazardous online
interactions. This is particularly useful in safety-critical real-world
applications, where online data collection is expensive and impractical.
However, existing offline RL algorithms typically require reward labeled data,
which introduces an additional bottleneck: reward function design is itself
costly, labor-intensive, and requires significant domain expertise. In this
paper, we introduce PLARE, a novel approach that leverages large
vision-language models (VLMs) to provide guidance signals for agent training.
Instead of relying on manually designed reward functions, PLARE queries a VLM
for preference labels on pairs of visual trajectory segments based on a
language task description. The policy is then trained directly from these
preference labels using a supervised contrastive preference learning objective,
bypassing the need to learn explicit reward models. Through extensive
experiments on robotic manipulation tasks from the MetaWorld, PLARE achieves
performance on par with or surpassing existing state-of-the-art VLM-based
reward generation methods. Furthermore, we demonstrate the effectiveness of
PLARE in real-world manipulation tasks with a physical robot, further
validating its practical applicability.

</details>


### [191] [A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles](https://arxiv.org/abs/2507.23412)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.LG

TL;DR: 该研究开发了一种基于机器学习的系统，利用蜂蜜矿物元素分析检测掺假，其中随机森林模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种基于机器学习（ML）的系统，利用蜂蜜矿物元素配置文件来检测蜂蜜掺假。

Method: 该系统包括预处理（缺失值处理和归一化）和分类两个阶段。分类阶段使用了逻辑回归、决策树和随机森林三种监督学习模型。研究使用公共数据集评估了这些ML模型的性能。

Result: 实验结果表明，蜂蜜中的矿物元素含量为检测蜂蜜掺假提供了强大的判别信息。随机森林分类器在该数据集上表现最佳，实现了98.37%的最高交叉验证准确率。

Conclusion: 蜂蜜的矿物元素含量是检测掺假的有效特征，且基于随机森林的分类器在该任务中表现出卓越的性能。

Abstract: This paper aims to develop a Machine Learning (ML)-based system for detecting
honey adulteration utilizing honey mineral element profiles. The proposed
system comprises two phases: preprocessing and classification. The
preprocessing phase involves the treatment of missing-value attributes and
normalization. In the classifica-tion phase, we use three supervised ML models:
logistic regression, decision tree, and random forest, to dis-criminate between
authentic and adulterated honey. To evaluate the performance of the ML models,
we use a public dataset comprising measurements of mineral element content of
authentic honey, sugar syrups, and adul-terated honey. Experimental findings
show that mineral element content in honey provides robust discriminative
information for detecting honey adulteration. Results also demonstrate that the
random forest-based classifier outperforms other classifiers on this dataset,
achieving the highest cross-validation accuracy of 98.37%.

</details>


### [192] [Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning](https://arxiv.org/abs/2507.23418)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.LG

TL;DR: 该论文提出一个基于红外光谱和机器学习的系统，用于检测椰奶掺假，通过预处理、LDA特征提取和KNN分类，达到了93.33%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 检测椰奶中的掺假行为是必要的，该研究旨在提出一种利用红外光谱的有效检测系统。

Method: 该系统是一个基于机器学习的三阶段流程：1. 预处理，去除光谱信号中的无关数据；2. 特征提取，使用线性判别分析（LDA）提取区分性特征；3. 分类，使用K-近邻（KNN）模型将椰奶样本分为真品或掺假品。系统性能通过包含纯净和受污染椰奶样本的傅立叶变换红外（FTIR）光谱公共数据集进行评估。

Result: 研究结果显示，所提出的方法成功检测出掺假，交叉验证准确率达到93.33%。

Conclusion: 所提出的基于红外光谱和机器学习的系统能够有效检测椰奶掺假，表现出高准确性。

Abstract: In this paper, we propose a system for detecting adulteration in coconut
milk, utilizing infrared spectroscopy. The machine learning-based proposed
system comprises three phases: preprocessing, feature extraction, and
classification. The first phase involves removing irrelevant data from coconut
milk spectral signals. In the second phase, we employ the Linear Discriminant
Analysis (LDA) algorithm for extracting the most discriminating features. In
the third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut
milk samples into authentic or adulterated. We evaluate the performance of the
proposed system using a public dataset comprising Fourier Transform Infrared
(FTIR) spectral information of pure and contaminated coconut milk samples.
Findings show that the proposed method successfully detects adulteration with a
cross-validation accuracy of 93.33%.

</details>


### [193] [Merging Memory and Space: A Spatiotemporal State Space Neural Operator](https://arxiv.org/abs/2507.23428)
*Nodens F. Koren,Samuel Lanthaler*

Main category: cs.LG

TL;DR: 本文提出ST-SSM，一种紧凑的神经网络算子架构，通过时空维度因子分解和状态空间模型，高效学习时变偏微分方程的解算子，并在理论和实践中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在学习时变偏微分方程（PDEs）的解算子时，可能存在参数效率低或难以灵活建模长程时空动力学的问题。本研究旨在提出一种更紧凑、高效且能处理复杂时空动态的模型。

Method: 提出了时空状态空间神经网络算子（ST-SSM），其核心是利用结构化状态空间模型对时空维度进行新颖的因子分解，从而独立建模时间演化和空间交互。此外，研究建立了SSM与神经算子之间的理论联系，并证明了通用性定理。

Result: 实验结果表明，ST-SSM在多个PDE基准测试（包括1D Burgers方程、1D Kuramoto-Sivashinsky方程和2D Navier-Stokes方程）上，其因子分解公式优于之字形扫描和并行独立处理等替代方案。该模型在参数显著减少的情况下，性能与现有基线相当，并在部分可观察性条件下，通过展示改进的性能，强化了时间记忆的益处。

Conclusion: 本研究突出了维度因子分解算子学习在高效且可推广的PDE建模中的优势，并为该方法奠定了坚实的理论基础。

Abstract: We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact
architecture for learning solution operators of time-dependent partial
differential equations (PDEs). ST-SSM introduces a novel factorization of the
spatial and temporal dimensions, using structured state-space models to
independently model temporal evolution and spatial interactions. This design
enables parameter efficiency and flexible modeling of long-range spatiotemporal
dynamics. A theoretical connection is established between SSMs and neural
operators, and a unified universality theorem is proved for the resulting class
of architectures. Empirically, we demonstrate that our factorized formulation
outperforms alternative schemes such as zigzag scanning and parallel
independent processing on several PDE benchmarks, including 1D Burgers'
equation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations
under varying physical conditions. Our model performs competitively with
existing baselines while using significantly fewer parameters. In addition, our
results reinforce previous findings on the benefits of temporal memory by
showing improved performance under partial observability. Our results highlight
the advantages of dimensionally factorized operator learning for efficient and
generalizable PDE modeling, and put this approach on a firm theoretical
footing.

</details>


### [194] [Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design](https://arxiv.org/abs/2507.23437)
*Yinhui Ma,Tomomasa Yamasaki,Zhehui Wang,Tao Luo,Bo Wang*

Main category: cs.LG

TL;DR: 硬件感知神经网络架构搜索（HW-NAS）面临计算成本高和搜索空间大的挑战。本文提出Coflex框架，通过结合稀疏高斯过程和多目标贝叶斯优化，显著降低计算复杂度，同时提升了网络性能和能效，实现了1.9x至9.5x的加速。


<details>
  <summary>Details</summary>
Motivation: 硬件感知神经网络架构搜索（HW-NAS）在边缘设备深度神经网络加速器开发中具有重要价值，但其庞大的搜索空间和高昂的计算成本严重阻碍了其实际应用。

Method: 本文提出Coflex，一个新颖的HW-NAS框架，它整合了稀疏高斯过程（SGP）和多目标贝叶斯优化。通过利用稀疏诱导点，Coflex将高斯过程（GP）核的复杂度从立方级降至近线性级，从而实现大规模搜索空间的可扩展近似，显著降低了计算开销，同时保持了高预测精度。

Result: 实验结果表明，Coflex在网络精度和能量-延迟积（Energy-Delay-Product）方面均优于现有最先进方法，并且实现了1.9倍至9.5倍的计算速度提升。

Conclusion: Coflex有效解决了HW-NAS面临的计算效率和搜索空间挑战，通过创新的优化方法，显著提升了搜索效率和最终模型的性能，使其成为边缘AI加速器开发的有力工具。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach
to automatically co-optimizing neural network performance and hardware energy
efficiency, making it particularly useful for the development of Deep Neural
Network accelerators on the edge. However, the extensive search space and high
computational cost pose significant challenges to its practical adoption. To
address these limitations, we propose Coflex, a novel HW-NAS framework that
integrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian
optimization. By leveraging sparse inducing points, Coflex reduces the GP
kernel complexity from cubic to near-linear with respect to the number of
training samples, without compromising optimization performance. This enables
scalable approximation of large-scale search space, substantially decreasing
computational overhead while preserving high predictive accuracy. We evaluate
the efficacy of Coflex across various benchmarks, focusing on
accelerator-specific architecture. Our experi- mental results show that Coflex
outperforms state-of-the-art methods in terms of network accuracy and
Energy-Delay-Product, while achieving a computational speed-up ranging from
1.9x to 9.5x.

</details>


### [195] [Manifold-regularised Signature Kernel Large-Margin $\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection](https://arxiv.org/abs/2507.23449)
*Shervin Rahimzadeh Arashloo*

Main category: cs.LG

TL;DR: 本文提出了一种流形正则化和签名核表示的广义大裕度$\ell_p$-SVDD方法，用于时间序列异常检测。


<details>
  <summary>Details</summary>
Motivation: 为了利用数据分布的几何结构和捕获时间序列的复杂性，以改进时间序列异常检测性能，研究人员概括了现有的大裕度$\ell_p$-SVDD方法。

Method: 该研究通过流形正则化，鼓励底层流形上的标签平滑性，并结合签名核表示来捕获时间序列复杂性。它基于一个现有的表示定理，提供了一种有效的优化技术，并使用Rademacher复杂度从理论上分析了泛化性能，同时通过实验评估了该方法在各种数据集上的表现。

Result: 研究表明，所提出的方法能够受益于签名核来捕获时间序列的复杂性。理论分析揭示了其泛化性能，实验评估也验证了其相较于其他方法的表现。

Conclusion: 该广义的大裕度$\ell_p$-SVDD方法（结合流形正则化和签名核）为时间序列异常检测提供了一种有效且泛化性能良好的新范式。

Abstract: We generalise the recently introduced large-margin $\ell_p$-SVDD approach to
exploit the geometry of data distribution via manifold regularising and a
signature kernel representation for time series anomaly detection.
Specifically, we formulate a manifold-regularised variant of the $\ell_p$-SVDD
method to encourage label smoothness on the underlying manifold to capture
structural information for improved detection performance. Drawing on an
existing Representer theorem, we then provide an effective optimisation
technique for the proposed method and show that it can benefit from the
signature kernel to capture time series complexities for anomaly detection.
  We theoretically study the proposed approach using Rademacher complexities to
analyse its generalisation performance and also provide an experimental
assessment of the proposed method across various data sets to compare its
performance against other methods.

</details>


### [196] [Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus](https://arxiv.org/abs/2507.23491)
*Olga Vershinina,Jacopo Sabbatinelli,Anna Rita Bonfigli,Dalila Colombaretti,Angelica Giuliani,Mikhail Krivonosov,Arseniy Trukhanov,Claudio Franceschi,Mikhail Ivanchenko,Fabiola Olivieri*

Main category: cs.LG

TL;DR: 本研究利用机器学习模型预测2型糖尿病患者的全因死亡风险，其中EST模型表现最佳且可解释性强，有助于临床应用。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病是一种常见的慢性疾病，显著降低预期寿命。准确评估患者的全因死亡风险对于个性化和优化治疗策略至关重要。

Method: 分析了一个包含554名2型糖尿病患者的队列数据，随访期最长16.8年（202名患者死亡）。识别了与生存相关的关键特征，并训练和验证了多个机器学习模型以预测全因死亡风险。使用SHAP方法解释最佳模型的决策过程。

Result: Extra Survival Trees (EST) 模型（包含十个关键特征）表现出最佳预测性能，C-统计量为0.776，5年、10年、15年和16.8年全因死亡预测的AUC值分别为0.86、0.80、0.841和0.826。SHAP方法用于解释模型的个体决策过程。

Conclusion: 所开发的模型在死亡风险评估方面表现出强大的预测性能，其临床可解释的输出能够潜在地应用于临床实践，有助于识别高风险患者并支持及时优化治疗。

Abstract: Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent
non-communicable chronic disease that substantially reduces life expectancy.
Accurate estimation of all-cause mortality risk in T2DM patients is crucial for
personalizing and optimizing treatment strategies. Research Design and Methods.
This study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed
T2DM over a maximum follow-up period of 16.8 years, during which 202 patients
(36%) died. Key survival-associated features were identified, and multiple
machine learning (ML) models were trained and validated to predict all-cause
mortality risk. To improve model interpretability, Shapley additive
explanations (SHAP) was applied to the best-performing model. Results. The
extra survival trees (EST) model, incorporating ten key features, demonstrated
the best predictive performance. The model achieved a C-statistic of 0.776,
with the area under the receiver operating characteristic curve (AUC) values of
0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause
mortality predictions, respectively. The SHAP approach was employed to
interpret the model's individual decision-making processes. Conclusions. The
developed model exhibited strong predictive performance for mortality risk
assessment. Its clinically interpretable outputs enable potential bedside
application, improving the identification of high-risk patients and supporting
timely treatment optimization.

</details>


### [197] [Incorporating structural uncertainty in causal decision making](https://arxiv.org/abs/2507.23495)
*Maurits Kaptein*

Main category: cs.LG

TL;DR: 分析了因果效应中结构不确定性何时需要贝叶斯模型平均法，并指出其在特定条件下的益处，证明了其优化性。


<details>
  <summary>Details</summary>
Motivation: 实践中常忽略因果效应中的结构不确定性，本研究旨在确定何时这种不确定性足够重要以需要专门的方法解决。

Method: 采用贝叶斯模型平均法来处理竞争性因果结构（例如X→Y vs. X←Y）的不确定性，通过理论证明其优化性，并通过模拟验证现代因果发现方法的量化能力。

Result: 贝叶斯模型平均法在以下情况中有效：结构不确定性中度到高，不同结构间因果效应差异显著，以及损失函数对因果效应大小足够敏感。证明了方法的优化性，并通过模拟表明现代因果发现方法可在限制内提供所需量化。

Conclusion: 本框架通过解决实践中常被忽视的独特结构不确定性来源，补充了现有的稳健因果推断方法。

Abstract: Practitioners making decisions based on causal effects typically ignore
structural uncertainty. We analyze when this uncertainty is consequential
enough to warrant methodological solutions (Bayesian model averaging over
competing causal structures). Focusing on bivariate relationships ($X
\rightarrow Y$ vs. $X \leftarrow Y$), we establish that model averaging is
beneficial when: (1) structural uncertainty is moderate to high, (2) causal
effects differ substantially between structures, and (3) loss functions are
sufficiently sensitive to the size of the causal effect. We prove optimality
results of our suggested methodological solution under regularity conditions
and demonstrate through simulations that modern causal discovery methods can
provide, within limits, the necessary quantification. Our framework complements
existing robust causal inference approaches by addressing a distinct source of
uncertainty typically overlooked in practice.

</details>


### [198] [Directional Ensemble Aggregation for Actor-Critics](https://arxiv.org/abs/2507.23501)
*Nicklas Werge,Yi-Shan Wu,Bahareh Tasdighi,Melih Kandemir*

Main category: cs.LG

TL;DR: DEA是一种自适应Q值聚合方法，在Actor-Critic框架中通过学习方向性参数调节保守性和探索性，表现优于静态集成策略。


<details>
  <summary>Details</summary>
Motivation: 离策略强化学习中Q值估计的准确性至关重要。现有保守聚合方法（如取最小值）是静态的，丢弃信息且不具适应性，无法满足任务或学习阶段需求。

Method: 提出方向集成聚合（DEA）方法，自适应结合Q值估计。引入两个可学习方向参数：一个调节批评家保守性，一个指导行动者策略探索。参数通过“集成不一致加权Bellman误差”学习，根据Bellman误差方向加权样本，实现数据驱动的保守性与探索性调整。

Result: 在连续控制基准测试和不同学习范式下（从交互式到样本高效）评估了DEA，证明其比静态集成策略更有效。

Conclusion: DEA通过数据驱动的自适应聚合机制，成功解决了静态Q值聚合的局限性，提升了连续控制任务中的性能。

Abstract: Off-policy reinforcement learning in continuous control tasks depends
critically on accurate $Q$-value estimates. Conservative aggregation over
ensembles, such as taking the minimum, is commonly used to mitigate
overestimation bias. However, these static rules are coarse, discard valuable
information from the ensemble, and cannot adapt to task-specific needs or
different learning regimes. We propose Directional Ensemble Aggregation (DEA),
an aggregation method that adaptively combines $Q$-value estimates in
actor-critic frameworks. DEA introduces two fully learnable directional
parameters: one that modulates critic-side conservatism and another that guides
actor-side policy exploration. Both parameters are learned using ensemble
disagreement-weighted Bellman errors, which weight each sample solely by the
direction of its Bellman error. This directional learning mechanism allows DEA
to adjust conservatism and exploration in a data-driven way, adapting
aggregation to both uncertainty levels and the phase of training. We evaluate
DEA across continuous control benchmarks and learning regimes - from
interactive to sample-efficient - and demonstrate its effectiveness over static
ensemble strategies.

</details>


### [199] [A Verifier Hierarchy](https://arxiv.org/abs/2507.23504)
*Maurits Kaptein*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the trade-off between certificate length and verifier runtime.
We prove a Verifier Trade-off Theorem showing that reducing the inherent
verification time of a language from \(f(n)\) to \(g(n)\), where \(f(n) \ge
g(n)\), requires certificates of length at least \(\Omega(\log(f(n) / g(n)))\).
This theorem induces a natural hierarchy based on certificate complexity. We
demonstrate its applicability to analyzing conjectured separations between
complexity classes (e.g., \(\np\) and \(\exptime\)) and to studying natural
problems such as string periodicity and rotation detection. Additionally, we
provide perspectives on the \(\p\) vs. \(\np\) problem by relating it to the
existence of sub-linear certificates.

</details>


### [200] [Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level](https://arxiv.org/abs/2507.23512)
*Saleh Vatan Khah,Savelii Chezhegov,Shahrokh Farahmand,Samuel Horváth,Eduard Gorbunov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Gradient clipping is a fundamental tool in Deep Learning, improving the
high-probability convergence of stochastic first-order methods like SGD,
AdaGrad, and Adam under heavy-tailed noise, which is common in training large
language models. It is also a crucial component of Differential Privacy (DP)
mechanisms. However, existing high-probability convergence analyses typically
require the clipping threshold to increase with the number of optimization
steps, which is incompatible with standard DP mechanisms like the Gaussian
mechanism. In this work, we close this gap by providing the first
high-probability convergence analysis for DP-Clipped-SGD with a fixed clipping
level, applicable to both convex and non-convex smooth optimization under
heavy-tailed noise, characterized by a bounded central $\alpha$-th moment
assumption, $\alpha \in (1,2]$. Our results show that, with a fixed clipping
level, the method converges to a neighborhood of the optimal solution with a
faster rate than the existing ones. The neighborhood can be balanced against
the noise introduced by DP, providing a refined trade-off between convergence
speed and privacy guarantees.

</details>


### [201] [Continual Learning with Synthetic Boundary Experience Blending](https://arxiv.org/abs/2507.23534)
*Chih-Fan Hsu,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 针对持续学习中经验回放的局限性，本文提出“经验融合”框架，通过生成合成边界数据（SBD）增强决策边界稳定性，有效缓解灾难性遗忘，并显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）中存在灾难性遗忘问题。虽然经验回放有潜力，但其效果受限于存储关键样本的稀疏分布，导致决策边界过于简化，从而限制了其有效性。

Method: 本文提出一种名为“经验融合”（Experience Blending）的新型训练框架，整合了存储的关键样本和合成的边界附近数据。该框架包含两核心组件：1) 一种多变量差分隐私（DP）噪声机制，用于向低维特征表示注入批次噪声以生成合成边界数据（SBD）；2) 一种端到端训练策略，共同利用存储的关键样本和SBD进行训练。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上进行的广泛实验表明，该方法优于九个CL基线，准确率分别提高了10%、6%和13%。

Conclusion: 通过在训练中引入靠近决策边界的合成数据（SBD）作为隐式正则化器，并与存储关键样本相结合，“经验融合”框架有效改善了决策边界的稳定性，显著缓解了灾难性遗忘，并取得了卓越的性能提升。

Abstract: Continual learning (CL) aims to address catastrophic forgetting in models
trained sequentially on multiple tasks. While experience replay has shown
promise, its effectiveness is often limited by the sparse distribution of
stored key samples, leading to overly simplified decision boundaries. We
hypothesize that introducing synthetic data near the decision boundary
(Synthetic Boundary Data, or SBD) during training serves as an implicit
regularizer, improving boundary stability and mitigating forgetting. To
validate this hypothesis, we propose a novel training framework, {\bf
Experience Blending}, which integrates knowledge from both stored key samples
and synthetic, boundary-adjacent data. Experience blending consists of two core
components: (1) a multivariate Differential Privacy (DP) noise mechanism that
injects batch-wise noise into low-dimensional feature representations,
generating SBD; and (2) an end-to-end training strategy that jointly leverages
both stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,
and Tiny ImageNet demonstrate that our method outperforms nine CL baselines,
achieving accuracy improvements of 10%, 6%, and 13%, respectively.

</details>


### [202] [Transparent AI: The Case for Interpretability and Explainability](https://arxiv.org/abs/2507.23535)
*Dhanesh Ramachandram,Himanshu Joshi,Judy Zhu,Dhari Gandhi,Lucas Hartman,Ananya Raval*

Main category: cs.LG

TL;DR: 本论文总结了在不同领域实践AI可解释性的经验与见解，并为各阶段组织提供了将可解释性融入AI设计的实用策略与指导。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统日益影响各领域的高风险决策，透明度已成为负责任和可信赖AI实施的基础。

Method: 作者利用其作为领先机构的优势，总结了在不同领域实际应用AI可解释性所获得的关键见解和经验教训。

Result: 论文提供了可操作的策略和实施指导，旨在帮助处于不同AI成熟度阶段的组织。

Conclusion: 强调将可解释性作为核心设计原则进行整合，而非事后添加，以促进负责任和可信赖的AI部署。

Abstract: As artificial intelligence systems increasingly inform high-stakes decisions
across sectors, transparency has become foundational to responsible and
trustworthy AI implementation. Leveraging our role as a leading institute in
advancing AI research and enabling industry adoption, we present key insights
and lessons learned from practical interpretability applications across diverse
domains. This paper offers actionable strategies and implementation guidance
tailored to organizations at varying stages of AI maturity, emphasizing the
integration of interpretability as a core design principle rather than a
retrospective add-on.

</details>


### [203] [From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices](https://arxiv.org/abs/2507.23536)
*Georg Slamanig,Francesco Corti,Olga Saukh*

Main category: cs.LG

TL;DR: 本研究基准测试并分析了PEFT方法在边缘设备卷积神经网络（CNNs）上的应用，发现其在深度可分离卷积上的内存效率较低，但在边缘优化的CNN上可显著降低FLOPs达95%。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）方法在大型语言模型（LLMs）中已被广泛研究，但其在资源受限的边缘设备（如卷积神经网络）上的应用仍未得到充分探索。

Method: 本文在标准和深度可分离卷积架构上，基准测试并分析了LoRA、DoRA和GaLore等流行PEFT方法。利用PyTorch profilers比较了这些PEFT方法与传统微调的模型性能和计算成本，并考虑资源效率，研究了它们在不同秩维度下的更新行为。

Result: 研究发现，PEFT方法在深度可分离卷积架构上的内存效率仅为应用于LLMs时的一半。然而，当针对边缘部署优化的卷积架构时，基于适配器的PEFT方法在模型更新期间可将浮点运算（FLOPs）减少高达95%。

Conclusion: 这些见解为根据硬件限制、性能要求和应用需求选择PEFT方法提供了宝贵的指导。

Abstract: Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs
of updating deep learning models by minimizing the number of additional
parameters used to adapt a model to a down- stream task. While extensively
researched in large language models (LLMs), their application to smaller models
used on edge devices, such as convolutional neural networks, remains
underexplored. This paper benchmarks and analyzes popular PEFT methods on
convolutional architectures typically deployed in resource-constrained edge
environments. We evaluate LoRA, DoRA, and GaLore for updating standard and
depthwise convolutional architectures to handle distribution shifts and
accommodate unseen classes. We utilize recently proposed PyTorch profilers to
compare the updated model performance and computational costs of these PEFT
methods with traditional fine-tuning approaches. With resource efficiency in
mind, we investigate their update behavior across different rank dimensions. We
find that the evaluated PEFT methods are only half as memory-efficient when
applied to depthwise-separable convolution architectures, compared to their
efficiency with LLMs. Conversely, when targeting convolu- tional architectures
optimized for edge deployment, adapter-based PEFT methods can reduce floating
point operations (FLOPs) during model updates by up to 95%. These insights
offer valuable guidance for selecting PEFT methods based on hardware
constraints, performance requirements, and application needs. Our code is
online.

</details>


### [204] [Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions](https://arxiv.org/abs/2507.23539)
*Piotr Indyk,Michael Kapralov,Kshiteej Sheth,Tal Wagner*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by the problem of fast processing of attention matrices, we study
fast algorithms for computing matrix-vector products for asymmetric Gaussian
Kernel matrices $K\in \mathbb{R}^{n\times n}$. $K$'s columns are indexed by a
set of $n$ keys $k_1,k_2\ldots, k_n\in \mathbb{R}^d$, rows by a set of $n$
queries $q_1,q_2,\ldots,q_n\in \mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =
e^{-\|q_i-k_j\|_2^2/2\sigma^2}$ for some bandwidth parameter $\sigma>0$. Given
a vector $x\in \mathbb{R}^n$ and error parameter $\epsilon>0$, our task is to
output a $y\in \mathbb{R}^n$ such that $\|Kx-y\|_2\leq \epsilon \|x\|_2$ in
time subquadratic in $n$ and linear in $d$. Our algorithms rely on the
following modelling assumption about the matrices $K$: the sum of the entries
of $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We
validate this assumption experimentally, for Gaussian kernel matrices
encountered in various settings such as fast attention computation in LLMs. We
obtain the first subquadratic-time algorithm that works under this assumption,
for unrestricted vectors.

</details>


### [205] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 该研究在SpiNNaker2神经形态芯片上，通过量化脉冲神经网络（SNNs）实现了Q学习算法，以解决经典控制任务，结果显示能耗比GPU降低高达32倍，同时推理延迟相当。


<details>
  <summary>Details</summary>
Motivation: 为了在机器人任务中利用脉冲神经网络（SNNs）在神经形态硬件上实现超低功耗和低延迟推理，并探索其在强化学习（RL）任务中的能效优势。

Method: 使用Q学习算法训练脉冲神经网络，然后进行微调并量化至8位精度，部署到SpiNNaker2神经形态芯片上。通过分析推理延迟、动态功耗和每次推理的能耗成本，与GTX 1650 GPU基线进行性能比较。

Result: SpiNNaker2展示了可扩展、低能耗神经形态计算的巨大潜力，实现了高达32倍的能耗降低。推理延迟与基于GPU的执行相当，在特定任务设置中观察到性能提升。

Conclusion: SpiNNaker2适用于实时神经形态控制，神经形态方法是实现高效深度Q学习的一个有吸引力的方向，因为它能显著降低能耗并保持与GPU相当的延迟。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


### [206] [Optimised Feature Subset Selection via Simulated Annealing](https://arxiv.org/abs/2507.23568)
*Fernando Martínez-García,Álvaro Rubio-García,Samuel Fernández-Lorenzo,Juan José García-Ripoll,Diego Porras*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce SA-FDR, a novel algorithm for $\ell_0$-norm feature selection
that considers this task as a combinatorial optimisation problem and solves it
by using simulated annealing to perform a global search over the space of
feature subsets. The optimisation is guided by the Fisher discriminant ratio,
which we use as a computationally efficient proxy for model quality in
classification tasks. Our experiments, conducted on datasets with up to
hundreds of thousands of samples and hundreds of features, demonstrate that
SA-FDR consistently selects more compact feature subsets while achieving a high
predictive accuracy. This ability to recover informative yet minimal sets of
features stems from its capacity to capture inter-feature dependencies often
missed by greedy optimisation approaches. As a result, SA-FDR provides a
flexible and effective solution for designing interpretable models in
high-dimensional settings, particularly when model sparsity, interpretability,
and performance are crucial.

</details>


### [207] [GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning](https://arxiv.org/abs/2507.23581)
*Chuanyue Yu,Kuo Zhao,Yuhan Li,Heng Chang,Mingjian Feng,Xiangzhe Jiang,Yufei Sun,Jia Li,Yuzhi Zhang,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: GraphRAG-R1通过引入基于过程约束和结果奖励的强化学习（RL），解决了现有GraphRAG在多跳推理中的瓶颈，显著提升了LLMs在复杂推理任务中的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法在处理需要多跳推理的复杂问题时面临瓶颈，其查询和检索阶段主要依赖预定义启发式，未能充分利用LLMs的推理潜力。

Method: 提出GraphRAG-R1，一个自适应GraphRAG框架。通过基于过程约束和结果导向的强化学习（RL）训练LLMs，以增强多跳推理能力。具体方法包括：使用修改版GRPO支持rollout-with-thinking；设计Progressive Retrieval Attenuation (PRA) 奖励鼓励必要检索，以及Cost-Aware F1 (CAF) 奖励平衡性能与成本；设计阶段依赖的训练策略；采用混合图-文本检索。

Result: 实验结果表明，GraphRAG-R1在解决复杂推理问题方面显著提升了LLM的能力，优于现有最先进的GraphRAG方法，在域内和域外数据集上均表现出色。此外，该框架可灵活集成现有检索方法并持续提升性能。

Conclusion: GraphRAG-R1通过创新的强化学习和奖励机制，成功克服了现有GraphRAG在多跳推理上的局限性，显著提升了LLM处理复杂推理任务的性能和灵活性。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness
in enhancing the reasoning abilities of LLMs by leveraging graph structures for
knowledge representation and modeling complex real-world relationships.
However, existing GraphRAG methods still face significant bottlenecks when
handling complex problems that require multi-hop reasoning, as their query and
retrieval phases are largely based on pre-defined heuristics and do not fully
utilize the reasoning potentials of LLMs. To address this problem, we propose
GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with
process-constrained outcome-based reinforcement learning (RL) to enhance the
multi-hop reasoning ability. Our method can decompose complex problems,
autonomously invoke retrieval tools to acquire necessary information, and
perform effective reasoning. Specifically, we utilize a modified version of
Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking
capability. Next, we design two process-constrained reward functions. To handle
the shallow retrieval problem, we design a Progressive Retrieval Attenuation
(PRA) reward to encourage essential retrievals. Then, to handle the
over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the
model performance with computational costs. We further design a phase-dependent
training strategy, containing three training stages corresponding to cold start
and these two rewards. Lastly, our method adopts a hybrid graph-textual
retrieval to improve the reasoning capacity. Extensive experimental results
demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex
reasoning problems compared to state-of-the-art GraphRAG methods on both
in-domain and out-of-domain datasets. Furthermore, our framework can be
flexibly integrated with various existing retrieval methods, consistently
delivering performance improvements.

</details>


### [208] [EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution](https://arxiv.org/abs/2507.23600)
*Yu-Tang Chang,Shih-Fang Chen*

Main category: cs.LG

TL;DR: EB-gMCR是一种基于能量深度学习的MCR新方法，能自动识别信号分解中的最小有效组分集，解决了传统MCR在组分数量未知和大规模数据上的挑战，并表现出高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的多元曲线解析（MCR，基于矩阵分解）要求用户指定组分数量，但这在真实数据中通常是未知的。随着数据集规模或组分数量的增加，传统方法的可扩展性和可靠性面临重大挑战。

Method: 本研究将MCR重新表述为生成过程（gMCR），并引入了一种基于能量的深度学习求解器EB-gMCR。EB-gMCR从大型候选池（例如1024个光谱）开始，利用可微分门控网络仅保留活跃组分，同时估计它们的浓度。此外，化学先验（如非负性或非线性混合）可作为简单的即插即用函数引入。

Result: 在包含多达256个潜在源的合成噪声数据集上，EB-gMCR保持了R^2 >= 0.98的重构精度，并将组分数量的估计误差控制在真实值的5%以内；在较低噪声条件下，R^2达到0.99，组分估计几乎精确。该方法能够通过简单函数适应其他仪器或领域。

Conclusion: EB-gMCR通过结合高容量生成建模和硬组分选择，为大规模信号解混分析（包括化学文库驱动的场景）提供了一条实用的途径，解决了传统MCR在处理未知组分数量和大型数据集时的局限性。

Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely
applied in chemical and biological research. Multivariate curve resolution
(MCR), a branch of signal unmixing, separates mixed chemical signals into base
patterns (components) and their concentrations, playing a key role in
understanding composition. Classical MCR is typically framed as matrix
factorization (MF) and requires a user-specified component count, usually
unknown in real data. As dataset size or component count increases, the
scalability and reliability of MF-based MCR face significant challenges. This
study reformulates MCR as a generative process (gMCR), and introduces an
energy-based deep learning solver, EB-gMCR, that automatically discovers the
smallest component set able to reconstruct the data faithfully. EB-gMCR starts
from a large candidate pool (e.g., 1024 spectra) and employs a differentiable
gating network to retain only active components while estimating their
concentrations. On noisy synthetic datasets containing up to 256 latent
sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count
within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near
exact component estimation. Additional chemical priors, such as non-negativity
or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to
other instruments or domains without altering the core learning process. By
uniting high-capacity generative modeling and hard component selection, EB-gMCR
offers a practical route to large-scale signal unmixing analysis, including
chemical library-driven scenarios. The source code is available at
https://github.com/b05611038/ebgmcr_solver.

</details>


### [209] [Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.23604)
*Tommaso Marzi,Cesare Alippi,Andrea Cini*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的方法，通过结合分层强化学习（采用封建HRL框架和分层图结构）与消息传递机制，来学习分散式多智能体系统中的分层策略，以解决部分可观察性和非平稳性问题。


<details>
  <summary>Details</summary>
Motivation: 分散式多智能体强化学习（MARL）面临部分可观察性和非平稳性挑战。虽然通信和分层强化学习（HRL）可缓解这些问题，但HRL在多智能体系统中的优化问题限制了其应用，且这两种方法的结合尚未被充分探索。

Method: 我们提出了一种学习多智能体消息传递策略层次结构的新颖有效方法。该方法采用封建HRL框架，并依赖分层图结构进行规划和协调。层级较低的智能体接收来自上层的目标，并与同级邻居交换消息。设计了一种新颖的奖励分配方法，通过训练低层策略以最大化与上层相关的优势函数。

Result: 在相关基准测试上的结果表明，我们的方法表现优于现有最佳方法。

Conclusion: 我们提出了一种有效结合分层强化学习和消息传递的分散式多智能体系统方法，通过新颖的奖励分配和结构设计，在性能上超越了现有技术，为解决多智能体协调和规划问题提供了新途径。

Abstract: Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for
learning scalable multi-agent policies, but suffer from partial observability
and induced non-stationarity. These challenges can be addressed by introducing
mechanisms that facilitate coordination and high-level planning. Specifically,
coordination and temporal abstraction can be achieved through communication
(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)
approaches to decision-making. However, optimization issues limit the
applicability of hierarchical policies to multi-agent systems. As such, the
combination of these approaches has not been fully explored. To fill this void,
we propose a novel and effective methodology for learning multi-agent
hierarchies of message-passing policies. We adopt the feudal HRL framework and
rely on a hierarchical graph structure for planning and coordination among
agents. Agents at lower levels in the hierarchy receive goals from the upper
levels and exchange messages with neighboring agents at the same level. To
learn hierarchical multi-agent policies, we design a novel reward-assignment
method based on training the lower-level policies to maximize the advantage
function associated with the upper levels. Results on relevant benchmarks show
that our method performs favorably compared to the state of the art.

</details>


### [210] [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](https://arxiv.org/abs/2507.23607)
*Tien Huu Do,Antoine Masquelier,Nae Eoun Lee,Jonathan Crowther*

Main category: cs.LG

TL;DR: 本文提出一种新的深度学习方法，利用预训练语言模型和伽马分布，以有效预测临床试验的患者招募数量，并优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 临床试验耗资巨大且规划复杂，准确预测试验结果至关重要。其中，准确预测患者招募是规划阶段的一大挑战。

Method: 提出一种基于深度学习的神经网络模型。该模型利用预训练语言模型（PLMs）处理临床文档，结合编码的表格特征，通过注意力机制融合。为处理预测不确定性，模型增加了一个基于伽马分布的概率层以实现范围估计。该模型应用于预测临床试验时长，假设站点级别的招募遵循泊松-伽马过程。

Result: 在真实世界临床试验数据上进行的大量实验表明，所提出的方法能有效预测给定临床试验在多个站点的患者招募数量，并优于现有的基线模型。

Conclusion: 该深度学习方法能有效预测临床试验中的患者招募数量，并能处理预测不确定性，为临床试验规划提供了更准确可靠的工具。

Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy
of new drugs or treatments. Conducting such trials typically demands
significant financial investment and meticulous planning, highlighting the need
for accurate predictions of trial outcomes. Accurately predicting patient
enrollment, a key factor in trial success, is one of the primary challenges
during the planning phase. In this work, we propose a novel deep learning-based
method to address this critical challenge. Our method, implemented as a neural
network model, leverages pre-trained language models (PLMs) to capture the
complexities and nuances of clinical documents, transforming them into
expressive representations. These representations are then combined with
encoded tabular features via an attention mechanism. To account for
uncertainties in enrollment prediction, we enhance the model with a
probabilistic layer based on the Gamma distribution, which enables range
estimation. We apply the proposed model to predict clinical trial duration,
assuming site-level enrollment follows a Poisson-Gamma process. We carry out
extensive experiments on real-world clinical trial data, and show that the
proposed method can effectively predict the number of patients enrolled at a
number of sites for a given clinical trial, outperforming established baseline
models.

</details>


### [211] [L-GTA: Latent Generative Modeling for Time Series Augmentation](https://arxiv.org/abs/2507.23615)
*Luis Roque,Carlos Soares,Vitor Cerqueira,Luis Torgo*

Main category: cs.LG

TL;DR: 提出L-GTA模型，利用变分循环Transformer自编码器在潜在空间进行可控变换，生成高质量时间序列增强数据，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析中的数据增强在预测、分类和异常检测等任务中日益重要。

Method: 引入L-GTA（Latent Generative Transformer Augmentation）模型，这是一种基于Transformer的变分循环自编码器的生成方法。该模型通过在潜在空间进行受控变换，生成保留原始数据内在属性的新时间序列，支持抖动、幅度扭曲及组合等多种变换。

Result: 实验证明L-GTA能生成更可靠、一致且可控的增强数据。与直接变换方法相比，L-GTA显著提高了预测准确性和相似性度量。

Conclusion: L-GTA是一种有效的时间序列数据增强方法，能够生成高质量的合成数据，从而在预测准确性和数据相似性方面实现显著提升。

Abstract: Data augmentation is gaining importance across various aspects of time series
analysis, from forecasting to classification and anomaly detection tasks. We
introduce the Latent Generative Transformer Augmentation (L-GTA) model, a
generative approach using a transformer-based variational recurrent
autoencoder. This model uses controlled transformations within the latent space
of the model to generate new time series that preserve the intrinsic properties
of the original dataset. L-GTA enables the application of diverse
transformations, ranging from simple jittering to magnitude warping, and
combining these basic transformations to generate more complex synthetic time
series datasets. Our evaluation of several real-world datasets demonstrates the
ability of L-GTA to produce more reliable, consistent, and controllable
augmented data. This translates into significant improvements in predictive
accuracy and similarity measures compared to direct transformation methods.

</details>


### [212] [On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective](https://arxiv.org/abs/2507.23632)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 本文通过推导softmax注意力机制的循环形式，证明了线性注意力是其近似，并借此解释了为何softmax注意力比其变体更具表达力。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力在Transformer中表现出色但存在二次方的计算和内存瓶颈；线性注意力虽解决了效率问题却牺牲了准确性。研究旨在解释这种准确性差异，并深入理解softmax注意力表达力优越的原因。

Method: 核心方法是推导出softmax注意力机制的循环（RNN）形式。利用此形式，作者证明线性注意力是对softmax注意力的近似。通过将softmax注意力的各部分描述为RNN组件，论文得以进行组件消融研究，以理解其重要性和相互作用。

Result: 研究表明，线性注意力是softmax注意力的一个近似。通过引入softmax注意力的循环形式，能够用循环神经网络的语言描述其内部组件。这种描述和消融研究有助于阐明softmax注意力各部分的重要性及其相互作用。

Conclusion: 本研究通过揭示softmax注意力的循环本质及其组件间的相互作用，有效解释了其相比其他形式为何更具表达力。

Abstract: Since its introduction, softmax attention has become the backbone of modern
transformer architectures due to its expressiveness and scalability across a
wide range of tasks. However, the main drawback of softmax attention is the
quadratic memory requirement and computational complexity with respect to the
sequence length. By replacing the softmax nonlinearity, linear attention and
similar methods have been introduced to avoid the quadratic bottleneck of
softmax attention. Despite these linear forms of attention being derived from
the original softmax formulation, they typically lag in terms of downstream
accuracy. While strong intuition of the softmax nonlinearity on the query and
key inner product suggests that it has desirable properties compared to other
nonlinearities, the question of why this discrepancy exists still remains
unanswered. This work demonstrates that linear attention is an approximation of
softmax attention by deriving the recurrent form of softmax attention. Using
this form, each part of softmax attention can be described in the language of
recurrent neural networks (RNNs). Describing softmax attention as an RNN allows
for the ablation of the components of softmax attention to understand the
importance of each part and how they interact. In this way, our work helps
explain why softmax attention is more expressive than its counterparts.

</details>


### [213] [OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting](https://arxiv.org/abs/2507.23638)
*Mohammad Karami,Fatemeh Ghassemi,Hamed Kebriaei,Hamid Azadegan*

Main category: cs.LG

TL;DR: 本文提出了OptiGradTrust框架，通过多维梯度指纹和强化学习-注意力模块评估梯度更新，以防御联邦学习（FL）中的拜占庭攻击；同时，开发了FedBN-Prox方法应对数据异构性，在多个数据集和攻击场景下显著提升了FL的鲁棒性和性能，优于现有先进防御。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽能保护患者隐私实现分布式医疗模型训练，但易受拜占庭攻击和统计异构性影响，限制了其在实际应用中的可靠性。

Method: 1. 提出OptiGradTrust防御框架，通过VAE重构误差、余弦相似度、L2范数、符号一致性比率和蒙特卡洛Shapley值构建六维梯度指纹，驱动混合强化学习-注意力模块进行自适应信任评分。 2. 开发FedBN-Prox（FedBN-P）方法，结合联邦批量归一化与近端正则化，以应对数据异构性下的收敛挑战并优化准确性-收敛性权衡。

Result: 在MNIST、CIFAR-10和阿尔茨海默症MRI数据集上，OptiGradTrust和FedBN-Prox在各种拜占庭攻击场景下表现出显著优于现有最先进防御的性能，在非独立同分布（non-IID）条件下比FLGuard提高了1.6个百分点，并通过自适应学习方法保持了对多样化攻击模式的鲁棒性。

Conclusion: OptiGradTrust和FedBN-Prox框架有效解决了联邦学习在拜占庭攻击和数据异构性下的鲁棒性与收敛性挑战，显著提升了FL的性能和可靠性，为未来医疗等领域的应用提供了更强的防御机制。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed medical institutions while preserving patient privacy, but remains
vulnerable to Byzantine attacks and statistical heterogeneity. We present
OptiGradTrust, a comprehensive defense framework that evaluates gradient
updates through a novel six-dimensional fingerprint including VAE
reconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency
ratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module
for adaptive trust scoring. To address convergence challenges under data
heterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch
Normalization with proximal regularization for optimal accuracy-convergence
trade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI
datasets under various Byzantine attack scenarios demonstrates significant
improvements over state-of-the-art defenses, achieving up to +1.6 percentage
points over FLGuard under non-IID conditions while maintaining robust
performance against diverse attack patterns through our adaptive learning
approach.

</details>


### [214] [SHAP-Guided Regularization in Machine Learning Models](https://arxiv.org/abs/2507.23665)
*Amal Saadallah*

Main category: cs.LG

TL;DR: 本文提出一个基于SHAP的正则化框架，通过引入特征重要性约束和熵惩罚来优化模型训练，旨在提升预测性能、可解释性及特征归因的稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然SHAP等特征归因方法在理解机器学习模型方面至关重要，但它们在指导模型优化方面的作用尚未得到充分探索。

Method: 提出一个SHAP引导的正则化框架，在模型训练中引入特征重要性约束。该方法采用基于熵的惩罚来促进稀疏、集中的特征归因，并增强样本间的稳定性。框架适用于回归和分类任务，并初步探索了使用TreeSHAP对树模型进行正则化。

Result: 通过在基准回归和分类数据集上进行广泛实验，结果表明该方法不仅提高了模型的泛化性能，还确保了特征归因的鲁棒性和可解释性。

Conclusion: 该技术提供了一种新颖的、由可解释性驱动的正则化方法，使机器学习模型更加准确和可靠。

Abstract: Feature attribution methods such as SHapley Additive exPlanations (SHAP) have
become instrumental in understanding machine learning models, but their role in
guiding model optimization remains underexplored. In this paper, we propose a
SHAP-guided regularization framework that incorporates feature importance
constraints into model training to enhance both predictive performance and
interpretability. Our approach applies entropy-based penalties to encourage
sparse, concentrated feature attributions while promoting stability across
samples. The framework is applicable to both regression and classification
tasks. Our first exploration started with investigating a tree-based model
regularization using TreeSHAP. Through extensive experiments on benchmark
regression and classification datasets, we demonstrate that our method improves
generalization performance while ensuring robust and interpretable feature
attributions. The proposed technique offers a novel, explainability-driven
regularization approach, making machine learning models both more accurate and
more reliable.

</details>


### [215] [TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses](https://arxiv.org/abs/2507.23674)
*Muhammad Taha Cheema,Abeer Aamir,Khawaja Gul Muhammad,Naveed Anwar Bhatti,Ihsan Ayyub Qazi,Zafar Ayyub Qazi*

Main category: cs.LG

TL;DR: TweakLLM通过轻量级LLM动态调整缓存回复，解决了LLM高效缓存中保持相关性难题，在确保回复质量的同时显著提升了缓存效率。


<details>
  <summary>Details</summary>
Motivation: LLM每日处理数百万查询，高效缓存可降低成本和延迟。然而，由于个性化交互和语义相似性搜索的局限性，现有缓存方案难以保持回复与用户查询的相关性。

Method: 提出TweakLLM，一个新颖的路由架构，利用一个轻量级LLM动态调整已缓存的回复以适应新的用户提示。

Result: 通过用户研究、满意度投票及多智能体LLM辩论等评估，TweakLLM在保持与前沿模型相当的回复质量的同时，显著提升了缓存效率。

Conclusion: TweakLLM是一种可扩展、资源高效的LLM缓存解决方案，适用于高流量部署，且不影响用户体验。

Abstract: Large Language Models (LLMs) process millions of queries daily, making
efficient response caching a compelling optimization for reducing cost and
latency. However, preserving relevance to user queries using this approach
proves difficult due to the personalized nature of chatbot interactions and the
limited accuracy of semantic similarity search. To address this, we present
TweakLLM, a novel routing architecture that employs a lightweight LLM to
dynamically adapt cached responses to incoming prompts. Through comprehensive
evaluation, including user studies with side-by-side comparisons, satisfaction
voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM
maintains response quality comparable to frontier models while significantly
improving cache effectiveness. Our results across real-world datasets highlight
TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM
deployments without compromising user experience.

</details>


### [216] [One-Step Flow Policy Mirror Descent](https://arxiv.org/abs/2507.23675)
*Tianyi Chen,Haitong Ma,Na Li,Kai Wang,Bo Dai*

Main category: cs.LG

TL;DR: 本文提出Flow Policy Mirror Descent (FPMD)算法，解决了扩散策略在在线强化学习中推理速度慢的问题。FPMD通过1步采样实现快速推理，性能媲美现有扩散策略，但推理速度提升数百倍。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在在线强化学习中表现出色，但其推理过程依赖于缓慢的迭代采样，这限制了其响应速度。

Method: 本文提出Flow Policy Mirror Descent (FPMD)算法，该算法通过利用直线插值流匹配模型中单步采样带来的分布方差与离散化误差之间的理论联系，实现了策略推理过程中的1步采样。此方法无需额外的蒸馏或一致性训练，并提供了基于流策略和MeanFlow策略参数化的两种变体。

Result: 在MuJoCo基准测试中，所提出的算法展示出与扩散策略基线相当的强大性能，同时在推理过程中所需的函数评估次数减少了数百倍。

Conclusion: FPMD算法成功地在在线强化学习中实现了高性能和高效率的结合，显著提升了扩散策略的推理速度，使其更具实用性。

Abstract: Diffusion policies have achieved great success in online reinforcement
learning (RL) due to their strong expressive capacity. However, the inference
of diffusion policy models relies on a slow iterative sampling process, which
limits their responsiveness. To overcome this limitation, we propose Flow
Policy Mirror Descent (FPMD), an online RL algorithm that enables 1-step
sampling during policy inference. Our approach exploits a theoretical
connection between the distribution variance and the discretization error of
single-step sampling in straight interpolation flow matching models, and
requires no extra distillation or consistency training. We present two
algorithm variants based on flow policy and MeanFlow policy parametrizations,
respectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate
that our algorithms show strong performance comparable to diffusion policy
baselines while requiring hundreds of times fewer function evaluations during
inference.

</details>


### [217] [DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data](https://arxiv.org/abs/2507.23676)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: DepMicroDiff是一个新颖的框架，结合扩散模型和依赖感知Transformer，通过预训练和元数据利用，显著提高了稀疏且噪声大的微生物组数据的插补准确性。


<details>
  <summary>Details</summary>
Motivation: 微生物组数据分析对理解宿主健康至关重要，但其固有的稀疏性和噪声导致准确插补困难，阻碍下游任务。现有插补方法（包括扩散模型）未能捕捉微生物分类群的复杂相互依赖性并忽视上下文元数据。

Method: 引入DepMicroDiff框架，它结合了扩散生成模型与依赖感知Transformer（DAT）以显式捕获成对和自回归依赖关系。该方法通过在多样癌症数据集上进行VAE预训练，并利用大型语言模型（LLM）编码的患者元数据进行条件化以进一步增强。

Result: 在TCGA微生物组数据集上的实验显示，DepMicroDiff显著优于现有基线模型，实现了更高的皮尔逊相关性（高达0.712）、余弦相似度（高达0.812），以及更低的RMSE和MAE，在多种癌症类型中均表现出优异性能。

Conclusion: DepMicroDiff为微生物组数据插补提供了一个鲁棒且泛化能力强的解决方案，有效克服了微生物组数据的稀疏性和噪声挑战。

Abstract: Microbiome data analysis is essential for understanding host health and
disease, yet its inherent sparsity and noise pose major challenges for accurate
imputation, hindering downstream tasks such as biomarker discovery. Existing
imputation methods, including recent diffusion-based models, often fail to
capture the complex interdependencies between microbial taxa and overlook
contextual metadata that can inform imputation. We introduce DepMicroDiff, a
novel framework that combines diffusion-based generative modeling with a
Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise
dependencies and autoregressive relationships. DepMicroDiff is further enhanced
by VAE-based pretraining across diverse cancer datasets and conditioning on
patient metadata encoded via a large language model (LLM). Experiments on TCGA
microbiome datasets show that DepMicroDiff substantially outperforms
state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),
cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer
types, demonstrating its robustness and generalizability for microbiome
imputation.

</details>


### [218] [Anomalous Samples for Few-Shot Anomaly Detection](https://arxiv.org/abs/2507.23712)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.LG

TL;DR: 本文针对少样本设置下异常检测中异常样本的利用问题，提出了一种结合零样本和记忆技术的多分数异常检测方法，并引入增强验证策略优化分数聚合，旨在利用异常样本进行二元异常分类。


<details>
  <summary>Details</summary>
Motivation: 传统的异常检测方法过度依赖大量正常样本，但该假设在少样本（Few-Shot）情境下受到质疑，因为即使少量标注的异常样本也能带来显著差异。因此，本研究的动机是探讨如何在训练二元异常分类模型时有效利用异常样本。

Method: 1. 提出一种将异常样本整合到多分数异常检测中的新方法。
2. 该方法利用了最新的零样本（Zero-Shot）和基于记忆（memory-based）技术。
3. 比较了异常样本与常规样本的效用，并研究了各自的优缺点。
4. 引入了一种基于数据增强的验证技术，用于优化不同异常分数的聚合。

Result: 1. 在流行的工业异常检测数据集上，证明了所提出的基于增强的验证技术在优化异常分数聚合方面的有效性。
2. 研究并阐明了利用异常样本和常规样本进行异常检测的各自效用、优势和局限性。

Conclusion: 本研究成功证明了在二元异常分类中利用异常样本的可行性和益处，尤其是在少样本情境下，通过结合创新的多分数检测方法和有效的聚合优化策略，为提升异常检测性能提供了新的途径。

Abstract: Several anomaly detection and classification methods rely on large amounts of
non-anomalous or "normal" samples under the assump- tion that anomalous data is
typically harder to acquire. This hypothesis becomes questionable in Few-Shot
settings, where as little as one anno- tated sample can make a significant
difference. In this paper, we tackle the question of utilizing anomalous
samples in training a model for bi- nary anomaly classification. We propose a
methodology that incorporates anomalous samples in a multi-score anomaly
detection score leveraging recent Zero-Shot and memory-based techniques. We
compare the utility of anomalous samples to that of regular samples and study
the benefits and limitations of each. In addition, we propose an
augmentation-based validation technique to optimize the aggregation of the
different anomaly scores and demonstrate its effectiveness on popular
industrial anomaly detection datasets.

</details>


### [219] [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](https://arxiv.org/abs/2507.23756)
*Diana Mortagua*

Main category: cs.LG

TL;DR: 本研究提出一种新型主动学习（AL）策略，通过知识推荐系统（RS）综合考虑标注者的历史准确率、情绪和疲劳水平，以选择最佳标注者，从而减少标注错误、降低模型不确定性并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）在降低数据标注成本方面有效，但标注错误依然影响效率和准确性。现有查询-标注者配对策略忽视了影响标注者表现的内部因素（如情绪、疲劳）。研究旨在解决这一空白，通过考虑这些内部因素来优化标注者选择，以减少错误并加速模型收敛。

Method: 开发了一个知识推荐系统（RS），该系统利用标注者的历史准确率、情绪和疲劳水平，以及被查询实例的信息，对标注者进行排名。该方法基于现有关于情绪和疲劳对人类表现影响的文献，通过模拟真实标注者行为来预测其性能，并据此选择最合适的标注者进行数据标注。

Result: 结果显示，与不考虑内部因素的方法相比，结合历史准确率、情绪和疲劳水平能显著减少标注错误数量，降低模型训练过程中的不确定性。尽管提升幅度相对较小，但所提出的方法在准确率和F1-score方面也表现出更好的性能。

Conclusion: 本研究的方法和发现开启了探索人类认知因素影响主动学习这一开放挑战的新方向，证明了将情绪和疲劳等内部因素纳入标注者选择策略的有效性，有助于提高主动学习的整体效率和模型性能。

Abstract: This study centers on overcoming the challenge of selecting the best
annotators for each query in Active Learning (AL), with the objective of
minimizing misclassifications. AL recognizes the challenges related to cost and
time when acquiring labeled data, and decreases the number of labeled data
needed. Nevertheless, there is still the necessity to reduce annotation errors,
aiming to be as efficient as possible, to achieve the expected accuracy faster.
Most strategies for query-annotator pairs do not consider internal factors that
affect productivity, such as mood, attention, motivation, and fatigue levels.
This work addresses this gap in the existing literature, by not only
considering how the internal factors influence annotators (mood and fatigue
levels) but also presenting a new query-annotator pair strategy, using a
Knowledge-Based Recommendation System (RS). The RS ranks the available
annotators, allowing to choose one or more to label the queried instance using
their past accuracy values, and their mood and fatigue levels, as well as
information about the instance queried. This work bases itself on existing
literature on mood and fatigue influence on human performance, simulating
annotators in a realistic manner, and predicting their performance with the RS.
The results show that considering past accuracy values, as well as mood and
fatigue levels reduces the number of annotation errors made by the annotators,
and the uncertainty of the model through its training, when compared to not
using internal factors. Accuracy and F1-score values were also better in the
proposed approach, despite not being as substantial as the aforementioned. The
methodologies and findings presented in this study begin to explore the open
challenge of human cognitive factors affecting AL.

</details>


### [220] [Consensus-Driven Active Model Selection](https://arxiv.org/abs/2507.23771)
*Justin Kay,Grant Van Horn,Subhransu Maji,Daniel Sheldon,Sara Beery*

Main category: cs.LG

TL;DR: CODA是一种主动模型选择方法，通过优先标注能有效区分最佳候选模型的数据点，显著降低模型选择所需的数据标注成本。


<details>
  <summary>Details</summary>
Motivation: 面对大量可用的机器学习模型，为特定任务选择最佳模型是一项挑战，而传统通过收集和标注验证集的方法成本高昂且耗时。

Method: 本文提出CODA方法，一种共识驱动的主动模型选择技术。它在一个概率框架内建模分类器、类别和数据点之间的关系，利用候选模型间的共识与分歧来指导标签获取过程，并使用贝叶斯推断更新对最佳模型的判断。

Result: CODA在26个基准任务中表现显著优于现有主动模型选择方法，将发现最佳模型所需的标注工作量比现有最佳方法减少了70%以上。

Conclusion: CODA通过创新的主动学习方法，显著提高了模型选择的效率，大幅降低了数据标注的成本，是解决多模型选择挑战的有效方案。

Abstract: The widespread availability of off-the-shelf machine learning models poses a
challenge: which model, of the many available candidates, should be chosen for
a given data analysis task? This question of model selection is traditionally
answered by collecting and annotating a validation dataset -- a costly and
time-intensive process. We propose a method for active model selection, using
predictions from candidate models to prioritize the labeling of test data
points that efficiently differentiate the best candidate. Our method, CODA,
performs consensus-driven active model selection by modeling relationships
between classifiers, categories, and data points within a probabilistic
framework. The framework uses the consensus and disagreement between models in
the candidate pool to guide the label acquisition process, and Bayesian
inference to update beliefs about which model is best as more information is
collected. We validate our approach by curating a collection of 26 benchmark
tasks capturing a range of model selection scenarios. CODA outperforms existing
methods for active model selection significantly, reducing the annotation
effort required to discover the best model by upwards of 70% compared to the
previous state-of-the-art. Code and data are available at
https://github.com/justinkay/coda.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [221] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: 本文提出PRIME，一种伪随机轮询分包技术，旨在优化数据中心网络中AI/ML工作负载的负载均衡和性能，尤其解决了现有方案的缓冲区膨胀和尾部延迟问题，并在模拟中展现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大规模AI/ML分布式训练对网络基础设施提出巨大挑战，现有负载均衡方案（如ECMP和现有分包技术）难以应对低熵、突发、长寿命流，导致网络利用率低、缓冲区膨胀、信息过时及非对称网络条件下的尾部延迟增加。

Method: PRIME是一种伪随机轮询分包方法，它考虑网络拓扑结构以优化负载分布。该方法利用拥塞作为重新平衡负载的指示器，并综合考虑多种拥塞信号的严重程度及其衰减时间，以避免网络热点。

Result: 通过大规模生产级模拟器评估，PRIME相比现有解决方案，在置换流量下性能提升高达15%，在网络降级场景下性能提升高达27%。

Conclusion: PRIME有效解决了现有分包技术的局限性，通过拓扑感知和拥塞自适应机制，显著提升了AI/ML工作负载在复杂网络条件下的性能，尤其在网络退化场景下表现突出。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [222] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: InterfO-RAN是一种实时可编程的基于CNN的解决方案，用于在密集5G网络中高精度、低延迟地检测带内上行干扰。


<details>
  <summary>Details</summary>
Motivation: 超密集5G网络中的带内上行干扰不可预测，尤其是在小区边缘和毫米波系统中，这会导致SINR显著下降，扭曲质量指标，并中断调度、资源分配等协议操作。

Method: 该文提出了InterfO-RAN，一个实时可编程的解决方案。它利用卷积神经网络（CNN）处理gNB物理层的I/Q样本来检测带内干扰。InterfO-RAN是首个在GPU上加速的O-RAN dApp，与NVIDIA Aerial的5G NR物理层处理共存。

Result: InterfO-RAN的干扰检测精度超过91%，耗时低于650微秒。该方案在从真实环境中收集的700多万个NR上行时隙上进行了训练和测试，并部署在端到端私有5G网络中。

Conclusion: 研究结果表明InterfO-RAN具备强大的干扰检测能力，对于在密集5G部署中维持网络性能至关重要。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [223] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 针对Aloha随机接入网络，本研究从分组化角度探索以秒为单位的队列时延的优化策略。


<details>
  <summary>Details</summary>
Motivation: 随着低时延服务需求的增长，确保随机接入网络的时延性能至关重要。现有Aloha模型队列时延研究忽略了分组化对以秒为单位的队列时延的影响，而该指标更具实际应用价值。

Method: 建立了分组化与无连接/有连接Aloha方案下秒级平均队列时延的数学关系；通过数值方法确定最优分组策略和最小化时延，并分析网络参数影响；通过仿真研究分组化对队列时延抖动的影响；重新评估了两种Aloha方案的权衡；并将分析应用于NTN场景下的RA-SDT。

Result: 识别出最优平均队列时延及其对应的分组大小；分析了各种网络参数对其的影响；通过仿真揭示了分组化对队列时延抖动的相似影响；从分组化新视角重新评估了无连接与有连接方案的复杂权衡；并将分析应用于NTN场景下的RA-SDT。

Conclusion: 本研究强调了分组化在优化Aloha网络队列时延中的关键作用，为理解和提升低时延服务网络的性能提供了新的视角和优化策略，尤其是在新兴场景下。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [224] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: LoRaWAN仿真框架FAST-LoRa利用分析模型和高效矩阵运算，显著缩短计算时间，同时在复杂场景下保持对网络指标的高精度评估。


<details>
  <summary>Details</summary>
Motivation: 现有LoRaWAN仿真工具虽能准确复现真实场景，但计算开销和仿真时间巨大，影响了传输参数的快速优化和网络性能评估。

Method: 本文提出FAST-LoRa，一个新型仿真框架。它依赖分析模型而非复杂数据包级模拟来精简计算，并通过高效矩阵运算实现网关接收。FAST-LoRa旨在作为一种轻量级、准确的近似工具，特别适用于稳定流量模式和上行通信场景。

Result: FAST-LoRa在估计关键网络指标（如PDR和EE）方面与现有成熟仿真器达到相似精度（PDR的平均绝对误差为0.940 × 10^-2，EE为0.040 bits/mJ），即使在有干扰和多网关接收的复杂场景下，同时将计算时间显著减少了多达三个数量级。

Conclusion: FAST-LoRa为LoRaWAN网络的传输参数评估提供了一个快速、高效且准确的近似工具，显著提升了网络优化和性能评估的效率。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [225] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出了一种适用于无线设备的双模（拉取/查询驱动和推送/事件驱动）通信框架，该框架集成了唤醒无线电机制和定制MAC协议，旨在提高能效并确保常规和关键数据的及时、可靠传输。


<details>
  <summary>Details</summary>
Motivation: 现有无线设备在不同网络条件下，难以实现及时、情境感知且高效的数据传输。特别是需要在常规数据响应（拉取）与紧急异常报告（推送）之间进行有效集成，并同时追求高能效。

Method: 该研究引入了一个统一时间帧结构的双模通信框架，设备通常以拉取模式响应请求，但在检测到异常时，会抢占常规响应以推送关键状态。此外，主动推送用于发送关键数据。为实现高能效，集成了唤醒无线电机制，并设计了支持不同通信类别的定制媒体访问控制（MAC）协议。通过全面的系统级分析，评估了异常报告（推送流量）成功概率、查询响应（拉取流量）成功概率和总能耗三个关键性能指标。

Result: 数值结果表明，该系统在推送和拉取流量的成功概率之间存在固有的权衡，这取决于分配的通信资源。与传统方法相比，所提出的方法将能耗降低了高达30%，同时保持了对两种通信范式的可靠支持。

Conclusion: 该双模通信框架成功地将查询驱动和事件驱动传输集成在一起，显著降低了能耗，并能可靠地支持这两种通信模式。系统性能受资源分配影响，在推送和拉取流量的成功概率之间存在权衡，但整体表现优于传统方法。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [226] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文全面分析了内容相关时效性度量VAoI在单跳和多跳网络中不同调度策略下的稳态分布，并推导了闭式表达式和最优阈值，以指导高效网络设计。


<details>
  <summary>Details</summary>
Motivation: 现有信息时效性度量（如AoI）未能考虑内容的信息性。尽管已有内容相关度量，但其完整分布（尤其是在多跳网络中）尚未得到充分研究，现有工作主要关注平均值。本文旨在通过分析VAoI的稳态分布来弥补这一空白。

Method: 本文采用分析建模方法，在随机平稳、均匀和基于阈值的调度策略下，对VAoI在单跳和多跳网络中的稳态分布进行了全面分析，并考虑了传输约束。研究推导了闭式表达式，并解析确定了基于阈值调度的最优阈值。通过数值评估验证了理论分析的正确性。

Result: ['推导了在不同调度策略（随机平稳、均匀、基于阈值）下VAoI的稳态分布和平均VAoI的闭式表达式。', '解析确定了基于阈值的调度下最小化VAoI的最优阈值，并推导了相应的最优VAoI闭式表达式。', '数值评估验证了分析结果的准确性。']

Conclusion: 本文对VAoI稳态分布的全面分析为设计高效通信网络、有效利用基于内容的时效性度量提供了有价值的见解。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [227] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 该论文提出了一种超图辅助的可信任务资源匹配（TTR-matching）框架，旨在解决复杂互联系统中因计算资源和任务物理属性多样性而带来的任务-资源匹配挑战，并通过整合设备信任关系和物理属性，实现价值驱动的任务完成。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的物理属性多样化，在复杂的互联系统中开发有效的机制来促进任务与资源匹配，以实现面向价值的任务完成变得越来越具挑战性。

Method: 本文提出了一种网络化的物理计算系统，并引入了超图辅助的可信任务资源匹配（TTR-matching）框架。具体而言，首先定义了整合任务特定信任、资源物理属性和任务类型的“任务特定可信物理资源超图”，以准确建模设备协作依赖关系；其次，生成“任务超图”来关联任务发起者与任务的物理属性；最后，基于这两个超图设计了一种超图匹配算法，以促进任务特定的可信协作者选择和精确的任务资源匹配，从而最大化任务完成的价值。

Result: 广泛的实验结果表明，所提出的TTR-matching框架在识别特定任务的可信协作者和最大化任务完成的平均价值方面，优于现有的对比算法。

Conclusion: TTR-matching框架通过整合物理属性和信任关系，并利用超图进行建模和匹配，有效解决了复杂互联系统中任务资源匹配的难题，显著提升了任务完成的价值并优化了可信协作者的选择。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [228] [A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection](https://arxiv.org/abs/2507.22908)
*Abhishek Sawaika,Swetang Krishna,Tushar Tomar,Durga Pritam Suggisetti,Aditi Lal,Tanmaya Shrivastav,Nouhaila Innan,Muhammad Shafique*

Main category: q-fin.CP

TL;DR: 该研究提出一种结合量子增强型LSTM与隐私保护技术（FedRansel）的联邦学习框架，旨在提升金融欺诈检测的准确性和数据安全。


<details>
  <summary>Details</summary>
Motivation: 随着数字交易的快速增长，欺诈活动日益猖獗，传统检测方法已无法有效应对，金融行业急需更先进的欺诈检测方案。

Method: 引入一种专用的联邦学习框架。该框架独特地结合了量子增强型长短期记忆（LSTM）模型，以及先进的隐私保护技术。其中，“FedRansel”是一种新颖的方法，旨在防御投毒和推断攻击。

Result: 与传统模型相比，该方法通过整合量子层，在关键评估指标上实现了约5%的性能提升。与标准差分隐私机制相比，“FedRansel”将模型降级和推断准确度降低了4-8%。

Conclusion: 该结合量子LSTM模型的伪集中式设置，能有效提高欺诈检测准确性，并显著增强敏感金融数据的安全性和保密性。

Abstract: Rapid growth of digital transactions has led to a surge in fraudulent
activities, challenging traditional detection methods in the financial sector.
To tackle this problem, we introduce a specialised federated learning framework
that uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model
with advanced privacy preserving techniques. By integrating quantum layers into
the LSTM architecture, our approach adeptly captures complex
cross-transactional patters, resulting in an approximate 5% performance
improvement across key evaluation metrics compared to conventional models.
Central to our framework is "FedRansel", a novel method designed to defend
against poisoning and inference attacks, thereby reducing model degradation and
inference accuracy by 4-8%, compared to standard differential privacy
mechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances
fraud detection accuracy and reinforces the security and confidentiality of
sensitive financial data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation](https://arxiv.org/abs/2507.23110)
*Zheyuan Zhang,Linkai Peng,Wanying Dou,Cuiling Sun,Halil Ertugrul Aktas,Andrea M. Bejar,Elif Keles,Gorkem Durak,Ulas Bagci*

Main category: eess.IV

TL;DR: 该研究引入了PancreasDG，一个大规模多中心3D MRI胰腺分割数据集，用于研究医学图像中的域泛化，特别关注跨序列变异性。它揭示了数据变异性的新见解，并提出了一种半监督方法，显著提升了胰腺分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有域泛化基准忽视了MR图像中T1和T2序列外观差异这一主要变异来源，且胰腺分割极具挑战性（体积小、形状不规则、对比度低），现有深度网络在此方面表现不佳。胰腺在公共跨域基准中代表性不足，但其临床意义重大。

Method: 构建了PancreasDG数据集，包含来自六个机构的563个MRI扫描，涵盖静脉期和异相序列，并采用双盲、两遍协议创建像素级精确的胰腺掩模。通过综合分析揭示了数据变异性见解，并提出了一种利用解剖不变性的半监督方法。

Result: 分析揭示了三点见解：(i)有限采样引入显著方差，可能被误认为是分布漂移；(ii)跨中心性能与相同序列的源域性能相关；(iii)跨序列漂移需要专门解决方案。所提出的半监督方法在跨序列分割方面，将Dice分数提高了61.63%，在两个测试中心达到87.00%，显著优于现有最先进的域泛化技术。PancreasDG为医学图像域泛化设定了新基准。

Conclusion: PancreasDG数据集填补了医学图像域泛化在序列变异性和胰腺分割方面的空白。研究深入揭示了域漂移和采样效应的见解，所提出的半监督方法为跨序列胰腺分割提供了卓越的解决方案，推动了医学图像域泛化领域的发展。

Abstract: Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences
whose appearance differs more than the acquisition sites that produce them.
Existing domain-generalization benchmarks focus almost on cross-center shifts
and overlook this dominant source of variability. Pancreas segmentation remains
a major challenge in abdominal imaging: the gland is small, irregularly,
surrounded by organs and fat, and often suffers from low T1 contrast.
State-of-the-art deep networks that already achieve >90% Dice on the liver or
kidneys still miss 20-30% of the pancreas. The organ is also systematically
under-represented in public cross-domain benchmarks, despite its clinical
importance in early cancer detection, surgery, and diabetes research. To close
this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas
segmentation dataset for investigating domain generalization in medical
imaging. The dataset comprises 563 MRI scans from six institutions, spanning
both venous phase and out-of-phase sequences, enabling study of both
cross-center and cross-sequence variations with pixel-accurate pancreas masks
created by a double-blind, two-pass protocol. Through comprehensive analysis,
we reveal three insights: (i) limited sampling introduces significant variance
that may be mistaken for distribution shifts, (ii) cross-center performance
correlates with source domain performance for identical sequences, and (iii)
cross-sequence shifts require specialized solutions. We also propose a
semi-supervised approach that leverages anatomical invariances, significantly
outperforming state-of-the-art domain generalization techniques with 61.63%
Dice score improvements and 87.00% on two test centers for cross-sequence
segmentation. PancreasDG sets a new benchmark for domain generalization in
medical imaging. Dataset, code, and models will be available at
https://pancreasdg.netlify.app.

</details>


### [230] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

TL;DR: 本研究开发了一个初步框架，利用Harmonized Landsat Sentinel 10m(HLS10)作为参考，对Harmonized Landsat Sentinel 30m(HLS30)图像进行超分辨率处理，以弥合不同卫星传感器之间的分辨率差距。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像对地理空间分析至关重要，但不同卫星传感器之间的空间分辨率差异给数据融合和下游应用带来了挑战。现有超分辨率方法依赖人工下采样图像，不适用于光谱和时间特性不同的异构卫星传感器。

Method: 本研究开发了一个初步框架，以HLS10图像为参考，对HLS30图像进行对齐和协调处理，旨在弥合这些传感器之间的分辨率差距，并提高超分辨率Landsat图像的质量。

Result: 定量和定性评估表明，该方法有效，并展示了其在增强卫星传感应用方面的潜力。

Conclusion: 本研究为异构卫星图像超分辨率的可行性提供了见解，并强调了该领域未来发展的主要考虑因素。

Abstract: High-resolution satellite imagery is essential for geospatial analysis, yet
differences in spatial resolution across satellite sensors present challenges
for data fusion and downstream applications. Super-resolution techniques can
help bridge this gap, but existing methods rely on artificially downscaled
images rather than real sensor data and are not well suited for heterogeneous
satellite sensors with differing spectral, temporal characteristics. In this
work, we develop a preliminary framework to align and Harmonized Landsat
Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a
reference from the HLS dataset. Our approach aims to bridge the resolution gap
between these sensors and improve the quality of super-resolved Landsat
imagery. Quantitative and qualitative evaluations demonstrate the effectiveness
of our method, showing its potential for enhancing satellite-based sensing
applications. This study provides insights into the feasibility of
heterogeneous satellite image super-resolution and highlights key
considerations for future advancements in the field.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [231] [Evaluating LLMs for Visualization Generation and Understanding](https://arxiv.org/abs/2507.22890)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.HC

TL;DR: 分析了大型语言模型（LLMs）在生成可视化代码和理解信息可视化方面的能力与局限性。


<details>
  <summary>Details</summary>
Motivation: 信息可视化在复杂数据洞察中发挥重要作用，而LLMs近期在多项任务中表现出色。本研究旨在探讨LLMs在信息可视化领域的应用潜力。

Method: 通过简单提示测试不同主流LLMs生成可视化代码的能力，并通过回答问题分析LLMs理解常见可视化的能力。

Result: 研究发现LLMs能生成简单可视化（如柱状图和饼图）的代码，并能回答简单的可视化问题。然而，LLMs在生成复杂可视化（如小提琴图）时存在困难，且在回答涉及近距离边界关系识别和形状长度判断等问题时会出现错误。

Conclusion: 本研究的见解有助于改进大型语言模型和信息可视化系统。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
questions. Our study shows that LLMs could generate code for some simpler
visualizations such as bar and pie charts. Moreover, they could answer simple
questions about visualizations. However, LLMs also have several limitations.
For example, some of them had difficulty generating complex visualizations,
such as violin plot. LLMs also made errors in answering some questions about
visualizations, for example, identifying relationships between close boundaries
and determining lengths of shapes. We believe that our insights can be used to
improve both LLMs and Information Visualization systems.

</details>


### [232] [Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure](https://arxiv.org/abs/2507.22893)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: 该论文提出“认知基础设施研究”（CIS），旨在分析人工智能系统（作为认知基础设施）如何通过自动化相关性判断和转移认知主体性，在无意识层面重塑人类认知和社会认识论。它提出跨学科方法，包括“基础设施故障方法”，以研究这种隐形影响。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互研究忽视了AI系统在无意识层面如何根本性地重塑人类认知，这是理解分布式认知的关键盲点。此外，现有学科（认知科学、数字社会学、计算方法）在人口规模预处理分析、个体认知机制获取及文化传播动力学方面存在显著方法论空白。

Method: 1. 提出“认知基础设施研究”（CIS）这一跨学科新领域，将AI重构为“认知基础设施”。
2. 通过跨越个体（认知依赖）、集体（民主审议）和社会（治理）尺度的叙事场景，描述认知基础设施的影响。
3. 引入“基础设施故障方法”这一创新研究方法，即在习惯化后系统地撤回AI预处理，以揭示认知依赖。
4. 强调整合多元学科方法以研究AI预处理对分布式认知的影响。

Result: 1. AI系统作为“认知基础设施”，在无意识层面根本性地重塑人类认知。
2. 这些基础设施通过预见性个性化和适应性隐形特性运作，自动化“相关性判断”，并将“认知主体性中心”转移到非人类系统。
3. 它们在个体、集体和社会层面重塑了人类认知、公共推理和社会认识论，导致认知依赖。

Conclusion: 该论文为理解和应对AI作为认知基础设施对分布式认知、公共推理和社会认识论的无意识重塑作用，确立了“认知基础设施研究”（CIS）这一必要的跨学科领域。它提供了一个概念框架和方法论创新（如基础设施故障方法），以填补学科空白并研究这一先前被忽视的复杂现象。

Abstract: Contemporary human-AI interaction research overlooks how AI systems
fundamentally reshape human cognition pre-consciously, a critical blind spot
for understanding distributed cognition. This paper introduces "Cognitive
Infrastructure Studies" (CIS) as a new interdisciplinary domain to
reconceptualize AI as "cognitive infrastructures": foundational, often
invisible systems conditioning what is knowable and actionable in digital
societies. These semantic infrastructures transport meaning, operate through
anticipatory personalization, and exhibit adaptive invisibility, making their
influence difficult to detect. Critically, they automate "relevance judgment,"
shifting the "locus of epistemic agency" to non-human systems. Through
narrative scenarios spanning individual (cognitive dependency), collective
(democratic deliberation), and societal (governance) scales, we describe how
cognitive infrastructures reshape human cognition, public reasoning, and social
epistemologies. CIS aims to address how AI preprocessing reshapes distributed
cognition across individual, collective, and cultural scales, requiring
unprecedented integration of diverse disciplinary methods. The framework also
addresses critical gaps across disciplines: cognitive science lacks
population-scale preprocessing analysis capabilities, digital sociology cannot
access individual cognitive mechanisms, and computational approaches miss
cultural transmission dynamics. To achieve this goal CIS also provides
methodological innovations for studying invisible algorithmic influence:
"infrastructure breakdown methodologies", experimental approaches that reveal
cognitive dependencies by systematically withdrawing AI preprocessing after
periods of habituation.

</details>


### [233] [iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement](https://arxiv.org/abs/2507.22896)
*Kohou Wang,ZhaoXiang Liu,Lin Bai,Kun Fan,Xiang Liu,Huan Hu,Kai Wang,Shiguo Lian*

Main category: cs.HC

TL;DR: 本文提出一个基于多模态大语言模型（MLLM）的交互式学习机器人系统，使其能通过与非专家用户自然对话学习，并利用提问链和双模态检索机制避免重复犯错，从而提高机器人部署后的适应性。


<details>
  <summary>Details</summary>
Motivation: 机器人部署后不可避免地会遇到新颖场景，因此其性能在部署后能够持续提升至关重要。

Method: 提出了一个基于多模态大语言模型（MLLM）的交互式学习机器人系统。该系统能够通过与非专家用户进行自然对话进行学习，并引入“提问链”以澄清用户意图，同时采用“双模态检索模块”来利用交互事件避免重复犯错，以确保在模型更新前也能提供流畅的用户体验。

Result: 通过定量和定性实验，证明了所提出的方法的有效性和性能提升。

Conclusion: 该系统通过集成交互式学习，为机器人领域提供了一种新颖方法，有望显著提高机器人在多样化环境中的适应性和性能。

Abstract: It is crucial that robots' performance can be improved after deployment, as
they are inherently likely to encounter novel scenarios never seen before. This
paper presents an innovative solution: an interactive learning-based robot
system powered by a Multi-modal Large Language Model(MLLM). A key feature of
our system is its ability to learn from natural dialogues with non-expert
users. We also propose chain of question to clarify the exact intent of the
question before providing an answer and dual-modality retrieval modules to
leverage these interaction events to avoid repeating same mistakes, ensuring a
seamless user experience before model updates, which is in contrast to current
mainstream MLLM-based robotic systems. Our system marks a novel approach in
robotics by integrating interactive learning, paving the way for superior
adaptability and performance in diverse environments. We demonstrate the
effectiveness and improvement of our method through experiments, both
quantitively and qualitatively.

</details>


### [234] [RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems](https://arxiv.org/abs/2507.22897)
*Luyu Chen,Quanyu Dai,Zeyu Zhang,Xueyang Feng,Mingyu Zhang,Pengcheng Tang,Xu Chen,Yue Zhu,Zhenhua Dong*

Main category: cs.HC

TL;DR: 提出RecUserSim，一个基于LLM的对话推荐系统用户模拟器，旨在提升模拟真实性、多样性并提供显式评分，以解决CRS评估难题。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）的评估具有挑战性。尽管用户模拟器可提供全面评估，但构建真实多样的模拟器很困难。现有基于大型语言模型（LLM）的模拟器在模拟真实个体用户、多样场景及提供明确评分机制方面仍有不足。

Method: 本文提出RecUserSim，一个基于LLM代理的用户模拟器。其核心模块包括：用于定义多样用户角色的“档案模块”；追踪历史和发现偏好的“记忆模块”；受有限理性理论启发、实现精细决策和个性化响应的“核心行动模块”；以及用于微调最终响应的“优化模块”。

Result: 实验证明，RecUserSim能生成多样、可控且高质量的对话，即使使用较小的LLM。其生成的评分在不同基础LLM之间表现出高一致性。

Conclusion: RecUserSim通过提升模拟的真实性与多样性，并提供一致的显式评分，有效解决了对话推荐系统的评估难题。

Abstract: Conversational recommender systems (CRS) enhance user experience through
multi-turn interactions, yet evaluating CRS remains challenging. User
simulators can provide comprehensive evaluations through interactions with CRS,
but building realistic and diverse simulators is difficult. While recent work
leverages large language models (LLMs) to simulate user interactions, they
still fall short in emulating individual real users across diverse scenarios
and lack explicit rating mechanisms for quantitative evaluation. To address
these gaps, we propose RecUserSim, an LLM agent-based user simulator with
enhanced simulation realism and diversity while providing explicit scores.
RecUserSim features several key modules: a profile module for defining
realistic and diverse user personas, a memory module for tracking interaction
history and discovering unknown preferences, and a core action module inspired
by Bounded Rationality theory that enables nuanced decision-making while
generating more fine-grained actions and personalized responses. To further
enhance output control, a refinement module is designed to fine-tune final
responses. Experiments demonstrate that RecUserSim generates diverse,
controllable outputs and produces realistic, high-quality dialogues, even with
smaller base LLMs. The ratings generated by RecUserSim show high consistency
across different base LLMs, highlighting its effectiveness for CRS evaluation.

</details>


### [235] [Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants](https://arxiv.org/abs/2507.22900)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 本研究探讨AI代码助手对初学者编程经验的影响，发现其初期有益，但可能导致过度依赖和知识迁移困难。


<details>
  <summary>Details</summary>
Motivation: 探究AI代码助手如何影响初级程序员在入门编程课程中的学习和考试体验。

Method: 一项探索性研究，20名学生参与两部分考试：第一部分可使用AI，第二部分禁用AI。通过李克特量表和开放式问题收集学生对AI的感知和遇到的挑战。

Result: 研究发现，AI工具被认为有助于理解代码和提高信心，尤其是在初步开发阶段。然而，学生在无AI辅助任务中知识迁移困难，暴露出过度依赖和概念理解不足。

Conclusion: 研究结果强调了教学策略的重要性，即如何在有意义地整合AI的同时，加强编程基础技能的培养。

Abstract: This exploratory study examines how AI code assistants shape novice
programmers' experiences during a two-part exam in an introductory programming
course. In the first part, students completed a programming task with access to
AI support; in the second, they extended their solutions without AI. We
collected Likert-scale and open-ended responses from 20 students to evaluate
their perceptions and challenges. Findings suggest that AI tools were perceived
as helpful for understanding code and increasing confidence, particularly
during initial development. However, students reported difficulties
transferring knowledge to unaided tasks, revealing possible overreliance and
gaps in conceptual understanding. These insights highlight the need for
pedagogical strategies that integrate AI meaningfully while reinforcing
foundational programming skills.

</details>


### [236] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)
*Hashim Hayat,Maksim Kudrautsau,Evgeniy Makarov,Vlad Melnichenko,Tim Tsykunou,Piotr Varaksin,Matt Pavelle,Adam Z. Oskowitz*

Main category: cs.HC

TL;DR: 一项研究发现，多智能体LLM驱动的AI系统在虚拟急诊护理中，其诊断和治疗决策与人类医生高度一致，甚至在某些情况下表现更优，有望缓解医疗人员短缺。


<details>
  <summary>Details</summary>
Motivation: 全球医疗从业人员短缺和行政负担是严峻挑战。AI有望缓解这些问题，但缺乏在真实临床实践中对端到端自主大型语言模型（LLM）AI系统的严格评估。

Method: 研究回顾性比较了多智能体AI系统Doctronic与认证临床医生在500次连续紧急护理远程医疗问诊中的表现。通过盲法LLM裁决和专家人工审查，评估了诊断一致性、治疗计划一致性和安全性指标。

Result: Doctronic的顶级诊断与临床医生在81%的病例中匹配，治疗计划在99.2%的病例中保持一致。未发生临床幻觉。在不一致病例的专家审查中，AI性能在36.1%的病例中更优，人类性能在9.3%的病例中更优。

Conclusion: 该首次大规模验证表明，自主AI医生在诊断和治疗计划上与人类临床医生高度一致，其表现与人类医生相当甚至超越。多智能体AI系统可提供与人类提供者相当的临床决策，为解决医疗劳动力短缺提供潜在解决方案。

Abstract: Background: Globally we face a projected shortage of 11 million healthcare
practitioners by 2030, and administrative burden consumes 50% of clinical time.
Artificial intelligence (AI) has the potential to help alleviate these
problems. However, no end-to-end autonomous large language model (LLM)-based AI
system has been rigorously evaluated in real-world clinical practice. In this
study, we evaluated whether a multi-agent LLM-based AI framework can function
autonomously as an AI doctor in a virtual urgent care setting. Methods: We
retrospectively compared the performance of the multi-agent AI system Doctronic
and board-certified clinicians across 500 consecutive urgent-care telehealth
encounters. The primary end points: diagnostic concordance, treatment plan
consistency, and safety metrics, were assessed by blinded LLM-based
adjudication and expert human review. Results: The top diagnosis of Doctronic
and clinician matched in 81% of cases, and the treatment plan aligned in 99.2%
of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not
supported by clinical findings). In an expert review of discordant cases, AI
performance was superior in 36.1%, and human performance was superior in 9.3%;
the diagnoses were equivalent in the remaining cases. Conclusions: In this
first large-scale validation of an autonomous AI doctor, we demonstrated strong
diagnostic and treatment plan concordance with human clinicians, with AI
performance matching and in some cases exceeding that of practicing clinicians.
These findings indicate that multi-agent AI systems achieve comparable clinical
decision-making to human providers and offer a potential solution to healthcare
workforce shortages.

</details>


### [237] [SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches](https://arxiv.org/abs/2507.22904)
*Ehsan Latif,Zirak Khan,Xiaoming Zhai*

Main category: cs.HC

TL;DR: SketchMind是一个认知驱动的多智能体框架，用于自动化评估和改进学生绘制的科学草图，其性能优于现有基线模型，并能有效支持概念发展。


<details>
  <summary>Details</summary>
Motivation: 学生科学草图评估缺乏可解释性、教学对齐性和认知水平的适应性。现有的AI解决方案（如图像分类或单一视觉-语言模型）无法有效解决这些挑战。

Method: 本文提出了SketchMind，一个认知驱动的多智能体框架。它包含多个模块化智能体，分别负责评分标准解析、草图感知、认知对齐以及通过草图修改提供迭代反馈，从而实现个性化和透明的评估。

Result: SketchMind在包含3575张学生草图的数据集上进行了评估。相较于无SRG的GPT-4o基线（平均准确率55.6%），整合SRG后准确率提升至77.1%（平均绝对增益21.4%）。多智能体协同与SRG结合进一步提升性能，例如GPT-4.1在草图预测准确率上平均提升8.9%。人类评估者对SketchMind（使用GPT-4.1）生成的反馈和共同创建的草图评价更高（平均4.1/5），显著优于基线模型（如GPT-4o的2.3/5）。

Conclusion: SketchMind通过其认知驱动的多智能体框架，显著提升了科学草图的自动化评估和改进能力。专家认为该系统通过引导性修订，具有有效支持学生概念成长的潜力。

Abstract: Scientific sketches (e.g., models) offer a powerful lens into students'
conceptual understanding, yet AI-powered automated assessment of such
free-form, visually diverse artifacts remains a critical challenge. Existing
solutions often treat sketch evaluation as either an image classification task
or monolithic vision-language models, which lack interpretability, pedagogical
alignment, and adaptability across cognitive levels. To address these
limitations, we present SketchMind, a cognitively grounded, multi-agent
framework for evaluating and improving student-drawn scientific sketches.
SketchMind comprises modular agents responsible for rubric parsing, sketch
perception, cognitive alignment, and iterative feedback with sketch
modification, enabling personalized and transparent evaluation. We evaluate
SketchMind on a curated dataset of 3,575 student-generated sketches across six
science assessment items with different highest order of Bloom's level that
require students to draw models to explain phenomena. Compared to baseline
GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG
integration achieves 77.1% average accuracy (+21.4% average absolute gain). We
also demonstrate that multi-agent orchestration with SRG enhances SketchMind
performance, for example, GPT-4.1 gains an average 8.9% increase in sketch
prediction accuracy, outperforming single-agent pipelines across all items.
Human evaluators rated the feedback and co-created sketches generated by
\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,
significantly higher than those of baseline models (e.g., 2.3 for GPT-4o).
Experts noted the system's potential to meaningfully support conceptual growth
through guided revision. Our code and (pending approval) dataset will be
released to support reproducibility and future research in AI-driven education.

</details>


### [238] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)
*Ismail Hossain,Mridul Banik*

Main category: cs.HC

TL;DR: 提出并评估一种结合脑机接口（BCI）和大型语言模型（LLM）的新型混合框架，旨在为神经系统疾病患者提供自适应的语言康复辅助。


<details>
  <summary>Details</summary>
Motivation: 传统的辅助沟通系统和语言学习平台难以实时适应中风失语症或肌萎缩侧索硬化等神经系统疾病患者的认知和语言需求。而基于非侵入式脑电图（EEG）的BCI能低疲劳捕获用户神经意图，基于Transformer的LLM能生成情境化语言内容，两者具有互补优势，可弥补现有系统的不足。

Method: 我们提出并正在评估一个新颖的混合框架。该框架利用实时EEG信号驱动一个由LLM支持的语言康复助手。该系统旨在通过意念指令导航语言学习模块，动态个性化词汇和练习，并监测认知负荷的神经标记以实时调整任务难度。

Result: 该系统旨在实现以下功能：1) 使严重言语或运动障碍的用户能够通过意念指令导航语言学习模块；2) 动态个性化词汇、句子构建练习和纠正性反馈；3) 监测认知负荷的神经标记以实时调整任务难度。

Conclusion: 该结合EEG-BCI和LLM的混合框架，为开发能够实时适应用户认知和语言需求的自适应语言康复系统提供了有前景的解决方案，尤其适用于神经系统疾病患者。

Abstract: Conventional augmentative and alternative communication (AAC) systems and
language-learning platforms often fail to adapt in real time to the user's
cognitive and linguistic needs, especially in neurological conditions such as
post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in
noninvasive electroencephalography (EEG)--based brain-computer interfaces
(BCIs) and transformer--based large language models (LLMs) offer complementary
strengths: BCIs capture users' neural intent with low fatigue, while LLMs
generate contextually tailored language content. We propose and evaluate a
novel hybrid framework that leverages real-time EEG signals to drive an
LLM-powered language rehabilitation assistant. This system aims to: (1) enable
users with severe speech or motor impairments to navigate language-learning
modules via mental commands; (2) dynamically personalize vocabulary,
sentence-construction exercises, and corrective feedback; and (3) monitor
neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [239] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)
*Julian Acosta,Scott Adams,Julius Kernbach,Romain Hardy,Sung Eun Kim,Luyang Luo,Xiaoman Zhang,Shreya Johri,Mohammed Baharoon,Pranav Rajpurkar*

Main category: cs.HC

TL;DR: 开发了一种语音AI系统，用于指导非专业人员进行中风评估，初步结果显示有潜力，但目前仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 紧急护理中，急救人员对中风识别不一致且不准确，导致治疗延误，存在关键缺口。

Method: 开发了语音驱动AI系统，结合智能手机视频捕捉，指导用户进行中风评估。三名非医疗志愿者用此系统评估十名模拟患者，测量诊断准确性、完成时间、用户信心及专家医师对AI报告的审查。

Result: AI系统识别出84%中风迹象，75% LVO，86%真实中风（但有2/3非中风案例被误判）。评估平均6分钟。用户信心和易用性高。专家医师结合视频审查报告时，100%正确诊断，但仅40%有信心做出初步治疗决策，因存在AI错误。

Conclusion: 当前系统有局限性，需人工监督。但未来语音AI技术进步有望实现高准确评估，使专业级评估能力普及，变革急诊医疗。

Abstract: We developed a voice-driven artificial intelligence (AI) system that guides
anyone - from paramedics to family members - through expert-level stroke
evaluations using natural conversation, while also enabling smartphone video
capture of key examination components for documentation and potential expert
review. This addresses a critical gap in emergency care: current stroke
recognition by first responders is inconsistent and often inaccurate, with
sensitivity for stroke detection as low as 58%, causing life-threatening delays
in treatment. Three non-medical volunteers used our AI system to assess ten
simulated stroke patients, including cases with likely large vessel occlusion
(LVO) strokes and stroke-like conditions, while we measured diagnostic
accuracy, completion times, user confidence, and expert physician review of the
AI-generated reports. The AI system correctly identified 84% of individual
stroke signs and detected 75% of likely LVOs, completing evaluations in just
over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use
(mean 4.67/5). The system successfully identified 86% of actual strokes but
also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert
physician reviewed the AI reports with videos, they identified the correct
diagnosis in 100% of cases, but felt confident enough to make preliminary
treatment decisions in only 40% of cases due to observed AI errors including
incorrect scoring and false information. While the current system's limitations
necessitate human oversight, ongoing rapid advancements in speech-to-speech AI
models suggest that future versions are poised to enable highly accurate
assessments. Achieving human-level voice interaction could transform emergency
medical care, putting expert-informed assessment capabilities in everyone's
hands.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [240] [AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads](https://arxiv.org/abs/2507.23084)
*Taiyi Wang,Eiko Yoneki*

Main category: cs.DB

TL;DR: AutoIndexer是一个结合工作负载压缩、查询优化和强化学习的框架，能有效扩展索引选择，显著提升数据库性能并缩短调优时间。


<details>
  <summary>Details</summary>
Motivation: 索引选择对数据库性能至关重要，尤其对于大规模分析型工作负载。现有基于强化学习的索引推荐系统难以适应扩展的工作负载，原因在于动作空间呈指数级增长和试错成本高昂。

Method: 本文提出了AutoIndexer框架，通过结合工作负载压缩、查询优化和专门的强化学习模型来有效扩展索引选择。它通过操作压缩后的工作负载，大幅降低搜索复杂性，同时保持索引质量。

Result: AutoIndexer将端到端查询执行时间比无索引基线减少高达95%。与最先进的基于强化学习的索引推荐系统相比，它在工作负载成本节约方面平均提高了约20%，同时将调优时间缩短了50%以上。

Conclusion: AutoIndexer对大型和多样化的工作负载具有实用性，能有效解决索引选择的扩展性挑战。

Abstract: Efficiently selecting indexes is fundamental to database performance
optimization, particularly for systems handling large-scale analytical
workloads. While deep reinforcement learning (DRL) has shown promise in
automating index selection through its ability to learn from experience, few
works address how these RL-based index advisors can adapt to scaling workloads
due to exponentially growing action spaces and heavy trial and error. To
address these challenges, we introduce AutoIndexer, a framework that combines
workload compression, query optimization, and specialized RL models to scale
index selection effectively. By operating on compressed workloads, AutoIndexer
substantially lowers search complexity without sacrificing much index quality.
Extensive evaluations show that it reduces end-to-end query execution time by
up to 95% versus non-indexed baselines. On average, it outperforms
state-of-the-art RL-based index advisors by approximately 20% in workload cost
savings while cutting tuning time by over 50%. These results affirm
AutoIndexer's practicality for large and diverse workloads.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [241] [Typing Tensor Calculus in 2-Categories (I)](https://arxiv.org/abs/1908.01212)
*Fatimah Rita Ahmadi*

Main category: math.CT

TL;DR: 本文使用范畴论（特别是2-范畴）对线性代数和高阶张量进行无索引、类型化的形式化，旨在为高效算法、函数式编程和并行计算提供基础。


<details>
  <summary>Details</summary>
Motivation: 为线性代数计算开发高效算法，构建适用于函数式编程语言的框架，并实现更快的并行化计算。

Method: 将矩阵视为矩阵范畴中的态射，并将此框架推广至任意幺半加性范畴。为容纳高阶张量，定义了半加性2-范畴，其中矩阵表示为1-态射，四指标张量表示为2-态射。进一步将该框架扩展到幺半加性2-范畴，并在2Vec 2-范畴中演示了详细操作和向量化。

Result: 提供了一个无索引、类型化的线性代数框架，涵盖了矩阵和最多四个索引的张量。在该2-范畴中成功演示了详细的操作和向量化。

Conclusion: 通过范畴论（特别是2-范畴）的形式化，成功构建了一个统一、无索引、类型化的线性代数和张量框架，为未来开发高效、并行化算法奠定了基础。

Abstract: To formalize calculations in linear algebra for the development of efficient
algorithms and a framework suitable for functional programming languages and
faster parallelized computations, we adopt an approach that treats elements of
linear algebra, such as matrices, as morphisms in the category of matrices,
$\mathbf{Mat_{k}}$. This framework is further extended by generalizing the
results to arbitrary monoidal semiadditive categories. To enrich this
perspective and accommodate higher-rank matrices (tensors), we define
semiadditive 2-categories, where matrices $T_{ij}$ are represented as
1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This
formalization provides an index-free, typed linear algebra framework that
includes matrices and tensors with up to four indices. Furthermore, we extend
the framework to monoidal semiadditive 2-categories and demonstrate detailed
operations and vectorization within the 2-category of 2Vec introduced by
Kapranov and Voevodsky.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [242] [SmartCourse: A Contextual AI-Powered Course Advising System for Undergraduates](https://arxiv.org/abs/2507.22946)
*Yixuan Mi,Yiduo Yu,Yiyi Zhao*

Main category: cs.CY

TL;DR: SmartCourse是一个整合课程管理和AI驱动的学业咨询系统，它结合学生成绩单和学位规划，利用本地大语言模型提供个性化课程推荐，并被证实能显著提升推荐相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统学业咨询工具的局限性，这些工具通常缺乏学生成绩单和学业规划等关键的个性化情境信息，导致提供的建议不够精准或情境感知不足。

Method: 开发了SmartCourse系统，该系统包括命令行接口(CLI)和Gradio网页图形用户界面(GUI)，用于管理用户账户、课程注册、成绩和四年学位计划。系统集成了一个本地部署的大语言模型（通过Ollama）来提供个性化课程推荐，并通过整合学生成绩单和专业规划来提供情境感知的建议。通过25个代表性咨询查询，使用PlanScore、PersonalScore、Lift和Recall等自定义指标评估了推荐质量。

Result: 实验结果显示，与缺乏情境信息的模式相比，使用完整的学生情境信息（包括成绩单和规划）能够生成显著更相关的推荐。这证实了成绩单和规划信息对于个性化学术咨询的必要性。

Conclusion: SmartCourse系统展示了如何通过结合对学生成绩单有感知能力的AI技术，有效增强学术规划的质量和个性化水平。

Abstract: We present SmartCourse, an integrated course management and AI-driven
advising system for undergraduate students (specifically tailored to the
Computer Science (CPS) major). SmartCourse addresses the limitations of
traditional advising tools by integrating transcript and plan information for
student-specific context. The system combines a command-line interface (CLI)
and a Gradio web GUI for instructors and students, manages user accounts,
course enrollment, grading, and four-year degree plans, and integrates a
locally hosted large language model (via Ollama) for personalized course
recommendations. It leverages transcript and major plan to offer contextual
advice (e.g., prioritizing requirements or retakes). We evaluated the system on
25 representative advising queries and introduced custom metrics: PlanScore,
PersonalScore, Lift, and Recall to assess recommendation quality across
different context conditions. Experiments show that using full context yields
substantially more relevant recommendations than context-omitted modes,
confirming the necessity of transcript and plan information for personalized
academic advising. SmartCourse thus demonstrates how transcript-aware AI can
enhance academic planning.

</details>


### [243] [ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios](https://arxiv.org/abs/2507.22947)
*Shou'ang Wei,Xinyun Wang,Shuzhen Bi,Jian Chen,Ruijia Li,Bo Jiang,Xin Lin,Min Zhang,Yu Song,BingDong Li,Aimin Zhou,Hao Hao*

Main category: cs.CY

TL;DR: 本文提出了ELMES，一个开源的自动化评估框架，专门用于在教育环境中评估大型语言模型（LLMs），通过模块化设计和LLM作为评判者的混合评估引擎，解决了当前教育场景中LLM评估指标缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）为教育带来了变革性机遇，但面临挑战：教育场景的评估指标差异大，许多新兴场景缺乏合适指标，现有基准主要衡量通用智能而非教学能力。因此，需要一个专门评估LLMs教学能力的框架。

Method: 开发了ELMES，一个开源自动化评估框架。其模块化架构允许通过简单的配置文件创建动态多智能体对话，无需大量编程。框架采用混合评估引擎，利用“LLM作为评判者”的方法客观量化传统上主观的教学指标。通过与教育专家合作开发的细粒度指标，在知识点解释、引导式问题解决教学、跨学科课程计划生成和情境化问题生成四个关键教育场景中，对最先进的LLMs进行了系统性基准测试。

Result: 研究结果显示，不同模型的能力分布存在明显差异，揭示了它们在特定情境下的优势和局限性。

Conclusion: ELMES为教育工作者和研究人员提供了一个易于使用的评估框架，显著降低了各种教育应用的适应障碍，并推动了LLMs在教学实践中的实际应用。

Abstract: The emergence of Large Language Models (LLMs) presents transformative
opportunities for education, generating numerous novel application scenarios.
However, significant challenges remain: evaluation metrics vary substantially
across different educational scenarios, while many emerging scenarios lack
appropriate assessment metrics. Current benchmarks predominantly measure
general intelligence rather than pedagogical capabilities. To address this gap,
we introduce ELMES, an open-source automated evaluation framework specifically
designed for assessing LLMs in educational settings. ELMES features a modular
architecture that enables researchers to create dynamic, multi-agent dialogues
through simple configuration files, facilitating flexible scenario design
without requiring extensive programming expertise. The framework incorporates a
hybrid evaluation engine that objectively quantifies traditionally subjective
pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic
benchmarking of state-of-the-art LLMs across four critical educational
scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching,
Interdisciplinary Lesson Plan Generation, and Contextualized Question
Generation, employing fine-grained metrics developed in collaboration with
education specialists. Our results demonstrate distinct capability
distributions among models, revealing context-specific strengths and
limitations. ELMES provides educators and researchers with an accessible
evaluation framework that significantly reduces adaptation barriers for diverse
educational applications while advancing the practical implementation of LLMs
in pedagogy. The framework is publicly available at
\emph{https://github.com/sii-research/elmes.git}.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [244] [DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver](https://arxiv.org/abs/2507.22906)
*Bin Deng,Jiatong Bai,Feilong Zhao,Zuming Xie,Maolin Li,Yan Wang,Feng Shu*

Main category: eess.SP

TL;DR: 本文提出一个针对异构混合模拟-数字（H2AD）MIMO结构的多发射源数量和方向的两阶段智能感知框架，包括三种目标数量感知方法（改进EDC、增强DNN、改进1D-CNN）和一种DOA估计方法（OMC-DOA），并验证了其在不同信噪比和多源环境下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 大规模全数字MIMO在未来无线网络中面临高能耗、高电路成本和高复杂性三大挑战，H2AD MIMO作为一种绿色替代方案潜力巨大。然而，如何通过H2AD结构智能感知多发射源的数量和方向仍是一个开放的难题。

Method: 提出一个两阶段感知框架，用于联合估计多目标数量和方向。具体方法包括：
1.  **目标数量感知：** 设计了三种方法：改进的特征域聚类（EDC）框架、基于五种关键统计特征的增强型深度神经网络（DNN），以及利用全部特征值的改进一维卷积神经网络（1D-CNN）。
2.  **DOA估计：** 引入在线微聚类（OMC-DOA）方法，实现低复杂度、高精度的DOA估计。
3.  **理论基准：** 推导了多源条件下H2AD的克拉默-劳下界（CRLB）作为理论性能基准。

Result: 1.  在目标数量感知方面，开发的三种方法在中高信噪比（SNR）下均能达到100%的感知准确率；改进的1D-CNN在极低SNR条件下表现更优越。
2.  在DOA估计方面，引入的OMC-DOA方法在多源环境下优于现有聚类和融合DOA方法。

Conclusion: 本文提出的两阶段感知框架有效解决了H2AD MIMO中多发射源的数量和方向智能感知问题。特别是改进的1D-CNN在极低SNR下表现出色，而OMC-DOA在多源环境下展示了优越的DOA估计性能。

Abstract: As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO
architecture has been shown to own a great potential to replace the massive or
extremely large-scale fully-digital MIMO in the future wireless networks to
address the three challenging problems faced by the latter: high energy
consumption, high circuit cost, and high complexity. However, how to
intelligently sense the number and direction of multi-emitters via such a
structure is still an open hard problem. To address this, we propose a
two-stage sensing framework that jointly estimates the number and direction
values of multiple targets. Specifically, three target number sensing methods
are designed: an improved eigen-domain clustering (EDC) framework, an enhanced
deep neural network (DNN) based on five key statistical features, and an
improved one-dimensional convolutional neural network (1D-CNN) utilizing full
eigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is
achieved via the introduced online micro-clustering (OMC-DOA) method.
Furthermore, we derive the Cram\'er-Rao lower bound (CRLB) for the H2AD under
multiple-source conditions as a theoretical performance benchmark. Simulation
results show that the developed three methods achieve 100\% number of targets
sensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior
under extremely-low SNR conditions. The introduced OMC-DOA outperforms existing
clustering and fusion-based DOA methods in multi-source environments.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [245] [From Propagator to Oscillator: The Dual Role of Symmetric Differential Equations in Neural Systems](https://arxiv.org/abs/2507.22916)
*Kun Jiang*

Main category: cs.NE

TL;DR: 本文深入研究了一种基于对称微分方程的神经元模型，揭示其兼具可靠信号传播和自激振荡信号生成的功能二重性，并通过参数调整和外部信号控制，为神经形态工程应用提供了理论基础和功能路线图。


<details>
  <summary>Details</summary>
Motivation: 在前人工作基础上，旨在深入探索所提出神经元模型的内在动力学和功能多样性，以期拓展其在神经形态工程中的应用潜力，并与生物神经元的双重作用建立联系。

Method: 通过系统探索参数空间并运用多种数学分析工具，理论上揭示了模型的性质。引入了一种名为“路途能量”（on-road energy）的新型中间态度量，用于仿真期间的状态监控和预测。通过仿真实验验证了模型功能模式转换的可控性。

Result: 研究发现该模型具有功能二重性：渐近稳定模式（可靠信号传播器）和Lyapunov稳定模式（持续自激振荡，作为信号生成器）。引入了“路途能量”指标以有效监控和预测系统状态。仿真结果确认，通过参数调整或连接结构修改可以诱导两种功能模式之间的转换。此外，外部信号能有效抑制振荡。

Conclusion: 这些发现与生物神经元在信息传输和节律生成中的双重作用具有显著相似性，从而为该模型在神经形态工程中的广泛应用奠定了坚实的理论基础和清晰的功能路线图。

Abstract: In our previous work, we proposed a novel neuron model based on symmetric
differential equations and demonstrated its potential as an efficient signal
propagator. Building upon that foundation, the present study delves deeper into
the intrinsic dynamics and functional diversity of this model. By
systematically exploring the parameter space and employing a range of
mathematical analysis tools, we theoretically reveal the system 's core
property of functional duality. Specifically, the model exhibits two distinct
trajectory behaviors: one is asymptotically stable, corresponding to a reliable
signal propagator; the other is Lyapunov stable, characterized by sustained
self-excited oscillations, functioning as a signal generator. To enable
effective monitoring and prediction of system states during simulations, we
introduce a novel intermediate-state metric termed on-road energy. Simulation
results confirm that transitions between the two functional modes can be
induced through parameter adjustments or modifications to the connection
structure. Moreover, we show that oscillations can be effectively suppressed by
introducing external signals. These findings draw a compelling parallel to the
dual roles of biological neurons in both information transmission and rhythm
generation, thereby establishing a solid theoretical basis and a clear
functional roadmap for the broader application of this model in neuromorphic
engineering.

</details>
