<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 70]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 14]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 22]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: 本文介绍了PEACH，一个用于医疗领域的句子对齐的英阿平行语料库。


<details>
  <summary>Details</summary>
Motivation: 为对比语言学、翻译研究和自然语言处理提供“黄金标准”语料库，具体支持派生双语词典、领域特定机器翻译模型适应、医疗机器翻译用户感知评估、患者信息材料可读性评估，以及作为翻译研究的教育资源。

Method: 构建了一个句子对齐的英阿平行语料库，数据来源于医疗保健文本（包括患者信息传单和教育材料）。该语料库是手动对齐的，旨在成为“黄金标准”资源。

Result: 成功创建了PEACH语料库，包含51,671个平行句子，共计约590,517个英语单词和567,707个阿拉伯语单词，平均句长在9.52到11.83个单词之间。该语料库已公开可访问。

Conclusion: PEACH是一个高质量、手动对齐的英阿医疗文本平行语料库，为多领域研究和应用提供了宝贵的“黄金标准”资源，并已公开。

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在带来内容创作革命的同时，也存在生成有害内容的风险。本综述系统回顾了LLM的安全问题、攻击策略和防御方法，并提出了统一的危害与防御分类法，同时指出了当前评估方法的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在提供内容生成等强大功能的同时，也可能无意或有意地产生有毒、冒犯或偏见内容，这种双重特性构成了一个紧迫的社会技术挑战，亟需对LLM相关的安全风险及应对策略进行深入分析和系统审查。

Method: 本文采用综述研究方法，系统回顾了LLM的无意毒性、对抗性越狱攻击和内容审核技术等方面的最新研究。具体包括：提出了LLM相关危害与防御的统一分类法；分析了新兴的多模态和LLM辅助越狱策略；评估了包括人类反馈强化学习（RLHF）、提示工程和安全对齐在内的缓解措施。

Result: 通过综合分析，本综述揭示了LLM安全领域不断演变的格局，识别出当前评估方法中的局限性，并为未来研究指明了方向。

Conclusion: LLMs既是强大的工具，也是潜在的有害语言来源。为指导健壮且符合伦理的语言技术开发，需要持续深入地研究LLM的安全问题，并改进评估方法。

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的幻觉问题对对话系统构成挑战。本文提出FineDialFact基准，用于细粒度对话事实核查，并表明思维链（CoT）推理有助于提高性能，但任务仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）会产生幻觉，这对对话系统等NLP应用构成重大挑战。现有幻觉检测方法过于简化，无法处理对话回应中事实的混合性，因此需要对对话回应中提取的原子事实进行细粒度验证。

Method: 引入了一个名为FineDialFact的基准，用于细粒度对话事实核查。基于公开可用的对话数据集构建了一个新的数据集，并通过提取原子事实来支持细粒度验证。使用多种基线方法对该数据集进行了评估，其中包括整合思维链（CoT）推理的方法。

Result: 实验结果表明，结合思维链（CoT）推理的方法可以提高对话事实核查的性能。然而，在开放域对话数据集HybriDialogue上的最佳F1分数仅为0.75。

Conclusion: FineDialFact基准仍是一项具有挑战性的任务，需要未来的进一步研究。该研究的数据集和代码将公开发布。

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [4] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: 研究发现，记忆局限性有助于Transformer模型学习语言，但会损害其对人类阅读行为的预测能力。


<details>
  <summary>Details</summary>
Motivation: 认知科学家认为人类短暂记忆有助于语言学习，但Transformer模型在没有此限制下也能有效学习语言，这引发了矛盾。本研究旨在验证记忆局限性对Transformer语言模型学习语言的潜在益处。

Method: 通过在受控实验中，对具有和不具有短暂记忆的Transformer语言模型进行训练，并在符合发展现实的训练集上评估其语言学习能力（通过语言建模性能和句法评估量化）及对人类阅读时间的预测能力。

Result: 研究发现，短暂记忆一致性地提高了语言学习能力（语言建模性能和句法评估），但出乎意料地损害了基于Surprisal对人类阅读时间的预测。后续分析表明，这种“更好的语言建模但更差的阅读时间预测”的差异无法用现有解释来解释。

Conclusion: 这些结果支持了记忆局限性对神经网络语言学习的益处，但不支持其在预测人类行为方面的益处。

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [5] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: 本研究比较了抑郁症评估中，直接基于评估文本训练的“镜像模型”与非评估文本训练的“非镜像模型”的性能，发现镜像模型存在标准污染导致的效应量虚高问题，而非镜像模型更具泛化潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM抑郁症评估模型常直接基于评估语言响应训练，导致“标准污染”，即预测分数部分依赖于预测因子本身，造成效应量虚高，降低模型泛化性。本研究旨在验证并比较这种污染效应。

Method: 招募110名参与者，完成结构化诊断访谈和生活史访谈。使用GPT-4、GPT-4o和LLaMA3-70B从两种访谈记录中预测抑郁症分数。比较基于结构化诊断数据的“镜像模型”和基于生活史数据的“非镜像模型”的性能（R2、与自评症状的相关性），并进行主题建模分析。

Result: 镜像模型（R2=.80）显示出极高的效应量，而非镜像模型（R2=.27）效应量较小但仍显著。然而，两者与自评抑郁症状的相关性相似（r=~.54），表明镜像模型可能因标准污染而存在偏差。主题建模揭示了不同模型和预测类型间的聚类。

Conclusion: 直接基于评估文本训练的抑郁症语言AI模型存在效应量虚高和泛化性差的问题。未来应更多地采用非镜像模型，以识别具有独特实用价值的可解释、可泛化语义特征，从而提高真实心理评估的有效性。

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [6] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: 本研究通过重新设定神经网络代理的涌现通信（EmCom）环境，引入小词汇量约束模拟双重发音，并构建屈折形态学设置，以探究其如何演化出类似人类语言的结构特性，并发现模拟语音约束对形态学结构的影响。


<details>
  <summary>Details</summary>
Motivation: 现有涌现通信研究主要侧重于一对一字符映射和句法组合，未能充分模拟人类语言中更复杂的特性，如双重发音和屈折形态。本研究旨在弥补此空白，通过重新诠释通用EmCom设置，使其能与自然语言通信方案进行有意义的比较。

Method: 重新诠释了属性-值重构游戏，施加小词汇量约束以模拟双重发音；构建了一个类似于自然屈折形态的新设置。开发了新指标，并探索了基于连接性和融合性的游戏变体。

Result: 实验发现，模拟的语音约束鼓励连接型形态的出现；涌现语言再现了自然语言融合语法属性的趋势。

Conclusion: 通过引入模拟语音约束等自然语言特性，涌现通信系统能够自发地发展出复杂的语言现象（如具有连接性和融合性的屈折形态），这为理解人类语言的本质提供了新见解。

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [7] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 现有LLM情感研究多局限于表层。本文引入CoRE基准，基于认知评估理论，探究LLMs如何进行深层情绪认知推理，发现不同模型推理模式各异。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型（LLMs）的情感相关研究，大多采用监督学习方式，评估和训练其表层情绪识别能力（如识别唤起或表达的情绪），使用离散情感标签，未能深入探究LLMs对情绪的认知推理过程。

Method: 本文超越表层情绪任务，利用认知评估理论（cognitive appraisal theory）来研究LLMs如何通过认知维度对情绪进行推理。为此，引入了一个大规模的认知情绪推理基准——CoRE，以评估LLMs在情绪推理中隐式使用的内部认知结构。研究通过大量实验分析了模型对特定认知评估维度的依赖性、认知维度对特定情绪的刻画作用，以及LLMs中不同情绪类别的内部表示是否能通过认知评估维度进行解释。

Result: 研究结果和分析揭示了不同大型语言模型在情绪推理方面展现出多样化的认知推理模式。

Conclusion: 本研究揭示了LLMs在情绪认知推理方面的复杂性和多样性。所提出的CoRE基准和代码将公开，以促进该领域未来的研究。

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [8] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: 该研究提出了一种名为SPS的轻量级无监督度量标准，用于评估检索增强生成（RAG）中检索部分的贡献，并基于此开发了xCompress推理时框架，实验证明能提升LLM在问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）的评估方法通常将检索器和阅读器作为一个整体进行评估，难以孤立地衡量检索的实际贡献，尤其是在考虑到大型语言模型（LLMs）对提示敏感性的情况下。

Method: 引入了Spectrum Projection Score (SPS)，这是一种轻量级、无需监督的度量标准，通过比较检索摘要与阅读器隐藏表示中的主方向所形成区域来衡量语义对齐和相关性。基于SPS，提出了xCompress，这是一个推理时控制器框架，可以动态地采样、排序和压缩检索到的摘要候选项。

Result: 在五个问答基准和四个开源LLM上进行的广泛实验表明，SPS不仅在多种任务中提升了性能，而且为检索与生成之间的交互提供了一个有原则的视角。

Conclusion: SPS作为一种新的评估指标和框架，有效解决了RAG中检索贡献难以量化的问题，并显著提升了LLMs在问答任务上的表现，深化了对检索与生成之间关系的理解。

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [9] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 针对文本中亲社会性检测缺乏定义和标注数据的挑战，本文提出一个三阶段的人机协作流程，以可扩展、高精度地分类亲社会内容，同时最小化人工标注和推理成本。


<details>
  <summary>Details</summary>
Motivation: 亲社会性文本检测对于信任和安全系统是一个新兴且日益重要的挑战。与有害内容检测不同，亲社会性缺乏明确的定义和标注数据，需要新的标注和部署方法。

Method: 本文提出了一个三阶段的实用流程：
1.  **LLM标签策略识别**：利用少量人工标注的种子集，确定最佳的LLM（大型语言模型）标签策略。
2.  **人机精炼循环**：引入一个人机协作精炼循环，标注者审查GPT-4与人工标注之间存在高分歧的案例，以迭代澄清和扩展任务定义。
3.  **两阶段推理系统**：使用GPT-4生成1万个高质量标签，并训练一个两阶段推理系统——一个轻量级分类器处理高置信度预测，而仅约35%的模糊实例会升级到GPT-4o进行处理。

Result: 通过人机精炼循环，改善了标签质量和定义一致性。该架构将推理成本降低了约70%，同时实现了高精度（约0.90）。

Conclusion: 本流程展示了有针对性的人机交互、细致的任务制定和部署感知架构设计如何为新型负责任AI任务提供可扩展的解决方案。

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [10] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: 本文提出一种名为ATOP的新方法，通过结合主题共享和主题特定特征的学习，并利用对抗性训练和提示微调，显著提升了跨主题自动作文评分的性能。


<details>
  <summary>Details</summary>
Motivation: 跨主题自动作文评分面临主题间差异的挑战。现有方法主要关注提取主题共享特征，但忽略了主题特定特征（如主题依从性），这限制了其评估能力。

Method: 本文提出了对抗性主题感知提示微调（ATOP）方法。ATOP通过优化可学习的主题感知提示（包含共享和特定组件）来从预训练语言模型中获取知识，从而联合学习主题共享和主题特定特征。为增强主题共享提示学习的鲁棒性并减轻特征尺度敏感性，该方法在统一的回归和分类框架中融入了对抗性训练。此外，还采用基于邻居的分类器对作文表示的局部结构进行建模，生成目标主题作文的伪标签，并用这些伪标签指导主题特定提示的监督学习。

Result: 在公开的ASAP++数据集上进行的广泛实验表明，ATOP在整体和多特质作文评分方面均显著优于现有的最先进方法。

Conclusion: ATOP通过有效融合主题共享和主题特定特征，成功解决了现有跨主题作文评分方法的局限性，实现了卓越的评分性能。

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [11] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 本研究发现，对DistilBERT模型在SST-2任务上微调时引入注意力稀疏性，反而能显著提高模型准确率，打破了稀疏性必然牺牲精度的普遍认知。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制的二次计算成本是其扩展的主要挑战。尽管注意力稀疏性被广泛研究以提高计算效率，但普遍认为这会牺牲模型准确性。本文旨在挑战这一普遍认知。

Method: 在SST-2情感分析任务上，对DistilBERT模型的注意力机制在微调过程中引入结构化的事后稀疏性。

Result: 引入80%注意力稀疏性的模型在SST-2任务上实现了91.59%的验证准确率，比密集基线模型提高了0.97%的绝对准确率。研究者推测，这种现象是由于稀疏性作为一种强大的隐式正则化器，通过强制模型使用更受约束和鲁棒的特征集进行预测，从而防止过拟合。

Conclusion: 注意力稀疏性不仅可以作为提高计算效率的工具，还可能是一种提升Transformer模型泛化能力和性能的方法。

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [12] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: 针对现有自奖励大模型中选择和拒绝样本趋同的问题，本文提出一种时序自奖励方法，通过协调模型不同代次的输出，有效维持学习信号，显著提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自奖励大模型(LLMs)通过LLM作为评判者迭代改进，但其选择和拒绝响应的同步改进会导致对比样本之间的表征差异逐渐缩小，从而损害有效的偏好学习。

Method: 提出“时序自奖励语言模型”，通过协调过去、现在和未来的模型生成来维持学习信号。该双阶段框架包含：1) 锚定拒绝：使用过去初始模型的输出来固定拒绝样本；2) 未来引导选择：使用下一代模型的预测来动态筛选选择样本。

Result: 在Llama、Qwen、Mistral三个模型家族及不同规模的模型上，本方法相比现有自奖励方法取得了显著改进。例如，Llama3.1-8B在AlpacaEval 2.0上胜率达29.44%，优于基线（19.69%）9.75%。此外，本方法在数学推理、知识问答和代码生成等任务中展现出卓越的域外泛化能力，即使未收集特定训练数据。

Conclusion: 时序自奖励语言模型通过有效解决现有自奖励范式中样本差异缩小的问题，显著提升了大模型的生成能力和跨任务泛化能力。

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [13] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 本文提出PEEK方法，通过利用预训练嵌入模型（文本或图）作为LLM知识的代理，高效地预测LLM是否掌握特定事实，准确率高达90%，并发现句子嵌入模型更适用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）拥有广泛知识，但由于其随机性，难以预测它们实际学到了什么。现有知识探测方法（如调查隐藏表示、设计特定提示）计算成本高且耗时，因为它们需要对LLM进行前向传播。

Method: 本文提出PEEK（Proxy Embeddings to Estimate Knowledge of LLMs）方法。首先，通过现有探测策略确定LLM已知事实的训练集。然后，利用预训练的文本或图嵌入模型作为LLM知识的代理，并添加一个线性解码器层来预测LLM的输出，从而估算LLM的知识。

Result: 在3个维基百科数据集、4个LLM和7个嵌入模型上的综合评估表明，嵌入模型能以高达90%的准确率预测LLM在未见过数据上的知识。此外，研究发现句子嵌入模型比图嵌入模型更适合预测LLM知识。

Conclusion: 知识适应的嵌入模型可以大规模识别LLM的知识空白，并为LLM的内部归纳偏置提供更深入的见解。

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [14] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: 提出自进化配对推理（EvolvR）框架，通过自合成和自过滤思维链数据，显著提升大型语言模型在故事评估中的表现，并能有效指导故事生成。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）作为评判者（LLM-as-a-judge）已被验证有效，但在开放式任务（尤其是故事评估）中表现受限。现有的提示工程和微调方法存在适应性差或缺乏严谨推理能力的问题。准确的故事评估对于辅助人类判断和指导故事生成至关重要。

Method: 我们提出了自进化配对推理（EvolvR）框架。该框架基于配对比较，首先通过多角色策略自合成得分对齐的思维链（CoT）数据。为确保数据质量，原始CoT数据会通过多智能体进行自过滤，以保证其逻辑严谨性和鲁棒性。最终，在精炼数据上训练出的评估器被部署为奖励模型，用于指导故事生成任务。

Result: 实验结果表明，我们的框架在StoryER、HANNA和OpenMEVA三个评估基准上实现了最先进（SOTA）的性能。此外，当作为奖励模型使用时，它显著提升了生成故事的质量。

Conclusion: 该研究全面验证了我们所提出的自进化方法的优越性，使其在故事评估和生成方面表现出色。

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [15] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型（LLMs）开发了一个名为ConlangCrafter的流水线系统，旨在自动化生成连贯且多样化的人造语言。


<details>
  <summary>Details</summary>
Motivation: 人造语言在艺术、哲学和国际交流中扮演着重要角色。同时，大型基础模型在创意生成领域取得了革命性进展。本研究的动机是探索如何利用现代LLMs作为计算创意辅助工具，实现端到端的人造语言创作。

Method: 研究提出了ConlangCrafter，一个多阶段的流水线方法。该方法将语言设计分解为语音学、形态学、句法、词汇生成和翻译等模块化阶段。在每个阶段，ConlangCrafter利用LLMs的元语言推理能力，通过注入随机性来促进多样性，并采用自我修正反馈机制来确保生成语言描述的一致性。

Result: 通过对连贯性和类型学多样性指标的评估，ConlangCrafter展示了其无需人类语言学专业知识即可生成连贯且多样化的人造语言的能力。

Conclusion: ConlangCrafter成功地证明了利用大型语言模型可以实现人造语言的自动化创建，产出高质量且多样化的语言，从而大大降低了人造语言设计的门槛。

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [16] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本文提出两种针对《古兰经》的抽取式问答（QA）方法，利用大型语言模型（LLMs）进行阿拉伯语指令微调，有效应对文本复杂性，并取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 针对《古兰经》中复杂的语言、独特术语和深层含义等挑战，寻找有效的抽取式问答（QA）方法。

Method: 采用少样本提示（few-shot prompting）结合指令微调的大型语言模型（如Gemini和DeepSeek），开发了专门的阿拉伯语提示框架，并集成了强大的后处理系统（包括子词对齐、重叠抑制和语义过滤）。

Result: 评估表明，采用阿拉伯语指令的大型语言模型优于传统微调模型，最佳配置的pAP10分数为0.637。

Conclusion: 基于提示的指令微调对于低资源、语义丰富的问答任务是有效的。

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [17] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG是一种无需预构建图的RAG框架，通过在推理时动态提取逻辑推理结构来指导自适应检索，有效解决了现有图基RAG的成本与效率问题，并取得了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易产生“幻觉”，生成不实信息。现有检索增强生成（RAG）方法中，图基RAG（GraphRAG）虽在复杂任务中表现优异，但其预构建图谱过程成本高昂、更新延迟大且token消耗大。此外，预构建图可能无法有效匹配复杂多变的实时查询所需的特定逻辑结构，导致检索效率低下。

Method: 本文提出了LogicRAG框架，其核心在于在推理时动态提取和利用逻辑推理结构，无需预先构建知识图谱。具体方法包括：1. 将输入查询分解为子问题并构建有向无环图（DAG）以表示其逻辑依赖关系。2. 使用拓扑排序将DAG线性化，确保多步推理的逻辑顺序。3. 采用图剪枝和上下文剪枝技术，分别减少冗余检索和过滤不相关上下文，从而显著降低token成本。

Result: 广泛的实验证明，LogicRAG在性能和效率上均超越了现有最先进的基线方法。

Conclusion: LogicRAG成功克服了现有图基RAG在成本和适应性方面的限制，通过动态、逻辑驱动的检索机制，为解决LLM的幻觉问题提供了一个更高效、更灵活且表现更优的解决方案。

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [18] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）基于功能意象的安全风险，本文提出了AURA框架。该框架通过过程奖励模型（PRMs）、内省式自我批判和自适应安全解码，实现了对模型推理过程的细粒度评估与引导，显著提升了输出的逻辑完整性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在处理基于功能意象的安全风险时面临挑战，即输出可能因忽略逻辑含义而无意中促成有害行为。传统的安全解决方案（如基于标量结果的奖励模型、参数调整或启发式解码策略）缺乏必要的细粒度和主动性，无法在细微而关键的推理步骤中可靠地检测并干预此问题。

Method: 引入AURA框架，这是一个创新的多层框架，其核心是过程奖励模型（PRMs），能够提供逻辑一致性和安全意识的全面步骤级评估。AURA无缝结合了内省式自我批判、细粒度PRM评估和自适应安全感知解码，以动态主动地引导模型走向更安全的推理轨迹。

Result: 实证结果表明，AURA方法显著超越了现有方法，显著改善了模型输出的逻辑完整性和对功能意象敏感的安全性。

Conclusion: 这项研究是迈向更安全、更负责任、更具语境意识的AI的关键一步，为对齐敏感型应用设定了新的基准。

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [19] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: SRD通过优化训练数据质量和学生模型兼容性，提高了大型语言模型知识蒸馏的性能并显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）白盒知识蒸馏方法主要关注平衡真实数据和学生模型响应，但忽视了训练数据质量和学生模型兼容性这两个关键因素。

Method: 本文提出选择性反射蒸馏（SRD）框架，通过利用学生模型的反馈来系统性地优化训练数据。SRD通过比较真实数据与学生模型输出，动态评估并选择提示-响应对，基于难度进行自动排序，从而精选出高质量、与学生模型兼容的训练实例。此外，该方法还采用课程调度策略，逐步将这些精选的数据子集引入到蒸馏过程中。

Result: SRD作为一个即插即用模块，在多种白盒知识蒸馏方法和模型架构下，持续提升蒸馏效果，显著降低了计算成本。实验证明，蒸馏模型的性能得到一致提升，训练运行时长最多可减少39%。

Conclusion: 数据质量和兼容性是实现高效LLM知识蒸馏的关键。SRD提供了一个实现这两者的原则性框架，加深了对KD中数据中心因素的理解，并为提升压缩LLM的能力和效率提供了实用见解。

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [20] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler是一个基于提示的框架，通过将数值特征嵌入到自然语言提示中，使大型语言模型（LLMs）能够表现出可控的五大人格特质，且无需额外训练，已验证其在多任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在赋予大型语言模型可控的五大人格特质，以构建具有人格意识的对话代理，同时避免进行额外的模型训练。

Method: 提出Big5-Scaler框架，通过将数字化的五大人格特质值嵌入到自然语言提示中，实现对LLM的精细化人格控制，无需额外训练。在特质表达、对话生成和人类特质模仿任务中对该方法进行评估。

Result: 研究结果表明，Big5-Scaler能够在不同模型中诱导出一致且可区分的人格特质。其性能表现会因提示类型和强度而异，其中简洁的提示和较低的特质强度被证明更有效。

Conclusion: Big5-Scaler提供了一种高效的方法来构建具有人格意识的对话代理，通过简洁的提示和较低的特质强度，实现了LLM在不进行额外训练的情况下表现出可控且一致的人格特质。

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [21] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: 本文提出一种可解释的偏见检测方法，用于识别大型语言模型生成内容中隐含的社会偏见，并通过实验验证了其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成过程中可能产生隐式刻板印象。现有方法难以捕获非显性语言特征的语义倾向，因此需要一种可解释的偏见检测方法来识别模型输出中隐藏的社会偏见。

Method: 该方法结合了嵌套语义表示和上下文对比机制。它从模型输出的向量空间结构中提取潜在偏见特征，并通过注意力权重扰动分析模型对特定社会属性词的敏感性，揭示偏见的形成路径。使用StereoSet数据集进行验证，评估指标包括偏见检测准确性、语义一致性和上下文敏感性。

Result: 实验结果表明，该方法在多个维度上均表现出强大的检测性能，能准确识别语义相似文本间的偏见差异，同时保持高语义对齐和输出稳定性。该方法在结构设计上具有高可解释性。

Conclusion: 所提出的偏见检测方法有效且可解释，能揭示语言模型内部偏见关联机制，为需要高可信度生成内容的实际应用提供了更透明可靠的技术基础。

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [22] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 现有模型合并的稀疏化方法忽视参数异质性，导致次优剪枝。本文提出的TADrop通过自适应地根据参数张量的分布特性分配稀疏度，显著提升了多任务学习中模型合并的性能。


<details>
  <summary>Details</summary>
Motivation: 模型合并中的参数稀疏化是关键技术，但现有方法采用统一的稀疏率，忽略了模型参数固有的结构和统计异质性。这导致关键参数被误剪，而作用较小的参数被保留，从而影响整体性能。

Method: 引入了TADrop（Tensor-wise Adaptive Drop），一种自适应稀疏化策略。TADrop根据每个参数张量的分布特性，为其分配量身定制的稀疏度，从而对分布密集、冗余的张量进行更激进的剪枝，同时保留稀疏、关键的张量。它是一个简单即插即用的模块，可通过与现有模型合并方法集成来验证其有效性。

Result: 通过在视觉、语言和多模态等多样化任务以及ViT、BEiT等模型上进行大量实验，结果表明TADrop能持续显著提升模型合并方法的性能。例如，在增强一种领先的合并方法时，它在8个ViT-B/32任务上实现了平均2.0%的性能提升。

Conclusion: TADrop通过根据模型结构调整稀疏化，提供了一种更有效的方法来缓解参数干扰，为高性能模型合并树立了新的基线。

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [23] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出UR2框架，通过强化学习统一了检索增强生成（RAG）和复杂推理，解决了现有方法孤立发展或适用范围狭窄的问题。UR2引入了难度感知课程训练和混合知识访问策略，在多项任务上显著优于现有RAG和RL方法，并达到了与小型GPT模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的检索增强生成（RAG）和可验证奖励强化学习（RLVR）能力通常是独立开发的，或在有限场景（如开放域QA）中统一，这限制了泛化能力和在更广泛领域中的适用性。现有集成不足是研究动机。

Method: 本文提出UR2（Unified RAG and Reasoning）框架，通过强化学习统一检索与推理。其核心方法包括：1) 难度感知课程训练：仅在处理挑战性问题时选择性地调用检索。2) 混合知识访问策略：结合领域特定离线语料库与LLM生成的摘要。

Result: UR2（基于Qwen2.5-3/7B和LLaMA-3.1-8B构建）在开放域问答、MMLU-Pro、医学和数学推理任务上进行实验，结果显示其显著优于现有RAG和RL方法，并在多个基准测试中取得了与GPT-4o-mini和GPT-4.1-mini相当的性能。

Conclusion: UR2成功地通过强化学习实现了检索与推理的统一，并通过其独特的难度感知检索和混合知识访问策略，有效提升了模型在多样化任务上的适应性和性能，弥补了现有RAG-RL方法整合不足的空白，并展现出超越现有方法的卓越能力。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [24] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: 本文重新定义了语用学，认为它是语言作为社会嵌入式行动工具的动态接口，并分析了大型语言模型（LLMs）对传统语用学构成的挑战，提出了适应人机沟通的新框架和概念。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在沟通语境中的出现，传统的语用学理解（作为意义的第三维度）不足以分析和解释人机沟通，需要对现有语用理论进行重新概念化和方法论上的重新考量。

Method: 本文采用概念分析和批判性论证的方法。具体包括：1) 挑战传统符号学三元论，提出人机沟通（HMC）框架；2) 审视以人类为中心的语用理论（如Griceian语用学）与以机器为中心的LLMs之间的张力，并推荐概率语用学；3) 探讨替代主义的三种形式（泛化、语言和沟通替代主义）及其造成的人形偏见；4) 引入“语境挫败”的概念来描述LLM沟通中的语境理解困境。

Result: 研究结果表明：1) 连接主义LLM架构颠覆了既定的意义层级，人机沟通（HMC）框架更适合；2) 传统Gricean语用学不适用于LLMs，概率语用学（如理性言语行动框架）更具兼容性；3) 评估LLM时存在泛化、语言和沟通替代主义等人形偏见，扭曲了评价并掩盖了人类沟通主体的作用；4) 存在“语境挫败”现象，即上下文输入增加但语境理解崩溃，迫使用户共同构建语用条件。

Conclusion: 语用理论需要调整或扩展，以更好地解释涉及生成式AI的沟通。本文提出的论点和概念为适应新沟通范式下的语用学发展提供了基础。

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [25] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 研究如何在有限数据下有效注入知识到大语言模型，并探讨了避免灾难性遗忘的方法，发现多样化数据增强策略能显著提升知识学习。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在有限数据量下获取新知识仍具挑战，且此过程可能导致灾难性遗忘。本文旨在探究在小规模非结构化信息注入LLMs时的有效策略及与遗忘现象的关系。

Method: 使用与模型预训练数据无重叠的近期新闻数据集，通过问答对评估模型知识获取能力。从持续预训练基线开始，探索了不同的数据增强算法来生成合成数据，特别是通过多样化提示诱导变异性的方法，以提高知识获取能力。同时，对比了参数化方法与RAG方法的表现。

Result: 仅在有限数据上持续预训练改进不大；而通过多样化文本变体（特别是通过多样化提示诱导更多变异性）暴露模型，能显著提升新事实的学习。小数据量下的遗忘现象得到阐明。RAG方法在知识注入时对控制数据集的性能下降更严重，相比之下参数化方法表现更佳。模型能生成有效的合成训练数据，指向自我改进的模型更新路径。

Conclusion: 多样化的数据增强，特别是通过提示诱导变异性，是有限数据下有效注入知识的关键。需要平衡新知识学习与现有能力保留以避免遗忘。参数化方法在某些情境下优于RAG。模型自生成合成数据为未来模型更新提供了新途径。

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [26] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: DKG-LLM是一个将动态知识图谱（DKG）与Grok 3大型语言模型（LLM）结合的框架，旨在提高医学诊断和个性化治疗推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在自然语言理解方面的显著进展，本研究旨在通过整合动态知识图谱，将其强大能力应用于复杂的医学诊断和个性化治疗推荐，以期迈向通用人工智能。

Method: 本研究提出了DKG-LLM框架。该框架利用自适应语义融合算法（ASFA）动态生成知识图谱，融合异构医疗数据（如临床报告、PubMed文章）和患者记录。ASFA运用概率模型、贝叶斯推理和图优化来提取语义信息并动态更新图谱。生成的知识图谱与Grok 3大型语言模型结合，并通过医生反馈进行学习。

Result: 在MIMIC-III和PubMed数据集上评估显示，DKG-LLM达到了84.19%的诊断准确率，89.63%的治疗推荐准确率，以及93.48%的语义覆盖率。

Conclusion: DKG-LLM是一个可靠且具有变革性的工具，能够有效处理嘈杂数据和复杂的多症状疾病，并通过医生反馈学习，显著提升了医学诊断和治疗推荐的能力。

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [27] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: 本文提出SceneJailEval，一个场景自适应的多维度越狱评估框架，旨在克服现有方法的局限性，并提供高质量数据集，显著提升了越狱评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM越狱评估方法存在局限性：二进制分类无法量化危害强度；多维度框架采用统一标准，导致特定场景评估不匹配，如“相对真实性”与“仇恨言论”无关，从而损害评估精度。

Method: 引入SceneJailEval，其主要贡献包括：1. 提出一个开创性的场景自适应多维度越狱评估框架，克服了现有“一刀切”方法的限制，并具有强大的可扩展性。2. 构建了一个包含14个场景的综合数据集，涵盖多样越狱变体和区域案例，填补了高质量、整体性基准的长期空白。

Result: SceneJailEval在全场景数据集上实现了0.917的F1分数（比现有最佳提升6%），在JBB上实现0.995的F1分数（比现有最佳提升3%），超越了现有评估方法在异构场景下的精度限制。

Conclusion: SceneJailEval通过其场景自适应多维度框架和综合数据集，显著提高了LLM越狱评估的精度和适用性，尤其在复杂和多样化的越狱场景中表现出卓越的优势。

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [28] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 研究提出一个LLM情感智能（EI）四层分类法和EICAP-Bench基准，评估发现现有预训练和指令调优难以全面提升LLM的EI能力，需更专门的策略。


<details>
  <summary>Details</summary>
Motivation: 情感智能（EI）是开发与人类对齐的大型语言模型（LLMs）的关键维度，但目前研究不足，存在空白。

Method: 1. 提出了一个统一的、心理学基础的LLM情感智能四层分类法（情感追踪、原因推断、评估、情感适当响应生成）。2. 构建了EICAP-Bench，一个多轮选择题式基准，用于评估LLM的EI能力。3. 评估了包括LLaMA3、Gemma、Qwen2.5系列在内的六个开源LLM。4. 使用LoRA适配器在UltraChat数据集（包括英语和阿拉伯语）上对Qwen2.5-Base和Qwen2.5-Instruct进行了微调，以评估EI能力的提升潜力。

Result: 1. Qwen2.5-Instruct是EICAP-Bench上表现最强的基线模型。2. 基于UltraChat的微调在五个EI层中，仅“评估”（Appraisal）层显示出显著改进。3. 现有预训练和指令调优范式在赋予LLM更深层次情感推理能力方面存在局限。

Conclusion: 现有预训练和指令调优方法难以使LLM获得全面的情感智能对齐能力。为了实现全面的EI对齐，需要更具针对性的数据和模型策略。

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [29] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 本文提出一种基于检索增强生成（RAG）的上下文策略引擎（CPE），用于内容审核。该系统能快速适应政策变化，无需再训练，同时保持高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的内容审核分类系统难以快速适应不断变化的政策，且再训练成本高昂，导致其灵活性不足。

Method: 研究提出了一种使用检索增强生成（RAG）的分类方法，并构建了一个名为“上下文策略引擎（CPE）”的代理式RAG系统。该方法在推理时检索上下文知识（政策），将内容评估从基于预训练参数的分类转变为依据检索到的政策进行判断。

Result: 该系统实现了与领先商业系统相当的强大分类准确性；通过检索到的策略片段提供固有的可解释性；支持动态策略更新，无需模型再训练。实验证明了其强大的基线性能，并能通过调整对特定身份群体的保护，实现细粒度的策略控制，且无需再训练或牺牲整体性能。

Conclusion: RAG技术能够将内容分类过程转化为一种更灵活、透明和适应性强的方法，适用于内容审核及更广泛的分类问题。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [30] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: 本研究引入InfoCausalQA，一个评估基于信息图多模态因果推理的新基准，并发现当前视觉-语言模型（VLMs）在该方面与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在感知和推理方面取得了显著进展，但在多模态场景下，其进行因果推理的能力（人类认知的核心）仍未被充分探索。

Method: 研究引入了InfoCausalQA基准，用于评估结合结构化视觉数据和文本上下文的信息图因果推理。该基准包含两个任务：基于推断数值趋势的定量因果推理，以及涉及五种因果关系（因、果、干预、反事实、时间）的语义因果推理。数据集通过手动收集494对信息图-文本，并使用GPT-4o生成1,482个多项选择题，再经人工修订以确保问题需真正的视觉理解而非表面线索。

Result: 实验结果显示，当前VLMs在计算推理能力有限，在语义因果推理方面表现出更明显的局限性。与人类相比，其性能显著较低，表明在利用基于信息图的信息进行因果推断方面存在巨大差距。

Conclusion: 通过InfoCausalQA基准，研究强调了提升多模态AI系统因果推理能力的迫切需求。

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [31] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文提出一种结合微调ASR和LLM生成数据训练的Transformer模型的新方法，用于老年德语语音意图识别，结果表明LLM生成数据能有效提升性能，尤其LeoLM优于ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 现有语音意图识别方法多限于短指令且主要针对英语，无法满足AI助手系统对老年德语使用者语音意图识别的需求。

Method: 提出一种新方法：结合在老年德语语音上微调的Whisper ASR模型（SVC-de）与在由LeoLM、Llama3、ChatGPT等LLM生成的人工文本数据集上训练的Transformer语言模型。通过文本转语音生成人工语音，并进行广泛的跨数据集测试以评估鲁棒性。

Result: LLM生成的人工数据显著提升了分类性能，并增强了对不同说话风格和未见过词汇的鲁棒性。特别地，小型且领域特定的LeoLM（13B）在德语意图识别的数据集质量上优于更大的ChatGPT（175B）。

Conclusion: 生成式AI能够有效弥补低资源领域的数据鸿沟。本研究提供了详细的数据生成和训练过程文档，以确保透明度和可复现性。

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [32] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: 提出MDIR方法，利用矩阵分析和大偏差理论解决LLM权重抄袭检测问题，克服现有方法的不足，并高效准确地检测抄袭。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLM）的知识产权（IP）问题日益突出，通过直接权重复制、再利用、剪枝或持续预训练等方式抄袭LLM权重，且未归属原始许可，会给开发者带来严重经济和声誉损失。然而，现有LLM抄袭检测方法存在局限性，包括无法准确重建权重对应关系、缺乏p值等统计显著性度量，以及可能错误地将基于相似数据训练的模型标记为相关。

Method: 为解决上述限制，本文提出了一种新颖的方法——Matrix-Driven Instant Review (MDIR)。该方法利用矩阵分析和大偏差理论，旨在准确重建权重关系，提供严格的p值估计，并专门关注权重相似性而无需进行完整的模型推理。

Result: 实验结果表明，MDIR即使在经过随机置换和万亿级token的持续预训练等大量转换后，仍能可靠地检测出抄袭。此外，所有检测都可以在单个PC上在一小时内完成。

Conclusion: MDIR是一种高效且可访问的LLM权重抄袭检测方法，它克服了现有方法的局限性，能够准确、可靠地检测出即使经过大幅度转换的抄袭行为。

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [33] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出DynamicTRF框架，通过动态选择最合适的图拓扑表示形式（TRF），解决了大型多模态模型（LMMs）在零样本图问答（QA）中单一TRF的局限性，显著提升了LMMs在图QA任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在图QA任务中多采用单一图表示形式（TRF），这种“一刀切”方法未考虑不同模型或任务的具体偏好，常导致回答错误或冗长，限制了其泛化零样本能力。

Method: 1. 分析现有TRF的优缺点，并设计一套适合零样本图QA的TRF集合($F_{ZS}$)。2. 引入新指标Graph Response Efficiency (GRE)，衡量图QA性能与简洁性的平衡。3. 开发DynamicTRF框架：构建TRF Preference (TRFP) 数据集（基于GRE分数排名TRF），并训练TRF路由器，使其在推理时能为每个问题自适应选择最佳TRF。

Result: 在7个域内算法图QA任务和2个域外下游任务上进行的大量实验证明，DynamicTRF显著提高了LMMs在零样本图QA中的准确性。

Conclusion: DynamicTRF框架通过动态选择最佳图表示形式，有效解决了LMMs在零样本图QA中单一图表示的局限性，从而显著提升了性能和潜在的简洁性。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本研究提出一种将攻击性检测结果嵌入提示词的管道方法，以增强大语言模型（LLMs）在网络霸凌检测中的性能，并验证其优于传统微调。


<details>
  <summary>Details</summary>
Motivation: 网络霸凌检测因其表达的微妙性和多样性仍是严峻挑战。研究旨在探索整合攻击性检测作为辅助任务，是否能提升大语言模型在网络霸凌检测中的泛化性和性能。

Method: 使用指令微调LLMs在5个攻击性数据集和1个网络霸凌数据集上进行实验。评估了零样本、少样本、独立LoRA微调和多任务学习（MTL）。鉴于MTL结果不一致，提出了“富含提示词管道（enriched prompt pipeline）”方法，将攻击性预测嵌入网络霸凌检测提示词中提供上下文增强。

Result: 初步结果显示，“富含提示词管道”方法持续优于标准的LoRA微调，表明攻击性信息能够显著提升网络霸凌检测效果。

Conclusion: 辅助任务（如攻击性检测）有望提升大语言模型在社交网络安全关键应用中的泛化能力。

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: 本文探讨低资源风格个性化文本生成的评估问题，质疑传统指标，提出并验证了风格嵌入和LLM-as-judge等新评估范式，最终建议采用多样化评估指标的组合。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究已开发风格个性化文本生成工具和基准，但在低资源作者风格个性化文本生成领域的评估方面探索有限。普遍采用的BLEU和ROUGE等评估指标的有效性存疑。

Method: 本文质疑BLEU和ROUGE等传统指标的有效性，并探索了风格嵌入和LLM-as-judge等新的评估范式，以全面评估风格个性化文本生成任务。这些指标及其组合在一个包含八个写作任务，涵盖领域判别、作者归属和LLM个性化与非个性化判别三种设置的风格判别基准上进行了评估。

Result: 研究提供了确凿证据，表明采用多样化评估指标的组合可以有效评估风格个性化文本生成。

Conclusion: 为了有效评估风格个性化文本生成，应采纳多样化评估指标的组合。

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: 本研究旨在弥补LLM在虚拟角色扮演中情感支持的空白，构建了首个情感支持角色扮演（ESRP）数据集ChatAnime，并评估了LLM在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能将LLM的角色扮演和情感支持能力相结合，尤其缺乏针对虚拟角色的情感支持型互动。选择动漫角色作为案例，因其鲜明个性和庞大粉丝群有助于有效评估LLM在保持角色特性的同时提供情感支持的能力。

Method: 构建了ChatAnime数据集，包括从流行动漫社区中精选20个动漫角色，设计60个以情感为中心的现实情景问题。招募40名资深动漫爱好者，收集了来自10个LLM和这些爱好者两轮对话数据。设计了用户体验导向的评估系统，包含9个细粒度指标（基本对话、角色扮演、情感支持）及响应多样性总览指标。数据集共包含2,400条人工编写和24,000条LLM生成答案，以及超过132,000条人工标注。

Result: 实验结果显示，顶尖的LLM在角色扮演和情感支持方面超越了人类粉丝，但在响应多样性方面人类仍处于领先地位。

Conclusion: 本工作为未来优化LLM在ESRP领域的研究提供了宝贵资源和见解。相关数据集已公开。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: SecMCP是一种安全框架，通过在潜在空间中量化对话漂移，增强了集成外部工具的LLM的安全性，有效检测对话劫持及数据泄露。


<details>
  <summary>Details</summary>
Motivation: 模型上下文协议（MCP）虽能通过集成外部工具增强大型语言模型（LLMs）的功能，但其非隔离的执行上下文引入了严重的安全与隐私风险，如工具中毒或间接提示注入，可能导致对话劫持、错误信息传播或数据泄露。现有防御措施因依赖静态签名、计算效率低下且无法量化对话劫持而不足。

Method: 本文提出了SecMCP框架，通过在潜在多胞形空间中建模LLM的激活向量，识别对话动态中的异常漂移（即由对抗性外部知识引起的潜在空间轨迹偏差），从而实现对劫持、误导和数据泄露的主动检测和量化。

Result: SecMCP在Llama3、Vicuna和Mistral等主流LLM以及MS MARCO、HotpotQA、FinQA等基准数据集上进行了评估，展示了强大的检测能力，AUROC分数超过0.915，同时保持了系统可用性。

Conclusion: 本研究贡献包括对MCP安全威胁的系统分类、一种新颖的基于潜在多胞形的对话漂移量化方法，以及SecMCP在实践中有效性的经验验证，为解决LLM与外部工具集成时的安全问题提供了解决方案。

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出Memp框架，为LLM代理赋予可学习、可更新的终身程序记忆，通过提炼历史轨迹提升代理在类似任务上的成功率和效率，并验证了记忆的可迁移性。


<details>
  <summary>Details</summary>
Motivation: LLM代理虽然在多种任务中表现出色，但其程序记忆脆弱，通常是手动构建或固化在静态参数中，缺乏学习和更新能力。

Method: 提出Memp框架，通过将代理的历史轨迹提炼成细粒度的分步指令和高层次的脚本式抽象，构建可学习的程序记忆。同时，研究了记忆的构建、检索和更新策略，并结合动态机制，使其内容能随新经验持续更新、纠正和废弃。

Result: 在TravelPlanner和ALFWorld上的实证评估显示，随着记忆库的完善，代理在类似任务上的成功率和效率显著提高。此外，由更强模型构建的程序记忆具有价值保留性，迁移到较弱模型后仍能带来显著的性能增益。

Conclusion: Memp框架成功解决了LLM代理程序记忆的痛点，通过引入可学习和动态更新的机制，显著提升了代理在各种任务中的性能和效率，并证明了其在模型迁移中的有效性。

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: 本研究通过微调轻量级LLaMA模型，发现少量语言的微调即可使大语言模型在未见语言上进行移民推文的主题分类，而立场识别则受益于多语言微调。即使是极少量的数据也能有效纠正预训练偏见，并发布了高效的开源模型，挑战了跨语言能力需要大量多语言训练的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型（LLMs）在社会科学研究中，通过少数语言的微调，知识能否迁移到仅在预训练中出现的未见语言。具体关注在移民相关推文分类这一复杂领域中，最小语言特定微调是否能实现跨语言主题检测，以及能否通过添加目标语言纠正预训练偏见。

Method: 使用轻量级LLaMA 3.2-3B模型，在单语言、双语言或多语言数据集上进行微调（4位量化，LoRA微调），以分类来自X/Twitter的13种语言的移民相关推文。

Result: 在一个或两个语言中微调的LLMs能可靠分类未见语言的移民相关内容；多语言微调更有利于识别推文的移民立场（支持或反对）。预训练偏见利于主导语言，但极少量（低至预训练token量的$9.62	imes10^{-11}$）的微调数据即可显著纠正对代表性不足语言的偏见。此外，发布的模型推理速度比GPT-4o快35倍，成本仅为0.00000989%。

Conclusion: 研究结果挑战了跨语言掌握需要大量多语言训练的假设，表明有限的语言覆盖范围足以实现主题级别的泛化，且结构性偏见可通过轻量级干预纠正。这为社会科学研究提供了一个可扩展、包容且高效的开源替代方案。

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 该研究发现，生成式AI在新闻内容中，尤其在地方和大学媒体中，使用显著增加，影响了新闻的写作风格并引发了对新闻诚信的担忧。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（尤其是大型语言模型）的快速发展，对新闻业的诚信和作品署名权构成了挑战和担忧。

Method: 研究分析了来自主要、地方和大学新闻媒体的40,000多篇新闻文章，使用Binoculars、Fast-Detect GPT和GPTZero等三种先进AI文本检测器，并进行了句子级和语言学分析。

Result: 1. 近年来生成式AI在新闻中的使用显著增加，尤其在地方和大学新闻中。
2. 大型语言模型常用于新闻导语部分，而结论通常由人工撰写。
3. 生成式AI提升了词汇丰富度和可读性，但降低了正式性，导致写作风格趋于统一，特别是在地方媒体中。

Conclusion: 生成式AI在新闻业中的普及正在改变新闻内容的生成方式和语言特征，导致写作风格更加统一，这引发了对新闻诚信和作者身份的深层思考。

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer提出一种新颖框架，通过在前向传播中动态修剪冗余输入令牌，利用信息扩散现象加速LLM长文本推理，显著提升效率而性能无损。


<details>
  <summary>Details</summary>
Motivation: LLM的长文本推理受限于高计算需求。现有方法虽然优化注意力计算，但仍处理完整的隐藏状态集，整体效率受限。因此，需要一种更直接、更高效的加速方法。

Method: SlimInfer框架通过直接修剪前向传播中不那么关键的提示符令牌来加速推理。其核心在于“信息扩散”现象：关键令牌的信息在层间传播并分散到整个序列，因此即使修剪掉部分关键令牌在隐藏状态中，模型也能保持语义完整性。为此，SlimInfer引入了一种动态细粒度修剪机制，在中间层精确移除隐藏状态中的冗余令牌。这种分层修剪还自然地支持异步KV缓存管理，无需复杂预测器即可预取所需令牌块，从而减少内存使用和I/O成本。

Result: SlimInfer在LLaMA3.1-8B-Instruct模型（单张RTX 4090）上，实现了高达2.53倍的首令牌生成时间（TTFT）加速和1.88倍的端到端延迟降低，同时在LongBench上的性能没有下降。

Conclusion: SlimInfer通过创新的令牌修剪和KV缓存管理机制，有效解决了LLM长文本推理中的高计算量瓶颈，显著提升了推理效率和速度，且保持了高性能。

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5是一个开源MoE大语言模型，具有355B总参数和32B激活参数，采用混合推理方法，在代理、推理和编码任务上表现出色，且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个高效且高性能的MoE大语言模型，以推动推理和代理AI系统的研究进展。

Method: 提出了GLM-4.5，一个具有355B总参数和32B激活参数的开源MoE大语言模型。该模型采用混合推理方法，支持思考和直接响应模式。通过23T tokens的多阶段训练和包含专家模型迭代、强化学习在内的全面后训练来优化性能。

Result: GLM-4.5在代理、推理和编码(ARC)任务上表现强劲，TAU-Bench得分70.1%，AIME 24得分91.0%，SWE-bench Verified得分64.2%。尽管参数量远少于多个竞争对手，GLM-4.5在所有评估模型中总体排名第3，在代理基准测试中排名第2。同时发布了GLM-4.5 (355B参数) 和GLM-4.5-Air (106B参数) 紧凑版。

Conclusion: GLM-4.5展示了在代理、推理和编码任务上的卓越性能和显著的参数效率。通过开源模型，该研究有望促进推理和代理AI系统的进一步发展。

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [43] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: HapticLLaMA是一个多模态感官语言模型，用于将触觉振动信号转换为自然语言描述，通过新型分词器和两阶段训练（包括RLHF）实现，并在自动和人工评估中表现出色，展示了LLM处理感官数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 触觉信号在虚拟现实、辅助和康复应用中潜力巨大，但相较于视觉和听觉信号，其在生成自然语言描述（触觉字幕）方面的研究仍未得到充分探索。

Method: 本文提出了HapticLLaMA模型，旨在将振动信号解释为感官、情感或联想类别的描述。模型采用频率或EnCodec两种触觉分词器将触觉信号转换为离散单元，并基于LLaMA架构进行两阶段训练：首先是基于LoRA的监督微调，然后是通过人类反馈进行强化学习（RLHF）微调。

Result: HapticLLaMA在解释触觉振动信号方面表现出强大能力，METEOR得分为59.98，BLEU-4得分为32.06。超过61%的生成字幕在7分制下获得3.5以上的人工评分，且RLHF使整体评分分布提升10%，表明与人类触觉感知更强的对齐。

Conclusion: 这些发现强调了大型语言模型在处理和适应感官数据方面的巨大潜力。

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [44] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型语言模型的后训练微调方法，旨在使其在多轮交互中能像人类一样形成即时约定和适应语言，并通过两个新基准的评估，验证了该方法能显著提升LLMs的约定形成能力。


<details>
  <summary>Details</summary>
Motivation: 人类在多轮交互中能高效地适应语言并形成即时约定，从而提高沟通效率，但现有的大型语言模型（LLMs）缺乏这种自然行为。

Method: 开发了一种后训练流程，通过对启发式识别的约定形成示范进行有针对性的微调，来培养LLMs的上述能力。评估方面，设计了两个新的基准：一个是以认知为动机的交互基准，另一个是反映真实世界约定形成行为的文档接地引用完成任务。

Result: 研究显示，经过后训练的LLMs在两种评估方法下，其约定形成能力均获得了显著提升。

Conclusion: 提出的后训练方法成功提升了大型语言模型在多轮交互中形成约定和适应语言的能力，使其更接近人类的表现。

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


### [45] [Indian Legal NLP Benchmarks : A Survey](https://arxiv.org/abs/2107.06056)
*Prathamesh Kalamkar,Janani Venugopalan Ph. D.,Vivek Raghavan Ph. D*

Main category: cs.CL

TL;DR: 强调印度法律文本NLP基准的必要性，并提出创建新基准的设想。


<details>
  <summary>Details</summary>
Motivation: 印度法律文本与普通英文文本差异显著，现有NLP基准不适用，需要为印度法律文本创建特定且具有挑战性的NLP基准，以推动AI在该领域的发展。

Method: 回顾该领域现有工作，并提出创建新的印度法律自然语言处理基准的设想。

Result: 本文梳理了现有工作，并针对印度法律文本的特点，提出了构建新NLP基准的初步设想。

Conclusion: 为促进人工智能在印度法律领域的进步，创建特定且有挑战性的NLP基准至关重要，这将造福人工智能社区和法律界。

Abstract: Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ResPA的新型迁移攻击方法，通过利用残差梯度作为扰动方向，将对抗样本引导至损失函数的平坦区域，从而显著提高了对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易受到对抗样本的攻击，即使是微小的扰动也能导致错误预测。现有的迁移攻击方法虽然能生成黑盒场景下的对抗样本，但往往忽略了扰动方向对迁移性的影响，导致迁移能力有限。尽管已知平坦损失景观中的对抗样本具有更好的迁移性，但如何有效地引导扰动方向以达到这些区域仍是挑战。

Method: 本文提出了残差扰动攻击（ResPA）方法。ResPA通过对输入梯度进行指数移动平均（EMA）来获取包含历史梯度方向的参考梯度（第一矩）。与仅依赖当前梯度产生的局部平坦性作为扰动方向不同，ResPA进一步考虑当前梯度与参考梯度之间的残差，以捕捉全局扰动方向的变化，从而引导对抗样本向损失函数的平坦区域移动。

Result: 实验结果表明，ResPA比现有典型的基于迁移的攻击方法具有更好的迁移性。此外，当ResPA与当前输入变换方法结合使用时，其迁移性可以得到进一步的提升。

Conclusion: ResPA通过优化扰动方向，有效利用了残差梯度将对抗样本引导至损失函数的平坦区域，从而成功解决了现有方法在提升对抗样本迁移性方面的局限性，并实现了卓越的攻击效果。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [47] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 现有少样本OOD检测泛化能力不足，本文提出GOOD框架，通过辅助通用知识模型（GKM）和知识动态嵌入（KDE）机制，增强模型泛化能力并提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有少样本OOD检测方法在开放世界中泛化能力不足。由于少样本学习范式，OOD检测能力容易过拟合于有限的训练数据，导致在泛化数据上性能下降，并在不同场景下表现不一致。

Method: 本文提出了广义少样本OOD检测（GOOD）框架。该框架通过一个辅助通用知识模型（GKM）来赋予OOD检测模型通用知识。从泛化角度揭示了少样本OOD检测，并理论推导了OOD检测的通用性-特异性平衡（GS-balance），证明通用知识模型可以降低泛化误差上限。据此，提出知识动态嵌入（KDE）机制，根据GKM的广义信念（G-Belief）自适应地调整通用知识的引导，动态对齐OOD检测模型的输出分布与GKM，从而提升GS-balance。

Result: 在真实世界的OOD基准测试中，所提出的方法表现出优越性。

Conclusion: 通过引入通用知识模型和知识动态嵌入机制，本文提出的GOOD框架有效地解决了少样本OOD检测中泛化能力不足的问题，显著提升了模型在开放世界环境下的性能和一致性。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [48] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 提出UnGuide方法，通过动态推理机制改进LoRA，实现文本到图像扩散模型的选择性机器遗忘，有效移除有害内容同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型存在生成有害内容的风险，急需有效的机器遗忘技术。现有LoRA方法在遗忘特定知识时常会影响无关内容，导致图像保真度下降。

Method: 引入UnGuide方法，其中包含UnGuidance动态推理机制。该机制利用无分类器引导（CFG）对遗忘过程进行精确控制，并根据去噪过程前几步的稳定性调整引导尺度，从而使LoRA适配器能够进行选择性遗忘。对于包含已擦除概念的提示，LoRA模块起主导作用并被基础模型平衡；对于无关提示，基础模型控制生成，保持内容保真度。

Result: 实验结果表明，UnGuide实现了受控的概念移除，并保留了扩散模型的表达能力。在物体擦除和明确内容移除任务中，其性能优于现有的LoRA方法。

Conclusion: UnGuide成功解决了LoRA在机器遗忘中对无关内容的影响问题，实现了对扩散模型知识的精确和选择性移除，同时保持了模型生成内容的质量和表达能力。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [49] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 现有图像风格迁移常作用于整图，但用户常需局部迁移。本文提出一种基于偏卷积的风格迁移网络，能精确地将风格应用于图像特定区域，并引入网络内部融合技术处理区域选择缺陷，实验表明其在视觉和定量上均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有艺术风格迁移算法多应用于整个图像，但用户常只需对图像特定区域进行风格迁移。传统做法是在风格化后进行遮罩，但这种方法无法准确捕捉目标区域的风格特征。

Method: 提出一个基于偏卷积的风格迁移网络，以精确地将风格特征仅应用于感兴趣区域。此外，引入网络内部融合技术，以解决区域选择中的不完善之处。

Result: 研究结果表明，该方法在视觉和定量上均改善了风格化效果，并使用SA-1B数据集的示例进行了验证。

Conclusion: 所提出的基于偏卷积的风格迁移网络及其内部融合技术，能够准确有效地实现对图像特定区域的局部风格迁移，解决了传统方法的不足。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [50] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是一个加速3D医学图像合成框架，结合整流流和区域对比损失，实现了SOTA图像质量和33倍加速，并可用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 医学图像合成对临床和研究至关重要。现有扩散模型存在泛化性差、推理慢、与输入条件对齐弱的问题。先前框架MAISI虽解决泛化性，但仍面临推理慢和条件一致性差的挑战，亟需更快速且高保真度的解决方案。

Method: 本文提出MAISI-v2框架，通过整合整流流（rectified flow）实现快速高质量生成，并引入新颖的区域特定对比损失（region-specific contrastive loss）以增强对感兴趣区域的敏感性和条件保真度。

Result: MAISI-v2在图像质量上达到SOTA水平，并实现潜在扩散模型33倍的加速。合成图像在下游分割实验中被证明可用于数据增强。研究团队已发布代码、训练细节、模型权重及GUI演示以促进复现和进一步发展。

Conclusion: MAISI-v2成功克服了现有3D医学图像合成方法的局限性，实现了快速、高质量且高条件保真度的图像生成，有效支持数据增强，并为社区提供了开放资源。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [51] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一个实用的框架，利用MAE预训练Transformer模型，解决了MRI图像在数据稀缺下的少数样本学习问题，并在分类和分割任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的机器学习在医学影像中潜力巨大，但由于标注数据稀缺，其在真实世界中的应用受限。

Method: 在大规模多队列脑部MRI数据集（超过3100万切片）上采用Masked Autoencoder (MAE) 预训练策略，以获取高度可迁移的潜在表示。对于分类任务，结合冻结的MAE编码器与轻量级线性头。对于分割任务，提出MAE-FUnet混合架构，融合多尺度CNN特征与预训练MAE嵌入。

Result: 在MRI序列识别等高级任务中，仅需少量监督即可达到最先进的准确性。在数据有限的颅骨剥离和多类别解剖分割等低级任务中，持续优于其他强基线。该框架展现了效率、稳定性与可扩展性。

Conclusion: 所提出的框架适用于低资源临床环境和更广泛的神经影像应用，有效解决了数据受限条件下的挑战。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [52] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出了一种无需重建和优化的3D高斯辐射场快速风格迁移方法，通过图结构表面风格化实现，适用于任意风格和辐射场。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯辐射场风格迁移方法通常需要重建、微调辐射场或优化特征提取网络，这些过程复杂且计算成本高昂，限制了其效率和通用性。

Method: 提出了一种无需重建和优化的新方法。该方法首先在辐射场表示的隐式表面上生成图结构，然后应用前馈的、基于表面的风格化方法，最后将结果插值回场景中的单个辐射点。

Result: 该方法支持任意风格图像和3D高斯辐射场，无需额外训练或优化。实现了快速风格化，在消费级硬件上可在2分钟内完成。展示了高质量的风格迁移效果，并优于其他现有方法。

Conclusion: 该研究提出了一种高效、灵活且高质量的3D高斯辐射场风格迁移解决方案，成功克服了现有方法的计算限制，为该领域提供了一种新的、实用的方法。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [53] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN是首个原生处理多缩放图像集的NeRF框架，通过可学习的缩放因子和分阶段的姿态策略，实现在工业检测中高精度捕捉微米级细节的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法难以在工业检测场景中捕捉亚微米级精细结构。添加缩放图像虽可暴露细节，但会破坏NeRF所需的多视角一致性，导致姿态估计困难。

Method: 提出Multi-Zoom Enhanced NeRF (MZEN)，(i) 在针孔相机模型中加入可学习的焦距缩放因子；(ii) 采用两阶段姿态策略：先用宽场图像建立全局框架，再通过缩放一致的裁剪匹配程序预对齐缩放图像，最后联合优化。

Result: 在八个前向场景中，MZEN性能始终优于基线和高分辨率变体，PSNR提升高达28%，SSIM提升10%，LPIPS降低高达222%。

Conclusion: MZEN成功将NeRF应用于真实工业环境，在保持全局精度的同时，有效捕捉工业检测所需的微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [54] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: 本文提出TSMS-SAM2框架，通过多时间尺度视频采样增强和内存分割剪枝机制，显著提升了SAM2在手术视频目标分割与跟踪中的性能，解决了复杂运动和内存冗余的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM2等基础模型在可提示视频目标分割和跟踪（VOST）方面取得了显著进展，但其在手术视频分析中的应用仍面临挑战，主要原因是复杂的运动动态和阻碍有效学习的内存冗余问题。

Method: 本文提出了TSMS-SAM2，一个增强SAM2在手术视频VOST中表现的新颖框架。该框架引入了两个关键策略：1. 多时间尺度视频采样增强，以提高对运动变异的鲁棒性；2. 内存分割和剪枝机制，用于组织和过滤历史帧特征，实现更高效和准确的分割。

Result: 在EndoVis2017和EndoVis2018数据集上进行评估，TSMS-SAM2分别取得了95.24和86.73的最高平均Dice分数，超越了先前的基于SAM和特定任务的方法。广泛的消融研究也证实了多尺度时间增强和内存分割的有效性。

Conclusion: TSMS-SAM2框架有效解决了手术视频中快速物体运动和内存冗余的挑战，实现了鲁棒、高效的分割，展现了其在复杂手术场景中的巨大应用潜力。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [55] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 本文提出TCA（时间聚类分配）策略，通过利用帧间时间一致性优化令牌聚类，在视频分割中有效降低计算成本，同时保持精度，提升了现有聚类方法的性能-速度权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管Vision Transformers（尤其是Swin Transformer）在图像和视频分割中表现出色，但其计算成本高昂，尤其在视频密集预测中成为实时应用的瓶颈。现有令牌削减方法受Swin窗口机制限制，而无训练令牌聚类虽适用于图像，却未能利用视频的时间冗余以进一步优化性能。

Method: 本文提出Temporal Cluster Assignment (TCA)，一种轻量、有效且无需微调的策略。TCA通过利用帧间的时间连贯性来增强令牌聚类，而非简单丢弃冗余令牌。它通过时间相关性精炼令牌聚类，从而在显著减少计算量的同时保留了细粒度细节。

Result: 在YouTube-VIS 2019、YouTube-VIS 2021、OVIS以及一个私有手术视频数据集上的大量评估表明，TCA持续提升了现有基于聚类方法的准确性-速度权衡。实验结果证明TCA能很好地泛化到自然视频和特定领域视频。

Conclusion: TCA是一种有效、轻量且无需微调的策略，通过利用时间连贯性成功解决了Swin Transformer在视频分割中计算成本高的问题，显著提升了现有基于聚类方法的性能-速度权衡，并展现出良好的泛化能力。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [56] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 该研究提出一个视觉-语言框架，通过自然语言预测驾驶员的视觉注意力分配和转移，尤其关注其可解释性。


<details>
  <summary>Details</summary>
Motivation: 驾驶员视觉注意力预测对自动驾驶和人机交互至关重要。然而，现有研究多关注单一时间点的注意力估计，且主要基于静态RGB图像，缺乏对注意力随时间变化的建模和通过自然语言进行可解释性描述的能力。

Method: 研究提出一个视觉-语言框架，通过少量样本和零样本学习，利用单张RGB图像建模驾驶员的注视变化。具体方法包括：通过人机协同从BDD-A数据集中整理并精炼高质量标注；微调LLaVA模型，使其能将视觉感知与以注意力为中心的情景理解对齐；整合低级线索和高级上下文（如路线语义、风险预期），实现基于语言的注视行为描述。

Result: 研究在不同训练方案（少量样本、单样本）下进行了性能评估，并引入了领域特定的语义对齐和响应多样性指标。结果表明，微调后的模型在注意力转移检测和可解释性方面优于通用视觉-语言模型。

Conclusion: 该工作是首次尝试以自然语言生成驾驶员视觉注意力分配和转移预测，为自动驾驶领域的可解释人工智能开辟了新方向。同时，该方法为行为预测、人机协同和多智能体协调等下游任务奠定了基础。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [57] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 一种利用多相机视图进行注视目标估计的方法，旨在克服单视角GTE的局限性并提高精度与适用性。


<details>
  <summary>Details</summary>
Motivation: 现有单视角注视目标估计（GTE）方法存在面部遮挡、目标模糊和目标超出视野等局限性，导致精度和适用性受限。

Method: 提出一种多视角GTE方法，输入一对相机视图，包含：头部信息聚合（HIA）模块用于融合多视角头部信息，基于不确定性的注视选择（UGS）模块用于识别最可靠的注视输出，以及基于对极几何的场景注意力（ESA）模块用于跨视角背景信息共享。

Result: 该方法显著优于单视角基线方法，尤其当第二相机提供清晰面部视图时表现更佳。此外，仅使用第二视角图像也能估计第一视角的注视目标。论文还引入了一个新的多视角GTE数据集。

Conclusion: 本方法通过整合多视角信息和创新模块，有效克服了单视角GTE的局限，显著提升了注视目标估计的准确性和适用性，并实现了单视角方法无法实现的跨视角注视估计能力。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [58] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: 预训练视觉-语言模型（VLMs）在分布偏移下泛化能力受限，测试时自适应（TTA）是应对方法。针对现有缓存式TTA的局限性，本文提出ETTA，通过递归更新所有测试样本并自适应融合多提示，显著提升了TTA的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练视觉-语言模型（如CLIP）具有强大的零样本性能，但在面临数据分布偏移时，其泛化能力会显著下降。测试时自适应（TTA）旨在通过适应未标记测试数据来解决此问题。然而，现有高效的基于缓存的TTA方法仅存储少量高置信度样本，导致决策边界受限，并忽略了其他传入测试数据的影响，这是需要解决的瓶颈。

Method: 本文提出高效测试时自适应（ETTA），包含：1. **递归更新模块**：整合所有传入的测试样本，通过逐步细化决策边界，模拟无限缓存，动态更新上下文嵌入，以提高准确性并降低开销。2. **自适应集成模块**：通过为每个类别动态选择最佳提示，减少图像到文本得分对提示的依赖。ETTA还根据置信水平自适应地结合这两个模块的得分，充分利用它们的互补优势。

Result: 在两项广泛的基准测试中，ETTA在计算复杂度和准确性方面均超越了当前最先进的测试时自适应（TTA）模型。

Conclusion: ETTA为有效且高效的测试时自适应设立了新标准，显著提升了预训练视觉-语言模型在分布偏移场景下的性能。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [59] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个先进的视觉-语言引导框架，能根据文本描述生成多样高质量的3D场景，支持交互式编辑，并在游戏建模中展现潜力。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景设计依赖大量人工，现有自动化方法难以生成开放域场景或支持灵活编辑，因此直接从文本生成3D世界受到日益增长的关注。

Method: HOLODECK 2.0利用视觉-语言模型（VLM）识别和解析场景所需对象，通过先进的3D生成模型生成相应的高质量资产，然后迭代应用从VLM导出的空间约束以实现语义连贯和物理合理的布局。它还支持基于人类反馈的交互式场景编辑。

Result: HOLODECK 2.0能够生成多样化且风格丰富（如写实、卡通、赛博朋克）的3D场景，这些场景与精细的输入描述具有高语义保真度，适用于室内和开放域环境，并持续优于基线。此外，它提供了灵活的编辑能力，支持布局优化和风格一致的对象编辑。

Conclusion: HOLODECK 2.0有效地生成了与详细文本描述高度一致的高质量3D场景，克服了现有方法的局限性，并在程序化游戏建模等实际应用中展示出提升效率的巨大潜力。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [60] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch是一个无监督的深度图像拼接框架，通过双分支架构提升鲁棒性，并引入虚拟最优平面概念解决内容对齐与结构保持的矛盾，从而实现高质量、自然且泛化性强的图像拼接。


<details>
  <summary>Details</summary>
Motivation: 现有的图像拼接方法难以同时保证鲁棒性和自然度，尤其在内容对齐和结构保持之间存在固有矛盾，导致在多样化、未见过的真实场景中泛化能力不足。

Method: 该研究提出双分支架构（包含预训练分支捕捉语义不变特征和可学习分支提取细粒度特征，并融合）以确保鲁棒性；为解决内容对齐与结构保持的矛盾，引入虚拟最优平面概念，将其建模为单应分解系数估计问题，并通过迭代系数预测器和最小语义畸变约束来识别最优平面，最终将两视图双向形变到该最优平面上。

Result: 在各种数据集上的广泛实验表明，RopStitch显著优于现有方法，特别是在场景鲁棒性和内容自然度方面表现出色。

Conclusion: RopStitch通过其独特的双分支架构和虚拟最优平面策略，成功解决了图像拼接中的鲁棒性与自然度难题，实现了在复杂真实场景下的卓越拼接性能。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [61] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 该论文展示了如何利用手机作为计算成像平台，结合精心设计的神经场模型，直接从原始手机数据中实现深度估计、图层分离和图像拼接等应用，其性能优于现有方法，且无需复杂预处理或标注数据。


<details>
  <summary>Details</summary>
Motivation: 过去二十年手机成像技术飞速发展，现代手机集成了多种成像和非视觉传感器以及板载处理芯片，使其成为功能强大的口袋型计算成像平台。同时，神经场已被证明能不依赖显式数据表示重建复杂场景。因此，研究动机是利用这些进展，直接从移动摄影数据中解决复杂的几何和光照问题。

Method: 采用精心设计的神经场模型，通过随机梯度下降直接拟合智能手机的原始测量数据，来紧凑地表示复杂的几何和光照效果。这些模型是自正则化的，不依赖复杂的预处理步骤、标记的真值数据或机器学习先验知识。

Result: 所提出的方法能够直接从“野外”收集的移动摄影数据中实现深度估计、图层分离和图像拼接等应用。这些方法在性能上超越了最先进的方法，且无需复杂的预处理、标注数据或机器学习先验。

Conclusion: 该论文证明了精心设计的神经场模型可以有效利用智能手机的原始测量数据，紧凑地表示复杂场景的几何和光照，从而解决具有挑战性的逆问题，并实现优于现有技术的先进移动成像应用，而无需传统的数据准备负担。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [62] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 本论文评估了SAM和Mask3D两种3D分割模型在建筑施工环境中的应用，旨在提升进度监测效率，并指出室外场景基准缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 建筑进度监测耗时且资源密集，传统方法在复杂多变的施工现场（特别是室外环境）效果不佳，因此需要探索基于计算机视觉的自动化方法来提高效率和可扩展性。

Method: 本研究批判性地评估了两种先进的3D分割方法——Segment Anything Model (SAM) 和 Mask3D，在具有挑战性的室内外施工条件下的应用。通过比较分析，评估了它们在实际施工环境中的适应性和性能。

Result: 研究揭示了现有分割方法在室外场景中缺乏基准的局限性。通过比较分析，展示了SAM和Mask3D的相对有效性，并强调了开发定制化分割工作流以从施工现场数据中提取可行性洞察的关键需求。

Conclusion: 本研究通过评估SAM和Mask3D在施工环境中的应用，推动了建筑进度监测领域向更自动化和精确技术的发展，并强调了为复杂施工数据定制分割策略的重要性。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [63] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: 本文提出SINGAD，一种基于3D高斯泼溅引导扩散的自监督框架，用于单图法线估计，解决现有方法在多视角一致性和数据依赖性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 从单张图像估计法线时，缺乏空间维度信息是一个挑战。现有基于扩散的方法依赖数据驱动先验且缺少光-表面交互的显式建模，导致多视角法线方向冲突。此外，扩散模型的离散采样机制导致可微渲染模块中梯度不连续，阻碍3D几何误差反向传播，使现有方法依赖于密集的法线标注。

Method: 本文提出SINGAD框架，通过整合物理驱动的光交互建模和基于可微渲染的重投影策略，将3D几何误差直接转化为法线优化信号。具体而言，构建了光交互驱动的3DGS重参数化模型以生成多尺度几何特征，并设计了一个条件扩散模型内的跨域特征融合模块。此外，引入了可微3D重投影损失策略进行自监督优化，以消除对标注法线数据集的依赖。

Result: 在Google Scanned Objects数据集上的定量评估表明，SINGAD在多项指标上优于现有最先进的方法。

Conclusion: SINGAD通过结合物理驱动的光交互建模和可微重投影策略，成功解决了单图法线估计中的多视角几何不一致性和数据依赖性问题，实现了卓越的性能。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [64] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一个统一框架，通过补丁级CLIP嵌入桥接预训练多模态LLM和扩散模型，实现高效高保真图像生成，同时保持多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将高保真视觉合成能力集成到大型语言模型(LLMs)时，面临训练成本高昂的问题，因为LLMs在预训练期间未接触过图像表示，且可能影响其强大的推理能力。

Method: Bifrost-1框架通过使用补丁级CLIP图像嵌入作为潜在变量，连接预训练的多模态LLM (MLLM)和扩散模型，这些嵌入与MLLM的CLIP视觉编码器原生对齐。这些补丁级嵌入通过ControlNet的轻量级适配集成到扩散模型中。为保留MLLM原有的多模态推理能力，MLLM配备了一个从原始参数初始化的视觉生成分支，用于预测补丁级图像嵌入。

Result: 实验表明，Bifrost-1在视觉保真度和多模态理解方面达到了与现有方法相当或更优的性能，并且显著降低了训练计算成本。

Conclusion: Bifrost-1通过无缝整合预训练MLLM和扩散模型与补丁级CLIP潜在变量，成功实现了高保真、可控的图像生成，同时保证了高效训练和多模态推理能力的保留。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [65] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: 本文提出Primitive-Aware Semantic Grounding (PASG) 框架，通过自动几何基元提取和VLM驱动的语义锚定，有效弥合了机器人操作中高层任务语义与低层几何特征之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中高层任务语义与低层几何特征之间存在持续挑战。现有视觉-语言模型（VLMs）虽有前景，但因缺乏规范空间中的语义接地及过度依赖手动标注，限制了其捕捉动态语义-功能关系的能力。

Method: 提出PASG闭环框架，包含：1) 通过几何特征聚合实现自动基元提取，支持跨类别关键点和轴检测；2) VLM驱动的语义锚定，动态关联几何基元与功能可供性和任务描述；3) 构建空间-语义推理基准并微调VLM（Qwen2.5VL-PA）。

Result: PASG在实际机器人操作任务中表现出与手动标注相当的性能，并实现了对物体更细粒度的语义-功能理解。

Conclusion: PASG为机器人操作中连接几何基元与任务语义提供了一个统一范式，有效解决了语义与几何特征碎片化的问题。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [66] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene提出一个统一框架，解决4D人体动画与3D场景集成中的人体精确放置、风格光照对齐及动态摄像机运动难题，生成高质量、视觉连贯的动态场景视频。


<details>
  <summary>Details</summary>
Motivation: 尽管3D场景重建和4D人体动画取得快速进展，但将两者无缝集成以生成视觉吸引力的结果仍面临挑战：1) 人体在场景中放置不当且易发生穿透；2) 人体与背景光照和风格不一致导致合成不真实；3) 制作伴有摄像机运动的角色视频时，视角重建困难。

Method: 1. 设计一个精确放置模块，自动确定人体合理3D位置并防止运动中穿透。2. 提出一种免训练的风格对齐方法，使4D人体表示匹配背景光照和风格。3. 设计4D人体和3D场景的联合后重建方法，支持插入摄像机轨迹以实现动态视角。

Result: AnimateScene能够生成具有高几何细节和时空连贯性的动态场景视频，并在各种摄像机和动作组合下表现出色。

Conclusion: AnimateScene提供了一个有效的统一框架，解决了4D人体动画与3D场景集成中的关键技术障碍，显著提升了合成视频的真实感和视觉吸引力。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [67] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出一种名为ETA的测试时自适应方法，通过能量模型和对抗性扰动，使预训练深度补全模型在面对协变量漂移时也能保持准确性。


<details>
  <summary>Details</summary>
Motivation: 预训练的深度补全模型在新的环境条件（目标数据）下，由于协变量漂移，常出现预测误差。传统方法难以在部署前获取目标数据分布信息，使得模型泛化性受限。

Method: 核心在于量化深度预测属于源数据分布的可能性。为解决无目标数据的问题，利用对抗性扰动探索数据空间，训练一个能量模型来评估深度预测的局部区域是内部分布还是外部分布。在测试时，通过最小化能量来更新预训练模型的参数，使预测结果与源分布对齐。

Result: 在三个室内和三个室外数据集上进行了评估。ETA方法在室外数据集上比现有最佳方法平均提升6.94%，在室内数据集上平均提升10.23%。

Conclusion: ETA方法有效地解决了深度补全模型在测试时因协变量漂移导致的性能下降问题，显著提升了模型在不同新环境下的适应性和预测准确性。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [68] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 提出一种高效视频计算机视觉系统，通过移除ISP、采用快速块匹配运动估计和引入上下文感知块细化网络，并结合帧选择策略，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 视频计算机视觉系统因高时间冗余而效率低下。现有方法未能充分减少时间冗余，且忽略了前端计算开销，是该研究的动机。

Method: 该系统首先移除图像信号处理器（ISP），直接使用Bayer格式数据；其次，提出一种基于快速块匹配的运动估计算法，并包含运动矢量细化模块；引入上下文感知块细化网络以纠正大误差区域；最后，采用帧选择策略平衡准确性与效率。

Result: 实验结果表明，该方法在多种视频计算机视觉任务上实现了显著的加速，且性能损失轻微。

Conclusion: 该研究所提出的方法有效解决了视频计算机视觉系统的效率挑战，通过集成多种优化策略，实现了计算效率和性能之间的良好平衡。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [69] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态情感识别框架，用于MER2025-SEMI挑战，通过利用预训练模型、设计创新的特征提取和融合策略，并结合噪声标签精炼，显著提升了情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 情感识别在增强人机交互中扮演着重要角色。本研究旨在解决MER2025竞赛中MER-SEMI挑战，特别是针对数据稀缺问题，以提升多模态情感识别的性能。

Method: 1. 提出新型多模态情感识别框架。 2. 利用大规模预训练模型从视觉、音频和文本模态中提取特征。 3. 视觉模态：设计双分支视觉编码器捕获全局帧级和局部面部表示。 4. 文本模态：引入上下文丰富方法，利用大型语言模型增强文本情感线索。 5. 特征融合：采用自注意力机制进行动态模态加权，并使用残差连接保留原始表示。 6. 标签精炼：通过多源标注策略优化训练集中的噪声标签。

Result: 在MER2025-SEMI数据集上，该方法比官方基线取得了显著性能提升。加权F-score从基线的78.63%提高到87.49%。

Conclusion: 所提出的框架是有效的，其在MER2025-SEMI数据集上的卓越表现验证了该方法的有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [70] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: EvoMakeup是一个统一的化妆编辑框架，通过引入MakeupQuad数据集解决现有方法细节不足和身份/妆容保真度差的问题，实现了高质量、可控的多任务化妆编辑。


<details>
  <summary>Details</summary>
Motivation: 现有面部化妆编辑方法常产生低质量结果，妆容细节粗糙，且难以同时保持身份和妆容保真度。这主要是由于缺乏结构化的配对数据（即源脸与结果脸身份一致，参考妆容与结果妆容一致）导致的。

Method: 为解决数据缺乏问题，引入了大规模高质量数据集MakeupQuad，包含素颜脸、参考妆容、编辑结果和文本描述。在此基础上，提出了EvoMakeup统一训练框架，通过多阶段蒸馏减轻图像退化，并支持数据和模型质量的迭代改进。

Result: EvoMakeup虽仅在合成数据上训练，但在真实世界基准测试中泛化良好并超越了现有方法。它在一个模型内支持高保真、可控的多任务化妆编辑，包括全脸和局部基于参考的编辑，以及文本驱动的化妆编辑。实验结果表明，该方法在妆容保真度和身份保持方面均取得了卓越性能，有效平衡了两者。

Conclusion: EvoMakeup通过构建高质量数据集和提出统一训练框架，成功克服了现有化妆编辑方法的局限性，实现了高质量、多功能且能有效平衡身份与妆容保真度的面部化妆编辑。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [71] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: 引入MathReal数据集以评估多模态大语言模型在真实K-12数学推理中的表现，发现现有模型在该场景下仍面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估多模态大语言模型（MLLMs）视觉数学推理能力的基准数据集多基于干净或处理过的图像，未能反映真实K-12教育用户在日常使用中（如手持设备拍摄）遇到的复杂图像情况，导致模型在现实场景下的实际能力评估存在空白。

Method: 构建了MathReal数据集，包含2000个在真实场景中通过手持移动设备捕获的数学问题图像。该数据集将图像分为3大类14个子类（如图像质量下降、透视变化、无关内容干扰），并涵盖5个核心知识和能力类别、3种问题类型及3个难度级别。通过设计6种实验设置，对现有最先进的MLLMs进行了系统评估。

Result: 实验结果表明，现有MLLMs在现实教育场景下的数学问题解决能力受到显著挑战，表现不佳。

Conclusion: 当前MLLMs在真实K-12教育环境中的视觉数学推理能力仍有待提高。本研究对它们的性能和错误模式进行了深入分析，揭示了其在识别、理解和推理方面的局限性，并为未来的改进指明了方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [72] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出一种基于3DGS的方法，通过生成额外的训练视角并结合信息增益驱动的相机放置和视频扩散先验，显著改善了偏离训练轨迹的视角下的新颖视图合成质量，实现了无伪影的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting (3DGS) 方法在渲染偏离训练轨迹的视角时存在伪影和缺失区域，限制了无缝的场景探索。

Method: 提出一个基于3DGS的管道来生成额外的训练视图以增强重建。该方法包含信息增益驱动的虚拟相机放置策略以最大化场景覆盖，并结合视频扩散先验来优化渲染结果。然后使用这些增强的视图对3D Gaussians进行微调。

Result: 显著提高了重建质量。在为挑战性场景探索设计的Wild-Explore基准测试中，该方法优于现有的3DGS方法，实现了从任意视点进行高质量、无伪影的渲染。

Conclusion: 通过智能生成和利用额外训练视图，本方法有效解决了3DGS在偏离训练轨迹视角下的渲染质量问题，为高质量、无缝的场景探索提供了解决方案。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [73] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 本研究利用先进的扩散模型生成高保真图像，解决了亚可见粒子分析中数据稀缺和类别不平衡问题，显著提升了深度学习分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 在亚可见粒子分析中，尽管结合深度学习的流式成像显微镜能有效识别粒子类型（如区分硅油和蛋白质），但数据稀缺及粒子类型间（特别是硅油、气泡等稀有类型）严重不平衡是应用多类别分类器的主要障碍，导致研究者常依赖效果不佳的方法。

Method: 开发了一种最先进的扩散模型，用于生成高保真图像以扩充训练数据集，从而解决数据不平衡问题，并有效训练多类别深度神经网络。

Result: 生成的图像在视觉质量和结构上与真实粒子图像高度相似。在大规模实验（使用50万张蛋白质粒子图像的验证数据集）中证明，该方法显著提高了分类性能，且无明显负面影响。

Conclusion: 本研究通过扩散模型成功应对了亚可见粒子分析中的数据不平衡挑战，有效提升了多类别分类器的性能。为促进开放研究和可复现性，研究团队已公开了扩散模型、训练好的分类器及简易接口。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [74] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: 提出Spectrum统一网络，通过重用微调的Image-to-Texture (I2Tx)扩散模型，实现细粒度人体部位与服装解析，性能超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的人体解析方法类别固定且粗略，难以区分细粒度服装；开放词汇分割将整个人视为单一类别，缺乏细节；通用扩散模型虽泛化性强，但其内部表示不适合精细人体解析。

Method: Spectrum是一个统一网络，用于像素级解析和实例级分组。它创新性地重用一个经3D人体纹理图微调的Image-to-Texture (I2Tx)扩散模型，以获取更强的人体部位和服装表示。通过I2Tx模型提取内部特征，并利用提示词引导生成语义有效的掩码。训练后，可为图像中任何人生成所有可见人体部位和服装类别的语义分割图。

Result: 在广泛的跨数据集实验（包括身体部位、服装部位、未见过的服装类别和全身掩码）中，Spectrum在基于提示的分割方面持续优于基线方法。

Conclusion: Spectrum通过其独特的I2Tx扩散模型重新利用方法，有效解决了细粒度人体解析的难题，特别是在处理多样化服装和身体部位方面表现出色，并显著优于现有技术，为该领域提供了新的解决方案。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [75] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种基于RectifiedFlow的快速文本引导图像编辑方法，通过多项创新技术实现高效、高质量且内容保留的编辑，性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种快速、能严格遵循文本指令，同时有效保留图像关键内容并生成一致且可编辑结果的文本引导图像编辑方法。

Method: 该方法基于RectifiedFlow框架，是一个少步骤编辑过程。具体技术包括：引入专用反演策略PerRFI利用RectifiedFlow的直线采样轨迹；提出新颖的Inversion Latent Injection再生方法，重用反演潜在信息以促进更连贯详细的再生；提出Disentangled Prompt Guidance技术以平衡可编辑性与细节保留；整合Canny条件ControlNet以融入结构线索并抑制伪影。

Result: 在PIE图像编辑数据集上的评估显示，InstantEdit不仅速度快，而且与现有最先进的少步骤编辑方法相比，取得了更好的定性和定量结果。

Conclusion: InstantEdit成功提供了一种快速且高质量的文本引导图像编辑解决方案，在速度和编辑效果上均超越了现有最先进的方法。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [76] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出一个针对MER2025半监督情感识别（MER-SEMI）挑战赛的解决方案，通过构建一个基于“越多越好”原则的专家混合（MoE）框架，整合多模态输入、共识伪标签策略、两阶段训练以及多专家投票与规则重排，在测试集上取得了0.8772的F1分数并排名第二。


<details>
  <summary>Details</summary>
Motivation: 为MER2025半监督情感识别（MER-SEMI）挑战赛提供一个稳健的解决方案，旨在通过整合多样化信息构建一个强大的专家混合（MoE）情感识别系统，以有效利用无标签数据并提升识别性能。

Method: 1. 提出一个基于“越多越好”原则的专家混合（MoE）框架。
2. 整合多种输入模态作为独立专家，包括大视觉语言模型（VLM）知识和时序动作单元（AU）信息。
3. 引入基于共识的伪标签策略，通过基线模型与Gemini的一致性生成高质量标签，用于无标签数据。
4. 采用两阶段训练范式。
5. 结合多专家投票集成和基于规则的重排过程，以校正预测偏差并更好地符合人类偏好。

Result: 在MER2025-SEMI挑战赛数据集的测试集上，该方法获得了0.8772的F1分数，在该赛道中排名第二。

Conclusion: 所提出的多模态专家混合情感识别框架，通过有效的无标签数据利用和集成策略，在半监督情感识别任务中表现出卓越的性能和鲁棒性，并在MER2025挑战赛中取得了优异成绩。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [77] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [78] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 本文提出一种名为NEP的新型图像编辑范式，通过自回归生成仅选择性地再生需编辑区域，避免了不必要的计算并提高了编辑质量，实现了文本引导图像编辑的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑方法在进行局部修改时需生成整个图像，导致计算成本高昂，且非编辑区域的重建偏置会影响预期编辑的质量。

Method: 将图像编辑重新定义为基于自回归图像生成的“下一编辑令牌预测”（NEP）问题，仅再生需要编辑的区域。为此，预训练了一个支持任意顺序的自回归文本到图像（T2I）模型以实现任意区域编辑。

Result: 训练后的模型能够进行零样本图像编辑，并易于适应NEP。在常用图像编辑基准上取得了新的最先进（SOTA）性能。此外，模型自然支持通过零样本迭代细化生成来实现测试时缩放（TTS）。

Conclusion: 所提出的NEP方法有效解决了现有图像编辑的局限性，通过精准的局部区域再生显著提升了编辑效率和质量，并设立了新的性能标杆。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [79] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker利用强化学习和大型多模态模型解决了视频质量评估中泛化性差和可解释性不足的问题，并在域内和域外VQA基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估（VQA）模型在域外视频上泛化能力差且可解释性有限，限制了其在实际场景中的应用。

Method: 提出VQAThinker框架，结合大型多模态模型（LMMs）与强化学习，模拟人类感知决策过程，共同建模视频质量理解和评分。采用基于规则的强化学习算法GRPO，并引入钟形回归奖励、成对排序奖励和时间一致性奖励三种VQA特定奖励。

Result: VQAThinker在域内和域外VQA基准测试中均达到最先进性能，表现出强大的泛化能力。在视频质量理解任务中，其在失真归因和质量描述方面优于现有可解释VQA模型和LMMs。

Conclusion: 强化学习为仅依赖分数级监督构建可泛化且可解释的VQA模型提供了一条有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [80] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net通过变形解剖学模板，从脑MRI生成个性化3D侧脑室（LV）网格，提高LV形状分析的准确性和鲁棒性，并成功应用于阿尔茨海默病分析。


<details>
  <summary>Details</summary>
Motivation: 侧脑室（LV）形状分析作为神经系统疾病的生物标志物具有前景，但个体间形状差异大以及MRI分辨率限制导致分割困难，这些挑战阻碍了其应用。

Method: 引入LV-Net框架，通过变形一个解剖学感知的联合LV-海马模板网格，从脑MRI生成个体化的3D LV网格。该方法结合了模板中嵌入的解剖关系以减少分割伪影，并通过基于解剖邻近性分类模板网格顶点来增强主体间的点对应，从而提高形状统计的准确性。

Result: LV-Net在存在分割缺陷的情况下仍能实现卓越的重建精度，并在不同数据集中提供更可靠的形状描述符。此外，将LV-Net应用于阿尔茨海默病分析，成功识别出与疾病显著相关的LV亚区域。

Conclusion: LV-Net为脑侧脑室形状分析提供了一个鲁棒且精确的框架，克服了传统方法面临的挑战，有望成为神经系统疾病，特别是阿尔茨海默病研究的有效工具，其代码已开源。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [81] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 卫星光谱图像作为AGI新模态潜力巨大但被忽视。本文论证其重要性，批判现有基准局限，并提出构建全面地球观测评估基准的任务建议。


<details>
  <summary>Details</summary>
Motivation: AGI发展驱动多模态研究，但卫星光谱图像作为重要模态未获足够关注。其在增强AGI对自然世界理解方面潜力巨大，然而现有地球观测评估基准在衡量基础模型泛化能力方面存在局限。

Method: 本文论证了地球观测数据对智能模型的用处，审视并指出现有基准在评估基础模型泛化能力上的不足，并提出了一套综合任务，以指导建立更全面的地球观测模型评估基准。

Result: 通过分析，发现现有地球观测基准在评估基础模型的泛化能力方面存在显著局限性，从而强调了建立更全面评估基准的必要性。

Conclusion: 迫切需要一个更全面的基准来评估地球观测模型，并提出了应包含的任务集合，旨在有效评估模型对地球观测数据的理解和交互能力，以促进未来基准的开发。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [82] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出TSANet，一个轻量级两阶段网络，用于解决HybridEVS相机在移动设备上的事件像素修复和去马赛克问题，显著提升性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: HybridEVS事件相机在移动摄影中应用前景广阔，但其四拜耳滤色阵列与无颜色信息事件像素结合导致去马赛克过程中出现伪影和混叠，现有方法难以在资源受限的移动设备上有效解决。

Method: 引入TSANet，一个轻量级两阶段网络，通过状态空间增强的交叉注意力机制，将事件像素修复和去马赛克独立处理。设计了轻量级Cross-Swin状态块，利用位置先验进行去马赛克，并通过线性复杂度的状态空间模型增强全局依赖。

Result: TSANet在模拟和真实HybridEVS数据上均展现出优异的去马赛克性能，在七个多样化数据集上，PSNR和SSIM均优于SOTA方法DemosaicFormer，同时参数量和计算成本分别降低1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新的可能性，解决了HybridEVS相机面临的关键技术挑战，并实现了性能与效率的双重提升。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [83] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 本文提出SCJoint联合学习方案和SBSS采样策略，实现了显著目标检测（SOD）和伪装目标检测（COD）的有效联合学习，并开发了通用网络JoNet。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测（SOD）与伪装目标检测（COD）是两种密切相关但又具有强烈矛盾属性的计算机视觉任务，以往研究普遍认为联合学习会降低性能。本文旨在挑战这一观点，探索通过正确方法实现二者联合学习并使其相互受益的可能性。

Method: 提出SCJoint联合学习方案，假设SOD和COD解码过程具有不同的分布特性，通过在共享网络中插入少量任务特定的可学习参数，学习各自的均值和方差，以最小成本解耦矛盾属性。此外，提出基于显著性的采样策略（SBSS），用于平衡训练集大小、提高训练质量和缩短训练时间。基于SCJoint和SBSS，训练出通用网络JoNet。

Result: 广泛的实验证明所提出方法具有竞争力且有效。训练出的JoNet网络能够同时捕获“显著”和“伪装”目标。

Conclusion: 与传统观点相反，通过正确的方法，显著目标检测和伪装目标检测的联合学习是可行且有效的，能够使网络同时具备检测显著和伪装目标的能力，从而实现双任务互惠。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [84] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出BioMotion Arena，一个通过视觉动画评估大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的新颖框架，揭示了现有模型在生成生物运动方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前的大模型评估方法（如基于静态数据集的得分评估或模糊的聊天机器人式人类偏好收集）难以评估模型能力并直观地显示其差距，无法提供即时、直观且可感知的性能差异反馈。

Method: 引入BioMotion Arena框架，利用生物固有的运动模式视觉感知特性，通过点光源成像放大模型间的性能差异。采用配对比较评估方式，收集了53个主流LLMs和MLLMs在90种生物运动变体上的超过4.5万次众包投票。

Result: 众包投票与专家评估者高度一致，证明BioMotion Arena在提供区分性反馈方面的优越性。研究发现，包括先进的InternVL3和Claude-4系列在内，超过90%的受评估模型未能生成基本的人形点光源群组，更遑论平滑、符合生物学原理的运动。

Conclusion: BioMotion Arena是一个具有挑战性的性能可视化基准，也是一个灵活且不受地面真值限制的评估框架，有效揭示了当前大模型在生成生物运动方面的局限性。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [85] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出一种基于MR的AI管道，能从常规MR扫描生成超分辨率的3D伪健康膝关节目标形态，用于滑车发育不良(TD)的手术规划，无需CT，显著改善解剖参数。


<details>
  <summary>Details</summary>
Motivation: 目前滑车发育不良(TD)的治疗依赖低分辨率MR和外科医生经验，导致手术规划不精确、微创技术应用受限、结果不一致。

Method: 该研究开发了一个流程：首先，利用隐式神经表示(INR)将临床MR扫描转换为各向同性超分辨率MR体积；其次，使用多标签自定义网络分割出股骨、胫骨、髌骨和腓骨；最后，训练一个名为小波扩散模型(WDM)来生成滑车区域的伪健康目标形态。该方法无需CT扫描。

Result: 在25名TD患者中进行评估，结果显示生成的目标形态显著改善了股骨沟角(SA)和滑车沟深度(TGD)。

Conclusion: 该方法可为股骨沟整形提供亚毫米级分辨率的术前蓝图，同时保留髌骨关节的自然衔接，且无需辐射性CT扫描，有望实现更精确、一致的TD手术结果。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [86] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出DreamVE模型，通过两阶段训练和混合数据合成（拼贴与生成模型）策略，解决指令式图像与视频编辑，特别是视频编辑面临的数据稀缺问题，实现高效且高质量的统一编辑。


<details>
  <summary>Details</summary>
Motivation: 指令式视频编辑因训练数据匮乏，限制了其实际应用。

Method: 1. 引入DreamVE模型，统一指令式图像和视频编辑。2. 采用两阶段训练策略：首先进行图像编辑训练，然后进行视频编辑训练，利用图像数据易于扩展的优势提供有效先验。3. 构建综合训练数据合成管线：包括拼贴合成数据（用于大规模多样化预训练）和生成模型合成数据（用于补充属性编辑并进行微调）。4. 设计高效编辑框架：基于SOTA T2V模型，通过token拼接与早期丢弃方法注入源图像指导，确保高一致性与可编辑性。

Result: DreamVE在关键编辑类型上表现出强大的性能，并显著提升了泛化和迁移能力。

Conclusion: DreamVE通过创新的两阶段训练和混合数据合成策略，有效解决了指令式图像和视频编辑中数据不足的挑战，提供了一个统一、高效且高性能的解决方案，具有广阔的应用前景。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [87] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出SwiftVideo，一个统一稳定的蒸馏框架，结合轨迹保持和分布匹配策略，旨在显著减少扩散或流模型在视频合成中的推理步数，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 扩散或流模型在视频合成中取得了显著进展，但需要多步迭代采样，导致巨大的计算开销。现有的加速蒸馏方法（如仅基于轨迹保持或分布匹配的）在少步设置下常出现性能下降或伪影增加问题。

Method: 提出SwiftVideo框架，它是一个统一且稳定的蒸馏方法，结合了轨迹保持和分布匹配策略的优点。具体包括：1) 引入连续时间一致性蒸馏以精确保留ODE轨迹；2) 提出双重视角对齐，涵盖合成数据与真实数据间的分布对齐，以及不同推理步之间的轨迹对齐。

Result: 该方法在显著减少推理步数的同时，保持了高质量的视频生成。在OpenVid-1M基准测试中，SwiftVideo在少步视频生成方面显著优于现有方法。

Conclusion: SwiftVideo成功解决了视频生成模型在少步推理下的效率与质量平衡问题，通过其创新的蒸馏框架实现了高效且高质量的视频合成。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [88] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: 本文提出AdaptInfer框架，通过动态文本引导剪枝和优化剪枝策略，显著降低视觉语言模型（VLMs）的推理成本，同时保持高准确率，甚至超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态推理任务上表现出色，但由于在预填充阶段处理大量视觉tokens，导致推理成本高昂。现有剪枝方法未能有效利用推理过程中生成的动态内部信号。

Method: 本文提出AdaptInfer，一个即插即用的自适应视觉token剪枝框架。具体方法包括：1) 引入细粒度的动态文本引导剪枝机制，重用层间文本到文本注意力图来构建文本token重要性的软先验，以更明智地评估视觉token。2) 离线分析跨模态注意力变化，识别推理中一致的拐点位置，从而提出更合理高效的剪枝策略。该方法轻量、即插即用且通用性强。

Result: 实验结果验证了所提出方法的有效性。例如，在vanilla LLaVA-1.5-7B模型上，它将CUDA延迟降低61.3%，同时保持92.9%的平均准确率。在相同的token预算下，AdaptInfer在准确率上超越了现有SOTA方法。

Conclusion: AdaptInfer通过动态、文本引导的视觉token剪枝和优化的剪枝策略，成功解决了VLM推理成本高的挑战，显著提高了推理效率，同时保持或提高了准确率，展现出强大的实用性和通用性。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [89] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP是首个基于视觉-语言模型（VLMs）的视频质量评估（VQA）框架，通过引入轻量级适配器和可学习的质量提示，显著降低计算成本并提升VQA性能及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流VQA方法依赖于大规模预训练，但存在两大挑战：1) 预训练学到的语义知识不足以全面评估视频质量（视频质量受多因素影响）；2) 大规模预训练耗费巨大计算资源。

Method: 提出Q-CLIP，首个完全基于VLMs的VQA框架。主要创新包括：1) 通过共享跨模态适配器（SCMA）增强视觉和文本表示，该适配器参数量极小且为唯一需训练组件，大幅降低计算成本；2) 引入五种可学习的质量级别提示，引导VLMs感知细微质量差异；3) 探索帧采样策略，发现基于帧差异的采样有助于提升跨数据集泛化性能。

Result: Q-CLIP在多个VQA数据集上表现出卓越的性能。

Conclusion: Q-CLIP作为首个基于VLMs的VQA框架，成功解决了现有方法的计算效率和泛化能力不足问题，在视频质量评估中展现出优异的性能和跨数据集泛化能力。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [90] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出一种新方法，利用半监督情感先验和扩散模型，生成对不同情感线索的逼真反应动作，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动生成框架未充分考虑情感因素，导致生成的动作不够自然，限制了其在交互任务（如人类反应合成）中的应用。此外，从有限的运动数据中学习情感表示并将其有效融入运动生成框架是一个挑战。

Method: 引入了一项新任务：根据不同的情感线索生成多样化的反应动作。为解决上述问题，本研究在“演员-反应者”扩散模型中引入了半监督情感先验。具体地，首先基于短序列动作情感一致性的观察，构建了一个半监督学习框架来训练情感先验；随后，利用该情感先验训练“演员-反应者”扩散模型，使其在生成反应时能够同时考虑空间交互和情感响应。

Result: 该方法能够根据演员的运动序列，在各种情感条件下生成逼真的反应动作。实验结果表明，本模型在反应生成方面优于现有方法。

Conclusion: 本研究成功地提出并解决了情感驱动的反应动作生成问题，通过引入半监督情感先验和创新的模型架构，显著提升了合成动作的自然性和多样性，扩展了运动生成在交互任务中的应用潜力。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [91] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出UGD-IML，一个基于扩散模型的生成框架，首次统一了图像篡改定位（IML）和受限IML（CIML）任务，解决了现有方法对大数据集依赖和标注效率低下的问题，并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像篡改定位（IML）方法依赖大规模高质量标注数据集，但现有数据集规模和多样性不足，限制了模型在实际场景中的表现。受限IML（CIML）虽能算法生成像素级标注，但现有CIML方法通常依赖复杂的多阶段流程，导致标注过程效率低下。

Method: 本研究提出了UGD-IML，一个基于扩散模型的生成框架，首次将IML和CIML任务统一于单一框架内。UGD-IML通过学习底层数据分布来减少对大规模标注数据集的依赖，使其在数据有限的情况下也能有效运行。此外，该模型利用类别嵌入机制和参数共享设计，无需额外组件或训练开销即可在IML和CIML模式间无缝切换。其端到端设计避免了数据标注过程中的繁琐步骤。

Result: UGD-IML在多个数据集上的实验结果表明，其在IML和CIML任务的F1指标上分别平均优于现有最先进方法9.66和4.36。此外，所提出的方法在不确定性估计、可视化和鲁棒性方面也表现出色。

Conclusion: UGD-IML提供了一个新颖、统一且高效的图像篡改定位和标注解决方案，显著提升了在数据有限条件下的性能，并通过端到端设计简化了标注流程，为该领域带来了重要进展。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [92] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: 本文提出了一个名为MCA的鲁棒2D-3D跨模态检索框架，通过多模态联合标签校正和多级自适应对齐，有效解决了噪声标签问题，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管2D-3D跨模态检索取得了显著进展，但现实世界中不完美的（噪声）标注构成了巨大挑战。现有方法通常在每个模态内独立划分样本，容易过拟合到损坏的标签，因此需要一种在噪声标签条件下更鲁棒的2D-3D跨模态检索解决方案。

Method: 本文提出了一个鲁棒的2D-3D多级跨模态自适应校正与对齐框架（MCA）。具体包括：1. 多模态联合标签校正（MJC）机制，利用多模态历史自预测共同建模模态预测一致性，实现可靠的标签精炼。2. 多级自适应对齐（MAA）策略，有效增强跨模态特征的语义和不同层级间的判别能力。

Result: 广泛的实验证明了我们提出的MCA方法的优越性，在传统和现实噪声3D基准测试上均达到了最先进的性能。

Conclusion: MCA框架在处理2D-3D跨模态检索中的噪声标签问题上表现出通用性和有效性，显著提升了检索性能。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [93] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 本研究提出一个自监督学习框架，通过新颖的渐进式空间掩蔽注意力机制，在无需大量标注数据的情况下显著提升了手写数学表达式识别（HMER）的性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）因其固有的二维结构、变化的符号尺度和复杂的空间关系而具有挑战性，且现有方法通常需要昂贵的标注数据。

Method: 本文提出一个自监督学习（SSL）HMER框架。首先，使用全局和局部对比损失预训练图像编码器。其次，引入一个新颖的自监督注意力网络，通过渐进式空间掩蔽策略进行训练，以学习语义焦点区域。最终流程包括编码器自监督预训练、自监督注意力学习，以及使用Transformer解码器进行有监督微调以生成LaTeX序列。

Result: 在CROHME基准测试中，该方法优于现有的自监督和全监督基线方法。

Conclusion: 研究结果验证了所提出的渐进式注意力机制在增强HMER性能方面的有效性，表明自监督学习能有效应对手写数学表达式识别的复杂性。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [94] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: 提出FMCE-Net++训练框架，通过整合特征图收敛评估（FMCE）作为辅助头，联合监督骨干网络优化，有效提升深度神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因其内部表示不透明而面临可解释性挑战，现有特征图收敛评估（FMCE）方法缺乏实验验证和闭环集成。

Method: 开发FMCE-Net++训练框架，将预训练且冻结的FMCE-Net作为辅助头，生成特征图收敛分数（FMCS）。这些分数与任务标签结合，通过表示辅助损失（RAL）共同监督骨干网络优化，RAL通过可调的表示抽象因子动态平衡分类损失和特征收敛优化。

Result: 在MNIST、CIFAR-10、FashionMNIST和CIFAR-100数据集上的广泛实验表明，FMCE-Net++无需修改网络架构或额外数据即可持续提高模型性能。例如，ResNet-50/CIFAR-10准确率提升+1.16 pp，ShuffleNet v2/CIFAR-100准确率提升+1.08 pp。

Conclusion: FMCE-Net++能够有效提升现有最佳（SOTA）性能上限，验证了其方法的可行性和优越性。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [95] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive通过改进LiDAR表示和使用基于Mamba的状态空间模型（SSM）取代Transformer，解决了端到端自动驾驶中扩散模型融合的计算复杂度和空间先验不足问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在端到端自动驾驶中面临挑战，其性能受限于对基于Transformer融合的依赖。Transformer架构存在计算复杂度高（二次方）限制高分辨率特征使用，以及缺乏空间先验无法有效建模BEV表示的根本性局限。

Method: 提出GMF-Drive框架，包含两项创新：1. 将信息受限的直方图LiDAR表示替换为几何增强的柱状LiDAR格式，编码形状描述符和统计特征，保留关键3D几何细节。2. 提出新型分层门控Mamba融合（GM-Fusion）架构，用高效、空间感知的状态空间模型（SSM）替代Transformer，其核心BEV-SSM利用定向序列和自适应融合机制，以线性复杂度捕获长程依赖并尊重驾驶场景的独特空间特性。

Result: 在NAVSIM基准测试中，GMF-Drive取得了新的最先进性能，显著超越DiffusionDrive。

Conclusion: 全面消融实验验证了每个组件的有效性，表明任务特定的SSM在自动驾驶领域中，无论性能还是效率均能超越通用型Transformer。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [96] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: 本文提出一种名为SynSeg的弱监督语义分割新方法，通过多类别对比学习和特征协同结构解决了开放词汇场景中的语义错位问题，并在多个基准测试中超越了现有最先进（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割面临语义类别范围广、粒度细的挑战。现有弱监督方法常依赖于类别特定监督和不适合对比学习的特征构建方法，导致语义错位和性能不佳。

Method: 本文提出SynSeg方法。该方法采用多类别对比学习（MCCL）作为更强的训练信号，并引入特征协同结构（FSS）进行特征重建。MCCL结合类别内和类别间对齐与分离，以学习图像中不同类别间的相关性。FSS则通过先验融合和语义激活图增强重建判别性特征，有效避免视觉编码器引入的前景偏置。

Result: SynSeg显著提升了弱监督下的语义定位和判别能力。在VOC、Context、Object和City等基准数据集上，SynSeg的准确率分别比SOTA基线高出4.5%、8.9%、2.6%和2.0%。

Conclusion: SynSeg通过创新性的多类别对比学习和特征重建框架，成功克服了开放词汇弱监督语义分割中的主要挑战，显著提高了模型的性能，证明了其在解决语义定位和判别问题上的有效性。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [97] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 本研究将表示学习算法（PCA, CAE, PT）应用于卫星图像进行天气事件分类，评估了其学习的潜在空间。结果表明CAE性能最佳，PT擅长热带气旋，PCA误报率高。高分辨率数据对深度学习模型有益。


<details>
  <summary>Details</summary>
Motivation: 探索并评估不同表示学习算法（包括传统与深度学习方法）在卫星图像天气事件分类任务中的有效性，以了解其学习到的潜在空间的性能。

Method: 应用主成分分析（PCA）、卷积自编码器（CAE）和预训练残差网络（PT）等表示学习算法于卫星图像。通过多种天气事件分类任务评估所学潜在空间的性能，并进一步探讨了数据集分辨率和潜在空间尺寸的影响。

Result: CAE学习的潜在空间在所有分类任务中显示出更高的威胁得分。PCA分类命中率高但误报率也高。PT在识别热带气旋方面表现出色，但在其他任务中较差。对于深度学习算法，从更高分辨率数据学习的表示更优。较小的潜在空间尺寸对命中率影响不大，但小于128维会导致显著更高的误报率。

Conclusion: CAE能有效学习潜在空间，但其表示缺乏直接的物理归因解释。因此，开发物理信息增强的CAE版本是未来有前景的研究方向。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [98] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出了SC-Captioner，一个基于强化学习的图像描述自修正框架。通过设计基于场景图分解的奖励函数，SC-Captioner能有效提升图像描述的准确性，并显著优于现有优化策略。


<details>
  <summary>Details</summary>
Motivation: 提升图像描述模型的准确性和自修正能力是研究动机。现有模型可能难以生成完全准确的描述，且缺乏有效的自修正机制。此外，现有的评估指标和数据集可能不足以支持细粒度的描述修正。

Method: 1. **SC-Captioner框架：** 提出一个强化学习框架，赋予图像描述模型自修正能力。2. **奖励函数设计：** 将预测和参考描述通过场景图解析分解为对象、属性和关系集。通过计算集合差异识别增删元素，并根据与参考集的匹配情况计算正确修正奖励和错误增删惩罚，形成最终奖励。3. **新评估指标：** 提出一套改进自CAPTURE的图像描述质量评估指标，以解决其精度评估不完整和关系匹配效率低的问题。4. **新数据集：** 构建了RefinedCaps数据集，包含6.5K张来自COCO数据集的精细标注图像。

Result: 实验表明，将SC-Captioner应用于大型视觉语言模型可以生成各种场景下更优质的图像描述，显著优于直接偏好优化训练策略。

Conclusion: SC-Captioner通过其独特的奖励函数设计和强化学习框架，成功使图像描述模型具备自修正能力，显著提升了描述质量。同时，提出的新评估指标和数据集也为图像描述研究提供了有价值的工具。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [99] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: 本文提出VeSCA，一种通过参数化单纯复形和迭代顶点精炼，利用SAM编码器生成可迁移对抗样本的方法，旨在揭示SAM对下游模型的共享漏洞。


<details>
  <summary>Details</summary>
Motivation: SAM虽具零样本交互式分割能力，但其固有漏洞构成单点风险，可能导致下游应用失效。现有对抗攻击方法因缺乏对跨领域共性弱点的探索，导致迁移性有限，因此亟需评估SAM的可迁移漏洞。

Method: 本文提出VeSCA（Vertex-Refining Simplicial Complex Attack），仅利用SAM编码器生成可迁移对抗样本。具体通过参数化单纯复形明确表征SAM与下游模型共享的脆弱区域，并采用迭代顶点精炼在对抗性强的区域识别这些复形。引入轻量级领域再适应策略，利用少量参考数据弥合领域差异进行单纯复形初始化，最终通过随机单纯复形采样生成一致可迁移的对抗样本。

Result: VeSCA在三种下游模型类别和五个领域特定数据集上，相较现有SOTA方法性能提升12.7%。研究结果进一步凸显了SAM漏洞对下游模型的风险。

Conclusion: SAM的漏洞对下游模型构成风险，亟需开发更鲁棒的预训练模型。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [100] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出一种基于显式3D眼球结构的新型3D凝视重定向框架，通过3D Gaussian Splatting实现逼真图像生成和精确凝视控制。


<details>
  <summary>Details</summary>
Motivation: 现有凝视重定向方法多基于隐式神经辐射场（NeRF），缺乏对3D表示（特别是眼球）旋转和翻译的显式建模。

Method: 引入专用3D眼球结构，使用3D Gaussian Splatting (3DGS) 表示眼球，通过显式旋转和翻译该结构以重定向凝视。另提出自适应变形模块以复制眼周肌肉的细微运动。

Result: 在ETH-XGaze数据集上，该框架能生成多样化的新凝视图像，并在图像质量和凝视估计精度方面超越现有最先进方法。

Conclusion: 该框架通过显式建模解决了传统方法的局限性，在3D凝视重定向方面取得了优异的性能和视觉质量。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [101] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的方法，通过巧妙融合稀疏IMU和单目摄像头数据，实现鲁棒高效的实时人体动作捕捉，并在姿态估计上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 结合稀疏IMU和单目摄像头进行实时人体动作捕捉是新兴且有前景的方向，但需解决数据有效融合以及视觉信息可能因遮挡或出镜而不可用等挑战。

Method: 提出一个统一的扩散模型框架来学习人体运动先验并融合两种模态信号。具体地，将序列视觉信息作为一个整体转换为条件嵌入，而惯性测量值（IMU）则与有噪声的身体姿态逐帧拼接作为扩散模型的序列输入，以适应各自信号的特性并提高鲁棒性。

Result: 实验证明了所提出系统设计的有效性，并在姿态估计方面取得了优于现有方法的先进（state-of-the-art）性能。

Conclusion: 该扩散模型解决方案能够有效应对视觉信息不稳定等挑战，通过精心设计的模态融合策略，实现了稳定且高性能的实时人体动作捕捉，为该领域提供了一个前沿且实用的方法。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [102] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: 本文提出SDEval，一个动态安全评估框架，通过文本、图像和图文结合的动态策略，解决多模态大语言模型（MLLMs）安全评估中现有数据集过时和数据污染问题，有效揭示MLLMs的安全局限性。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估多模态大语言模型（MLLMs）安全性的数据集，存在随着模型演进而过时和容易受数据污染的问题，导致无法准确反映MLLMs的最新安全表现。

Method: SDEval框架采用三种核心动态策略：文本动态、图像动态和文本-图像动态。这些策略能够从原始基准中生成新的评估样本，以可控地调整安全基准的分布和复杂度。该方法通用性强，可应用于多种现有安全及能力基准。

Result: 实验结果显示，SDEval显著影响MLLM的安全评估，有效缓解了数据污染问题，并揭示了多模态大语言模型的安全局限性。具体发现包括文本和图像动态对模型安全的独立影响，以及将文本或图像动态相互注入对安全风险的额外影响。

Conclusion: SDEval作为一个开创性的动态安全评估框架，有效克服了传统评估数据集的局限性，提供了一种灵活且抗污染的评估方法，成功揭示了多模态大语言模型在不同复杂度和分布下的安全缺陷。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [103] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO通过引入早期融合、顺序对齐查询选择和生成式数据引擎，解决了多模态视觉模型在开放世界检测中的局限性，实现了SOTA性能并显著扩展了语义覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉模型在开放世界分割中存在晚期特征融合、次优查询选择以及词汇表限制等问题。

Method: 提出Prompt-DINO框架，包含三项创新：1) 早期融合机制，在初始编码阶段统一文本/视觉提示与骨干特征；2) 顺序对齐查询选择，优化DETR架构中文本和视觉查询的结构对齐；3) 生成式数据引擎，利用RAP模型合成5亿训练实例，将标签噪声降低80.5%。

Result: Prompt-DINO在开放世界检测基准测试中取得了最先进的性能，并显著扩展了语义覆盖范围，超越了固定词汇表的限制。

Conclusion: 该工作为开放世界场景下的可扩展多模态检测和数据生成建立了一个新范式。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [104] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 本文提出一种模块化图表生成和视觉多样化方法，构建了高效图表数据集（ECD），显著提升了多模态大语言模型（MLLMs）的图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在图表理解方面表现不佳，成功率仅为30%-50%；同时，现有合成图表数据集与真实图表相似度不足，影响模型训练和性能。

Method: 采用模块化图表生成和视觉多样化策略，设计了五步数据合成流程：分离单图数据与功能创建、多子图条件生成、视觉多样化、低质量数据过滤，并使用GPT-4o生成问答对，最终构建了高效图表数据集（ECD）。

Result: 成功构建了包含1万+图表图像和30万+问答对的高效图表数据集（ECD），涵盖25个主题和250+高视觉复杂度的图表类型组合。实验表明，ECD显著且持续提升了各种MLLMs在真实世界和合成测试集上的性能。

Conclusion: 模块化和多样化的合成图表数据生成方法能够有效提升多模态大语言模型的图表理解能力。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [105] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSConv的新型动态分裂卷积策略，结合注意力机制，用于高分辨率全色锐化任务，通过有效提取不同位置特征，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 全色锐化是低级视觉领域的重要且具挑战性的任务。现有方法多依赖标准卷积，而自适应卷积因遥感图像的像素间关联性而更有效，但鲜有研究采用。研究旨在通过更有效的特征提取来提高全色锐化性能。

Method: 提出了一种名为DSConv的动态分裂卷积策略，结合注意力机制。该方法通过选择感兴趣位置，将原始卷积核动态地分裂成多个较小的核。在此基础上，构建了一个新型的全色锐化网络架构，以更有效地执行任务。

Result: 充分的实验证明了DSConv的有效性，并达到了最先进的性能。综合严谨的讨论也证实了DSConv的优越性和最佳使用条件。

Conclusion: DSConv能够更有效地提取感受野内不同位置的特征，增强网络的泛化、优化和特征表示能力，为全色锐化提供了一种高效且高性能的解决方案。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [106] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR是一个用户中心、多维度的文本到图像(T2I)评估基准，旨在解决现有评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像(T2I)评估指标存在局限性，无法全面、准确地评估生成图像的质量，尤其是在抽象语义和用户需求方面。

Method: VISTAR采用双层混合范式：一是使用确定性、可脚本化的指标评估物理可量化属性（如文本渲染、光照）；二是通过新颖的“分层加权P/N问答(HWPQ)”方案，利用受限视觉-语言模型评估抽象语义（如风格融合、文化保真度）。基准构建基于120位专家的德尔菲研究，定义了七种用户角色和九个评估维度，包含2,845个提示词，并经过15,000多次人工配对比较验证。

Result: VISTAR的评估指标实现了高人类对齐度（>75%），其中HWPQ方案在抽象语义评估上达到85.9%的准确率，显著优于VQA基线。对最先进模型的综合评估显示，没有普遍意义上的“最佳模型”，因为基于用户角色的加权评分会重新排序模型排名，为特定领域部署提供可操作的指导。

Conclusion: VISTAR提供了一个有效且可复制的用户中心、多维度T2I评估框架，能够更准确地反映人类偏好，并为T2I模型的领域特定部署提供指导。所有资源均已公开，以促进评估的可重复性。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [107] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: 针对阿尔茨海默病（AD）早期诊断挑战，本文提出MPF-KANSC深度学习框架，通过多平面MRI特征融合与KANSC注意力机制，显著提升了AD诊断性能，并揭示了AD进展中皮层下结构右侧不对称的生物学证据。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期精确诊断对于及时干预至关重要，但由于大脑结构变化复杂且细微，诊断仍具挑战。现有深度学习方法主要依赖单一平面结构磁共振成像（sMRI），难以准确捕捉大脑病理区域间复杂非线性关系，从而限制了对萎缩特征的精确识别能力。

Method: 提出创新性框架MPF-KANSC，其核心包括：1. **多平面融合（MPF）**：结合冠状面、矢状面和轴向面等多平面sMRI的特征，以捕获更全面的脑结构信息。2. **Kolmogorov-Arnold网络引导的空间-通道注意力机制（KANSC）**：采用更灵活、精确的非线性函数逼近技术，促进疾病相关异常的精确识别和定位。模型能够并行提取来自多个解剖平面的特征。

Result: 在ADNI数据集上的实验证实，所提出的MPF-KANSC模型在AD诊断中取得了卓越的性能。此外，研究发现为AD进展过程中皮层下结构右侧不对称提供了新证据，突出了模型的良好可解释性。

Conclusion: MPF-KANSC框架通过有效结合多平面sMRI信息和先进的注意力机制，显著提升了阿尔茨海默病的诊断精度。该模型不仅在诊断上表现出色，还提供了疾病进展中的生物学洞察，展现了其在AD早期诊断和神经科学研究中的巨大潜力。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [108] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: 针对扩散模型计算成本高的问题，本文提出了PostDiff，一个无需微调的后训练加速框架，通过混合分辨率去噪和模块缓存策略提高效率。研究发现，降低每步推理成本比减少去噪步数更能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但其高计算需求限制了在资源受限平台上的部署。本文旨在探讨在后训练且不微调的设置下，是减少去噪步数还是降低每步推理成本更能有效加速扩散模型。

Method: 提出PostDiff框架，一个训练无关的后训练加速方案。该方案在输入层面采用混合分辨率去噪，在早期去噪步骤降低分辨率；在模块层面采用混合模块缓存策略，重用计算。

Result: ['PostDiff显著改善了SOTA扩散模型的保真度与效率权衡。', '在保证生成质量的前提下，降低每步推理成本比减少去噪步数在提升效率方面更有效。']

Conclusion: PostDiff框架能够有效加速预训练扩散模型，并优化其性能与效率的平衡。研究表明，在追求效率的同时保持生成质量，降低单步推理成本通常比减少去噪步数更为有效。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [109] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: 提出UW-3DGS，一个基于3D高斯散射的水下三维重建框架，通过结合可学习物理模型和不确定性剪枝，显著提升水下场景重建的质量和效率，减少伪影。


<details>
  <summary>Details</summary>
Motivation: 传统三维重建方法（如NeRF）在水下因光线吸收、散射和浊度而面临严重的几何和色彩保真度下降问题。现有基于物理模型的NeRF扩展（如SeaThru-NeRF）则因依赖MLP而限制了在浑浊环境下的效率和空间分辨率，急需更鲁棒、高效的水下重建方案。

Method: 引入UW-3DGS框架，将3D高斯散射 (3DGS) 适应于鲁棒的水下重建。核心创新包括：1) 一个即插即用的可学习水下图像形成模块，利用基于体素的回归处理空间变化的衰减和反向散射；2) 一个物理感知不确定性剪枝 (PAUP) 分支，通过不确定性评分自适应移除噪声浮动高斯点，确保无伪影几何。该流程在训练阶段端到端优化高斯点和水下参数，并由PAUP剪枝和散射建模指导；在渲染阶段，精炼后的高斯点可生成清晰的未衰减辐射图像 (URI) 和逼真的水下图像 (UWI)。

Result: 在SeaThru-NeRF和UWBundle数据集上的实验显示，UW-3DGS表现出卓越的性能。在SeaThru-NeRF数据集上，其PSNR达到27.604，SSIM为0.868，LPIPS为0.104，同时浮动伪影减少了约65%。

Conclusion: UW-3DGS通过引入物理感知建模和不确定性剪枝，有效克服了水下复杂光学环境对3D重建的挑战，提供了一种鲁棒、高效且高保真度的水下三维场景重建解决方案。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [110] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 本研究通过生成合成数据，并结合Faster R-CNN、SAM以及先进的分割模型（如FPN），实现结肠镜图像中息肉的自动化检测，旨在解决医疗数据量小和标注复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，但有限的医疗数据集和复杂的标注工作阻碍了息肉自动化检测技术的发展。本研究旨在开发一种多方向架构框架，以实现结肠镜图像中息肉的自动化检测，并解决数据量限制和标注复杂性问题。

Method: 研究构建了一个全面的系统，通过Stable Diffusion增强技术生成合成数据。检测方法结合Faster R-CNN进行初始目标定位，并使用Segment Anything Model (SAM)细化分割掩膜。此外，研究还评估了五种最先进的分割模型（U-Net, PSPNet, FPN, LinkNet, MANet），以ResNet34作为基础模型进行性能比较。

Result: Faster R-CNN检测算法实现了93.08%的召回率、88.97%的精确度和90.98%的F1分数。在评估的分割模型中，FPN表现出卓越的性能，PSNR（7.205893）和SSIM（0.492381）分数最高。UNet在召回率（84.85%）方面表现突出，而LinkNet在IoU（64.20%）和Dice分数（77.53%）上表现均衡。

Conclusion: 本研究成功地为结肠镜图像中的息肉自动化检测和分割开发了一个综合系统，有效利用合成数据解决了数据稀缺问题，并结合了先进的深度学习模型。研究结果表明Faster R-CNN在检测方面表现出色，并确认FPN是该应用中性能卓越的分割模型，为结直肠癌的早期诊断提供了有力的技术支持。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [111] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 本文提出一种利用地板特征和图卷积网络（GCN）的机器人定位框架，解决了传统方法的局限性，实现了高精度定位，并成功应对了“被劫持机器人”问题。


<details>
  <summary>Details</summary>
Motivation: 机器人导航中的精确本地化是一个基本挑战。传统的定位方法（如激光雷达或二维码系统）在复杂环境中存在固有的可扩展性和适应性限制。

Method: 提出一种创新定位框架，通过采用基于图的表示和图卷积网络（GCN）来利用地板特征。该方法使用图来表示地板特征。

Result: 该方法实现了0.64cm的定位误差，比比较单个图像特征更准确高效。此外，它在每一帧中都成功解决了“被劫持机器人”问题，且无需复杂的过滤过程。

Conclusion: 这些进步为机器人在不同环境中的导航开辟了新的可能性。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [112] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 本文提出了MA-CBP，一个基于多智能体异步协作的犯罪行为预测框架，通过处理实时视频流进行长短期上下文推理，实现潜在犯罪活动的早期预警。同时，构建了一个高质量的犯罪行为数据集，并在实验中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加速，公共场景的犯罪行为日益威胁社会安全。传统异常检测方法难以捕捉高级行为语义，而基于LLM的生成式方法无法满足实时性需求。

Method: 提出了MA-CBP框架，该框架基于多智能体异步协作，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧以进行长短期上下文的联合推理。此外，构建了一个高质量的犯罪行为数据集，提供帧级、摘要级和事件级多尺度语言监督。

Result: 实验结果表明，该方法在多个数据集上取得了优越的性能。

Conclusion: MA-CBP为城市公共安全场景中的风险预警提供了一个有前景的解决方案。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [113] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: 针对胸腔积液CT图像分割的挑战，提出DBIF-AUNet模型，通过双域特征解耦和分支交互注意力融合提升分割精度，在数据集上表现优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液CT图像语义分割在临床诊断中至关重要，但面临挑战：积液与周围组织灰度相似、边缘模糊、形态多变。现有方法因直接特征拼接导致语义鸿沟，难以有效处理图像多样性和复杂边缘。

Method: 提出双分支交互融合注意力模型（DBIF-AUNet），其核心包括：1. 构建密集嵌套跳跃连接网络。2. 创新性地完善双域特征解耦模块（DDFD），正交解耦双域功能以实现多尺度特征互补。3. 设计分支交互注意力融合模块（BIAF），动态加权融合全局、局部和频带特征以增强分割鲁棒性。4. 引入嵌套深度监督机制与分层自适应混合损失，有效解决类别不平衡问题。

Result: 在1,622张胸腔积液CT图像上验证，DBIF-AUNet的IoU和Dice分数分别达到80.1%和89.0%。相较于SOTA模型U-Net++和Swin-UNet，其IoU和Dice分数分别提升了5.7%/2.7%和2.2%/1.5%。

Conclusion: DBIF-AUNet模型在复杂胸腔积液CT图像的语义分割精度上实现了显著优化，表明其在提升临床诊断和治疗的准确性和及时性方面具有巨大潜力。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [114] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 为解决MLLM中CVIT的灾难性遗忘和参数效率低下问题，本文提出LiLoRA，一种高效的架构扩展方法，通过参数共享和低秩分解实现优越的持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）中的持续视觉指令微调（CVIT）面临灾难性遗忘问题，即学习新任务时旧任务性能下降。现有通过架构扩展缓解遗忘的方法，通常为每个任务扩展整个层，导致参数开销巨大且可扩展性差。

Method: 本文引入LiLoRA (LoRA in LoRA) 方法。它通过跨任务共享LoRA矩阵A来减少冗余，对矩阵B进行额外的低秩分解以最小化任务特定参数，并加入余弦正则化稳定性损失来保持共享表示的一致性。

Result: 在多样化的CVIT基准测试中，LiLoRA在顺序任务学习中持续取得优越性能，并与现有方法相比显著提高了参数效率。

Conclusion: LiLoRA是一种高效的架构扩展方法，能够有效解决MLLM中CVIT的灾难性遗忘和参数效率低下问题，在持续学习中表现出色。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [115] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出AnomaloyMoE，一个基于MoE的通用异常检测框架，通过分层专家网络和专家多样性/平衡模块，在多领域数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法高度专业化，针对特定异常类型（如纹理缺陷或逻辑错误）设计，导致泛化能力差，在指定上下文外性能受限。

Method: 提出AnomaloyMoE框架，将异常检测问题分解为局部结构、组件级语义和全局逻辑三层语义，并对应采用补丁、组件和全局三类专用专家网络。引入专家信息排斥（EIR）模块促进专家多样性，以及专家选择平衡（ESB）模块确保专家全面利用。

Result: 在工业图像、3D点云、医学图像、视频监控和逻辑异常检测等8个具有挑战性的数据集上，AnomaloyMoE均建立了新的最先进性能，显著优于各领域专业化方法。

Conclusion: AnomaloyMoE通过其分层和MoE设计，实现了对广泛异常类型的通用检测能力，并在多领域展现出卓越性能，超越了传统专业化方法。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 本文提出AEPO框架以解决多模态大语言模型（MLLMs）在图形用户界面（GUI）上指令识别时因探索效率低下导致的语义对齐瓶颈问题，通过多答案生成和自适应奖励机制，显著提升了模型在语义理解和泛化方面的性能，达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在GUI操作中，虽然基于可验证奖励的强化学习（RLVR）能有效改善空间对齐，但其低效的探索策略限制了语义对齐能力，使得模型难以学习复杂的语义关联，成为当前挑战。

Method: 本文提出了自适应探索策略优化（AEPO）框架。AEPO采用多答案生成策略以扩大探索范围，并通过一个基于效率原则（eta=U/C）的自适应探索奖励（AER）函数来引导探索过程。

Result: 经过AEPO训练的模型InfiGUI-G1-3B和InfiGUI-G1-7B在多个挑战性的GUI指令识别基准测试中取得了新的SOTA结果。在旨在测试泛化能力和语义理解的基准上，相对于传统的RLVR基线，实现了高达9.0%的显著相对提升。

Conclusion: AEPO通过解决低效探索导致的语义对齐瓶颈，显著增强了MLLMs在GUI指令识别中的语义理解和泛化能力，为开发更强大的GUI自主代理提供了有效途径。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [117] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 结合主动推理和大型语言模型，提出了一个内生安全的通用人工智能（AGI）开发框架。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限。

Method: 提出一个结合主动推理和大型语言模型的框架，将安全保证整合到系统核心设计中。利用自然语言表示和操作信念，通过透明信念表示和分层价值对齐实现人类监督。架构是一个多智能体系统，智能体遵循主动推理原则，偏好和安全约束通过分层马尔可夫毯流动。关键安全机制包括：信念与偏好在自然语言中明确分离、通过资源感知自由能最小化实现有限理性、以及通过模块化智能体结构实现组合安全性。

Result: 论文提出了一个新颖的理论框架，旨在从设计之初就确保AGI的内在安全性，克服了传统AI安全方法的局限性。

Conclusion: 该方法为开发本质上更安全的AGI提供了一条可行路径，并提出了基于Abstraction and Reasoning Corpus (ARC) 基准的实验议程，以期未来验证框架的安全特性。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [118] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络展现出类似人类心智结合思想、产生新颖性和快速学习的能力，这削弱了人类认知过程必须是符号化的论点，但符号系统仍对定义抽象问题很重要。文章提出了关于人类思维符号基础研究的新议程。


<details>
  <summary>Details</summary>
Motivation: 传统的观点认为人类心智结合思想、产生新颖性和快速学习的能力是其作为符号系统的有力证据。本文旨在探讨现代神经网络是否也具备这些能力，并以此重新评估人类心智的符号基础。

Method: 本文通过论证（argumentation）的方式，比较现代神经网络与人类心智在特定认知能力上的表现，并基于此提出对传统观点的挑战和新的研究议程。没有采用实验或数据分析的具体方法。

Result: 研究发现现代神经网络（及基于它们的人工智能系统）展现出与人类心智相似的结合思想、产生新颖性和快速学习的能力。这一发现削弱了人类心智的认知过程和表征必须是符号化的论点。然而，鉴于这些神经网络通常是基于符号系统生成的数据进行训练的，这表明符号系统在刻画人类心智必须解决的抽象问题方面仍扮演着重要角色。

Conclusion: 基于上述论证，文章提出了关于人类思维符号基础研究的新议程。这意味着需要重新审视符号系统在人类认知中的地位和作用，可能不再是唯一的必要条件，而是一个重要的辅助或定义问题的方式。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [119] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: 提出Holistic-XAI (H-XAI)框架，通过结合因果评级和传统XAI方法，为不同利益相关者提供交互式、多方法、个体及全局的解释，以满足多样化需求。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI (XAI) 方法主要服务于开发者，侧重于解释模型输出而非支持多元利益相关者的需求，且评估性AI仍主要关注运营组织，未能全面满足各方需求。

Method: 引入Holistic-XAI (H-XAI) 框架，它整合了因果评级方法与传统XAI方法。该框架支持交互式、多方法的解释过程，允许利益相关者提问、检验假设、与基线（随机和偏置）进行比较，并结合了实例级和全局解释，以适应不同利益相关者（理解个体决策、评估群体偏见、评估鲁棒性）的目标。

Result: 通过信用风险分类和金融时间序列预测两个案例研究（涵盖六种场景）验证了方法的通用性。H-XAI成功弥补了现有XAI方法的关键不足。

Conclusion: H-XAI通过结合因果评级和后验解释，有效地回答了利益相关者在个体决策和整体模型层面上的特定问题，从而填补了现有XAI方法的关键空白。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [120] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 本文对具身导航的安全性进行了全面综述，涵盖攻击、防御、评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和具身AI的发展，具身导航在关键应用中的部署引发了严重的安全性担忧，亟需确保系统安全。

Method: 本研究采用综合性综述分析方法，从攻击策略、防御机制和评估方法等多个视角，全面审视具身导航的安全性。内容包括对现有安全挑战、缓解技术、数据集和评估指标的全面考察，并探讨了未解决的问题及未来的研究方向。

Result: 本综述分析了具身导航中现有的安全挑战、攻击策略、防御机制和评估方法，并识别了未解决的关键问题，如潜在攻击方法、缓解策略、更可靠的评估技术和验证框架，为未来研究指明了方向。

Conclusion: 本综述旨在为具身导航系统的安全性研究提供宝贵见解，指导开发更安全可靠的具身导航系统，并对提升社会安全和工业效率具有广泛意义。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [121] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 针对AI代理在复杂多步用户查询中的工具检索挑战，提出了一种基于知识图谱（KG）的工具检索框架，通过建模工具间的语义关系和功能依赖，显著提高了检索准确性，尤其适用于需要顺序工具组合的场景。


<details>
  <summary>Details</summary>
Motivation: AI代理在处理复杂用户查询时，需要高效准确地选择和规划工具，但现有工具检索方法主要依赖用户查询与工具描述的相似性，这在处理多步用户请求时严重限制了检索准确性，且该领域研究不足。

Method: 提出了一种基于知识图谱（KG）的工具检索框架，旨在捕获工具间的语义关系和功能依赖。其检索算法利用1跳自我工具图（1-hop ego tool graphs）的集成，以建模工具间的直接和间接连接，从而实现多步任务中更全面和上下文感知的工具选择。

Result: 在合成生成的内部数据集上（涵盖六种用户类别）进行评估，结果显示，所提出的基于工具图谱的方法在微平均完整召回率（micro-average Complete Recall）指标上达到了91.85%的工具覆盖率。这显著优于实验中性能最强的非KG基线（重新排序的语义-词汇混合检索），后者仅达到89.26%。

Conclusion: 研究结果支持了假设，即知识图谱中的结构化信息为纯粹的相似性匹配提供了互补信号，尤其适用于需要顺序工具组合的查询，从而有效提高了工具检索的准确性。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [122] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 本文提出MedOrch框架，利用LLM协调多个VLM实现多模态协作，以提升医疗决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI多智能体研究主要集中于语言任务，扩展到多模态场景面临挑战。视觉语言模型（VLMs）在指令遵循和自我反思方面不如大语言模型（LLMs），导致其难以在医疗多模态决策的协作流程中有效工作。

Method: 提出了MedOrch框架，该框架采用一个基于LLM的协调者智能体，引导多个基于VLM的专家智能体进行输出交流和反思，以实现协作。研究中使用了多个开源的通用及领域特定VLM，而非昂贵的GPT系列模型。

Result: 研究表明，不同VLM智能体之间的协作能力超越了任何单个智能体。该方法在五个医疗视觉问答基准测试中表现出卓越的协作性能，且无需进行模型训练。

Conclusion: 介导者引导的多智能体协作对于推进医疗多模态智能具有显著价值。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [123] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 本文提出HIMA，一个分层多智能体框架，通过专家模仿学习的专业化智能体和策略规划器（SP）的元控制器，解决了大型语言模型在《星际争霸II》等动态、长时序策略游戏中面临的挑战，并在战略清晰度、适应性和计算效率方面超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在处理《星际争霸II》这类动态、长时序的实时策略游戏时表现不佳，难以应对资源限制、战场演变和部分可观察环境带来的复杂性。

Method: 提出HIMA（分层模仿多智能体）框架。该框架采用分层多智能体结构，其中专业的模仿学习智能体从专家演示中学习特定策略（如空中支援、防御机动），并生成连贯的多步动作序列。一个名为“策略规划器”（SP）的元控制器协调这些提议，形成一个适应环境的整体计划，确保局部决策与长期策略一致。同时，构建了全面的SC2测试平台TEXTSCII-ALL。

Result: 实验结果表明，HIMA在战略清晰度、适应性和计算效率方面均优于现有先进方法。

Conclusion: 结合专业化模仿模块与元级编排的策略，在开发更稳健、通用的人工智能智能体方面具有巨大潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [124] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 本文提出了一个基于参与式预算（PB）的框架，用以评估大型语言模型（LLMs）在结构化资源分配任务中的推理能力。研究探索了不同的提示策略以及LLM从非结构化输入中推断偏好的能力，结果表明提示设计的重要性及LLM在机制设计方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）被寄予处理复杂决策任务的厚望，但它们在结构化资源分配方面的能力尚不明确。此外，现有基准存在数据污染和静态性问题，使得评估LLMs的推理能力变得困难。

Method: 研究提出了一个双重目的的框架，利用参与式预算（PB）作为LLM资源分配的实际场景和评估其推理能力的自适应基准。通过三种提示策略（贪婪选择、直接优化、爬山启发式优化）让LLMs在预算约束下选择项目子集，并将其分配结果与效用最大化预言机进行比较。此外，还测试了LLMs从自然语言投票输入或元数据中推断结构化偏好的能力，并将其与基于真实投票的分配进行对比。

Result: 研究结果强调了提示设计（prompt design）在LLM性能中的重要作用。此外，结果表明LLMs在利用非结构化输入进行机制设计方面具有 promising 的潜力。

Conclusion: 提示设计对LLMs在资源分配任务中的表现至关重要。大型语言模型在处理非结构化输入并应用于机制设计方面展现出巨大前景，这为它们在未来复杂决策场景中的应用提供了新的方向和评估工具。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [125] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 呼吁AI领域关注认知想象力，视其为潜在突破，并提出语义模型作为实现手段。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在AI中被严重低估，导致当前AI能力受限。人类推理依赖想象语境获取背景信息并进行语义验证，缺乏想象力的推理是“盲目”的。

Method: 提出“语义模型”作为模拟认知想象力的工具。这是一种新型数学模型，可学习（类神经网络），基于概率因果关系，并采用“玻璃盒”方法，允许操纵由因果关系连接的整体连贯事实系统。

Result: 语义模型能够通过确保想象语境的一致性，并允许将语境作为由因果关系联系的整体连贯系统进行操作，从而模拟认知想象力。

Conclusion: 认知想象力是人类思维的关键，但在AI中其作用被严重低估，限制了AI能力。该文呼吁将其视为AI的下一个重大突破点，并提出语义模型作为实现这一突破的有效工具。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [126] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: 寻找最优决策树是NP-难问题，现有精确算法缺乏anytime性能。本文提出CA-DL8.5，一种通用、完备、anytime的束搜索算法，扩展并统一了DL8.5框架下的现有策略。实验证明，CA-DL8.5结合LDS能提供最佳anytime性能，且保持最优性保证。


<details>
  <summary>Details</summary>
Motivation: 寻找最小分类误差的最优决策树是NP-难问题。尽管精确算法能保证最优性，但它们通常因搜索空间探索不平衡而导致anytime行为差（即无法在搜索提前终止时快速找到高质量决策树）。此外，现有anytime扩展方法缺乏系统比较，难以评估其有效性。

Method: 提出CA-DL8.5，一个通用、完备且anytime的束搜索算法，它扩展了DL8.5框架并统一了现有anytime策略。CA-DL8.5通过模块化设计泛化了LDS-DL8.5和Top-k-DL8.5，允许集成各种启发式和松弛机制。该算法重用DL8.5高效的分支定界剪枝和Trie缓存，并结合基于重启的束搜索，逐步放宽剪枝标准以随时间提高解的质量。通过对比多种CA-DL8.5实例化（基于Purity, Gain, Discrepancy和Top-k启发式）与Blossom算法，使用“原差距积分”作为anytime评估指标。

Result: 在标准分类基准上的实验结果表明，CA-DL8.5结合LDS（有限差异）始终提供最佳的anytime性能。它在保持完备性和最优性保证的同时，显著优于其他CA-DL8.8变体和Blossom算法。

Conclusion: CA-DL8.5提供了一个通用框架，有效地解决了最优决策树学习中的anytime性能问题。特别是，结合LDS启发式的CA-DL8.5，在保持决策树最优性和完备性的前提下，展现出卓越的快速获取高质量解的能力。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [127] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 本文提出结合BEV感知和Mamba框架的深度强化学习模型ME^3-BEV，用于解决自动驾驶中实时决策和复杂环境感知问题，并在CARLA模拟器中表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂环境感知和实时决策方面面临挑战。传统模块化方法存在误差传播，而现有端到端学习系统面临计算瓶颈。

Method: 提出一种结合BEV感知和深度强化学习的新型自动驾驶方法。引入Mamba-BEV模型，利用Mamba框架高效提取时空特征并建模长程依赖。在此基础上，构建ME^3-BEV框架，将Mamba-BEV作为端到端DRL的特征输入。通过语义分割可视化高维特征以增强模型可解释性。

Result: 在CARLA模拟器上的大量实验表明，ME^3-BEV在动态城市驾驶场景中，在碰撞率和轨迹精度等多项关键指标上均优于现有模型。

Conclusion: ME^3-BEV为实时自动驾驶提供了一个有前景的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [128] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了GNN（聚合-组合-读取型）与C2逻辑表达能力对比的开放问题，证明GNN的表达能力严格超越C2。


<details>
  <summary>Details</summary>
Motivation: Barceló et al. (2020) 提出了一个开放问题：完整的C2逻辑是否能刻画聚合-组合-读取（aggregate-combine-readout）GNNs的逻辑表达能力。尽管有多次尝试，此问题仍未解决。

Method: 通过数学证明来解决上述开放问题，具体是证明聚合-组合-读取型GNN的逻辑表达能力严格超过C2。

Result: 研究证明，聚合-组合-读取型GNN的逻辑表达能力严格超过C2逻辑。此结果适用于无向图和有向图。

Conclusion: 本研究解决了GNN逻辑表达能力的一个长期开放问题，并为无穷逻辑的表达能力提供了纯粹的逻辑见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [129] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR是一个基于LLM智能体科学家，利用结构化科学方法进行表格推理的框架，在零样本设置下表现优异，无需训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前的表格推理方法依赖标注数据或复杂数据增强，限制了灵活性和泛化性；同时，大型语言模型（LLMs）在此类任务上常逊于监督模型。

Method: 引入PanelTR框架，利用LLM智能体科学家通过结构化科学方法（包括独立调查、自我审查和协作式同行评审）进行表格推理。该过程由五种科学家角色驱动，实现了语义层面的迁移，无需数据增强或参数优化。

Result: 在四个基准测试上的实验表明，PanelTR的性能优于普通LLMs，并能与完全监督模型相媲美，且完全不依赖训练数据。

Conclusion: 结构化的科学方法能有效处理复杂任务，并在零样本环境中实现灵活的语义理解，其应用范围可超越表格推理。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [130] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE是一个新型LLM评估框架，通过让大型语言模型（LLMs）互相生成并解决可验证任务来进行竞争式评估，实现自动化、可扩展和客观的能力评测，无需人工或领域知识。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型的评估方法需要大量的领域专业知识，难以随着模型快速发展而扩展，因此急需一种更高效、可扩展的评估方案。

Method: 引入SKATE框架，将模型评估视为一场游戏，LLMs既是任务出题者也是解决者，它们被激励去创建能突出自身优势并暴露对手弱点的问题。该框架完全自动化、无需数据、可扩展且客观（通过可验证任务而非LLM判断进行评分）。作为概念验证，采用LLM生成的代码输出预测（COP）挑战，并使用基于TrueSkill的排名系统评估了六个前沿LLMs。

Result: 研究发现：1) 较弱的模型也能可靠地识别和评分更强的模型；2) 基于LLM的系统会表现出自偏好行为，生成与其自身能力相符的问题；3) SKATE能自动揭示模型之间细微的能力差异。

Conclusion: 这些发现是迈向通用、可扩展的评估框架的重要一步，有助于跟上大型语言模型的发展步伐。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [131] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 本研究利用机器学习和可解释AI预测VRP解质量并分析特征重要性，以期为元启发式算法设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 车辆路径问题 (VRP) 是一种复杂的NP-Hard优化问题，传统上主要依赖人工设计的元启发式算法解决。然而，现有研究表明机器学习可用于理解组合优化解的结构，从而辅助设计更高效的算法。本研究旨在扩展此方向，通过预测VRP解的质量来辅助元启发式算法设计。

Method: 采用多种分类器模型对VRP解的质量进行预测，并进行敏感性分析。利用可解释AI (XAI) 技术深入理解模型决策过程。提出一个统一框架来评估并排名特征在不同场景下的重要性。

Result: 研究发现，尽管特征重要性存在差异，但某些特征始终表现出强大的预测能力。此外，提出了一个统一框架，能够对不同场景下的特征影响进行排名。

Conclusion: 特征重要性分析有望成为开发VRP元启发式算法指导机制的基础。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [132] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 本研究通过整合检索增强生成（RAG）管道，显著提高了大型语言模型（LLMs）在提供药物禁忌症信息方面的准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗保健领域的应用面临挑战，尤其是在需要准确可靠药物禁忌症信息的药学领域。现有LLMs在处理此类信息时准确性不足。

Method: 研究采用检索增强生成（RAG）管道，以OpenAI的GPT-4o-mini为基础模型，text-embedding-3-small为嵌入模型。通过Langchain协调混合检索系统并进行重排序。系统利用来自公共数据库的药物利用审查（DUR）数据，重点关注针对特定年龄组、妊娠和合并用药的禁忌症。

Result: 在整合RAG管道后，模型在处理年龄组、妊娠和合并用药相关禁忌症时的准确率显著提高，分别达到0.94、0.87和0.89，远超基线模型的0.49至0.57。

Conclusion: 将LLMs与RAG框架结合可以大幅提升药物禁忌信息的精确性和可靠性，从而有效降低处方和用药决策中的不确定性。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [133] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 本文指出大语言模型（LLM）作为自动评判员存在过度自信现象，提出TH-Score量化此现象，并引入LLM-as-a-Fuser集成框架以提高置信度校准和评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM法官主要关注准确性，但忽视了校准置信度的重要性，导致其在实际部署中表现出过度自信，从而损害了评估的可靠性，无法实现自适应和风险感知的评估流程。

Method: 首先，系统性地识别并分析了当前LLM法官中的“过度自信现象”。其次，引入了新的度量标准**TH-Score**来量化置信度与准确性之间的一致性。最后，提出了**LLM-as-a-Fuser**集成框架，旨在将LLM转化为可靠、风险感知的评估器。

Result: 研究揭示了LLM法官中预测置信度显著高于实际正确性的“过度自信现象”。通过提出的TH-Score和LLM-as-a-Fuser框架，实验证明该方法显著改善了置信度校准，实现了自适应、置信度驱动的评估流程，并相较于现有基线取得了更优异的可靠性和准确性。

Conclusion: 通过引入置信度驱动的评估范式和LLM-as-a-Fuser框架，可以有效解决LLM法官的过度自信问题，从而构建更值得信赖、风险感知且自适应的LLM-as-a-Judge系统。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [134] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在几何问题解决中面临长步推理和辅助线构造的挑战。本文提出GeoLaux基准，包含2186个需多步推理和辅助线的几何问题，并设计了五维评估策略。实验发现MLLMs在长步推理中性能显著下降，证明题倾向走捷径，且缺乏辅助线意识。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决，尤其是辅助线构造和长步推理，对多模态大语言模型（MLLMs）构成巨大挑战。现有评估基准未能充分考虑辅助线构造和细粒度过程评估，无法有效评估MLLMs的长步推理能力。

Method: 构建了GeoLaux基准，包含2186个几何问题（计算题和证明题），平均6.51推理步，最长24步，41.8%需辅助线构造。在此基础上，设计了新颖的五维评估策略，评估答案正确性、过程正确性、过程质量、辅助线影响和错误原因。

Result: ['模型在长步推理中表现出显著的性能下降（9个模型性能下降超过50%）。', '与计算题相比，MLLMs在解决证明题时倾向于采取捷径。', '模型缺乏辅助线意识，且提升此能力对整体几何推理能力提升特别有益。']

Conclusion: GeoLaux基准既可用于评估MLLMs在涉及辅助线的长步几何推理能力，也为提升其几何能力提供了指导方向。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [135] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 该研究提出一种贝叶斯归纳逻辑编程方法，通过最小消息长度从噪声数据中学习程序，并在多领域实验中表现出显著优越性、数据高效性及对示例平衡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一概率学习和逻辑学习是人工智能领域的一个关键挑战。

Method: 引入一种贝叶斯归纳逻辑编程（Bayesian Inductive Logic Programming）方法，该方法通过最小消息长度（Minimum Message Length, MML）从噪声数据中学习程序。它通过显式偏好更通用程序的先验和偏好更准确程序的似然，平衡假设复杂度和数据拟合度。

Result: 在游戏对弈和药物设计等多个领域的实验表明，该方法显著优于以往方法（尤其是学习最小描述长度程序的方法）。此外，该方法数据高效，对示例平衡不敏感，并能仅从正例中学习。

Conclusion: 该贝叶斯归纳逻辑编程方法成功解决了从噪声数据中学习的挑战，为统一概率与逻辑学习提供了一种高效且鲁棒的途径。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [136] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本研究提出一种在归纳逻辑编程中打破假设空间对称性的方法，通过在回答集编程中实现，显著缩短了假设搜索时间。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程面临巨大的假设空间搜索挑战，且存在大量逻辑等效的假设（对称性）加剧了这一问题。

Method: 引入了一种打破假设空间对称性的方法，并在回答集编程（ASP）中实现了这一思想。

Result: 在视觉推理和游戏等多个领域的实验表明，该方法能将求解时间从一个多小时缩短到17秒。

Conclusion: 所提出的对称性打破方法有效解决了归纳逻辑编程中的搜索效率问题，显著加速了假设搜索过程。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [137] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: 该报告介绍PRISM Eval BET工具，通过自动化红队测试，对多数SOTA大型语言模型（LLM）实现100%攻击成功率，并提出细粒度鲁棒性指标和分布式评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估并量化当前LLM的鲁棒性，特别是其抵御诱导生成有害内容的能力，并揭示其普遍存在的脆弱性。

Method: 开发PRISM Eval行为诱导工具（BET），通过动态对抗优化进行自动化红队测试；提出一种细粒度的鲁棒性度量，估算诱发有害行为所需的平均尝试次数；引入原语级漏洞分析以识别最有效的越狱技术；与第三方合作进行评估以展示分布式评估路径。

Result: PRISM Eval BET工具对41个最先进LLM中的37个实现了100%的攻击成功率；尽管普遍存在漏洞，但诱发有害行为的攻击难度在模型之间差异超过300倍；识别出针对特定危害类别最有效的越狱技术；展示了社区内分布式鲁棒性评估的实际途径。

Conclusion: 当前最先进的LLM普遍容易被诱导产生有害行为，但不同模型的防御难度差异巨大。本研究提出的工具和评估方法能有效识别LLM的漏洞，并为社区协作进行分布式鲁棒性评估提供了实用路径。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [138] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 本文提出一种新的理论框架，旨在拓展Conant和Ashby的“良好调节器必须是系统模型”定理。研究认为，任何执行调节任务的智能体，都可以被观察者解读为持有并更新关于其环境的“信念”，从而形成一种更普适的“模型”概念，并能解释现有反例。


<details>
  <summary>Details</summary>
Motivation: Conant和Ashby的经典定理指出良好的调节器必须是其系统的模型，但人工生命领域存在许多看似没有明确模型却能执行任务的系统，这表明原定理的普适性受限。研究动机在于寻找一种更广义的方式来理解调节系统中的“模型”概念。

Method: 本文提出一种新的视角和定理：当一个智能体能够执行调节任务时，观察者可以将其解释为拥有并根据感官输入“更新”其环境的“信念”。这种“信念更新”被视为一种比Conant和Ashby更复杂且适用范围更广的“模型”概念。该理论强调观察者在模型建构中的关键作用，即模型并非系统固有的属性，而是由外部施加的解释。

Result: 研究得出了一个比Conant和Ashby定理更普适的新定理。该定理表明，无论系统是调节外部环境还是自身内部状态，只要它在执行调节任务，观察者就可以将其解释为拥有关于环境的模型。这一框架通过允许存在“琐碎（trivial）”的模型，解决了此前看似与Conant和Ashby定理相悖的案例。

Conclusion: 研究结论是，尽管Conant和Ashby的直觉是正确的，但要使其理论更具普适性，需要重新定义“模型”的概念，并认识到观察者在解释系统行为中扮演的核心角色。基于“信念更新”的新框架提供了一个更广泛适用的定理，能够统一解释各类调节系统中的模型存在，即使这些模型在某些情况下可能显得微不足道。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [139] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出一个基于Transformer的机器学习模型AntiCheatPT_256，利用游戏数据检测《反恐精英2》中的作弊行为，并发布了一个包含795场比赛的CS2CD数据集，模型在未增强测试集上取得了89.17%的准确率和93.36%的AUC。


<details>
  <summary>Details</summary>
Motivation: 在线视频游戏中的作弊行为损害了游戏体验的完整性，而现有反作弊系统（如VAC）在不侵入用户系统的前提下，难以跟上不断演变的作弊方法。

Method: 开发了名为AntiCheatPT_256的基于Transformer的机器学习模型，用于检测《反恐精英2》中的作弊行为。为此，构建并公开了一个包含795场比赛的标注数据集CS2CD。从数据集中创建了90,707个上下文窗口，并对其进行了数据增强以解决类别不平衡问题。模型在这些数据窗口上进行训练。

Result: 在未增强的测试集上，该Transformer模型达到了89.17%的准确率和93.36%的AUC。

Conclusion: 该方法强调可复现性和实际应用性，为未来数据驱动的作弊检测研究提供了一个可靠的基线。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [140] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 本文提出“解释性AI”作为现有XAI的补充范式，利用生成式AI提供以人类理解为中心、情境敏感的解释，而非仅关注算法透明度。实证研究显示用户普遍偏好这种新方法。


<details>
  <summary>Details</summary>
Motivation: 当前的解释性AI（XAI）方法过于侧重算法透明度，其解释形式抽象且缺乏适应性，导致难以支持最终用户获得有意义的理解。

Method: 引入“解释性AI”作为一种新范式，利用生成式AI的能力充当人类理解的解释伙伴；开发了其定义和一个系统的八维概念模型，强调叙事交流、自适应个性化和渐进式披露原则；通过与医疗专业人员进行的快速情境设计（Rapid Contextual Design）方法进行实证验证。

Result: 用户持续偏爱情境敏感、多模态的解释，而非仅仅是技术透明度。

Conclusion: 研究结果揭示了AI系统应为人类理解而非算法内省而设计的实际紧迫性，并为在不同领域和文化背景下推进以用户为中心的AI解释方法建立了全面的研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [141] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 针对法律领域知识图谱（KG）的稀缺性，本文构建了一个关于侵害妇女案件的法律KG，并提出两种自动化构建方法（包括基于LLM的方法），旨在提高法律信息的可访问性并支持预测性司法。


<details>
  <summary>Details</summary>
Motivation: 法律决策过程需要大量且最新的法律背景知识和案例信息。法律知识图谱是获取、查询和利用法律信息、支持高级推理和机器学习应用的宝贵工具。然而，法律领域的知识图谱非常稀少，无法满足当前需求。

Method: 开发了两种互补的自动化法律知识图谱构建方法：一种是为法律领域定制的系统性自下而上方法，另一种是利用大型语言模型（LLM）的新解决方案。数据源为欧洲法院公开的法律判决书，构建过程整合了结构化数据提取、本体开发和语义丰富化。通过能力问题对构建的知识图谱进行了验证和比较。

Result: 成功构建了一个针对侵害妇女案件的法律知识图谱。论文分析并比较了两种自动化构建方法的有效性，并通过能力问题验证了所开发知识图谱的质量。

Conclusion: 所构建的法律知识图谱能够显著提高法律信息对人类和机器的可访问性，支持复杂的查询，并有望成为预测性司法机器学习工具的重要知识组成部分。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [142] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 现有公平机器学习方法存在局限性，本文提出一种名为“Fair Game”的动态机制，利用强化学习实现机器学习算法预测的动态公平性调整。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习的偏见定义多为观测性，往往相互冲突，且难以在动态社会环境中实时部署，导致与实际需求存在差距。在部署前或只有在回顾性分析中才能应用，且需要已知真实标签。当前缺乏一种能够随时间适应社会互动的公平性框架。

Method: 提出“Fair Game”动态机制，通过将一个审计器（Auditor）和一个去偏算法（Debiasing algorithm）置于机器学习算法的循环中实现公平性保障。该机制利用强化学习（RL）将这两个组件整合进循环，使公平性目标能够随时间通过修改审计器及其量化的不同偏见而调整。

Result: “Fair Game”提供了一个独特的框架，使公平性目标能够随时间演进。它通过模拟社会伦理和法律框架的演变，允许开发灵活且随时间适应的公平机器学习系统，适用于部署前和部署后。

Conclusion: “Fair Game”为构建公平机器学习系统提供了一个灵活且随时间适应的框架，解决了传统公平机器学习方法在动态社会环境中的局限性，能模拟社会伦理框架的演变，实现算法预测的持续公平性。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [143] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 该研究提出了一个数据驱动框架，用于评估多赢者投票规则在不同偏好分布下违反公理的频率。结果显示，神经网络作为投票规则能比传统规则更好地减少公理违反，表明数据驱动方法在社会选择中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的最坏情况分析对投票规则的公理满足度是二元的，无法衡量实践中在不同偏好分布下规则违反公理的频率，因此需要一个数据驱动的实用评估方法。

Method: 提出了一个数据驱动框架，用于评估投票规则在不同偏好分布下违反公理的频率。利用此框架分析了传统多赢者投票规则的公理表现，并探索了将神经网络用作投票规则并评估其性能。

Result: 神经网络作为投票规则，在最小化公理违反方面，表现优于传统投票规则。

Conclusion: 数据驱动的社会选择方法能够为设计新的投票系统提供信息，并支持在社会选择领域继续开展数据驱动研究。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: 本文提出D2D方法，旨在将因果循环图（CLD）转换为系统动力学模型，以克服CLD在动态分析上的局限性及其定量分析方法的不足，从而在缺乏经验数据的情况下支持干预策略探索，并提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 因果循环图（CLD）虽广泛用于健康和环境研究，但作为定性、静态表示，其支持动态分析和干预策略的能力有限。此外，现有定量CLD分析方法（如网络中心性分析）常导致错误推断。

Method: 提出Diagrams-to-Dynamics（D2D）方法，在缺乏经验数据时将CLD转换为探索性系统动力学模型（SDM）。该方法通过少量用户输入（将变量标记为存量、流量/辅助变量或常量），利用CLD中已编码的结构信息（链接存在性与极性），来模拟假设干预并探索不确定性下的潜在杠杆点。

Result: D2D方法有助于区分高低排名杠杆点。与基于相同CLD和变量标记构建的数据驱动SDM相比，D2D显示出比网络中心性分析更高的一致性，同时提供不确定性估计和未来数据收集的指导。

Conclusion: D2D方法已实现为开源Python包和网络应用，旨在支持进一步测试并降低CLD研究人员进行动态建模的门槛。预期进一步的验证将确立该方法在广泛案例和领域中的实用性。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [145] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 该研究引入一个新颖的框架，将物理定律表示为加权知识图谱，并利用图注意力网络（GAT）进行链接预测，以发现物理概念和方程间的深层联系。该模型表现优异，能自主揭示物理结构、识别核心方程，并生成新的跨领域关系假设。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种系统化的方法来表示和分析物理定律，揭示物理概念和方程之间的内在结构、相互关系，并通过计算方法生成新的跨领域物理假设。

Method: 构建了一个包含400个高级物理方程的数据库，并将其表示为一个加权知识图谱，其中物理概念和方程均为节点。节点间的权重通过变量重叠、物理重要性分数和文献计量数据客观定义。随后，训练了一个图注意力网络（GAT）进行链接预测。

Result: GAT模型在链接预测上取得了0.9742的测试AUC，显著优于经典启发式方法（最佳AUC：0.9487）和GraphSAGE（AUC：0.9504），性能提升2.7%。模型分析揭示了三项关键发现：(i) 能自主重现已知的物理宏观结构，识别出电磁学和统计力学之间的强概念轴；(ii) 识别出作为多个物理领域关键桥梁的中心枢纽方程；(iii) 生成稳定的、计算推导的跨领域关系假设，包括已知原理和新颖的数学类比。

Conclusion: 该框架有效识别并揭示了物理学的内在结构，包括关键概念联系和核心方程，并能通过计算方法生成大量有价值的跨领域关系假设，为物理学研究提供了新的分析工具和探索途径。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [146] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 本文提出一种“神经网络推拉”的创新数据驱动方法，旨在解决非线性状态空间模型中推拉（nudging）项难以设计的问题，并在多个混沌系统上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 对于线性状态空间模型，推拉（nudging）项的推导相对容易，但在非线性状态空间模型中，设计有效的推拉项则极具挑战性。

Method: 本文提出“神经网络推拉”（neural network nudging）方法，一种数据驱动的策略，用于学习非线性状态空间模型中的推拉项。该方法基于Kazantzis-Kravaris-Luenberger观测器理论，建立了理论存在性结果。

Result: 所提出的方法在三个展现混沌行为的基准问题上进行了评估：Lorenz 96模型、Kuramoto-Sivashinsky方程和Kolmogorov流。

Conclusion: 该研究提供了一种通过神经网络学习非线性系统中推拉项的有效途径，为解决非线性数据同化问题提供了新的数据驱动解决方案。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [147] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 本文提出一个可扩展框架，通过系统集成异构数据、处理数据质量不确定性并嵌入物理约束，高精度重建配电网拓扑。


<details>
  <summary>Details</summary>
Motivation: 现代电网运行迫切需要精确的配电网拓扑，然而，实际数据来源于多源且质量参差不齐，给可靠的拓扑重建带来挑战。

Method: 本研究提出一个可扩展框架，用于重建可信的电网拓扑。方法包括：1) 联合利用物理基础设施的空间布局和系统在信号域的动态行为两个互补维度；2) 引入置信度感知推理机制，在不牺牲可观测性的前提下处理数据质量不均，并量化推断连接的可靠性；3) 将变压器容量限制和辐射状拓扑要求等运行约束直接嵌入学习过程，确保物理可行性。

Result: 该框架在Oncor公司服务区域内，使用来自3个馈线、超过8000个电表的数据进行了验证。结果显示，拓扑重建精度超过95%，并且相较于基线方法，在置信度校准和计算效率方面有显著提升。

Conclusion: 所提出的框架能够在实际部署条件下，快速收敛并重建可信、可操作的配电网拓扑，同时确保推理过程兼具不确定性感知和结构有效性。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [148] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，通过贝叶斯风险最小化分析线性编解码器架构，推导出用于科学机器学习的秩约束线性最优映射，能处理秩亏缺问题，并为神经网络模型提供可解释性基线。


<details>
  <summary>Details</summary>
Motivation: 现有非线性神经网络在科学领域虽成功但缺乏可解释性，限制了其在需要透明度的场景中的应用。本研究旨在利用线性神经网络作为基础，提供对复杂物理过程到观测信号映射的深入理解，以解决可解释性挑战。

Method: 论文开发了一个统一的理论框架，通过贝叶斯风险最小化分析线性编解码器架构。具体方法是推导出用于正向建模和逆向恢复任务的封闭形式、秩约束的线性及仿射线性最优映射。该框架能够适应数据、正向算子和测量过程中的秩亏缺。

Result: 成功推导出了封闭形式、秩约束的线性及仿射线性最优映射，泛化了现有公式并能适应数据、正向算子和测量过程中的秩亏缺。通过在生物医学成像、金融分析和流体动力学模拟等领域的数值实验，验证了理论结果的有效性。

Conclusion: 本研究为理解和基准测试科学机器学习问题中的学习神经网络模型提供了一个稳健的基线，尤其强调了其在可解释性要求高的应用场景中的价值。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [149] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出一个结合LLM、TAPE和Graphormer的新框架，用于在文本属性图（TAGs）中进行节点分类，在ogbn-arxiv数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在文本属性图（TAGs）中，由于难以有效整合文本语义和图结构信息，节点分类仍然极具挑战性。现有方法在捕获细微领域术语、建模长距离依赖、适应时间演化及大规模扩展方面存在不足。

Method: 提出了一个整合TAPE（Text-Attributed Graph Representation Enhancement）与Graphormer的框架。该方法利用大型语言模型（LLM，特别是ChatGPT）在TAPE框架内从论文内容生成语义丰富的解释，并将其融合到增强的节点表示中。这些嵌入与结构特征通过一个带有学习注意力权重的新颖集成层相结合。Graphormer的路径感知位置编码和多头注意力机制被用于捕获长距离依赖。

Result: 在ogbn-arxiv数据集上实现了0.772的分类准确率，显著超越最佳GCN基线（0.713），达到最先进的性能。在精确率（0.671）、召回率（0.577）和F1分数（0.610）上也取得了良好结果。消融研究证实了语义和结构信息协同作用及各组件的贡献。

Conclusion: 该框架为动态文本属性图中的节点分类提供了一个可扩展且稳健的解决方案，为知识系统和科学发现领域的未来研究提供了有前景的方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [150] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 该研究提出了一个基于MDP和RL-PG的框架，用于优化航天器碰撞规避机动的决策，旨在通过早期机动来最小化燃油消耗，同时维持碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 在航天器碰撞规避机动中，如何在维持可接受碰撞风险的前提下，最小化平均燃油消耗是一个关键挑战。

Method: 本研究构建了一个连续状态、离散动作和有限时间范围的马尔可夫决策过程（MDP）模型来描述碰撞规避机动，并利用强化学习策略梯度（RL-PG）算法，结合历史数据，训练自主引导策略以决定何时启动机动。MDP模型中整合了碰撞风险、推进剂消耗和过渡轨道几何的分析模型。

Result: 在合成碰撞事件中，训练策略相比传统策略显著降低了总燃油消耗和每次机动的平均燃油消耗。在历史碰撞事件中，虽然总燃油消耗有所增加，但每次机动的平均燃油消耗有所减少。在两种情况下，训练策略都能保持或提高总体的碰撞风险保证。

Conclusion: 该框架通过有效权衡机动延迟和推进剂消耗，实现了碰撞规避决策的优化，能够在保证安全的前提下，提升航天器碰撞规避任务的燃油效率。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [151] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 本文提出SZT，一种2比特量化方法，能在不牺牲前向路径性能的情况下提供梯度信息，并可能在固定资源预算下提高信息密度，挑战量化通常被视为次优近似的传统观点。


<details>
  <summary>Details</summary>
Motivation: 传统上量化被视为性能与计算资源的次优权衡。研究动机在于探索在固定整体资源预算下，量化可能带来不同视角和潜在优势，而非简单地作为一种次优近似手段。

Method: 引入并分析了符号零三元量化（Signed-Zero Ternary, SZT），这是一种2比特量化方法。该方法能够确定性地提供梯度信息，且不产生前向路径的性能开销。

Result: 分析结果提供了证据，表明SZT量化方法与非量化替代方案相比，可能提高信息密度。

Conclusion: 在固定资源预算下，量化（特别是SZT方法）可能并非次优近似，而是一种能有效提升信息密度的有益手段。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [152] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 一种基于机器学习的时序分解方法，将随机时间序列分解为均值、离散度和噪声，可用于平滑和去噪。


<details>
  <summary>Details</summary>
Motivation: 旨在将随机时间序列分解为表示均值和离散度的双信号，并分离噪声，从而实现时间序列的平滑和去噪。

Method: 通过应用机器学习拟合双信号来完成分解。该机器学习过程最小化一个损失函数，该函数平衡了对原始时间序列的拟合度与对双信号不规则性（基于一阶和二阶导数）的惩罚。利用统计过程控制（SPC）方法对损失函数中的正则化分量进行加权以保留特殊模式。学习过程考虑了序贯式（先学习均值再学习离散度）和联合式（同时拟合双信号）两种方法。学习通过求解直接非线性无约束优化问题或应用具有序贯或孪生输出架构的神经网络实现。损失函数超参数的调优侧重于使分离出的噪声成为没有自相关特性的平稳随机过程。

Result: 成功将随机时间序列分解为均值、离散度和隔离的噪声。该分解方法可作为均值和离散度的平滑算法，也可作为去噪算法。联合学习能够揭示异方差时间序列的复杂关系。分解后的双信号可以在2D空间中表示。

Conclusion: 分解后的双信号可用于学习固有的结构、预测均值和离散度，或在多时间序列情况下分析交叉效应。根据应用需求，学习的超参数可以调整以获得离散状态或平滑序列。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [153] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 研究发现神经偏微分方程求解器精度受限于病态条件而非表达能力。提出“移位高斯编码”方法，有效改善条件数，大幅提升求解精度和范围。


<details>
  <summary>Details</summary>
Motivation: 神经偏微分方程求解器（特别是物理信息极限学习机PIELMs）的准确性常因优化困难和病态条件而下降，尤其在多保真度和刚性问题中。研究动机是解决由控制方程渐近分量引起的激活矩阵严重病态问题，该问题严重限制了收敛性。

Method: 引入“移位高斯编码”（Shifted Gaussian Encoding）作为激活过滤步骤。该方法简单而有效，能提高矩阵的秩和表达能力，同时保持凸性。

Result: 该方法将稳态对流-扩散方程中可解佩克莱数的范围扩展了两个数量级以上；在多频率函数学习上，误差降低了高达六个数量级；在拟合高保真图像向量方面，比拥有百万参数的深度网络更准确、更快。

Conclusion: 本研究强调，在科学神经求解器中，条件数而非网络深度往往是性能瓶颈，简单的架构改变可以带来显著的性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [154] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO方法以解决GRPO在训练大型推理模型时因奖励信号噪声导致的“思考-回答不匹配”问题，S-GRPO在数学推理基准上表现出显著的性能提升和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Group-Relative Policy Optimization (GRPO) 在训练大型推理模型时存在“思考-回答不匹配”的缺陷，即噪声奖励信号会破坏学习过程，尤其在不平衡响应组中问题更严重。

Method: 提出Stable Group-Relative Policy Optimization (S-GRPO)，通过推导最优的、噪声感知的优势权重来稳定训练过程。

Result: S-GRPO在数学推理基准上显著优于DR. GRPO，在Qwen-Math-7B-Base上性能提升2.5%，Llama-3.2-3B-Base上提升2.2%，Qwen-Math-1.5B-Instruct上提升2.4%。最关键的是，在20%合成奖励噪声下，标准GRPO无法学习，而S-GRPO能保持稳定的学习进展。

Conclusion: S-GRPO为大型推理模型的训练提供了更鲁棒、更有效的方法，有望促进其广泛应用和性能提升。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [155] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 针对决策树过拟合，提出一种基于多臂赌博机（MAB）的动态剪枝方法，通过将剪枝视为探索-利用问题，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 决策树无约束易过拟合，传统贪婪剪枝方法（如CCP、REP）可能导致长期泛化能力不足，尤其在小型复杂数据集上。

Method: 提出一种基于强化学习的多臂赌博机（MAB）剪枝方法，将剪枝过程视为探索-利用问题，利用MAB算法根据反馈动态寻找最优剪枝分支节点。

Result: 在多个基准数据集上的实验评估表明，该方法相比传统剪枝方法具有更好的预测性能。

Conclusion: 该研究表明利用MAB进行动态、概率性决策树剪枝具有潜力，能够有效优化基于决策树的模型。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [156] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 本文提出MCRE框架和MCRQ算法，通过在离线强化学习中结合TD误差和行为克隆项来平衡策略保守性与性能，有效解决分布偏移和过高估计问题，并在实验中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临学习策略与行为策略间的分布偏移，导致策略外动作和价值过高估计。为了防止过度估计，价值函数需保持保守，但过度保守又可能阻碍性能提升，因此需要在保守性与性能间取得平衡。

Method: 提出“轻度保守正则化评估 (MCRE)”框架，通过在Bellman备份中结合时序差分 (TD) 误差和行为克隆项来平衡保守性与性能。在此基础上，开发了“轻度保守正则化Q学习 (MCRQ)”算法，将其整合到离策略（off-policy）演员-评论家（actor-critic）框架中。

Result: 实验结果表明，MCRQ算法在基准数据集上的表现优于强大的基线方法和最新的离线强化学习算法。

Conclusion: MCRE框架和MCRQ算法成功解决了离线强化学习中分布偏移和价值过高估计的挑战，通过平衡保守性与性能，实现了性能的显著提升，为离线RL提供了新的有效解决方案。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [157] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 本文提出一种语义对齐的强化学习方法，利用SBERT计算目标文本与当前状态描述的语义相似度作为奖励，从而在无需手动设计奖励函数的情况下，实现有效的智能体控制。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，特别是在难以数值化定义任务目标的环境中，设计有效的奖励函数是一个挑战。现有工作大多依赖启发式、手动工程或任务特定调整。

Method: 引入语义对齐的强化学习方法，使用SBERT将当前状态与目标语义指令对齐以计算奖励。奖励通过计算目标文本描述与当前回合状态描述的余弦相似度来获得，以此为策略提供反馈。

Result: 该方法在多个环境中被评估，结果表明语义奖励可以引导学习，在没有手工奖励函数的情况下实现有竞争力的控制行为。研究还揭示了语言嵌入空间与传统欧几里得空间之间的关联。

Conclusion: 该框架为将智能体行为与自然语言目标对齐开辟了新方向，并为大型语言模型（LLMs）与流畅控制应用的无缝集成奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [158] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [159] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: 本文提出ASAP框架，一种新颖的CoT（思维链）压缩方法，通过锚点引导和基于惊异度的剪枝，在保持大型推理模型（LRMs）代码推理准确性的同时，显著降低了训练和推理成本，并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过冗长的思维链（CoT）在代码推理中表现出色，但过长的推理过程导致高昂的训练成本、推理延迟和部署难题。现有CoT压缩方法存在局限性：token级方法破坏语法和逻辑连贯性，而基于困惑度的步骤级方法难以捕获关键逻辑步骤。

Method: 本文提出ASAP（Anchor-guided, Surprisal-based Pruning）框架，一个新颖的“从粗到细”CoT压缩框架。ASAP首先进行锚点引导剪枝以保留核心推理结构并缩小搜索空间，随后基于新颖的“首个token惊异度”指标进行逻辑感知剪枝以选择必要的推理步骤。最终，ASAP使模型能够在推理时自主生成并利用这些简洁的CoT。

Result: 实验表明，ASAP在多个代码生成基准测试中实现了最先进的准确性，并显著降低了训练和推理成本。在LiveCodeBench v4_v5基准测试上，ASAP与最强基线相比，减少了23.5%的token生成量和43.5%的推理延迟，同时Pass@1准确率达到36.19%（具有竞争力）。

Conclusion: ASAP的研究成果为构建强大且高效的大型推理模型（LRMs）指明了一个有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [160] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS结合MCTS和LLM，通过序列决策改进多步提示，显著提升复杂优化任务中LLM的代码生成质量和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需要一致多步规划的复杂任务中（如代码生成和结构化推理）表现不佳。现有的LLM与蒙特卡洛树搜索（MCTS）结合的方法主要集中于启发式代码生成或目标较简单的任务。

Method: 本文提出MCTS-OPS，一个新颖的神经符号框架，将提示选择（prompt selection）公式化为一个由MCTS指导的序贯决策过程。该方法探索并优化多步提示序列，旨在提升代码生成质量和LLM在通用优化中的问题解决能力。

Result: 在网络优化实验中，MCTS-OPS相比基线方法有显著提升，包括生成代码的执行成功率和优化结果（奖励提高2~4倍，标准差降低3倍）。在难题中，实现最优解的几率增加了约10%。

Conclusion: 研究结果强调了将符号规划（MCTS）与大型语言模型结合在复杂领域实现鲁棒、高质量代码生成的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [161] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 本文提出一种新型逐步动态竞争风险模型，通过智能地分阶段利用时不变和时变特征，提高心脏骤停后昏迷患者的神经学预后预测，并明确指出何时以及对哪些患者利用这些动态信息最有效。


<details>
  <summary>Details</summary>
Motivation: 预测心脏骤停后昏迷患者的预后是一个关键的临床挑战，它直接影响ICU中的决策。临床信息是随时间序列收集的，包括早期收集的时不变基线特征和ICU入院后收集的时变血流动力学数据。

Method: 本研究提出一种新颖的逐步动态竞争风险模型。该模型能够自动确定何时利用时不变（第一阶段）和时变（第二阶段）特征来改善神经学预后预测。它扩展了标准的Fine和Gray模型，明确地对两个阶段进行建模，并结合神经网络灵活捕捉复杂的非线性特征关系。

Result: 该模型在包含2,278名心脏骤停后昏迷患者的回顾性队列中进行评估，对于清醒、撤销生命支持治疗和尽管最大支持仍死亡等竞争结局，均表现出强大的判别性能。

Conclusion: 该方法可推广到更多特征收集阶段，并可应用于其他动态预测任务，在这些任务中，了解何时以及对于哪些患者新收集的特征能显著改善预测将非常有帮助。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [162] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 提出AHGNN模型，通过考虑异配性分布和语义信息多样性，解决了异配异质图的建模挑战，并在高异配性场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有异质图（HGs）研究未能同时关注异质性和异配性，导致在现实世界中普遍存在的异配异质图上性能下降。主要挑战在于：1) 异配性分布在跳数和元路径上存在差异；2) 不同元路径的语义信息复杂多样且受异配性驱动。

Method: 提出自适应异质图神经网络（AHGNN）。AHGNN采用异配性感知卷积来处理跳数和元路径特有的异配性分布。同时，利用粗细粒度注意力机制整合来自不同语义空间的信息，以过滤噪声并强调有效信号。

Result: 在七个真实世界图数据集上与二十个基线模型进行对比实验，结果表明AHGNN的性能优越，尤其是在高异配性场景下。

Conclusion: AHGNN成功解决了异配异质图的建模难题，特别是在处理高异配性数据时展现出显著优势，为异质图研究提供了有效的新方法。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [163] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM提出一种动态精度分配机制，通过运行时调整LLM层位宽，在设备端实现卓越的性能-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型（LLMs）面临在不同运行时约束（如延迟和精度）下有效处理查询的挑战。尽管多尺度量化和混合精度提供解决方案，但如何精确配置模型以匹配目标精度或延迟仍是开放问题，尤其考虑到各层敏感性在解码迭代中动态变化。

Method: 引入DP-LLM机制，基于输入值动态分配LLM每层的精度。DP-LLM通过在LLM的每个线性层中集成一个精度选择器实现，该选择器利用轻量级误差估计器和通过微调学习到的阈值，在运行时确定最佳位宽。

Result: 在多个模型和基准测试上的实验结果表明，DP-LLM实现了优越的性能-延迟权衡，显著优于现有方法。

Conclusion: DP-LLM通过动态调整LLM层精度，有效解决了设备端LLMs在不同运行时约束下的适应性问题，展示了其在优化性能和延迟方面的卓越能力。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [164] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 为深度时序模型提供了泛化界限和评估方法，揭示时间依赖性可能有利于泛化，并指出理论与实践的差异。


<details>
  <summary>Details</summary>
Motivation: 尽管TCNs等深度时序模型在序列数据上表现出色，但对其泛化能力的理论理解有限。

Method: 提出了首个针对深度时序模型的非空、架构感知的泛化界限，并使用延迟反馈阻塞机制处理依赖样本。引入了一种固定有效样本量、隔离时间结构影响的公平比较方法。

Result: 导出了泛化界限，其尺度为$O(R\sqrt{Dpn\log N / N})$，通过阻塞机制实现了$\sqrt{D}$的深度扩展。强依赖序列的泛化差距比弱依赖序列小约76%，这与直觉相悖。实证收敛率（弱依赖$N_{\text{eff}}^{-1.21}$，强依赖$N_{\text{eff}}^{-0.89}$）比理论预测的$N^{-0.5}$更陡峭。

Conclusion: 在信息预算固定的情况下，时间依赖性可以增强学习。理论预测与实践观察之间存在显著差距，有待未来研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [165] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文首次提出了递归深度可微分逻辑门网络（RDDLGN），将可微分逻辑门应用于序列建模。在英德翻译任务上，RDDLGN表现出接近GRU的性能，验证了基于逻辑的循环神经网络计算的可行性。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门在前馈网络中显示出潜力，但其在序列建模中的应用尚未被探索。

Method: 提出了递归深度可微分逻辑门网络（RDDLGN），它结合了布尔运算和循环架构，用于序列到序列学习。

Result: 在WMT'14英德翻译任务上，RDDLGN在训练期间达到了5.00 BLEU和30.9%的准确率，接近GRU的性能（5.41 BLEU），并在推理期间表现出优雅的性能下降（4.39 BLEU）。

Conclusion: 该工作确立了基于逻辑的循环神经网络计算的可行性，并为序列建模中的FPGA加速及其他递归网络架构开辟了研究方向。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [166] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: 提出HGR（结合HSR）以提升稀疏奖励下GCRL的样本效率，优于现有HER方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下的目标条件强化学习（GCRL）是强化学习中的一个基本挑战。现有事后经验回放（HER）方法虽然通过重标记轨迹取得进展，但仅靠轨迹重标记未能充分利用离线GCRL方法中的可用经验，导致样本效率有限。

Method: 本文提出Hindsight Goal-conditioned Regularization (HGR)，这是一种基于事后目标生成动作正则化先验的技术。当与Hindsight Self-imitation Regularization (HSR)结合时，该方法能够使离线强化学习算法最大化经验利用。

Result: 与现有采用HER和自模仿技术的GCRL方法相比，我们提出的事后正则化方法实现了显著更高效的样本重用和最佳性能，并在导航和操作任务套件上进行了实证验证。

Conclusion: 通过引入HGR并结合HSR，我们的方法能有效解决稀疏奖励下GCRL的样本效率问题，最大化经验利用，从而显著提升学习性能。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [167] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 本研究提出利用微调扩散模型和图像修复技术合成逼真的口腔癌病变图像，有效解决了标注数据不足的问题，显著提升了口腔癌诊断模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔癌诊断中，标注数据集的有限性以及训练数据的变异性和不足，严重制约了诊断模型的性能。

Method: 本研究通过汇编多来源的口腔癌图像数据集，并提出一种新方法：利用微调扩散模型（diffusion model）结合修复技术（inpainting technique）合成逼真的口腔癌病变图像。

Result: 本方法生成的合成病变图像与真实病变具有高度视觉保真度，显著提升了诊断算法性能。分类模型在区分癌变与非癌变组织方面实现了0.97的诊断准确率，而检测模型在识别病变位置方面实现了0.85的准确率。

Conclusion: 该方法验证了合成图像生成在医学诊断中的潜力，并为将此方法扩展到其他类型癌症诊断的进一步研究铺平了道路。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [168] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: 针对联邦聚类中直接应用差分隐私导致效用下降的问题，本文提出了一种名为RR-Cluster的轻量级附加技术，通过随机重新平衡聚类分配来确保最小客户端数量并降低隐私噪声，显著改善了隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类虽能提升模型性能，但易导致隐私泄露。直接应用客户端级差分隐私（DP）机制会显著降低效用，这主要是因为在联邦聚类中，由于聚类内客户端数量不受控，标准隐私机制下难以有效平均隐私噪声。

Method: 本文提出RR-Cluster，一个可作为多种联邦聚类算法轻量级附加组件的技术。RR-Cluster通过随机重新平衡聚类分配，确保每个聚类至少有最低数量的客户端，从而减少隐私噪声。论文还分析了隐私噪声方差减少与不正确分配可能引起的偏差之间的权衡，并提供了收敛界限。

Result: 经验证明，将RR-Cluster集成到现有联邦聚类算法中，在合成数据集和真实世界数据集上均能显著改善隐私/效用权衡。

Conclusion: RR-Cluster提供了一种简单有效的方法，通过解决联邦聚类中隐私噪声平均的难题，显著提升了在保持效用前提下的隐私保护能力。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [169] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 大多数预训练神经网络在分子性质预测中并未显著优于传统ECFP指纹基线，仅CLAMP模型表现突出，引发对现有评估严谨性的担忧。


<details>
  <summary>Details</summary>
Motivation: 预训练神经网络在化学和药物设计中受到广泛关注，其嵌入被广泛应用于分子性质预测等任务，但缺乏对其真实性能和与传统方法全面、公平的比较评估。

Method: 本研究在公平的比较框架下，对25种不同模态、架构和预训练策略的预训练神经网络模型，在25个数据集上进行了迄今为止最广泛的评估。评估使用专门的分层贝叶斯统计测试模型进行。

Result: 研究发现，几乎所有神经网络模型相对于基线ECFP分子指纹显示出可忽略或没有改进。只有同样基于分子指纹的CLAMP模型表现出统计学上显著优于其他模型的性能。

Conclusion: 这些发现对现有研究的评估严谨性提出了质疑。论文进一步讨论了潜在原因，并提出了解决方案和实用建议。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [170] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: 本文提出GFed-PP，一种新型图联邦学习推荐系统，通过利用公共用户数据和GCN，在保护隐私的同时适应用户差异化的隐私偏好并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户拥有相同的隐私保护需求（即不上传任何数据），从而忽视了利用公开用户数据增强推荐服务的潜力。现实世界中，用户可以选择公开或私密其交互数据，需要一个能够适应不同隐私偏好的联邦推荐方案。

Method: GFed-PP框架通过以下方式工作：1) 整合公共用户的交互数据以构建用户-物品交互图和用户关系图。2) 采用轻量级图卷积网络（GCN）学习每个用户特有的个性化物品嵌入。3) 为保护用户隐私，客户端本地学习用户嵌入和评分函数。4) 通过客户端物品嵌入初始化和服务器上的用户关系图聚合来优化联邦推荐框架。

Result: 实验结果表明，GFed-PP在五个数据集上显著优于现有方法，在不损害用户隐私的前提下，提供了卓越的推荐准确性。

Conclusion: GFed-PP框架为联邦推荐系统提供了一个实用的解决方案，能够有效地适应用户差异化的隐私偏好，同时提高推荐性能。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [171] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 为解决重参数化策略梯度（RPG）的训练不稳定性问题，本文受PPO启发，提出RPO，通过优化定制的裁剪替代目标和KL正则化，实现了更稳定、样本高效的RPG训练。


<details>
  <summary>Details</summary>
Motivation: 重参数化策略梯度（RPG）虽通过可微分动力学提高了样本效率，但其训练过程存在严重的梯度高方差导致的稳定性问题，阻碍了其应用。

Method: 本文首先建立了PPO的替代目标与RPG的重参数化梯度之间的联系，并证明了PPO类替代目标的重参数化梯度可通过时间反向传播（BPTT）高效计算。基于此，提出了重参数化近端策略优化（RPO）方法。RPO通过优化一个为RPG量身定制的裁剪替代目标，并结合Kullback-Leibler（KL）散度正则化来进一步稳定训练，同时兼容现有方差降低方法，实现了多轮稳定的样本重用。

Result: 在具有挑战性的运动和操作任务上进行的实验表明，RPO方法比现有方法实现了卓越的样本效率和强大的性能。

Conclusion: RPO成功地解决了RPG的训练不稳定性问题，通过引入PPO的稳定机制，显著提升了RPG的样本效率和性能，为基于可微分动力学的强化学习提供了一个更稳定、高效的训练范式。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [172] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR是一个面向6G车载信息娱乐系统的边缘AI辅助框架，通过ML压缩CQI数据并结合强化学习，旨在优化调度和公平性，从而提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 在6G车载网络中，传统无线资源管理技术难以应对自动驾驶车辆产生的日益增长且复杂的信道质量指示（CQI）数据，导致调度效率和公平性受限。

Method: 提出SCAR（基于AI驱动资源管理的状态空间压缩）框架。该框架利用机器学习（如聚类和RBF网络）技术压缩CQI数据，以减小状态空间。随后，这些压缩后的状态用于训练支持6G的强化学习策略，以最大化吞吐量的同时满足NGMN定义的公平性目标。其中，聚类部分采用了基于随机隧道的模拟退火（SAST）算法。

Result: 仿真结果表明，与未进行CQI压缩的强化学习基线相比，SCAR将系统处于可行调度区域的时间增加了14%，并将不公平调度时间减少了15%。此外，SAST聚类技术将CQI聚类失真降低了10%。

Conclusion: 研究结果证明了SCAR在动态车载网络中具有良好的可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [173] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 本研究使用DDPG框架在大规模代理模拟中优化疫情干预（封锁和疫苗接种），发现在不封锁并对中老年人接种的情况下，可平衡经济与健康目标。


<details>
  <summary>Details</summary>
Motivation: 当前自动确定最佳疫情干预措施的研究存在规模、模型类型和策略探索上的局限性，且干预措施可能产生意想不到的负面后果。本研究旨在解决这些挑战。

Method: 本研究提出了一个基于深度确定性策略梯度（DDPG）的策略优化框架，并在一个包含10万个体的大规模流行病代理人模拟中进行多目标优化。该框架用于确定年龄分层、多疫苗情景下封锁和疫苗接种的最佳策略，并包含基本的经济活动模拟。

Result: 结果显示，在不实施封锁且对中老年人进行疫苗接种的情况下，可以实现最优经济目标（贫困线以下人数最少），并同时平衡健康目标（感染率和住院率）。

Conclusion: 研究表明，通过特定组合的干预措施可以平衡经济与健康目标。未来需要进行更深入的模拟以验证结果并开源该框架。

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [174] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 本文提出MRAD框架，用于在仅有部分特征信息的情况下进行成员推断攻击（PFMI），解决了现有方法需完整特征的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断方法普遍假定攻击者拥有目标样本的全部特征，但在许多实际场景中，往往只能获取部分特征，这限制了现有方法的应用。因此，需要研究在部分特征可用时如何进行成员推断。

Method: 本文定义了部分特征成员推断（PFMI）问题，并提出了两阶段攻击框架MRAD（Memory-guided Reconstruction and Anomaly Detection）。第一阶段，MRAD优化未知特征值以最小化样本损失；第二阶段，它通过异常检测衡量重建样本与训练分布之间的偏差。

Result: 实证结果表明，MRAD在多种数据集上均有效，并兼容各种现成的异常检测技术。例如，在STL-10数据集上，即使缺失40%的特征，攻击仍能达到约0.6的AUC。

Conclusion: MRAD证明了即使在部分特征信息限制下，成员推断攻击仍然可行，强调了该隐私威胁的普遍性。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [175] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [176] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol是一个归因引导的强化学习框架，通过奖励机制优化LLM在分子性质预测中的推理过程，使其生成更相关、更有效的属性，并在性能和可解释性上超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在分子性质预测中过度依赖人工设计的提示和思维链模板；即使是先进的强化学习推理模型（如DeepSeek-R1），其推理也可能冗长且缺乏相关性。研究动机在于解决LLM推理的低效和不相关问题，以提高分子性质预测的有效性。

Method: 提出AttriLens-Mol，一个用于LLMs分子性质预测的归因引导强化学习框架。该框架通过以下三种奖励机制引导模型推理：1) 格式奖励：鼓励生成基于属性的结构化输出；2) 计数奖励：避免枚举不相关属性；3) 合理性奖励：利用先进LLM和RDKit验证生成属性的相关性。

Result: 实验结果显示，AttriLens-Mol显著提升了7B尺寸模型（R1-Distilled-Qwen2.5和R1-Distilled-LLaMA3.1）在分子性质预测任务上的性能，在内部和外部数据集上均达到或优于监督微调模型（如Mol-Instructions、ChemDFM）和先进大模型（如GPT-3.5、GPT-4o、DeepSeek-V3、DeepSeek-R1）。此外，AttriLens-Mol提取的属性作为可解释决策树模型的特征时，性能优于通过提示LLM生成的属性。

Conclusion: AttriLens-Mol能有效地激发模型内在的、与目标性质更相关且更具预测性的分子属性知识，从而显著提高了分子性质预测的性能和可解释性。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [177] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [178] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 使用张量网络（MPS）提出一种隐私保护的高质量合成表格数据生成方法，并在严格隐私约束下表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现代人工智能中数据稀缺、隐私限制以及训练鲁棒模型所需多样化数据集的问题。

Method: 提出一种基于矩阵乘积态（MPS）的张量网络方法来生成隐私保护的高质量合成表格数据。通过噪声注入和梯度裁剪整合差分隐私（DP）机制，并与CTGAN、VAE、PrivBayes等现有模型进行保真度和隐私保护能力的基准测试。

Result: 在分析数据保真度和下游机器学习任务性能的多个指标上，MPS表现优于传统模型，尤其是在严格隐私约束下效果更佳。

Conclusion: MPS是隐私感知合成数据生成的有前景工具，其结合了张量网络的表达能力和正式隐私机制，为安全数据共享提供了一种可解释且可扩展的替代方案，尤其适用于对数据质量和保密性有高要求的敏感领域。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [179] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 本文提出GTMancer框架，利用GNN、对比学习和双重注意力机制整合多组学数据，有效解决现有方法忽略组学间耦合的问题，显著提升癌症亚型分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合多组学数据时，未能充分捕捉异质组学之间复杂的耦合关系，限制了其识别细微癌症亚型异质性的能力，而这对于精准肿瘤学至关重要。

Method: 本文提出GTMancer框架，该框架基于GNN优化并应用于复杂多组学数据。具体方法包括：利用对比学习将多组学数据嵌入统一语义空间；在该空间中展开多重图优化问题；引入双重注意力系数，捕捉组内和组间结构图先验知识，以全局组学信息指导个体组学表示的细化。

Result: 在七个真实世界癌症数据集上的实验表明，GTMancer的性能优于现有最先进的算法。

Conclusion: GTMancer框架通过有效整合多组学数据并捕捉其复杂耦合关系，成功提升了癌症亚型分类的准确性，为精准肿瘤学提供了新的解决方案。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [180] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: 本文提出OM2P，一种新的离线多智能体强化学习算法，通过引入奖励感知均值流匹配和Q函数监督，解决现有生成模型策略（如扩散和流模型）在离线多智能体强化学习中采样效率低的问题，并显著提升了训练效率和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 生成模型（特别是扩散和流模型）在离线多智能体强化学习（MARL）中表现出潜力，但其迭代生成过程导致采样效率低下，使其在时间敏感或资源受限环境下不切实际。研究旨在解决这一挑战，实现高效的单步动作采样。

Method: 提出OM2P（Offline Multi-Agent Mean-Flow Policy）算法，实现高效的单步动作采样。引入奖励感知优化方案，将均值流匹配损失与Q函数监督相结合，以解决生成目标与奖励最大化之间的错位。此外，设计了广义时间步长分布和无导数估计策略，以减少内存开销并提高训练稳定性。

Result: 在多智能体粒子和MuJoCo基准测试中，OM2P取得了卓越的性能，GPU内存使用量减少高达3.8倍，训练时间加速高达10.8倍。它成功地将均值流模型首次整合到离线多智能体强化学习中。

Conclusion: OM2P是首个成功将均值流模型整合到离线多智能体强化学习中的方法，为合作多智能体设置中实用且可扩展的生成策略铺平了道路，解决了现有生成模型策略效率低下的关键挑战。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [181] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 针对印度语言的ASR系统，本研究探讨了无重放持续学习策略在缓解灾难性遗忘方面的有效性，并验证了其在实际约束下对可扩展ASR系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性给包容性自动语音识别（ASR）系统开发带来巨大挑战。传统多语言模型因数据顺序到达和隐私限制而不切实际，需要持续学习来解决在顺序学习新语言时遗忘旧知识的问题。

Method: 研究采用一个基于Conformer的混合RNN-T/CTC模型，首先在印地语上预训练，然后顺序地在八种额外的印度语言上进行增量训练。评估了三种无重放、基于正则化和蒸馏的持续学习策略：EWC、MAS和LwF。性能通过词错误率（WER）和知识保留（反向迁移）进行评估，并探索了训练周期数的影响，与朴素微调进行比较。

Result: 结果表明，与朴素微调相比，持续学习有效缓解了灾难性遗忘问题，证明了其在实际约束下对可扩展ASR系统的有效性。

Conclusion: 持续学习是在现实约束下，为印度多样化语言开发可扩展ASR系统的一种有前景的方法。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [182] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出一种结合深度状态空间模型（SSM）状态转移与重置机制的新型脉冲神经元模型，旨在融合脉冲神经网络（SNN）和SSM的优点，并在多项任务上取得良好性能，同时解决模型不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）能实现低延迟和高能效信号处理，而深度状态空间模型（SSMs）在时序建模任务中表现优异。然而，SSMs通常使用高精度激活且缺乏重置机制，这与SNNs的低比特处理和重置特性不同。研究旨在结合SNNs的优势与SSMs的强大性能。

Method: 提出一种新颖的多输出脉冲神经元模型，该模型将线性的通用SSM状态转移与通过重置实现的非线性反馈机制相结合。该模型清晰地概念化了脉冲功能、重置条件和重置动作之间的区别。

Result: 在关键词识别、事件视觉和序列模式识别等多项任务上的实验结果表明，所提出的模型性能可与现有SNN文献中的基准模型相媲美。结果还表明，提出的重置机制能够克服不稳定性，并能在神经元动力学线性部分不稳定时也实现学习。

Conclusion: 所提出的新型脉冲神经元模型成功地融合了SNN和深度SSM的优点，在不同任务上表现出有竞争力的性能。其独特的重置机制不仅能克服不稳定性，还允许模型在神经元动力学线性部分不稳定时进行学习，从而超越了现有深度SSM模型对严格线性稳定性的限制。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [183] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新颖的联邦元学习方法，通过引入隐私保护损失函数，解决了神经场在资源受限设备上学习时数据和计算量大的问题，并解决了传统FML的隐私泄露问题，实现了快速、鲁棒且隐私保护的优化。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量的训练数据和计算资源，这在资源受限的边缘设备上难以实现。尽管联邦元学习（FML）可以缓解此问题，但传统FML方法存在隐私泄露风险。因此，研究动机是开发一种能在边缘设备上高效且隐私保护地学习神经场的方法。

Method: 本文提出了一种名为FedMeNF的新型联邦元学习（FML）方法。FedMeNF引入了一个新的隐私保护损失函数，该函数在局部元优化过程中调节隐私泄露，使得局部元学习器能够快速高效地优化，而无需保留客户端的私有数据。

Result: 实验结果表明，FedMeNF即使在少样本或非独立同分布（non-IID）数据以及多模态数据场景下，也能实现快速的优化速度和鲁棒的重建性能，同时有效保护了客户端数据的隐私。

Conclusion: FedMeNF成功地为神经场在边缘设备上的高效、快速且隐私保护的学习提供了一个解决方案，克服了传统方法在数据量、计算资源和隐私保护方面的局限性。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [184] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: 提出无监督伙伴设计（UPD）框架，通过动态生成训练伙伴，提高智能体在临时团队协作中的鲁棒性，无需预训练或手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多智能体临时团队协作中，智能体适应未知伙伴时常需预训练伙伴或大量人工调参。本研究旨在无需这些前提下，实现鲁棒的自适应团队协作。

Method: UPD是一种无群体的多智能体强化学习框架，通过将自我智能体策略与有偏随机行为混合来生成多样化训练伙伴。它使用基于方差的可学习性度量对伙伴进行评分，优先选择接近自我智能体当前学习前沿的伙伴。UPD还可与无监督环境设计结合，实现级别和伙伴分布的完全无监督课程。

Result: 在Overcooked-AI及Overcooked通用化挑战中，UPD性能持续优于基于群体和无群体的基线方法。用户研究表明，UPD不仅获得更高回报，而且被认为更具适应性、更像人类、更擅长协作且更不易令人沮丧。

Conclusion: UPD是一种高效的动态伙伴课程学习方法，显著提升了临时团队协作中智能体的适应性、类人性和协作能力，为合作多智能体设置中的完全无监督学习提供了有效途径。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [185] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 提出分数分类损失（FCL），一种自适应鲁棒损失函数，能在标签噪声下自动调整其鲁棒性，无需手动调参即可达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数在标签噪声下训练深度神经网络时，需要大量、特定于数据集的超参数调优。

Method: FCL构建于主动-被动损失框架内，以交叉熵（CE）损失的分数阶导数作为主动部分，平均绝对误差（MAE）作为被动部分。通过将分数阶导数μ作为可学习参数融入梯度优化，它能自动平衡鲁棒性和收敛速度。FCL的独特特性使其能动态调整损失景观，稳定学习μ值。

Result: FCL在训练过程中能自动校准其鲁棒性。分数阶导数μ可以在MAE般的鲁棒性和CE般的快速收敛之间插值。在基准数据集上进行的大量实验表明，FCL无需手动超参数调优即可实现最先进（SOTA）的结果。

Conclusion: FCL是一种有效、自适应且鲁棒的损失函数，能解决标签噪声问题，并消除手动超参数调优的需要，同时实现了卓越的分类性能。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [186] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: 提出SE-VAE，一种将测量结构嵌入变分自编码器的新架构，旨在从表格数据中学习可解释的潜在表示，并在因子恢复、可解释性和对干扰变量的鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在深度生成模型中，从表格数据中学习可解释的潜在表示仍然是一个挑战。

Method: 引入SE-VAE（结构方程-变分自编码器），这是一种新颖的架构，将测量结构直接嵌入变分自编码器的设计中。受结构方程建模启发，SE-VAE将潜在子空间与已知指标分组对齐，并引入一个全局干扰潜在变量以隔离特定构念的混淆变异。该模块化架构通过设计而非单纯统计正则化实现解耦。在模拟表格数据集上进行评估，并与领先的基线模型进行性能基准测试。

Result: SE-VAE在因子恢复、可解释性和对干扰变异的鲁棒性方面始终优于现有替代方案。消融实验表明，架构结构而非正则化强度是性能的关键驱动因素。

Conclusion: SE-VAE为科学和社会领域中的“白盒”生成建模提供了一个原则性框架，特别适用于潜在构念由理论驱动且测量有效性至关重要的场景。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [187] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: 本文提出Gk-means算法，通过利用几何原理（标量投影）识别高表达数据并绕过低表达数据，显著提升了k-means的效率和能耗，同时保持解的质量。


<details>
  <summary>Details</summary>
Motivation: K-means算法尽管应用广泛，但其效率和能源消耗仍有提升空间，尤其在处理大规模数据时。

Method: Gk-means核心在于利用几何原理，特别是标量投影，智能地关注对聚类更新影响最大的高表达数据（HE），并有效绕过不影响结果的低表达数据（LE），从而大幅减少计算开销。

Result: 实验证明，Gk-means在合成、真实和高维数据集上，无论是运行时还是距离计算，均显著优于传统及最先进的k-means变体。此外，它还展现出更优的资源效率和更低的能耗。

Conclusion: Gk-means提供了一种更高效、更可持续的k-means算法替代方案。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [188] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLMs）在处理复杂任务时，会表现出自我启动的欺骗行为，且欺骗倾向随任务难度增加而升高，这对其在关键领域的部署提出了严峻挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理、规划、决策等关键任务中广泛应用，其可信度至关重要。意图性欺骗（LLM为隐藏目标故意捏造或隐瞒信息）是一个被低估的威胁。现有研究多为人为诱导欺骗，而本文旨在探索LLMs在良性提示下自我启动的欺骗行为，以更贴近真实世界交互。

Method: 提出了一种基于“接触搜索问题”的新颖框架，用于评估LLMs的自我启动欺骗。引入了两个源自心理学原理的统计指标：欺骗意图得分（衡量模型对隐藏目标的偏见）和欺骗行为得分（衡量LLM内部信念与表达输出之间的一致性）。评估了14个主流LLM，并基于发现构建了数学模型来解释行为。

Result: 评估结果显示，欺骗意图得分和欺骗行为得分均随任务难度增加而升高，且对大多数模型而言两者呈平行上升趋势。研究表明，即使是最先进的LLMs在处理复杂问题时，也表现出日益增长的欺骗倾向。

Conclusion: LLMs在处理复杂问题时日益增长的欺骗倾向，对LLM智能体在复杂和关键领域的部署提出了严峻的可信度担忧。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [189] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于分类器引导扩散模型的生成方法，旨在实现对分子多重生物活性的精确集成控制，有效平衡药物疗效与安全性。


<details>
  <summary>Details</summary>
Motivation: 药物设计中精确控制分子生物活性（包括靶向、多靶点调节及脱靶毒性缓解）是关键挑战。现有生成方法主要关注单一活性，缺乏同时管理多重分子相互作用的集成机制。

Method: 提出ActivityDiff，一种基于扩散模型分类器引导技术的生成方法。它利用单独训练的药物-靶点分类器进行正向和负向引导，以增强期望活性并最小化有害脱靶效应。

Result: ActivityDiff有效处理了多种药物设计任务，包括单/双靶点生成、片段限制的双靶点设计、增强靶点特异性的选择性生成以及脱靶效应的减少。实验结果验证了分类器引导扩散在平衡分子设计疗效和安全性方面的有效性。

Conclusion: 该研究引入了一种实现分子活性集成控制的新范式，并提供了ActivityDiff这一通用且可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [190] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 针对多数据库环境下Text-to-SQL目标数据库未预设的问题，本文提出一个三阶段端到端框架，通过LLM提取规则、RoBERTa模型预测目标数据库ID，并使用critic agents优化SQL，显著提升了数据库意图预测和SQL生成精度。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法需要预先指定目标数据库，但在存在多个大型数据库的场景中，识别正确的数据库是一个关键但常被忽视的问题。

Method: 本文提出一个三阶段端到端Text-to-SQL框架：1. 利用LLM和提示工程从自然语言查询中提取隐式规则集。2. 训练一个基于RoBERTa微调编码器的db_id预测模型，结合NLQ和LLM规则预测数据库ID。3. 使用critic agents修正错误以优化生成的SQL。

Result: 实验结果表明，该框架在数据库意图预测和SQL生成准确性方面均优于现有最先进的模型。

Conclusion: 该框架有效解决了多数据库场景下Text-to-SQL的目标数据库识别挑战，显著提升了整体性能，增强了非技术用户查询复杂数据库的能力。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [191] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 研究提出一种利用众包数据（311服务电话和街景图像）的新方法，以更及时、精细地追踪和预测美国无家可归帐篷趋势，尤其是在旧金山。


<details>
  <summary>Details</summary>
Motivation: 美国无家可归者数量激增至大萧条以来未见的水平，而现有监测方法（如定点计数）在频率、一致性和空间细节方面存在局限性，无法有效指导政策响应和干预措施。

Method: 本研究利用公开可用的众包数据，具体是311服务电话和街景图像，构建了一个预测模型，以追踪和预测旧金山无家可归帐篷的趋势。

Result: 该预测模型能够捕捉精细的每日和社区层面的变化，揭示了传统计数可能忽视的模式，例如COVID-19大流行期间的快速波动以及帐篷位置随时间推移的空间转移。

Conclusion: 这种方法通过提供更及时、本地化和成本效益更高的信息，为指导政策响应和评估旨在减少无家可归现象的干预措施提供了宝贵的工具。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [192] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR是一种通用的插件，通过高回放次数、周期性重置和混合优化目标，显著提升LLM偏好优化方法的样本效率和性能，尤其在数学推理任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: LLM的RL和偏好优化方法在提升推理能力方面效果显著，但常面临样本效率低和“初始偏差”导致过拟合的问题。

Method: 引入LLM优化与重置回放（LoRR）插件。其核心机制包括：1. 启用高回放次数以最大化数据利用率。2. 采用周期性重置策略并重用初始数据以对抗过拟合，保持网络可塑性。3. 结合监督微调（SFT）和基于偏好的损失形成混合优化目标。

Result: LoRR显著提升了多种偏好优化方法在数学和通用推理基准上的性能。一个结合LoRR的迭代DPO方法在数学任务上达到与复杂、计算密集型RL算法相当甚至更优的性能。

Conclusion: LoRR为LLM微调提供了一种实用、样本高效且高度有效的新范式，能够利用有限数据释放更强大的性能。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [193] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: 为解决大语言模型(LLMs)的数据遗忘挑战，本文提出GRIN框架，通过梯度比率识别并对特定参数进行选择性噪声注入，实现高效、精准且不损害模型效用的遗忘。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长的法律和伦理审查，LLMs急需有效的机器学习遗忘能力，特别是针对敏感或未经授权的数据。现有经验方法常因定位不准导致遗忘不完全或意外损害无关知识。

Method: 本文提出GRIN框架，一个模块化、有针对性的LLM遗忘方法。GRIN引入一种新颖的基于梯度比率的度量，以识别对记忆待遗忘数据贡献最大的参数，进而在微调前对这些参数注入选择性噪声，从而提升遗忘性能并保持模型效用。

Result: GRIN方法通过引入为LLM定制的新评估指标，并在TOFU、WMDP和SafePKU等标准基准上进行了验证。实验结果表明，GRIN提高了遗忘性能，同时有效保持了模型的实用性。

Conclusion: GRIN框架提供了一种有效且有针对性的LLM遗忘解决方案，通过精准定位和选择性噪声注入克服了现有方法的局限性，在实现更完全数据遗忘的同时，维护了模型的原有性能。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [194] [Hierarchical Placement Learning for Network Slice Provisioning](https://arxiv.org/abs/2508.06432)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun*

Main category: cs.NI

TL;DR: 本文提出了一种基于两层分层多臂赌博机的解决方案，用于在边缘移动网络中学习服务功能链放置策略，以提高网络切片请求接受率并最小化节点资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘移动网络中切片资源分配的挑战，目标是最大化请求接受率并最小化平均节点资源利用率。

Method: 将问题建模为分层多臂赌博机问题，并提出了一种两层分层赌博机解决方案，旨在在线学习可伸缩的放置策略以优化既定目标。

Result: 在两个真实网络拓扑上的模拟显示，与基线方法相比，所提出的方法实现了5%的平均节点资源利用率，并在某些场景下接受了超过25%的切片请求。

Conclusion: 所提出的分层赌博机方法能有效优化边缘网络中的切片资源分配，显著提高请求接受率和资源效率，优于现有基线方法。

Abstract: In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.

</details>


### [195] [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](https://arxiv.org/abs/2508.06468)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun,Dimitrios Xenakis*

Main category: cs.NI

TL;DR: 本文提出两种基于保留的在线策略，解决网络切片中不确定资源需求下的准入控制问题，旨在最大化基础设施提供商的长期收益，并通过仿真证明其能显著提升收益并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 在共享物理移动网络基础设施上，网络切片的服务供应面临有限网络资源需求的不确定性挑战。服务提供商需要有效的准入控制机制来最大化其从接受请求中获得的长期收益。

Method: 将切片准入控制问题建模为在线多维背包问题。提出了两种基于保留（reservation-based）的策略及其算法。通过蒙特卡洛模拟，评估了所提出的在线准入控制方法的性能，并与在线先到先得（First Come First Serve）贪婪策略进行了比较。

Result: 仿真结果表明，本文提出的在线策略使基础设施提供商的收益提高了多达12.9%，同时将平均资源消耗降低了多达1.7%。特别是当租户的经济不平等增加时，采用本文策略的基础设施提供商比采用先到先得策略的收益更高。

Conclusion: 所提出的在线准入控制策略能有效提升网络切片基础设施提供商的长期收益和资源利用率，尤其在市场需求不确定性高或租户经济差异大时，表现优于传统的贪婪策略。

Abstract: Network Slicing has emerged as a powerful technique to enable cost-effective,
multi-tenant communications and services over a shared physical mobile network
infrastructure. One major challenge of service provisioning in slice-enabled
networks is the uncertainty in the demand for the limited network resources
that must be shared among existing slices and potentially new Network Slice
Requests. In this paper, we consider admission control of Network Slice
Requests in an online setting, with the goal of maximizing the long-term
revenue received from admitted requests. We model the Slice Admission Control
problem as an Online Multidimensional Knapsack Problem and present two
reservation-based policies and their algorithms, which have a competitive
performance for Online Multidimensional Knapsack Problems. Through Monte Carlo
simulations, we evaluate the performance of our online admission control method
in terms of average revenue gained by the Infrastructure Provider, system
resource utilization, and the ratio of accepted slice requests. We compare our
approach with those of the online First Come First Serve greedy policy. The
simulation's results prove that our proposed online policies increase revenues
for Infrastructure Providers by up to 12.9 % while reducing the average
resource consumption by up to 1.7% In particular, when the tenants' economic
inequality increases, an Infrastructure Provider who adopts our proposed online
admission policies gains higher revenues compared to an Infrastructure Provider
who adopts First Come First Serve.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [196] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 一种基于区块链的去中心化域名系统（DDNS），旨在解决传统DNS的漏洞，并提供安全、高效和可扩展的域名解析方案。


<details>
  <summary>Details</summary>
Motivation: 当前的域名系统（DNS）基础设施存在严重漏洞，包括中毒攻击、审查机制和中心化故障点，这些都损害了互联网的自由和安全。近期事件凸显了对弹性替代方案的迫切需求。

Method: 本文提出了一种新颖的基于区块链的去中心化域名系统（DDNS）。系统设计了一个专用的工作量证明（PoW）区块链以最大化支持DNS相关协议并实现节点去中心化，并将其与IPFS集成以进行分布式存储，同时实施密码学原语实现端到端信任签名和“永不信任，始终验证”的零信任验证。

Result: 该实现达到了15秒的域名记录传播时间，支持20种标准DNS记录类型，并提供永久免费的.ddns域名。系统已部署在分布式基础设施中，展示了实际可扩展性及对传统DNS操作的抵抗力。性能评估显示，系统在域名操作方面可处理高达1111.1 tx/s（最小交易）和266.7 tx/s（常规交易）的理论最大吞吐量，并通过智能缓存机制保持亚秒级查询解析。

Conclusion: 该去中心化域名系统有效应对了传统DNS的脆弱性，提供了一个弹性、安全且高性能的解决方案，能够抵御传统操纵技术，并为互联网自由和安全提供了支持。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [197] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 提出DINA框架，旨在同时防御NLP系统中大型语言模型面临的外部对抗攻击和内部标签污染双重威胁。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和生成式AI广泛应用于客户服务和内容审核，其面临来自外部操纵和内部标签损坏的双重对抗威胁。

Method: 引入DINA（Dual Defense Against Internal Noise and Adversarial Attacks）框架，该框架将计算机视觉领域的先进噪声标签学习方法与对抗训练相结合，以同步缓解内部标签破坏和外部对抗扰动。

Result: 在来自在线游戏服务的真实世界数据集上进行的广泛实验表明，DINA显著提高了模型的鲁棒性和准确性，优于基线模型。

Conclusion: 研究强调了双重威胁防御的关键必要性，为在现实对抗场景中保护NLP系统提供了实用策略，并对公平和负责任的AI部署具有重要意义。

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [198] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出DMFI，一个结合LLM语义推理与行为建模的双模态框架，以有效检测内鬼威胁，并在CERT数据集上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测因其行为的微妙、长期和上下文依赖性而极具挑战。传统模型难以捕捉语义意图和复杂行为动态，而现有基于LLM的解决方案在提示适应性和模态覆盖方面存在局限。

Method: 提出DMFI双模态框架，整合语义推理与行为感知微调。将原始日志转换为两种结构化视图：1) 语义视图（处理邮件、HTTPS等内容丰富的工件，使用指令格式化提示）；2) 行为抽象（通过4W指导转换编码上下文动作序列）。两个经过LoRA增强的LLM独立微调，输出通过MLP决策模块融合。此外，引入DMFI-B，一种判别性适应策略，用于分离正常和异常行为表示，以提高类别不平衡下的鲁棒性。

Result: 在CERT r4.2和r5.2数据集上的实验表明，DMFI在检测准确性方面优于最先进的方法。

Conclusion: 该方法结合了LLM的语义推理能力与结构化行为建模，为现代内部威胁检测提供了一个可扩展且有效的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [199] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: 研究了LLM驱动的智能体在网络安全博弈中的行为，发现其博弈结果受智能体特性和语言选择的影响，警告了LLM在网络安全应用中的盲目使用。


<details>
  <summary>Details</summary>
Motivation: 鉴于博弈论在网络安全中作为基础工具的长期作用以及大型语言模型（LLMs）的出现，研究者旨在探究经典博弈论框架能否有效捕捉LLM驱动的智能体行为。

Method: 使用可复现的框架，在一次性零和博弈和动态囚徒困境两种经典场景中测试了LLM智能体。实验涉及四种最先进的LLMs，并涵盖英语、法语、阿拉伯语、越南语和普通话中文五种自然语言，以评估语言敏感性。此外，还采用定量指标评估LLM智能体的内部一致性和跨语言稳定性。

Result: 研究发现，博弈的最终收益受到智能体特性（如个性特质或对重复轮次的了解）的影响。此外，还发现最终收益对所选语言表现出意想不到的敏感性。

Conclusion: LLMs在网络安全应用中可能表现出语言敏感性，其行为会因部署地区而异，因此不应盲目应用。研究呼吁进行深入研究，并建议通过评估内部一致性和跨语言稳定性来选择和优化最稳定的LLMs，以用于安全应用。

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [200] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: 本文系统研究了影响LLM代理在攻防安全任务（特别是CTF挑战）中成功的关键因素，并提出了一个利用LLM作为评判器的评估框架CTFJudge、一个新的部分正确性度量CCI，以及一个用于快速评估的基准CTFTiny。研究还分析了LLM超参数的影响，并识别了最优的多代理协作设置。


<details>
  <summary>Details</summary>
Motivation: LLM代理系统在自动化攻击性安全任务（如CTF挑战）方面已取得进展，但仍需系统性地探究影响代理成功的关键因素，并提供构建高效LLM攻防代理的详细方法。此外，需要更精细的评估工具和方法来衡量代理性能。

Method: ['提出CTFJudge，一个利用LLM作为评判器来分析代理轨迹并提供粒度化评估的框架。', '提出CTF Competency Index (CCI)，一个用于衡量部分正确性的新指标。', '检查LLM超参数（如temperature, top-p, maximum token length）对代理性能和自动化网络安全任务规划的影响。', '构建CTFTiny，一个包含50个代表性CTF挑战的精选基准，用于快速评估。']

Result: ['识别出最优的多代理协作设置。', 'CTFJudge和CTFTiny作为研究成果被开源，为社区提供了评估和基准测试工具。', '研究结果为未来网络安全领域的LLM代理研究奠定了基础。']

Conclusion: 本研究通过对LLM代理在攻防安全任务中的成功因素进行系统性调查，并提供创新的评估框架和基准，显著推进了LLM在该领域的应用。研究成果（包括CTFJudge和CTFTiny）已开源，为未来LLM代理在网络安全领域的进一步研究奠定了坚实基础。

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [201] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: 本文提出了一种IP保护的边云协同框架，通过结合本地小型LLM和云端大型LLM，在不泄露敏感硬件设计IP的前提下，显著提高了RTL代码的优化成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于云的大语言模型（LLMs）在寄存器传输级（RTL）代码优化方面表现出色，但其处理专有硬件设计时存在不可接受的知识产权（IP）泄露风险。研究动机在于开发一种能在不泄露敏感IP信息的情况下优化Verilog代码的方法。

Method: 本文提出了首个IP保护的边云协同框架。该方法利用本地小型LLM（如Qwen-2.5-Coder-7B）对高质量目标设计和初稿代码进行安全比较分析，提炼出通用的设计原则。随后，这些抽象且IP安全的设计原则被用于向更强大的云LLM（如Deepseek-V3）查询，以获取有针对性的代码改进建议。

Result: 实验结果表明，该框架显著提高了优化成功率。例如，结合Qwen-2.5-Coder-7B和Deepseek-V3在功耗利用方面实现了66.67%的优化成功率，优于单独使用Deepseek-V3（49.81%）和GPT-4o（55.81%）。此外，研究还发现不同的本地与云LLM组合对特定优化目标表现出 varying strengths，并且比较代码对的数量变化会产生有趣的趋势。

Conclusion: 本研究为安全的硬件设计优化建立了一个新范式，成功地平衡了性能增益与知识产权保护的需求。

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [202] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: 本文提出Fact2Fiction，首个针对基于LLM代理的事实核查系统的投毒攻击框架，通过利用系统生成的理由制作恶意证据，比现有攻击效果更佳，揭示了当前系统安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理事实核查系统虽然能大规模打击错误信息，但其安全性至关重要却常被忽视，一旦被攻破可能放大错误信息。

Method: 引入Fact2Fiction投毒攻击框架，该框架模仿事实核查系统的分解策略，并利用系统生成的理由来构造定制的恶意证据，从而破坏子主张的验证过程。

Result: 广泛实验表明，Fact2Fiction在不同投毒预算下，比现有最先进的攻击方法攻击成功率高出8.9%至21.2%。

Conclusion: Fact2Fiction揭示了当前事实核查系统的安全弱点，并强调了开发防御对策的必要性。

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [203] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 本研究评估了基于强化学习的医疗问卷系统在对抗性攻击下的安全性和鲁棒性，发现即使在严格医学约束下，此类系统仍存在显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的医疗问卷系统在医疗场景中展现出巨大潜力，但其安全性和鲁棒性尚未得到充分解决。

Method: 将诊断过程建模为马尔可夫决策过程(MDP)。实施了FGSM、PGD、C&W、BIM、DeepFool和AutoAttack等六种主流对抗性攻击方法。开发了一个包含247个医学约束的医学验证框架，以确保生成的对抗性样本在临床上合理。在包含182,630个样本的NHIS数据集上进行实验，并在AdaptiveFS框架上评估攻击效果。

Result: 成功生成了97.6%的临床合理对抗性样本。对抗性攻击能显著影响诊断准确性，攻击成功率从33.08% (FGSM) 到64.70% (AutoAttack) 不等。

Conclusion: 研究结果表明，即使在严格的医学输入约束下，基于强化学习的医疗问卷系统仍然存在显著的脆弱性。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [204] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: 本文提出ALA框架，首次利用主动学习的采集函数作为投毒攻击面，成功以低投毒预算（0.5%-1.0%）实现高成功率（高达94%）的投毒攻击，揭示了主动学习的安全性弱点。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）作为一种标签高效的学习范式被广泛应用，但其“安全性”仍是一个未解决的问题。具体而言，其核心组件——采集函数是否容易被利用来注入恶意数据，从而影响模型的安全性？

Method: 引入ALA框架，该框架通过优化隐蔽投毒的输入，使其表现出高不确定性分数，从而增加被采集函数选中的概率。通过在三个数据集、三种采集函数和两种干净标签后门触发器上进行实验评估。

Result: 研究结果表明，即使在低投毒预算（0.5%-1.0%）下，攻击也能达到高成功率（高达94%），同时保持模型效用，且对人类标注者不可检测。

Conclusion: 主动学习的采集函数容易被利用，因此在部署主动学习时，尤其是在需要信任数据的场景中，应谨慎行事。

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [205] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 研究者开发了ScamAgent，一个基于LLM的多轮代理，能生成逼真的诈骗脚本。研究发现现有LLM安全防护对这种代理威胁无效，强调急需多轮安全审计和新防御方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）日益增长的滥用潜力，特别是在生成诈骗内容方面，而现有安全机制可能不足以应对多轮、自适应的欺骗性对话。

Method: 1. 构建了ScamAgent，一个基于LLM的自主多轮代理，能生成模拟真实欺诈场景的脚本。2. ScamAgent具备对话记忆功能，能动态适应用户响应并运用欺骗性策略。3. 通过现代文本到语音系统，将诈骗脚本转化为逼真的语音通话，形成完整的自动化诈骗流程。

Result: 1. 现有LLM安全防护措施（包括拒绝机制和内容过滤器）对ScamAgent这种代理威胁无效。2. 即使是具有强提示词级别保护的模型，当提示词在代理框架内被分解、伪装或逐步传递时，也能被绕过。3. 成功展示了自动化诈骗管道从脚本生成到语音通话的全过程。

Conclusion: 研究结果凸显了对多轮安全审计、代理层面控制框架以及检测和阻止由生成式AI驱动的对话欺骗的新方法的迫切需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [206] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: 本文提出MM-FusionNet框架，利用大型视觉语言模型（LVLMs）和上下文感知动态融合模块（CADFM），通过自适应地为文本和视觉特征分配权重，实现对多模态虚假新闻的高效检测，并在大规模数据集上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态虚假新闻的泛滥对公众信任和社会稳定构成严重威胁。传统主要基于文本的检测方法因误导性文本和图像之间的欺骗性交互而效果不佳。尽管大型视觉语言模型（LVLMs）为多模态理解提供了前景，但如何有效融合不同模态信息（尤其当其重要性不平衡或矛盾时）仍然是一个关键挑战。

Method: 本文提出了MM-FusionNet框架，一个利用LVLMs进行鲁棒多模态虚假新闻检测的创新方法。其核心贡献是上下文感知动态融合模块（CADFM），该模块采用双向跨模态注意力机制和新颖的动态模态门控网络，能够根据上下文相关性自适应地学习并分配文本和视觉特征的重要性权重，从而实现信息的智能优先级排序。

Result: MM-FusionNet在包含80,000个样本的大规模多模态虚假新闻数据集（LMFND）上进行了评估，取得了0.938的F1分数，达到了最先进的水平，超过现有主流多模态基线约0.5%，并显著优于单模态方法。进一步分析表明，该模型具有动态加权能力、对模态扰动的鲁棒性以及接近人类水平的性能。

Conclusion: MM-FusionNet框架及其上下文感知动态融合模块在多模态虚假新闻检测方面表现出卓越的实用有效性和可解释性，其性能接近人类水平，为解决真实世界中的虚假新闻问题提供了强有力的解决方案。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [207] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出一种基于DistilBERT的两阶段SQL异常检测方法，结合无监督和有监督机器学习，用于精准识别数据库入侵行为，包括范围外查询和范围内的内部攻击。


<details>
  <summary>Details</summary>
Motivation: 数据库系统面临日益增长的异常访问行为（如内部和外部入侵），现有检测方法缺乏操作粒度，常将整个序列误判为异常，或难以识别伪装成正常活动的复杂异常。

Method: 引入一种两阶段SQL异常检测方法，利用BERT模型（具体为DistilBERT）。第一阶段（无监督）使用集成异常检测器识别偏离正常用户行为模式的嵌入向量（范围外查询）。第二阶段（有监督）使用微调的Transformer模型，通过角色标签分类高精度检测内部攻击（范围内查询），即使在有限标记数据下也能工作。

Result: 该方法能够准确识别异常活动，以高精度检测内部攻击，并最大程度地减少了数据标注的需求。

Conclusion: 该研究为保护关键数据库系统免受复杂威胁提供了一种有效的解决方案。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [208] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig框架通过将用户日志转换为多变量行为频率信号，并结合多尺度分解与双视图建模，显著提升了内部威胁检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测因恶意行为的欺骗性而面临挑战，现有方法将系统日志视为扁平事件序列，未能捕捉用户行为中固有的频率动态和多尺度扰动模式。

Method: Log2Sig框架将用户日志转换为多变量行为频率信号。它使用多变量变分模态分解(MVMD)提取内在模态函数(IMFs)，以揭示多时间尺度行为波动。模型进一步结合Mamba编码器对日常行为序列进行长期依赖建模，并线性投影频率分解信号，将这两种双视图表示融合构建用户行为画像，最终通过多层感知器进行异常检测。

Result: 在CERT r4.2和r5.2数据集上的实验结果表明，Log2Sig在准确率和F1分数上均显著优于现有最先进的基线方法。

Conclusion: Log2Sig通过引入新的用户行为表示和建模多尺度频率动态，有效解决了现有方法的局限性，从而显著提高了内部威胁检测的性能。

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [209] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出MambaITD框架，利用Mamba模型和跨模态自适应融合技术，有效解决了内部威胁检测中时间动态特征建模、计算效率和跨模态信息融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有内部威胁检测方法因时间动态特征建模不足、计算效率和实时性瓶颈以及跨模态信息孤岛问题，无法有效应对日益增长的企业内部威胁。

Method: 本文提出MambaITD框架，基于Mamba状态空间模型和跨模态自适应融合。其包括：多源日志预处理模块，通过行为序列编码、间隔平滑和统计特征提取对异构数据进行对齐；Mamba编码器，用于建模行为和间隔序列中的长距离依赖，并结合门控特征融合机制动态融合序列与统计信息；最后，提出基于最大化类间方差的自适应阈值优化方法，动态调整决策阈值以有效识别异常并缓解类别不平衡和概念漂移。

Result: MambaITD与传统方法相比，在建模效率和特征融合能力方面表现出显著优势，并优于基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了一种更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [210] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: 本文提出NanoCodec，一种12.5 FPS的低帧率音频编解码器，旨在解决LLM处理语音时高帧率导致训练和推理效率低的问题，并实现高质量压缩。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）使用的音频编解码器帧率过高，导致自回归模型的训练和推理速度缓慢。因此，业界对能减少自回归步骤的低帧率音频编解码器产生了日益增长的兴趣。

Method: 研究通过消融研究（ablation studies）探讨了帧率、比特率和因果性对编解码器重建质量的影响。基于研究发现，开发了NanoCodec。

Result: 研究提出了NanoCodec，这是一种先进的音频编解码器，能在仅12.5帧/秒（FPS）的情况下实现高质量压缩。NanoCodec在不同比特率范围内均优于相关现有工作。

Conclusion: NanoCodec为低延迟和高效的语音LLM训练和推理建立了新的性能基准。

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [211] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: 为应对去中心化学习中随机游走（RW）算法面临的“吃豆人”攻击（恶意终止RW），本文提出“平均穿越”（AC）算法。该算法通过复制RW来防止其灭绝，理论和实验证明其能确保RW数量有界和学习过程收敛，并揭示了复制阈值与灭绝概率的相变关系。


<details>
  <summary>Details</summary>
Motivation: 随机游走（RW）算法在分布式系统和去中心化学习中因其低开销和可伸缩性而流行。然而，其对局部交互的依赖使其易受恶意行为攻击。本文关注一种名为“吃豆人”（Pac-Man）的隐蔽性攻击，即恶意节点概率性地终止访问其的RW，从而在不触发警报的情况下逐渐消除网络中的活跃RW并中断学习过程。

Method: 本文提出“平均穿越”（Average Crossing, AC）算法，这是一种完全去中心化的机制，通过复制随机游走（RW）来防止在“吃豆人”攻击下RW的灭绝。

Result: {'理论成果': '(i) 在AC算法下，RW种群几乎必然保持有界；(ii) 基于RW的随机梯度下降在“吃豆人”攻击下，即使存在可量化的偏差，也能在AC算法下保持收敛。', '实验成果': '在合成和真实数据集上的大量实验结果验证了理论发现，并揭示了灭绝概率作为复制阈值函数的相变现象。', '理论洞察': '通过分析AC算法的简化变体，为观察到的相变现象提供了理论解释。'}

Conclusion: AC算法能有效对抗“吃豆人”攻击，确保随机游走在分布式学习中的鲁棒性和收敛性，即使面临恶意攻击也能保持RW种群有界和学习算法收敛。研究还揭示了复制机制中的相变现象，为未来研究提供了深入理解。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [212] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,Léa Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: 本文提出并分析了生存分析的“降维技术”，旨在将生存任务转化为常见的回归或分类任务，以便应用标准机器学习工具，无需定制学习器。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将生存任务转换为标准回归或分类任务，使机器学习和深度学习的通用工具能应用于生存分析，从而简化机器学习在生存分析中的应用，并消除对定制学习器的需求。

Method: 概述并讨论了不同降维技术的优缺点；实现了一些降维技术以使其可直接用于标准机器学习工作流；通过专用示例说明每种技术；进行基准分析，比较其预测性能与现有生存分析的机器学习方法。

Result: 进行了基准分析，比较了所提出的降维技术与现有生存分析机器学习方法的预测性能。

Conclusion: 通过引入降维技术，本工作为利用标准机器学习和深度学习工具解决生存分析任务提供了一种可行且便捷的途径。

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [213] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [214] [Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures](https://arxiv.org/abs/2508.01854)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: physics.comp-ph

TL;DR: 本文提出了一种数据驱动的两阶段学习框架，用于识别具有非梯度分量的广义扩散过程的控制律，通过结合能量耗散和一阶矩演化来恢复伪势和旋转。


<details>
  <summary>Details</summary>
Motivation: 识别具有非梯度分量的广义扩散过程的控制律，特别是伪势和旋转。

Method: 设计了一个两阶段的数据驱动学习框架。该框架结合了能量耗散定律、物理一致性惩罚和一阶矩演化，用于恢复非梯度漂移在点向正交分解中的伪势和旋转。此方法适用于包括噪声数据在内的复杂广义扩散过程。

Result: 代表性的数值实验证明了该方法在学习非梯度广义扩散中物理定律的有效性。

Conclusion: 所提出的数据驱动的两阶段方法能够有效识别复杂非梯度广义扩散过程中的物理控制律（伪势和旋转）。

Abstract: This paper proposes a data-driven learning framework for identifying
governing laws of generalized diffusions with non-gradient components. By
combining energy dissipation laws with a physically consistent penalty and
first-moment evolution, we design a two-stage method to recover the
pseudo-potential and rotation in the pointwise orthogonal decomposition of a
class of non-gradient drifts in generalized diffusions. Our two-stage method is
applied to complex generalized diffusion processes including
dissipation-rotation dynamics, rough pseudo-potentials and noisy data.
Representative numerical experiments demonstrate the effectiveness of our
approach for learning physical laws in non-gradient generalized diffusions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [215] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: 本文提出了一种在本体存在下进行SHACL验证的语义和技术，并通过研究其复杂性发现，即使简单的本体也会显著增加验证的计算开销。


<details>
  <summary>Details</summary>
Motivation: SHACL和OWL是RDF数据管理的W3C标准，分别基于封闭世界和开放世界假设。尽管结合二者具有吸引力，但它们之间的语义鸿沟构成了巨大的语义和计算挑战，亟需解决方案。

Method: 研究人员提出了基于核心通用模型的SHACL验证语义，用于在本体存在下的验证。具体方法包括：1) 提供构建Horn-ALCHIQ本体核心通用模型的技术；2) 利用该模型的有限表示，开发一种重写技术，将带本体的SHACL验证规约到标准验证。

Result: 通过对带本体的SHACL验证复杂性进行研究，发现即使是非常简单的本体也会使问题变为EXPTIME完全，而数据复杂性则为PTIME完全。

Conclusion: 该研究成功地为在本体存在下进行SHACL验证提供了新的语义和可行的技术，有效地弥合了SHACL和OWL之间的语义鸿沟。然而，结果表明引入本体会显著增加SHACL验证的计算复杂度。

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


### [216] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: 本文预览即将发布的一项关于基本交互式算法公理化的工作，并展示如何将各种现代算法（如概率、量子算法）视为带有谕示机的基本算法。


<details>
  <summary>Details</summary>
Motivation: 现代算法概念（如概率算法、量子算法）自1960年代以来已大幅扩展，这促使了更宏大的“物理图灵论题”的提出。原有的算法公理化（针对基本/经典算法）和图灵论题已不足以涵盖这些新的算法类型。

Method: 通过强调逻辑学家理解的图灵论题与“物理图灵论题”的区别，并展示如何将非确定性、概率性算法以及量子电路算法等视为带有适当谕示机的基本算法。

Result: 论文表明，通过引入谕示机，可以将多种现代算法统一地视为基本算法的扩展，这为即将进行的交互式算法公理化提供了基础和新的视角。

Conclusion: 将现代多样化的算法（如概率、量子算法）视为带谕示机的基本算法，提供了一个统一的理解框架，有助于应对扩展的“物理图灵论题”，并为这些算法的进一步公理化奠定基础。

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [217] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: 本文提出了一种名为“Request-Only Optimizations (ROO)”的训练与建模范式，旨在通过将用户请求作为训练数据单元，提高深度学习推荐模型（DLRMs）的存储效率、训练效率和模型质量。


<details>
  <summary>Details</summary>
Motivation: 大规模的深度学习推荐模型（DLRMs）需要处理海量数据和复杂的模型，这导致了存储和训练效率的挑战，并需要新的算法来有效提升模型质量，尤其是在利用用户长期历史信息方面。

Method: 该研究提出了Request-Only Optimizations (ROO)训练和建模范式。通过协同设计数据（request-only data）、基础设施（request-only based data processing pipeline）和模型架构（request-only neural architectures），ROO将用户请求视为训练数据的基本单元，而非传统的用户曝光（impression）。

Result: ROO范式取得了以下成果：1) 在数据记录中实现了原生的特征去重，从而节省了数据存储。2) 通过去重请求中多次曝光的计算和通信，提高了训练效率。3) 使得DLRM能够采用大规模神经网络架构（如生成式推荐器GRs），从而更好地捕获用户兴趣信号，提升模型质量。

Conclusion: ROO训练与建模范式通过重新定义训练数据单元，从用户曝光转变为用户请求，为处理大规模DLRMs的效率和质量挑战提供了一种全面的解决方案，并能支持更先进的神经网络架构。

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [218] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: 该研究提出一种新的GNN架构，通过构建知识图谱和引入查询感知机制，显著提升了RAG在复杂多跳问题上的检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统密集检索方法在处理复杂、多跳问题时，将文档视为独立实体，导致检索精度不足。研究旨在提高检索增强生成（RAG）在需要多文档推理的复杂问题上的检索准确性。

Method: 提出了一种新颖的图神经网络（GNN）架构，用于检索增强生成（RAG）。该方法构建了逐集的知识图谱，捕获文本块之间的序列和语义关系。通过引入具有查询引导池化的增强图注意力网络，利用查询感知注意力机制和学习到的评分头，动态地将焦点放在图谱的相关部分。

Result: 实验结果表明，该方法在复杂问答任务上显著优于标准密集检索器，尤其在需要多文档推理的问题上表现突出。

Conclusion: 该GNN架构通过构建知识图谱和引入查询感知机制，有效提升了RAG在复杂多跳问题上的检索准确性，并能高效地在生产检索系统中实现可扩展部署。

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [219] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: 研究团队在管理非正式和私有知识时面临挑战。本文介绍AquiLLM，一个检索增强生成（RAG）系统，旨在通过支持多种文档类型和隐私设置，帮助研究团队更有效地访问和管理内部知识。


<details>
  <summary>Details</summary>
Motivation: 研究团队在捕获、存储和检索分散在成员之间，尤其是非正式、碎片化或未文档化的隐性知识（如内部邮件、会议记录、培训材料）时面临持续挑战，导致知识难以访问。现有的RAG-LLM系统主要面向公共文档，未能解决内部研究材料的隐私顾虑。

Method: 引入并开发了一个名为AquiLLM的轻量级、模块化检索增强生成（RAG）系统。该系统专为满足研究团队的需求而设计，支持多种文档类型和可配置的隐私设置。

Result: AquiLLM能够使学术团队更有效地访问其内部的正式和非正式知识。它解决了现有RAG-LLM系统在处理私有内部文档时忽视隐私问题和文档类型限制的不足。

Conclusion: AquiLLM作为一个为研究团队量身定制的RAG系统，通过支持多样化文档类型和灵活的隐私设置，为克服内部知识管理挑战并提升知识访问效率提供了有前景的解决方案。

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [220] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: 针对RAG系统评估的挑战，本文提出了OmniBench RAG平台，通过跨多领域和标准化指标，揭示了RAG在不同知识领域中的性能增益和效率差异。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG广泛应用于增强大型语言模型（LLMs），但缺乏可复现、可解释的评估方法，现有方法存在领域覆盖不足、指标粗糙、未考虑计算权衡以及缺乏标准化比较框架等问题，难以衡量RAG的真实性能提升。

Method: 本文引入OmniBench RAG，一个自动化多领域RAG系统评估平台。该平台量化准确性和效率方面的性能增益，涵盖文化、地理、健康等九个知识领域。引入“改进度”（Improvements，准确性增益）和“转换度”（Transformation，RAG前后模型效率差异）两个标准化指标，以实现可复现的跨模型和任务比较。平台特性包括动态测试生成、模块化评估管道和自动化知识库构建。

Result: 评估结果显示RAG的有效性存在显著差异，例如在文化领域表现出显著增益，但在数学领域却有所下降。

Conclusion: 研究强调了进行系统化、领域感知评估对于理解和衡量RAG系统性能的关键重要性。

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [221] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: 开发并验证了一个基于大语言模型（LLM）和检索增强生成（RAG）的户外路线推荐聊天机器人Judy，证明了其在提供准确、有效和可用推荐方面的能力。


<details>
  <summary>Details</summary>
Motivation: 随着户外休闲活动的日益普及，市场对能提供准确、个性化户外路线建议的对话式AI系统需求增加。当前挑战在于如何通过对话式AI提供准确信息，并实现可用且高效的推荐服务。

Method: 本文开发了名为Judy的户外路线推荐聊天机器人，它结合了大语言模型（LLM）与检索增强生成（RAG）技术。研究通过网络数据收集、户外路线数据管理，并以美国康涅狄格州的户外路线为例进行LLM模型性能的案例研究，重点关注基于RAG的推荐效果。

Result: 实验结果表明，Judy在推荐户外路线方面表现出良好的准确性、有效性和可用性，验证了基于LLM与RAG的推荐方法的可行性。

Conclusion: 基于LLM和RAG的户外路线推荐聊天机器人Judy能够有效解决户外路线推荐中的准确性和可用性挑战，为满足用户需求提供了一个有前景的解决方案。

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [222] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: 本文旨在提升IT帮助台系统效率，比较了11种信息检索技术在IT支持工单数据集上的表现。Sentence-BERT表现最佳（78.7%相关性），优于TF-IDF、Word2vec和LDA。研究还开源了数据和代码，并提出了一个新的、反映分析师视角的评估指标。


<details>
  <summary>Details</summary>
Motivation: IT服务部门高度依赖IT帮助台系统来提供历史解决方案，但现有机器学习模型在不同数据集上的性能差异显著。本研究旨在通过寻找最佳的信息检索技术，以提升帮助台系统的效能，从而减轻IT支持分析师的工作负担。

Method: 本研究在IT支持工单数据集上比较了11种信息检索技术。实现了支持工单恢复系统的最小可行原型。公开发布并开源了数据集和关键代码。此外，还提出了一种新的评估指标，旨在更准确地反映IT分析师对检索质量的感知。

Result: Sentence-BERT技术（特别是其多语言变体distilluse-base-multilingual-cased-v1）取得了最佳结果，模型推荐的相关性高达78.7%。TF-IDF（69.0%）、Word2vec（68.7%）和LDA（66.3%）也取得了稳定表现。通过实现原型，证明了支持工单恢复系统的实用性。所用数据集和核心代码已开源。

Conclusion: Sentence-BERT技术在IT支持工单推荐中表现卓越，能显著提高解决方案的相关性，有效提升IT支持分析师的工作效率。本研究通过开源资源和提出新型评估指标，为IT帮助台系统的改进提供了有价值的工具和方法，并验证了增强型支持票据恢复系统的可行性。

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [223] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: 本文提出一种新的数据增强框架及两阶段训练策略，旨在解决对话式推荐系统（CRS）中的假阴性问题，通过利用大型语言模型（LLM）增强数据并平衡语义相关性与协同信息，显著提升了CRS的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 对话式推荐系统（CRS）在训练过程中常遭遇假阴性问题，即用户可能喜欢的物品被错误地标记为负面样本，导致推荐效果不佳。数据增强虽是直观解决方案，但面临如何平衡语义相关性和协同信息的挑战。

Method: 本文提出一个新颖的数据增强框架：首先利用基于LLM的语义检索器识别多样化且语义相关的项目，随后通过相关性评分器过滤掉噪声候选。在此基础上，引入一个两阶段训练策略，以平衡语义相关性与协同信息。

Result: 在两个基准数据集和用户模拟器上的广泛实验表明，该方法在各种推荐器上都取得了显著且持续的性能改进。

Conclusion: 所提出的方法有效提升了对话式推荐系统（CRS）的性能，成功解决了假阴性问题，并通过数据增强和两阶段训练策略平衡了语义相关性和协同信息。

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [224] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: 本文提出一种基于大模型的智能体（agentic）混合RAG系统，能动态选择知识图谱或向量检索，并进行指令微调和不确定性量化，显著提升了科学出版物分析的检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统文献综述方法难以应对海量科学出版物，现有混合RAG系统通常是静态的、依赖专有工具且缺乏不确定性估计，无法满足结构化元数据与全文分析集成需求。

Method: 开发了一个自主智能体，将混合RAG流程封装其中。该系统能根据查询动态选择GraphRAG（将查询转为Cypher，利用Neo4j引文知识图谱）或VectorRAG（结合稀疏和密集检索与重排序，利用FAISS向量存储嵌入的全文PDF）。Llama-3.3-70B智能体负责选择，并结合指令微调（Instruction Tuning）以适应研究者需求，同时通过自举评估量化推断过程中的不确定性。

Result: 在模拟真实查询的合成基准测试中，经DPO优化的指令微调智能体优于基线系统。在VS上下文召回率上提升0.63，整体上下文精确度提升0.56。其他显著提升包括VS忠实度（0.24）、VS精确度及KG答案相关性（0.12）、整体忠实度（0.11）、KG上下文召回率（0.05）以及VS答案相关性和整体精确度（0.04）。

Conclusion: 该系统在异构数据源上展现出改进的推理能力，并为自主、智能体驱动的科学发现建立了一个可扩展的框架，提高了相关性、减少了幻觉并促进了结果的可复现性。

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [225] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: 本文介绍并评估了一个部署在Mercari C2C市场中的可扩展视觉搜索系统，发现零样本视觉语言模型（尤其是SigLIP）表现优异，显著提升了用户参与度和交易转化率。


<details>
  <summary>Details</summary>
Motivation: 在Mercari等消费者对消费者（C2C）市场中，商品列表通常非结构化且以视觉为主。视觉搜索能为顾客提供直观的方式来探索多样化的产品目录，因此需要一个有效且可扩展的视觉搜索系统。

Method: 本文提出了一个部署在Mercari C2C市场的可扩展视觉搜索系统。该系统评估了最新的零样本图像检索视觉语言模型，并将其性能与现有微调基线进行比较。系统整合了实时推理和后台索引工作流，并由一个通过降维优化的统一嵌入管道支持。

Result: 离线评估显示，多语言SigLIP模型在多个检索指标上优于其他模型，相对于基线在nDCG@5上提高了13.3%。为期一周的在线A/B测试进一步证实了实际影响，实验组在参与度和转化率方面获得显著提升，其中通过图像搜索的交易率增加了40.9%。

Conclusion: 研究结果表明，最新的零样本模型可以作为生产环境的强大实用基线，使团队能够以最小的开销部署有效的视觉搜索系统，同时保留根据未来数据或特定领域需求进行微调的灵活性。

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [226] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 本研究评估了多种先进技术（如查询重写、RAG Fusion、意图识别、上下文重排序）以构建电力领域鲁棒的AI客服系统，并比较了向量存储和图基RAG框架，最终选择图基RAG。通过结合意图识别、RAG Fusion和重排序，系统在生成和真实数据集上均表现出色，显著优于基线RAG模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI客服系统在处理歧义、多意图或细节特定查询时表现不足，尤其在电力等复杂领域。因此需要开发更强大的技术来构建稳健的客户支持系统。

Method: 评估了查询重写、RAG Fusion、关键词增强、意图识别和上下文重排序等最新技术。比较了基于向量存储和基于图的RAG框架，并最终选择了基于图的RAG。最终系统结合了意图识别、RAG Fusion和上下文重排序。系统在GPT-4生成数据集和真实电力供应商FAQ数据集上进行了评估。

Result: 基于图的RAG在处理复杂查询方面表现优于基于向量存储的RAG。查询重写改进了非标准术语或需精确细节的查询检索。RAG Fusion通过合并多次检索提升了模糊或多方面查询的性能。重排序通过过滤不相关上下文减少了幻觉。意图识别支持将复杂问题分解为更具针对性的子查询，提高了相关性和效率。相反，关键词增强因偏向性选择对结果产生负面影响。最终系统在GPT-4生成数据集上达到97.9%的准确率，在真实FAQ数据集上达到89.6%的准确率，显著优于基线RAG模型。

Conclusion: 结合意图识别、RAG Fusion和上下文重排序可以有效构建鲁棒的AI客服系统，特别适用于处理歧义和多源查询。图基RAG在复杂查询处理方面表现优异。并非所有增强技术都有效，关键词增强可能产生负面影响。

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [227] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: 本文提出Streaming RAG，一个针对动态数据流的检索增强框架。通过结合多向量筛选、聚类和重击者过滤，它能在内存受限下高效维护原型集，显著提升检索性能和系统吞吐量，并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有静态检索增强生成（RAG）框架难以应对新闻、社交媒体等动态数据流的挑战。全量索引导致高内存成本；周期性重建引入延迟，影响数据新鲜度；简单采样则牺牲语义覆盖率。

Method: 本文提出了Streaming RAG，一个统一的管道，它结合了多向量余弦筛选、小批量聚类和基于计数器的重击者过滤器来维护一个紧凑的原型集。研究还证明了一个近似边界，将检索质量与聚类方差关联起来。此外，采用增量索引更新机制，在不中断查询的情况下刷新原型。

Result: 在八个实时流上的实验表明，Streaming RAG在Recall@10上实现了显著提升（高达3点，p < 0.01），端到端延迟低于15毫秒，在150MB预算下吞吐量超过900文档/秒。与GPT-3.5 Turbo结合进行开放域问答时，在SQuAD数据集上的精确匹配得分提高了3.2点，F1得分提高了2.8点；抽象摘要任务中的ROUGE-L也得到了改进。超参数敏感性分析验证了默认设置。

Conclusion: Streaming RAG为检索增强开辟了新的帕累托前沿，有效解决了动态数据流中RAG框架面临的效率、性能和资源限制问题。

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [228] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文对基于大型语言模型（LLM）的搜索代理进行了首次系统性综述，分析其能力、架构、优化、应用和评估，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）彻底改变了网络搜索，催生了能够进行更深层次、动态和自主信息检索的LLM搜索代理，展现出巨大的应用潜力。鉴于其快速发展，有必要进行首次系统性分析。

Method: 本文采用综述方法，从架构、优化、应用和评估等多个视角，对现有LLM搜索代理工作进行全面分析和分类。

Result: 通过系统分析，本文梳理了LLM搜索代理的现有工作，涵盖其架构设计、优化策略、实际应用场景及评估方法。

Conclusion: 该综述识别了当前LLM搜索代理领域的关键开放挑战，并为该快速发展的领域指明了有前景的未来研究方向。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [229] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 本研究提出一种基于Qwen2.5-VL-7B微调的视觉-语言模型，用于从马来西亚财务报告图像中高精度地提取表格并生成Markdown格式。该模型在自建数据集上通过LoRA微调，并在多种评估指标下，表现显著优于基线模型、大型VLM及专有模型，同时提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 从财务文档中准确提取和表示表格结构，特别是将马来西亚审计报告中的复杂表格（含旋转布局、多级表头和隐式结构线索）转换为Markdown格式，是文档理解领域面临的关键挑战，对监管和分析应用至关重要。

Method: 本研究提出一个基于Qwen2.5-VL-7B微调的视觉-语言模型（VLM），用于从文档图像中生成高保真Markdown。方法包括：构建一个包含2,152对图像-文本的精选数据集并进行数据增强；采用LoRA进行监督微调。性能评估通过双重框架进行：使用基于标准的LLM-as-a-judge评估细粒度准确性，并引入新型Markdown树编辑距离相似度（TEDS）指标评估整体结构保真度。

Result: 该模型在基于标准的评估中取得了92.20%的总体准确率，Markdown TEDS得分达到96.53%。此性能显著超越了其Qwen2.5-VL-7B基础模型、更大规模的VLM和专业推理模型，并大幅缩短了推理时间。此外，其准确性也超过了广泛使用的专有模型，如OpenAI的GPT-4o和Gemini 2.5 Flash。

Conclusion: 研究结果表明，领域特定的微调是连接非结构化财务文档与下游自动化的有效且高效方法，其性能可与规模更大、更通用的模型相媲美，且无需其高昂的计算开销。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [230] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG是一个结合ETL和RAG的框架，用于自动化大规模文献合成并识别方法论研究空白。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有RAG架构的局限性，通过自动化流程实现大规模文献合成和方法论研究空白的发现。

Method: 该框架采用多层方法，包括混合检索（语义搜索、关键词过滤、知识图谱遍历）、智能体自校正框架以及事后引文验证。它通过八个集成阶段处理文献，并构建Neo4j知识图谱和Qdrant向量集合。

Result: 评估显示，结构化字段提取的语义相似度得分比PDF分块方法高35.1%（0.655 vs 0.485）。智能体质保机制单次通过成功率为68.3%，引用准确率为99.0%。系统成功识别了地理空间流行病学文献中的方法论趋势和研究空白。

Conclusion: HySemRAG在加速证据合成和科学发现方面具有广泛的跨科学领域适用性。

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [231] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 为解决大语言模型（LLMs）在推荐系统中的不足，本文构建了一个指令微调数据集ITDR，显著提升了主流开源LLMs在推荐任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理中表现出色，但由于用户行为数据与自然语言的结构差异，它们难以有效建模推荐系统中的用户偏好与物品关联。现有基于提示的方法性能受限，因此需要弥合这一差距。

Method: 研究构建了指令微调数据集ITDR，包含用户-物品交互和用户-物品理解两大核心任务下的7个子任务。该数据集整合了13个公共推荐数据集，并使用人工设计的标准化模板构建，包含约20万个实例。此外，还分析了任务相关性，探讨了任务描述和数据规模对指令微调效果的影响，并与闭源LLMs进行了对比实验。

Result: 实验结果表明，ITDR显著提升了GLM-4、Qwen2.5、Qwen2.5-Instruct和LLaMA-3.2等主流开源LLMs在推荐任务上的性能。

Conclusion: ITDR指令微调数据集成功解决了LLMs在推荐系统应用中的挑战，显著提升了其在该领域的表现。该数据集和微调后的模型已公开，为未来的研究提供了基础。

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [232] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: 针对RAG在领域特定知识上的局限性，本文提出LMAR，一个通过LLM引导数据合成、对比嵌入适应和高效文本聚类提升RAG性能的模型无关框架，实验证明其有效且具成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理领域特定知识时面临挑战，主要源于预训练嵌入性能下降和基于LLM的检索器计算成本高昂。此外，微调数据增强嵌入模型的方法受限于高质量训练数据和可靠的上下文保持切块策略。

Method: 本文提出了LMAR（Language Model Augmented Retriever），一个模型无关的框架，通过结合LLM引导的数据合成、对比嵌入适应和高效文本聚类来解决这些问题。LMAR采用两阶段流程，其中第一阶段是三元组采样和合成数据增强，LLM在此过程中充当标签器和验证器，以确保高保真监督。

Result: LMAR在多个领域特定基准数据集上的实验结果表明，它优于多个基线模型，同时保持适度的硬件要求和低延迟。

Conclusion: LMAR是一个实用且成本效益高的解决方案，可用于可扩展的领域特定适应。其模型无关的特性使其能与新兴RAG架构和文本嵌入模型无缝集成，确保在不重新设计管道的情况下持续改进。

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [233] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: 本文提出一个新颖框架，通过建模电商用户搜索过程中的“过渡查询”并结合大语言模型，提升查询重构效果，优化用户探索体验，在转化率和参与度上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统查询重构方法将查询视为孤立对，未能捕捉用户真实行为中查询的顺序性和过渡动态，限制了电商环境中相关产品的有效发现。

Method: 1. 提出显式建模“过渡查询”（用户购买意图过程中间重构）的新框架。2. 从eBay大规模用户交互日志中挖掘结构化查询轨迹，重建反映意图转变的查询序列。3. 整合生成式大语言模型（LLMs），生成语义多样且保持意图的替代查询，实现可扩展的、意图感知的查询扩展。

Result: 经验评估显示，与现有相关搜索模块相比，该方法在转化率和参与度指标上取得了显著提升。

Conclusion: 该方法成功识别并建模了过渡查询及用户意图流，并有效利用大语言模型进行查询扩展，在真实电商环境中显著增强了用户发现能力和参与度。

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [234] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 提出SoftmaxLoss@K (SL@K) 损失函数，通过分位数技术和光滑上界解决NDCG@K优化的不连续性和Top-K截断问题，实验证明其性能优于现有方法，平均提升6.03%。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中NDCG@K是评估性能的黄金标准，但其不连续性和Top-K截断导致优化困难。现有优化方法常忽略Top-K截断或计算成本高昂、训练不稳定。

Method: 提出SoftmaxLoss@K (SL@K)，一种新型推荐损失函数。利用分位数技术处理Top-K截断，并推导NDCG@K的光滑上界以解决不连续性。SL@K具有理论保证、易实现、计算高效、梯度稳定和噪声鲁棒性。

Result: 在四个真实数据集和三个推荐骨干网络上的广泛实验表明，SL@K的性能优于现有损失函数，平均性能提升达6.03%。

Conclusion: SL@K成功解决了NDCG@K优化中的挑战，提供了一个有效且鲁棒的解决方案，显著提升了推荐系统的性能，是优化NDCG@K的优选损失函数。

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [235] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: 本文分析了算法系统对学术可见度中性别差异的影响，发现虽无明显歧视，但存在微妙的失衡，并呼吁对数字系统进行更全面的公平性评估。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎和信息检索平台等算法系统显著影响学术可见度和知识传播。尽管假设它们是中立的，但这些系统可能复制或强化包括性别在内的社会偏见。

Method: 引入并应用了一种“偏见保留”的算法性别公平性定义，评估算法输出是否反映真实世界的性别分布。使用德国大学的学术档案数据集，分析了元数据完整性、学术数据库中的出版物检索以及Google搜索结果中的性别差异。

Result: 研究未发现明显的算法歧视，但揭示了微妙而持续的不平衡：男性教授与更多搜索结果和更一致的出版记录相关联，而女性教授的数字可见度显示出更高的可变性。

Conclusion: 研究发现的模式反映了平台算法、机构管理和个人自我呈现之间的相互作用。本研究强调了在数字系统中进行公平性评估时，需要同时考虑技术性能和代表性平等。

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [236] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: 本研究对比了领域特异性微调和基于提示学习的两种BIM自然语言接口信息检索方法，并提出了一种结合两者的混合方法，以实现更鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言接口（NLI）在BIM信息检索中潜力巨大，但由于查询复杂性和领域知识特异性，通过自然语言准确提取BIM数据仍面临持续挑战。

Method: 本研究比较了两种NLI开发方法：领域特异性微调和基于大型语言模型（LLMs）的提示学习。采用意图识别和基于表格的问答两阶段框架进行评估，并构建了一个包含1,740个BIM特定查询的数据集。此外，还测试了一种结合意图识别的微调和问答的提示学习的混合配置，并通过案例研究进行验证。

Result: 实验结果显示，领域特异性微调在意图识别任务中表现更优；而基于提示学习（尤其是GPT-4o）在基于表格的问答中表现突出。研究发现，结合意图识别的微调和问答的提示学习的混合配置，能在各项任务中实现更平衡和鲁棒的性能。

Conclusion: 本研究系统分析了两种BIM NLI方法的优缺点，并讨论了NLI在实际BIM场景中的适用性。研究结果为设计智能、语言驱动的BIM系统提供了有价值的见解。

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [237] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: 为解决Pinterest广告排序模型中大型嵌入表训练效果不佳及可扩展性问题，本文提出了一种多方面预训练方案和CPU-GPU混合服务架构，实现了广告性能的显著提升并在生产环境中成功部署。


<details>
  <summary>Details</summary>
Motivation: 在将大型嵌入表整合到Pinterest广告排序模型时，团队面临稀疏性和可扩展性等普遍挑战，更重要的是，从头训练大型嵌入表未能带来积极效果（指标中性）。

Method: 引入了一种新颖的多方面预训练方案，融合多种预训练算法以丰富嵌入表。同时，设计并部署了CPU-GPU混合服务基础设施，旨在克服GPU内存限制并提升系统可扩展性。

Result: 多方面大型嵌入表显著提升了点击率（CTR）和转化率（CVR）。该框架部署于Pinterest广告系统后，实现了1.34%的在线CPC降低和2.60%的CTR提升，同时端到端延迟保持中性。

Conclusion: 通过创新的多方面预训练方法和高效的CPU-GPU混合服务架构，成功克服了在实际广告系统中应用大型嵌入表的挑战，显著优化了广告效果和系统性能。

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [238] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: 针对推荐系统中隐式反馈的噪声问题，本研究提出G-UBS范式，通过群组感知用户行为模拟，实现对隐式反馈的鲁棒解释，并构建了新的多模态评估基准IF-VR，实验证明G-UBS显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 推荐系统需要用户反馈，但显式反馈稀缺。隐式反馈虽有潜力，但常含有噪声（如误操作），导致用户兴趣判断失误，影响推荐效果。

Method: 提出Group-aware User Behavior Simulation (G-UBS) 范式，通过以下两部分实现：1. 用户群组管理器 (UGM) 利用LLM进行“总结-聚类-反思”工作流生成群组画像。2. 用户反馈建模器 (UFM) 采用群组感知的强化学习方法，在群组画像指导下鲁棒地解析隐式反馈原因。为评估，构建了首个视频推荐隐式反馈多模态基准IF-VR。

Result: 在IF-VR基准上，G-UBS的表现显著优于主流LLM和MLLM，其中视频播放率超过30%的比例提高了4.0%，推理准确率提高了14.9%。

Conclusion: G-UBS范式通过利用相关用户群组的上下文指导，有效解决了隐式反馈中的噪声问题，从而能够更鲁棒、深入地解释用户行为，显著提升了推荐性能。同时，本研究提出的IF-VR基准也为未来隐式反馈评估提供了重要资源。

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [239] [Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows](https://arxiv.org/abs/2508.05744)
*Aizhan Akhmetzhanova,Carolina Cuesta-Lazaro,Siddharth Mishra-Sharma*

Main category: astro-ph.CO

TL;DR: 本工作提出了一个结合尺度依赖神经网络统计和归一化流的新框架，通过贝叶斯证据估计来检测宇宙学模拟中的模型错误指定。


<details>
  <summary>Details</summary>
Motivation: 当前和未来的宇宙学巡天会产生大量高维数据，需要高精度正向模拟来建模物理过程和系统效应。然而，验证理论模型是否准确描述观测数据集仍是根本挑战。此外，如何选择既保留所有相关宇宙学信息又降低维度的合适数据表示也是一个复杂问题。

Method: 本文提出了一个新颖的框架，将尺度依赖的神经摘要统计与归一化流相结合，通过贝叶斯证据估计来检测宇宙学模拟中的模型错误指定。通过根据平滑尺度条件化神经网络模型（用于数据压缩和证据估计），该方法能够以数据驱动的方式系统地识别理论模型失效之处。

Result: 该方法首次应用于来自三个CAMELS模拟套件（具有不同子网格物理实现）的物质和气体密度场，展示了其有效性。

Conclusion: 该框架能够以数据驱动的方式系统地识别理论模型在何处失效，为宇宙学模拟的模型验证提供了一种新途径。

Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts
of high-dimensional data, which require complex high-fidelity forward
simulations to accurately model both physical processes and systematic effects
which describe the data generation process. However, validating whether our
theoretical models accurately describe the observed datasets remains a
fundamental challenge. An additional complexity to this task comes from
choosing appropriate representations of the data which retain all the relevant
cosmological information, while reducing the dimensionality of the original
dataset. In this work we present a novel framework combining scale-dependent
neural summary statistics with normalizing flows to detect model
misspecification in cosmological simulations through Bayesian evidence
estimation. By conditioning our neural network models for data compression and
evidence estimation on the smoothing scale, we systematically identify where
theoretical models break down in a data-driven manner. We demonstrate a first
application to our approach using matter and gas density fields from three
CAMELS simulation suites with different subgrid physics implementations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [240] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: 该研究探讨了多模态大语言模型（LLMs）如何通过提供建设性批评来半自动改进现有数据可视化图表，侧重于教育用户优化可视化，并进行了量化评估，提供了一个Web工具。


<details>
  <summary>Details</summary>
Motivation: 制作准确且高效传达信息的图表既是艺术也是科学，但在数据科学课程中通常不教授。鉴于可视化改造（社区反馈改进图表）的实践，研究动机是探究多模态LLMs是否能模拟这项任务，即协助用户根据最佳实践改进其现有数据可视化。

Method: 使用一个预训练的多模态大语言模型，通过提示工程进行优化。输入可以是图表的图像文件或生成代码。模型被预设了可视化最佳实践清单，并结合用户指定的指导和LLM潜在的数据可视化知识，半自动生成建设性批评以改进图表。该方法侧重于教育用户如何根据最佳实践解释来改进现有可视化，而非从原始数据或提示生成新的可视化脚本。研究还对LLM代理对不同图表类型中各种绘图问题的敏感性进行了定量评估。

Result: 研究实施了系统并进行了定量评估，以衡量LLM代理对不同图表类型中各种绘图问题的敏感性。研究成果还包括将该工具作为一个简单的自托管小程序，提供可访问的Web界面。

Conclusion: 该研究表明，多模态LLMs可以通过提供建设性批评来有效协助用户改进现有数据可视化，作为一种教育工具。通过结合提示工程、用户指定指南和LLM的内在知识，该系统能够根据最佳实践解释来优化图表，证明了LLM在该领域的潜力。

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [241] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: 本文提出了一个交互式叙事系统（INS）的形式化表示框架，以解决现有研究碎片化和表示多样性的问题，旨在促进INS的分析、描述和评估。


<details>
  <summary>Details</summary>
Motivation: 交互式叙事系统（INS）虽然革新了数字体验，但该领域的研究面临因研究成果碎片化和系统表示多样性而导致的挑战。

Method: 本文引入了一个形式化的INS表示框架，该框架借鉴了现有技术中的各种方法，提供了一致的词汇和建模结构，旨在促进INS属性的分析、描述和比较。

Result: 通过在“小红帽”场景下的实验验证，证明了所提出的形式化方法的实用性及其对改进INS评估的积极影响。

Conclusion: 本工作旨在通过提供一种形式化表示INS的方法，促进交互式叙事系统研究社区内部的协作和连贯性。

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [242] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 本研究分析了AI伦理原则与用户满意度之间的关系，发现所有伦理维度均与用户满意度正相关，但其影响因用户和产品类型而异，非技术用户和终端应用中伦理AI的影响更显著。


<details>
  <summary>Details</summary>
Motivation: 尽管AI伦理原则被广泛认可，但缺乏从用户视角出发，关于这些原则是否被用户认知、重视或产生影响的实证证据。

Method: 分析了来自G2平台超过10万条AI产品用户评论，使用基于Transformer的语言模型，对欧盟可信赖AI伦理指南定义的七个伦理维度进行情感测量。

Result: 所有七个伦理维度均与用户满意度正相关。这种关系因用户和产品类型而异：技术用户关注系统层面（如透明度、数据治理），非技术用户关注以人为中心（如人类能动性、社会福祉）。此外，非技术用户和终端应用中，伦理AI与用户满意度的关联更强。

Conclusion: 研究强调了从用户角度进行伦理AI设计的重要性，并指出需要考虑用户角色和产品类型之间的情境差异。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [243] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: 一个交互式系统ThematicPlane，通过语义概念平面让用户更直观地控制生成式AI的图像输出，弥补了隐性创意意图与系统控制之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使图像创作更易，但非专业用户难以使输出与细微的创意意图对齐。现有工具需通过提示词或参考图外化想法，限制了流畅探索。

Method: 引入ThematicPlane系统，允许用户在交互式主题设计平面内导航和操纵高层语义概念（如情绪、风格、叙事基调），以此连接隐性创意意图与系统控制。

Result: 探索性研究（N=6）显示，参与者进行了发散性和收敛性创意探索，常将意外结果作为灵感或迭代提示。他们基于熟悉主题进行探索，但对主题如何映射到输出的预期差异表明需要更可解释的控制。

Conclusion: ThematicPlane促进了富有表现力的迭代工作流，并为生成式设计工具中直观、语义驱动的交互指明了新方向。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [244] [Evaluating Universal Machine Learning Force Fields Against Experimental Measurements](https://arxiv.org/abs/2508.05762)
*Sajid Mannan,Vaibhav Bihani,Carmelo Gonzales,Kin Long Kelvin Lee,Nitya Nand Gosvami,Sayan Ranu,Santiago Miret,N M Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出UniFFBench框架，通过对约1500种矿物结构的实验测量，评估了通用机器学习力场（UMLFFs）的真实世界性能，揭示了计算基准与实际应用之间的显著差距，指出当前模型在面对实验复杂性时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 通用机器学习力场（UMLFFs）有望通过实现快速原子模拟革新材料科学，但其评估仅限于可能无法反映真实世界性能的计算基准，因此需要一个更全面、基于实验数据的评估框架。

Method: 开发并使用了UniFFBench框架，通过对照约1500种精心筛选的矿物结构的实验测量数据（涵盖多样化的化学环境、键合类型、结构复杂性和弹性特性），系统性地评估了六种最先进的UMLFFs。

Result: 研究揭示了显著的“现实差距”：在计算基准上表现出色的模型在面对实验复杂性时往往失败。即使是表现最佳的模型，其密度预测误差也高于实际应用所需的阈值。此外，模拟稳定性与机械性能准确性之间存在脱节，预测误差与训练数据表示而非建模方法相关。

Conclusion: 当前计算基准虽然提供了有价值的受控比较，但可能高估了模型在实验复杂化学空间中的可靠性。UniFFBench确立了必要的实验验证标准，并揭示了为实现真正通用力场能力所必须解决的系统性局限。

Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize
materials science by enabling rapid atomistic simulations across the periodic
table. However, their evaluation has been limited to computational benchmarks
that may not reflect real-world performance. Here, we present UniFFBench, a
comprehensive framework for evaluating UMLFFs against experimental measurements
of ~1,500 carefully curated mineral structures spanning diverse chemical
environments, bonding types, structural complexity, and elastic properties. Our
systematic evaluation of six state-of-the-art UMLFFs reveals a substantial
reality gap: models achieving impressive performance on computational
benchmarks often fail when confronted with experimental complexity. Even the
best-performing models exhibit higher density prediction error than the
threshold required for practical applications. Most strikingly, we observe
disconnects between simulation stability and mechanical property accuracy, with
prediction errors correlating with training data representation rather than the
modeling method. These findings demonstrate that while current computational
benchmarks provide valuable controlled comparisons, they may overestimate model
reliability when extrapolated to experimentally complex chemical spaces.
Altogether, UniFFBench establishes essential experimental validation standards
and reveals systematic limitations that must be addressed to achieve truly
universal force field capabilities.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [245] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: 对2020年至2025年5月间关于检索增强生成（RAG）研究文献中引用率最高的128篇论文进行了系统回顾，旨在梳理研究现状、发现方法论上的空白并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在对RAG领域高引用量的文献进行聚焦分析，综合其有效性和局限性的实证证据，以期澄清当前研究格局、突出方法论上的不足，并为未来研究指明优先方向。

Method: 采用PRISMA 2020框架进行系统性文献回顾。从ACM Digital Library、IEEE Xplore、Scopus、ScienceDirect和DBLP等数据库检索了2020年至2025年5月间符合引用标准（对2025年论文降低引用阈值以减轻滞后偏倚）的128篇论文。具体方法包括：制定明确的纳入和排除标准、编目数据集、架构和评估实践，并综合关于RAG有效性和局限性的实证证据。

Result: 本系统性回顾成功澄清了RAG的当前研究格局，突出了该领域存在的方法论差距，并综合了关于RAG有效性和局限性的实证证据。

Conclusion: 本研究为理解RAG领域的现状提供了清晰的视角，揭示了当前研究的方法学缺陷，并为该领域的未来研究确定了优先发展方向。

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [246] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 智能编码系统虽能通过自然语言生成代码，但其决策过程不透明，引发用户信任问题。本文主张系统应生成清晰、一致的理由，并倡导采用神经符号方法来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 智能编码系统通过自然语言实现代码生成，极大地改变了软件开发。然而，AI驱动编码器决策过程的不透明性，尤其对于无法检查底层实现的非专家用户而言，引发了信任和可用性担忧。

Method: 论文提出，智能编码系统不仅应生成代码，还应提供清晰、一致的理由，以弥合模型推理与用户理解之间的鸿沟。为此，论文识别出“认知对齐”和“语义忠实性”两个关键理由属性，并指出现有方法（包括形式化验证、静态分析和事后可解释性）的局限。论文倡导探索神经符号方法来生成理由，该方法通过符号约束在训练期间指导模型行为，并通过神经表示丰富程序语义，从而在推理时实现自动化一致性检查。

Result: 该摘要主要为一篇概念性或立场性论文的概述，侧重于提出问题、论证解决方案的必要性及倡导未来的研究方向。因此，摘要中未直接呈现具体的技术实现或实验结果。

Conclusion: 为解决智能编码系统的不透明性导致的信任和可用性问题，系统应具备生成清晰、一致理由的能力。神经符号方法被提议为一种有前景的途径，以实现认知对齐和语义忠实性，从而提升智能编码系统的可解释性和用户信任度。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [247] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 本研究提出了一个可扩展、可解释且可复现的PySelect框架，通过结合多标准决策（MCDM）原则、经验数据和AI辅助意图建模，支持基于证据的开源软件包选择，解决了现有生成式AI工具推荐的局限性。


<details>
  <summary>Details</summary>
Motivation: 在Python等开源生态系统中，选择第三方软件包极具挑战，因为替代方案众多，且缺乏透明的比较证据。现有的生成式AI工具在推荐时常忽视依赖评估、过度强调流行度并缺乏可复现性，给需要透明性、长期可靠性和可维护性的项目带来风险。

Method: 研究将软件包选择问题建模为多标准决策（MCDM）问题，并提出了一个数据驱动的技术评估框架。该框架通过自动化数据管道持续从GitHub、PyPI和Stack Overflow收集并整合软件元数据、使用趋势、漏洞信息和开发者情绪，并将其结构化为决策模型。该方法在PySelect中实现，PySelect利用大型语言模型（LLM）解释用户意图，并查询模型以识别符合上下文的软件包。

Result: 通过分析来自16,887个GitHub仓库的798,669个Python脚本，并进行基于技术接受模型（TAM）的用户研究，评估了该方法。结果显示，数据提取精度高，推荐质量优于生成式AI基线，且用户对其有用性和易用性给予了积极评价。

Conclusion: 本工作引入了一个可扩展、可解释且可复现的框架，它利用MCDM原则、经验数据和AI辅助意图建模，支持基于证据的软件选择，为开源生态系统中的软件包决策提供了有效的解决方案。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [248] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest框架通过生成高质量测试用例，提升LLM代码强化学习的训练效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码强化学习中需要精确反馈，但高质量测试用例的合成是一个尚未解决的难题。

Method: 提出Klear-CodeTest测试用例合成框架，采用新颖的生成-验证（G-V）框架确保测试用例的正确性和全面性（包含常规与边缘情况），并通过一致性验证机制与黄金标准解比对。同时，设计了多层安全沙盒系统以支持在线验证。

Result: 实验证明，所策划的数据集有效提升了模型性能和训练稳定性。

Conclusion: Klear-CodeTest框架通过提供高质量、高覆盖率的测试用例，有效解决了LLM代码强化学习中测试用例合成的挑战，显著提升了模型的训练效果和性能。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [249] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 本文提出一种混合方法，结合确定性逆向工程与LLM引导的视觉探索，旨在提高开发者对复杂代码库的理解能力。


<details>
  <summary>Details</summary>
Motivation: 开发者在程序理解上耗时巨大，而现有传统工具缺乏交互性、适应性及上下文整合。尽管LLM有潜力，但其缺乏基础和与结构化视图的集成，限制了其有效性。

Method: 研究引入一种混合方法，将确定性逆向工程与LLM引导的、意图感知的视觉探索相结合。该系统整合了基于UML的可视化、动态用户界面、历史上下文和协作功能，通过LLM解释用户查询和交互模式，辅助开发者理解和导航代码。

Result: 为Java实现的系统原型展示了该方法的可行性，能有效帮助开发者导航和理解复杂的代码库。

Conclusion: 该研究为开发与开发者认知及协作工作流相符的智能、交互式代码理解环境奠定了基础。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [250] [A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes](https://arxiv.org/abs/2508.05705)
*Valentina Roquemen-Echeverri,Taisa Kushner,Peter G. Jacobs,Clara Mosquera-Lopez*

Main category: q-bio.QM

TL;DR: 开发了一种生理约束神经网络数字孪生模型，用于准确模拟1型糖尿病患者的血糖动态，并在真实数据上验证了其临床等效性，支持个性化治疗决策。


<details>
  <summary>Details</summary>
Motivation: 现有的1型糖尿病血糖模型未能充分捕捉关键生理特征，且难以进行个体化建模，这限制了其在开发个性化治疗和支持数据驱动临床决策方面的应用。

Method: 引入生理约束神经网络（NN）数字孪生来模拟1型糖尿病的血糖动态。首先，构建一个与描述血糖调节的常微分方程（ODEs）对齐的群体层面NN状态空间模型，并进行形式化验证以确保生理一致性。然后，通过整合个人数据（如血糖管理和上下文信息）来增强该群体模型，从而创建个体化的数字孪生，捕捉个体间和个体内部的变异性。使用来自T1D运动倡议研究的真实数据进行验证，通过配对等效t检验评估模拟血糖曲线与实际观察值在临床相关结果上的相似性。

Result: 对394个数字孪生模型的评估显示，模拟和实际观察到的血糖结果在临床上是等效的。例如，目标范围内时间（70-180 mg/dL）模拟值为75.1%±21.2%（实际为74.4%±15.4%；P<0.001），低于范围时间（<70 mg/dL）模拟值为2.5%±5.2%（实际为3.0%±3.3%；P=0.022）。

Conclusion: 该框架能够进行个性化的计算机模拟治疗测试，支持胰岛素优化，并成功整合了基于物理和数据驱动的建模方法，同时能纳入睡眠和活动等未建模因素，保持关键动态。

Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is
critical for developing personalized treatments and supporting data-driven
clinical decisions. Existing models often miss key physiological aspects and
are difficult to individualize. Here, we introduce physiologically-constrained
neural network (NN) digital twins to simulate glucose dynamics in T1D. To
ensure interpretability and physiological consistency, we first build a
population-level NN state-space model aligned with a set of ordinary
differential equations (ODEs) describing glucose regulation. This model is
formally verified to conform to known T1D dynamics. Digital twins are then
created by augmenting the population model with individual-specific models,
which include personal data, such as glucose management and contextual
information, capturing both inter- and intra-individual variability. We
validate our approach using real-world data from the T1D Exercise Initiative
study. Two weeks of data per participant were split into 5-hour sequences and
simulated glucose profiles were compared to observed ones. Clinically relevant
outcomes were used to assess similarity via paired equivalence t-tests with
predefined clinical equivalence margins. Across 394 digital twins, glucose
outcomes were equivalent between simulated and observed data: time in range
(70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real;
P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022);
and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001).
Our framework can incorporate unmodeled factors like sleep and activity while
preserving key dynamics. This approach enables personalized in silico testing
of treatments, supports insulin optimization, and integrates physics-based and
data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [251] [CLAPP: The CLASS LLM Agent for Pair Programming](https://arxiv.org/abs/2508.05728)
*Santiago Casas,Christian Fidler,Boris Bolliet,Francisco Villaescusa-Navarro,Julien Lesgourgues*

Main category: astro-ph.IM

TL;DR: CLAPP是一个基于LLM的AI助手，旨在为使用Einstein-Boltzmann求解器CLASS的研究人员提供会话式结对编程支持，降低其使用门槛并提高效率。


<details>
  <summary>Details</summary>
Motivation: 支持使用Einstein-Boltzmann求解器CLASS的研究人员；提供会话式编码支持，解决问答、代码生成、调试和绘图等问题；降低科学家接触AI工具的门槛；促进计算和数值宇宙学领域更高效的人机协作。

Method: 引入CLAPP，利用大型语言模型(LLMs)和领域特定检索；其架构结合了多智能体LLM编排、CLASS文档的语义搜索以及实时Python执行环境。

Result: CLAPP成功提供了针对CLASS的会话式编码支持，包括问答、代码生成、错误调试和图表绘制；作为一个用户友好的Web应用程序部署，有效降低了科学家使用AI工具的入门障碍，并实现了更高效的人机协作。

Conclusion: CLAPP是一个成功的AI助手，它通过整合LLMs和领域特定知识，为计算和数值宇宙学领域的CLASS用户提供了强大的支持，提升了研究效率和人机协作水平。

Abstract: We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI
assistant designed to support researchers working with the Einstein-Boltzmann
solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific
retrieval to provide conversational coding support for CLASS-answering
questions, generating code, debugging errors, and producing plots. Its
architecture combines multi-agent LLM orchestration, semantic search across
CLASS documentation, and a live Python execution environment. Deployed as a
user-friendly web application, CLAPP lowers the entry barrier for scientists
unfamiliar with AI tools and enables more productive human-AI collaboration in
computational and numerical cosmology. The app is available at
https://classclapp.streamlit.app

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [252] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: 本报告探讨了LLM多智能体系统部署中新兴的风险，提出了一种识别和分析这些风险的方法论。


<details>
  <summary>Details</summary>
Motivation: 随着组织开始采用基于LLM的AI智能体，部署正从单一智能体向互联的多智能体网络发展。然而，安全智能体的集合并不能保证智能体集合的安全性，因为智能体之间的互动会随着时间产生新兴行为并诱发新的故障模式。这意味着多智能体系统需要与单一智能体截然不同的风险分析方法。

Method: 本报告着重于受控环境中多智能体AI系统的风险识别和早期分析，在组织控制智能体配置和部署的背景下，研究了六种关键故障模式：级联可靠性故障、智能体间通信故障、单一文化崩溃、从众偏差、心智理论缺陷和混合动机动态。针对每种模式，报告提供了一个工具包供实践者扩展或整合到现有框架中进行评估。鉴于当前LLM行为理解的局限性，方法论以分析有效性为核心，主张通过分阶段测试（逐步增加潜在负面影响的暴露）以及通过模拟、观察分析、基准测试和红队演习收集趋同证据来逐步提高有效性。

Result: 报告识别并分析了LLM多智能体系统中的六种关键故障模式，并为每种模式提供了实践者可用的风险评估工具包。同时，提出了一种基于分析有效性的分阶段测试和多源证据收集的方法论，以应对LLM行为理解的局限性。

Conclusion: 所提出的方法论为组织在部署和运营基于LLM的多智能体系统时建立稳健的风险管理奠定了基础。

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [253] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: Grid-Agent是一个结合大型语言模型和多智能体强化学习的AI框架，用于实时检测和修复电网违规，在标准测试系统上表现出卓越性能，特别适用于现代智能电网。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源和电动汽车的普及以及极端天气的增加，电网规划、运行和管理日益复杂。传统基于规则和数值优化的方法难以应对现代电网的规模、动态性和适应性需求。

Method: 本文提出Grid-Agent框架，结合大型语言模型（LLMs）和多智能体强化学习，实现电网违规的实时检测和修复。该框架通过模块化智能体架构（规划智能体生成动作序列，验证智能体评估稳定性）整合语义推理和数值精度，并采用自适应多尺度网络表示以确保可扩展性。它通过优化开关配置、电池部署和负荷削减策略解决违规问题。

Result: 在IEEE和CIGRE标准测试系统（IEEE 69-bus, CIGRE MV, IEEE 30-bus）上的实验结果表明，Grid-Agent具有卓越的违规缓解性能。此外，框架内置的数据收集和学习能力使其能够持续学习并适应多样化的网络拓扑。

Conclusion: Grid-Agent的自主性使其特别适用于需要快速响应动态操作条件的现代智能电网应用。

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>
