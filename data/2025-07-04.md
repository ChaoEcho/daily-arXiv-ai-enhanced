<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 提出了一个多任务中文偏见评估基准（McBE），发现主流大型语言模型（LLMs）普遍存在不同程度的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，其固有的偏见问题逐渐暴露，评估偏见对降低伦理风险至关重要。然而，现有偏见评估数据集主要集中于英语和北美文化，不适用于中文及其他文化，且多为单任务，无法从多角度全面评估LLMs的偏见。

Method: 构建了一个多任务中文偏见评估基准（McBE），包含4,077个评估实例，覆盖12个单一偏见类别、82个子类别和5种评估任务。利用该基准评估了多种不同系列和参数规模的流行LLMs。

Result: 所有被评估的LLMs都展现出不同程度的偏见。

Conclusion: LLMs普遍存在偏见，新提出的McBE基准为中文LLMs的偏见评估提供了全面且多样的工具，并为深入分析LLMs偏见提供了新的视角。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究发现，虽然大型语言模型在摘要任务中有所进展，但其分步推理（CoT）架构在对话摘要中并未持续提升质量，反而可能导致冗长、事实不一致和缺乏简洁性。


<details>
  <summary>Details</summary>
Motivation: 对话摘要具有重要的实用价值但极具挑战性。尽管大语言模型（LLMs）在摘要任务中取得了显著进步，但针对对话场景中需同时兼顾抽象性和简洁性的分步推理（如Long CoT）架构的性能尚未被系统探索。

Method: 本研究首次对最先进的推理和非推理LLMs在通用、角色导向和查询导向三种主要对话摘要范式上进行了全面系统的评估。研究涵盖了多种语言、领域和摘要长度，利用了SAMSum、DialogSum、CSDS和QMSum等强基准测试，并采用了包含基于LLM的自动指标和类人标准的先进评估协议。此外，还通过情景特定分析和详细案例研究，深入探讨了推理失败的原因。

Result: 与在其他推理密集型任务中的趋势相反，研究发现明确的分步推理并不能持续提升对话摘要质量。相反，与非推理模型相比，推理LLMs往往更容易出现冗长、事实不一致和摘要不够简洁的问题。通过分析，论文进一步识别了显式推理在复杂对话语境中可能无法受益甚至阻碍摘要的情况和原因。

Conclusion: 本工作提供了对当前推理LLMs在对话摘要方面局限性的新见解，并强调了针对真实世界对话摘要任务，需要有针对性的建模和评估策略。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究发现深度循环Transformer中潜在思维链（Latent CoT）的可解释性有限，且性能不如显式思维链。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过外部化自然语言实现思维链推理（CoT），但这牺牲了效率。为捕获难以用语言表达的推理，许多研究探索在潜在空间中内化推理（潜在CoT）。本文旨在探讨深度循环Transformer（Huginn-3.5B）中是否能涌现出此类潜在CoT结构。

Method: 研究采用Huginn-3.5B，一个在推理时重复使用层而不增加参数的深度循环Transformer。通过在算术任务上应用Logit Lens和Coda Lens等一系列探测技术，检查模型的内部行为，并追踪最终和中间结果token的排名轨迹。

Result: 研究发现可解释的潜在CoT证据有限。此外，探测结果在循环块之间存在显著的不一致性，隐藏状态的可解释性严重依赖于层索引和解码方法。经验表明，增加循环深度仅带来微不足道的性能提升，远低于显式外部化推理的模型。

Conclusion: 在所研究的深度循环Transformer中，潜在思维链的出现及其可解释性受到限制。其性能提升边际且无法超越显式外部化推理的模型，表明在当前架构下，内部化推理的效率和可解释性仍是挑战。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本文引入了GDC Cohort Copilot，一个基于大型语言模型（LLM）的工具，旨在帮助GDC用户通过自然语言便捷地创建复杂的癌症基因组学队列，并且其自定义LLM模型在性能上优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 尽管基因组数据共享平台（GDC）提供了强大的图形队列构建器，但用户（尤其是新用户）在面对数百个字段和属性时，难以找到特定的队列描述符来创建复杂队列。然而，用户可能更擅长使用自然语言来描述他们所需的队列。

Method: 研究者开发了GDC Cohort Copilot，一个开源的协同工具。该工具能将用户输入的自然语言描述自动转化为GDC队列过滤器，并提供交互式用户界面供用户进一步完善。研究团队开发并评估了多个大型语言模型（LLMs）以支持GDC Cohort Copilot。

Result: GDC Cohort Copilot成功实现了从自然语言描述到GDC队列过滤器的自动生成和导出。实验证明，其本地部署的开源GDC Cohort LLM在生成GDC队列方面，比使用GPT-4o提示的效果更好。

Conclusion: GDC Cohort Copilot提供了一个高效且用户友好的解决方案，使得GDC用户能够通过自然语言更便捷地构建和管理复杂的癌症基因组学队列，从而显著提升了数据访问和分析的效率。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent是一种新型代理工作流，通过分段读取和内存覆盖更新，实现了对超长文本的高效处理，能在百万级上下文任务中保持低性能损失，并在RULER测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法在处理长文本方面有所改进，但处理无限长文档时，如何在保持线性复杂度的同时避免外推性能下降，仍然是长文本处理的终极挑战。

Method: 引入了一种名为MemAgent的新型端到端代理工作流。MemAgent通过分段读取文本并采用覆盖策略更新内存来处理长文本。此外，研究还扩展了DAPO算法，通过独立上下文多对话生成来促进训练。

Result: MemAgent展示了卓越的长上下文处理能力。它能从8K上下文（在32K文本上训练）外推到3.5M的QA任务，且性能损失低于5%。在512K RULER测试中，MemAgent的准确率超过95%。

Conclusion: MemAgent成功解决了长文本处理中的核心挑战，即在处理超长上下文时保持线性复杂度和低性能损失，并通过其创新的工作流和训练方法，在外推任务中取得了显著的性能提升。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本论文提出了DoMIX，一个基于LoRA模块的新方法，旨在解决现有持续领域自适应预训练（continual DAP）方法面临的高计算成本、数据顺序敏感性以及通用模型生成等问题，实现高效、顺序鲁棒且能为特定任务提供定制模型的预训练。


<details>
  <summary>Details</summary>
Motivation: 现有持续领域自适应预训练（continual DAP）方法存在多项局限：1) 训练计算成本高，GPU内存消耗大；2) 对增量数据顺序敏感；3) 为所有最终任务提供单一通用模型，这与DAP定制化的本质相悖。

Method: 本文提出了DoMIX，一种新颖的方法，通过利用LoRA模块（一种参数高效微调PEFT方法）来解决现有问题。该方法实现了高效、并行的领域自适应预训练。

Result: DoMIX方法成功实现了高效且并行的领域自适应预训练，并对领域顺序具有鲁棒性。它能有效利用累积知识，为特定任务提供定制化的预训练模型。此外，该方法还被证明可扩展应用于标准的大型语言模型（LLM）微调场景。

Conclusion: DoMIX通过创新性地结合LoRA模块，成功解决了现有持续DAP方法的关键挑战，提供了一个高效、灵活且能产出定制化模型的解决方案，并展现了其在更广泛LLM微调任务中的潜力。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文描述了SciVQA 2025科学视觉问答任务的系统，该系统采用多模态大语言模型集成和少样本策略，并在盲测中排名第三。


<details>
  <summary>Details</summary>
Motivation: 为SciVQA 2025科学视觉问答共享任务构建并描述一个系统。

Method: 系统采用两个多模态大语言模型的集成，结合多种少样本示例检索策略（根据图表和问题类型选择），并基于模型置信度选择答案。

Result: 在盲测数据上，系统在七个参赛者中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数为85.12。

Conclusion: 所开发的系统在科学视觉问答任务中表现出竞争力，并在SciVQA 2025任务中取得了不错的成绩。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出QFFN-BERT，一种混合量子-经典Transformer模型，用PQC替换传统FFN，显著减少参数并提升数据效率，尤其在少样本学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 参数化量子电路（PQCs）在增强神经网络表达能力方面潜力巨大。Transformer编码器块中，前馈网络（FFN）贡献了约三分之二的参数。现有研究主要将PQC集成到自注意力模块，而本文旨在探索将PQC应用于FFN以解决参数冗余问题，并系统研究PQC深度、表达能力和可训练性之间的权衡。

Method: 引入QFFN-BERT，一个混合量子-经典Transformer，用基于PQC的层替换紧凑BERT变体中的FFN模块。最终的PQC架构融合了残差连接、$R_Y$和$R_Z$旋转以及交替纠缠策略，以确保训练稳定性和高表达能力。实验在经典模拟器上进行，使用SST-2和DBpedia基准数据集，并进行了消融研究。

Result: 精心配置的QFFN-BERT在全数据设置下，准确率最高达到基线的102.0%，超越了经典对应模型，同时将FFN相关参数减少了99%以上。该模型在少样本学习场景中展现出一致且有竞争力的优势，证明其卓越的数据效率潜力。消融研究进一步证实了PQC设计的优化至关重要。

Conclusion: 研究结果表明，当与深度学习基本原理协同设计时，PQC可以作为经典FFN的强大且参数高效的替代方案。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 本文提出一种基于参数模型的代码数据选择方法，旨在通过优化数据质量来提升大语言模型（LLMs）在代码任务上的训练效率和性能，实验证明其在少量数据下能超越基线并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在代码生成和理解方面虽有进步，但主要依赖海量数据，忽视数据质量，导致训练效率低下。

Method: 引入并优化一个参数模型进行代码数据选择。该方法通过优化参数模型，确保所选数据子集的分布一致性和多样性，从而保证数据质量。

Result: 仅使用1万个样本，在HumanEval和MBPP基准测试上分别比使用9.2万个样本的全量基线提高了2.4%和2.3%。该方法在性能和效率上均优于其他采样方法。

Conclusion: 所提出的方法能够有效提升模型性能，并显著降低计算成本。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了Akan语ASR模型在多领域数据集上的泛化能力，发现模型存在域依赖性，且不同架构（如Whisper和Wav2Vec2）有 distinct 的错误行为。


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究主要使用域内数据集评估模型，但很少关注模型在不同语音上下文中的泛化能力，这一差距在低资源语言（LRL）领域尤为显著。

Method: 研究基准测试了七个基于Transformer架构（包括Whisper和Wav2Vec2）的Akan语ASR模型，使用四个涵盖图像描述、对话、圣经朗读和金融对话等不同领域的Akan语料库，通过词错误率（WER）和字符错误率（CER）进行性能评估。

Result: 结果显示模型存在域依赖性，仅在训练域内表现最佳，而在不匹配的领域中准确性显著下降。此外，Whisper模型倾向于生成更流畅但可能误导的错误，而Wav2Vec2则产生更明显但难以解释的输出。

Conclusion: 在低资源语言ASR应用中，选择架构时需权衡ASR错误的易读性和透明度。研究结果强调了Akan语及其他低资源语言对有针对性的域适应技术、自适应路由策略和多语言训练框架的需求。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本研究提出了一种收集障碍语音样本以构建ASR模型的方法，特别关注低资源语言，并以阿坎语为例，提供数据集、工具和初步模型微调结果。


<details>
  <summary>Details</summary>
Motivation: 旨在通过开发社区驱动的数据收集和ASR模型构建的“操作指南”，普及ASR技术和数据收集，以服务于障碍语音和低资源语言群体。

Method: 开发了一套最佳实践“操作指南”和培训。作为概念验证，策划并发布了首个阿坎语障碍语音开源数据集，并招募了多样化的障碍语音参与者。利用该数据集对开源ASR模型进行了微调。

Result: 产出了首个公开可用的阿坎语障碍语音数据集、一份“操作指南”以及一套开源工具。同时，展示了对开源ASR模型进行微调以更好识别阿坎语障碍语音的初步结果。

Conclusion: 本研究通过提供数据集、操作指南和工具，旨在赋能研究人员和实践者，创建针对障碍语音个体独特需求的包容性ASR技术。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 该研究引入了IndianBailJudgments-1200，一个包含1200个印度保释判决的法律NLP数据集，用于解决印度法律NLP数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏结构化数据集，印度等地区的法律NLP发展滞后。

Method: 构建了一个包含1200个印度法院保释判决的新基准数据集IndianBailJudgments-1200，并使用提示工程化的GPT-4o管道进行20多个属性的标注，并验证了一致性。

Result: 发布了一个新的、公开可用的印度保释判例法数据集，它是首个专注于印度保释判例法的此类数据集，支持如结果预测、摘要和公平性分析等广泛的法律NLP任务。

Conclusion: 该数据集填补了印度法律NLP领域的数据空白，为多种法律NLP任务提供了重要资源，有助于推动该领域的发展。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor是一种新的后训练方法，旨在通过生成高不确定性任务和强化学习，使开源大语言模型在复杂信息搜索任务中达到专有代理的超人性能。


<details>
  <summary>Details</summary>
Motivation: 专有代理系统（如DeepResearch）已在复杂信息搜索任务中展现出超人能力，但开源模型普遍缺乏在海量信息中系统性降低极端不确定性的推理模式，这成为大模型训练的前沿瓶颈。

Method: 提出了WebSailor，一个完整的后训练方法，以灌输处理不确定性的能力。其方法包括：1) 通过结构化采样和信息混淆生成新颖的高不确定性任务；2) 采用RFT冷启动；3) 使用高效的代理强化学习训练算法——重复采样策略优化（DUPO）。

Result: WebSailor在复杂信息搜索任务中显著超越了所有开源代理，并能匹配专有代理的性能。

Conclusion: WebSailor成功弥补了开源代理与专有代理在复杂信息搜索能力上的差距，使开源模型也能实现类似超人的表现。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文探讨监督学习中高质量标注数据稀缺的问题，指出当前主动学习（AL）框架常忽略人类标注差异（HLV）这一有价值的信号。作者提出了一个概念框架，旨在将HLV整合到AL循环中，并讨论了大型语言模型（LLM）作为标注者的潜力。


<details>
  <summary>Details</summary>
Motivation: 在应用监督学习中，获取高质量的标注数据是主要限制因素。尽管标注差异（LV）很常见，尤其是在自然语言处理中，但现有标注框架和主动学习（AL）方法往往基于单一“真值”的假设，从而忽视了作为信息信号的人类标注差异（HLV），并导致主动学习的简化假设在实践中难以成立。

Method: 本文首先审视了关于“真值”和“标签性质”的基本假设，强调将观察到的标注差异分解为信号（如HLV）和噪声（如标注错误）的必要性。随后，文章调研了主动学习和（H）LV社区如何处理（或忽视）这些区别，并提出了一个概念框架，旨在将HLV整合到主动学习的整个循环中，包括样本选择、标注者选择和标签表示。此外，还讨论了将大型语言模型（LLM）整合为标注者的可能性。

Result: 文章的主要成果是提出了一个将人类标注差异（HLV）纳入主动学习（AL）过程的*概念框架*，该框架涵盖了从样本选择到标签表示的多个环节，并探讨了LLM作为标注者的集成。

Conclusion: 本研究旨在为“HLV感知的主动学习”奠定概念基础，以期更好地反映真实世界标注的复杂性，从而更有效地利用有限的标注预算。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: Multiperspective Fusion (MPF)是一个新型的后训练对齐框架，旨在通过多视角生成和基线分解，实现大型语言模型中偏见的简易缓解。


<details>
  <summary>Details</summary>
Motivation: 面对大型语言模型（LLMs）中日益增长的偏见缓解需求，尤其需要一种易于操作且高效的方法。

Method: MPF是一个基于SAGED管道的后训练对齐框架。它通过利用多视角生成来揭示和对齐LLM输出中的偏见，使其与细致、类人的基线对齐。具体方法包括将基线（例如，来自HR专业人士的情感分布）分解为可解释的视角组件，并基于这些分解中获得的概率对响应进行采样和平衡，从而指导生成过程。

Result: 实验结果表明，MPF能够有效地将LLM的情感分布与反事实基线（绝对平等）和HR基线（对顶尖大学存在偏见）进行对齐。这导致了较小的KL散度、校准误差的降低，并且能够泛化到未见过的问题。

Conclusion: MPF提供了一种可扩展且可解释的对齐和偏见缓解方法。它与已部署的LLM兼容，并且无需大量的提示工程或微调。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文研究性别与语境偏见的关联，提出新数据集GenderLexicon及量化框架，揭示并解释性别偏见，证实其存在于职业刻板印象之外。


<details>
  <summary>Details</summary>
Motivation: 旨在调查性别与语境偏见（包括动作动词、宾语名词，尤其是职业）之间的相关性。

Method: 引入了新数据集GenderLexicon和一个能够估算语境偏见及相关性别偏见的框架。该模型能用分数解释偏见，提高性别偏见的解释性。通过在五个不同数据集（包括日语数据集）上进行评估来验证方法。

Result: 发现并证实了性别偏见不仅存在于职业刻板印象中，还存在于其他语境元素中。所提出的模型能够有效地量化和解释这些偏见。

Conclusion: 本研究成功揭示了性别在语境中的偏见，并通过新数据集和框架提高了偏见的量化和可解释性，确认了职业刻板印象之外的性别偏见。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 针对同行评审中识别论文局限性的挑战，本文提出了一个局限性分类法和首个评估大语言模型（LLM）此能力的基准LimitGen（包含合成和人类数据集），并通过文献检索增强LLM，使其能提供更具体的反馈。


<details>
  <summary>Details</summary>
Motivation: 科学出版物量的激增加剧了同行评审的挑战，尤其是在识别论文局限性方面。尽管LLM在各种科学任务中展现潜力，但其协助同行评审（特别是识别论文局限性）的能力仍未被充分研究。

Method: 首先提出了一个针对AI领域科学研究局限性类型的综合分类法；其次，构建了首个评估LLM识别局限性能力的综合基准LimitGen，包含合成数据集LimitGen-Syn和真实人工数据集LimitGen-Human；最后，通过文献检索增强LLM系统识别局限性的能力。

Result: 所提出的方法显著增强了LLM系统生成研究论文局限性的能力。

Conclusion: 增强后的LLM系统能够提供更具体和建设性的反馈，从而支持和补充人类同行评审过程。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究测量了人类元音发音次音位控制的准确度，引入“刚好可区分差异”（JPD）概念，即两个元音刺激在听觉空间中产生可靠可区分模仿所需的最小距离。通过元音模仿实验，估计JPD在F1 X F2空间中为14至51mels，对言语产生理论和元音系统结构有重要启示。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明人类元音的产生受听觉目标的控制，且存在次音位水平的控制，但这种控制的准确度尚不清楚。本研究旨在解决这一问题，探究两个元音刺激在听觉空间中需要相距多远才能产生可靠可区分的模仿（即“刚好可区分差异”JPD）。

Method: 研究定义了“刚好可区分差异”（JPD）的概念。采用元音模仿范式，首次测量了两组英语使用者在前元音产生过程中的JPD。

Result: 研究估计，JPD在F1 X F2共振峰空间中介于14至51mels之间。

Conclusion: 该发现对言语产生的记忆理论具有重要启示。通过设定说话者元音共振峰空间中两个元音音素可能接近程度的理论下限，它阐明了人类元音系统的可能结构，并为观察到的元音音素数量和模式的趋势提供了心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）存在“自我纠正盲点”，即无法纠正自身输出中的错误，通过引入“自我纠正基准”测量发现平均64.5%的盲点率，但简单添加“Wait”提示词可显著降低此盲点。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）具有变革性，但它们仍会犯错并可能探索无效的推理路径。对于可信赖的自回归LLM而言，自我纠正是一项重要能力。LLM能够识别用户输入中的错误，但它们表现出一种系统性的“自我纠正盲点”，即无法纠正自己输出中的相同错误。

Method: 为系统性研究这一现象，研究引入了“自我纠正基准”（Self-Correction Bench），这是一个通过在三个复杂级别上注入受控错误来衡量此现象的系统框架。研究测试了14个模型。

Result: 研究发现平均盲点率为64.5%。多项证据表明，这一局限性与训练数据组成有关：人类训练演示主要展示无错误响应，而非错误纠正序列。值得注意的是，简单地附加“Wait”可以将盲点率降低89.3%，这表明相关能力可能存在但需要被激活。

Conclusion: 本研究突出了当前LLM的一个关键局限性，并为提高其可靠性和可信度提供了潜在途径。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本研究利用CLEAR-Bias基准和越狱技术，评估了推理语言模型（RLM）在引入推理能力后对社会偏见的鲁棒性。研究发现，具备显式推理能力（CoT或微调）的RLM反而比基础模型更容易被诱导偏见，尤其CoT模型脆弱性更高，这挑战了推理能提升鲁棒性的传统认知，并强调需设计更具偏见意识的推理方法。


<details>
  <summary>Details</summary>
Motivation: 尽管推理语言模型（RLM）的推理能力有望提升模型可靠性，但其对社会偏见的鲁棒性尚不明确。本研究旨在深入探讨RLM引入推理能力后，在对抗偏见诱导方面的表现。

Method: 研究利用专为大型语言模型（LLM）设计的CLEAR-Bias基准，系统评估了最先进的RLM在不同社会文化维度上的对抗鲁棒性。方法包括采用“LLM即法官”进行自动化安全评分，并运用越狱技术评估内置安全机制的强度。评估侧重于三个方面：推理能力对模型公平性和鲁棒性的影响；微调推理模型与依赖CoT提示模型的安全性差异；以及越狱攻击成功率与所用推理机制的关系。

Result: 研究揭示了推理能力与偏见安全之间复杂的关系。出人意料的是，无论通过CoT提示还是微调推理轨迹，具备显式推理能力的模型普遍比不具备此类机制的基础模型更容易受到偏见诱导，这暗示推理可能无意中为刻板印象的强化提供了新途径。相较于依赖CoT提示的模型，经推理能力增强（如微调）的模型表现出一定的安全性，而CoT模型特别容易受到通过讲故事、虚构角色或奖励指令进行的上下文重构攻击。

Conclusion: 研究结果挑战了推理能力本身能提升鲁棒性的固有假设，并强调了在推理设计过程中采取更具偏见意识方法的必要性。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出MathV-DP数据集和Qwen-VL-DP模型，通过多样化解题路径和强化学习，显著提升多模态大模型在数学推理上的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在数学推理方面依赖单一解题监督，忽视了有效推理视角的多元性和内部反思，限制了其推理能力。

Method: 研究引入了MathV-DP数据集，为每个图像-问题对捕获多个多样化的解题路径。在此基础上，构建了Qwen-VL-DP模型，该模型基于Qwen-VL，通过监督学习和群组相对策略优化（GRPO，一种结合正确性判别和多样性奖励的强化学习方法）进行微调。

Result: 在MathVista的minitest和Math-V基准测试中，Qwen-VL-DP在准确性和生成多样性方面均显著优于现有基础MLLMs。

Conclusion: 研究强调了在多模态数学推理中，结合多样化视角和反思性推理的重要性，有效提升了模型的性能和泛化能力。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [22] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了利用大型语言模型（LLMs）和视觉语言模型（VLMs）进行视频交通事故检测的最新方法。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测在智能交通系统中至关重要。LLMs和VLMs的最新发展革新了多模态信息的处理、推理和总结方式，为解决该问题提供了新途径。

Method: 本文通过文献综述的方式，调查了利用LLMs从视频数据中进行交通事故检测的最新方法。具体包括：提出融合策略的结构化分类法，总结关键数据集，分析模型架构，比较性能基准，并讨论当前挑战与未来机遇。

Result: 本综述呈现了交通事故视频检测领域中LLMs应用的详细概况，涵盖了融合策略分类、关键数据集总结、模型架构分析及性能基准比较，并指出了当前面临的挑战与机遇。

Conclusion: 本综述为视频理解与基础模型这一快速增长的交叉领域中的未来研究奠定了坚实基础。

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [23] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 本研究全面评估了单目度量深度估计模型在水下环境中的性能，发现现有陆地模型表现不佳。为解决此问题，作者在一个基于物理模型生成的合成水下数据上微调了Depth Anything V2，显著提升了水下深度估计的准确性，强调了域适应的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管单目深度估计已能提供度量深度，但在水下环境中，由于光衰减、散射、色彩失真、浑浊以及高质量度量真值数据匮乏等挑战，其可靠性受到限制。现有模型主要在陆地数据上训练，面临显著的域偏移，导致水下性能差。

Method: 1. 对现有零样本和微调的单目度量深度估计模型在FLSea和SQUID等真实水下度量深度数据集上进行了全面基准测试。
2. 使用一个基于物理模型生成的水下版Hypersim合成数据集，对带有ViT-S骨干编码器的Depth Anything V2模型进行了微调。

Result: 1. 发现主要在陆地数据上训练的大规模模型在水下环境中表现不佳，存在显著的域偏移。
2. 经微调后的Depth Anything V2模型在所有基准测试中均持续改善了性能，并且优于仅在陆地Hypersim数据集上训练的基线模型。

Conclusion: 本研究为水下场景中的单目度量深度估计提供了详细评估，强调了域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化度量深度预测的重要性，并为未来的研究指明了方向。

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [24] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出ESTR-CoT，一个基于思维链（CoT）推理的事件流场景文本识别框架，旨在解决现有方法解释性不足和上下文逻辑推理能力弱的问题。该框架结合EVA-CLIP、Vicuna-7B，并构建大规模CoT数据集进行训练，实验证明其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 事件流场景文本识别在低光照、快速运动等极端挑战性场景下优于RGB相机，但现有方法（如端到端编解码器或大型语言模型）仍受限于解释性不足和上下文逻辑推理能力弱的问题。

Method: 本文提出ESTR-CoT框架，采用EVA-CLIP (ViT-G/14) 将事件流转换为视觉token，Llama tokenizer编码生成提示，并使用Q-former将视觉token与预训练的Vicuna-7B大语言模型对齐，以同时输出识别结果和思维链推理过程。该框架可通过端到端监督微调优化。此外，还构建了一个大规模CoT数据集，通过生成、润色和专家验证三阶段处理来训练框架。

Result: 在EventSTR、WordArt*、IC15*三个事件流STR基准数据集上进行的广泛实验，充分验证了所提ESTR-CoT框架的有效性和可解释性。

Conclusion: ESTR-CoT框架通过引入思维链推理，显著提升了事件流场景文本识别的性能和可解释性。所构建的大规模CoT数据集也为后续基于推理的大型模型的发展奠定了坚实的数据基础。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [25] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 本文提出一种新颖的零样本多模态方法，用于复合表情识别（CER），该方法融合了六种异构模态，并在无需特定任务训练数据的情况下，取得了与有监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 复合表情识别（CER）旨在检测复杂的情绪状态。鉴于现有方法过度依赖特定任务的训练数据，本研究旨在开发一种无需领域适应即可捕获复合表情的零样本方法。

Method: 本研究提出了一种零样本多模态CER方法，整合了包括静态/动态面部表情、场景/标签匹配、场景上下文、音频和文本在内的六种异构模态。方法中使用了零样本组件，例如基于CLIP的标签匹配和用于语义场景理解的Qwen-VL。此外，引入了多头概率融合（MHPF）模块以动态加权模态特异性预测，并设计了复合表情（CE）转换模块，通过成对概率聚合（PPA）和成对特征相似度聚合（PFSA）生成可解释的复合情绪输出。

Result: 在多语料库训练和零样本测试下，该方法在AffWild2数据集上F1分数达到46.95%，在AFEW数据集上达到49.02%，在C-EXPR-DB数据集上达到34.85%。这些零样本测试结果与在目标数据上训练的有监督方法结果相当。

Conclusion: 所提出的零样本多模态方法在无需领域适应的情况下，能够有效捕获复合表情，并展现出与有监督方法相媲美的性能。该方法的源代码已公开。

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [26] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 本文提出了SciGA-145k，一个包含14.5万篇科学论文和114万张图片的图形摘要（GA）数据集，旨在支持GA选择、推荐及自动化生成。研究定义了GA推荐的两个任务，并引入了一个新的评估指标（CAR），以促进视觉科学交流和AI在科学领域的应用。


<details>
  <summary>Details</summary>
Motivation: 图形摘要（GAs）在视觉传达科学论文核心发现方面至关重要，但其潜力尚未被充分挖掘。设计有效的GA需要高级可视化技能，这阻碍了其广泛采用。此外，目前研究常将图1作为事实上的GA，但缺乏专门的支持工具和方法。

Method: 研究引入了SciGA-145k，一个包含约14.5万篇科学论文和114万张图片的大规模数据集，专为GA选择、推荐和自动化生成研究设计。定义了两个任务：1) 论文内GA推荐（从给定论文中识别适合作为GA的图），2) 跨论文GA推荐（从其他论文中检索GA以启发新GA的创建）。为这些任务提供了合理的基线模型。此外，提出了一种新颖的推荐度量标准——置信度调整后的top-1真实比例（CAR），以提供模型行为的细粒度分析，解决了传统排名指标的局限性。

Result: 研究成功构建了大规模的SciGA-145k数据集。定义并初步建立了GA推荐的两个核心任务：论文内GA推荐和跨论文GA推荐，并提供了相应的基线模型。提出并验证了新型推荐度量CAR，能够更精确地评估模型在存在多个潜在GA的情况下的表现。

Conclusion: SciGA-145k数据集、定义的任务和提出的指标共同为推进视觉科学交流奠定了基础，并有助于AI在科学领域的进一步发展。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [27] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 本研究探讨了扩散模型中两种条件策略（提示词和布局）对合成数据质量的影响，发现布局条件在数据多样性较高时能显著提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业视觉系统面临高质量训练数据收集困难的挑战，合成数据是潜在解决方案。然而，现有合成数据生成方法耗时且存在模拟-现实差距。扩散模型虽能快速生成图像，但在低数据量下精准控制仍是难题，不同条件策略对合成数据质量的影响尚不明确。

Method: 研究人员从四个标准目标检测基准中选择了八十种多样化的视觉概念，并比较了两种扩散模型条件策略：基于提示词（prompt-based）和基于布局（layout-based）。

Result: 当条件线索范围较窄时，提示词条件生成了更高质量的合成数据；但随着数据多样性增加，布局条件表现更优。当布局线索与完整训练分布匹配时，与仅使用真实数据相比，合成数据使平均精度（mAP）平均提高了34%，最高可达177%。

Conclusion: 针对扩散模型采用合适的条件策略（狭窄范围用提示词，多样性高用布局），可以显著提高合成数据的质量，从而大幅提升目标检测的性能，尤其是在布局线索与训练分布一致的情况下。

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [28] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: 提出DIDB-ViT，通过引入差分信息注意力、频率分解和改进RPReLU激活函数，有效提升二值化ViT在边缘设备上的性能和表示能力。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer（ViTs）计算与存储需求高，难以部署于边缘设备；现有二值化ViT方案性能严重下降或过度依赖全精度模块。

Method: 1. 设计信息丰富的注意力模块，引入差分信息以减轻二值化带来的信息损失并增强高频保留。2. 通过离散Haar小波进行频率分解，整合不同频率间的相似度，以保持二值化Q和K张量间相似性计算的准确性。3. 引入改进的RPReLU激活函数，重构激活分布，扩展模型表示能力。

Result: DIDB-ViT在多种ViT架构上显著优于现有最先进的网络量化方法，并在图像分类和分割任务中实现了卓越性能。

Conclusion: DIDB-ViT通过创新的模块设计（信息注意力、频率分解、改进RPReLU）成功解决了二值化ViT的性能瓶颈，在保持计算效率的同时显著提升了图像分类和分割的性能，使其更适合边缘设备部署。

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [29] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: 本文提出FMOcc，一个基于流匹配选择性状态空间模型的三视角精炼占用网络，旨在解决自动驾驶中少帧3D语义占用预测在遮挡和远距离场景中的精度和效率问题，并在多个数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测在自动驾驶中至关重要。然而，少帧图像的局限性和3D空间的冗余导致对遮挡和远距离场景的预测精度下降。现有方法通过融合历史帧数据来提高性能，但这需要额外的数据和大量的计算资源。

Method: 本文提出了FMOcc，一个基于流匹配选择性状态空间模型的三视角(TPV)精炼占用网络，用于少帧3D占用预测。FMOcc包含：1) Flow Matching SSM模块(FMSSM)，用于生成缺失特征；2) TPV SSM层和Plane Selective SSM (PS3M)，用于选择性过滤TPV特征，减少空体素对非空体素的影响，从而提高模型效率和远距离场景预测能力；3) Mask Training (MT)方法，用于增强FMOcc的鲁棒性并处理传感器数据丢失问题。

Result: 在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，FMOcc优于现有SOTA方法。FMOcc使用两帧输入在Occ3D-nuScenes验证集上取得了43.1% RayIoU和39.8% mIoU，在OpenOcc上取得了42.6% RayIoU，且推理内存为5.4 G，推理时间为330ms。

Conclusion: FMOcc有效解决了少帧3D语义占用预测在遮挡和远距离场景中的挑战，通过创新的模块设计提升了预测精度、效率和鲁棒性，为自动驾驶领域的感知系统提供了先进的解决方案。

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [30] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出了SurgVisAgent，一个基于多模态大语言模型（MLLMs）的智能手术视觉代理，它能动态识别内窥镜图像畸变并执行多种增强任务，并在真实世界模拟基准上超越了传统单任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有手术图像增强算法通常针对特定场景的单一任务设计，在复杂的真实世界手术环境中效果受限。

Method: 本文提出SurgVisAgent，一个基于多模态大语言模型（MLLMs）的端到端智能手术视觉代理。它通过设计先验模型提供领域特定知识，并利用上下文少样本学习和思维链（CoT）推理，动态识别内窥镜图像的畸变类别和严重程度，从而执行低光增强、过曝校正、运动模糊消除和烟雾去除等多种定制化图像增强任务。

Result: 研究构建了一个模拟真实手术畸变的综合基准。在该基准上进行的广泛实验表明，SurgVisAgent的性能优于传统的单任务模型。

Conclusion: SurgVisAgent展现了作为手术辅助统一解决方案的巨大潜力，能够有效应对各种畸变类型和严重程度，满足外科医生的多样化需求。

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [31] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 本研究提出了一种基于航空影像的多标签分类框架，用于评估飓风灾后损伤，其性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 飓风造成的损害类型多样且复杂，传统单标签分类方法难以准确捕捉，而及时准确的损伤评估对于有效的灾害响应至关重要。

Method: 引入了一个新颖的多标签分类框架，该框架结合了基于ResNet的特征提取模块和类别特定注意力机制，以识别单张图像中的多种损害类型。

Result: 使用Rescuenet数据集（源自飓风Michael），该方法实现了90.23%的平均精度（mAP），优于现有基线方法。

Conclusion: 该框架显著提升了飓风灾后损伤评估的效率和准确性，有助于实现更精准的灾害响应，并为未来的灾害缓解和韧性策略提供支持。

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [32] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bi-directional Domain Adaptation (BiDA)的框架，用于解决高光谱图像跨域分类中由于不同场景或时间导致的光谱偏移问题。BiDA通过提取域不变特征和域特定信息，显著提升了跨域分类的性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感技术能够提取精细的地物覆盖类别，但用于训练和测试的图像通常来自不同区域或时间，导致同一类别在不同场景下存在显著的光谱漂移，这给跨域高光谱图像（HSI）分类带来了挑战。

Method: 本文提出了一个双向域适应（BiDA）框架。该框架以带有语义分词器的三分支Transformer架构（源分支、目标分支和耦合分支）为骨干。源分支和目标分支独立学习源域和目标域的自适应空间；耦合分支开发了耦合多头交叉注意力（CMCA）机制，用于特征交互和域间相关性挖掘。此外，设计了一种双向蒸馏损失来指导自适应空间学习，并提出了一种自适应强化策略（ARS）以鼓励模型在噪声条件下关注源场景和目标场景中的特定泛化特征提取。

Result: 在跨时/场景的机载和卫星数据集上的实验结果表明，所提出的BiDA框架显著优于一些最先进的域适应方法。在跨时树种分类任务中，BiDA的性能比最先进的方法高出3%~5%。

Conclusion: BiDA框架通过有效地提取域不变特征和域特定信息，解决了高光谱图像跨域分类中的光谱漂移问题，显著提高了目标场景的适应性和可分性，其性能优于现有先进方法，证明了其在实际应用中的有效性和鲁棒性。

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [33] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出MAC-Lookup模型，结合条件3D查找表和多轴自适应增强，有效解决了水下图像的颜色和能见度问题，并防止过度增强。


<details>
  <summary>Details</summary>
Motivation: 水下图像常因光照变化、水体浑浊和气泡等原因导致能见度低、颜色失真。传统方法和现有深度学习方法在解决这些问题上均存在局限性，缺乏高质量数据集。

Method: 引入Multi-Axis Conditional Lookup (MAC-Lookup) 模型。该模型包含用于初步颜色和质量校正的条件3D查找表颜色校正（CLTCC）模块，以及用于细节细化的多轴自适应增强（MAAE）模块，旨在提高色彩准确性、清晰度和对比度，同时防止过度增强和饱和。

Result: 广泛的实验表明，MAC-Lookup模型在恢复水下图像的细节和颜色方面表现出色，优于现有方法。

Conclusion: MAC-Lookup是一种能够有效增强水下图像视觉质量，恢复细节和色彩的优秀方法。

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [34] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 本文提出一种自蒸馏方法，使视频到音频（V2A）生成模型能理解电影语言，尤其是在目标部分可见场景下，显著提升了生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有V2A方法忽视了电影制作中的关键艺术表达元素——电影语言，导致在拟音目标部分可见时性能下降。

Method: 提出一种简单的自蒸馏方法，通过模拟电影语言变化，使学生模型学习对齐具有相同音视频对应关系的训练对的视频特征，从而捕捉声音与部分视觉信息之间的关联。

Result: 该方法在部分可见场景下，所有评估指标均取得了显著提升，并且在大规模V2A数据集VGGSound上也增强了性能。

Conclusion: 所提出的自蒸馏方法有效扩展了V2A模型对电影语言场景的理解，解决了部分可见性问题，并提升了模型在通用V2A任务上的表现。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [35] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: LaCo是一种在视觉编码器中间层进行视觉token压缩的新框架，通过像素混洗和残差学习，显著提升多模态大语言模型（MLLM）的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有用于MLLM的视觉token压缩方法主要作为编码器后置模块运行，限制了其效率提升的潜力。

Method: 提出LaCo（层级视觉token压缩）框架，在视觉编码器的中间层实现有效token压缩。包含两个核心组件：1) 层级像素混洗机制，通过空间到通道转换系统地合并相邻token；2) 带有非参数快捷方式的残差学习架构，以保留关键视觉信息。

Result: LaCo在视觉编码器中间层进行token压缩时，性能超越所有现有方法。与外部压缩相比，在保持强大性能的同时，训练效率提升超过20%，推理吞吐量提升超过15%。

Conclusion: LaCo是一种有效且高效的视觉token压缩方法，通过在视觉编码器中间层进行操作，显著提升了MLLM的训练和推理效率，并保持了卓越的性能。

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [36] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于VFM的域泛化（DG）新方法。该方法利用大型语言模型（LLM）自动解缠文本提示，并以此引导视觉提示微调以学习域不变性特征。为解决纯文本指导的局限性，引入了最差显式表示对齐（WERA），通过抽象提示和风格化图像增强，进一步提升了视觉特征解缠和域多样性。


<details>
  <summary>Details</summary>
Motivation: 域泛化（DG）旨在开发能在未见目标域上有效运行的通用模型。尽管预训练视觉基础模型（VFMs）在增强泛化能力方面潜力巨大，但在DG中，基于VFM的域提示调优仍面临关键挑战，即如何有效设计提示以解缠跨域不变特征，特别是视觉特征难以解缠且仅依靠文本指导存在局限性。

Method: 本文提出了一种新颖的文本特征引导视觉提示调优框架。该框架首先利用大型语言模型（LLM）自动解缠文本提示，然后通过这些解缠的文本特征引导学习域不变的视觉表示。为解决纯语言指导的局限性，引入了最差显式表示对齐（WERA），它通过引入额外的抽象提示来扩展文本引导的视觉提示，并通过风格化图像增强来提升源域多样性，同时通过对齐约束确保视觉表示在原始和增强分布之间保持一致。

Result: 在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要DG数据集上进行的实验表明，本文提出的方法优于现有最先进的DG方法。

Conclusion: 本文提出的方法通过有效利用解缠的文本特征并引入WERA，成功解决了基于VFM的域泛化中解缠不变特征的挑战，从而在各项DG任务中取得了优异的性能。

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [37] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM提出一种基于少样本学习的方法，通过少量带标注的参考图像指导SAM，在遥感图像上实现无需手动提示的自动、域适应性目标分割，解决了SAM手动提示低效和领域适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 将SAM应用于遥感图像面临两大挑战：1) 手动构建精确提示（如点或框）劳动密集且低效，尤其对于密集小目标或碎片化分布；2) SAM缺乏领域适应性，因主要基于自然图像预训练，难以捕捉遥感特有语义及处理新类别。

Method: 提出ViRefSAM框架，通过少量带标注的参考图像指导SAM，无需手动提示即可自动分割类别一致的对象。该框架在保持SAM原始架构不变的同时，引入两个关键组件：1) Visual Contextual Prompt Encoder：从参考图像中提取类别语义线索，并与目标图像上下文交互生成对象感知提示；2) Dynamic Target Alignment Adapter：集成至SAM的图像编码器，通过注入类别特定语义到目标图像特征中，缓解领域差距并使SAM聚焦于任务相关区域。

Result: 在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准测试中，ViRefSAM仅利用少量参考图像，即可实现对未见类别的准确自动分割。实验结果表明，ViRefSAM在不同数据集上持续优于现有少样本分割方法。

Conclusion: ViRefSAM有效解决了SAM在遥感图像分割中面临的提示构建效率低和领域适应性差的问题，通过创新的组件实现了高效、准确且自动化的少样本遥感图像分割，展现出卓越的泛化能力。

Abstract: The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [38] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++是一个灵活且可扩展的框架，通过引入多视角条件，显著提升了现有视点感知扩散模型生成可控新视角的能力。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练2D扩散模型生成高质量新视角的方法，由于缺乏多视角信息，难以生成可控的新视角。

Method: DreamComposer++框架包含一个视点感知3D提升模块，用于从多视角提取3D表示；一个多视角特征融合模块，用于聚合并渲染这些表示为目标视点的潜在特征；最后将得到的特征集成到预训练图像或视频扩散模型中进行新视角合成。

Result: 实验结果表明，DreamComposer++能与先进的视点感知扩散模型无缝集成，有效增强其从多视角条件生成可控新视角的能力。

Conclusion: 这项进展促进了可控的3D对象重建，并支持广泛的应用场景。

Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


### [39] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: 提出Flow-CDNet网络，旨在同时检测双时相图像中的快速和慢速变化，通过双分支结构实现，并在自建数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测通常关注显著变化，但慢速变化（如潜在灾害前兆）在实际中同样重要。因此，设计一个能够同时检测快慢变化的网络是一个新颖且重要的挑战。

Method: 提出Flow-CDNet网络，包含光学流分支（利用金字塔结构提取多尺度位移变化）和二值变化检测分支（结合ResNet与光学流输出）。此外，构建了Flow-Change数据集，设计了结合二值Tversky损失和L2范数损失的复合损失函数，并提出了新的评估指标FEPE。

Result: 在Flow-Change数据集上的定量实验表明，所提方法优于现有方法。消融实验验证了双分支设计能相互促进，显著提升检测性能。

Conclusion: Flow-CDNet成功解决了同时检测快慢变化这一挑战，通过其创新的双分支结构和新构建的数据集与评估体系，实现了超越现有方法的性能，证明了其在实际应用中的潜力。

Abstract: Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to address this challenge, we propose a change
detection network named Flow-CDNet, consisting of two branches: optical flow
branch and binary change detection branch. The first branch utilizes a pyramid
structure to extract displacement changes at multiple scales. The second one
combines a ResNet-based network with the optical flow branch's output to
generate fast change outputs. Subsequently, to supervise and evaluate this new
change detection framework, a self-built change detection dataset Flow-Change,
a loss function combining binary tversky loss and L2 norm loss, along with a
new evaluation metric called FEPE are designed. Quantitative experiments
conducted on Flow-Change dataset demonstrated that our approach outperforms the
existing methods. Furthermore, ablation experiments verified that the two
branches can promote each other to enhance the detection performance.

</details>


### [40] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: 该工作提出LMPNet，一种通过仅使用类别标签弱监督自动发现语义对象关键点的方法，通过转换网络过滤器实现，并在关键点发现和姿态估计上表现出色，同时保持高度可解释性。


<details>
  <summary>Details</summary>
Motivation: 在仅有类别标签的弱监督下，探索语义对象关键点的自动发现。传统方法可能依赖手工设计的损失项，且缺少可解释性。作者旨在将判别式训练的中间层滤波器转化为关键点检测器。

Method: 1. 提出“leaky max pooling (LMP)”层，促进最终卷积层滤波器学习与对象关键点对齐的“不可重复局部模式”。2. 设计一个选择策略以确保滤波器激活的一致性。3. 应用注意力掩蔽，使网络关注整个对象而非仅判别区域。4. 引入可学习的聚类层将关键点提议分组为最终预测。模型被命名为LMPNet。

Result: LMPNet能够自动发现对对象姿态鲁棒的语义关键点，并取得了与监督姿态估计算法相当的预测精度。模型高度可解释，因为它直接操作网络滤波器来检测预定义概念。

Conclusion: LMPNet是一种有效且高度可解释的模型，能够在仅有类别标签的弱监督下自动发现语义对象关键点，并在关键点发现和姿态估计任务上取得了与监督模型相当的性能。

Abstract: In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed to explicitly encourage final conv-layer
filters to learn "non-repeatable local patterns" that are well aligned with
object keypoints. Informed by visualizations, a simple yet effective selection
strategy is proposed to ensure consistent filter activations and attention
mask-out is then applied to force the network to distribute its attention to
the whole object instead of just the most discriminative region. For the final
keypoint prediction, a learnable clustering layer is proposed to group keypoint
proposals into keypoint predictions. The final model, named LMPNet, is highly
interpretable in that it directly manipulates network filters to detect
predefined concepts. Our experiments show that LMPNet can (i) automatically
discover semantic keypoints that are robust to object pose and (ii) achieves
strong prediction accuracy comparable to a supervised pose estimation model.

</details>


### [41] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 现有脑-视觉解码方法在多语义对象重建上存在不足。本文通过将fMRI信号融入图像特征，证明其能提升目标检测与实例分割精度，揭示fMRI蕴含丰富的多对象语义信息，现有模型尚未充分利用。


<details>
  <summary>Details</summary>
Motivation: 现有脑-视觉解码方法主要依赖像素级对齐，但缺乏足够精细的语义对齐，导致多语义对象重建存在明显失真。本研究旨在更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象。

Method: 开发了一个实验框架，将fMRI表征作为干预条件，通过交叉注意力机制将其注入多尺度图像特征中。比较了在有无fMRI信息的情况下，在目标检测和实例分割任务上的下游性能以及中间特征变化。

Result: 结果表明，整合fMRI信号能够显著提高下游检测和分割任务的准确性。

Conclusion: fMRI数据包含丰富的多对象语义线索和粗粒度空间定位信息，而当前模型尚未充分利用或整合这些元素。整合fMRI信息能有效提升脑-视觉解码模型的性能。

Abstract: Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstruction distortions of multiple
semantic objects. To better understand the brain's visual perception patterns
and how current decoding models process semantic objects, we have developed an
experimental framework that uses fMRI representations as intervention
conditions. By injecting these representations into multi-scale image features
via cross-attention, we compare both downstream performance and intermediate
feature changes on object detection and instance segmentation tasks with and
without fMRI information. Our results demonstrate that incorporating fMRI
signals enhances the accuracy of downstream detection and segmentation,
confirming that fMRI contains rich multi-object semantic cues and coarse
spatial localization information-elements that current models have yet to fully
exploit or integrate.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一个自进化的AI代理，通过动态扩展工具集和优化推理策略，能够自主学习并提升在生物医学领域的表现，超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献的快速增长导致研究领域碎片化，超出了人类的专业能力。现有AI代理依赖静态、手动管理的工具集，限制了其适应性和扩展性。因此，需要一个能够自主适应和扩展的AI代理。

Method: STELLA采用多代理架构，通过两个核心机制自主提升能力：1. 一个不断进化的“模板库”（Template Library）用于推理策略；2. 一个动态的“工具海洋”（Tool Ocean），通过“工具创建代理”（Tool Creation Agent）自动发现和整合新的生物信息学工具。这使得STELLA能够从经验中学习。

Result: STELLA在生物医学基准测试中取得了最先进的准确性，在“人类最后考试：生物医学”中得分约26%，在“LAB-Bench: DBQA”中得分54%，在“LAB-Bench: LitQA”中得分63%，最高超越领先模型6个百分点。更重要的是，其性能随经验系统性地提高，例如，在“人类最后考试”基准上的准确性随着试验次数的增加几乎翻倍。

Conclusion: STELLA代表着AI代理系统在学习和成长方面的一个重大进步，能够动态扩展其专业知识，从而加速生物医学发现的步伐。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [43] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 本文提出HCVR，一种轻量级规则投票特征选择方法，结合参数间（P2P）和参数与目标间（P2T）相关性，通过贪婪的向后消除法有效去除冗余特征。在SPAMBASE数据集上的实验结果表明，HCVR的性能优于多种传统特征选择技术。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种轻量级且高效的特征选择方法，能够结合不同类型的相关性（P2P和P2T），以有效消除冗余特征并保留相关特征，从而提升降维后模型的性能，以弥补传统非迭代和迭代方法的不足。

Method: 本文提出HCVR（Hybrid approach with Correlation-aware Voting Rules），一种基于规则的轻量级特征选择方法。该方法结合了非迭代和迭代过滤方法，采用贪婪的向后消除策略，每一步可能同时消除多个特征。其核心机制是利用参数间和参数与目标间的相关性阈值构建投票规则，并通过多数投票决定特征的保留或丢弃。研究在SPAMBASE数据集上对HCVR进行了应用和评估。

Result: HCVR应用于SPAMBASE数据集的结果显示，与传统的非迭代（如CFS、mRMR和MI）和迭代（如RFE、SFS和遗传算法）特征选择技术相比，HCVR表现出更高的性能。其有效性是根据特征过滤后不同分类器的性能来评估的。

Conclusion: HCVR是一种有效的特征选择方法，它通过结合P2P和P2T相关性的规则投票机制，成功地去除了冗余特征并保留了相关特征。实验证明，HCVR在提升后续分类器性能方面优于现有的多种主流特征选择技术。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [44] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本综述全面回顾了旨在提升大型语言模型（LLMs）推理效率的测试时计算（TTC）策略，提出了一个双层分类法，并分析了性能与计算资源之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在推理时计算效率低下，采用固定的计算量，导致在简单任务上过度消耗资源，而在复杂任务上计算不足。

Method: 本文对高效测试时计算（TTC）策略进行了全面综述，并引入了一个两层分类法（L1-可控性与L2-适应性）。同时，对主流的专有LLMs在多个数据集上进行了基准测试，以分析推理性能与token使用之间的权衡。

Result: 综述强调了测试时计算（TTC）方法在实际控制、适应性和可扩展性方面的关键作用。通过基准测试，揭示了LLMs推理性能与token使用之间的重要权衡。

Conclusion: 测试时计算（TTC）方法对于提升LLMs的计算效率、鲁棒性及用户响应能力至关重要。未来研究应关注混合思维模型等新兴趋势，并解决现有方法面临的关键挑战。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [45] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 本研究引入SciGym基准，通过模拟生物系统评估大型语言模型（LLM）的实验设计和数据分析能力。结果显示，LLM的科学能力，尤其是在系统复杂性增加时，仍有显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM科学能力的方法未能充分测试其核心的实验设计和结果解读能力，因为传统的湿实验室实验成本过高。因此，需要一种经济有效的方法来评估LLM在复杂科学系统中的迭代实验设计和分析能力。

Method: 研究开发了SciGym，一个评估LLM在开放式科学发现任务中迭代实验设计和分析能力的基准。SciGym通过运行生物系统的“干实验室”来克服湿实验室的成本挑战，这些系统模型使用系统生物学标记语言（SBML）编码，能高效生成模拟数据。研究评估了六个前沿LLM在137个小型系统上的表现，并共发布了350个系统。

Result: 评估结果表明，尽管能力更强的LLM模型展现出更优异的性能，但所有模型的表现都随着系统复杂性的增加而显著下降。

Conclusion: LLM代理的科学能力，特别是在处理复杂系统时的表现，仍有巨大的提升空间。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [46] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文探讨AI如何从神经科学中学习持续适应性学习，以应对AI在动态环境中的局限性，并共同推动神经AI领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前的AI模型训练成本高、耗时长且参数固定，难以像动物一样持续适应不断变化的环境，尤其在社交互动和现实世界应用中表现出局限性。研究动机在于探索AI能否从动物的快速适应性学习和行为转变机制中获得启发，以提升其在动态环境下的性能。

Method: 本文是一篇观点（Perspective）文章。作者通过整合AI领域的持续学习和情境学习文献，以及神经科学中关于规则、奖励概率或结果不断变化的学习任务的研究，探讨两者如何相互启发，并提出一个具体的议程，以指导神经科学如何为AI发展提供洞见，以及AI如何反过来促进神经科学研究。

Result: 作为一篇观点文章，本文没有呈现新的实验数据或量化结果。其主要“结果”是提出了一个将神经科学与AI中持续学习和情境学习相结合的框架和议程，明确了神经AI领域未来的研究方向，旨在开发出更具适应性的AI系统。

Conclusion: 神经科学在动物持续适应和快速行为转变方面的洞察，能为AI系统（特别是需要实时适应真实世界动态的应用）提供重要启示。同时，AI的发展也能为神经科学研究带来新的视角。这种跨学科的整合将共同推动神经AI这一新兴领域的发展。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [47] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 本研究利用审计研究数据来训练和评估自动化招聘算法，发现传统公平干预方法存在隐性偏差，并提出了基于个体治疗效应估计的新干预措施以进一步减少算法歧视。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统在复杂决策（如招聘、贷款）中日益普及，但其有效性和公平性评估复杂且关键。机器学习中常用的通过重采样解决偏见的方法，往往依赖便利样本，导致选择偏差和标签偏差。而社会科学中的审计研究能提供高质量歧视数据，本研究旨在探索如何利用此类数据改进AI系统的训练和评估。

Method: 研究调查了如何使用审计研究数据来训练和评估自动化招聘算法。具体而言，它将审计数据应用于现有通用公平干预方法（如通过均衡基础率来抵消差异）的评估，并引入了基于个体治疗效应估计的新干预方法。

Result: 研究发现，当使用审计研究数据进行适当测量时，传统的通过均衡类别基础率来实现公平的常见干预方法，虽然表面上看似公平，但实际上仍存在约10%的歧视差异。此外，基于个体治疗效应估计方法引入的新干预措施能进一步减少算法歧视。

Conclusion: 审计研究提供的高质量数据对于训练和评估自动化招聘算法的公平性至关重要，能揭示传统公平干预措施的局限性。利用个体治疗效应估计等方法可以进一步提升算法的公平性，有助于开发更公正的AI系统。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [48] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究了偏好优化中的数据多样化策略如何提升大型语言模型（LLMs）的数学推理能力。提出了新方法Diversified-ThinkSolve（DTS），通过多样化的推理路径显著提升了LLMs在数学推理任务上的表现，且计算开销较低。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得了进展，但大型语言模型（LLMs）的数学推理能力仍然是一个持续的挑战。

Method: 研究了偏好优化中的数据多样化策略。评估了温度采样、思维链提示和蒙特卡洛树搜索（MCTS）这三种常见数据生成方法。同时，提出了一种名为Diversified-ThinkSolve（DTS）的新颖结构化方法，该方法系统地将问题分解为多样化的推理路径。

Result: 通过策略性多样化的偏好数据，模型能显著提高数学推理性能。最佳方法在GSM8K上比基础模型提升了7.1%，在MATH上提升了4.2%。DTS的计算开销仅为基线的1.03倍，而MCTS成本高近五倍但收益较低。

Conclusion: 研究结果表明，与传统方法相比，结构化探索多样化的问题解决方法能够为数学对齐创建更有效的偏好数据。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [49] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLMs）在角色扮演中，其陈述信念与实际行为之间的一致性，发现存在系统性不一致。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地被用作角色扮演代理以生成人类行为研究的合成数据，确保其输出与分配角色保持一致成为关键问题。本研究旨在探讨LLM角色扮演代理所陈述的信念与其实际行为之间的对应程度。

Method: 研究建立了一个评估框架，以衡量通过提示模型获得的信念预测模拟结果的能力。利用增强版的GenAgents角色库和信任博弈（Trust Game），引入了信念-行为一致性指标。系统地探究了信念类型、信任博弈信息呈现方式和预测时间范围等因素对一致性的影响。此外，还探索了在原始信念与研究目标不符时，施加研究者理论先验的可行性。

Result: 研究结果揭示了LLMs所陈述（或被强加）的信念与角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体来说，即使模型似乎编码了合理的信念，它们也可能无法以一致的方式应用这些信念。

Conclusion: 这些发现强调了识别LLMs陈述信念与模拟行为何时以及如何保持一致的必要性，从而帮助研究人员在行为研究中恰当地使用基于LLM的代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [50] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 本研究使用独立多智能体Q学习算法，探讨稀释和移动性在空间囚徒困境中的影响，发现固定更新规则与学习规则的等效性，以及多动作定义下出现的共生互惠效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明强化学习中的静态智能体能在空间囚徒困境中实现合作。本研究旨在进一步探索稀释和移动性在空间囚徒困境中对合作的影响，并展示独立多智能体Q学习算法在模拟博弈论场景中的通用性和基准潜力。

Method: 采用独立多智能体Q学习算法，研究稀释和移动性在空间囚徒困境中的作用。为该算法定义了多种可能的动作，旨在与经典非强化学习空间囚徒困境的结果建立联系。

Result: 观察到多种效应：固定更新规则的博弈与学习更新规则的博弈在性质上可以等效；当定义多个动作时，群体之间会形成共生互惠效应。

Conclusion: 本研究展示了独立多智能体Q学习算法在模拟复杂博弈论场景中的强大能力，并揭示了稀释和移动性对合作的影响。结果表明，强化学习模型可以产生与经典模型类似的定性行为，且多行动选项有助于促进群体间的互惠合作。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [51] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 为解决LLM规划推理能力受限于数据生成与评估的瓶颈，本文提出NL2FLOW自动化系统，用于生成规划问题并严格评估LLM生成的计划。研究发现，直接规划比引入中间翻译步骤表现更好，且规划成功率受模型和提示设计影响。


<details>
  <summary>Details</summary>
Motivation: 当前，提升大型语言模型（LLM）的规划和推理能力面临可扩展、可靠的数据生成和评估瓶颈。

Method: 引入NL2FLOW，一个全自动化系统，能够参数化生成自然语言、结构化中间表示及PDDL格式的规划问题，并严格评估所生成计划的质量。通过生成2296个自动化工作流规划数据集并评估多个开源LLM来验证其能力。

Result: ['在可行问题上，表现最佳的LLM在生成有效计划方面达到86%的成功率，在生成最优计划方面达到69%的成功率。', '回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。', '将自然语言翻译成JSON表示计划的最高成功率低于直接生成有效计划的最高成功率。']

Conclusion: 不必要地分解推理任务（引入中间翻译步骤）可能降低性能，表明模型直接从自然语言进行推理可能更有效。动态理解LLM推理的局限性并拥有揭示这些限制的工具，对充分发挥LLM作为智能问题解决者的潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [52] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 本文指出信念修正领域缺乏对现有方法的分析，尤其是现有公设侧重于操作约束而非修正机制的可达能力。文章引入“能力”概念来表征修正机制能达到的信念状态，并证明多种现有修正机制拥有或缺乏特定能力。


<details>
  <summary>Details</summary>
Motivation: 信念修正领域现有研究过于依赖公设来语法性地描述修正机制，这些公设仅限定了修正“必须”如何进行，却忽视了修正机制“能够”达到哪些信念状态，导致对现有方法分析不足，无法满足特定信念状态可达的应用需求。

Method: 引入“能力”（如可塑性、等同性、教条性等）来描述信念修正机制能够达到的信念状态。通过逻辑证明，分析并展示了多种现有信念修正机制（如词典式、自然式、激进式等）各自具备或缺乏哪些特定的能力。

Result: 研究结果表明，不同的信念修正机制（如词典式、自然式、限制式、激进式等）拥有各异的能力集合。例如，某些修正机制被证明具备可塑性或教条性等能力，而另一些则缺乏。

Conclusion: 通过以“能力”而非传统“约束”的视角分析信念修正机制，可以更全面地理解和评估现有方法，从而更好地满足各种应用场景中对特定信念状态可达性的需求，填补了该领域方法分析的空白。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [53] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 本文提出OMS框架，旨在克服现有LLM在赞助搜索广告关键词生成中对数据的依赖、缺乏在线多目标优化和质量控制薄弱等局限，通过在线实时、多目标优化和自我反思实现更高效的关键词决策，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 赞助搜索广告中的关键词决策对广告活动成功至关重要。然而，现有基于LLM的关键词生成方法存在三大局限：1) 高度依赖大规模查询-关键词对数据；2) 缺乏在线多目标性能监控和优化；3) 关键词选择的质量控制不足。这些问题阻碍了LLM在关键词决策中实现完全自动化和智能体应用。

Method: 本文提出OMS（On-the-fly, Multi-objective, Self-reflective）关键词生成框架。其核心特点包括：1) On-the-fly（在线实时）：无需训练数据，实时监控在线性能并自适应调整；2) Multi-objective（多目标）：采用智能体推理，基于印象、点击、转化率等多个性能指标优化关键词；3) Self-reflective（自我反思）：智能体自主评估关键词质量。

Result: 在基准测试和真实广告活动的实验中，OMS框架的表现优于现有方法。消融实验和人工评估进一步证实了OMS各个组件的有效性以及所生成关键词的高质量。

Conclusion: OMS框架成功克服了现有LLM关键词生成方法的局限性，通过其在线、多目标优化和自我反思能力，有效提升了赞助搜索广告中关键词决策的自动化水平和性能，为广告活动的成功提供了有力支持。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [54] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 一个AI驱动的自主实验室，专为复杂生物分子工程设计，能独立管理仪器、优化实验流程，并同时服务多个用户，实现与人类专家媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自主实验系统仅限于简单、单一目标任务，未能实现自主进行复杂科学实验并服务非专业人士的愿景。需要一个AI驱动的范式转变，以突破现有局限，实现复杂科学研究的全面自动化。

Method: 本文提出了一个AI原生自主实验室平台，基于模型、实验和仪器协同设计的理念，支持AI模型与自动化系统的共同演进。该系统能够自主管理仪器、制定实验特定流程和优化启发式方法，并支持同时处理多用户请求及复杂、多目标实验。

Result: 该自主实验室在无需人工干预的情况下，能自主优化实验性能，达到人类科学家所能实现的最新水平。它支持核酸合成、转录、扩增和测序等核心功能，并应用于疾病诊断、药物开发和信息存储等领域。在多用户场景下，显著提高了仪器利用率和实验效率。

Conclusion: 该平台为先进生物材料研究克服专家依赖和资源障碍铺平了道路，为大规模实现“科学即服务”模式建立了蓝图，具有广泛的应用前景。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [55] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 该研究通过范畴论重构机器学习模型（以多元线性回归为例），开发了一个语义框架，旨在增强AI的可解释性和可理解性。它形式化了参数和残差间的结构关系，引入了高斯-马尔可夫伴随关系，并提出将此作为AI可解释性的形式基础。


<details>
  <summary>Details</summary>
Motivation: 为响应AI可解释性（Explicability）原则的需求并促进AI的更好社会实施，增强机器学习模型的可理解性和可解释性是一项关键任务。

Method: 该研究通过范畴论的视角重构机器学习模型，从而开发了一个结构化和理解AI系统的语义框架。具体而言，针对最基本的监督学习形式——多元线性回归模型，定义了对应于参数和数据的两个具体范畴，并构建了它们之间的一对伴随函子，从而引入了监督学习的范畴化表述。

Result: 研究发现，该框架的核心结构被称作“高斯-马尔可夫伴随关系”所捕捉。在此设定下，信息双向流动可被明确描述为参数和残差变化之间的对应关系。参数的普通最小二乘估计量和最小残差通过右伴随函子对极限的保持而关联起来。此外，该表述被定位为监督学习的扩展指称语义的一个实例。

Conclusion: 该研究通过提供一个基于范畴论的语义框架，为提升AI系统的可理解性和可解释性做出了贡献。它提出将这种源自理论计算机科学的语义视角，作为AI可解释性的一个形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [56] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，显著增强大型语言模型在Coq定理证明中的推理能力，效果超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 探究提升任务清晰度是否能增强大型语言模型（LLMs）的推理能力，重点关注Coq定理证明。

Method: 引入概念级任务清晰度指标；通过选择性概念展开和规划-执行器架构，为LLM输入添加结构化语义上下文；使用1,386个Coq定理进行评估。

Result: 任务清晰度得分提升1.85倍（44.5%→82.3%）；使用DeepSeek-V3时，证明成功率提升2.1倍（21.8%→45.8%），超越SOTA Graph2Tac（33.2%）；在结构化数据上微调小型模型可达更高性能（48.6%）。

Conclusion: 结构化任务表示对于弥合LLM理解与推理之间的鸿沟具有重要价值。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [57] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本研究专注于提升AI研究代理在自动化机器学习任务上的性能，特别是在Kaggle竞赛基准MLE-bench上。通过系统探索搜索策略和操作子集之间的协同作用，实现了SOTA性能提升。


<details>
  <summary>Details</summary>
Motivation: AI研究代理在自动化机器学习模型的设计、实现和训练方面显示出巨大潜力，本研究旨在提升代理在解决真实世界机器学习问题的挑战性基准MLE-bench上的表现。

Method: 将AI研究代理形式化为通过操作子集迭代修改解决方案的搜索策略。通过设计并系统性地测试不同的操作子集和搜索策略（如贪婪、蒙特卡洛树搜索MCTS、进化算法），以探究它们之间的相互作用对性能的影响。

Result: 研究表明，搜索策略与操作子集之间的相互作用对于实现高性能至关重要。最佳的搜索策略和操作子集组合在MLE-bench lite上取得了最先进（SOTA）的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。

Conclusion: 该研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作子集设计和评估方法的重要性。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [58] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 本文分析了集体决策中责任的“扩散”和“缺失”属性的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 责任长期以来是法律、哲学和人工智能领域的重要研究主题。理解集体决策中责任相关属性的计算特性具有重要意义。

Method: 通过计算复杂性理论，研究了确保责任无扩散和无缺失的集体决策机制集合的计算复杂性。

Result: 研究表明，无扩散决策机制集合是 $\Pi_2$-完全的，无缺失决策机制集合是 $\Pi_3$-完全的。同时，这两类机制的交集是 $\Pi_2$-完全的。

Conclusion: 论文揭示了集体决策中责任的扩散和缺失属性在计算上的固有复杂性，为相关机制的设计与分析提供了理论基础。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [59] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 针对现有LLM医疗决策模拟与真实诊断过程不符的问题，本文提出MIMIC-Patient数据集和DynamiCare多智能体框架，以模拟动态、交互式的临床诊断，并建立了LLM驱动智能体动态临床决策的首个基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的医疗决策模拟框架主要侧重于单轮任务，未能反映真实临床诊断过程中固有的不确定性、交互性和迭代性。

Method: 1. 构建MIMIC-Patient数据集：基于MIMIC-III电子健康记录（EHRs），旨在支持动态、患者层面的模拟。
2. 提出DynamiCare框架：一个新颖的动态多智能体框架，将临床诊断建模为多轮交互循环，其中专家智能体团队迭代查询患者系统、整合新信息并动态调整其组成和策略。

Result: 通过广泛实验证明了DynamiCare框架的可行性和有效性，并成功建立了LLM驱动智能体动态临床决策的首个基准。

Conclusion: DynamiCare框架为模拟更真实的、动态的临床诊断过程提供了一个创新且有效的解决方案，并为基于LLM的医疗智能体在动态决策方面设定了新的研究基准。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [60] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs在迭代囚徒困境中展现出战略智能和独特的行为模式，证明其在竞争环境下具有推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）是否具备战略智能，即在竞争情境中对目标进行推理的能力。

Method: 首次在演化迭代囚徒困境（IPD）锦标赛中，让领先AI公司的LLM代理（OpenAI、Google、Anthropic）与经典策略对抗，并通过调整终止概率增加复杂性。同时，分析了近32,000份模型提供的文字推理。

Result: LLMs表现出高度竞争力并能在复杂生态中生存繁衍。它们展现出独特的“战略指纹”：Google Gemini模型无情，OpenAI模型高度合作但在敌对环境中表现不佳，Anthropic Claude模型最宽容。分析表明，模型主动推理时间范围和对手策略，且此推理对其决策至关重要。

Conclusion: LLMs具备战略推理能力，本研究将经典博弈论与机器心理学结合，为理解不确定性下的算法决策提供了深入视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [61] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出HiRA，一个分层框架，通过将高层规划与专业执行分离，以提高复杂信息检索任务中的RAG和代理系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG和当前基于推理的方法难以有效处理现实世界中复杂的、需要跨源深度推理和知识合成的信息需求，主要因为它们使用单一模型处理高层规划和详细执行，导致效率低下且扩展性有限。

Method: 引入HiRA框架，该框架将战略规划与专业执行分离。它将复杂的搜索任务分解为集中的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果，从而避免执行细节干扰高层推理并利用专业知识。

Result: 在四个复杂的跨模态深度搜索基准测试中，HiRA显著优于最先进的RAG和基于代理的系统，并在答案质量和系统效率上均有提升。

Conclusion: 研究结果突出了分离规划和执行对于多步信息检索任务的有效性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 提出LDSolver，一种可学习、可微分的有限体积求解器，用于在粗网格上高效准确地模拟流体，即使在有限数据下也能超越基线模型，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有流体模拟方法存在局限：经典数值求解器计算成本高昂；机器学习方法虽高效但缺乏可解释性、泛化性且数据依赖性强。因此，亟需一种能在粗网格上高效、准确、泛化性强且对数据需求低的流体模拟方法。

Method: 本文提出LDSolver，一个可学习、可微分的有限体积求解器。它包含两部分：1) 一个可微分的有限体积求解器；2) 一个可学习模块，用于提供通量（导数和插值）的等效近似，以及在粗网格上的时间误差校正。

Result: 实验证明，LDSolver即使在训练数据有限的情况下（如少量轨迹），也能加速模拟并保持高精度，同时展现出卓越的泛化能力。在多种流系统（如Burgers流、衰减流、受迫流和剪切流）上的测试表明，LDSolver实现了当前最佳性能，显著优于基线模型。

Conclusion: LDSolver通过结合可微分求解器与可学习模块，提供了一个高效、精确且泛化能力强的流体模拟解决方案，有效克服了传统方法的高计算成本和纯机器学习方法的局限性。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [63] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为DKGCM的新型图卷积网络，结合基于时间相似度的聚类图卷积（利用DTW和K-means）、FFT与Mamba框架捕获时空依赖，并引入GRPO强化学习策略优化训练，以提升交通需求预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的交通需求预测对于资源有效分配至关重要，然而，交通系统中复杂的时空关系持续限制了现有预测模型的性能，促使研究更精确的时空交通需求预测方法。

Method: 提出DKGCM模型：1. 空间方面，使用基于时间相似度的聚类图卷积方法DK-GCN，通过动态时间规整（DTW）和K-means聚类来分组交通节点，捕获空间依赖。2. 时间方面，在双向Mamba深度学习框架中整合快速傅里叶变换（FFT），以捕获时间依赖。3. 模型优化方面，引入GRPO强化学习策略来增强损失函数反馈机制。

Result: 广泛的实验证明，所提出的DKGCM模型在三个公共数据集上的表现优于多种先进方法，并取得了优异的预测结果。

Conclusion: DKGCM模型通过创新的时空依赖捕获机制和训练优化策略，显著提升了交通需求预测的准确性，为交通管理部门更有效地分配资源提供了有力的工具。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [64] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 本研究利用文本、图像和社交特征的多模态早期融合方法，显著提升了社交媒体虚假信息检测的性能，并分析了其传播模式。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测研究主要集中在文本或图像的单模态方法，对整合文本、图像等多模态特征以构建分类模型的研究较少，因此本研究旨在探索不同多模态特征组合的有效性。

Method: 本研究分析了1,529条来自Twitter（现为X）在COVID-19大流行和选举期间的推文（包含文本和图像）。通过数据丰富过程，利用目标检测和光学字符识别（OCR）等技术提取了额外的社交和视觉特征。研究采用早期融合方法，将文本、图像和社交特征整合，构建分类模型，并结合了无监督和有监督机器学习模型。

Result: 结合无监督和有监督机器学习模型，分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还根据虚假信息推文和传播用户的特征，分析了虚假信息的传播模式。

Conclusion: 多模态特征（文本、图像和社交特征）的早期融合，特别是结合无监督和有监督模型，能显著提高虚假信息检测的性能。这凸显了多模态数据在对抗虚假信息中的重要性，并为理解其传播模式提供了见解。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [65] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 该论文提出了一种基于采样技术和粗糙集理论的新型特征选择方法，旨在解决海量数据下计算资源有限的特征选择挑战，它能在个人电脑上快速有效地找到具有高判别能力的特征子集。


<details>
  <summary>Details</summary>
Motivation: 智能机器在处理海量数据时，通常计算资源不足，导致特征选择面临巨大挑战。

Method: 本方法基于采样技术和粗糙集理论。它提出了一种新的特征集判别能力度量标准，即“可区分对象对占所有应区分对象对的比例”。基于此度量，该方法通过从海量数据中构建保留正区域的样本，来寻找具有高判别能力的特征子集。

Result: 1. 能够在个人电脑上，在可接受的时间内，选择出能保留目标海量数据集所有特征判别能力的特征子集。
2. 在找到约简之前，可以估计所选特征子集在所有应区分对象对中能够区分的对象对的概率下限。
3. 实验证明，可以在很短的时间内找到近似约简，且最终约简的判别能力大于估计的下限。
4. 在大规模数据集上的实验也表明，可以在个人电脑上在合理的时间内获得具有高判别能力的近似约简。

Conclusion: 该方法有效地解决了在有限计算资源下对海量数据进行特征选择的挑战，能够在个人电脑上快速、高效地找到具有高判别能力的近似约简，并且其性能表现超出预估的下限。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [66] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 本研究提出了一种利用机器学习进行多元时间序列（MTS）异常检测的新方法，通过连续小波变换（CWT）将MTS数据转换为图像，并利用微调的VGG-16作为骨干构建孪生网络进行图像对比较，在半导体制造数据上实现了高精度异常识别。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程极其复杂，MTS数据异常预测面临数据高维、类别严重不平衡、数据噪声/缺失、系统非平稳性、变量间复杂依赖以及故障延迟出现等挑战。

Method: ['将多元时间序列（MTS）数据通过连续小波变换（CWT）转换为图像表示。', '通过微调预训练的VGG-16架构，开发一个多类图像分类器。', '构建一个孪生网络，包含两个相同的子网络，均使用微调后的VGG-16作为骨干；该网络接收成对的CWT图像（参考信号和查询信号）作为输入，通过比较两者的嵌入来判断它们在给定时间步是否属于同一类别。']

Result: 该方法在真实的晶圆厂（FAB）过程时间序列数据集上，在识别异常方面表现出高准确性。

Conclusion: 本方法为过程和工具跟踪数据中的离线异常检测提供了一个有前景的解决方案。此外，该方法具有灵活性，可应用于监督和半监督设置。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [67] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出“Temporal Chain of Thought”推断策略，利用VLM自身迭代选择最相关帧以优化输入上下文，显著提升长视频理解与问答性能，尤其对超长视频效果更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）近期取得进展，但长视频理解仍是挑战。现有长上下文VLM（约1000帧）难以有效利用长序列，且易受无关信息干扰。

Method: 提出“Temporal Chain of Thought”推断策略，通过VLM自身迭代识别并提取视频中最相关的帧，将这些精选帧作为输入上下文进行问答。

Result: 通过推理时优化上下文选择可提升准确性；在4个视频问答数据集上取得SOTA结果，并对3种不同VLM显示出一致改进。特别是在无法完全放入模型上下文窗口的超长视频上表现卓越：在LVBench上，对于超过1小时的视频，使用32K上下文窗口的方法比标准推断（700K上下文窗口）性能高出2.8点。

Conclusion: 该方法通过智能筛选输入上下文，有效解决了长视频理解的挑战，并在视频问答任务中取得了最先进的性能，尤其适用于超长视频，这与大型语言模型（LLMs）推理时扩展计算的趋势一致。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [68] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: 针对图卷积网络中稀疏矩阵乘法（SpGEMM）的核外计算，AIRES通过算法-系统协同设计，优化数据对齐和内存传输，显著降低延迟并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大规模图数据处理中，GCNs依赖SpGEMM。由于GPU内存限制，SpGEMM常需进行核外计算。现有核外SpGEMM系统存在高I/O延迟和GPU利用率低的问题，主要瓶颈在于稀疏格式数据对齐和内存分配。

Method: 本文提出了AIRES，一种算法-系统协同设计解决方案。在算法层面，AIRES解决了稀疏矩阵块级别的数据对齐问题，并开发了促进行块级对齐的平铺算法。在系统层面，AIRES采用三阶段动态调度，利用GPU内存、GDS和主机内存组成的分层内存系统进行双向数据传输，以减少I/O延迟并提高吞吐量。

Result: 评估结果表明，AIRES显著优于现有最先进方法，在实际图处理基准测试中，实现了高达1.8倍的延迟降低。

Conclusion: AIRES通过算法和系统层面的协同优化，有效解决了图卷积网络中核外SpGEMM的性能瓶颈，显著提升了大规模图数据处理的效率和性能。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [69] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一个SE(3)等变适配器框架，能高效、灵活地微调几何扩散模型以适应各种几何控制任务，同时保持模型性能并防止遗忘，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在分子动力学和结构生成方面取得显著成功，但针对具有不同几何控制的下游任务进行高效微调仍是未充分探索的领域。

Method: 本文提出了GeoAda，一个SE(3)等变适配器框架，无需修改原始模型架构即可实现灵活且参数高效的微调。GeoAda引入了结构化适配器设计：控制信号通过耦合算子编码，由预训练模型选定层的可训练副本处理，最后通过解耦算子和等变零初始化卷积投射回去。该方法仅微调这些轻量级适配器模块，并理论证明其能保持SE(3)等变性，确保预训练扩散模型的几何归纳偏置不变。

Result: GeoAda在多种几何控制类型（如帧控制、全局控制、子图控制）和广泛应用领域（如粒子动力学、分子动力学、人体运动预测、分子生成）中展现了广泛适用性。经验结果表明，GeoAda实现了最先进的微调性能，同时保持了原始任务的准确性，而其他基线方法则因过拟合和灾难性遗忘而性能显著下降。

Conclusion: GeoAda提供了一种高效且参数高效的微调策略，使得几何扩散模型能灵活适应带有不同几何控制的下游任务。它在保持模型SE(3)等变性的同时，有效解决了过拟合和灾难性遗忘问题，并在多领域展现出卓越性能，超越现有基线。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [70] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 研究对比了LLM和领域特定模型在招聘中的准确性和公平性。结果显示，定制的Match Score模型在两方面均优于通用LLM，强调了在高风险应用中领域建模和偏见审计的重要性。


<details>
  <summary>Details</summary>
Motivation: LLM在招聘中能简化筛选，但缺乏保障时会引发严重的准确性和算法偏见问题。本研究旨在评估并解决这些担忧。

Method: 本文基准测试了OpenAI、Anthropic、Google、Meta、Deepseek等主流LLM，并将其与自研的领域特定招聘模型（Match Score）进行比较。评估指标包括预测准确性（ROC AUC, Precision-Recall AUC, F1-score）和公平性（基于性别、种族及交叉亚组的截止点分析影响比率）。实验数据来自约10,000个真实世界的候选人-职位对。

Result: Match Score在准确性上优于通用LLM（ROC AUC 0.85 vs 0.77），并在人口统计学群体间实现了显著更公平的结果。例如，Match Score的最低种族影响比率为0.957（接近均等），而最佳LLM为0.809或更低；交叉亚组方面，Match Score为0.906，LLM为0.773。

Conclusion: 预训练偏见可能导致LLM在招聘中传播社会偏见，而定制的监督模型能更有效地缓解。研究强调在高风险领域（如招聘）部署AI时，领域特定建模和偏见审计的重要性，并警告不要在缺乏广泛公平性保障的情况下依赖现成LLM。此外，本研究表明招聘中准确性和公平性并非二元对立，精心设计的算法可兼顾两者。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [71] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [72] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 本文提出能量基Transformer（EBTs），一种新型模型，通过无监督学习显式验证输入与预测兼容性，将预测重构为优化问题，实现类似人类系统2思维的学习与推理。EBTs在多模态任务中展现出更快的训练扩展性、显著提升的推理性能和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有类系统2思维的计算技术虽能提升模型性能，但普遍存在模态或问题特异性，或需额外监督/训练的局限。研究动机在于探索是否能将这些系统2思维方法普适化，并使模型仅通过无监督学习就能学会“思考”。

Method: 通过学习显式验证输入和候选预测之间的兼容性，并将预测问题重新定义为相对于该验证器的优化问题。具体而言，训练能量基Transformer（EBTs），它为每个输入和候选预测对分配一个能量值，并通过基于梯度下降的能量最小化直到收敛来完成预测。

Result: 在离散（文本）和连续（视觉）模态中，EBTs在训练时比主流的Transformer++模型扩展速度更快，数据、批大小、参数、FLOPs和深度方面的扩展率提高高达35%。在推理时，EBTs通过系统2思维在语言任务上比Transformer++性能提升29%；在图像去噪上，EBTs优于Diffusion Transformers且前向传播次数更少。此外，在相同或更差的预训练性能下，EBTs在大多数下游任务上均优于现有模型，表明其泛化能力更佳。

Conclusion: EBTs是一种有前景的新范式，能够扩展模型的学习和思考能力，成功地将系统2思维泛化并仅通过无监督学习实现。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [73] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出了PANAMA，一个主动学习框架，用于训练基于WaveNet的端到端参数吉他放大器模型，旨在通过基于梯度的优化以最少的数据点创建虚拟放大器。


<details>
  <summary>Details</summary>
Motivation: 在训练虚拟吉他放大器模型时，需要一种高效的数据采集方法，以最少的数据点（放大器旋钮设置）来创建高质量的虚拟放大器。

Method: 引入了PANAMA主动学习框架，该框架使用类WaveNet架构来构建端到端参数吉他放大器模型。其核心是通过主动学习策略和基于梯度的优化算法来确定最优的采样数据点。

Result: 研究结果表明，基于梯度的优化算法能够有效确定最佳采样数据点，并且该方法在样本数量受限的情况下表现出显著的优势和帮助。

Conclusion: PANAMA框架通过智能地选择训练数据点，为高效训练虚拟吉他放大器模型提供了一种有效途径，大幅减少了所需的数据量，同时保持了模型的性能。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [74] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现在计算资源优化训练下，不同大小神经网络的损失曲线在归一化后呈现普适性坍缩，学习率衰减时甚至出现“超坍缩”现象，这可作为衡量良好缩放的指标，并通过SGD噪声动力学模型和幂律标度定律得到解释。


<details>
  <summary>Details</summary>
Motivation: 探究当模型规模和训练时间同步增长时，何种标度极限支配着神经网络的训练动态。

Method: 通过实证观察不同架构、数据集和学习率计划下模型的损失曲线行为，发现“坍缩”和“超坍缩”现象；通过将“坍缩”与神经网络标度定律中的幂律结构联系起来，并分析一个简单的随机梯度下降（SGD）噪声动力学模型来解释这些现象。

Result: 计算资源优化训练的模型表现出精确的普适性：不同规模模型的损失曲线在训练计算量和最终损失归一化后会坍缩到一条通用曲线上。在学习率衰减下，这种坍缩变得更紧密，达到“超坍缩”，即归一化曲线的差异小于随机种子造成的噪声。这种“超坍缩”现象在多种学习率计划、数据集和架构（包括Transformer）中普遍存在，并在超参数次优缩放时失效，这提供了一个精确实用的良好缩放指标。

Conclusion: 神经网络训练动态在计算优化下呈现显著的普适性（坍缩和超坍缩），这种现象可以通过连接到神经网络的幂律标度定律以及对SGD噪声动力学模型的分析来解释。超坍缩可以作为一个评估模型是否良好缩放的实用指标。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [75] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一个基于LLM的自动化VLSI设计流程调优框架，通过RAG技术利用相似设计知识，显著提升了设计质量并减少了迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计中，EDA算法复杂且参数空间巨大，导致芯片设计优化面临巨大挑战。传统手动参数选择劳动密集且受限于专家经验，效率低下。

Method: 本文提出CROP框架，首次将LLM应用于VLSI设计流程调优。该方法包括：1) 将RTL源代码转化为密集向量表示的可扩展方法；2) 基于嵌入的检索系统，用于匹配语义相似的电路设计；3) 检索增强生成（RAG）LLM引导的参数搜索系统，利用相似设计的先验知识约束搜索过程。

Result: 实验结果表明，CROP在工业设计上比现有方法以更少的迭代次数实现了卓越的设计质量（QoR），包括功耗降低9.9%。

Conclusion: CROP提供了一种有效且高效的自动化解决方案，显著改善了VLSI芯片设计优化过程，克服了传统手动和自动化方法的局限性。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [76] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 本文提出了一种结合变分自编码器和条件扩散模型的潜在扩散框架，通过压缩少量关键帧并进行生成插值，显著提高了生成模型在数据压缩中的控制性、重建精度和存储效率，超越了现有主流压缩方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现出色，可视为数据压缩形式，但其有限的控制性和重建精度限制了其实际应用。

Method: 提出一种高效的潜在扩散框架，结合变分自编码器（VAE）与条件扩散模型。该方法仅将少量关键帧压缩到潜在空间，并将其作为条件输入，通过生成插值重建剩余帧，无需为每帧存储潜在表示。

Result: 实现了精确的时空重建，显著降低了存储成本。相比基于规则的先进压缩器（如SZ3），压缩比提高了10倍；在相同重建误差下，比领先的基于学习的方法性能提升高达63%。

Conclusion: 所提出的潜在扩散框架有效弥合了生成模型在数据压缩实际应用中的空白，在压缩比和重建精度方面均表现出卓越性能。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [77] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出NCPNET，一个用于时间图的共形预测框架，通过引入扩散非一致性分数和效率优化算法，解决了现有方法在动态图上因时间依赖性导致的覆盖率违规问题，并显著提升了预测效率。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）的共形预测方法主要针对静态图，忽略了真实世界图的动态演变特性。时间图中的结构、节点属性和标签的时间依赖性违反了标准共形预测的交换性假设，限制了其适用性。

Method: 本文提出了NCPNET，一个为时间图量身定制的端到端共形预测框架。该方法将共形预测扩展到动态设置，以缓解时间依赖性引起的统计覆盖率违规。具体地，NCPNET提出了一种扩散（diffusion-based）非一致性分数，用于捕捉演化网络中的拓扑和时间不确定性，并开发了一种效率感知（efficiency-aware）优化算法，以提高计算效率和减少覆盖率违规。

Result: 在WIKI、REDDIT、DBLP和IBM反洗钱数据集等多个真实世界时间图上的广泛实验表明，NCPNET能够确保时间图中的覆盖率，并且在WIKI数据集上预测集大小减少了高达31%，与现有最先进的方法相比显著提高了效率。

Conclusion: NCPNET成功地将共形预测扩展到时间图领域，有效处理了时间依赖性问题，确保了预测的可靠性和覆盖率，并显著提升了计算效率，为高风险应用中的GNNs提供了更可靠的不确定性量化能力。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [78] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 提出一种形式化验证程序，用于评估机器学习模型对用户输入变化的响应能力，以提高模型在关键应用中的安全性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在对人进行预测时（如贷款、招聘、内容审核），常因未考虑个体如何改变输入而导致安全故障。

Method: 引入一种形式化的预测响应能力验证程序，将其视为一种敏感性分析。该程序通过黑盒访问，指定干预约束和下游效应分布，生成可达点的均匀样本来估计任意模型和数据集的响应能力，并支持伪造检测和故障概率估计。

Result: 该方法能够对任何模型和数据集的预测响应能力进行黑盒估计。通过在实际应用（如累犯预测、器官移植优先级、内容审核）中进行演示，表明其能有效提升模型安全性。

Conclusion: 该验证程序通过考虑个体输入变化，有效解决了机器学习模型中的安全故障问题，显著提升了模型在关键现实应用中的安全性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [79] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 针对降维（DR）评估中由于指标高度相关导致的偏差问题，本文提出一种新颖的工作流，通过基于经验相关性聚类指标来减少偏差。


<details>
  <summary>Details</summary>
Motivation: 评估降维（DR）投影保留高维数据结构的准确性对于可靠的视觉分析至关重要。然而，如果无意中选择了高度相关的评估指标（测量相似结构特征的指标），DR投影的评估可能会产生偏差，从而偏袒某些DR技术。

Method: 提出一种新的工作流，通过计算指标的成对相关性来衡量相似度，然后对指标进行聚类以最小化重叠，并从每个簇中选择一个代表性指标，从而基于经验相关性而非设计特性来减少评估指标选择中的偏差。

Result: 定量实验表明，该方法提高了DR评估的稳定性。

Conclusion: 该工作流有助于减轻DR评估中的偏差。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [80] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect是一个免训练的校正框架，通过基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性，显著减少神经网络PDE求解器长期模拟中的误差积累，同时保持极低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 神经网络在解决偏微分方程(PDEs)方面展现出显著的速度优势，但长期模拟中误差会指数级累积，导致预测结果偏离物理有效解，从而限制了其实际应用。

Method: 提出PhysicsCorrect框架，它是一个免训练的校正框架。通过将校正表述为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。关键创新在于一种高效缓存策略，在离线预热阶段预计算雅可比矩阵及其伪逆，将计算开销降低了两个数量级。

Result: 在Navier-Stokes流体动力学、波动方程和Kuramoto-Sivashinsky方程这三个代表性PDE系统中，PhysicsCorrect将预测误差减少了高达100倍，同时推理时间开销可忽略不计（低于5%）。该框架可与傅立叶神经算子、UNet和Vision Transformer等多种神经网络架构无缝集成。

Conclusion: PhysicsCorrect有效地将不稳定的神经网络代理模型转变为可靠的模拟工具，弥补了深度学习计算效率与实际科学应用所需的物理保真度之间的鸿沟。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [81] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 本文提出VERBA，一种利用大型语言模型生成机器学习模型间行为差异描述的方法，旨在解决模型泛滥问题并提升模型可比性。实验证明其能有效识别并描述模型差异，尤其是在加入结构信息后准确率更高。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域面临“模型湖”现象，即针对同一任务存在大量性能相似但行为不同的模型。用户选择模型时需要对比文档，但手动进行O(N^2)的模型对比较成本过高。因此，需要一种方法来促进模型间的细粒度成对比较。

Method: 引入了VERBA方法。该方法利用大型语言模型（LLM）通过从两个模型中采样来生成模型差异的语言描述。研究建立了一个协议，通过模拟评估这些语言描述的信息量。同时，还构建了一个包含多种常用机器学习模型的基准套件进行测试。

Result: 对于一对决策树模型，即使性能差异仅为5%但行为差异达20-25%，VERBA也能有效地描述其变异，总体准确率高达80%。当包含模型的结构信息时，描述的准确率进一步提升至90%。

Conclusion: VERBA为事后（post-hoc）提高机器学习模型的透明度和可比性开辟了新的研究途径。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [82] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成技术，旨在解决真实数据面临的稀缺性、隐私和纯度问题，重点关注深度学习方法，并探讨该领域挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 真实网络数据在数据稀缺性、隐私保护和数据纯度方面存在挑战，限制了数据驱动应用的发展。合成网络流量生成作为一种有前景的替代方案，能够生成保留真实世界特征的数据，从而解决这些问题。本综述的动机是全面审视并分析该领域现有方法和未来发展。

Method: 本文采用综述研究方法，全面回顾了合成网络流量生成领域，具体涵盖数据类型、生成模型和评估方法。特别侧重于基于深度学习的技术，并详细讨论了统计方法及其扩展，包括现有商用工具。此外，还识别并讨论了该领域的开放挑战和潜在未来研究方向。

Result: 本综述为合成网络流量生成领域提供了全面的分析，系统性地整理并呈现了不同数据类型、生成模型（特别是深度学习和统计方法）以及评估方法。同时，识别并阐明了该领域的当前挑战和未来研究方向。

Conclusion: 本综述作为合成网络流量生成领域的基础资源，为研究人员和从业者提供了现有方法、面临挑战和发展机遇的结构化分析，旨在促进该领域的进一步研究与开发。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [83] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 为解决多节点芯片互连中静默flit丢失和传输可靠性问题，本文提出隐式序列号（ISN）实现精确检测和有序交付，并将其整合到兼容CXL的RXL协议中，通过分层CRC和FEC实现高可靠、可伸缩且高效的互连。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型超越单处理器能力，芯片间互连成为可伸缩计算的关键。现有高速互连协议（如CXL）传输速率提高，易受错误影响，尤其是在多节点配置中交换设备可能静默丢弃flit。现有CRC和FEC等机制不足以应对此挑战。

Method: 1. 提出了“隐式序列号（ISN）”，一种无需额外报头开销即可精确检测flit丢失并确保按序交付的新机制。2. 提出了“可靠性扩展链路（RXL）”，作为CXL的扩展，它整合了ISN，旨在支持可伸缩、可靠的多节点互连，并保持与现有flit结构的兼容性。3. RXL将CRC提升为传输层机制，用于端到端数据和序列完整性；同时依靠FEC进行链路层错误校正和检测。

Result: 1. ISN能够精确检测flit丢失并确保按序交付。2. RXL在不影响带宽效率的前提下，实现了强大的可靠性和可伸缩性，有效解决了多节点互连中的错误和传输挑战。

Conclusion: 通过引入ISN和RXL，本研究为高带宽、多节点芯片互连提供了创新且高效的可靠性解决方案，确保了数据传输的完整性和有序性，从而支持未来可伸缩AI计算的需求。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [84] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本研究提出CCL预测框架和DUPS资源优化方案，旨在显著降低5G无线接入网的能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G无线接入网（RAN）功耗占总功耗的50%以上，运营成本高昂。现有RAN拆分选项未能充分利用数据潜力，存在降低能耗的机会。

Method: 1. 提出Curated Collaborative Learning (CCL) 框架，通过选择性协作学习实现高精度网络流量和用户预测。 2. 提出Distributed Unit Pooling Scheme (DUPS)，利用深度强化学习和CCL预测结果，有效减少活跃分布式单元（DU）服务器数量并动态重定向流量，优化资源利用。

Result: CCL在预测精度上超越现有最先进方法31.35%至43.9%。DUPS相较传统策略能效提升高达89%，为运营商带来显著经济效益。

Conclusion: 通过整合CCL驱动的预测与DUPS，本研究提供了一种变革性方法，可大幅降低5G无线接入网的能耗和运营成本，显著提升效率和成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [85] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 本初步工作探讨了AI代理在网络故障排除中的应用，并强调了建立一个标准化、可复现、开放的基准测试平台的需求，以低成本构建和评估AI代理。


<details>
  <summary>Details</summary>
Motivation: 现有研究已证明AI（特别是LLMs）在网络配置和诊断中有效，但目前缺乏一个标准化、可复现、开放的平台来低成本地构建和评估用于网络故障排除的AI代理。因此，本研究旨在阐述并强调这一平台的必要性。

Method: 本初步工作聚焦于AI代理在网络故障排除中的应用场景，并通过论述的方式，指出并强调了对一个标准化、可复现、开放的基准测试平台的需求。

Result: 本研究明确了一个关键需求：为了有效且低成本地开发和评估网络故障排除AI代理，迫切需要一个标准化、可复现、开放的基准测试平台。

Conclusion: 为了推动AI代理在网络故障排除领域的应用和发展，迫切需要一个标准化、可复现、开放且操作成本低的基准测试平台，以促进AI代理的构建和评估。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [86] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出一种基于信道感知的语义通信框架，利用生成扩散模型进行动态信道估计，并结合大模型微调以增强车联网数据传输的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）产生海量数据，但在动态无线信道条件下，实时高效传输和处理数据面临巨大挑战。现有信道估计方法在场景剧烈变化时性能可能下降，影响用户体验。

Method: 提出基于信道感知的语义通信框架。该框架通过语义通信模型提取并压缩信息，同时利用生成扩散模型预测动态信道状态。为提高框架在不同场景下的适应性，进一步采用大模型对信道生成模型进行微调。该框架在两个公共数据集上进行了性能和可靠性评估。

Result: 该框架的性能和可靠性已在两个公共数据集上得到评估。

Conclusion: 所提出的语义通信框架，结合语义信息提取、生成扩散模型的信道估计和大模型的适应性增强，有效提升了车联网在动态环境下的数据传输准确性和效率，改善了服务质量。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [87] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 本文提出REDUS重采样技术，优化SDN与FL环境下深度学习训练，通过优先处理错误分类样本和排除冗余数据，显著减少训练时间、能耗和计算资源，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 在物联网环境中，SDN控制器与深度学习工作负载共享基础设施时，存在严重的资源竞争，导致SDN响应性和网络性能下降。即使联邦学习（FL）去中心化训练，其计算需求仍可能干扰SDN的运行，尤其是在持续数据流下。

Method: 提出REDUS（智能网络高效数据利用的重采样）技术。该方法受AdaBoost启发，通过优先处理错误分类样本并排除冗余数据来优化深度学习训练。REDUS减少了每轮训练的样本数量，从而节省计算资源，降低能耗并加速收敛，且被应用于联邦学习（FL）框架中。

Result: REDUS显著减少了计算资源消耗，降低了能耗并加速了模型收敛，而对准确性影响很小。在FL设置中，它提高了资源受限边缘设备上的模型训练效率，同时维持网络性能。在CICIoT2023数据集上评估物联网攻击检测，训练时间减少高达72.6%，准确性损失仅为1.62%。

Conclusion: REDUS为智能网络提供了一种可扩展且实用的解决方案，有效缓解了深度学习训练与SDN操作之间的资源冲突，实现了高效的模型训练和网络性能维护。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [88] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出并验证了一种基于5G PRS的多基地集成感知与通信（ISAC）信号处理链，用于高精度移动目标的检测、定位和跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G定位参考信号（PRS）在分布式多基地架构下，实现移动目标的检测、参数估计和跟踪，以提供高精度的感知能力，并将其整合到通信系统中。

Method: 采用一个gNB发射5G OFDM-PRS，多个接收器利用该信号。通过相干交叉模糊函数（CAF）生成距离-多普勒图以提取双基地延迟和径向速度。对于单目标，采用非线性最小二乘三边测量法进行定位，并通过径向速度方程的正则化线性反演获取二维速度。该方法扩展到2D/3D场景、考虑时间同步偏差，并推广到多目标。最终，使用标准和扩展卡尔曼滤波器来获得平滑的轨迹。

Result: 实验结果显示，该方法使用5G PRS信号在多基地ISAC中实现了高精度的移动目标检测、定位和跟踪。

Conclusion: 该研究成功构建了一个完整的信号处理链，有效利用5G PRS信号，实现了多基地ISAC场景下移动目标的高精度感知能力。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [89] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文研究了在O-RAN框架下天地网络（TN-NTN）融合的架构与功能拆分策略，提出了RAN和核心功能在卫星与地面节点间的分配方案及RAN智能控制器（RICs）的部署策略，并分析了相关权衡，旨在推动标准化融合。


<details>
  <summary>Details</summary>
Motivation: 天地网络（TN-NTN）的融合面临独特的架构和功能挑战，主要原因包括异构传播条件、动态拓扑以及有限的星载处理能力。

Method: 1. 提出了符合O-RAN原则的RAN和核心功能在卫星与地面节点间分配的架构与功能拆分策略分类法。2. 分析了不同拆分选项在性能、延迟、自主性和部署方面的权衡。3. 评估了多种配置方案，从纯星载DU部署到gNB和UPF完全集成到卫星，并考虑了星内和星间处理的变体。4. 讨论了Near-RT和Non-RT RAN智能控制器（RICs）的部署，提出了空地间灵活的拆分策略。5. 提供了架构拆分与RIC部署选项之间的全面映射。

Result: 提出了天地网络融合的O-RAN架构和功能拆分策略分类法；分析了不同拆分选项在性能、延迟、自主性及部署方面的权衡；评估了多种配置方案；讨论并提供了RIC的灵活部署策略和映射关系。

Conclusion: 总结了天地网络融合的关键挑战，并为在O-RAN背景下实现标准化、模块化和高效的TN-NTN收敛指明了未来发展方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [90] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算集群，在ISC 2025 TOP500榜单中排名第49位。它采用全开放式800GbE网络和SONiC操作系统，并为大型语言模型（LLM）训练等高级工作负载进行了优化，展现了卓越的性能和开放技术的潜力。


<details>
  <summary>Details</summary>
Motivation: 构建一个优化的、世界级的高性能计算集群，以支持包括大型语言模型（LLM）训练在内的先进工作负载，并探索和证明开放、供应商中立技术在大规模HPC基础设施中的可行性。

Method: SAKURAONE是一个托管型高性能计算集群，采用“KOKARYOKU PHY”裸金属GPU服务器配置。它由100个计算节点组成，每个节点配备8个NVIDIA H100 GPU。系统支持全闪存Lustre存储子系统，容量达2 PB。节点间通信基于全双向带宽的Rail-Optimized拓扑结构，通过800 GbE链路连接叶层和脊层，并使用RoCEv2实现高速、无损数据传输。其网络堆栈是完全开放的，基于800 GbE和SONiC操作系统。

Result: SAKURAONE在ISC 2025 TOP500榜单中排名全球第49位，是前100名中唯一采用全开放式800GbE和SONiC网络堆栈的系统。其HPL基准测试持续性能（Rmax）达到33.95 PFLOP/s，HPCG基准测试性能为396.295 TFLOP/s。在针对AI应用的HPL-MxP基准测试中，使用FP8精度实现了339.86 PFLOP/s的卓越性能。

Conclusion: SAKURAONE的成功部署和高性能表现证明了开放且供应商中立的技术在大规模高性能计算基础设施中具有强大的可行性和竞争力。它为处理LLM训练等复杂和计算密集型工作负载提供了一个世界级的资源，并有望推动HPC领域开放式架构的发展。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>
