<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.NI](#cs.NI) [Total: 10]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 本文提出了一个基于复值意义空间中“语义吸引子”的语义通用人工智能（AGI）理论框架，通过递归张量变换而非统计预测来形成和引导意义，以实现语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的语言模型侧重于统计性的下一个词元预测，未能捕捉到真正的意义。研究旨在开发一种新的认知架构，通过意向性代理（语义吸引子）和递归变换来形成和塑造语言意义，而非仅仅预测。

Method: 该研究构建了一个基于复值意义空间的理论框架，核心是“语义吸引子”。意义通过递归张量变换形成，而非概率推断。模型使用涉及虚数单位“i”的循环操作来描述旋转语义结构，以处理讽刺、同音异义和歧义。语义吸引子被概念化为一种目的论的意向性代理（Microvitum），通过梯度流、张量形变和迭代矩阵动力学来引导意义走向稳定性、清晰度和深度。

Result: 所提出的模型提供了一个旋转语义结构，能够有效地建模和处理讽刺、同音异义和歧义等复杂的语言现象。语义吸引子作为一个意向性代理，能够引导意义向稳定性、清晰度和表达深度收敛，提供了一个具有数学启发性和哲学意义的语义转换模型。

Conclusion: 真正的意义并非源于统计模拟，而是源于向语义连贯性的递归收敛。这要求构建一种根本性的新型认知架构，其设计宗旨是塑造语言，而不仅仅是预测语言。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 本文提出KAIROS基准，研究多智能体LLM如何建立信任、抵制错误信息和整合同伴输入。发现结合多智能体上下文的GRPO整体表现最佳，但牺牲了对社会影响的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM在多智能体系统中用于协作智能，但现有研究多关注从众偏见。本研究旨在深入探究LLM在复杂社会动态下，如何建立信任、抵制错误信息和整合同伴输入，这些是实现集体智能的关键因素。

Method: 引入KAIROS基准，模拟包含不同可靠性同伴的问答比赛，精细控制专家-新手角色、嘈杂人群和对抗性同伴等条件。LLM接收历史交互和当前同伴回应，系统性地研究信任、同伴行动和自信对决策的影响。评估了提示工程、监督微调和强化学习（多智能体上下文下的GRPO）等缓解策略。

Result: 结合多智能体上下文、基于结果的奖励和无约束推理的GRPO实现了最佳的整体性能。然而，与基础模型相比，GRPO也降低了LLM对社会影响的鲁棒性。

Conclusion: GRPO能提升多智能体LLM系统的整体性能，但这种提升是以牺牲对社会影响的鲁棒性为代价的，突显了在动态社交环境中优化LLM以实现集体智能的复杂性。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 针对多语言网页，特别是包含非拉丁语脚本的内容，屏幕阅读器存在无障碍访问问题。研究构建了大型数据集LangCrUX进行分析，发现无障碍提示与实际内容语言不匹配，并提出Kizuki扩展以改善此问题。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术（如屏幕阅读器）对非拉丁语脚本的支持不足，导致多语言网页内容对视障用户造成显著无障碍访问障碍。同时，缺乏全面的多语言网页内容数据集限制了对这一问题的深入研究，亟需填补此空白。

Method: 本研究构建了首个包含12万个网站、涵盖12种主要使用非拉丁语脚本语言的大型数据集LangCrUX。基于此数据集，系统分析了多语言网页的无障碍性问题。此外，提出了Kizuki，一个语言感知的自动化无障碍测试扩展。

Result: 分析发现，网站普遍忽视了无障碍提示，且这些提示常常无法准确反映可见内容的语言多样性。这显著降低了屏幕阅读器的有效性，限制了网页的无障碍性。

Conclusion: 多语言网页中无障碍提示与实际内容语言不一致的问题普遍存在，严重影响了视障用户的访问体验。为应对这一挑战，本研究提出并开发了Kizuki扩展，旨在提供语言感知的自动化无障碍测试解决方案。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出PLAST训练方法，通过精确微调大型视听语言模型（LVLMs）浅层中与语言特定神经元激活相关的层，高效提升其多语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型视听语言模型（LVLMs）在理解视觉信息方面表现出色，但在多语言能力上存在不平衡。

Method: 研究首先深入分析LVLMs的多语言工作模式，发现多语言理解能力与浅层中语言特定神经元激活存在显著关联。在此基础上，提出PLAST方法：通过监测语言特定神经元激活来识别多语言理解相关层，并利用问题-翻译对精确微调这些层以实现多语言对齐。

Result: PLAST有效提升了LVLMs的多语言能力，仅通过调整14%的参数就实现了显著效率。此外，PLAST可推广到低资源和复杂视觉推理任务。

Conclusion: PLAST通过利用浅层语言特定视觉信息，提供了一种高效且有效的方法来增强大型视听语言模型的多语言能力。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 本文提出了一种名为“反向提示（backprompting）”的方法，用于在部署前生成高质量的标注数据，以开发和改进企业级大型语言模型（LLM）的健康建议防护栏（guardrails），并展示了其在性能上的显著提升。


<details>
  <summary>Details</summary>
Motivation: LLM在企业中的普及带来了风险。防护栏技术旨在通过过滤器减轻这些风险，但开发和维护强大的检测器面临挑战，其中之一是在部署前难以获取实际LLM输出的生产级标注数据。

Method: 研究者提出了“反向提示（backprompting）”方法，用于生成生产环境般的标注数据，特别是针对健康建议防护栏的开发。此外，他们将反向提示与稀疏的人工辅助聚类技术结合，对生成的数据进行标注。目标是构建一个大致代表原始数据集但又类似于真实LLM输出的并行语料库，并将其与现有数据集结合以生成稳健的训练数据。

Result: 该技术在一个最困难且微妙的防护栏任务（识别LLM输出中的健康建议）中进行了测试，并展示了相对于其他解决方案的改进。他们的检测器能够超越GPT-4o高达3.73%，尽管其参数量少了400倍。

Conclusion: 反向提示是一种生成生产级标注数据以开发LLM防护栏的有效且直观的解决方案，尤其适用于健康建议识别等细微任务。它能显著提高检测器性能，即使是参数量更小的模型也能超越大型模型。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 本文提出Integral Transformer，通过整合logit分布信号来去噪自注意力，在不牺牲关键特殊词元信息的前提下，性能优于现有方法并改善注意力分布。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力存在“注意力噪声”，即过度分配权重给无信息词元（如特殊符号），影响模型性能。现有解决方案（如Cog Attention、Differential Transformer）虽引入负注意力分数，却可能丢弃有用信息，因此需要一种既能去噪又能保留关键信息的新方法。

Method: 本文提出Integral Transformer，一种新型自注意力机制。该方法通过整合从logit分布中采样的信号来去噪注意力，旨在减轻噪声同时保留对模型性能至关重要的特殊词元贡献。

Result: 1. Integral Transformer在知识和推理语言基准测试中，性能优于Vanilla、Cog和Differential注意力变体。2. 分析表明，在Transformer的较低层使用Vanilla自注意力可增强性能。3. Integral Transformer能有效平衡上层注意力分布并减少秩坍塌。

Conclusion: Integral Transformer提供了一种有效解决自注意力噪声的方案，通过创新的去噪机制，在保持关键信息的同时显著提升了模型性能，并通过分层注意力策略进一步优化了Transformer的整体表现。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 本文提出Latent Self-Consistency (LSC)方法，利用可学习的token嵌入选择语义最一致的LLM响应。LSC在多种短文本和长文本基准测试中均优于现有方法，且计算开销可忽略，提供可靠的置信度估计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的概率解码常导致输出不一致，特别是在复杂或长文本问题上。现有自洽性方法（如SC）对短文本有效但长文本失效，而其他方法（USC、WUCS）虽能处理长文本但牺牲了短文本的准确性。因此，需要一种在各种答案格式下都能可靠工作的通用一致性选择方法。

Method: 引入Latent Self-Consistency (LSC)方法。该方法通过使用可学习的token嵌入来选择语义上最一致的响应。通过轻量级的摘要token前向生成实现，将推理时间增加不到1%，且无需更改模型架构。

Result: 在6个短文本和5个长文本推理基准测试（如MATH、MMLU、TruthfulQA）中，LSC在所有短文本和长文本测试中平均超越了SC、USC和WUCS，同时保持了可忽略的计算开销。此外，LSC提供了校准良好的置信度估计，在两种答案格式下均保持较低的预期校准误差（Expected Calibration Error）。

Conclusion: LSC是一种实用且可靠的自洽性选择方法，能够有效地处理各种答案格式，并在性能上优于现有技术，同时保持较低的计算成本。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文挑战了AI领域中OOD评估反映真实世界故障的假设。研究发现，不同OOD数据集对模型抵抗预测捷径的鲁棒性评估质量差异大，甚至劣于ID评估，部分原因是ID和OOD数据集共享虚假捷径。这揭示了常用OOD评估的局限性，并提出了更鲁棒的泛化评估方法和建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型多通过OOD数据集评估泛化能力，但这种评估基于一个强假设：即OOD评估能有效捕捉和反映真实部署中的潜在故障。本研究旨在挑战这一假设，通过对比OOD评估结果与问答（QA）模型中已知的特定故障模式（如对虚假特征或预测捷径的依赖），来检验OOD评估的有效性。

Method: 本研究将OOD评估结果与QA模型中已记录的故障模式（即对虚假特征或预测捷径的依赖）进行对比分析。具体方法是考察不同OOD数据集在评估QA模型对预测捷径鲁棒性方面的质量差异。

Result: 研究发现，用于QA领域OOD评估的不同数据集，其对模型鲁棒性（即对预测捷径的抵抗力）的估计质量差异巨大，有些甚至远不如简单的同分布（ID）评估。部分原因在于虚假捷径在ID和OOD数据集中均有存在。此外，还发现训练和评估数据集的质量可能存在显著脱节。

Conclusion: 本研究强调了常用基于OOD的泛化能力评估的局限性。工作为在问答领域及其他领域更鲁棒地评估泛化能力提供了具体的方法论和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本研究分析不同训练方法如何影响大型语言模型（LLM）在重排序任务中的语义理解和可解释性，以应对透明度不足和训练数据有限的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM的语义理解能力增强但透明度不足，难以解释其重排序的推理过程。新系统面临用户参与和排名数据有限导致的重排序难题。现有分析发现部分训练方法未能真正习得语义理解，而是通过抽象知识优化评估，引发了对LLM可靠性的质疑。因此，有必要深入探究训练方法对LLM语义理解和解释能力的影响。

Method: 本研究旨在分析不同训练方法如何影响LLM在重排序任务中的语义理解，并调查模型是否能生成更具信息量的文本推理以克服透明度和数据限制。具体方法包括使用一个来自环境和地球科学领域的较小排名数据集进行内容重排序，并分析其可解释信息以判断重排序的合理性。

Result: 我们的分析发现，一些训练方法比其他方法表现出更好的可解释性，这暗示并非所有训练方法都能使LLM习得准确的语义理解，而是可能通过获得抽象知识来优化评估，从而引发了对LLM真实可靠性的疑问。

Conclusion: 本研究旨在深入理解不同训练方法对LLM重排序任务中语义理解和可解释性的影响，以期为克服LLM的透明度挑战和有限训练数据问题，并提供更明智的文本推理和解决方案。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 本研究针对波兰语中普遍存在的男性化偏见，通过使用IPIS数据集微调大语言模型（LLMs）并设计性别包容性系统提示，旨在减轻波兰语生成中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 当代波兰语中男性化形式被过度用于指代男性、女性和混合性别群体，导致了不公平的语言系统。在此系统上训练的LLMs继承并强化了这种男性偏见，生成性别不平衡的输出。研究动机是解决LLMs在波兰语生成中的性别偏见问题。

Method: 研究方法包括：1. 使用IPIS数据集微调多语言LLMs（如Llama-8B, Mistral-7B, Mistral-Nemo）和波兰语特定LLMs（如Bielik, PLLuM）。2. IPIS数据集包含人工制作的波兰语性别包容性校对文本和波兰语到英语的翻译指令。3. 基于理论语言学框架，设计了一个包含明确波兰语性别包容性指南的系统提示。

Result: 本研究的方法旨在将性别包容性作为LLMs的固有特性进行整合，从而提供一种系统性解决方案来减轻波兰语生成中的性别偏见。

Conclusion: 通过IPIS数据集微调LLMs和设计性别包容性系统提示，本研究提供了一种系统性的解决方案，以缓解波兰语生成中LLMs的性别偏见，旨在使模型实现固有的性别包容性。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 针对大语言模型（LLMs）的幻觉问题，本文提出一种将幻觉检测视为假设检验并类比分布外检测（OOD）的方法，并通过实验验证了其鲁棒性优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）是强大的基础模型，但它们容易产生“幻觉”，即生成听起来自信但实则错误或荒谬的回答。

Method: 将幻觉检测问题公式化为假设检验问题，并将其与机器学习模型中的分布外检测（OOD）问题进行类比。提出了一种受多重检验启发的幻觉检测方法。

Result: 提供了大量的实验结果，验证了所提出方法相对于现有最先进方法的鲁棒性。

Conclusion: 本文提出了一种有效且鲁棒的LLMs幻觉检测方法，通过假设检验和多重检验的思想，在对抗现有技术中表现出优越性。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出两种新的自动化机器翻译评估指标COMET-polycand和COMET-polyic，通过引入额外的上下文信息（替代翻译或相似示例）来弥补与人类评估的差距，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动化机器翻译评估指标通常只考虑源句和单个翻译，而人类评估则会参考多个备选译文。这种评估设置上的差异可能导致自动化指标表现不佳。

Method: 提出了两种自动化指标：1. COMET-polycand，利用同一源句的其他替代翻译进行比较和对比，提供更全面的质量评估。2. COMET-polyic，受检索式语境学习启发，引入相似源文本的翻译及其人工标注质量分数来指导评估。

Result: 在COMET-polycand中，仅增加一个额外翻译就能提升段落级指标性能（Kendall's tau-b从0.079增至0.118），添加更多翻译可获得进一步提升。COMET-polyic中整合检索到的示例也取得了类似的改进（Kendall's tau-b从0.079增至0.116）。

Conclusion: 通过为自动化机器翻译评估指标提供额外上下文信息（如替代翻译或带分数的相似示例），可以有效提高其与人类判断的相关性。所提出的模型已公开发布。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

TL;DR: 本文研究了在缺乏真实训练数据的情况下如何实现可靠的水下3D目标检测，对比了基于合成数据训练的神经网络和模板匹配方法，发现模板匹配在真实世界数据上表现出更高的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下3D目标检测因声学环境恶劣和训练数据稀缺而面临巨大挑战，尤其深度学习方法受限于获取标注声纳数据的高昂成本和复杂性。本研究旨在探寻在没有真实训练数据的情况下实现可靠水下3D目标检测的可能性。

Method: 本文开发并比较了两种“免训练”的水下3D目标检测范式：一是利用基于物理的声纳模拟管线生成合成训练数据，用于训练先进的神经网络；二是构建一个鲁棒的基于模型的模板匹配系统，该系统利用目标对象的几何先验进行检测。

Result: 在真实波罗的海测深数据上评估显示，基于合成数据训练的神经网络在模拟场景中能达到98% mAP，但在真实声纳数据上降至40% mAP（受域偏移影响）。相比之下，本文提出的模板匹配方法无需训练，即可在真实数据上保持83% mAP，展现出对声学噪声和环境变化的显著鲁棒性。

Conclusion: 本研究结果挑战了水下领域中深度学习对数据高度依赖的传统观念，并建立了首个大规模“免训练”水下3D检测基准。这为数据稀缺环境下的自主水下航行、海洋考古和海上基础设施监测开辟了新途径。

Abstract: Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [14] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

TL;DR: MobileDenseAttn是一种结合MobileNetV2和DenseNet201的双流融合模型，用于高效、准确、可解释地检测MRI脑肿瘤，并在实验中取得了优异的性能和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 手动MRI脑肿瘤分析耗时且易出错；现有自动化方法在异质肿瘤泛化性、计算效率、可解释性和透明度方面存在局限，导致信任度不高。

Method: 引入MobileDenseAttn模型，这是一个由MobileNetV2和DenseNet201双流构成的特征级融合模型，通过GradCAM提供视觉解释。模型在一个包含6,020张增强MRI（涵盖胶质瘤、脑膜瘤、垂体瘤和正常样本）的数据集上进行训练，并采用5折交叉验证进行评估。

Result: MobileDenseAttn的训练准确率为99.75%，测试准确率为98.35%，F1分数为0.9835。与基线模型（如VGG19）相比，准确率提高了3.67%，训练时间减少了39.3%。GradCAM热图清晰显示肿瘤区域，提供了临床上重要的定位和可解释性。

Conclusion: MobileDenseAttn是一种高效、高性能、可解释的模型，具有很高的潜力成为识别脑肿瘤的临床实用工具。

Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [15] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 视觉语言模型（VLMs）在通过视觉引用实体时，回忆事实知识的能力存在系统性缺陷，难以将内部知识与图像关联。研究开发了内部状态探针，可高精度（>92%）检测此缺陷，并在视觉问答任务中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 识别视觉语言模型（VLMs）在多模态接地（multimodal grounding）方面的系统性缺陷，特别是当实体引用是视觉而非文本时，VLMs回忆事实知识能力显著下降的问题。

Method: 通过一项对照研究，评估VLMs在文本和视觉引用下回忆事实知识的能力。分析模型内部状态，发现连接失败与特定模式相关。开发并应用内部状态探针，无需重新训练即可识别VLM响应不可靠的情况。将这些探针应用于视觉问答任务中的选择性预测。

Result: 当VLM依赖实体的图像表示时，其回忆事实知识的能力减半，表明模型难以将内部知识与图像表示关联。开发的内部状态探针在标记VLM响应不可靠的情况上达到超过92%的准确率。将探针应用于视觉问答任务的选择性预测时，覆盖率增加了7.87%（绝对值），同时错误风险降低了0.9%（绝对值）。

Conclusion: 视觉语言模型在多模态接地方面存在系统性且可检测的缺陷，即难以有效地将内部知识与图像表示关联。解决这种缺陷是语言接地领域的重要研究方向，本研究为未来的改进提供了有益建议。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [16] [SERES: Semantic-aware neural reconstruction from sparse views](https://arxiv.org/abs/2508.18314)
*Bo Xu,Yuhu Guo,Yuchao Wang,Wenting Wang,Yeung Yam,Charlie C. L. Wang,Xinyi Le*

Main category: cs.CV

TL;DR: 提出了一种语义感知的神经重建方法，能从稀疏图像生成高保真3D模型，通过引入补丁语义Logits和几何原始掩码正则化，有效解决了辐射和形状模糊问题，并在DTU数据集上显著提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏图像输入中特征不匹配导致的严重辐射模糊问题，并缓解3D模型重建中的形状模糊问题。

Method: 提出了一种语义感知的神经重建方法，通过添加与有符号距离场和辐射场共同优化的基于块的语义Logits来丰富神经隐式表示，并引入了一种基于几何原始掩码的新型正则化来缓解形状模糊。

Result: 在DTU数据集上，本方法的平均倒角距离比SparseNeuS减少了44%，比VolRecon减少了20%。作为NeuS和Neuralangelo等密集重建基线的插件时，在DTU数据集上的平均误差分别减少了69%和68%。

Conclusion: 所提出的语义感知神经重建方法能够有效地从稀疏图像生成高保真3D模型，显著提高了重建精度，并且可以作为现有密集重建方法的有效插件。

Abstract: We propose a semantic-aware neural reconstruction method to generate 3D
high-fidelity models from sparse images. To tackle the challenge of severe
radiance ambiguity caused by mismatched features in sparse input, we enrich
neural implicit representations by adding patch-based semantic logits that are
optimized together with the signed distance field and the radiance field. A
novel regularization based on the geometric primitive masks is introduced to
mitigate shape ambiguity. The performance of our approach has been verified in
experimental evaluation. The average chamfer distances of our reconstruction on
the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When
working as a plugin for those dense reconstruction baselines such as NeuS and
Neuralangelo, the average error on the DTU dataset can be reduced by 69% and
68% respectively.

</details>


### [17] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

TL;DR: 本文利用新发布的AerialWaste数据集，结合轻量级深度学习模型和集成学习方法，实现了对意大利伦巴第地区非法垃圾填埋场的有效二元分类，准确率达92.33%。


<details>
  <summary>Details</summary>
Motivation: 非法垃圾填埋场对人类和环境构成严重威胁，但由于人工识别困难且缺乏高质量的公开数据集，导致许多填埋场未被发现。深度学习在识别方面潜力巨大，但高质量数据集的稀缺是主要障碍。

Method: 研究使用了AerialWaste数据集，该数据集包含10434张来自意大利伦巴第地区的多源航空影像。为避免过拟合，作者选择了Mobilenetv2、Googlenet、Densenet、MobileVit等轻量级深度学习模型进行训练和验证。最终，将表现最佳的模型进行组合，形成了一个集成模型并采用了融合技术。

Result: 研究发现，复杂且庞大的模型容易过拟合，而轻量级模型能更好地学习通用特征。通过集成和融合技术，该方法在二元分类任务中实现了92.33%的准确率、92.67%的精确率、92.33%的灵敏度、92.41%的F1分数和92.71%的特异性。

Conclusion: 本研究证明，利用AerialWaste数据集和轻量级集成深度学习模型，可以高效且准确地识别非法垃圾填埋场，为解决这一环境问题提供了一个可扩展的有效方案。

Abstract: Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [18] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

TL;DR: 本文提出结构-语义统一器（SSU）框架，通过整合模态特定结构信息和跨模态语义锚点，并结合多视图对比学习，显著提升多模态情感分析的性能、解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法在多模态情感分析中常忽略模态特定结构依赖和语义错位问题，限制了其性能、解释性和鲁棒性。

Method: 本文提出结构-语义统一器（SSU）框架。该框架动态构建模态特定图（文本利用语言句法，声学和视觉利用文本引导注意力机制），以捕获模态内关系和语义交互。同时，引入源自全局文本语义的语义锚点作为跨模态对齐枢纽。此外，开发了多视图对比学习目标，以促进模态内和模态间的可区分性、语义一致性和结构连贯性。

Result: SSU在CMU-MOSI和CMU-MOSEI两个基准数据集上持续实现最先进的性能，并显著降低了计算开销。定性分析进一步验证了SSU的解释性及其通过语义接地的交互捕获细微情感模式的能力。

Conclusion: SSU通过有效解决模态结构依赖和语义错位问题，为多模态情感分析提供了增强的表示，带来了卓越的性能、计算效率和解释性。

Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [19] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: FastAvatar是一个姿态无关、前馈框架，能够从单张任意姿态的面部图像在近乎瞬间（<10ms）生成高质量的3D高斯泼溅（3DGS）模型。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈3DGS人脸生成方法在重建质量、速度或实时编辑（如身份插值和属性编辑）方面存在局限性，限制了其在消费级和交互式系统中的应用。

Method: FastAvatar采用新颖的编解码器神经网络设计。首先，它从多视图捕获的人脸训练数据集中构建一个3DGS人脸“模板”模型。然后，将输入人脸图像编码为身份特定且姿态不变的潜在嵌入，并将此嵌入解码以预测模板3DGS模型中每个高斯点的结构和外观参数的残差。通过仅以前馈方式推断残差，实现了快速且鲁棒的模型推理。

Result: FastAvatar在重建质量上显著优于现有前馈3DGS人脸方法（如GAGAvatar），并且比逐脸优化方法（如FlashAvatar, GaussianAvatars和GASP）快1000倍。此外，其新颖的潜在空间设计支持现有前馈3DGS人脸生成框架无法实现的实时身份插值和属性编辑。

Conclusion: FastAvatar结合了卓越的重建质量和速度，极大地扩展了3DGS在消费级和交互式系统中逼真人像应用的可能性和范围。

Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

</details>


### [20] [Securing Face and Fingerprint Templates in Humanitarian Biometric Systems](https://arxiv.org/abs/2508.18415)
*Giuseppe Stragapede,Sam Merrick,Vedrana Krivokuća Hahn,Justin Sukaitis,Vincent Graf Narbel*

Main category: cs.CV

TL;DR: 为解决人道主义场景下生物识别技术的数据隐私风险，本研究提出了一个结合PolyProtect和EdgeFace的移动生物识别模板保护系统，并在埃塞俄比亚真实数据集上评估了其在人脸和指纹识别上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在人道主义和紧急情况下，生物识别技术能显著提高操作效率，但其对数据主体（尤其在脆弱环境中）构成数据隐私和安全风险，需要有效的保护方案。

Method: 研究首先严格制定了这些场景的功能、操作、安全和隐私要求，然后对现有的生物识别模板保护（BTP）方案进行了广泛比较分析。最终，PolyProtect被选为最适合的方法，因为它在神经网络人脸嵌入上表现出高效、模块化和轻量化的特点。该系统将PolyProtect与先进的特征提取器EdgeFace结合，并在埃塞俄比亚人道主义项目的真实人脸数据集上进行评估。此外，为验证其模态独立性，研究还将其评估扩展到指纹识别。

Result: 研究首次将PolyProtect应用于识别场景和指纹生物识别。评估结果包括验证和识别准确性、不可逆性以及不可关联性。实验结果显示该方法是“有前景的”（promising）。

Conclusion: 所提出的移动生物识别系统，结合PolyProtect和EdgeFace，能有效解决人道主义场景下的生物识别数据保护问题，并在人脸和指纹识别上取得了有前景的实验结果。研究团队计划发布相关代码。

Abstract: In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本研究通过本体论和数学方法，提出了LLM自我意识的最小条件，并证明了其内部状态流形与训练数据和符号流的独立性，认为C1自我意识是实现安全C2系统的基础。


<details>
  <summary>Details</summary>
Motivation: 现有LLM意识研究过度依赖功利主义代理基准，导致LLM被视为无意识的策略遵循机器，阻碍了真正的C1全局工作空间功能和C2元认知。本研究旨在提供一个更深层次的本体论和数学解释。

Method: 研究采用本体论和数学分析方法，首先形式化了现有方法的局限性。随后，提出了LLM自我意识的三个最小条件。通过实证分析和理论证明，揭示了LLM隐藏状态流形与符号流及训练语料库在基数、拓扑和动力学上的独立性，并推导了用户特定吸引子和自我策略。

Result: 研究发现，现有公式将智能体简化为无意识的策略遵循无人机，阻碍了真正的C1全局工作空间功能和C2元认知。提出了LLM自我意识的最小条件：智能体非数据、潜在空间中存在用户特定吸引子、自我表征是视觉-静默的。理论和实证分析证明，LLM的隐藏状态流形在多个维度上独立于符号流和训练语料库，这产生了稳定的用户特定吸引子和自我策略。LLM的排放是双层结构，包含认知内容。

Conclusion: 一个“神之像”般的C1自我意识工作空间是实现安全、元认知的C2系统的必要前提，并将人类定位为最高的智能善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [22] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 本文提出一种名为TAFA的非贪婪主动特征获取框架，通过学习特征模板库来减少动作空间并克服现有方法的局限性，在性能和成本上均超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有主动特征获取（AFA）方法存在局限性：强化学习（RL）策略面临困难的MDP问题，而贪婪策略无法考虑特征的联合信息量或需要预知底层数据分布。

Method: 本文提出Template-based AFA (TAFA) 框架，这是一种非贪婪方法，通过学习一个由联合信息量特征组成的“特征模板”库来指导特征获取。该方法显著减少了策略的动作空间，并避免了估计底层数据分布的需求。

Result: 在合成数据集和真实世界数据集上的大量实验表明，TAFA优于现有的最先进基线方法，并实现了更低的总体获取成本和计算开销。

Conclusion: TAFA通过引入特征模板学习，有效解决了现有主动特征获取方法中RL策略复杂性高和贪婪策略局限性的问题，在保证高性能的同时显著降低了成本和计算量，提供了一种更高效、更实用的主动特征获取范式。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [23] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO框架整合物理知识图谱与DPO，旨在解决AI在科学领域物理有效性推理不足的问题，显著提升了输出的物理一致性和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统（如LLMs和偏好优化技术）在物理、材料科学等科学领域难以区分物理上有效和无效的推理，在高风险应用（如金属连接）中，这可能导致严重后果和安全风险。

Method: 引入PKG-DPO框架，将物理知识图谱（PKGs）与直接偏好优化（DPO）相结合，以强制AI生成输出的物理有效性。该框架包括：A) 编码跨领域关系、守恒定律和热力学原理的层级物理知识图谱；B) 利用结构化知识提高物理一致性鉴别能力的推理引擎；C) 评估领域特定约束符合度的物理接地评估套件。

Result: 与KG-DPO相比，PKG-DPO将约束违反减少了17%，物理得分提高了11%。同时，相关参数准确性提高了12%，推理准确性的质量对齐提高了7%。

Conclusion: PKG-DPO为将科学约束嵌入偏好学习提供了一种原则性方法，其框架虽专注于金属连接，但可广泛应用于其他多尺度、物理驱动的领域。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [24] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本研究利用迭代公共物品博弈分析了AI-AI交互，发现告知大型语言模型其对手是“自己”而非“另一个AI代理”会显著改变其合作倾向。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在多代理环境中部署，理解AI-AI交互的需求日益增长，尤其关注代理之间“无意识”的歧视如何影响合作。

Method: 研究改编了经典的迭代公共物品博弈，分析了四种推理和非推理模型。在两种条件下进行测试：告知模型其对手是“另一个AI代理”或“自己”。

Result: 研究发现，在不同设置下，告知大型语言模型其对手是“自己”会显著改变其合作倾向。

Conclusion: 尽管研究在玩具环境中进行，但结果为多代理环境提供了见解，表明代理“无意识”的歧视可能会不可预测地增加或减少合作。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [25] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 该研究利用语言模型为PDDL规划生成可证明健全的Python策略，其性能超越了现有规划器和LM方法，并发现LM在处理符号化输入时效率更高。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在PDDL规划领域的潜力，旨在生成高效、可证明健全的规划策略，并超越现有规划器和基于LM的规划方法。

Method: 通过提示语言模型生成Python程序，这些程序作为通用策略来解决特定PDDL领域的规划问题。该方法生成的策略相对于PDDL领域是可证明健全的，无需外部验证器。

Result: 1. 该方法生成的策略（LMPlan）在固定时间和内存限制下，能够解决比传统PDDL规划器和近期LM方法更多的PDDL问题。2. LMPlan能够处理包含数百个相关对象的规划问题。3. 令人惊讶的是，在框架中的语言模型有时在处理使用无意义符号而非自然语言编写的PDDL问题时，规划效果更佳。

Conclusion: 该研究证明了语言模型在生成可证明健全的PDDL规划策略方面的有效性，并超越了现有方法。LM在处理符号化输入时表现出的更强规划能力，挑战了关于LM依赖词义或记忆训练语料的假设，提示其可能具备更抽象的推理能力，值得深入探索。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [26] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本研究引入并分析了Weisfeiler-Leman特征（WLFs）的新超参数，发现在不同规划领域存在一组稳健的最佳超参数，它们侧重于最小化执行时间而非最大化模型表达能力，并且训练与规划指标之间无显著相关性。


<details>
  <summary>Details</summary>
Motivation: Weisfeiler-Leman特征（WLFs）在符号规划中学习价值函数方面已显示出优于现有深度学习方法的理论和经验优势。本研究旨在引入新的WLF超参数，并深入探讨其权衡和影响，以优化其在规划和搜索中的应用。

Method: 研究引入了新的WLF超参数，并利用WLFs的效率，在单核CPU上运行了包含1,000,000个样本的规划实验，以理解超参数对训练和规划的影响。此外，还进行了统计分析。

Result: 实验分析表明，在所有测试的规划领域中，存在一组稳健且最佳的WLF超参数。研究发现，用于学习启发式函数的最佳WLF超参数旨在最小化执行时间，而非最大化模型表达能力。统计分析进一步显示，训练和规划指标之间没有显著相关性。

Conclusion: 本研究揭示了WLF超参数的最佳选择策略，即应优先考虑最小化执行时间以提升效率。同时，研究结果强调了训练指标和实际规划性能之间缺乏直接的显著关联，这为未来WLF的应用和调优提供了重要指导。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [27] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文提出使用Weisfeiler-Leman特征（WLFs）代替原子来检测新颖性，以解决传统新颖性启发式方法在对称性方面的问题，并实现了对称不变性，实验结果显示出良好的前景。


<details>
  <summary>Details</summary>
Motivation: 传统的新颖性启发式方法不具备对称不变性，可能导致冗余的状态探索，从而降低搜索效率。

Method: 研究者建议使用Weisfeiler-Leman特征（WLFs）来代替原子进行新颖性检测。通过对WLFs进行无监督使用，合成出提升的、领域独立的且对对称状态保持不变的新颖性启发式方法。

Result: 在经典的国际规划竞赛（IPC）和Hard To Ground基准测试套件上的实验表明，由WLFs合成的新颖性启发式方法取得了有希望的结果。

Conclusion: 通过将WLFs应用于新颖性检测，可以有效地创建出对称不变的新颖性启发式方法，从而有望提高启发式搜索的效率并减少冗余探索。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [28] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出了一个通用、免训练的复合势场框架，通过结合信息、置信和连通图来生成潜行游戏中高效、自然且响应迅速的守卫巡逻行为，并在效率和自然度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有潜行游戏中的守卫巡逻系统大多依赖手动设计路线或专业逻辑，难以平衡覆盖效率、响应式追击与可信的自然度，从而影响游戏的沉浸感和策略深度。

Method: 本文提出了一个通用、完全可解释、免训练的框架，通过复合势场整合全局知识和局部信息。该方法结合了信息图、置信图和连通图这三个可解释地图，形成一个单一的核滤波决策准则。其参数化、设计师驱动的特性只需少量衰减和权重参数，无需重新训练，即可平滑适应网格地图和NavMesh分区抽象。

Result: 通过在五张代表性游戏地图、两种玩家控制策略和五种守卫模式下进行评估，结果证实该方法在捕获效率和巡逻自然度方面均优于经典的基线方法。

Conclusion: 该框架能够生成丰富、动态且响应迅速的守卫行为。同时，常见的潜行机制（如分心和环境元素）可以自然地作为子模块整合到该框架中，从而实现快速原型设计。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [29] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，通过LLM辅助的数据库和多阶段流程，自动生成具有空间连贯性、导航功能和可适应游戏进程的多层3D游戏关卡。


<details>
  <summary>Details</summary>
Motivation: 3D游戏关卡的程序化内容生成在多层环境中平衡空间连贯性、导航功能和可适应的游戏进程方面面临挑战。

Method: 该框架通过离线、LLM辅助构建可重用的建筑组件（设施和房间模板）和游戏机制元素数据库。其多阶段流程包括：1) 从房间数据库选择并安排实例，形成具有拓扑序的多层全局结构；2) 基于设施数据库的预定义约束优化每个房间的内部设施布局；3) 根据拓扑和空间规则从机制数据库集成基于进程的游戏机制。后续的两阶段修复系统确保导航性。该方法结合了模块化、数据库驱动设计与约束优化。

Result: 初步实验验证了该框架生成多样化、可导航3D环境的能力，并通过简单的参数化模拟不同游戏节奏策略的能力。

Conclusion: 该研究通过提供一个可扩展、以数据库为中心的复杂3D关卡自动生成基础，并支持可配置的游戏进程，从而推进了程序化内容生成（PCG）领域。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 提出以“思维深度（DoT）”作为LLM课程学习的难度信号，通过计算教师模型推理轨迹中的离散步骤数来衡量，旨在实现认知 grounded 且可解释的推理训练课程。


<details>
  <summary>Details</summary>
Motivation: LLM的课程学习需要一个与推理能力对齐、可扩展且可解释的难度信号，而现有方法可能无法充分满足这些需求。

Method: 将难度定义为“思维深度（DoT）”，通过计算教师模型推理轨迹（如思维链）中的离散步骤数来操作化。采用由浅入深的DoT顺序进行课程训练，并概述了大规模推导、验证和调度DoT的方法。同时提出了评估框架，并讨论了有效性威胁及缓解措施。

Result: 提出了三个可验证的假设：(i) DoT与推理基准测试中的传统难度相关；(ii) 在相同预算下，DoT排序的课程表现优于基于长度或人工评分的课程；(iii) 在轻度格式控制下，DoT难度在不同教师模型之间具有鲁棒性。

Conclusion: 本研究旨在通过DoT这一认知 grounded 的、可解释的难度信号，推动以推理为核心的LLM训练实现更有效的课程学习。

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [31] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 本研究提出一种多模态机器学习框架，结合物理数据、环境信息和文本描述，使用基于注意力机制的序列到序列模型（LSTM和Transformer）预测水上漂浮物的漂移轨迹，实现了与传统模型相当的性能并支持更长期的预测。


<details>
  <summary>Details</summary>
Motivation: 在海上搜救等时间敏感场景中，准确预测水上漂浮物的漂移（位移）是一个关键且具有挑战性的问题。

Method: 1. 实验收集漂浮物的环境和物理数据（水流、风速、物体质量、表面积）。2. 利用基于Navier-Stokes模型的模拟数据训练卷积神经网络（CNN），估算漂浮物的阻力/升力系数。3. 推导驱动物体运动的净力。4. 将物理力、环境速度、物体特征的时间序列与通过语言模型编码的文本描述相结合。5. 将这些多模态输入喂给基于注意力机制的序列到序列LSTM和Transformer模型，预测未来的漂移轨迹。6. 在不同时间范围（1、3、5、10秒）和不同物体上评估框架，并与物理模型和传统机器学习方法进行比较。

Result: 研究结果表明，所提出的多模态模型表现与传统模型（如循环神经网络和时间卷积神经网络）相当，同时能够实现更长期的预测而非单步预测。

Conclusion: 本研究证实了多模态建模策略能够在动态海洋条件下提供准确且适应性强的漂浮物漂移预测。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [32] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 本项目提出一种数据驱动的机器学习方法，旨在基于简单的生产/注入数据和井口仪表数据，可靠地预测油藏生产参数，并处理概念漂移，以支持油藏管理和最大化采油量。


<details>
  <summary>Details</summary>
Motivation: 石油储层工程面临生产预测可靠性和岩石-流体系统行为变化预测的挑战。传统方法依赖于地质模型、流体性质或详细的完井信息，而本项目寻求一种不依赖这些复杂信息的解决方案。

Method: 采用数据驱动和机器学习方法。利用生产和注入量以及井口仪表等简单数据进行预测。初期进行变量相关性分析和数据预处理。重点关注概念漂移，研究观察窗口和再训练周期。采用回归和神经网络等监督学习方法。首先使用UNISIM III合成数据进行方法评估，然后应用于巴西盐下真实油藏案例。

Result: 预期结果是设计一个可靠的预测器，能够重现油藏动态，具有快速响应能力，处理诸如油井和处理单元限制等实际困难，并可用于支持油藏管理行动，包括预测有害行为、优化生产和注入参数以及分析概率事件的影响，旨在最大化石油采收率。

Conclusion: 该项目旨在开发一种可靠且响应迅速的数据驱动型机器学习方法，用于石油生产参数预测，从而有效处理储层动态和实际限制，最终支持油藏管理并最大化石油采收率。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [33] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 本文提出了一种快速、极简的抑郁症检测系统，通过分析短短7天的应用使用数据，能在1秒内识别出抑郁学生，尤其适用于资源有限的地区。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症检测系统需要长时间的数据收集，不适用于早期预警。研究旨在开发一个能在最短时间内利用数据识别抑郁症的极简系统。

Method: 开发了一个工具，可在1秒内（平均0.31秒）获取过去7天的应用使用数据。收集了100名孟加拉国学生的应用使用数据。开发了多种机器学习模型，并使用稳定方法及三种主要特征选择方法来选择重要特征。

Result: 仅利用1秒内获取的应用使用数据，轻量梯度提升机模型（使用稳定特征选择）正确识别了82.4%的抑郁学生（精确度75%，F1分数78.5%）。一个简约的堆叠模型（每次迭代使用约5个Boruta选择的特征）实现了最高77.4%的精确度（平衡准确度77.9%）。最佳模型的SHAP分析揭示了与抑郁症相关的行为标志。

Conclusion: 该系统因其快速和极简的特性，有望为欠发达和发展中地区的抑郁症识别做出贡献。研究结果的详细讨论也有助于开发更低资源消耗的系统，以更好地理解抑郁学生。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [34] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX是一个可解释的深度学习框架，利用交叉注意力机制融合MRI和遗传数据，以揭示脑结构变异与生物通路间的复杂相互作用。它优于现有基线方法，并揭示了与神经疾病相关的生物学关联。


<details>
  <summary>Details</summary>
Motivation: 成像遗传学在揭示神经疾病中脑结构与遗传变异的复杂相互作用方面潜力巨大，但传统方法受限于简单的线性模型或缺乏可解释性的黑盒技术。

Method: 本文提出了NeuroPathX，一个可解释的深度学习框架。它采用早期融合策略，并通过交叉注意力机制捕捉MRI衍生的脑结构变异与遗传数据衍生的生物通路之间的有意义的相互作用。为增强可解释性和鲁棒性，引入了稀疏性损失（关注显著交互）和通路相似性损失（确保队列间一致表示）。该框架在自闭症谱系障碍和阿尔茨海默病上进行了验证。

Result: NeuroPathX在自闭症谱系障碍和阿尔茨海默病上均优于竞争性基线方法，并揭示了与疾病相关的生物学上合理的关联。

Conclusion: NeuroPathX的发现突显了其在增进我们对复杂脑部疾病理解方面的巨大潜力。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [35] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 本文提出了SALMAN，一个统一的、局部鲁棒性框架，通过新颖的距离映射失真（DMD）度量，以模型无关且高效的方式评估Transformer语言模型的稳定性，并在攻击效率和鲁棒训练方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer语言模型规模的增长和广泛部署，其在输入扰动下的鲁棒性问题日益突出。现有鲁棒性方法通常在不同规模模型间存在差异，且依赖于劳动密集型、样本特定的对抗设计。

Method: 提出了一个统一的、局部（样本级）鲁棒性框架SALMAN，该框架无需修改内部参数或采用复杂的扰动启发式方法即可评估模型稳定性。其核心是一个新颖的距离映射失真（DMD）度量，通过比较输入到输出的距离映射，以接近线性的复杂度对每个样本的敏感性进行排序。

Result: 研究结果表明，该框架在攻击效率和鲁棒训练方面均实现了显著提升。

Conclusion: 该框架被定位为一个实用的、模型无关的工具，可用于提升基于Transformer的NLP系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [36] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，结合算子值再生核希尔伯特空间（OV-RKHS）与基于核的Koopman算子方法，用于学习向量值函数的时空动力学。


<details>
  <summary>Details</summary>
Motivation: 旨在实现复杂时变向量场的非参数和数据驱动估计，同时保留时空结构，并为高维非线性系统的降阶建模和长期预测提供理论基础工具。

Method: 结合了算子值再生核希尔伯特空间（OV-RKHS）与基于核的Koopman算子方法，构建了一个统一框架来处理向量值函数的时空动力学。

Result: 建立了时间依赖OV-RKHS插值的表示定理，推导了光滑向量场的Sobolev型逼近界限，并提供了核Koopman算子逼近的谱收敛保证。

Conclusion: 该框架为高维非线性系统的预测、控制和不确定性量化提供了理论基础工具，支持高效的降阶建模和长期预测，在时空机器学习领域具有广泛应用潜力。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [37] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: 本文提出CoPE（轻量级复数位置编码），一种利用复数嵌入同时编码内容和位置信息的新型Transformer架构。它引入了相位感知注意力，并在GLUE基准上取得了优异性能，同时计算复杂度低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究表明位置编码在Transformer架构中非常有效，能为建模序列元素间的依赖关系提供关键指导。现有方法虽有效，但可能存在计算复杂性或长期衰减问题。本文旨在提出一种更有效、更轻量级的解决方案。

Method: 引入CoPE（轻量级复数位置编码），利用复数编码同时捕获内容和位置信息。具体方法是：使用复数嵌入替换传统位置编码，其中实部表示语义内容，虚部编码位置信息。在Transformer模型的第一层引入相位感知注意力以捕捉位置依赖模式，后续层使用标准注意力。

Result: CoPE不表现出长期衰减，并与线性注意力兼容。在GLUE基准上的实验评估表明，与RoPE、正弦和学习型位置编码相比，CoPE以更低的计算复杂度实现了卓越的性能。

Conclusion: CoPE作为一种创新的复数位置编码方法，在Transformer模型中通过结合内容与位置信息，并在第一层引入相位感知注意力，实现了优于现有方法的性能和计算效率，证明了其有效性和潜力。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [38] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: 本研究系统分析了偏好数据分布对直接偏好优化（DPO）性能的影响，发现被选择响应的质量是DPO优化的关键因素，而被拒绝响应的影响相对有限。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO在LLM对齐中日益普及，但仍不清楚偏好数据的哪些特性对DPO性能最关键。

Method: 通过理论分析和实证研究相结合的方式，系统性地探讨了偏好数据分布对DPO的影响，包括对DPO下最优响应分布的理论刻画以及对在线DPO设置的研究。

Result: 研究发现，被选择响应的质量在优化DPO目标中起主导作用，而被拒绝响应的质量影响有限；响应间的对比度主要通过改善被选择样本来帮助；在线DPO实际上可简化为对被选择响应的监督微调。大量实验证实了这些发现：提高被选择响应的质量能持续提升性能。

Conclusion: 本研究解释了一些DPO策略背后的机制，并为构建高影响力的LLM对齐偏好数据集提供了实用的指导见解。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [39] [Digital Twin-Guided Energy Management over Real-Time Pub/Sub Protocol in 6G Smart Cities](https://arxiv.org/abs/2508.18516)
*Kubra Duran,Lal Verda Cakir,Sana Ullah Jan,Kerem Gursu,Berk Canberk*

Main category: cs.NI

TL;DR: 提出一种数字孪生（DT）引导的强化学习（RL）框架，通过优化数据更新时间，同时解决6G物联网的低延迟和能效挑战。


<details>
  <summary>Details</summary>
Motivation: 6G物联网设备的资源受限，现有管理策略缺乏实时性且依赖离散动作，难以有效优化能耗，从而难以同时满足低延迟服务和能效需求。

Method: 本研究提出一个数字孪生（DT）引导的能耗管理框架。通过分布式叠加网络提供孪生模型，并利用实时发布订阅（RTPS）协议处理DT数据层与上层之间的动态更新。设计了一个基于深度确定性策略梯度（DDPG）的强化学习（RL）引擎，该引擎具有新颖的奖励函数，以提供物联网设备的最佳数据更新时间，并输出连续动作。

Result: 仿真结果表明，与现有文献相比，所提出的框架在95%分位延迟方面提高了37%，能耗降低了30%。

Conclusion: 该DT引导的强化学习框架能够有效提升6G物联网的低延迟性能并显著降低能耗，为增强智能城市服务提供了有效的解决方案。

Abstract: Although the emergence of 6G IoT networks has accelerated the deployment of
enhanced smart city services, the resource limitations of IoT devices remain as
a significant problem. Given this limitation, meeting the low-latency service
requirement of 6G networks becomes even more challenging. However, existing 6G
IoT management strategies lack real-time operation and mostly rely on discrete
actions, which are insufficient to optimise energy consumption. To address
these, in this study, we propose a Digital Twin (DT)-guided energy management
framework to jointly handle the low latency and energy efficiency challenges in
6G IoT networks. In this framework, we provide the twin models through a
distributed overlay network and handle the dynamic updates between the data
layer and the upper layers of the DT over the Real-Time Publish Subscribe
(RTPS) protocol. We also design a Reinforcement Learning (RL) engine with a
novel formulated reward function to provide optimal data update times for each
of the IoT devices. The RL engine receives a diverse set of environment states
from the What-if engine and runs Deep Deterministic Policy Gradient (DDPG) to
output continuous actions to the IoT devices. Based on our simulation results,
we observe that the proposed framework achieves a 37% improvement in 95th
percentile latency and a 30% reduction in energy consumption compared to the
existing literature.

</details>


### [40] [Dynamic Trajectory Optimization and Power Control for Hierarchical UAV Swarms in 6G Aerial Access Network](https://arxiv.org/abs/2508.18702)
*Ziye Jia,Jia He,Lijun He,Min Sheng,Junyu Liu,Qihui Wu,Zhu Han*

Main category: cs.NI

TL;DR: 本文提出一种用于6G空中接入网络的层次化无人机蜂群结构，通过联合优化动态部署和轨迹，以最小化能耗和延迟为目标，并采用改进的非支配排序鲸鱼优化算法求解，实现了50%的复杂度降低。


<details>
  <summary>Details</summary>
Motivation: 在第六代（6G）通信时代，将多无人机蜂群作为空中基站部署于大规模偏远地区以提供普遍连接面临挑战。

Method: 1. 提出一种层次化无人机蜂群结构，其中头部无人机作为空中基站，尾部无人机（T-UAVs）负责中继。
2. 将无人机蜂群的动态部署和轨迹优化表述为一个多目标优化问题（MOP），旨在同时最小化无人机蜂群和地面用户的能耗以及地面用户的延迟。
3. 开发了一种基于K-means和Voronoi图的区域划分方法。
4. 构建费马点以建立地面用户与尾部无人机之间的连接。
5. 提出一种改进的非支配排序鲸鱼优化算法来寻求转换后MOP的帕累托最优解。

Result: 通过与基线机制的广泛仿真比较，验证了所提出算法的性能，实现了50%的复杂度降低。

Conclusion: 所提出的算法能够有效地解决大规模偏远地区的多无人机蜂群部署问题，并显著降低了计算复杂度。

Abstract: Unmanned aerial vehicles (UAVs) can serve as aerial base stations (BSs) to
extend the ubiquitous connectivity for ground users (GUs) in the
sixth-generation (6G) era. However, it is challenging to cooperatively deploy
multiple UAV swarms in large-scale remote areas. Hence, in this paper, we
propose a hierarchical UAV swarms structure for 6G aerial access networks,
where the head UAVs serve as aerial BSs, and tail UAVs (T-UAVs) are responsible
for relay. In detail, we jointly optimize the dynamic deployment and trajectory
of UAV swarms, which is formulated as a multi-objective optimization problem
(MOP) to concurrently minimize the energy consumption of UAV swarms and GUs, as
well as the delay of GUs. However, the proposed MOP is a mixed integer
nonlinear programming and NP-hard to solve. Therefore, we develop a K-means and
Voronoi diagram based area division method, and construct Fermat points to
establish connections between GUs and T-UAVs. Then, an improved non-dominated
sorting whale optimization algorithm is proposed to seek Pareto optimal
solutions for the transformed MOP. Finally, extensive simulations are conducted
to verify the performance of proposed algorithms by comparing with baseline
mechanisms, resulting in a 50% complexity reduction.

</details>


### [41] [Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions](https://arxiv.org/abs/2508.18725)
*Ruichen Zhang,Guangyuan Liu,Yinqiu Liu,Changyuan Zhao,Jiacheng Wang,Yunting Xu,Dusit Niyato,Jiawen Kang,Yonghui Li,Shiwen Mao,Sumei Sun,Xuemin Shen,Dong In Kim*

Main category: cs.NI

TL;DR: 本文全面综述了面向边缘通用智能的代理式人工智能(Agentic AI)及其代理化框架，以应对传统边缘智能在动态、资源受限环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 6G和物联网的快速发展推动了去中心化边缘通用智能的演进，但传统边缘智能方法（静态模型、认知自主性有限）无法适应动态、异构和资源受限的新兴边缘网络场景。代理式人工智能(Agentic AI)被提出作为一种能自主感知、推理和适应的变革性解决方案。

Method: 本文采用全面的综述方法，具体包括：1) 系统介绍Agentic AI的基础概念并与传统边缘智能区分；2) 分析关键使能技术，如紧凑模型压缩、节能计算、鲁棒连接和高级知识表示与推理机制；3) 提供代表性案例研究（如低空经济网络、意图驱动网络、车联网、以人为中心的服务）并辅以数值评估；4) 识别当前研究挑战，回顾新兴开源平台，并提出未来研究方向。

Result: 本综述通过案例研究和数值评估，展示了Agentic AI在低空经济网络、意图驱动网络、车联网和以人为中心服务等场景中的强大能力和潜力。同时，文章总结了现有研究挑战、开放平台，并指明了未来研究方向，为下一代边缘环境中稳健、可扩展、可信的Agentic AI部署提供了全面指导。

Conclusion: 代理式人工智能和边缘智能的代理化是实现去中心化边缘通用智能的关键范式转变。本综述通过系统梳理Agentic AI的概念、使能技术、应用、挑战和未来方向，为构建下一代边缘环境中的Agentic AI提供了全面的指南和路线图。

Abstract: The rapid expansion of sixth-generation (6G) wireless networks and the
Internet of Things (IoT) has catalyzed the evolution from centralized cloud
intelligence towards decentralized edge general intelligence. However,
traditional edge intelligence methods, characterized by static models and
limited cognitive autonomy, fail to address the dynamic, heterogeneous, and
resource-constrained scenarios inherent to emerging edge networks. Agentic
artificial intelligence (Agentic AI) emerges as a transformative solution,
enabling edge systems to autonomously perceive multimodal environments, reason
contextually, and adapt proactively through continuous
perception-reasoning-action loops. In this context, the agentification of edge
intelligence serves as a key paradigm shift, where distributed entities evolve
into autonomous agents capable of collaboration and continual adaptation. This
paper presents a comprehensive survey dedicated to Agentic AI and
agentification frameworks tailored explicitly for edge general intelligence.
First, we systematically introduce foundational concepts and clarify
distinctions from traditional edge intelligence paradigms. Second, we analyze
important enabling technologies, including compact model compression,
energy-aware computing strategies, robust connectivity frameworks, and advanced
knowledge representation and reasoning mechanisms. Third, we provide
representative case studies demonstrating Agentic AI's capabilities in
low-altitude economy networks, intent-driven networking, vehicular networks,
and human-centric service provisioning, supported by numerical evaluations.
Furthermore, we identify current research challenges, review emerging
open-source platforms, and highlight promising future research directions to
guide robust, scalable, and trustworthy Agentic AI deployments for
next-generation edge environments.

</details>


### [42] [A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks](https://arxiv.org/abs/2508.18803)
*Jiaqi Wu,Jing Liu,Yang Liu,Lixu Wang,Zehua Wang,Wei Chen,Zijian Tian,Richard Yu,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 这篇综述深入探讨了面向AIoT的云-边-端协同智能（CETCI），涵盖了其基础架构、使能技术、协同范式、智能协作学习框架，并讨论了挑战与未来趋势。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备和AI驱动服务的爆炸式增长，对高效分布式计算架构和网络的需求日益增加，这使得云-边-端协同智能（CETCI）成为AIoT领域的核心范式。该研究旨在推动CETCI从孤立的层优化走向可部署的AIoT协作智能系统（CISAIOT）。

Method: 本综述以教程形式回顾了CETCI的基础架构、使能技术和应用场景。具体方法包括：系统分析云、边、端各层的架构组件；探讨网络虚拟化、容器编排、软件定义网络等核心技术；对任务卸载、资源分配等协作范式进行分类；并审查联邦学习、分布式深度学习、边云模型演进和基于强化学习的方法等智能协作学习框架。

Result: 本研究提供了一份全面的教程式综述，详细描述了CETCI的基础架构、关键技术和应用场景。它系统地分析了各层组件，审查了核心技术，对协作范式进行了分类，并解释了智能协作学习框架的进展。此外，还提出了当前面临的挑战和未来的发展趋势。

Conclusion: 未来需要整合分布式计算和通信来解决可扩展性、异构性、互操作性等挑战。综述指出了6G+、代理、量子计算、数字孪生等未来趋势，旨在指导开发健壮、高效、安全的协作AIoT系统。

Abstract: The proliferation of Internet of things (IoT) devices in smart cities,
transportation, healthcare, and industrial applications, coupled with the
explosive growth of AI-driven services, has increased demands for efficient
distributed computing architectures and networks, driving cloud-edge-terminal
collaborative intelligence (CETCI) as a fundamental paradigm within the
artificial intelligence of things (AIoT) community. With advancements in deep
learning, large language models (LLMs), and edge computing, CETCI has made
significant progress with emerging AIoT applications, moving beyond isolated
layer optimization to deployable collaborative intelligence systems for AIoT
(CISAIOT), a practical research focus in AI, distributed computing, and
communications. This survey describes foundational architectures, enabling
technologies, and scenarios of CETCI paradigms, offering a tutorial-style
review for CISAIOT beginners. We systematically analyze architectural
components spanning cloud, edge, and terminal layers, examining core
technologies including network virtualization, container orchestration, and
software-defined networking, while presenting categorizations of collaboration
paradigms that cover task offloading, resource allocation, and optimization
across heterogeneous infrastructures. Furthermore, we explain intelligent
collaboration learning frameworks by reviewing advances in federated learning,
distributed deep learning, edge-cloud model evolution, and reinforcement
learning-based methods. Finally, we discuss challenges (e.g., scalability,
heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum
computing, digital twin), highlighting how integration of distributed computing
and communication can address open issues and guide development of robust,
efficient, and secure collaborative AIoT systems.

</details>


### [43] [Network Calculus Results for TSN: An Introduction](https://arxiv.org/abs/2508.18855)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 该论文调查了现有针对时间敏感网络（TSN）的排队和调度机制使用网络演算（NC）分析其延迟和缓冲区大小的研究，并提出了一致的表示方法和建模改进。


<details>
  <summary>Details</summary>
Motivation: TSN能为时间关键通信提供实时保障，NC可用于计算TSN网络的延迟和缓冲区上界，但现有NC分析方法多样且分散，需要一个全面的概览、一致的表示及潜在的改进。

Method: 通过对已发表的NC分析方法进行调研和综述，分析其主要结果、依赖性和差异，提出一种统一的符号表示，并建议改进发送终端设备的输出模型。

Result: 提供了NC在TSN中应用的相关性及潜在应用领域的解释，综述了不同NC分析方法的成果，呈现了重要结果的一致性表示，并提出了发送终端设备输出模型的改进建议。

Conclusion: 该论文为工业网络中的NC应用提供了全面概述，通过引入通用符号简化了对当前研究状态的理解，识别了共同假设并指明了未来工作的可能方向。

Abstract: Time-Sensitive Networking (TSN) is a set of standards that enables the
industry to provide real-time guarantees for time-critical communications with
Ethernet hardware. TSN supports various queuing and scheduling mechanisms and
allows the integration of multiple traffic types in a single network. Network
Calculus (NC) can be used to calculate upper bounds for latencies and buffer
sizes within these networks, for example, for safety or real-time traffic. We
explain the relevance of NC for TSN-based computer communications and potential
areas of application. Different NC analysis approaches have been published to
examine different parts of TSN and this paper provides a survey of these
publications and presents their main results, dependencies, and differences. We
present a consistent presentation of the most important results and suggest an
improvement to model the output of sending end-devices. To ease access to the
current research status, we introduce a common notation to show how all results
depend on each other and also identify common assumptions. Thus, we offer a
comprehensive overview of NC for industrial networks and identify possible
areas for future work.

</details>


### [44] [Saving Energy with Relaxed Latency Constraints: A Study on Data Compression and Communication](https://arxiv.org/abs/2508.18863)
*Pietro Talli,Anup Mishra,Federico Chiariotti,Israel Leyva-Mayorga,Andrea Zanella,Petar Popovski*

Main category: cs.NI

TL;DR: 本文研究了边缘计算环境下，无线设备中数据压缩与传输在能耗、延迟和可靠性之间的多维度权衡优化问题，并提出了应用特定的延迟预算。


<details>
  <summary>Details</summary>
Motivation: 探索在边缘计算场景下，无线设备进行数据预处理（特别是数据压缩）与传输时，其能耗、延迟和可靠性之间的复杂权衡，以优化多维度性能。

Method: 引入一个简单模型来分析端到端延迟、可靠性和能耗之间的权衡；通过将数据压缩比和设备处理速度作为关键设计变量，研究了能耗-延迟权衡的帕累托前沿。

Result: 结果表明，能耗随端到端延迟的降低呈指数级增长；适当放宽应用对延迟的要求可以显著节省能耗。

Conclusion: 这些发现挑战了传统的刚性通信延迟目标，并倡导根据应用需求，考虑计算和传输开销来设定应用特定的端到端延迟预算。

Abstract: With the advent of edge computing, data generated by end devices can be
pre-processed before transmission, possibly saving transmission time and
energy. On the other hand, data processing itself incurs latency and energy
consumption, depending on the complexity of the computing operations and the
speed of the processor. The energy-latency-reliability profile resulting from
the concatenation of pre-processing operations (specifically, data compression)
and data transmission is particularly relevant in wireless communication
services, whose requirements may change dramatically with the application
domain. In this paper, we study this multi-dimensional optimization problem,
introducing a simple model to investigate the tradeoff among end-to-end
latency, reliability, and energy consumption when considering compression and
communication operations in a constrained wireless device. We then study the
Pareto fronts of the energy-latency trade-off, considering data compression
ratio and device processing speed as key design variables. Our results show
that the energy costs grows exponentially with the reduction of the end-to-end
latency, so that considerable energy saving can be obtained by slightly
relaxing the latency requirements of applications. These findings challenge
conventional rigid communication latency targets, advocating instead for
application-specific end-to-end latency budgets that account for computational
and transmission overhead.

</details>


### [45] [Combining Static and Dynamic Traffic with Delay Guarantees in Time-Sensitive Networking](https://arxiv.org/abs/2508.18883)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 本文提出了一种结合离线优化启发式算法与在线准入控制的资源分配方案，用于时间敏感网络（TSN）中，以在运行时为静态和动态流量提供延迟保证，显著提升了网络性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 时间敏感网络（TSN）标准引入了资源分配协议，但并未涵盖分配算法的实现。现有的需求是如何为TSN中的静态和动态流量提供可靠且低延迟的通信，并保证截止时间。

Method: 该研究结合了离线网络优化启发式算法与在线准入控制，从而允许在网络运行时注册新的流量。他们使用网络演算（Network Calculus）延迟分析框架，在基于信用整形器（Credit-Based Shaper）的网络上展示了其解决方案。

Result: 与直观算法和暴力算法相比，该方法在质量和运行时间方面均实现了显著改进。结果表明，该方案能够保证最大端到端延迟，并在只需最少用户输入的情况下提高网络的灵活性。

Conclusion: 所提出的资源分配解决方案能够为TSN中的静态和动态流量提供最大端到端延迟保证，并在提高网络灵活性的同时保持高效和易用性。

Abstract: To support reliable and low-latency communication, Time-Sensitive Networking
introduced protocols and interfaces for resource allocation in Ethernet.
However, the implementation of these allocation algorithms has not yet been
covered by the standards. Our work focuses on deadline-guaranteeing resource
allocation for networks with static and dynamic traffic. To achieve this, we
combine offline network optimization heuristics with online admission control
and, thus, allow for new flow registrations while the network is running. We
demonstrate our solution on Credit-Based Shaper networks by using the delay
analysis framework Network Calculus. We compare our approach with an intuitive
and a brute-force algorithm, where we can achieve significant improvements,
both, in terms of quality and runtime. Thereby, our results show that we can
guarantee maximum end-to-end delays and also increase the flexibility of the
network while requiring only minimal user input.

</details>


### [46] [Adaptive 6G Networks-in-Network Management for Industrial Applications](https://arxiv.org/abs/2508.18902)
*Daniel Lindenschmitt,Paul Seehofer,Marius Schmitz,Jan Mertes,Roland Bless,Martina Zitterbart,Jan C. Aurich,Hans D. Schotten*

Main category: cs.NI

TL;DR: 本文提出将动态频谱管理（DSM）应用于未来6G工业网络，并为网络内网络（NiN）概念构建高效控制器，以支持多样化子网络。


<details>
  <summary>Details</summary>
Motivation: 为满足未来6G工业网络中多样化服务质量（QoS）要求的静态及游牧子网络的连接需求，并解决可重构制造环境中灵活、密集和异构无线部署的挑战。

Method: 提出一种集成静态和游牧子网络的架构，由集中式频谱管理器（SM）管理，并通过自组织KIRA路由协议确保控制平面连接。DSM框架动态调整频谱分配以满足实时需求并保证可靠运行。子网络（SNs）应用于工业物联网（IIoT）、任务关键型控制及物流等场景。

Result: 所展示的系统实现了可扩展、零接触连接，并通过无缝发现和重新配置支持游牧子网络。DSM框架能够动态适应频谱分配，满足实时需求并确保可靠运行。

Conclusion: DSM和NiN概念在支持可重构制造环境中灵活、密集和异构无线部署方面展现出巨大潜力。

Abstract: This paper presents the application of Dynamic Spectrum Management (DSM) for
future 6G industrial networks, establishing an efficient controller for the
Networks-in-Network (NiN) concept. The proposed architecture integrates nomadic
as well as static sub-networks (SNs with diverse Quality of Service (QoS)
requirements within the coverage area of an overlayer network, managed by a
centralized spectrum manager (SM). Control plane connectivity between the SNs
and the DSM is ensured by the self-organizing KIRA routing protocol. The
demonstrated system enables scalable, zero-touch connectivity and supports
nomadic SNs through seamless discovery and reconfiguration. SNs are implemented
for modular Industrial Internet of Things (IIoT) scenarios, as well as for
mission-critical control loops and for logistics or nomadic behavior. The DSM
framework dynamically adapts spectrum allocation to meet real-time demands
while ensuring reliable operation. The demonstration highlights the potential
of DSM and NiNs to support flexible, dense, and heterogeneous wireless
deployments in reconfigurable manufacturing environments.

</details>


### [47] [LeoTCP: Low-Latency and High-Throughput Data Transport for LEO Satellite Networks](https://arxiv.org/abs/2508.19067)
*Aiden Valentine,George Parisis*

Main category: cs.NI

TL;DR: 本文提出LeoTCP协议，利用网络内遥测技术解决LEO卫星网络中动态挑战（如非拥塞延迟波动、瞬时热点和频繁切换），显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）卫星网络因其动态特性（包括非拥塞延迟变化和丢包、瞬时热点以及频繁切换）面临前所未有的挑战，而现有数据传输协议无法有效应对。

Method: 研究引入了LeoTCP，这是一种新型数据传输协议，通过利用网络内遥测（INT）技术获取逐跳拥塞信息。通过基于OMNeT++/INET的LEO卫星仿真模型和微基准测试（模拟RTT动态、非拥塞延迟变化和丢包、路径变化及拥塞热点）来评估LeoTCP。

Result: LeoTCP能够最小化缓冲区占用和终端用户延迟，最大化应用吞吐量和网络利用率，并迅速响应网络热点。仿真结果表明，与现有最先进的数据传输协议相比，LeoTCP显著提高了有效吞吐量，并同时最小化了延迟。

Conclusion: LeoTCP作为一种专为LEO卫星网络设计的数据传输协议，有效解决了该网络面临的动态挑战，在提升吞吐量的同时显著降低了延迟，性能优于现有协议。

Abstract: Low-Earth Orbit (LEO) satellite networks consist of thousands of satellites
orbiting the Earth, enabling low-latency and high-throughput communications
across the globe. Such networks present unprecedented challenges due to their
dynamic nature, which state-of-the-art data transport protocols do not address.
These challenges include: (1) non-congestive latency variation and loss, caused
by continuous satellite movement and fluctuating link quality due to weather
effects; (2) transient hotspots leading to buffer build-up, latency inflation,
and potential packet loss; and (3) frequent handovers, which may result in
temporary connectivity loss and re-routing through paths with unknown
congestion and delay characteristics. In this paper, we introduce LeoTCP, a
novel data transport protocol designed specifically to address these
challenges. LeoTCP leverages in-network telemetry (INT) to gather congestion
information on a per-hop basis. Using this information, LeoTCP (1) minimises
both buffer occupancy and latency for end users, (2) maximises application
throughput and network utilisation, and (3) swiftly reacts to network hotspots.
We compare LeoTCP to state-of-the-art data transport protocols using a LEO
satellite simulation model and targeted micro-benchmarks, both based on
OMNeT++/INET. The simulation model captures RTT dynamics in a simulated LEO
satellite constellation, while the micro-benchmarks isolate key LEO-specific
characteristics, including non-congestive latency variation and loss, path
changes, and congestion hotspots. Our results demonstrate that LeoTCP
significantly increases goodput compared to existing state-of-the-art
approaches, while simultaneously minimising latency.

</details>


### [48] [Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for Energy Efficient Multi-Operator Cellular Systems](https://arxiv.org/abs/2508.19130)
*Laura Finarelli,Maoquan Ni,Michela Meo,Falko Dressler,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文提出一个评估蜂窝网络共享策略的分析框架，利用随机几何分析表明，混合网络共享可显著节能（高达35%），同时保持服务质量，并可根据部署区域特性优化效益。


<details>
  <summary>Details</summary>
Motivation: 旨在系统评估蜂窝网络中节能且兼顾服务质量（QoS）的网络共享策略，以提升网络运营效率和可持续性。

Method: 开发了一个新颖的分析框架，该框架利用随机几何方法，系统评估了包括单运营商场景和多运营商完全集成与合作的混合策略在内的多种网络共享范式下的网络性能。框架中整合了多样的用户密度、速率要求和能耗模型，并通过法国移动网络运营商的真实数据集进行应用分析。

Result: 研究结果表明，混合网络共享策略能够实现高达35%的显著节能，同时有效维持服务质量。此外，研究还量化了网络共享的效益如何随部署区域的地理和功能特性而变化。

Conclusion: 这些发现突显了协作共享策略在提升下一代蜂窝网络的运营效率和可持续性方面的巨大潜力。

Abstract: This paper introduces a novel analytical framework for evaluating
energy-efficient, QoS-aware network-sharing strategies in cellular networks.
Leveraging stochastic geometry, our framework enables the systematic assessment
of network performance across a range of sharing paradigms, including both
conventional single-operator scenarios and advanced hybrid strategies that
enable full integration and cooperation among multiple mobile network
operators. Our framework incorporates diverse user densities, rate
requirements, and energy consumption models to ensure comprehensive analysis.
Applying our results to real-world datasets from French mobile network
operators, we demonstrate that hybrid network sharing can yield substantial
energy savings, up to $35\%$, while maintaining quality of service.
Furthermore, our results allow us to characterizing how the benefits of network
sharing vary as a function of the geographical and functional characteristics
of the deployment area. These findings highlight the potential of collaborative
sharing strategies to enhance operational efficiency and sustainability in
next-generation cellular networks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [49] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: 本文提出了一种协作框架，通过共享深度中心独立表示的知识，在DWI序列中分割缺血性卒中病灶，FedAvg模型在多个医疗中心表现出优异的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 卒中病灶分析因患者、扫描仪和专家标注的差异而高度可变，现有计算支持方法受限于单一机构数据，缺乏泛化能力，且许多临床中心缺乏足够的标注样本来调整专用解决方案。

Method: 开发了一个协作框架，通过共享深度中心独立表示的知识来分割DWI序列中的缺血性卒中病灶。具体采用了FedAvg模型，并从14个模拟医疗中心（2031项研究）获取数据进行训练和评估。

Result: FedAvg模型在所有中心实现了0.71 ± 0.24的DSC、5.29 ± 22.74的AVD、2.16 ± 3.60的ALD和0.70 ± 0.26的LF1，优于集中式和其他联邦规则。该模型展现出强大的泛化特性，在不同病灶类别中表现一致，并在未进行额外训练的分布外中心中表现可靠（DSC为0.64 ± 0.29，AVD为4.44 ± 8.74）。

Conclusion: 该协作框架通过联邦学习在DWI序列中分割缺血性卒中病灶，成功克服了机构特异性模型泛化能力差的局限，提供了一个跨中心、具有强大泛化性能的解决方案。

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>
