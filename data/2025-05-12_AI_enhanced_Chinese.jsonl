{"id": "2505.05583", "pdf": "https://arxiv.org/pdf/2505.05583", "abs": "https://arxiv.org/abs/2505.05583", "authors": ["Qianbo Zang", "Christophe Zgrzendek", "Igor Tchappi", "Afshin Khadangi", "Johannes Sedlmeir"], "title": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "Hierarchical Text Classification (HTC) involves assigning documents to labels\norganized within a taxonomy. Most previous research on HTC has focused on\nsupervised methods. However, in real-world scenarios, employing supervised HTC\ncan be challenging due to a lack of annotated data. Moreover, HTC often faces\nissues with large label spaces and long-tail distributions. In this work, we\npresent Knowledge Graphs for zero-shot Hierarchical Text Classification\n(KG-HTC), which aims to address these challenges of HTC in applications by\nintegrating knowledge graphs with Large Language Models (LLMs) to provide\nstructured semantic context during classification. Our method retrieves\nrelevant subgraphs from knowledge graphs related to the input text using a\nRetrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to\nunderstand label semantics at various hierarchy levels. We evaluate KG-HTC on\nthree open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental\nresults show that KG-HTC significantly outperforms three baselines in the\nstrict zero-shot setting, particularly achieving substantial improvements at\ndeeper levels of the hierarchy. This evaluation demonstrates the effectiveness\nof incorporating structured knowledge into LLMs to address HTC's challenges in\nlarge label spaces and long-tailed label distributions. Our code is available\nat: https://github.com/QianboZang/KG-HTC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKG-HTC\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96c6\u6210\uff0c\u4ee5\u89e3\u51b3\u5206\u5c42\u6587\u672c\u5206\u7c7b\uff08HTC\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u7a7a\u95f4\u5927\u548c\u957f\u5c3e\u5206\u5e03\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5c42\u6587\u672c\u5206\u7c7b\uff08HTC\uff09\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u76d1\u7763\u5f0fHTC\u9762\u4e34\u6311\u6218\u3002\u6b64\u5916\uff0cHTC\u8fd8\u5e38\u9047\u5230\u6807\u7b7e\u7a7a\u95f4\u5de8\u5927\u548c\u957f\u5c3e\u5206\u5e03\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u77e5\u8bc6\u56fe\u8c31\u8d4b\u80fd\u7684\u96f6\u6837\u672c\u5206\u5c42\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\uff08KG-HTC\uff09\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u601d\u8def\uff0c\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u4e0e\u8f93\u5165\u6587\u672c\u76f8\u5173\u7684\u5b50\u56fe\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u589e\u5f3aLLM\u5bf9\u4e0d\u540c\u5c42\u7ea7\u6807\u7b7e\u8bed\u4e49\u7684\u7406\u89e3\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90HTC\u6570\u636e\u96c6\uff08WoS\u3001DBpedia\u548cAmazon\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKG-HTC\u5728\u4e25\u683c\u7684\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u66f4\u6df1\u5c42\u6b21\u7684\u5206\u7c7b\u4e0a\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u6539\u8fdb\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u6765\u81ea\u77e5\u8bc6\u56fe\u8c31\uff09\u878d\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u5206\u5c42\u6587\u672c\u5206\u7c7b\u4e2d\u6807\u7b7e\u7a7a\u95f4\u5927\u548c\u957f\u5c3e\u6807\u7b7e\u5206\u5e03\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u3002"}}
{"id": "2505.05648", "pdf": "https://arxiv.org/pdf/2505.05648", "abs": "https://arxiv.org/abs/2505.05648", "authors": ["Abdelrahman Abouelenin", "Mohamed Abdelrehim", "Raffy Fahim", "Amr Hendy", "Mohamed Afify"], "title": "Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5dee\u5206\u9690\u79c1Transformer\u6a21\u578b\u7528\u4e8eSwiftKey\u7684\u8bed\u8a00\u5efa\u6a21\uff0c\u5e76\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5728\u4e0b\u4e00\u8bcd\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684GRU\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5728SwiftKey\u8f93\u5165\u6cd5\u4e2d\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u5dee\u5206\u9690\u79c1\u8981\u6c42\uff0c\u5e76\u5e73\u8861\u6a21\u578b\u5927\u5c0f\u3001\u8fd0\u884c\u901f\u5ea6\u548c\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u671f\u6539\u8fdb\u73b0\u6709\u751f\u4ea7\u73af\u5883\u4e2d\u7684GRU\u6a21\u578b\u3002", "method": "1. \u91c7\u7528Transformer\u67b6\u6784\u3002 2. \u4f7f\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u8fdb\u884c\u8bad\u7ec3\u3002 3. \u5c06GPT-2\u67b6\u6784\u7f29\u5c0f\u4ee5\u9002\u5e94\u8bbe\u5907\u8981\u6c42\u3002 4. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728\u901a\u7528\u6570\u636e\u4e0a\u8bad\u7ec3\u79cd\u5b50\u6a21\u578b\uff0c\u7136\u540e\u5728\u7528\u6237\u8f93\u5165\u6570\u636e\u4e0a\u8fdb\u884c\u5dee\u5206\u9690\u79c1\u5fae\u8c03\u3002 5. \u4f7f\u7528ONNX\u8fdb\u884c\u6a21\u578b\u96c6\u6210\u4ee5\u5b9e\u73b0\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "result": "\u4e0e\u751f\u4ea7\u73af\u5883\u4e2d\u7684GRU\u6a21\u578b\u76f8\u6bd4\uff0c\u5dee\u5206\u9690\u79c1Transformer\u6a21\u578b\u5728\u4e0b\u4e00\u8bcd\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e86\u5c0f\u5e45\u4f46\u4e00\u81f4\u7684\u63d0\u5347\uff0c\u540c\u65f6\u5185\u5b58\u5360\u7528\u548c\u8fd0\u884c\u901f\u5ea6\u7684\u589e\u957f\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "conclusion": "\u901a\u8fc7\u7f29\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u4e24\u9636\u6bb5\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\uff0c\u6210\u529f\u5c06Transformer\u6a21\u578b\u5e94\u7528\u4e8eSwiftKey\u8bed\u8a00\u5efa\u6a21\uff0c\u5728\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u548c\u8d44\u6e90\u9650\u5236\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.05687", "pdf": "https://arxiv.org/pdf/2505.05687", "abs": "https://arxiv.org/abs/2505.05687", "authors": ["Cindy Kim", "Daniela Puchall", "Jiangyi Liang", "Jiwon Kim"], "title": "Exploration of COVID-19 Discourse on Twitter: American Politician Edition", "categories": ["cs.CL"], "comment": null, "summary": "The advent of the COVID-19 pandemic has undoubtedly affected the political\nscene worldwide and the introduction of new terminology and public opinions\nregarding the virus has further polarized partisan stances. Using a collection\nof tweets gathered from leading American political figures online (Republican\nand Democratic), we explored the partisan differences in approach, response,\nand attitude towards handling the international crisis. Implementation of the\nbag-of-words, bigram, and TF-IDF models was used to identify and analyze\nkeywords, topics, and overall sentiments from each party. Results suggest that\nDemocrats are more concerned with the casualties of the pandemic, and give more\nmedical precautions and recommendations to the public whereas Republicans are\nmore invested in political responsibilities such as keeping the public updated\nthrough media and carefully watching the progress of the virus. We propose a\nsystematic approach to predict and distinguish a tweet's political stance (left\nor right leaning) based on its COVID-19 related terms using different\nclassification algorithms on different language models.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u7f8e\u56fd\u653f\u5ba2\u7684\u63a8\u6587\uff0c\u7814\u7a76\u4e86COVID-19\u75ab\u60c5\u671f\u95f4\u4e24\u515a\u7684\u4e0d\u540c\u5173\u6ce8\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u6587\u5185\u5bb9\u9884\u6d4b\u653f\u6cbb\u7acb\u573a\u7684\u65b9\u6cd5\u3002", "motivation": "COVID-19\u75ab\u60c5\u5f71\u54cd\u4e86\u653f\u6cbb\u683c\u5c40\u5e76\u52a0\u5267\u4e86\u515a\u6d3e\u5206\u5316\uff0c\u9700\u8981\u63a2\u7a76\u4e24\u515a\u5728\u5e94\u5bf9\u5371\u673a\u65f6\u7684\u4e0d\u540c\u65b9\u6cd5\u3001\u53cd\u5e94\u548c\u6001\u5ea6\u3002", "method": "\u6536\u96c6\u7f8e\u56fd\u4e3b\u8981\u653f\u6cbb\u4eba\u7269\u7684\u63a8\u6587\uff0c\u4f7f\u7528\u8bcd\u888b\u6a21\u578b\u3001bigram\u6a21\u578b\u548cTF-IDF\u6a21\u578b\u5206\u6790\u5173\u952e\u8bcd\u3001\u4e3b\u9898\u548c\u60c5\u7eea\u3002\u63d0\u51fa\u4f7f\u7528\u5206\u7c7b\u7b97\u6cd5\u57fa\u4e8eCOVID-19\u76f8\u5173\u672f\u8bed\u9884\u6d4b\u63a8\u6587\u7684\u653f\u6cbb\u7acb\u573a\u3002", "result": "\u6c11\u4e3b\u515a\u66f4\u5173\u6ce8\u75ab\u60c5\u4f24\u4ea1\u548c\u533b\u7597\u5efa\u8bae\uff0c\u800c\u5171\u548c\u515a\u66f4\u5173\u6ce8\u653f\u6cbb\u8d23\u4efb\u3001\u5a92\u4f53\u66f4\u65b0\u548c\u75c5\u6bd2\u8fdb\u5c55\u76d1\u63a7\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u63a8\u6587\u4e2d\u4e0eCOVID-19\u76f8\u5173\u7684\u672f\u8bed\uff0c\u5229\u7528\u5206\u7c7b\u7b97\u6cd5\u9884\u6d4b\u5176\u653f\u6cbb\u503e\u5411\u3002"}}
{"id": "2505.05704", "pdf": "https://arxiv.org/pdf/2505.05704", "abs": "https://arxiv.org/abs/2505.05704", "authors": ["Julia Shuieh", "Prasann Singhal", "Apaar Shanker", "John Heyer", "George Pu", "Samuel Denton"], "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning", "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cdLLM\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08SFT, DPO, KTO\uff09\u5728\u4e0d\u540c\u865a\u5047\u76f8\u5173\u6027\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u662f\u6700\u4f18\u7684\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684LLM\u8bad\u7ec3\u6570\u636e\u5e38\u5305\u542b\u865a\u5047\u76f8\u5173\u6027\uff08\u5982\u504f\u89c1\u3001\u6570\u636e\u96c6\u4f2a\u5f71\uff09\uff0c\u8fd9\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728\u5305\u542b\u4e0d\u540c\u7a0b\u5ea6\uff0810% vs 90%\uff09\u548c\u7c7b\u578b\uff08\u7279\u5f81\u6a21\u7cca\u6027 vs \u5206\u5e03\u72ed\u7a84\u6027\uff09\u865a\u5047\u76f8\u5173\u6027\u7684\u5408\u6210\u4efb\u52a1\uff08\u6570\u5b66\u63a8\u7406\u3001\u53d7\u9650\u6307\u4ee4\u9075\u5faa\u3001\u6587\u6863\u95ee\u7b54\uff09\u4e0a\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548cKahneman-Tversky\u4f18\u5316\uff08KTO\uff09\u4e09\u79cd\u540e\u8bad\u7ec3\u7b97\u6cd5\u3002", "result": "\u6a21\u578b\u6027\u80fd\u901a\u5e38\uff08\u4f46\u4e0d\u603b\u662f\uff09\u5728\u66f4\u9ad8\u865a\u5047\u76f8\u5173\u6027\u4e0b\u964d\u4f4e\u3002\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\uff08DPO/KTO\uff09\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u76f8\u5bf9\u9c81\u68d2\u6027\uff0c\u800cSFT\u5728\u590d\u6742\u7684\u3001\u4e0a\u4e0b\u6587\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u4fdd\u6301\u66f4\u5f3a\u6027\u80fd\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u7684\u540e\u8bad\u7ec3\u7b56\u7565\u80fd\u5728\u6240\u6709\u573a\u666f\u4e2d\u666e\u904d\u80dc\u51fa\u3002\u6700\u4f73\u65b9\u6cd5\u7684\u9009\u62e9\u53d6\u51b3\u4e8e\u76ee\u6807\u4efb\u52a1\u7684\u7c7b\u578b\u548c\u865a\u5047\u76f8\u5173\u6027\u7684\u6027\u8d28\u3002"}}
{"id": "2505.05541", "pdf": "https://arxiv.org/pdf/2505.05541", "abs": "https://arxiv.org/abs/2505.05541", "authors": ["Markov Grey", "Charbel-Rapha\u00ebl Segerie"], "title": "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods", "categories": ["cs.AI"], "comment": null, "summary": "As frontier AI systems advance toward transformative capabilities, we need a\nparallel transformation in how we measure and evaluate these systems to ensure\nsafety and inform governance. While benchmarks have been the primary method for\nestimating model capabilities, they often fail to establish true upper bounds\nor predict deployment behavior. This literature review consolidates the rapidly\nevolving field of AI safety evaluations, proposing a systematic taxonomy around\nthree dimensions: what properties we measure, how we measure them, and how\nthese measurements integrate into frameworks. We show how evaluations go beyond\nbenchmarks by measuring what models can do when pushed to the limit\n(capabilities), the behavioral tendencies exhibited by default (propensities),\nand whether our safety measures remain effective even when faced with\nsubversive adversarial AI (control). These properties are measured through\nbehavioral techniques like scaffolding, red teaming and supervised fine-tuning,\nalongside internal techniques such as representation analysis and mechanistic\ninterpretability. We provide deeper explanations of some safety-critical\ncapabilities like cybersecurity exploitation, deception, autonomous\nreplication, and situational awareness, alongside concerning propensities like\npower-seeking and scheming. The review explores how these evaluation methods\nintegrate into governance frameworks to translate results into concrete\ndevelopment decisions. We also highlight challenges to safety evaluations -\nproving absence of capabilities, potential model sandbagging, and incentives\nfor \"safetywashing\" - while identifying promising research directions. By\nsynthesizing scattered resources, this literature review aims to provide a\ncentral reference point for understanding AI safety evaluations.", "AI": {"tldr": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u524d\u6cbfAI\u7684\u5b89\u5168\u6027\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cf\u5176\u80fd\u529b\u3001\u503e\u5411\u548c\u63a7\u5236\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u5e76\u6307\u5bfc\u6cbb\u7406\u3002", "motivation": "\u968f\u7740\u524d\u6cbfAI\u7cfb\u7edf\u80fd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u4f20\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u5df2\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5176\u6f5c\u5728\u98ce\u9669\u548c\u5b9e\u9645\u90e8\u7f72\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u6765\u6307\u5bfc\u6cbb\u7406\u3002", "method": "\u672c\u6587\u8fdb\u884c\u6587\u732e\u7efc\u8ff0\uff0c\u6574\u5408AI\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff1a\u8bc4\u4f30\u4ec0\u4e48\u7279\u6027\uff08\u80fd\u529b\u3001\u503e\u5411\u3001\u63a7\u5236\uff09\u3001\u5982\u4f55\u8bc4\u4f30\uff08\u884c\u4e3a\u6280\u672f\u548c\u5185\u90e8\u6280\u672f\uff09\u4ee5\u53ca\u5982\u4f55\u5c06\u8bc4\u4f30\u7ed3\u679c\u6574\u5408\u5230\u6cbb\u7406\u6846\u67b6\u4e2d\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86\u8bc4\u4f30\u5982\u4f55\u8d85\u8d8a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6df1\u5165\u5206\u6790\u4e86\u7f51\u7edc\u5b89\u5168\u5229\u7528\u3001\u6b3a\u9a97\u3001\u81ea\u4e3b\u590d\u5236\u3001\u6001\u52bf\u611f\u77e5\u7b49\u5173\u952e\u80fd\u529b\uff0c\u4ee5\u53ca\u6743\u529b\u5bfb\u6c42\u7b49\u4ee4\u4eba\u62c5\u5fe7\u7684\u503e\u5411\u3002\u8ba8\u8bba\u4e86\u8bc4\u4f30\u65b9\u6cd5\u5982\u4f55\u878d\u5165\u6cbb\u7406\uff0c\u5e76\u6307\u51fa\u4e86\u8bc4\u4f30\u9762\u4e34\u7684\u6311\u6218\uff08\u5982\u8bc1\u660e\u80fd\u529b\u7f3a\u5931\u3001\u6a21\u578b\u201c\u88c5\u50bb\u201d\u3001\u5b89\u5168\u6e05\u6d17\uff09\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u4e3a\u4e86\u786e\u4fddAI\u5b89\u5168\uff0c\u9700\u8981\u4ece\u8861\u91cf\u80fd\u529b\u3001\u503e\u5411\u548c\u63a7\u5236\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u91c7\u7528\u884c\u4e3a\u548c\u5185\u90e8\u6280\u672f\u76f8\u7ed3\u5408\u7684\u7efc\u5408\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53d6\u4ee3\u4ec5\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u7684\u4f20\u7edf\u65b9\u5f0f\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7406\u89e3AI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e00\u4e2a\u6838\u5fc3\u53c2\u8003\u3002"}}
{"id": "2505.05487", "pdf": "https://arxiv.org/pdf/2505.05487", "abs": "https://arxiv.org/abs/2505.05487", "authors": ["Shrinivas Pundlik", "Seonggyu Choe", "Patrick Baker", "Chen-Yuan Lee", "Naser Al-Madi", "Alex R. Bowers", "Gang Luo"], "title": "Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving", "categories": ["cs.CV", "cs.RO"], "comment": "19 pages, 11 figures", "summary": "Naturalistic driving studies use devices in participants' own vehicles to\nrecord daily driving over many months. Due to diverse and extensive amounts of\ndata recorded, automated processing is necessary. This report describes methods\nto extract and characterize driver head scans at intersections from data\ncollected from an in-car recording system that logged vehicle speed, GPS\nlocation, scene videos, and cabin videos. Custom tools were developed to mark\nthe intersections, synchronize location and video data, and clip the cabin and\nscene videos for +/-100 meters from the intersection location. A\ncustom-developed head pose detection AI model for wide angle head turns was run\non the cabin videos to estimate the driver head pose, from which head scans >20\ndeg were computed in the horizontal direction. The scene videos were processed\nusing a YOLO object detection model to detect traffic lights, stop signs,\npedestrians, and other vehicles on the road. Turning maneuvers were\nindependently detected using vehicle self-motion patterns. Stop lines on the\nroad surface were detected using changing intensity patterns over time as the\nvehicle moved. The information obtained from processing the scene videos, along\nwith the speed data was used in a rule-based algorithm to infer the\nintersection type, maneuver, and bounds. We processed 190 intersections from 3\nvehicles driven in cities and suburban areas from Massachusetts and California.\nThe automated video processing algorithm correctly detected intersection\nsignage and maneuvers in 100% and 94% of instances, respectively. The median\n[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]\nmeters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and\nestimated intersection bounds was 0.88[0.82-0.93].", "AI": {"tldr": "\u8be5\u62a5\u544a\u63cf\u8ff0\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u8868\u5f81\u9a7e\u9a76\u5458\u5728\u4ea4\u53c9\u8def\u53e3\u7684\u5934\u90e8\u626b\u63cf\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u573a\u666f\u89c6\u9891\u4fe1\u606f\u63a8\u65ad\u4ea4\u53c9\u8def\u53e3\u7c7b\u578b\u548c\u9a7e\u9a76\u884c\u4e3a\u3002", "motivation": "\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u4ea7\u751f\u6d77\u91cf\u591a\u6837\u7684\u6570\u636e\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5904\u7406\u65b9\u6cd5\u6765\u5206\u6790\u9a7e\u9a76\u5458\u884c\u4e3a\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u7406\u89e3\u9a7e\u9a76\u5458\u5728\u4ea4\u53c9\u8def\u53e3\u5982\u4f55\u901a\u8fc7\u5934\u90e8\u626b\u63cf\u89c2\u5bdf\u73af\u5883\u5e76\u4e0e\u4ea4\u901a\u72b6\u51b5\u4e92\u52a8\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u8f66\u8f7d\u8bb0\u5f55\u7cfb\u7edf\uff08\u8bb0\u5f55\u901f\u5ea6\u3001GPS\u3001\u573a\u666f\u89c6\u9891\u3001\u5ea7\u8231\u89c6\u9891\uff09\u6536\u96c6\u6570\u636e\u3002\u5f00\u53d1\u4e86\u5b9a\u5236\u5de5\u5177\u8fdb\u884c\u4ea4\u53c9\u8def\u53e3\u6807\u8bb0\u3001\u6570\u636e\u540c\u6b65\u548c\u89c6\u9891\u88c1\u526a\u3002\u91c7\u7528\u5b9a\u5236\u7684\u5934\u90e8\u59ff\u6001\u68c0\u6d4bAI\u6a21\u578b\u5206\u6790\u5ea7\u8231\u89c6\u9891\u4ee5\u4f30\u8ba1\u5934\u90e8\u626b\u63cf\u3002\u4f7f\u7528YOLO\u6a21\u578b\u5904\u7406\u573a\u666f\u89c6\u9891\u4ee5\u68c0\u6d4b\u4ea4\u901a\u4fe1\u53f7\u706f\u3001\u6807\u5fd7\u3001\u884c\u4eba\u548c\u5176\u4ed6\u8f66\u8f86\u3002\u901a\u8fc7\u8f66\u8f86\u8fd0\u52a8\u6a21\u5f0f\u68c0\u6d4b\u8f6c\u5411\u884c\u4e3a\uff0c\u901a\u8fc7\u56fe\u50cf\u5f3a\u5ea6\u53d8\u5316\u68c0\u6d4b\u505c\u6b62\u7ebf\u3002\u6700\u540e\uff0c\u7ed3\u5408\u573a\u666f\u89c6\u9891\u4fe1\u606f\u548c\u901f\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\u63a8\u65ad\u4ea4\u53c9\u8def\u53e3\u7c7b\u578b\u3001\u9a7e\u9a76\u884c\u4e3a\u548c\u8fb9\u754c\u3002", "result": "\u5728\u5904\u7406\u7684190\u4e2a\u4ea4\u53c9\u8def\u53e3\u6570\u636e\u4e2d\uff0c\u81ea\u52a8\u5316\u89c6\u9891\u5904\u7406\u7b97\u6cd5\u5728\u68c0\u6d4b\u4ea4\u53c9\u8def\u53e3\u6807\u5fd7\u65b9\u9762\u8fbe\u5230100%\u7684\u51c6\u786e\u7387\uff0c\u5728\u68c0\u6d4b\u9a7e\u9a76\u884c\u4e3a\u65b9\u9762\u8fbe\u523094%\u7684\u51c6\u786e\u7387\u3002\u68c0\u6d4b\u8f66\u8f86\u8fdb\u5165\u4ea4\u53c9\u8def\u53e3\u7684\u4e2d\u4f4d\u8bef\u5dee\u4e3a1.1\u7c73\u548c0.2\u79d2\u3002\u5730\u9762\u5b9e\u51b5\u4e0e\u4f30\u8ba1\u7684\u4ea4\u53c9\u8def\u53e3\u8fb9\u754c\u4e4b\u95f4\u7684\u4e2d\u4f4d\u91cd\u53e0\u5ea6\u4e3a0.88\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u4e2d\u51c6\u786e\u63d0\u53d6\u9a7e\u9a76\u5458\u5934\u90e8\u626b\u63cf\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u591a\u6e90\u6570\u636e\u63a8\u65ad\u4ea4\u53c9\u8def\u53e3\u7279\u5f81\u4e0e\u9a7e\u9a76\u884c\u4e3a\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u6790\u9a7e\u9a76\u5458\u5728\u4ea4\u53c9\u8def\u53e3\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.05537", "pdf": "https://arxiv.org/pdf/2505.05537", "abs": "https://arxiv.org/abs/2505.05537", "authors": ["Hamed Alimohammadi", "Sotiris Chatzimiltis", "Samara Mayhoub", "Mohammad Shojafar", "Seyed Ahmad Soleymani", "Ayhan Akbas", "Chuan Heng Foh"], "title": "KPI Poisoning: An Attack in Open RAN Near Real-Time Control Loop", "categories": ["cs.NI"], "comment": "Presented at IEEE FNWF 2024", "summary": "Open Radio Access Network (Open RAN) is a new paradigm to provide fundamental\nfeatures for supporting next-generation mobile networks. Disaggregation,\nvirtualisation, closed-loop data-driven control, and open interfaces bring\nflexibility and interoperability to the network deployment. However, these\nfeatures also create a new surface for security threats. In this paper, we\nintroduce Key Performance Indicators (KPIs) poisoning attack in Near Real-Time\ncontrol loops as a new form of threat that can have significant effects on the\nOpen RAN functionality. This threat can arise from traffic spoofing on the E2\ninterface or compromised E2 nodes. The role of KPIs is explored in the use\ncases of Near Real-Time control loops. Then, the potential impacts of the\nattack are analysed. An ML-based approach is proposed to detect poisoned KPI\nvalues before using them in control loops. Emulations are conducted to generate\nKPI reports and inject anomalies into the values. A Long Short-Term Memory\n(LSTM) neural network model is used to detect anomalies. The results show that\nmore amplified injected values are more accessible to detect, and using more\nreport sequences leads to better performance in anomaly detection, with\ndetection rates improving from 62% to 99%.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Open RAN\u4e2d\u7684\u4e00\u79cd\u65b0\u578bKPI\u6295\u6bd2\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u68c0\u6d4b\u6b64\u653b\u51fb\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "Open RAN\u7684\u65b0\u7279\u6027\u5728\u5e26\u6765\u7075\u6d3b\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u7684\u540c\u65f6\uff0c\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8fd1\u5b9e\u65f6\u63a7\u5236\u73af\u8def\u4e2d\u5173\u952e\u6027\u80fd\u6307\u6807 (KPI) \u7684\u6295\u6bd2\u653b\u51fb\uff0c\u8fd9\u79cd\u653b\u51fb\u53ef\u80fd\u6e90\u4e8eE2\u63a5\u53e3\u6d41\u91cf\u6b3a\u9a97\u6216\u53d7\u635fE2\u8282\u70b9\uff0c\u5bf9\u7f51\u7edc\u529f\u80fd\u6784\u6210\u663e\u8457\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u957f\u77ed\u671f\u8bb0\u5fc6 (LSTM) \u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u4eff\u771f\u751f\u6210KPI\u62a5\u544a\u5e76\u6ce8\u5165\u5f02\u5e38\u503c\uff0c\u6765\u68c0\u6d4b\u63a7\u5236\u73af\u8def\u4e2d\u4f7f\u7528\u524d\u7684\u88ab\u6295\u6bd2KPI\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6ce8\u5165\u7684\u5f02\u5e38\u503c\u8d8a\u660e\u663e\uff0c\u8d8a\u5bb9\u6613\u88ab\u68c0\u6d4b\u5230\uff1b\u4f7f\u7528\u66f4\u591a\u7684\u5386\u53f2KPI\u62a5\u544a\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3\u548c\u68c0\u6d4b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u68c0\u6d4b\u7387\u4ece62%\u63d0\u5347\u81f399%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLSTM\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4bOpen RAN\u4e2d\u8fd1\u5b9e\u65f6\u63a7\u5236\u73af\u8def\u7684KPI\u6295\u6bd2\u653b\u51fb\uff0c\u5176\u68c0\u6d4b\u6548\u679c\u53d7\u5f02\u5e38\u5f3a\u5ea6\u548c\u6240\u7528\u6570\u636e\u5e8f\u5217\u957f\u5ea6\u7684\u5f71\u54cd\uff0c\u4e3a\u589e\u5f3aOpen RAN\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u68c0\u6d4b\u673a\u5236\u3002"}}
{"id": "2505.05522", "pdf": "https://arxiv.org/pdf/2505.05522", "abs": "https://arxiv.org/abs/2505.05522", "authors": ["Luke Darlow", "Ciaran Regan", "Sebastian Risi", "Jeffrey Seely", "Llion Jones"], "title": "Continuous Thought Machines", "categories": ["cs.LG", "cs.AI"], "comment": "Technical report accompanied by online project page", "summary": "Biological brains demonstrate complex neural activity, where the timing and\ninterplay between neurons is critical to how brains process information. Most\ndeep learning architectures simplify neural activity by abstracting away\ntemporal dynamics. In this paper we challenge that paradigm. By incorporating\nneuron-level processing and synchronization, we can effectively reintroduce\nneural timing as a foundational element. We present the Continuous Thought\nMachine (CTM), a model designed to leverage neural dynamics as its core\nrepresentation. The CTM has two core innovations: (1) neuron-level temporal\nprocessing, where each neuron uses unique weight parameters to process a\nhistory of incoming signals; and (2) neural synchronization employed as a\nlatent representation. The CTM aims to strike a balance between oversimplified\nneuron abstractions that improve computational efficiency, and biological\nrealism. It operates at a level of abstraction that effectively captures\nessential temporal dynamics while remaining computationally tractable for deep\nlearning. We demonstrate the CTM's strong performance and versatility across a\nrange of challenging tasks, including ImageNet-1K classification, solving 2D\nmazes, sorting, parity computation, question-answering, and RL tasks. Beyond\ndisplaying rich internal representations and offering a natural avenue for\ninterpretation owing to its internal process, the CTM is able to perform tasks\nthat require complex sequential reasoning. The CTM can also leverage adaptive\ncompute, where it can stop earlier for simpler tasks, or keep computing when\nfaced with more challenging instances. The goal of this work is to share the\nCTM and its associated innovations, rather than pushing for new\nstate-of-the-art results. To that end, we believe the CTM represents a\nsignificant step toward developing more biologically plausible and powerful\nartificial intelligence systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8fde\u7eed\u601d\u7ef4\u673a (CTM) \u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u795e\u7ecf\u5143\u7ea7\u7684\u65f6\u95f4\u5904\u7406\u548c\u795e\u7ecf\u540c\u6b65\u6765\u6a21\u62df\u5927\u8111\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5177\u751f\u7269\u5408\u7406\u6027\u7684\u4eba\u5de5\u667a\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5927\u591a\u7b80\u5316\u4e86\u795e\u7ecf\u6d3b\u52a8\uff0c\u5ffd\u7565\u4e86\u751f\u7269\u5927\u8111\u4e2d\u5bf9\u4e8e\u4fe1\u606f\u5904\u7406\u81f3\u5173\u91cd\u8981\u7684\u795e\u7ecf\u5143\u65f6\u5e8f\u548c\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u8fde\u7eed\u601d\u7ef4\u673a (CTM)\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u662f\uff1a1) \u795e\u7ecf\u5143\u7ea7\u65f6\u95f4\u5904\u7406\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u4f7f\u7528\u72ec\u7279\u6743\u91cd\u5904\u7406\u8f93\u5165\u4fe1\u53f7\u5386\u53f2\uff1b2) \u4f7f\u7528\u795e\u7ecf\u540c\u6b65\u4f5c\u4e3a\u6f5c\u5728\u8868\u793a\u3002", "result": "CTM \u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8ff7\u5bab\u6c42\u89e3\u3001\u6392\u5e8f\u3001\u5947\u5076\u6821\u9a8c\u3001\u95ee\u7b54\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u5c55\u793a\u4e86\u4e30\u5bcc\u7684\u5185\u90e8\u8868\u793a\uff0c\u6613\u4e8e\u89e3\u91ca\uff0c\u5e76\u80fd\u8fdb\u884c\u590d\u6742\u987a\u5e8f\u63a8\u7406\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u3002", "conclusion": "CTM \u662f\u671d\u7740\u5f00\u53d1\u66f4\u5177\u751f\u7269\u5408\u7406\u6027\u548c\u66f4\u5f3a\u5927\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u5176\u76ee\u6807\u662f\u5206\u4eab\u6a21\u578b\u53ca\u5176\u521b\u65b0\uff0c\u800c\u975e\u8ffd\u6c42\u5f53\u524d\u6700\u4f18\u7ed3\u679c\u3002"}}
{"id": "2505.05714", "pdf": "https://arxiv.org/pdf/2505.05714", "abs": "https://arxiv.org/abs/2505.05714", "authors": ["Jinze Lv", "Jian Chen", "Zi Long", "Xianghua Fu", "Yin Chen"], "title": "TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries", "categories": ["cs.CL"], "comment": "NLDB 2025", "summary": "Most existing multimodal machine translation (MMT) datasets are predominantly\ncomposed of static images or short video clips, lacking extensive video data\nacross diverse domains and topics. As a result, they fail to meet the demands\nof real-world MMT tasks, such as documentary translation. In this study, we\ndeveloped TopicVD, a topic-based dataset for video-supported multimodal machine\ntranslation of documentaries, aiming to advance research in this field. We\ncollected video-subtitle pairs from documentaries and categorized them into\neight topics, such as economy and nature, to facilitate research on domain\nadaptation in video-guided MMT. Additionally, we preserved their contextual\ninformation to support research on leveraging the global context of\ndocumentaries in video-guided MMT. To better capture the shared semantics\nbetween text and video, we propose an MMT model based on a cross-modal\nbidirectional attention module. Extensive experiments on the TopicVD dataset\ndemonstrate that visual information consistently improves the performance of\nthe NMT model in documentary translation. However, the MMT model's performance\nsignificantly declines in out-of-domain scenarios, highlighting the need for\neffective domain adaptation methods. Additionally, experiments demonstrate that\nglobal context can effectively improve translation performance. % Dataset and\nour implementations are available at https://github.com/JinzeLv/TopicVD", "AI": {"tldr": "\u7814\u7a76\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u9898\u7684\u7eaa\u5f55\u7247\u89c6\u9891\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6TopicVD\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u53cc\u5411\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u89c6\u89c9\u4fe1\u606f\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u80fd\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u4f46\u4e5f\u5b58\u5728\u9886\u57df\u5916\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\uff08MMT\uff09\u6570\u636e\u96c6\u5927\u591a\u662f\u9759\u6001\u56fe\u50cf\u6216\u77ed\u89c6\u9891\u7247\u6bb5\uff0c\u7f3a\u4e4f\u8986\u76d6\u591a\u79cd\u9886\u57df\u548c\u4e3b\u9898\u7684\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7eaa\u5f55\u7247\u7ffb\u8bd1\u7b49\u771f\u5b9e\u4e16\u754cMMT\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u9898\u7684\u7eaa\u5f55\u7247\u89c6\u9891-\u5b57\u5e55\u5bf9\u6570\u636e\u96c6TopicVD\uff0c\u5305\u542b8\u4e2a\u4e3b\u9898\u7c7b\u522b\uff0c\u5e76\u4fdd\u7559\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002 2. \u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8de8\u6a21\u6001\u53cc\u5411\u6ce8\u610f\u529b\u6a21\u5757\u7684MMT\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u6587\u672c\u548c\u89c6\u9891\u4e4b\u95f4\u7684\u5171\u4eab\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1. \u5728TopicVD\u6570\u636e\u96c6\u4e0a\uff0c\u89c6\u89c9\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u7eaa\u5f55\u7247\u7ffb\u8bd1\u7684\u6027\u80fd\u3002 2. MMT\u6a21\u578b\u5728\u9886\u57df\u5916\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u9700\u8981\u6709\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u3002 3. \u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "TopicVD\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6a21\u578b\u63a8\u52a8\u4e86\u89c6\u9891\u6307\u5bfc\u7684MMT\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u7eaa\u5f55\u7247\u9886\u57df\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u89c6\u89c9\u4fe1\u606f\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u9886\u57df\u81ea\u9002\u5e94\u662f\u672a\u6765\u9700\u8981\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2505.05602", "pdf": "https://arxiv.org/pdf/2505.05602", "abs": "https://arxiv.org/abs/2505.05602", "authors": ["Lennart Luettgau", "Harry Coppock", "Magda Dubois", "Christopher Summerfield", "Cozmin Ududec"], "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics", "categories": ["cs.AI", "stat.AP"], "comment": "23 pages, 9 figures", "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.", "AI": {"tldr": "\u63d0\u51fa HiBayES\uff0c\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u7a33\u5065\u5730\u8bc4\u4f30 AI \u7cfb\u7edf\uff08\u5c24\u5176\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\uff09\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u7b49 AI \u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7a33\u5065\u5730\u8bc4\u4f30\u5176\u80fd\u529b\uff08\u57fa\u4e8e\u968f\u673a\u8f93\u51fa\uff09\uff0c\u7cfb\u7edf\u5730\u91cf\u5316\u8bc4\u4f30\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5e94\u5bf9\u590d\u6742\u3001\u9ad8\u6210\u672c\u3001\u4f4e\u6570\u636e\u91cf\u7684\u8bc4\u4f30\u6311\u6218\u3002", "method": "\u5f15\u5165 HiBayES\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b (GLMs)\u3001\u8d1d\u53f6\u65af\u6570\u636e\u5206\u6790\u548c\u5f62\u5f0f\u5316\u6a21\u578b\u6bd4\u8f83\u7684\u53ef\u63a8\u5e7f\u7684\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e AI \u8bc4\u4f30\u7edf\u8ba1\u3002", "result": "HiBayES \u652f\u6301\u5728\u7ecf\u5178\u95ee\u7b54\u57fa\u51c6\u548c\u9ad8\u7ea7\u667a\u80fd\u4f53\u8bc4\u4f30\u4e2d\u8fdb\u884c\u7a33\u5065\u7684\u63a8\u65ad\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6570\u636e\u573a\u666f\uff08\u4f8b\u5982\uff0c\u6bcf\u6b21\u8bc4\u4f30 < 20 \u4e2a\u6570\u636e\u70b9\uff09\u4e0b\u3002\u5b83\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u7a33\u5065\u7684\u53c2\u6570\u4f30\u8ba1\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a HiBayES \u8f6f\u4ef6\u5305\u3002", "conclusion": "HiBayES \u662f\u4e00\u4e2a\u901a\u7528\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9 AI \u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u968f\u673a\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u8bc4\u4f30\u7ed3\u6784\u590d\u6742\u3001\u6210\u672c\u9ad8\u4ee5\u53ca\u6570\u636e\u91cf\u6709\u9650\u7b49\u6311\u6218\u3002"}}
{"id": "2505.05488", "pdf": "https://arxiv.org/pdf/2505.05488", "abs": "https://arxiv.org/abs/2505.05488", "authors": ["Yunfan Lu", "Xiaogang Xu", "Pengteng Li", "Yusheng Wang", "Yi Cui", "Huizai Yao", "Hui Xiong"], "title": "From Events to Enhancement: A Survey on Event-Based Imaging Technologies", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras offering high dynamic range and low latency have emerged as\ndisruptive technologies in imaging. Despite growing research on leveraging\nthese benefits for different imaging tasks, a comprehensive study of recently\nadvances and challenges are still lacking. This limits the broader\nunderstanding of how to utilize events in universal imaging applications. In\nthis survey, we first introduce a physical model and the characteristics of\ndifferent event sensors as the foundation. Following this, we highlight the\nadvancement and interaction of image/video enhancement tasks with events.\nAdditionally, we explore advanced tasks, which capture richer light information\nwith events, \\eg~light field estimation, multi-view generation, and\nphotometric. Finally, we discuss new challenges and open questions offering a\nperspective for this rapidly evolving field. More continuously updated\nresources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u4e8b\u4ef6\u76f8\u673a\u5728\u6210\u50cf\u9886\u57df\u5e94\u7528\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u603b\u7ed3\u4e86\u5176\u57fa\u7840\u3001\u5728\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u548c\u9ad8\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u4e8b\u4ef6\u76f8\u673a\u7814\u7a76\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\u7684\u5168\u9762\u68b3\u7406\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u4eec\u5bf9\u5176\u5728\u901a\u7528\u6210\u50cf\u5e94\u7528\u4e2d\u6f5c\u529b\u7684\u5e7f\u6cdb\u7406\u89e3\u548c\u5229\u7528\u3002", "method": "\u9996\u5148\u4ecb\u7ecd\u4e8b\u4ef6\u76f8\u673a\u7684\u7269\u7406\u6a21\u578b\u548c\u7279\u6027\uff0c\u63a5\u7740\u56de\u987e\u5176\u5728\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u548c\u9ad8\u7ea7\u6210\u50cf\u4efb\u52a1\uff08\u5982\u5149\u573a\u4f30\u8ba1\u3001\u591a\u89c6\u56fe\u751f\u6210\u3001\u5149\u5ea6\u6d4b\u91cf\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u6700\u540e\u8ba8\u8bba\u8be5\u9886\u57df\u7684\u65b0\u6311\u6218\u548c\u5f00\u653e\u6027\u95ee\u9898\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e8b\u4ef6\u76f8\u673a\u6210\u50cf\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u57fa\u7840\u77e5\u8bc6\u3001\u4e0e\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u7684\u4ea4\u4e92\u3001\u5728\u9ad8\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5c1a\u5f85\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u662f\u5177\u6709\u98a0\u8986\u6027\u7684\u6210\u50cf\u6280\u672f\uff0c\u672c\u7efc\u8ff0\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u7814\u7a76\u73b0\u72b6\u548c\u6311\u6218\uff0c\u4e3a\u8be5\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u89c6\u89d2\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05996", "pdf": "https://arxiv.org/pdf/2505.05996", "abs": "https://arxiv.org/abs/2505.05996", "authors": ["Garegin Grigoryan", "Kevin Penkowski", "Minseok Kwon"], "title": "P4Kube: In-Network Load Balancer for Kubernetes", "categories": ["cs.NI"], "comment": null, "summary": "Kubernetes Services such as LoadBalancer and NodePort expose applications\nrunning on pods within a Kubernetes cluster to external users. While the\nLoadBalancer Service requires an external load-balancing middleware, its\nalternative, NodePort Service, adds additional hops on the path between clients\nand the worker nodes. In this paper, we propose P4Kube, a framework consisting\nof a P4 data plane program and a Kubernetes plugin. Our solution effectively\nperforms load balancing of requests to the worker nodes of a cluster based on\nthe number of running replicas. In P4Kube, the data packets completely bypass\nthe system's control plane. Unlike the previous work, to update its state, the\nP4Kube data plane works directly with the Kubernetes control plane without any\ninvolvement of the network control plane. Our experiments show up to 50%\nimprovement in the average request time to the cluster compared to conventional\napproaches.", "AI": {"tldr": "P4Kube\u662f\u4e00\u4e2a\u7ed3\u5408P4\u6570\u636e\u5e73\u9762\u548cKubernetes\u63d2\u4ef6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8d1f\u8f7d\u5747\u8861\u548c\u7ed5\u8fc7\u7cfb\u7edf\u63a7\u5236\u5e73\u9762\uff0c\u63d0\u9ad8\u4e86Kubernetes\u670d\u52a1\u7684\u8bf7\u6c42\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684Kubernetes\u670d\u52a1\u66b4\u9732\u673a\u5236\uff08LoadBalancer\u548cNodePort\uff09\u8981\u4e48\u9700\u8981\u5916\u90e8\u8d1f\u8f7d\u5747\u8861\u5668\uff0c\u8981\u4e48\u4f1a\u589e\u52a0\u7f51\u7edc\u5ef6\u8fdf\uff0c\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86P4Kube\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2aP4\u6570\u636e\u5e73\u9762\u7a0b\u5e8f\u548c\u4e00\u4e2aKubernetes\u63d2\u4ef6\u3002\u5b83\u6839\u636e\u8fd0\u884c\u7684\u526f\u672c\u6570\u91cf\u8fdb\u884c\u8d1f\u8f7d\u5747\u8861\uff0c\u6570\u636e\u5305\u5b8c\u5168\u7ed5\u8fc7\u7cfb\u7edf\u63a7\u5236\u5e73\u9762\uff0cP4\u6570\u636e\u5e73\u9762\u76f4\u63a5\u4e0eKubernetes\u63a7\u5236\u5e73\u9762\u901a\u4fe1\u4ee5\u66f4\u65b0\u72b6\u6001\uff0c\u65e0\u9700\u7f51\u7edc\u63a7\u5236\u5e73\u9762\u7684\u4ecb\u5165\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cP4Kube\u80fd\u591f\u5c06\u5230\u96c6\u7fa4\u7684\u5e73\u5747\u8bf7\u6c42\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe50%\u3002", "conclusion": "P4Kube\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684Kubernetes\u670d\u52a1\u8d1f\u8f7d\u5747\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u76f4\u63a5\u96c6\u6210P4\u6570\u636e\u5e73\u9762\u548cKubernetes\u63a7\u5236\u5e73\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.05525", "pdf": "https://arxiv.org/pdf/2505.05525", "abs": "https://arxiv.org/abs/2505.05525", "authors": ["Selim Mecanna", "Aurore Loisy", "Christophe Eloy"], "title": "A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Navigating in a fluid flow while being carried by it, using only information\naccessible from on-board sensors, is a problem commonly faced by small\nplanktonic organisms. It is also directly relevant to autonomous robots\ndeployed in the oceans. In the last ten years, the fluid mechanics community\nhas widely adopted reinforcement learning, often in the form of its simplest\nimplementations, to address this challenge. But it is unclear how good are the\nstrategies learned by these algorithms. In this paper, we perform a\nquantitative assessment of reinforcement learning methods applied to navigation\nin partially observable flows. We first introduce a well-posed problem of\ndirectional navigation for which a quasi-optimal policy is known analytically.\nWe then report on the poor performance and robustness of commonly used\nalgorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered\nin the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and\ntwo-dimensional turbulence. We show that they are vastly surpassed by PPO\n(Proximal Policy Optimization), a more advanced algorithm that has established\ndominance across a wide range of benchmarks in the reinforcement learning\ncommunity. In particular, our custom implementation of PPO matches the\ntheoretical quasi-optimal performance in turbulent flow and does so in a robust\nmanner. Reaching this result required the use of several additional techniques,\nsuch as vectorized environments and generalized advantage estimation, as well\nas hyperparameter optimization. This study demonstrates the importance of\nalgorithm selection, implementation details, and fine-tuning for discovering\ntruly smart autonomous navigation strategies in complex flows.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6d41\u4f53\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5e38\u7528\u7b97\u6cd5\uff08Q\u5b66\u4e60\u3001A2C\uff09\u6027\u80fd\u4e0d\u4f73\uff0c\u800cPPO\u7b97\u6cd5\u7ed3\u5408\u4f18\u5316\u6280\u672f\u80fd\u8fbe\u5230\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u9c81\u68d2\u5bfc\u822a\u6548\u679c\u3002", "motivation": "\u6d6e\u6e38\u751f\u7269\u6216\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u6d41\u4f53\u4e2d\u4ec5\u9760\u81ea\u8eab\u4f20\u611f\u5668\u5bfc\u822a\u662f\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u3002\u8fd1\u5e74\u6765\u6d41\u4f53\u529b\u5b66\u754c\u5e7f\u6cdb\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u5e38\u7528\u7b97\u6cd5\u7684\u6709\u6548\u6027\u5e76\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5b9a\u91cf\u8bc4\u4f30\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5177\u6709\u5df2\u77e5\u51c6\u6700\u4f18\u89e3\u7684\u5b9a\u5411\u5bfc\u822a\u95ee\u9898\u3002\u7136\u540e\uff0c\u5728\u591a\u79cd\u5e38\u89c1\u6d41\u573a\uff08\u6cf0\u52d2-\u683c\u6797\u6da1\u3001ABC\u6d41\u3001\u4e8c\u7ef4\u6e4d\u6d41\uff09\u4e2d\u6d4b\u8bd5\u4e86\u5e38\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08Q\u5b66\u4e60\u3001A2C\uff09\u548c\u66f4\u5148\u8fdb\u7684PPO\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u7814\u7a76\u8fd8\u7279\u522b\u5b9e\u73b0\u5e76\u4f18\u5316\u4e86PPO\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u5411\u91cf\u5316\u73af\u5883\u3001\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1\uff08GAE\uff09\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5e38\u7528\u7684Q\u5b66\u4e60\u548cA2C\u7b97\u6cd5\u5728\u6d4b\u8bd5\u6d41\u573a\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u9c81\u68d2\u6027\u5dee\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cPPO\u7b97\u6cd5\u6027\u80fd\u663e\u8457\u66f4\u4f18\u3002\u7ecf\u8fc7\u7279\u522b\u5b9e\u73b0\u548c\u4f18\u5316\u7684PPO\uff0c\u5728\u6e4d\u6d41\u4e2d\u80fd\u591f\u7a33\u5065\u5730\u8fbe\u5230\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "\u5bf9\u4e8e\u5728\u590d\u6742\u6d41\u573a\u4e2d\u53d1\u73b0\u771f\u6b63\u6709\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u7b56\u7565\u800c\u8a00\uff0c\u9009\u62e9\u5408\u9002\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3001\u5173\u6ce8\u5b9e\u65bd\u7ec6\u8282\u4ee5\u53ca\u8fdb\u884c\u7cbe\u7ec6\u8c03\u4f18\u81f3\u5173\u91cd\u8981\u3002PPO\u662f\u6bd4\u4f20\u7edf\u7b80\u5355\u7b97\u6cd5\u66f4\u6709\u6548\u7684\u9009\u62e9\u3002"}}
{"id": "2505.05755", "pdf": "https://arxiv.org/pdf/2505.05755", "abs": "https://arxiv.org/abs/2505.05755", "authors": ["Dhruvesh Patel", "Aishwarya Sahoo", "Avinash Amballa", "Tahira Naseem", "Tim G. J. Rudner", "Andrew McCallum"], "title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63d2\u5165\u5f0f\u8bed\u8a00\u6a21\u578b\uff08ILM\uff09\uff0c\u5b83\u901a\u8fc7\u5728\u4efb\u610f\u4f4d\u7f6e\u9010\u4e2a\u63d2\u5165\u6807\u8bb0\u6765\u751f\u6210\u5e8f\u5217\uff0c\u5728\u89c4\u5212\u4efb\u52a1\u548c\u4efb\u610f\u957f\u5ea6\u6587\u672c\u586b\u5145\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\uff08ARM\uff09\u96be\u4ee5\u5904\u7406\u590d\u6742\u7ea6\u675f\u548c\u975e\u987a\u5e8f\u4f9d\u8d56\uff0c\u800c\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u53ef\u80fd\u4ea7\u751f\u4e0d\u8fde\u8d2f\u8f93\u51fa\u4e14\u96be\u4ee5\u5904\u7406\u672a\u77e5\u957f\u5ea6\u7684\u586b\u5145\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u63d2\u5165\u5f0f\u8bed\u8a00\u6a21\u578b\uff08ILM\uff09\uff0c\u8be5\u6a21\u578b\u5b66\u4e60\u5728\u5e8f\u5217\u7684\u4efb\u610f\u4f4d\u7f6e\u8054\u5408\u9009\u62e9\u63d2\u5165\u4f4d\u7f6e\u548c\u8bcd\u6c47\u5143\u7d20\uff0c\u6bcf\u6b21\u63d2\u5165\u4e00\u4e2a\u6807\u8bb0\u3002\u4f7f\u7528\u5b9a\u5236\u7684\u7f51\u7edc\u53c2\u6570\u5316\u548c\u53bb\u566a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ILM \u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u4f18\u4e8e ARM \u548c MDM\u3002\u5728\u65e0\u6761\u4ef6\u6587\u672c\u751f\u6210\u4e2d\uff0cILM \u4f18\u4e8e MDM \u4e14\u4e0e ARM \u8868\u73b0\u76f8\u5f53\u3002\u5728\u4efb\u610f\u957f\u5ea6\u6587\u672c\u586b\u5145\u65b9\u9762\uff0cILM \u6bd4 MDM \u66f4\u7075\u6d3b\u3002", "conclusion": "ILM \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff0c\u80fd\u6709\u6548\u5904\u7406\u975e\u987a\u5e8f\u4f9d\u8d56\u548c\u590d\u6742\u7ea6\u675f\uff0c\u5728\u89c4\u5212\u548c\u7075\u6d3b\u6587\u672c\u586b\u5145\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2505.05612", "pdf": "https://arxiv.org/pdf/2505.05612", "abs": "https://arxiv.org/abs/2505.05612", "authors": ["Qing Wang", "Yining Pan", "Minghao Zhou", "Zijia Tang", "Yanfei Wang", "Guangyu Wang", "Qianqian Song"], "title": "scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "comment": "14 pages, 7 figures", "summary": "Drug resistance presents a major challenge in cancer therapy. Single cell\nprofiling offers insights into cellular heterogeneity, yet the application of\nlarge-scale foundation models for predicting drug response in single cell data\nremains underexplored. To address this, we developed scDrugMap, an integrated\nframework featuring both a Python command-line interface and a web server for\ndrug response prediction. scDrugMap evaluates a wide range of foundation\nmodels, including eight single-cell models and two large language models, using\na curated dataset of over 326,000 cells in the primary collection and 18,800\ncells in the validation set, spanning 36 datasets and diverse tissue and cancer\ntypes. We benchmarked model performance under pooled-data and cross-data\nevaluation settings, employing both layer freezing and Low-Rank Adaptation\n(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation\nachieved the best performance, with mean F1 scores of 0.971 (layer freezing)\nand 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.\nIn the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),\nwhile scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap\nprovides the first large-scale benchmark of foundation models for drug response\nprediction in single-cell data and serves as a user-friendly, flexible platform\nfor advancing drug discovery and translational research.", "AI": {"tldr": "scDrugMap\u662f\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u9884\u6d4b\u836f\u7269\u53cd\u5e94\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\u3002", "motivation": "\u764c\u75c7\u6cbb\u7597\u4e2d\u7684\u8010\u836f\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u800c\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u9884\u6d4b\u836f\u7269\u53cd\u5e94\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u4e86scDrugMap\u6846\u67b6\uff08\u542bPython\u547d\u4ee4\u884c\u754c\u9762\u548cWeb\u670d\u52a1\u5668\uff09\uff0c\u8bc4\u4f30\u4e86\u5305\u62ec8\u4e2a\u5355\u7ec6\u80de\u6a21\u578b\u548c2\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u57fa\u7840\u6a21\u578b\u3002\u4f7f\u7528\u4e86\u5305\u542b\u8d85\u8fc732.6\u4e07\u4e2a\u7ec6\u80de\u7684\u7b56\u5212\u6570\u636e\u96c6\uff0c\u5728\u6df7\u5408\u6570\u636e\u548c\u4ea4\u53c9\u6570\u636e\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\uff0c\u91c7\u7528\u5c42\u51bb\u7ed3\u548cLoRA\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u573a\u666f\u4e2d\uff0cscFoundation\u8868\u73b0\u6700\u4f73\uff08\u5c42\u51bb\u7ed3F1\u5747\u503c0.971\uff0c\u5fae\u8c03F1\u5747\u503c0.947\uff09\u3002\u5728\u4ea4\u53c9\u6570\u636e\u573a\u666f\u4e2d\uff0cUCE\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\uff08F1\u5747\u503c0.774\uff09\uff0cscGPT\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u9886\u5148\uff08F1\u5747\u503c0.858\uff09\u3002", "conclusion": "scDrugMap\u9996\u6b21\u5bf9\u7528\u4e8e\u5355\u7ec6\u80de\u6570\u636e\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u3001\u7075\u6d3b\u7684\u5e73\u53f0\uff0c\u4ee5\u4fc3\u8fdb\u836f\u7269\u53d1\u73b0\u548c\u8f6c\u5316\u7814\u7a76\u3002"}}
{"id": "2505.05491", "pdf": "https://arxiv.org/pdf/2505.05491", "abs": "https://arxiv.org/abs/2505.05491", "authors": ["TianYi Yu"], "title": "MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Detection of small objects, especially traffic signs, is a critical\nsub-task in object detection and autonomous driving. Despite signficant\nprogress in previous research, two main challenges remain. First, the issue of\nfeature extraction being too singular. Second, the detection process struggles\nto efectively handle objects of varying sizes or scales. These problems are\nalso prevalent in general object detection tasks. To address these challenges,\nwe propose a novel object detection network, Mamba-based Dynamic Dual Fusion\nNetwork (MDDFNet), for traffic sign detection. The network integrates a dynamic\ndual fusion module and a Mamba-based backbone to simultaneously tackle the\naforementioned issues. Specifically, the dynamic dual fusion module utilizes\nmultiple branches to consolidate various spatial and semantic information, thus\nenhancing feature diversity. The Mamba-based backbone leverages global feature\nfusion and local feature interaction, combining features in an adaptive manner\nto generate unique classification characteristics. Extensive experiments\nconducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that\nMDDFNet outperforms other state-of-the-art detectors, maintaining real-time\nprocessing capabilities of single-stage models while achieving superior\nperformance. This confirms the efectiveness of MDDFNet in detecting small\ntraffic signs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDDFNet\u7684\u65b0\u578b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u4e13\u7528\u4e8e\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u63d0\u53d6\u5355\u4e00\u548c\u591a\u5c3a\u5ea6\u7269\u4f53\u68c0\u6d4b\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5c0f\u76ee\u6807\uff08\u7279\u522b\u662f\u4ea4\u901a\u6807\u5fd7\uff09\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u63d0\u53d6\u8fc7\u4e8e\u5355\u4e00\u548c\u96be\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u540c\u5c3a\u5bf8\u7269\u4f53\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Mamba\u52a8\u6001\u53cc\u91cd\u878d\u5408\u7f51\u7edc\uff08MDDFNet\uff09\u3002\u8be5\u7f51\u7edc\u5305\u542b\u4e00\u4e2a\u52a8\u6001\u53cc\u91cd\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u5206\u652f\u6574\u5408\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u4ee5\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\uff1b\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8eMamba\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u5229\u7528\u5168\u5c40\u7279\u5f81\u878d\u5408\u548c\u5c40\u90e8\u7279\u5f81\u4ea4\u4e92\u81ea\u9002\u5e94\u5730\u7ec4\u5408\u7279\u5f81\u3002", "result": "\u5728TT100K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMDDFNet\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6SOTA\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5355\u9636\u6bb5\u6a21\u578b\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u5e76\u53d6\u5f97\u4e86\u66f4\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "MDDFNet\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u5c0f\u578b\u4ea4\u901a\u6807\u5fd7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u591a\u6837\u6027\u548c\u591a\u5c3a\u5ea6\u9002\u5e94\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.06025", "pdf": "https://arxiv.org/pdf/2505.06025", "abs": "https://arxiv.org/abs/2505.06025", "authors": ["Jianpeng Qi", "Chao Liu", "Chengxiang Xu", "Rui Wang", "Junyu Dong", "Yanwei Yu"], "title": "Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI", "categories": ["cs.NI", "cs.DC", "cs.SY", "eess.SY"], "comment": "11pages, 40 figures", "summary": "Timely and efficient dissemination of service information is critical in\ncompute-first networking systems, where user requests arrive dynamically and\ncomputing resources are constrained. In such systems, the access point (AP)\nplays a key role in forwarding user requests to a server based on its latest\nreceived service information. This paper considers a single-source,\nsingle-destination system and introduces an Age-and-Value-Aware (AVA) metric\nthat jointly captures both the timeliness and the task relevance of service\ninformation. Unlike traditional freshness-based metrics, AVA explicitly\nincorporates variations in server-side service capacity and AP forwarding\ndecisions, allowing more context-aware update evaluation. Building upon AVA, we\npropose a reinforcement learning-based update policy that learns to selectively\ntransmit service information updates to the AP. It aims to maximize overall\ntask success while minimizing unnecessary communications. Extensive simulations\nunder diverse user request patterns and varying service capacities demonstrate\nthat AVA reduces the update frequency by over 90% on average compared to\nbaselines, with reductions reaching 98% in certain configurations. Crucially,\nthis reduction is achieved without compromising the accuracy of task execution\nor the quality of decision making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5e74\u9f84\u548c\u4ef7\u503c\u611f\u77e5 (AVA) \u6307\u6807\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u66f4\u65b0\u7b56\u7565\uff0c\u4ee5\u5728\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\u4e2d\u9ad8\u6548\u5206\u53d1\u670d\u52a1\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u66f4\u65b0\u9891\u7387\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u7528\u6237\u8bf7\u6c42\u52a8\u6001\u4e14\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\u7cfb\u7edf\u4e2d\uff0c\u670d\u52a1\u4fe1\u606f\u7684\u53ca\u65f6\u9ad8\u6548\u5206\u53d1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u57fa\u4e8e\u65b0\u9c9c\u5ea6\u7684\u6307\u6807\u672a\u80fd\u5145\u5206\u8003\u8651\u670d\u52a1\u5bb9\u91cf\u53d8\u5316\u548c\u63a5\u5165\u70b9\u8f6c\u53d1\u51b3\u7b56\uff0c\u5bfc\u81f4\u66f4\u65b0\u6548\u7387\u4e0d\u9ad8\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u5e74\u9f84\u548c\u4ef7\u503c\u611f\u77e5 (AVA) \u6307\u6807\uff0c\u8be5\u6307\u6807\u7ed3\u5408\u4e86\u670d\u52a1\u4fe1\u606f\u7684\u53ca\u65f6\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u5e76\u8003\u8651\u4e86\u670d\u52a1\u5668\u7aef\u670d\u52a1\u80fd\u529b\u548c\u63a5\u5165\u70b9 (AP) \u8f6c\u53d1\u51b3\u7b56\u7684\u53d8\u5316\u30022. \u57fa\u4e8e AVA \u6307\u6807\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u66f4\u65b0\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5b66\u4e60\u9009\u62e9\u6027\u5730\u5411 AP \u4f20\u8f93\u670d\u52a1\u4fe1\u606f\u66f4\u65b0\uff0c\u65e8\u5728\u6700\u5927\u5316\u4efb\u52a1\u6210\u529f\u7387\u540c\u65f6\u6700\u5c0f\u5316\u4e0d\u5fc5\u8981\u7684\u901a\u4fe1\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0cAVA \u65b9\u6cd5\u5e73\u5747\u51cf\u5c11\u4e86\u8d85\u8fc7 90% \u7684\u66f4\u65b0\u9891\u7387\uff0c\u5728\u67d0\u4e9b\u914d\u7f6e\u4e0b\u6700\u9ad8\u53ef\u8fbe 98%\u3002\u5173\u952e\u5728\u4e8e\uff0c\u8fd9\u79cd\u66f4\u65b0\u9891\u7387\u7684\u964d\u4f4e\u5e76\u672a\u727a\u7272\u4efb\u52a1\u6267\u884c\u7684\u51c6\u786e\u6027\u6216\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "AVA \u6307\u6807\u548c\u76f8\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u66f4\u65b0\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u4f18\u5148\u7f51\u7edc\u4e2d\u670d\u52a1\u4fe1\u606f\u5206\u53d1\u7684\u6548\u7387\uff0c\u901a\u8fc7\u5927\u5e45\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u901a\u4fe1\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2505.05527", "pdf": "https://arxiv.org/pdf/2505.05527", "abs": "https://arxiv.org/abs/2505.05527", "authors": ["Giovanni Perin", "Cesare Bidini", "Riccardo Mazzieri", "Michele Rossi"], "title": "ADMM-Based Training for Spiking Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "eess.SP", "math.OC"], "comment": "6 pages, 4 figures. Preprint submitted to IEEE MLSP 2025", "summary": "In recent years, spiking neural networks (SNNs) have gained momentum due to\ntheir high potential in time-series processing combined with minimal energy\nconsumption. However, they still lack a dedicated and efficient training\nalgorithm. The popular backpropagation with surrogate gradients, adapted from\nstochastic gradient descent (SGD)-derived algorithms, has several drawbacks\nwhen used as an optimizer for SNNs. Specifically, it suffers from low\nscalability and numerical imprecision. In this paper, we propose a novel SNN\ntraining method based on the alternating direction method of multipliers\n(ADMM). Our ADMM-based training aims to solve the problem of the SNN step\nfunction's non-differentiability. We formulate the problem, derive closed-form\nupdates, and empirically show the optimizer's convergence properties, great\npotential, and possible new research directions to improve the method in a\nsimulated proof-of-concept.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eADMM\u7684\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u66ff\u4ee3\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684SNN\u8bad\u7ec3\u7b97\u6cd5\uff08\u5982\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\uff09\u5b58\u5728\u53ef\u6269\u5c55\u6027\u4f4e\u548c\u6570\u503c\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u4e14SNN\u7684\u9636\u8dc3\u51fd\u6570\u4e0d\u53ef\u5fae\u7ed9\u8bad\u7ec3\u5e26\u6765\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e58\u5b50\u4ea4\u66ff\u65b9\u5411\u6cd5\uff08ADMM\uff09\u7684SNN\u8bad\u7ec3\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u8bad\u7ec3\u8868\u8ff0\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u51fa\u95ed\u5f0f\u66f4\u65b0\u89c4\u5219\uff0c\u4ee5\u89e3\u51b3SNN\u9636\u8dc3\u51fd\u6570\u7684\u4e0d\u53ef\u5fae\u6027\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u7684\u6982\u5ff5\u9a8c\u8bc1\u4e2d\uff0c\u7ecf\u9a8c\u8bc1\u4e86\u8be5ADMM\u4f18\u5316\u5668\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u7279\u6027\u548c\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8eADMM\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e3aSNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u4f18\u5316\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u4e0d\u53ef\u5fae\u6027\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdb\u8be5\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05772", "pdf": "https://arxiv.org/pdf/2505.05772", "abs": "https://arxiv.org/abs/2505.05772", "authors": ["Zehao Fan", "Garrett Gagnon", "Zhenyu Liu", "Liu Liu"], "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.", "AI": {"tldr": "\u63d0\u51faSTARC\uff0c\u4e00\u79cd\u9488\u5bf9PIM\u67b6\u6784\u4f18\u5316\u7684\u7a00\u758f\u6570\u636e\u6620\u5c04\u65b9\u6848\uff0c\u901a\u8fc7\u805a\u7c7bKV\u7f13\u5b58\u5e76\u5bf9\u9f50\u5185\u5b58\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4eLLM\u89e3\u7801\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u56de\u5f52\u89e3\u7801\u56e0\u9891\u7e41\u5185\u5b58\u8bbf\u95ee\u548c\u4e0d\u65ad\u589e\u957f\u7684KV\u7f13\u5b58\u800c\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u3002\u5185\u5b58\u5904\u7406\uff08PIM\uff09\u67b6\u6784\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u8bbe\u8ba1\u96be\u4ee5\u9ad8\u6548\u5904\u7406KV\u7f13\u5b58\u7a00\u758f\u5316\u5e26\u6765\u7684\u4e0d\u89c4\u5219\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u6548\u7387\u964d\u4f4e\u3002", "method": "\u63d0\u51faSTARC\u65b9\u6848\uff1a1) \u6839\u636e\u8bed\u4e49\u76f8\u4f3c\u6027\u5c06KV\u5bf9\u805a\u7c7b\uff1b2) \u5c06\u805a\u7c7b\u6620\u5c04\u5230\u4e0ePIM\u5185\u5b58bank\u7ed3\u6784\u5bf9\u9f50\u7684\u8fde\u7eed\u5185\u5b58\u533a\u57df\uff1b3) \u89e3\u7801\u65f6\uff0c\u67e5\u8be2\u901a\u8fc7\u5339\u914d\u9884\u8ba1\u7b97\u7684\u805a\u7c7b\u8d28\u5fc3\u6765\u68c0\u7d22\u76f8\u5173\u4ee4\u724c\u7c07\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u6ce8\u610f\u529b\u548c\u5e76\u884c\u5904\u7406\uff0c\u907f\u514d\u9891\u7e41\u91cd\u805a\u7c7b\u548c\u6570\u636e\u79fb\u52a8\u5f00\u9500\u3002", "result": "\u5728HBM-PIM\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5e38\u89c1\u7684token\u7ea7\u7a00\u758f\u65b9\u6cd5\u76f8\u6bd4\uff0cSTARC\u5c06\u6ce8\u610f\u529b\u5c42\u5ef6\u8fdf\u964d\u4f4e\u4e8619%-31%\uff0c\u80fd\u8017\u964d\u4f4e\u4e8619%-27%\u3002\u57281024\u7684KV\u7f13\u5b58\u9884\u7b97\u4e0b\uff0c\u4e0e\u5168KV\u7f13\u5b58\u68c0\u7d22\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe54%-74%\uff0c\u80fd\u8017\u964d\u4f4e45%-67%\u3002", "conclusion": "STARC\u901a\u8fc7\u6709\u6548\u7ba1\u7406KV\u7f13\u5b58\u7a00\u758f\u6027\uff0c\u80fd\u591f\u5728PIM\u67b6\u6784\u4e0a\u5b9e\u73b0\u9ad8\u6548\u4e14\u786c\u4ef6\u53cb\u597d\u7684\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5148\u8fdb\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u76f8\u5f53\u7684\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2505.05616", "pdf": "https://arxiv.org/pdf/2505.05616", "abs": "https://arxiv.org/abs/2505.05616", "authors": ["Lorenzo Di Fruscia", "Jana Marie Weber"], "title": "Leveraging Large Language Models for enzymatic reaction prediction and characterization", "categories": ["cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting enzymatic reactions is crucial for applications in biocatalysis,\nmetabolic engineering, and drug discovery, yet it remains a complex and\nresource-intensive task. Large Language Models (LLMs) have recently\ndemonstrated remarkable success in various scientific domains, e.g., through\ntheir ability to generalize knowledge, reason over complex structures, and\nleverage in-context learning strategies. In this study, we systematically\nevaluate the capability of LLMs, particularly the Llama-3.1 family (8B and\n70B), across three core biochemical tasks: Enzyme Commission number prediction,\nforward synthesis, and retrosynthesis. We compare single-task and multitask\nlearning strategies, employing parameter-efficient fine-tuning via LoRA\nadapters. Additionally, we assess performance across different data regimes to\nexplore their adaptability in low-data settings. Our results demonstrate that\nfine-tuned LLMs capture biochemical knowledge, with multitask learning\nenhancing forward- and retrosynthesis predictions by leveraging shared\nenzymatic information. We also identify key limitations, for example challenges\nin hierarchical EC classification schemes, highlighting areas for further\nimprovement in LLM-driven biochemical modeling.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama-3.1\uff09\u5728\u9176\u4fc3\u53cd\u5e94\u9884\u6d4b\uff08EC\u53f7\u3001\u6b63\u5411\u5408\u6210\u3001\u9006\u5408\u6210\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5fae\u8c03\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\uff0c\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u9176\u4fc3\u53cd\u5e94\u9884\u6d4b\u5bf9\u751f\u7269\u50ac\u5316\u3001\u4ee3\u8c22\u5de5\u7a0b\u548c\u836f\u7269\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Llama-3.1\u6a21\u578b\uff088B\u548c70B\uff09\uff0c\u901a\u8fc7LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5176\u5728\u9176EC\u53f7\u9884\u6d4b\u3001\u6b63\u5411\u5408\u6210\u548c\u9006\u5408\u6210\u4e09\u4e2a\u6838\u5fc3\u751f\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u6bd4\u8f83\u4e86\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u5728\u4e0d\u540c\u6570\u636e\u91cf\uff08\u5305\u62ec\u4f4e\u6570\u636e\uff09\u4e0b\u7684\u9002\u5e94\u6027\u3002", "result": "\u5fae\u8c03\u540e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6355\u83b7\u751f\u7269\u5316\u5b66\u77e5\u8bc6\u3002\u591a\u4efb\u52a1\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u5171\u4eab\u7684\u9176\u4fe1\u606f\u589e\u5f3a\u4e86\u6b63\u5411\u5408\u6210\u548c\u9006\u5408\u6210\u7684\u9884\u6d4b\u80fd\u529b\u3002\u7814\u7a76\u4e5f\u53d1\u73b0\u4e86\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u5728\u5206\u5c42EC\u5206\u7c7b\u65b9\u6848\u4e2d\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u5316\u5b66\u5efa\u6a21\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u5c24\u5176\u6709\u6548\uff0c\u4f46\u5728\u5904\u7406\u5982EC\u53f7\u9884\u6d4b\u7b49\u590d\u6742\u5206\u5c42\u4efb\u52a1\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.05492", "pdf": "https://arxiv.org/pdf/2505.05492", "abs": "https://arxiv.org/abs/2505.05492", "authors": ["Ignacy St\u0119pka", "Lukasz Sztukiewicz", "Micha\u0142 Wili\u0144ski", "Jerzy Stefanowski"], "title": "DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While machine learning fairness has made significant progress in recent\nyears, most existing solutions focus on tabular data and are poorly suited for\nvision-based classification tasks, which rely heavily on deep learning. To\nbridge this gap, we introduce DetoxAI, an open-source Python library for\nimproving fairness in deep learning vision classifiers through post-hoc\ndebiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness\nmetrics, and visualization tools. It supports debiasing via interventions in\ninternal representations and includes attribution-based visualization tools and\nquantitative algorithmic fairness metrics to show how bias is mitigated. This\npaper presents the motivation, design, and use cases of DetoxAI, demonstrating\nits tangible value to engineers and researchers.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86DetoxAI\uff0c\u4e00\u4e2a\u7528\u4e8e\u901a\u8fc7\u4e8b\u540e\u53bb\u504f\u6765\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u5206\u7c7b\u5668\u516c\u5e73\u6027\u7684\u5f00\u6e90Python\u5e93\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u8868\u683c\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u5f15\u5165DetoxAI\u5e93\uff0c\u8be5\u5e93\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u53bb\u504f\u7b97\u6cd5\u3001\u516c\u5e73\u6027\u6307\u6807\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u652f\u6301\u901a\u8fc7\u5e72\u9884\u5185\u90e8\u8868\u5f81\u8fdb\u884c\u53bb\u504f\uff0c\u5e76\u5305\u542b\u57fa\u4e8e\u5f52\u56e0\u7684\u53ef\u89c6\u5316\u548c\u91cf\u5316\u516c\u5e73\u6027\u6307\u6807\u3002", "result": "DetoxAI\u63d0\u4f9b\u4e86\u5de5\u5177\u6765\u51cf\u8f7b\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u91cf\u5316\u6307\u6807\u5c55\u793a\u504f\u89c1\u5982\u4f55\u88ab\u7f13\u89e3\uff0c\u4ece\u800c\u4e3a\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5b9e\u9645\u4ef7\u503c\u3002", "conclusion": "DetoxAI\u4e3a\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u51b3\u548c\u6539\u5584\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u5206\u7c7b\u5668\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2505.06041", "pdf": "https://arxiv.org/pdf/2505.06041", "abs": "https://arxiv.org/abs/2505.06041", "authors": ["Garegin Grigoryan", "Minseok Kwon", "M. Mustafa Rafique"], "title": "Extending the Control Plane of Container Orchestrators for I/O Virtualization", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Single Root Input/Output Virtualization (SR-IOV) is a standard technology for\nforking a single PCI express device and providing it to applications while\nensuring performance isolation. It enables container orchestrators to share a\nlimited number of physical network interfaces without incurring significant\nvirtualization overhead. The allocation of virtualized network devices to\ncontainers, however, needs to be more configurable based on the bandwidth needs\nof running applications. Moreover, container orchestrators' network control\nover the virtualized interfaces is limited by the abilities of SR-IOV. We\nexplore the design considerations for a system with controlled SR-IOV\nvirtualization and present ConRDMA, a novel architecture that enables fine\ncontrol of RDMA virtualization for containers. Our evaluation shows that\nConRDMA enables containers to use RDMA allocated bandwidth more efficiently and\nto select best-suited nodes to meet their varying communication requirements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faConRDMA\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u5bb9\u5668\u7684SR-IOV RDMA\u865a\u62df\u5316\u8fdb\u884c\u7cbe\u7ec6\u63a7\u5236\uff0c\u4ee5\u63d0\u9ad8\u5e26\u5bbd\u5229\u7528\u6548\u7387\u548c\u8282\u70b9\u9009\u62e9\u3002", "motivation": "\u6807\u51c6\u7684SR-IOV\u6280\u672f\u5728\u5c06\u865a\u62df\u5316\u7f51\u7edc\u8bbe\u5907\u5206\u914d\u7ed9\u5bb9\u5668\u65f6\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u5e94\u7528\u5e26\u5bbd\u9700\u6c42\u7684\u914d\u7f6e\u80fd\u529b\uff0c\u5e76\u4e14\u5bb9\u5668\u7f16\u6392\u5668\u5bf9\u865a\u62df\u5316\u63a5\u53e3\u7684\u7f51\u7edc\u63a7\u5236\u53d7\u9650\u4e8eSR-IOV\u672c\u8eab\u7684\u80fd\u529b\u3002", "method": "\u63a2\u7d22\u4e86\u53d7\u63a7SR-IOV\u865a\u62df\u5316\u7cfb\u7edf\u7684\u8bbe\u8ba1\u8003\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConRDMA\u7684\u65b0\u9896\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u5bf9\u5bb9\u5668\u7684RDMA\u865a\u62df\u5316\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cConRDMA\u4f7f\u5bb9\u5668\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f7f\u7528\u5206\u914d\u7684RDMA\u5e26\u5bbd\uff0c\u5e76\u4e14\u80fd\u591f\u6839\u636e\u5176\u4e0d\u540c\u7684\u901a\u4fe1\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u8282\u70b9\u3002", "conclusion": "ConRDMA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5bb9\u5668\u73af\u5883\u4e2dRDMA\u865a\u62df\u5316\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4ece\u800c\u4f18\u5316\u4e86\u5e26\u5bbd\u4f7f\u7528\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2505.05530", "pdf": "https://arxiv.org/pdf/2505.05530", "abs": "https://arxiv.org/abs/2505.05530", "authors": ["Kai Liu", "Qian Zheng", "Kaiwen Tao", "Zhiteng Li", "Haotong Qin", "Wenbo Li", "Yong Guo", "Xianglong Liu", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "Low-bit Model Quantization for Deep Neural Networks: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "We have systematically collected and reviewed the state-of-the-art\n  quantization methods from the past five years, categorizing them into eight\n  distinct groups. A curated list of model quantization is provided at\n  https://github.com/Kai-Liu001/Awesome-Model-Quantization", "summary": "With unprecedented rapid development, deep neural networks (DNNs) have deeply\ninfluenced almost all fields. However, their heavy computation costs and model\nsizes are usually unacceptable in real-world deployment. Model quantization, an\neffective weight-lighting technique, has become an indispensable procedure in\nthe whole deployment pipeline. The essence of quantization acceleration is the\nconversion from continuous floating-point numbers to discrete integer ones,\nwhich significantly speeds up the memory I/O and calculation, i.e., addition\nand multiplication. However, performance degradation also comes with the\nconversion because of the loss of precision. Therefore, it has become\nincreasingly popular and critical to investigate how to perform the conversion\nand how to compensate for the information loss. This article surveys the recent\nfive-year progress towards low-bit quantization on DNNs. We discuss and compare\nthe state-of-the-art quantization methods and classify them into 8 main\ncategories and 24 sub-categories according to their core techniques.\nFurthermore, we shed light on the potential research opportunities in the field\nof model quantization. A curated list of model quantization is provided at\nhttps://github.com/Kai-Liu001/Awesome-Model-Quantization.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u603b\u7ed3\u4e86\u8fd1\u4e94\u5e74\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4f4e\u6bd4\u7279\u91cf\u5316\u6280\u672f\u7684\u8fdb\u5c55\uff0c\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "DNN\u8ba1\u7b97\u91cf\u5927\u3001\u6a21\u578b\u5c3a\u5bf8\u5927\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u4e2d\u90e8\u7f72\u3002\u6a21\u578b\u91cf\u5316\u53ef\u4ee5\u6709\u6548\u51cf\u5c0f\u6a21\u578b\u5e76\u52a0\u901f\u8ba1\u7b97\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u6709\u6548\u91cf\u5316\u5e76\u8865\u507f\u4fe1\u606f\u635f\u5931\u53d8\u5f97\u5341\u5206\u91cd\u8981\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8c03\u7814\u8fc7\u53bb\u4e94\u5e74\u7684\u6587\u732e\uff0c\u8ba8\u8bba\u548c\u6bd4\u8f83\u4e86\u6700\u5148\u8fdb\u7684DNN\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u5176\u6838\u5fc3\u6280\u672f\u5c06\u5176\u5206\u4e3a8\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c24\u4e2a\u5b50\u7c7b\u522b\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u8fd1\u671fDNN\u91cf\u5316\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\u548c\u5206\u7c7b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u5e76\u6574\u7406\u4e86\u4e00\u4e2a\u76f8\u5173\u7684\u8d44\u6e90\u5217\u8868\u3002", "conclusion": "\u6a21\u578b\u91cf\u5316\u662fDNN\u90e8\u7f72\u6d41\u7a0b\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u6b65\u3002\u672c\u7efc\u8ff0\u7cfb\u7edf\u5730\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u65b9\u6cd5\u5206\u7c7b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2505.05815", "pdf": "https://arxiv.org/pdf/2505.05815", "abs": "https://arxiv.org/abs/2505.05815", "authors": ["Machi Shimmei", "Masaki Uto", "Yuichiroh Matsubayashi", "Kentaro Inui", "Aditi Mallavarapu", "Noboru Matsuda"], "title": "Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted", "categories": ["cs.CL"], "comment": "This is a pre-print version of a paper to appear in AIED2025", "summary": "The primary goal of this study is to develop and evaluate an innovative\nprompting technique, AnaQuest, for generating multiple-choice questions (MCQs)\nusing a pre-trained large language model. In AnaQuest, the choice items are\nsentence-level assertions about complex concepts. The technique integrates\nformative and summative assessments. In the formative phase, students answer\nopen-ended questions for target concepts in free text. For summative\nassessment, AnaQuest analyzes these responses to generate both correct and\nincorrect assertions. To evaluate the validity of the generated MCQs, Item\nResponse Theory (IRT) was applied to compare item characteristics between MCQs\ngenerated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An\nempirical study found that expert instructors rated MCQs generated by both AI\nmodels to be as valid as those created by human instructors. However, IRT-based\nanalysis revealed that AnaQuest-generated questions - particularly those with\nincorrect assertions (foils) - more closely resembled human-crafted items in\nterms of difficulty and discrimination than those produced by ChatGPT.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u540d\u4e3aAnaQuest\u7684\u521b\u65b0\u63d0\u793a\u6280\u672f\uff0c\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9009\u62e9\u9898\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfChatGPT\u3002", "motivation": "\u63d0\u9ad8\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9009\u62e9\u9898\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u751f\u6210\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u51fa\u9898\u8005\u6c34\u5e73\u7684\u9898\u76ee\u3002", "method": "\u7814\u7a76\u63d0\u51faAnaQuest\u63d0\u793a\u6280\u672f\u3002\u8be5\u6280\u672f\u9996\u5148\u8ba9\u5b66\u751f\u4ee5\u81ea\u7531\u6587\u672c\u56de\u7b54\u5f00\u653e\u5f0f\u95ee\u9898\uff08\u5f62\u6210\u6027\u8bc4\u4f30\uff09\uff0c\u7136\u540eAnaQuest\u5206\u6790\u8fd9\u4e9b\u56de\u7b54\uff0c\u751f\u6210\u5173\u4e8e\u76ee\u6807\u6982\u5ff5\u7684\u6b63\u786e\u53ca\u9519\u8bef\u65ad\u8a00\u4f5c\u4e3a\u9009\u62e9\u9898\u9009\u9879\uff08\u603b\u7ed3\u6027\u8bc4\u4f30\uff09\u3002\u901a\u8fc7\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u5bf9\u6bd4AnaQuest\u3001\u57fa\u7ebfChatGPT\u53ca\u4eba\u5de5\u751f\u6210\u7684\u9009\u62e9\u9898\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u8ba4\u4e3a\uff0cAI\u6a21\u578b\uff08AnaQuest\u548cChatGPT\uff09\u751f\u6210\u7684\u9009\u62e9\u9898\u4e0e\u4eba\u5de5\u521b\u5efa\u7684\u9009\u62e9\u9898\u4e00\u6837\u6709\u6548\u3002\u7136\u800c\uff0c\u57fa\u4e8eIRT\u7684\u5206\u6790\u8868\u660e\uff0cAnaQuest\u751f\u6210\u7684\u9898\u76ee\uff0c\u7279\u522b\u662f\u5728\u9519\u8bef\u65ad\u8a00\uff08\u5e72\u6270\u9879\uff09\u65b9\u9762\uff0c\u5176\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u6bd4ChatGPT\u751f\u6210\u7684\u9898\u76ee\u66f4\u63a5\u8fd1\u4eba\u5de5\u521b\u5efa\u7684\u9898\u76ee\u3002", "conclusion": "AnaQuest\u63d0\u793a\u6280\u672f\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u5de5\u51fa\u9898\u8d28\u91cf\u76f8\u5f53\u7684\u9009\u62e9\u9898\uff0c\u5e76\u4e14\u5728\u9898\u76ee\u96be\u5ea6\u548c\u533a\u5206\u5ea6\uff0c\u7279\u522b\u662f\u5e72\u6270\u9879\u7684\u8d28\u91cf\u4e0a\uff0c\u4f18\u4e8e\u57fa\u7ebfChatGPT\u63d0\u793a\u65b9\u6cd5\u3002"}}
{"id": "2505.05684", "pdf": "https://arxiv.org/pdf/2505.05684", "abs": "https://arxiv.org/abs/2505.05684", "authors": ["Han Wu", "Jie Yin"], "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot knowledge graph completion (KGC) has obtained significant attention\ndue to its practical applications in real-world scenarios, where new knowledge\noften emerges with limited available data. While most existing methods for\nfew-shot KGC have predominantly focused on leveraging relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ngap, we propose a novel prompted meta-learning (PromptMeta) framework that\nseamlessly integrates meta-semantics with relational information for few-shot\nKGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that\ncaptures and consolidates high-level meta-semantics, enabling effective\nknowledge transfer and adaptation to rare and newly emerging relations. (2) a\nlearnable fusion prompt that dynamically combines meta-semantic information\nwith task-specific relational information tailored to different few-shot tasks.\nBoth components are optimized together with model parameters within a\nmeta-learning framework. Extensive experiments on two benchmark datasets\ndemonstrate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPromptMeta\u7684\u65b0\u578b\u63d0\u793a\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5143\u8bed\u4e49\u4e0e\u5173\u7cfb\u4fe1\u606f\u6765\u63d0\u5347\u5c0f\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5c0f\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u5927\u591a\u53ea\u5173\u6ce8\u5173\u7cfb\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u65b0\u77e5\u8bc6\u51fa\u73b0\u4e14\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86PromptMeta\u6846\u67b6\uff0c\u5305\u542b\u4e24\u5927\u521b\u65b0\uff1a1) \u5143\u8bed\u4e49\u63d0\u793a\u6c60\uff0c\u7528\u4e8e\u6355\u83b7\u548c\u6574\u5408\u9ad8\u7ea7\u5143\u8bed\u4e49\uff0c\u4ee5\u9002\u5e94\u7a00\u6709\u548c\u65b0\u5174\u5173\u7cfb\uff1b2) \u53ef\u5b66\u4e60\u7684\u878d\u5408\u63d0\u793a\uff0c\u52a8\u6001\u7ed3\u5408\u5143\u8bed\u4e49\u4fe1\u606f\u4e0e\u4efb\u52a1\u7279\u5b9a\u7684\u5173\u7cfb\u4fe1\u606f\u3002\u4e24\u8005\u5728\u5143\u5b66\u4e60\u6846\u67b6\u4e0b\u5171\u540c\u4f18\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002", "conclusion": "PromptMeta\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6574\u5408\u5143\u8bed\u4e49\u548c\u5173\u7cfb\u4fe1\u606f\uff0c\u4e3a\u5c0f\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.05495", "pdf": "https://arxiv.org/pdf/2505.05495", "abs": "https://arxiv.org/abs/2505.05495", "authors": ["Siyuan Zhou", "Yilun Du", "Yuncong Yang", "Lei Han", "Peihao Chen", "Dit-Yan Yeung", "Chuang Gan"], "title": "Learning 3D Persistent Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The ability to simulate the effects of future actions on the world is a\ncrucial ability of intelligent embodied agents, enabling agents to anticipate\nthe effects of their actions and make plans accordingly. While a large body of\nexisting work has explored how to construct such world models using video\nmodels, they are often myopic in nature, without any memory of a scene not\ncaptured by currently observed images, preventing agents from making consistent\nlong-horizon plans in complex environments where many parts of the scene are\npartially observed. We introduce a new persistent embodied world model with an\nexplicit memory of previously generated content, enabling much more consistent\nlong-horizon simulation. During generation time, our video diffusion model\npredicts RGB-D video of the future observations of the agent. This generation\nis then aggregated into a persistent 3D map of the environment. By conditioning\nthe video model on this 3D spatial map, we illustrate how this enables video\nworld models to faithfully simulate both seen and unseen parts of the world.\nFinally, we illustrate the efficacy of such a world model in downstream\nembodied applications, enabling effective planning and policy learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u6301\u4e45\u60273D\u8bb0\u5fc6\u7684\u5177\u8eab\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u751f\u6210\u7684RGB-D\u89c6\u9891\u52303D\u5730\u56fe\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u73af\u5883\u66f4\u4e00\u81f4\u7684\u957f\u671f\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u4e16\u754c\u6a21\u578b\u901a\u5e38\u662f\u77ed\u89c6\u7684\uff0c\u7f3a\u4e4f\u5bf9\u5f53\u524d\u672a\u89c2\u5bdf\u5230\u7684\u573a\u666f\u90e8\u5206\u7684\u8bb0\u5fc6\uff0c\u8fd9\u963b\u788d\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u5236\u5b9a\u4e00\u81f4\u7684\u957f\u671f\u8ba1\u5212\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5177\u6709\u663e\u5f0f\u8bb0\u5fc6\u7684\u6301\u4e45\u5316\u5177\u8eab\u4e16\u754c\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u9884\u6d4b\u672a\u6765\u7684RGB-D\u89c6\u9891\uff0c\u5e76\u5c06\u751f\u6210\u7684\u5185\u5bb9\u805a\u5408\u5230\u4e00\u4e2a\u6301\u4e45\u76843D\u5730\u56fe\u4e2d\u3002\u89c6\u9891\u6a21\u578b\u4ee5\u8be53D\u7a7a\u95f4\u5730\u56fe\u4e3a\u6761\u4ef6\u8fdb\u884c\u751f\u6210\uff0c\u4ece\u800c\u6a21\u62df\u4e16\u754c\u7684\u5df2\u89c1\u548c\u672a\u89c1\u90e8\u5206\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u5fe0\u5b9e\u5730\u6a21\u62df\u4e16\u754c\u7684\u5df2\u89c1\u548c\u672a\u89c1\u90e8\u5206\uff0c\u5e76\u5728\u4e0b\u6e38\u7684\u5177\u8eab\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u652f\u6301\u4e86\u6709\u6548\u7684\u89c4\u5212\u548c\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u5177\u6709\u663e\u5f0f\u8bb0\u5fc6\u7684\u6301\u4e45\u5316\u5177\u8eab\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u6574\u54083D\u7a7a\u95f4\u5730\u56fe\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u957f\u671f\u6a21\u62df\uff0c\u5bf9\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c4\u5212\u548c\u7b56\u7565\u5b66\u4e60\u6709\u76ca\u3002"}}
